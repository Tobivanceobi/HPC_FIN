no change     /home/modelrep/sadiya/miniconda/condabin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda-env
no change     /home/modelrep/sadiya/miniconda/bin/activate
no change     /home/modelrep/sadiya/miniconda/bin/deactivate
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.sh
no change     /home/modelrep/sadiya/miniconda/etc/fish/conf.d/conda.fish
no change     /home/modelrep/sadiya/miniconda/shell/condabin/Conda.psm1
no change     /home/modelrep/sadiya/miniconda/shell/condabin/conda-hook.ps1
no change     /home/modelrep/sadiya/miniconda/lib/python3.10/site-packages/xontrib/conda.xsh
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.csh
no change     /home/modelrep/sadiya/.bashrc
No action taken.
-----------------
Process ID:  7
Number of Nodes:  1
Number of Array Tasks:  16
-----------------
Using device: cuda

AMD Instinct MI210
Memory Usage:
Allocated: 0.0 GB
Cached:    0.0 GB
theta
whole_spec
delta
alpha
beta
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/106], [94mLoss[0m : 11.04935
[1mStep[0m  [10/106], [94mLoss[0m : 3.99412
[1mStep[0m  [20/106], [94mLoss[0m : 2.50640
[1mStep[0m  [30/106], [94mLoss[0m : 2.70039
[1mStep[0m  [40/106], [94mLoss[0m : 2.55862
[1mStep[0m  [50/106], [94mLoss[0m : 2.37709
[1mStep[0m  [60/106], [94mLoss[0m : 2.48621
[1mStep[0m  [70/106], [94mLoss[0m : 2.48526
[1mStep[0m  [80/106], [94mLoss[0m : 2.52884
[1mStep[0m  [90/106], [94mLoss[0m : 2.30891
[1mStep[0m  [100/106], [94mLoss[0m : 2.59662

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.104, [92mTest[0m: 10.731, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.26709
[1mStep[0m  [10/106], [94mLoss[0m : 2.41861
[1mStep[0m  [20/106], [94mLoss[0m : 2.10552
[1mStep[0m  [30/106], [94mLoss[0m : 2.55833
[1mStep[0m  [40/106], [94mLoss[0m : 2.51022
[1mStep[0m  [50/106], [94mLoss[0m : 2.52727
[1mStep[0m  [60/106], [94mLoss[0m : 2.72081
[1mStep[0m  [70/106], [94mLoss[0m : 2.52210
[1mStep[0m  [80/106], [94mLoss[0m : 2.47080
[1mStep[0m  [90/106], [94mLoss[0m : 2.51876
[1mStep[0m  [100/106], [94mLoss[0m : 2.30945

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.445, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.26317
[1mStep[0m  [10/106], [94mLoss[0m : 2.43235
[1mStep[0m  [20/106], [94mLoss[0m : 2.51649
[1mStep[0m  [30/106], [94mLoss[0m : 2.79410
[1mStep[0m  [40/106], [94mLoss[0m : 2.51863
[1mStep[0m  [50/106], [94mLoss[0m : 2.60639
[1mStep[0m  [60/106], [94mLoss[0m : 2.47967
[1mStep[0m  [70/106], [94mLoss[0m : 2.25327
[1mStep[0m  [80/106], [94mLoss[0m : 2.48484
[1mStep[0m  [90/106], [94mLoss[0m : 2.59010
[1mStep[0m  [100/106], [94mLoss[0m : 2.45711

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.411, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.28450
[1mStep[0m  [10/106], [94mLoss[0m : 2.38599
[1mStep[0m  [20/106], [94mLoss[0m : 2.57587
[1mStep[0m  [30/106], [94mLoss[0m : 2.49106
[1mStep[0m  [40/106], [94mLoss[0m : 2.42901
[1mStep[0m  [50/106], [94mLoss[0m : 2.55411
[1mStep[0m  [60/106], [94mLoss[0m : 2.53816
[1mStep[0m  [70/106], [94mLoss[0m : 2.22282
[1mStep[0m  [80/106], [94mLoss[0m : 2.48774
[1mStep[0m  [90/106], [94mLoss[0m : 2.52100
[1mStep[0m  [100/106], [94mLoss[0m : 2.69090

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.408, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.30962
[1mStep[0m  [10/106], [94mLoss[0m : 2.70465
[1mStep[0m  [20/106], [94mLoss[0m : 2.61466
[1mStep[0m  [30/106], [94mLoss[0m : 2.29507
[1mStep[0m  [40/106], [94mLoss[0m : 2.34454
[1mStep[0m  [50/106], [94mLoss[0m : 2.35156
[1mStep[0m  [60/106], [94mLoss[0m : 2.57565
[1mStep[0m  [70/106], [94mLoss[0m : 2.72165
[1mStep[0m  [80/106], [94mLoss[0m : 2.78149
[1mStep[0m  [90/106], [94mLoss[0m : 2.26149
[1mStep[0m  [100/106], [94mLoss[0m : 2.54131

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.505, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.32694
[1mStep[0m  [10/106], [94mLoss[0m : 2.37817
[1mStep[0m  [20/106], [94mLoss[0m : 2.37396
[1mStep[0m  [30/106], [94mLoss[0m : 2.32369
[1mStep[0m  [40/106], [94mLoss[0m : 2.47239
[1mStep[0m  [50/106], [94mLoss[0m : 2.21445
[1mStep[0m  [60/106], [94mLoss[0m : 2.38243
[1mStep[0m  [70/106], [94mLoss[0m : 2.58173
[1mStep[0m  [80/106], [94mLoss[0m : 2.35067
[1mStep[0m  [90/106], [94mLoss[0m : 2.18216
[1mStep[0m  [100/106], [94mLoss[0m : 2.41928

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.444, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.67442
[1mStep[0m  [10/106], [94mLoss[0m : 2.37377
[1mStep[0m  [20/106], [94mLoss[0m : 2.21329
[1mStep[0m  [30/106], [94mLoss[0m : 2.57535
[1mStep[0m  [40/106], [94mLoss[0m : 2.48681
[1mStep[0m  [50/106], [94mLoss[0m : 2.15364
[1mStep[0m  [60/106], [94mLoss[0m : 2.43131
[1mStep[0m  [70/106], [94mLoss[0m : 2.40557
[1mStep[0m  [80/106], [94mLoss[0m : 2.54679
[1mStep[0m  [90/106], [94mLoss[0m : 2.89614
[1mStep[0m  [100/106], [94mLoss[0m : 2.57390

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.434, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.38338
[1mStep[0m  [10/106], [94mLoss[0m : 2.66292
[1mStep[0m  [20/106], [94mLoss[0m : 2.48275
[1mStep[0m  [30/106], [94mLoss[0m : 2.26855
[1mStep[0m  [40/106], [94mLoss[0m : 2.30001
[1mStep[0m  [50/106], [94mLoss[0m : 2.64031
[1mStep[0m  [60/106], [94mLoss[0m : 2.54108
[1mStep[0m  [70/106], [94mLoss[0m : 2.65217
[1mStep[0m  [80/106], [94mLoss[0m : 2.59799
[1mStep[0m  [90/106], [94mLoss[0m : 2.67963
[1mStep[0m  [100/106], [94mLoss[0m : 2.56615

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.404, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.23413
[1mStep[0m  [10/106], [94mLoss[0m : 2.27324
[1mStep[0m  [20/106], [94mLoss[0m : 2.27366
[1mStep[0m  [30/106], [94mLoss[0m : 2.35035
[1mStep[0m  [40/106], [94mLoss[0m : 2.19173
[1mStep[0m  [50/106], [94mLoss[0m : 2.35206
[1mStep[0m  [60/106], [94mLoss[0m : 2.26520
[1mStep[0m  [70/106], [94mLoss[0m : 2.15682
[1mStep[0m  [80/106], [94mLoss[0m : 2.26470
[1mStep[0m  [90/106], [94mLoss[0m : 2.40485
[1mStep[0m  [100/106], [94mLoss[0m : 2.46391

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.446, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.19337
[1mStep[0m  [10/106], [94mLoss[0m : 2.18397
[1mStep[0m  [20/106], [94mLoss[0m : 2.59749
[1mStep[0m  [30/106], [94mLoss[0m : 2.42031
[1mStep[0m  [40/106], [94mLoss[0m : 2.49565
[1mStep[0m  [50/106], [94mLoss[0m : 2.48393
[1mStep[0m  [60/106], [94mLoss[0m : 2.82670
[1mStep[0m  [70/106], [94mLoss[0m : 2.59657
[1mStep[0m  [80/106], [94mLoss[0m : 2.40050
[1mStep[0m  [90/106], [94mLoss[0m : 2.49811
[1mStep[0m  [100/106], [94mLoss[0m : 2.35843

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.404, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.59288
[1mStep[0m  [10/106], [94mLoss[0m : 2.02100
[1mStep[0m  [20/106], [94mLoss[0m : 2.09549
[1mStep[0m  [30/106], [94mLoss[0m : 2.75948
[1mStep[0m  [40/106], [94mLoss[0m : 2.54887
[1mStep[0m  [50/106], [94mLoss[0m : 2.52096
[1mStep[0m  [60/106], [94mLoss[0m : 2.83514
[1mStep[0m  [70/106], [94mLoss[0m : 2.46959
[1mStep[0m  [80/106], [94mLoss[0m : 2.70493
[1mStep[0m  [90/106], [94mLoss[0m : 2.52678
[1mStep[0m  [100/106], [94mLoss[0m : 2.72016

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.413, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.40886
[1mStep[0m  [10/106], [94mLoss[0m : 2.52913
[1mStep[0m  [20/106], [94mLoss[0m : 2.55629
[1mStep[0m  [30/106], [94mLoss[0m : 2.12760
[1mStep[0m  [40/106], [94mLoss[0m : 2.33563
[1mStep[0m  [50/106], [94mLoss[0m : 2.51386
[1mStep[0m  [60/106], [94mLoss[0m : 2.72338
[1mStep[0m  [70/106], [94mLoss[0m : 2.35592
[1mStep[0m  [80/106], [94mLoss[0m : 2.33466
[1mStep[0m  [90/106], [94mLoss[0m : 2.37141
[1mStep[0m  [100/106], [94mLoss[0m : 2.47519

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.415, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.51112
[1mStep[0m  [10/106], [94mLoss[0m : 2.27160
[1mStep[0m  [20/106], [94mLoss[0m : 2.19822
[1mStep[0m  [30/106], [94mLoss[0m : 2.31228
[1mStep[0m  [40/106], [94mLoss[0m : 2.71407
[1mStep[0m  [50/106], [94mLoss[0m : 2.50268
[1mStep[0m  [60/106], [94mLoss[0m : 2.53790
[1mStep[0m  [70/106], [94mLoss[0m : 2.48197
[1mStep[0m  [80/106], [94mLoss[0m : 2.30252
[1mStep[0m  [90/106], [94mLoss[0m : 2.33690
[1mStep[0m  [100/106], [94mLoss[0m : 2.64139

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.395, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.29383
[1mStep[0m  [10/106], [94mLoss[0m : 2.57538
[1mStep[0m  [20/106], [94mLoss[0m : 2.28885
[1mStep[0m  [30/106], [94mLoss[0m : 2.56916
[1mStep[0m  [40/106], [94mLoss[0m : 2.37738
[1mStep[0m  [50/106], [94mLoss[0m : 2.57706
[1mStep[0m  [60/106], [94mLoss[0m : 2.05324
[1mStep[0m  [70/106], [94mLoss[0m : 2.39259
[1mStep[0m  [80/106], [94mLoss[0m : 3.07250
[1mStep[0m  [90/106], [94mLoss[0m : 2.81322
[1mStep[0m  [100/106], [94mLoss[0m : 2.37318

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.407, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.48245
[1mStep[0m  [10/106], [94mLoss[0m : 2.22504
[1mStep[0m  [20/106], [94mLoss[0m : 2.50692
[1mStep[0m  [30/106], [94mLoss[0m : 2.60802
[1mStep[0m  [40/106], [94mLoss[0m : 2.48348
[1mStep[0m  [50/106], [94mLoss[0m : 2.29548
[1mStep[0m  [60/106], [94mLoss[0m : 2.15097
[1mStep[0m  [70/106], [94mLoss[0m : 2.61242
[1mStep[0m  [80/106], [94mLoss[0m : 2.45587
[1mStep[0m  [90/106], [94mLoss[0m : 2.26115
[1mStep[0m  [100/106], [94mLoss[0m : 2.64050

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.393, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.66336
[1mStep[0m  [10/106], [94mLoss[0m : 2.50644
[1mStep[0m  [20/106], [94mLoss[0m : 2.42550
[1mStep[0m  [30/106], [94mLoss[0m : 2.26273
[1mStep[0m  [40/106], [94mLoss[0m : 2.63138
[1mStep[0m  [50/106], [94mLoss[0m : 2.39534
[1mStep[0m  [60/106], [94mLoss[0m : 2.24718
[1mStep[0m  [70/106], [94mLoss[0m : 2.14702
[1mStep[0m  [80/106], [94mLoss[0m : 2.35409
[1mStep[0m  [90/106], [94mLoss[0m : 2.68253
[1mStep[0m  [100/106], [94mLoss[0m : 2.68493

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.386, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.38316
[1mStep[0m  [10/106], [94mLoss[0m : 2.51993
[1mStep[0m  [20/106], [94mLoss[0m : 2.64788
[1mStep[0m  [30/106], [94mLoss[0m : 2.79361
[1mStep[0m  [40/106], [94mLoss[0m : 2.57837
[1mStep[0m  [50/106], [94mLoss[0m : 2.58049
[1mStep[0m  [60/106], [94mLoss[0m : 2.60905
[1mStep[0m  [70/106], [94mLoss[0m : 2.38029
[1mStep[0m  [80/106], [94mLoss[0m : 2.39542
[1mStep[0m  [90/106], [94mLoss[0m : 2.55698
[1mStep[0m  [100/106], [94mLoss[0m : 2.51230

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.398, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.29696
[1mStep[0m  [10/106], [94mLoss[0m : 2.48928
[1mStep[0m  [20/106], [94mLoss[0m : 2.49921
[1mStep[0m  [30/106], [94mLoss[0m : 2.56288
[1mStep[0m  [40/106], [94mLoss[0m : 2.41021
[1mStep[0m  [50/106], [94mLoss[0m : 2.52177
[1mStep[0m  [60/106], [94mLoss[0m : 2.29456
[1mStep[0m  [70/106], [94mLoss[0m : 2.36995
[1mStep[0m  [80/106], [94mLoss[0m : 2.61242
[1mStep[0m  [90/106], [94mLoss[0m : 2.58910
[1mStep[0m  [100/106], [94mLoss[0m : 2.32361

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.408, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.42888
[1mStep[0m  [10/106], [94mLoss[0m : 2.12117
[1mStep[0m  [20/106], [94mLoss[0m : 2.52172
[1mStep[0m  [30/106], [94mLoss[0m : 2.56896
[1mStep[0m  [40/106], [94mLoss[0m : 2.30631
[1mStep[0m  [50/106], [94mLoss[0m : 2.14450
[1mStep[0m  [60/106], [94mLoss[0m : 2.74478
[1mStep[0m  [70/106], [94mLoss[0m : 2.46931
[1mStep[0m  [80/106], [94mLoss[0m : 2.66228
[1mStep[0m  [90/106], [94mLoss[0m : 2.49098
[1mStep[0m  [100/106], [94mLoss[0m : 2.31262

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.390, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.25531
[1mStep[0m  [10/106], [94mLoss[0m : 2.61043
[1mStep[0m  [20/106], [94mLoss[0m : 2.56623
[1mStep[0m  [30/106], [94mLoss[0m : 2.12506
[1mStep[0m  [40/106], [94mLoss[0m : 2.76198
[1mStep[0m  [50/106], [94mLoss[0m : 2.23603
[1mStep[0m  [60/106], [94mLoss[0m : 2.31660
[1mStep[0m  [70/106], [94mLoss[0m : 2.64235
[1mStep[0m  [80/106], [94mLoss[0m : 2.16805
[1mStep[0m  [90/106], [94mLoss[0m : 2.88906
[1mStep[0m  [100/106], [94mLoss[0m : 2.27560

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.386, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.65770
[1mStep[0m  [10/106], [94mLoss[0m : 2.38919
[1mStep[0m  [20/106], [94mLoss[0m : 2.58841
[1mStep[0m  [30/106], [94mLoss[0m : 2.14728
[1mStep[0m  [40/106], [94mLoss[0m : 2.61903
[1mStep[0m  [50/106], [94mLoss[0m : 2.30715
[1mStep[0m  [60/106], [94mLoss[0m : 2.51817
[1mStep[0m  [70/106], [94mLoss[0m : 2.32541
[1mStep[0m  [80/106], [94mLoss[0m : 2.73952
[1mStep[0m  [90/106], [94mLoss[0m : 2.24604
[1mStep[0m  [100/106], [94mLoss[0m : 2.26914

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.385, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.45130
[1mStep[0m  [10/106], [94mLoss[0m : 2.46055
[1mStep[0m  [20/106], [94mLoss[0m : 2.55433
[1mStep[0m  [30/106], [94mLoss[0m : 2.22669
[1mStep[0m  [40/106], [94mLoss[0m : 2.47295
[1mStep[0m  [50/106], [94mLoss[0m : 2.40583
[1mStep[0m  [60/106], [94mLoss[0m : 2.33264
[1mStep[0m  [70/106], [94mLoss[0m : 2.57061
[1mStep[0m  [80/106], [94mLoss[0m : 2.35565
[1mStep[0m  [90/106], [94mLoss[0m : 2.23046
[1mStep[0m  [100/106], [94mLoss[0m : 2.53930

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.421, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.62389
[1mStep[0m  [10/106], [94mLoss[0m : 2.36886
[1mStep[0m  [20/106], [94mLoss[0m : 2.40964
[1mStep[0m  [30/106], [94mLoss[0m : 2.17402
[1mStep[0m  [40/106], [94mLoss[0m : 2.51990
[1mStep[0m  [50/106], [94mLoss[0m : 2.45520
[1mStep[0m  [60/106], [94mLoss[0m : 2.56828
[1mStep[0m  [70/106], [94mLoss[0m : 2.27708
[1mStep[0m  [80/106], [94mLoss[0m : 2.59914
[1mStep[0m  [90/106], [94mLoss[0m : 2.09965
[1mStep[0m  [100/106], [94mLoss[0m : 2.43597

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.401, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.28902
[1mStep[0m  [10/106], [94mLoss[0m : 2.48307
[1mStep[0m  [20/106], [94mLoss[0m : 2.27394
[1mStep[0m  [30/106], [94mLoss[0m : 2.23516
[1mStep[0m  [40/106], [94mLoss[0m : 2.42956
[1mStep[0m  [50/106], [94mLoss[0m : 2.21389
[1mStep[0m  [60/106], [94mLoss[0m : 2.33893
[1mStep[0m  [70/106], [94mLoss[0m : 2.40718
[1mStep[0m  [80/106], [94mLoss[0m : 2.22099
[1mStep[0m  [90/106], [94mLoss[0m : 2.28041
[1mStep[0m  [100/106], [94mLoss[0m : 2.70335

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.411, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.42426
[1mStep[0m  [10/106], [94mLoss[0m : 2.51121
[1mStep[0m  [20/106], [94mLoss[0m : 2.64339
[1mStep[0m  [30/106], [94mLoss[0m : 2.41707
[1mStep[0m  [40/106], [94mLoss[0m : 2.07473
[1mStep[0m  [50/106], [94mLoss[0m : 2.68084
[1mStep[0m  [60/106], [94mLoss[0m : 2.21063
[1mStep[0m  [70/106], [94mLoss[0m : 2.18647
[1mStep[0m  [80/106], [94mLoss[0m : 2.25724
[1mStep[0m  [90/106], [94mLoss[0m : 2.40576
[1mStep[0m  [100/106], [94mLoss[0m : 2.74022

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.390, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.07790
[1mStep[0m  [10/106], [94mLoss[0m : 2.59599
[1mStep[0m  [20/106], [94mLoss[0m : 2.65829
[1mStep[0m  [30/106], [94mLoss[0m : 2.31789
[1mStep[0m  [40/106], [94mLoss[0m : 2.35796
[1mStep[0m  [50/106], [94mLoss[0m : 2.55658
[1mStep[0m  [60/106], [94mLoss[0m : 2.58432
[1mStep[0m  [70/106], [94mLoss[0m : 2.56656
[1mStep[0m  [80/106], [94mLoss[0m : 2.58420
[1mStep[0m  [90/106], [94mLoss[0m : 2.09231
[1mStep[0m  [100/106], [94mLoss[0m : 2.37515

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.383, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.35286
[1mStep[0m  [10/106], [94mLoss[0m : 2.79127
[1mStep[0m  [20/106], [94mLoss[0m : 2.12617
[1mStep[0m  [30/106], [94mLoss[0m : 2.60121
[1mStep[0m  [40/106], [94mLoss[0m : 1.99833
[1mStep[0m  [50/106], [94mLoss[0m : 2.43973
[1mStep[0m  [60/106], [94mLoss[0m : 2.31080
[1mStep[0m  [70/106], [94mLoss[0m : 2.35023
[1mStep[0m  [80/106], [94mLoss[0m : 2.32117
[1mStep[0m  [90/106], [94mLoss[0m : 2.46986
[1mStep[0m  [100/106], [94mLoss[0m : 2.73959

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.459, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.28808
[1mStep[0m  [10/106], [94mLoss[0m : 2.41137
[1mStep[0m  [20/106], [94mLoss[0m : 2.66775
[1mStep[0m  [30/106], [94mLoss[0m : 2.19800
[1mStep[0m  [40/106], [94mLoss[0m : 2.77901
[1mStep[0m  [50/106], [94mLoss[0m : 2.21634
[1mStep[0m  [60/106], [94mLoss[0m : 2.44924
[1mStep[0m  [70/106], [94mLoss[0m : 2.51762
[1mStep[0m  [80/106], [94mLoss[0m : 2.24913
[1mStep[0m  [90/106], [94mLoss[0m : 2.47860
[1mStep[0m  [100/106], [94mLoss[0m : 2.65758

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.391, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.16119
[1mStep[0m  [10/106], [94mLoss[0m : 2.34535
[1mStep[0m  [20/106], [94mLoss[0m : 2.43387
[1mStep[0m  [30/106], [94mLoss[0m : 2.60158
[1mStep[0m  [40/106], [94mLoss[0m : 2.33185
[1mStep[0m  [50/106], [94mLoss[0m : 2.21526
[1mStep[0m  [60/106], [94mLoss[0m : 2.65872
[1mStep[0m  [70/106], [94mLoss[0m : 2.20445
[1mStep[0m  [80/106], [94mLoss[0m : 2.57128
[1mStep[0m  [90/106], [94mLoss[0m : 2.67318
[1mStep[0m  [100/106], [94mLoss[0m : 2.59291

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.413, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.33529
[1mStep[0m  [10/106], [94mLoss[0m : 2.70908
[1mStep[0m  [20/106], [94mLoss[0m : 2.44917
[1mStep[0m  [30/106], [94mLoss[0m : 2.23400
[1mStep[0m  [40/106], [94mLoss[0m : 2.23861
[1mStep[0m  [50/106], [94mLoss[0m : 2.68639
[1mStep[0m  [60/106], [94mLoss[0m : 2.23816
[1mStep[0m  [70/106], [94mLoss[0m : 2.56895
[1mStep[0m  [80/106], [94mLoss[0m : 2.50317
[1mStep[0m  [90/106], [94mLoss[0m : 2.45639
[1mStep[0m  [100/106], [94mLoss[0m : 2.12946

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.405, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.393
====================================

Phase 1 - Evaluation MAE:  2.392791509628296
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/106], [94mLoss[0m : 2.85186
[1mStep[0m  [10/106], [94mLoss[0m : 2.45553
[1mStep[0m  [20/106], [94mLoss[0m : 2.32497
[1mStep[0m  [30/106], [94mLoss[0m : 2.82223
[1mStep[0m  [40/106], [94mLoss[0m : 2.60754
[1mStep[0m  [50/106], [94mLoss[0m : 2.44420
[1mStep[0m  [60/106], [94mLoss[0m : 2.67660
[1mStep[0m  [70/106], [94mLoss[0m : 2.55469
[1mStep[0m  [80/106], [94mLoss[0m : 2.13610
[1mStep[0m  [90/106], [94mLoss[0m : 2.31775
[1mStep[0m  [100/106], [94mLoss[0m : 2.47425

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.387, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.34644
[1mStep[0m  [10/106], [94mLoss[0m : 2.42963
[1mStep[0m  [20/106], [94mLoss[0m : 2.35939
[1mStep[0m  [30/106], [94mLoss[0m : 2.44589
[1mStep[0m  [40/106], [94mLoss[0m : 2.35300
[1mStep[0m  [50/106], [94mLoss[0m : 2.47725
[1mStep[0m  [60/106], [94mLoss[0m : 2.30496
[1mStep[0m  [70/106], [94mLoss[0m : 2.42194
[1mStep[0m  [80/106], [94mLoss[0m : 2.57328
[1mStep[0m  [90/106], [94mLoss[0m : 2.37043
[1mStep[0m  [100/106], [94mLoss[0m : 2.34922

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.340, [92mTest[0m: 2.463, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.02153
[1mStep[0m  [10/106], [94mLoss[0m : 2.34093
[1mStep[0m  [20/106], [94mLoss[0m : 2.36069
[1mStep[0m  [30/106], [94mLoss[0m : 2.07922
[1mStep[0m  [40/106], [94mLoss[0m : 2.27589
[1mStep[0m  [50/106], [94mLoss[0m : 1.97539
[1mStep[0m  [60/106], [94mLoss[0m : 2.10977
[1mStep[0m  [70/106], [94mLoss[0m : 2.41929
[1mStep[0m  [80/106], [94mLoss[0m : 2.20968
[1mStep[0m  [90/106], [94mLoss[0m : 2.82449
[1mStep[0m  [100/106], [94mLoss[0m : 2.28339

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.239, [92mTest[0m: 2.387, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.17380
[1mStep[0m  [10/106], [94mLoss[0m : 1.92536
[1mStep[0m  [20/106], [94mLoss[0m : 2.05210
[1mStep[0m  [30/106], [94mLoss[0m : 2.19698
[1mStep[0m  [40/106], [94mLoss[0m : 2.28806
[1mStep[0m  [50/106], [94mLoss[0m : 2.26973
[1mStep[0m  [60/106], [94mLoss[0m : 2.04064
[1mStep[0m  [70/106], [94mLoss[0m : 2.52429
[1mStep[0m  [80/106], [94mLoss[0m : 2.37305
[1mStep[0m  [90/106], [94mLoss[0m : 2.84164
[1mStep[0m  [100/106], [94mLoss[0m : 2.15285

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.191, [92mTest[0m: 2.377, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.86213
[1mStep[0m  [10/106], [94mLoss[0m : 2.09679
[1mStep[0m  [20/106], [94mLoss[0m : 2.36322
[1mStep[0m  [30/106], [94mLoss[0m : 2.21371
[1mStep[0m  [40/106], [94mLoss[0m : 2.17444
[1mStep[0m  [50/106], [94mLoss[0m : 2.04818
[1mStep[0m  [60/106], [94mLoss[0m : 2.19901
[1mStep[0m  [70/106], [94mLoss[0m : 1.88960
[1mStep[0m  [80/106], [94mLoss[0m : 2.09846
[1mStep[0m  [90/106], [94mLoss[0m : 2.27378
[1mStep[0m  [100/106], [94mLoss[0m : 2.22767

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.126, [92mTest[0m: 2.388, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.16131
[1mStep[0m  [10/106], [94mLoss[0m : 2.04844
[1mStep[0m  [20/106], [94mLoss[0m : 2.01031
[1mStep[0m  [30/106], [94mLoss[0m : 1.91464
[1mStep[0m  [40/106], [94mLoss[0m : 2.02551
[1mStep[0m  [50/106], [94mLoss[0m : 1.95600
[1mStep[0m  [60/106], [94mLoss[0m : 2.16735
[1mStep[0m  [70/106], [94mLoss[0m : 2.00501
[1mStep[0m  [80/106], [94mLoss[0m : 2.23708
[1mStep[0m  [90/106], [94mLoss[0m : 2.00840
[1mStep[0m  [100/106], [94mLoss[0m : 2.24941

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.049, [92mTest[0m: 2.432, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.94691
[1mStep[0m  [10/106], [94mLoss[0m : 1.93023
[1mStep[0m  [20/106], [94mLoss[0m : 2.10870
[1mStep[0m  [30/106], [94mLoss[0m : 2.16572
[1mStep[0m  [40/106], [94mLoss[0m : 1.89538
[1mStep[0m  [50/106], [94mLoss[0m : 2.02829
[1mStep[0m  [60/106], [94mLoss[0m : 2.02231
[1mStep[0m  [70/106], [94mLoss[0m : 2.24485
[1mStep[0m  [80/106], [94mLoss[0m : 1.78758
[1mStep[0m  [90/106], [94mLoss[0m : 2.22834
[1mStep[0m  [100/106], [94mLoss[0m : 2.15140

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.964, [92mTest[0m: 2.472, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.84381
[1mStep[0m  [10/106], [94mLoss[0m : 1.96720
[1mStep[0m  [20/106], [94mLoss[0m : 2.03144
[1mStep[0m  [30/106], [94mLoss[0m : 1.66957
[1mStep[0m  [40/106], [94mLoss[0m : 1.72722
[1mStep[0m  [50/106], [94mLoss[0m : 1.78532
[1mStep[0m  [60/106], [94mLoss[0m : 2.14583
[1mStep[0m  [70/106], [94mLoss[0m : 1.73168
[1mStep[0m  [80/106], [94mLoss[0m : 2.12441
[1mStep[0m  [90/106], [94mLoss[0m : 2.03059
[1mStep[0m  [100/106], [94mLoss[0m : 2.10700

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.936, [92mTest[0m: 2.439, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.13771
[1mStep[0m  [10/106], [94mLoss[0m : 1.73449
[1mStep[0m  [20/106], [94mLoss[0m : 2.00663
[1mStep[0m  [30/106], [94mLoss[0m : 1.94383
[1mStep[0m  [40/106], [94mLoss[0m : 1.69327
[1mStep[0m  [50/106], [94mLoss[0m : 1.90390
[1mStep[0m  [60/106], [94mLoss[0m : 1.75712
[1mStep[0m  [70/106], [94mLoss[0m : 1.80076
[1mStep[0m  [80/106], [94mLoss[0m : 2.11430
[1mStep[0m  [90/106], [94mLoss[0m : 2.05566
[1mStep[0m  [100/106], [94mLoss[0m : 1.74345

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.888, [92mTest[0m: 2.430, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.74635
[1mStep[0m  [10/106], [94mLoss[0m : 1.84133
[1mStep[0m  [20/106], [94mLoss[0m : 1.67963
[1mStep[0m  [30/106], [94mLoss[0m : 1.82591
[1mStep[0m  [40/106], [94mLoss[0m : 1.92625
[1mStep[0m  [50/106], [94mLoss[0m : 1.84580
[1mStep[0m  [60/106], [94mLoss[0m : 1.81067
[1mStep[0m  [70/106], [94mLoss[0m : 1.85658
[1mStep[0m  [80/106], [94mLoss[0m : 1.82113
[1mStep[0m  [90/106], [94mLoss[0m : 1.78410
[1mStep[0m  [100/106], [94mLoss[0m : 1.96168

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.810, [92mTest[0m: 2.486, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.55486
[1mStep[0m  [10/106], [94mLoss[0m : 1.77811
[1mStep[0m  [20/106], [94mLoss[0m : 1.96714
[1mStep[0m  [30/106], [94mLoss[0m : 1.59230
[1mStep[0m  [40/106], [94mLoss[0m : 1.84215
[1mStep[0m  [50/106], [94mLoss[0m : 1.94527
[1mStep[0m  [60/106], [94mLoss[0m : 2.03465
[1mStep[0m  [70/106], [94mLoss[0m : 1.79479
[1mStep[0m  [80/106], [94mLoss[0m : 1.79648
[1mStep[0m  [90/106], [94mLoss[0m : 1.97878
[1mStep[0m  [100/106], [94mLoss[0m : 1.81526

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.794, [92mTest[0m: 2.453, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.65046
[1mStep[0m  [10/106], [94mLoss[0m : 1.67696
[1mStep[0m  [20/106], [94mLoss[0m : 1.69484
[1mStep[0m  [30/106], [94mLoss[0m : 1.52466
[1mStep[0m  [40/106], [94mLoss[0m : 1.73077
[1mStep[0m  [50/106], [94mLoss[0m : 1.74096
[1mStep[0m  [60/106], [94mLoss[0m : 1.57876
[1mStep[0m  [70/106], [94mLoss[0m : 1.55895
[1mStep[0m  [80/106], [94mLoss[0m : 1.87282
[1mStep[0m  [90/106], [94mLoss[0m : 1.78313
[1mStep[0m  [100/106], [94mLoss[0m : 1.90212

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.744, [92mTest[0m: 2.526, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.73113
[1mStep[0m  [10/106], [94mLoss[0m : 1.73855
[1mStep[0m  [20/106], [94mLoss[0m : 1.52570
[1mStep[0m  [30/106], [94mLoss[0m : 1.55701
[1mStep[0m  [40/106], [94mLoss[0m : 1.72882
[1mStep[0m  [50/106], [94mLoss[0m : 1.62799
[1mStep[0m  [60/106], [94mLoss[0m : 1.67006
[1mStep[0m  [70/106], [94mLoss[0m : 1.80325
[1mStep[0m  [80/106], [94mLoss[0m : 1.67506
[1mStep[0m  [90/106], [94mLoss[0m : 1.62200
[1mStep[0m  [100/106], [94mLoss[0m : 1.47970

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.715, [92mTest[0m: 2.581, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.48505
[1mStep[0m  [10/106], [94mLoss[0m : 1.56297
[1mStep[0m  [20/106], [94mLoss[0m : 1.69732
[1mStep[0m  [30/106], [94mLoss[0m : 1.56000
[1mStep[0m  [40/106], [94mLoss[0m : 1.54241
[1mStep[0m  [50/106], [94mLoss[0m : 1.51988
[1mStep[0m  [60/106], [94mLoss[0m : 1.62415
[1mStep[0m  [70/106], [94mLoss[0m : 1.63777
[1mStep[0m  [80/106], [94mLoss[0m : 2.12853
[1mStep[0m  [90/106], [94mLoss[0m : 1.50185
[1mStep[0m  [100/106], [94mLoss[0m : 1.40402

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.652, [92mTest[0m: 2.513, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.76496
[1mStep[0m  [10/106], [94mLoss[0m : 1.54560
[1mStep[0m  [20/106], [94mLoss[0m : 1.38089
[1mStep[0m  [30/106], [94mLoss[0m : 1.50161
[1mStep[0m  [40/106], [94mLoss[0m : 1.59642
[1mStep[0m  [50/106], [94mLoss[0m : 1.71037
[1mStep[0m  [60/106], [94mLoss[0m : 1.96852
[1mStep[0m  [70/106], [94mLoss[0m : 1.41171
[1mStep[0m  [80/106], [94mLoss[0m : 1.74636
[1mStep[0m  [90/106], [94mLoss[0m : 1.62781
[1mStep[0m  [100/106], [94mLoss[0m : 1.57850

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.619, [92mTest[0m: 2.502, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.22132
[1mStep[0m  [10/106], [94mLoss[0m : 1.59456
[1mStep[0m  [20/106], [94mLoss[0m : 1.60425
[1mStep[0m  [30/106], [94mLoss[0m : 1.51888
[1mStep[0m  [40/106], [94mLoss[0m : 1.61731
[1mStep[0m  [50/106], [94mLoss[0m : 1.74050
[1mStep[0m  [60/106], [94mLoss[0m : 1.36528
[1mStep[0m  [70/106], [94mLoss[0m : 1.53627
[1mStep[0m  [80/106], [94mLoss[0m : 1.68417
[1mStep[0m  [90/106], [94mLoss[0m : 1.77135
[1mStep[0m  [100/106], [94mLoss[0m : 1.45692

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.593, [92mTest[0m: 2.468, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.44836
[1mStep[0m  [10/106], [94mLoss[0m : 1.66748
[1mStep[0m  [20/106], [94mLoss[0m : 1.62385
[1mStep[0m  [30/106], [94mLoss[0m : 1.49671
[1mStep[0m  [40/106], [94mLoss[0m : 1.62772
[1mStep[0m  [50/106], [94mLoss[0m : 1.44665
[1mStep[0m  [60/106], [94mLoss[0m : 1.58532
[1mStep[0m  [70/106], [94mLoss[0m : 1.47773
[1mStep[0m  [80/106], [94mLoss[0m : 1.12454
[1mStep[0m  [90/106], [94mLoss[0m : 1.74233
[1mStep[0m  [100/106], [94mLoss[0m : 1.51047

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.549, [92mTest[0m: 2.509, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.30370
[1mStep[0m  [10/106], [94mLoss[0m : 1.32590
[1mStep[0m  [20/106], [94mLoss[0m : 1.26118
[1mStep[0m  [30/106], [94mLoss[0m : 1.28453
[1mStep[0m  [40/106], [94mLoss[0m : 1.62371
[1mStep[0m  [50/106], [94mLoss[0m : 1.55000
[1mStep[0m  [60/106], [94mLoss[0m : 1.29563
[1mStep[0m  [70/106], [94mLoss[0m : 1.47966
[1mStep[0m  [80/106], [94mLoss[0m : 1.68796
[1mStep[0m  [90/106], [94mLoss[0m : 1.46054
[1mStep[0m  [100/106], [94mLoss[0m : 1.45846

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.522, [92mTest[0m: 2.547, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.54111
[1mStep[0m  [10/106], [94mLoss[0m : 1.37154
[1mStep[0m  [20/106], [94mLoss[0m : 1.27997
[1mStep[0m  [30/106], [94mLoss[0m : 1.66888
[1mStep[0m  [40/106], [94mLoss[0m : 1.35394
[1mStep[0m  [50/106], [94mLoss[0m : 1.42747
[1mStep[0m  [60/106], [94mLoss[0m : 1.47277
[1mStep[0m  [70/106], [94mLoss[0m : 1.39242
[1mStep[0m  [80/106], [94mLoss[0m : 1.83529
[1mStep[0m  [90/106], [94mLoss[0m : 1.51682
[1mStep[0m  [100/106], [94mLoss[0m : 1.51482

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.511, [92mTest[0m: 2.536, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.48495
[1mStep[0m  [10/106], [94mLoss[0m : 1.43418
[1mStep[0m  [20/106], [94mLoss[0m : 1.29431
[1mStep[0m  [30/106], [94mLoss[0m : 1.19156
[1mStep[0m  [40/106], [94mLoss[0m : 1.47949
[1mStep[0m  [50/106], [94mLoss[0m : 1.48152
[1mStep[0m  [60/106], [94mLoss[0m : 1.43253
[1mStep[0m  [70/106], [94mLoss[0m : 1.36719
[1mStep[0m  [80/106], [94mLoss[0m : 1.37384
[1mStep[0m  [90/106], [94mLoss[0m : 1.37163
[1mStep[0m  [100/106], [94mLoss[0m : 1.31657

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.467, [92mTest[0m: 2.455, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.37905
[1mStep[0m  [10/106], [94mLoss[0m : 1.27690
[1mStep[0m  [20/106], [94mLoss[0m : 1.29001
[1mStep[0m  [30/106], [94mLoss[0m : 1.55980
[1mStep[0m  [40/106], [94mLoss[0m : 1.58548
[1mStep[0m  [50/106], [94mLoss[0m : 1.35154
[1mStep[0m  [60/106], [94mLoss[0m : 1.40096
[1mStep[0m  [70/106], [94mLoss[0m : 1.31907
[1mStep[0m  [80/106], [94mLoss[0m : 1.31052
[1mStep[0m  [90/106], [94mLoss[0m : 1.38205
[1mStep[0m  [100/106], [94mLoss[0m : 1.42343

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.421, [92mTest[0m: 2.528, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.31917
[1mStep[0m  [10/106], [94mLoss[0m : 1.38040
[1mStep[0m  [20/106], [94mLoss[0m : 1.50974
[1mStep[0m  [30/106], [94mLoss[0m : 1.33627
[1mStep[0m  [40/106], [94mLoss[0m : 1.35099
[1mStep[0m  [50/106], [94mLoss[0m : 1.51659
[1mStep[0m  [60/106], [94mLoss[0m : 1.45463
[1mStep[0m  [70/106], [94mLoss[0m : 1.27407
[1mStep[0m  [80/106], [94mLoss[0m : 1.51880
[1mStep[0m  [90/106], [94mLoss[0m : 1.43257
[1mStep[0m  [100/106], [94mLoss[0m : 1.63648

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.390, [92mTest[0m: 2.489, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.08993
[1mStep[0m  [10/106], [94mLoss[0m : 1.29463
[1mStep[0m  [20/106], [94mLoss[0m : 1.43017
[1mStep[0m  [30/106], [94mLoss[0m : 1.33628
[1mStep[0m  [40/106], [94mLoss[0m : 1.37687
[1mStep[0m  [50/106], [94mLoss[0m : 1.23085
[1mStep[0m  [60/106], [94mLoss[0m : 1.42681
[1mStep[0m  [70/106], [94mLoss[0m : 1.39452
[1mStep[0m  [80/106], [94mLoss[0m : 1.46185
[1mStep[0m  [90/106], [94mLoss[0m : 1.67457
[1mStep[0m  [100/106], [94mLoss[0m : 1.58061

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.366, [92mTest[0m: 2.505, [96mlr[0m: 0.004545
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 22 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.519
====================================

Phase 2 - Evaluation MAE:  2.519436058008446
MAE score P1      2.392792
MAE score P2      2.519436
loss              1.366035
learning_rate      0.00505
batch_size             128
hidden_sizes         [300]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping        True
dropout                0.4
momentum               0.9
weight_decay        0.0001
Name: 0, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/106], [94mLoss[0m : 11.06598
[1mStep[0m  [10/106], [94mLoss[0m : 3.85262
[1mStep[0m  [20/106], [94mLoss[0m : 2.57432
[1mStep[0m  [30/106], [94mLoss[0m : 2.53074
[1mStep[0m  [40/106], [94mLoss[0m : 2.57700
[1mStep[0m  [50/106], [94mLoss[0m : 2.51413
[1mStep[0m  [60/106], [94mLoss[0m : 2.50502
[1mStep[0m  [70/106], [94mLoss[0m : 2.58080
[1mStep[0m  [80/106], [94mLoss[0m : 2.40382
[1mStep[0m  [90/106], [94mLoss[0m : 2.79972
[1mStep[0m  [100/106], [94mLoss[0m : 2.20737

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.200, [92mTest[0m: 11.287, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.43235
[1mStep[0m  [10/106], [94mLoss[0m : 2.68553
[1mStep[0m  [20/106], [94mLoss[0m : 2.18993
[1mStep[0m  [30/106], [94mLoss[0m : 2.42317
[1mStep[0m  [40/106], [94mLoss[0m : 2.80738
[1mStep[0m  [50/106], [94mLoss[0m : 2.59044
[1mStep[0m  [60/106], [94mLoss[0m : 2.31674
[1mStep[0m  [70/106], [94mLoss[0m : 2.57869
[1mStep[0m  [80/106], [94mLoss[0m : 2.45793
[1mStep[0m  [90/106], [94mLoss[0m : 2.28268
[1mStep[0m  [100/106], [94mLoss[0m : 2.33824

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.491, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.25588
[1mStep[0m  [10/106], [94mLoss[0m : 2.23960
[1mStep[0m  [20/106], [94mLoss[0m : 2.99949
[1mStep[0m  [30/106], [94mLoss[0m : 2.21367
[1mStep[0m  [40/106], [94mLoss[0m : 2.47920
[1mStep[0m  [50/106], [94mLoss[0m : 2.12077
[1mStep[0m  [60/106], [94mLoss[0m : 2.50383
[1mStep[0m  [70/106], [94mLoss[0m : 2.38856
[1mStep[0m  [80/106], [94mLoss[0m : 2.41227
[1mStep[0m  [90/106], [94mLoss[0m : 2.43535
[1mStep[0m  [100/106], [94mLoss[0m : 2.64455

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.467, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.34431
[1mStep[0m  [10/106], [94mLoss[0m : 2.47843
[1mStep[0m  [20/106], [94mLoss[0m : 2.35187
[1mStep[0m  [30/106], [94mLoss[0m : 2.73640
[1mStep[0m  [40/106], [94mLoss[0m : 2.45717
[1mStep[0m  [50/106], [94mLoss[0m : 2.42876
[1mStep[0m  [60/106], [94mLoss[0m : 2.48941
[1mStep[0m  [70/106], [94mLoss[0m : 2.59116
[1mStep[0m  [80/106], [94mLoss[0m : 2.27510
[1mStep[0m  [90/106], [94mLoss[0m : 2.38459
[1mStep[0m  [100/106], [94mLoss[0m : 2.54223

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.458, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.33746
[1mStep[0m  [10/106], [94mLoss[0m : 2.53219
[1mStep[0m  [20/106], [94mLoss[0m : 2.67505
[1mStep[0m  [30/106], [94mLoss[0m : 2.14788
[1mStep[0m  [40/106], [94mLoss[0m : 2.67611
[1mStep[0m  [50/106], [94mLoss[0m : 2.49193
[1mStep[0m  [60/106], [94mLoss[0m : 2.26170
[1mStep[0m  [70/106], [94mLoss[0m : 2.27057
[1mStep[0m  [80/106], [94mLoss[0m : 2.38391
[1mStep[0m  [90/106], [94mLoss[0m : 2.44450
[1mStep[0m  [100/106], [94mLoss[0m : 2.51459

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.434, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.42331
[1mStep[0m  [10/106], [94mLoss[0m : 2.34846
[1mStep[0m  [20/106], [94mLoss[0m : 2.48901
[1mStep[0m  [30/106], [94mLoss[0m : 2.64542
[1mStep[0m  [40/106], [94mLoss[0m : 2.69486
[1mStep[0m  [50/106], [94mLoss[0m : 2.15374
[1mStep[0m  [60/106], [94mLoss[0m : 2.37839
[1mStep[0m  [70/106], [94mLoss[0m : 2.47407
[1mStep[0m  [80/106], [94mLoss[0m : 2.68856
[1mStep[0m  [90/106], [94mLoss[0m : 2.26366
[1mStep[0m  [100/106], [94mLoss[0m : 2.51404

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.447, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.23695
[1mStep[0m  [10/106], [94mLoss[0m : 2.31024
[1mStep[0m  [20/106], [94mLoss[0m : 2.87778
[1mStep[0m  [30/106], [94mLoss[0m : 2.20828
[1mStep[0m  [40/106], [94mLoss[0m : 2.43283
[1mStep[0m  [50/106], [94mLoss[0m : 2.45548
[1mStep[0m  [60/106], [94mLoss[0m : 2.36494
[1mStep[0m  [70/106], [94mLoss[0m : 2.52615
[1mStep[0m  [80/106], [94mLoss[0m : 2.34550
[1mStep[0m  [90/106], [94mLoss[0m : 2.43239
[1mStep[0m  [100/106], [94mLoss[0m : 2.35531

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.419, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.47411
[1mStep[0m  [10/106], [94mLoss[0m : 2.17650
[1mStep[0m  [20/106], [94mLoss[0m : 2.28510
[1mStep[0m  [30/106], [94mLoss[0m : 2.30649
[1mStep[0m  [40/106], [94mLoss[0m : 2.43450
[1mStep[0m  [50/106], [94mLoss[0m : 2.42695
[1mStep[0m  [60/106], [94mLoss[0m : 2.88592
[1mStep[0m  [70/106], [94mLoss[0m : 2.68785
[1mStep[0m  [80/106], [94mLoss[0m : 2.22465
[1mStep[0m  [90/106], [94mLoss[0m : 2.56058
[1mStep[0m  [100/106], [94mLoss[0m : 2.17723

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.419, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.36885
[1mStep[0m  [10/106], [94mLoss[0m : 2.21291
[1mStep[0m  [20/106], [94mLoss[0m : 2.26401
[1mStep[0m  [30/106], [94mLoss[0m : 2.72191
[1mStep[0m  [40/106], [94mLoss[0m : 2.34902
[1mStep[0m  [50/106], [94mLoss[0m : 2.22640
[1mStep[0m  [60/106], [94mLoss[0m : 2.56580
[1mStep[0m  [70/106], [94mLoss[0m : 2.68320
[1mStep[0m  [80/106], [94mLoss[0m : 2.64260
[1mStep[0m  [90/106], [94mLoss[0m : 2.34340
[1mStep[0m  [100/106], [94mLoss[0m : 2.45627

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.426, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.27229
[1mStep[0m  [10/106], [94mLoss[0m : 2.46323
[1mStep[0m  [20/106], [94mLoss[0m : 2.26116
[1mStep[0m  [30/106], [94mLoss[0m : 2.08905
[1mStep[0m  [40/106], [94mLoss[0m : 2.71644
[1mStep[0m  [50/106], [94mLoss[0m : 2.36928
[1mStep[0m  [60/106], [94mLoss[0m : 2.24052
[1mStep[0m  [70/106], [94mLoss[0m : 2.28822
[1mStep[0m  [80/106], [94mLoss[0m : 2.52781
[1mStep[0m  [90/106], [94mLoss[0m : 2.45440
[1mStep[0m  [100/106], [94mLoss[0m : 2.58382

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.413, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.77089
[1mStep[0m  [10/106], [94mLoss[0m : 2.47737
[1mStep[0m  [20/106], [94mLoss[0m : 2.50256
[1mStep[0m  [30/106], [94mLoss[0m : 2.61558
[1mStep[0m  [40/106], [94mLoss[0m : 2.47628
[1mStep[0m  [50/106], [94mLoss[0m : 2.31943
[1mStep[0m  [60/106], [94mLoss[0m : 2.28455
[1mStep[0m  [70/106], [94mLoss[0m : 2.62769
[1mStep[0m  [80/106], [94mLoss[0m : 2.64209
[1mStep[0m  [90/106], [94mLoss[0m : 2.24125
[1mStep[0m  [100/106], [94mLoss[0m : 2.53861

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.412, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.72290
[1mStep[0m  [10/106], [94mLoss[0m : 2.42869
[1mStep[0m  [20/106], [94mLoss[0m : 2.30259
[1mStep[0m  [30/106], [94mLoss[0m : 2.41683
[1mStep[0m  [40/106], [94mLoss[0m : 2.41463
[1mStep[0m  [50/106], [94mLoss[0m : 2.38524
[1mStep[0m  [60/106], [94mLoss[0m : 2.14345
[1mStep[0m  [70/106], [94mLoss[0m : 2.20106
[1mStep[0m  [80/106], [94mLoss[0m : 2.13822
[1mStep[0m  [90/106], [94mLoss[0m : 2.63914
[1mStep[0m  [100/106], [94mLoss[0m : 2.74810

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.424, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.50208
[1mStep[0m  [10/106], [94mLoss[0m : 2.37441
[1mStep[0m  [20/106], [94mLoss[0m : 2.05278
[1mStep[0m  [30/106], [94mLoss[0m : 2.46919
[1mStep[0m  [40/106], [94mLoss[0m : 2.30179
[1mStep[0m  [50/106], [94mLoss[0m : 2.29758
[1mStep[0m  [60/106], [94mLoss[0m : 2.16464
[1mStep[0m  [70/106], [94mLoss[0m : 2.29359
[1mStep[0m  [80/106], [94mLoss[0m : 2.26827
[1mStep[0m  [90/106], [94mLoss[0m : 2.42063
[1mStep[0m  [100/106], [94mLoss[0m : 2.46251

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.421, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.51753
[1mStep[0m  [10/106], [94mLoss[0m : 2.41372
[1mStep[0m  [20/106], [94mLoss[0m : 2.37800
[1mStep[0m  [30/106], [94mLoss[0m : 2.54947
[1mStep[0m  [40/106], [94mLoss[0m : 2.42276
[1mStep[0m  [50/106], [94mLoss[0m : 2.68252
[1mStep[0m  [60/106], [94mLoss[0m : 2.30267
[1mStep[0m  [70/106], [94mLoss[0m : 2.61751
[1mStep[0m  [80/106], [94mLoss[0m : 2.49126
[1mStep[0m  [90/106], [94mLoss[0m : 2.22136
[1mStep[0m  [100/106], [94mLoss[0m : 2.57682

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.415, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.30581
[1mStep[0m  [10/106], [94mLoss[0m : 2.28246
[1mStep[0m  [20/106], [94mLoss[0m : 2.24125
[1mStep[0m  [30/106], [94mLoss[0m : 2.54749
[1mStep[0m  [40/106], [94mLoss[0m : 2.27729
[1mStep[0m  [50/106], [94mLoss[0m : 2.58833
[1mStep[0m  [60/106], [94mLoss[0m : 2.32787
[1mStep[0m  [70/106], [94mLoss[0m : 2.18182
[1mStep[0m  [80/106], [94mLoss[0m : 2.42340
[1mStep[0m  [90/106], [94mLoss[0m : 2.66147
[1mStep[0m  [100/106], [94mLoss[0m : 2.27157

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.413, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.86652
[1mStep[0m  [10/106], [94mLoss[0m : 2.23879
[1mStep[0m  [20/106], [94mLoss[0m : 2.53818
[1mStep[0m  [30/106], [94mLoss[0m : 2.56471
[1mStep[0m  [40/106], [94mLoss[0m : 2.43858
[1mStep[0m  [50/106], [94mLoss[0m : 2.24749
[1mStep[0m  [60/106], [94mLoss[0m : 2.62979
[1mStep[0m  [70/106], [94mLoss[0m : 2.06431
[1mStep[0m  [80/106], [94mLoss[0m : 2.45903
[1mStep[0m  [90/106], [94mLoss[0m : 2.51889
[1mStep[0m  [100/106], [94mLoss[0m : 2.60586

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.411, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.18112
[1mStep[0m  [10/106], [94mLoss[0m : 2.64506
[1mStep[0m  [20/106], [94mLoss[0m : 2.03574
[1mStep[0m  [30/106], [94mLoss[0m : 2.57261
[1mStep[0m  [40/106], [94mLoss[0m : 2.54628
[1mStep[0m  [50/106], [94mLoss[0m : 2.64250
[1mStep[0m  [60/106], [94mLoss[0m : 2.65509
[1mStep[0m  [70/106], [94mLoss[0m : 2.26070
[1mStep[0m  [80/106], [94mLoss[0m : 2.47024
[1mStep[0m  [90/106], [94mLoss[0m : 2.38238
[1mStep[0m  [100/106], [94mLoss[0m : 2.61674

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.405, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.38625
[1mStep[0m  [10/106], [94mLoss[0m : 2.34024
[1mStep[0m  [20/106], [94mLoss[0m : 2.21976
[1mStep[0m  [30/106], [94mLoss[0m : 2.67121
[1mStep[0m  [40/106], [94mLoss[0m : 2.53797
[1mStep[0m  [50/106], [94mLoss[0m : 2.55369
[1mStep[0m  [60/106], [94mLoss[0m : 2.74170
[1mStep[0m  [70/106], [94mLoss[0m : 2.52617
[1mStep[0m  [80/106], [94mLoss[0m : 2.34029
[1mStep[0m  [90/106], [94mLoss[0m : 2.56355
[1mStep[0m  [100/106], [94mLoss[0m : 2.41382

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.404, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.36078
[1mStep[0m  [10/106], [94mLoss[0m : 2.73142
[1mStep[0m  [20/106], [94mLoss[0m : 2.28764
[1mStep[0m  [30/106], [94mLoss[0m : 2.32711
[1mStep[0m  [40/106], [94mLoss[0m : 2.21314
[1mStep[0m  [50/106], [94mLoss[0m : 2.38669
[1mStep[0m  [60/106], [94mLoss[0m : 2.18991
[1mStep[0m  [70/106], [94mLoss[0m : 2.60557
[1mStep[0m  [80/106], [94mLoss[0m : 2.38066
[1mStep[0m  [90/106], [94mLoss[0m : 2.23627
[1mStep[0m  [100/106], [94mLoss[0m : 2.43699

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.409, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.70787
[1mStep[0m  [10/106], [94mLoss[0m : 2.57418
[1mStep[0m  [20/106], [94mLoss[0m : 2.15290
[1mStep[0m  [30/106], [94mLoss[0m : 2.10497
[1mStep[0m  [40/106], [94mLoss[0m : 2.53058
[1mStep[0m  [50/106], [94mLoss[0m : 2.59112
[1mStep[0m  [60/106], [94mLoss[0m : 2.33830
[1mStep[0m  [70/106], [94mLoss[0m : 2.59762
[1mStep[0m  [80/106], [94mLoss[0m : 2.40653
[1mStep[0m  [90/106], [94mLoss[0m : 2.17965
[1mStep[0m  [100/106], [94mLoss[0m : 2.46321

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.407, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.62126
[1mStep[0m  [10/106], [94mLoss[0m : 2.65170
[1mStep[0m  [20/106], [94mLoss[0m : 2.16871
[1mStep[0m  [30/106], [94mLoss[0m : 2.70811
[1mStep[0m  [40/106], [94mLoss[0m : 2.53162
[1mStep[0m  [50/106], [94mLoss[0m : 2.29097
[1mStep[0m  [60/106], [94mLoss[0m : 2.43452
[1mStep[0m  [70/106], [94mLoss[0m : 2.42599
[1mStep[0m  [80/106], [94mLoss[0m : 2.56690
[1mStep[0m  [90/106], [94mLoss[0m : 2.47674
[1mStep[0m  [100/106], [94mLoss[0m : 2.26307

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.408, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.33164
[1mStep[0m  [10/106], [94mLoss[0m : 2.75771
[1mStep[0m  [20/106], [94mLoss[0m : 2.35494
[1mStep[0m  [30/106], [94mLoss[0m : 2.42549
[1mStep[0m  [40/106], [94mLoss[0m : 2.27725
[1mStep[0m  [50/106], [94mLoss[0m : 2.30714
[1mStep[0m  [60/106], [94mLoss[0m : 2.81055
[1mStep[0m  [70/106], [94mLoss[0m : 2.57694
[1mStep[0m  [80/106], [94mLoss[0m : 2.54388
[1mStep[0m  [90/106], [94mLoss[0m : 2.44635
[1mStep[0m  [100/106], [94mLoss[0m : 2.39447

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.403, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.20690
[1mStep[0m  [10/106], [94mLoss[0m : 2.51236
[1mStep[0m  [20/106], [94mLoss[0m : 2.28381
[1mStep[0m  [30/106], [94mLoss[0m : 2.37134
[1mStep[0m  [40/106], [94mLoss[0m : 2.28976
[1mStep[0m  [50/106], [94mLoss[0m : 2.47647
[1mStep[0m  [60/106], [94mLoss[0m : 2.35236
[1mStep[0m  [70/106], [94mLoss[0m : 2.38212
[1mStep[0m  [80/106], [94mLoss[0m : 2.48287
[1mStep[0m  [90/106], [94mLoss[0m : 2.31769
[1mStep[0m  [100/106], [94mLoss[0m : 2.44479

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.423, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.51074
[1mStep[0m  [10/106], [94mLoss[0m : 2.58032
[1mStep[0m  [20/106], [94mLoss[0m : 2.38434
[1mStep[0m  [30/106], [94mLoss[0m : 2.61303
[1mStep[0m  [40/106], [94mLoss[0m : 2.40142
[1mStep[0m  [50/106], [94mLoss[0m : 2.56912
[1mStep[0m  [60/106], [94mLoss[0m : 2.19373
[1mStep[0m  [70/106], [94mLoss[0m : 2.40790
[1mStep[0m  [80/106], [94mLoss[0m : 2.50757
[1mStep[0m  [90/106], [94mLoss[0m : 2.24301
[1mStep[0m  [100/106], [94mLoss[0m : 2.32073

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.409, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.17221
[1mStep[0m  [10/106], [94mLoss[0m : 2.37684
[1mStep[0m  [20/106], [94mLoss[0m : 2.98219
[1mStep[0m  [30/106], [94mLoss[0m : 2.42632
[1mStep[0m  [40/106], [94mLoss[0m : 2.70986
[1mStep[0m  [50/106], [94mLoss[0m : 2.46146
[1mStep[0m  [60/106], [94mLoss[0m : 2.17515
[1mStep[0m  [70/106], [94mLoss[0m : 2.82330
[1mStep[0m  [80/106], [94mLoss[0m : 2.48105
[1mStep[0m  [90/106], [94mLoss[0m : 2.28003
[1mStep[0m  [100/106], [94mLoss[0m : 2.35092

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.401, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.49304
[1mStep[0m  [10/106], [94mLoss[0m : 2.64958
[1mStep[0m  [20/106], [94mLoss[0m : 2.66539
[1mStep[0m  [30/106], [94mLoss[0m : 2.55245
[1mStep[0m  [40/106], [94mLoss[0m : 2.53743
[1mStep[0m  [50/106], [94mLoss[0m : 2.46584
[1mStep[0m  [60/106], [94mLoss[0m : 2.29167
[1mStep[0m  [70/106], [94mLoss[0m : 2.57453
[1mStep[0m  [80/106], [94mLoss[0m : 2.60354
[1mStep[0m  [90/106], [94mLoss[0m : 2.33332
[1mStep[0m  [100/106], [94mLoss[0m : 2.29154

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.405, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.54327
[1mStep[0m  [10/106], [94mLoss[0m : 2.64494
[1mStep[0m  [20/106], [94mLoss[0m : 2.48487
[1mStep[0m  [30/106], [94mLoss[0m : 2.24641
[1mStep[0m  [40/106], [94mLoss[0m : 2.65402
[1mStep[0m  [50/106], [94mLoss[0m : 2.39826
[1mStep[0m  [60/106], [94mLoss[0m : 2.37139
[1mStep[0m  [70/106], [94mLoss[0m : 2.52756
[1mStep[0m  [80/106], [94mLoss[0m : 2.27070
[1mStep[0m  [90/106], [94mLoss[0m : 2.45537
[1mStep[0m  [100/106], [94mLoss[0m : 2.21954

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.398, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.14900
[1mStep[0m  [10/106], [94mLoss[0m : 2.56034
[1mStep[0m  [20/106], [94mLoss[0m : 2.70881
[1mStep[0m  [30/106], [94mLoss[0m : 2.33202
[1mStep[0m  [40/106], [94mLoss[0m : 2.01241
[1mStep[0m  [50/106], [94mLoss[0m : 2.31582
[1mStep[0m  [60/106], [94mLoss[0m : 2.75341
[1mStep[0m  [70/106], [94mLoss[0m : 2.63359
[1mStep[0m  [80/106], [94mLoss[0m : 2.37256
[1mStep[0m  [90/106], [94mLoss[0m : 2.61898
[1mStep[0m  [100/106], [94mLoss[0m : 2.45145

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.395, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.57373
[1mStep[0m  [10/106], [94mLoss[0m : 2.47726
[1mStep[0m  [20/106], [94mLoss[0m : 2.55613
[1mStep[0m  [30/106], [94mLoss[0m : 2.15530
[1mStep[0m  [40/106], [94mLoss[0m : 2.48302
[1mStep[0m  [50/106], [94mLoss[0m : 2.51674
[1mStep[0m  [60/106], [94mLoss[0m : 2.29010
[1mStep[0m  [70/106], [94mLoss[0m : 2.51324
[1mStep[0m  [80/106], [94mLoss[0m : 2.46353
[1mStep[0m  [90/106], [94mLoss[0m : 2.46742
[1mStep[0m  [100/106], [94mLoss[0m : 2.35259

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.394, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.39601
[1mStep[0m  [10/106], [94mLoss[0m : 2.37229
[1mStep[0m  [20/106], [94mLoss[0m : 2.32322
[1mStep[0m  [30/106], [94mLoss[0m : 2.34137
[1mStep[0m  [40/106], [94mLoss[0m : 2.12691
[1mStep[0m  [50/106], [94mLoss[0m : 2.48900
[1mStep[0m  [60/106], [94mLoss[0m : 2.38047
[1mStep[0m  [70/106], [94mLoss[0m : 2.47294
[1mStep[0m  [80/106], [94mLoss[0m : 2.24281
[1mStep[0m  [90/106], [94mLoss[0m : 2.16010
[1mStep[0m  [100/106], [94mLoss[0m : 2.31069

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.400, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.407
====================================

Phase 1 - Evaluation MAE:  2.407297084916313
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/106], [94mLoss[0m : 2.11926
[1mStep[0m  [10/106], [94mLoss[0m : 2.58627
[1mStep[0m  [20/106], [94mLoss[0m : 2.24228
[1mStep[0m  [30/106], [94mLoss[0m : 2.52790
[1mStep[0m  [40/106], [94mLoss[0m : 2.82613
[1mStep[0m  [50/106], [94mLoss[0m : 2.47206
[1mStep[0m  [60/106], [94mLoss[0m : 2.64568
[1mStep[0m  [70/106], [94mLoss[0m : 2.42694
[1mStep[0m  [80/106], [94mLoss[0m : 2.47466
[1mStep[0m  [90/106], [94mLoss[0m : 2.78646
[1mStep[0m  [100/106], [94mLoss[0m : 2.42242

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.409, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.73042
[1mStep[0m  [10/106], [94mLoss[0m : 2.43215
[1mStep[0m  [20/106], [94mLoss[0m : 2.24331
[1mStep[0m  [30/106], [94mLoss[0m : 2.52570
[1mStep[0m  [40/106], [94mLoss[0m : 2.47150
[1mStep[0m  [50/106], [94mLoss[0m : 2.26622
[1mStep[0m  [60/106], [94mLoss[0m : 2.17773
[1mStep[0m  [70/106], [94mLoss[0m : 2.59545
[1mStep[0m  [80/106], [94mLoss[0m : 2.85932
[1mStep[0m  [90/106], [94mLoss[0m : 1.97976
[1mStep[0m  [100/106], [94mLoss[0m : 2.34490

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.390, [92mTest[0m: 2.664, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.41862
[1mStep[0m  [10/106], [94mLoss[0m : 2.05312
[1mStep[0m  [20/106], [94mLoss[0m : 2.20516
[1mStep[0m  [30/106], [94mLoss[0m : 2.14224
[1mStep[0m  [40/106], [94mLoss[0m : 2.18616
[1mStep[0m  [50/106], [94mLoss[0m : 2.24464
[1mStep[0m  [60/106], [94mLoss[0m : 2.58445
[1mStep[0m  [70/106], [94mLoss[0m : 2.23247
[1mStep[0m  [80/106], [94mLoss[0m : 2.27125
[1mStep[0m  [90/106], [94mLoss[0m : 2.60428
[1mStep[0m  [100/106], [94mLoss[0m : 2.20729

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.337, [92mTest[0m: 2.685, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.32336
[1mStep[0m  [10/106], [94mLoss[0m : 2.32233
[1mStep[0m  [20/106], [94mLoss[0m : 1.96400
[1mStep[0m  [30/106], [94mLoss[0m : 2.09308
[1mStep[0m  [40/106], [94mLoss[0m : 2.50339
[1mStep[0m  [50/106], [94mLoss[0m : 2.35859
[1mStep[0m  [60/106], [94mLoss[0m : 2.35297
[1mStep[0m  [70/106], [94mLoss[0m : 2.33205
[1mStep[0m  [80/106], [94mLoss[0m : 2.43592
[1mStep[0m  [90/106], [94mLoss[0m : 1.97209
[1mStep[0m  [100/106], [94mLoss[0m : 1.96288

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.286, [92mTest[0m: 2.617, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.27672
[1mStep[0m  [10/106], [94mLoss[0m : 2.13790
[1mStep[0m  [20/106], [94mLoss[0m : 2.27063
[1mStep[0m  [30/106], [94mLoss[0m : 2.15244
[1mStep[0m  [40/106], [94mLoss[0m : 2.03218
[1mStep[0m  [50/106], [94mLoss[0m : 2.30695
[1mStep[0m  [60/106], [94mLoss[0m : 2.34740
[1mStep[0m  [70/106], [94mLoss[0m : 2.17690
[1mStep[0m  [80/106], [94mLoss[0m : 2.23482
[1mStep[0m  [90/106], [94mLoss[0m : 2.38154
[1mStep[0m  [100/106], [94mLoss[0m : 2.34169

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.242, [92mTest[0m: 2.506, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.05543
[1mStep[0m  [10/106], [94mLoss[0m : 2.09304
[1mStep[0m  [20/106], [94mLoss[0m : 2.26980
[1mStep[0m  [30/106], [94mLoss[0m : 2.12905
[1mStep[0m  [40/106], [94mLoss[0m : 2.34274
[1mStep[0m  [50/106], [94mLoss[0m : 2.34560
[1mStep[0m  [60/106], [94mLoss[0m : 1.77766
[1mStep[0m  [70/106], [94mLoss[0m : 2.19104
[1mStep[0m  [80/106], [94mLoss[0m : 2.00810
[1mStep[0m  [90/106], [94mLoss[0m : 2.34443
[1mStep[0m  [100/106], [94mLoss[0m : 2.42209

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.191, [92mTest[0m: 2.593, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.95885
[1mStep[0m  [10/106], [94mLoss[0m : 2.05374
[1mStep[0m  [20/106], [94mLoss[0m : 1.90882
[1mStep[0m  [30/106], [94mLoss[0m : 2.07392
[1mStep[0m  [40/106], [94mLoss[0m : 2.23917
[1mStep[0m  [50/106], [94mLoss[0m : 1.99214
[1mStep[0m  [60/106], [94mLoss[0m : 2.17492
[1mStep[0m  [70/106], [94mLoss[0m : 2.04631
[1mStep[0m  [80/106], [94mLoss[0m : 2.19534
[1mStep[0m  [90/106], [94mLoss[0m : 2.12724
[1mStep[0m  [100/106], [94mLoss[0m : 2.03294

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.123, [92mTest[0m: 2.525, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.12444
[1mStep[0m  [10/106], [94mLoss[0m : 1.84018
[1mStep[0m  [20/106], [94mLoss[0m : 2.20503
[1mStep[0m  [30/106], [94mLoss[0m : 2.14407
[1mStep[0m  [40/106], [94mLoss[0m : 1.87226
[1mStep[0m  [50/106], [94mLoss[0m : 2.00791
[1mStep[0m  [60/106], [94mLoss[0m : 2.10294
[1mStep[0m  [70/106], [94mLoss[0m : 1.99585
[1mStep[0m  [80/106], [94mLoss[0m : 2.12621
[1mStep[0m  [90/106], [94mLoss[0m : 2.18805
[1mStep[0m  [100/106], [94mLoss[0m : 2.02098

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.068, [92mTest[0m: 2.524, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.00388
[1mStep[0m  [10/106], [94mLoss[0m : 2.03517
[1mStep[0m  [20/106], [94mLoss[0m : 1.65270
[1mStep[0m  [30/106], [94mLoss[0m : 1.95869
[1mStep[0m  [40/106], [94mLoss[0m : 1.93237
[1mStep[0m  [50/106], [94mLoss[0m : 1.78567
[1mStep[0m  [60/106], [94mLoss[0m : 2.26376
[1mStep[0m  [70/106], [94mLoss[0m : 1.94815
[1mStep[0m  [80/106], [94mLoss[0m : 1.87507
[1mStep[0m  [90/106], [94mLoss[0m : 2.19182
[1mStep[0m  [100/106], [94mLoss[0m : 2.19027

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.020, [92mTest[0m: 2.472, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.18167
[1mStep[0m  [10/106], [94mLoss[0m : 2.24645
[1mStep[0m  [20/106], [94mLoss[0m : 1.88211
[1mStep[0m  [30/106], [94mLoss[0m : 2.15851
[1mStep[0m  [40/106], [94mLoss[0m : 2.00708
[1mStep[0m  [50/106], [94mLoss[0m : 1.88195
[1mStep[0m  [60/106], [94mLoss[0m : 2.04471
[1mStep[0m  [70/106], [94mLoss[0m : 1.95882
[1mStep[0m  [80/106], [94mLoss[0m : 1.96193
[1mStep[0m  [90/106], [94mLoss[0m : 1.83364
[1mStep[0m  [100/106], [94mLoss[0m : 1.79100

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.978, [92mTest[0m: 2.566, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.81547
[1mStep[0m  [10/106], [94mLoss[0m : 1.78229
[1mStep[0m  [20/106], [94mLoss[0m : 1.77508
[1mStep[0m  [30/106], [94mLoss[0m : 1.76114
[1mStep[0m  [40/106], [94mLoss[0m : 2.00390
[1mStep[0m  [50/106], [94mLoss[0m : 1.83006
[1mStep[0m  [60/106], [94mLoss[0m : 2.01762
[1mStep[0m  [70/106], [94mLoss[0m : 1.88692
[1mStep[0m  [80/106], [94mLoss[0m : 1.81654
[1mStep[0m  [90/106], [94mLoss[0m : 2.14281
[1mStep[0m  [100/106], [94mLoss[0m : 1.94618

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.935, [92mTest[0m: 2.489, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.61463
[1mStep[0m  [10/106], [94mLoss[0m : 1.94883
[1mStep[0m  [20/106], [94mLoss[0m : 1.68366
[1mStep[0m  [30/106], [94mLoss[0m : 1.94408
[1mStep[0m  [40/106], [94mLoss[0m : 1.94029
[1mStep[0m  [50/106], [94mLoss[0m : 1.85902
[1mStep[0m  [60/106], [94mLoss[0m : 1.50772
[1mStep[0m  [70/106], [94mLoss[0m : 1.62096
[1mStep[0m  [80/106], [94mLoss[0m : 2.01155
[1mStep[0m  [90/106], [94mLoss[0m : 1.87270
[1mStep[0m  [100/106], [94mLoss[0m : 1.83615

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.873, [92mTest[0m: 2.412, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.01486
[1mStep[0m  [10/106], [94mLoss[0m : 2.05883
[1mStep[0m  [20/106], [94mLoss[0m : 1.93123
[1mStep[0m  [30/106], [94mLoss[0m : 1.87628
[1mStep[0m  [40/106], [94mLoss[0m : 1.64840
[1mStep[0m  [50/106], [94mLoss[0m : 2.02479
[1mStep[0m  [60/106], [94mLoss[0m : 1.78164
[1mStep[0m  [70/106], [94mLoss[0m : 1.80270
[1mStep[0m  [80/106], [94mLoss[0m : 1.75806
[1mStep[0m  [90/106], [94mLoss[0m : 2.04958
[1mStep[0m  [100/106], [94mLoss[0m : 2.10396

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.845, [92mTest[0m: 2.507, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.76609
[1mStep[0m  [10/106], [94mLoss[0m : 1.65395
[1mStep[0m  [20/106], [94mLoss[0m : 1.60716
[1mStep[0m  [30/106], [94mLoss[0m : 2.06758
[1mStep[0m  [40/106], [94mLoss[0m : 1.95271
[1mStep[0m  [50/106], [94mLoss[0m : 1.73099
[1mStep[0m  [60/106], [94mLoss[0m : 1.67913
[1mStep[0m  [70/106], [94mLoss[0m : 1.76025
[1mStep[0m  [80/106], [94mLoss[0m : 1.80449
[1mStep[0m  [90/106], [94mLoss[0m : 1.74528
[1mStep[0m  [100/106], [94mLoss[0m : 1.85104

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.816, [92mTest[0m: 2.475, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.67986
[1mStep[0m  [10/106], [94mLoss[0m : 1.71347
[1mStep[0m  [20/106], [94mLoss[0m : 1.80702
[1mStep[0m  [30/106], [94mLoss[0m : 1.84895
[1mStep[0m  [40/106], [94mLoss[0m : 1.69009
[1mStep[0m  [50/106], [94mLoss[0m : 1.94283
[1mStep[0m  [60/106], [94mLoss[0m : 1.96557
[1mStep[0m  [70/106], [94mLoss[0m : 1.84016
[1mStep[0m  [80/106], [94mLoss[0m : 1.74722
[1mStep[0m  [90/106], [94mLoss[0m : 1.64333
[1mStep[0m  [100/106], [94mLoss[0m : 1.48805

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.764, [92mTest[0m: 2.470, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.65246
[1mStep[0m  [10/106], [94mLoss[0m : 1.90593
[1mStep[0m  [20/106], [94mLoss[0m : 1.95702
[1mStep[0m  [30/106], [94mLoss[0m : 1.69142
[1mStep[0m  [40/106], [94mLoss[0m : 1.60459
[1mStep[0m  [50/106], [94mLoss[0m : 2.03901
[1mStep[0m  [60/106], [94mLoss[0m : 1.59468
[1mStep[0m  [70/106], [94mLoss[0m : 1.90028
[1mStep[0m  [80/106], [94mLoss[0m : 1.74453
[1mStep[0m  [90/106], [94mLoss[0m : 1.78486
[1mStep[0m  [100/106], [94mLoss[0m : 1.67705

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.734, [92mTest[0m: 2.520, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.75366
[1mStep[0m  [10/106], [94mLoss[0m : 1.58106
[1mStep[0m  [20/106], [94mLoss[0m : 1.84256
[1mStep[0m  [30/106], [94mLoss[0m : 1.73672
[1mStep[0m  [40/106], [94mLoss[0m : 1.76302
[1mStep[0m  [50/106], [94mLoss[0m : 1.69086
[1mStep[0m  [60/106], [94mLoss[0m : 1.49014
[1mStep[0m  [70/106], [94mLoss[0m : 1.69468
[1mStep[0m  [80/106], [94mLoss[0m : 1.80052
[1mStep[0m  [90/106], [94mLoss[0m : 1.57210
[1mStep[0m  [100/106], [94mLoss[0m : 1.48075

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.691, [92mTest[0m: 2.496, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.57939
[1mStep[0m  [10/106], [94mLoss[0m : 1.56933
[1mStep[0m  [20/106], [94mLoss[0m : 1.67655
[1mStep[0m  [30/106], [94mLoss[0m : 1.66753
[1mStep[0m  [40/106], [94mLoss[0m : 1.42855
[1mStep[0m  [50/106], [94mLoss[0m : 1.66231
[1mStep[0m  [60/106], [94mLoss[0m : 1.74626
[1mStep[0m  [70/106], [94mLoss[0m : 1.88279
[1mStep[0m  [80/106], [94mLoss[0m : 1.75776
[1mStep[0m  [90/106], [94mLoss[0m : 1.62399
[1mStep[0m  [100/106], [94mLoss[0m : 1.56627

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.659, [92mTest[0m: 2.423, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.53365
[1mStep[0m  [10/106], [94mLoss[0m : 1.59583
[1mStep[0m  [20/106], [94mLoss[0m : 1.32317
[1mStep[0m  [30/106], [94mLoss[0m : 1.49328
[1mStep[0m  [40/106], [94mLoss[0m : 1.59947
[1mStep[0m  [50/106], [94mLoss[0m : 1.64865
[1mStep[0m  [60/106], [94mLoss[0m : 1.58246
[1mStep[0m  [70/106], [94mLoss[0m : 1.77212
[1mStep[0m  [80/106], [94mLoss[0m : 1.95942
[1mStep[0m  [90/106], [94mLoss[0m : 1.63056
[1mStep[0m  [100/106], [94mLoss[0m : 1.58623

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.635, [92mTest[0m: 2.587, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.64537
[1mStep[0m  [10/106], [94mLoss[0m : 1.80616
[1mStep[0m  [20/106], [94mLoss[0m : 1.77235
[1mStep[0m  [30/106], [94mLoss[0m : 1.95216
[1mStep[0m  [40/106], [94mLoss[0m : 1.58106
[1mStep[0m  [50/106], [94mLoss[0m : 1.49373
[1mStep[0m  [60/106], [94mLoss[0m : 1.49438
[1mStep[0m  [70/106], [94mLoss[0m : 1.62104
[1mStep[0m  [80/106], [94mLoss[0m : 1.62468
[1mStep[0m  [90/106], [94mLoss[0m : 1.69999
[1mStep[0m  [100/106], [94mLoss[0m : 1.60279

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.612, [92mTest[0m: 2.505, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.87074
[1mStep[0m  [10/106], [94mLoss[0m : 1.61067
[1mStep[0m  [20/106], [94mLoss[0m : 1.60631
[1mStep[0m  [30/106], [94mLoss[0m : 1.45236
[1mStep[0m  [40/106], [94mLoss[0m : 1.65572
[1mStep[0m  [50/106], [94mLoss[0m : 1.50185
[1mStep[0m  [60/106], [94mLoss[0m : 1.34143
[1mStep[0m  [70/106], [94mLoss[0m : 1.68007
[1mStep[0m  [80/106], [94mLoss[0m : 1.62493
[1mStep[0m  [90/106], [94mLoss[0m : 1.64659
[1mStep[0m  [100/106], [94mLoss[0m : 1.65180

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.570, [92mTest[0m: 2.624, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.54500
[1mStep[0m  [10/106], [94mLoss[0m : 1.52055
[1mStep[0m  [20/106], [94mLoss[0m : 1.73091
[1mStep[0m  [30/106], [94mLoss[0m : 1.47277
[1mStep[0m  [40/106], [94mLoss[0m : 1.45607
[1mStep[0m  [50/106], [94mLoss[0m : 1.57804
[1mStep[0m  [60/106], [94mLoss[0m : 1.71199
[1mStep[0m  [70/106], [94mLoss[0m : 1.48021
[1mStep[0m  [80/106], [94mLoss[0m : 1.67455
[1mStep[0m  [90/106], [94mLoss[0m : 1.69241
[1mStep[0m  [100/106], [94mLoss[0m : 1.92994

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.511, [92mTest[0m: 2.525, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.45178
[1mStep[0m  [10/106], [94mLoss[0m : 1.41117
[1mStep[0m  [20/106], [94mLoss[0m : 1.28094
[1mStep[0m  [30/106], [94mLoss[0m : 1.49347
[1mStep[0m  [40/106], [94mLoss[0m : 1.52529
[1mStep[0m  [50/106], [94mLoss[0m : 1.68302
[1mStep[0m  [60/106], [94mLoss[0m : 1.46710
[1mStep[0m  [70/106], [94mLoss[0m : 1.54965
[1mStep[0m  [80/106], [94mLoss[0m : 1.53988
[1mStep[0m  [90/106], [94mLoss[0m : 1.40233
[1mStep[0m  [100/106], [94mLoss[0m : 1.62890

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.507, [92mTest[0m: 2.445, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.35813
[1mStep[0m  [10/106], [94mLoss[0m : 1.46889
[1mStep[0m  [20/106], [94mLoss[0m : 1.52578
[1mStep[0m  [30/106], [94mLoss[0m : 1.52336
[1mStep[0m  [40/106], [94mLoss[0m : 1.36562
[1mStep[0m  [50/106], [94mLoss[0m : 1.25762
[1mStep[0m  [60/106], [94mLoss[0m : 1.42719
[1mStep[0m  [70/106], [94mLoss[0m : 1.64559
[1mStep[0m  [80/106], [94mLoss[0m : 1.60970
[1mStep[0m  [90/106], [94mLoss[0m : 1.56165
[1mStep[0m  [100/106], [94mLoss[0m : 1.56247

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.460, [92mTest[0m: 2.443, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.32895
[1mStep[0m  [10/106], [94mLoss[0m : 1.49406
[1mStep[0m  [20/106], [94mLoss[0m : 1.53341
[1mStep[0m  [30/106], [94mLoss[0m : 1.37899
[1mStep[0m  [40/106], [94mLoss[0m : 1.53452
[1mStep[0m  [50/106], [94mLoss[0m : 1.62481
[1mStep[0m  [60/106], [94mLoss[0m : 1.38935
[1mStep[0m  [70/106], [94mLoss[0m : 1.59502
[1mStep[0m  [80/106], [94mLoss[0m : 1.70944
[1mStep[0m  [90/106], [94mLoss[0m : 1.37636
[1mStep[0m  [100/106], [94mLoss[0m : 1.25872

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.457, [92mTest[0m: 2.544, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.43538
[1mStep[0m  [10/106], [94mLoss[0m : 1.39750
[1mStep[0m  [20/106], [94mLoss[0m : 1.50932
[1mStep[0m  [30/106], [94mLoss[0m : 1.31204
[1mStep[0m  [40/106], [94mLoss[0m : 1.33279
[1mStep[0m  [50/106], [94mLoss[0m : 1.49404
[1mStep[0m  [60/106], [94mLoss[0m : 1.35458
[1mStep[0m  [70/106], [94mLoss[0m : 1.39320
[1mStep[0m  [80/106], [94mLoss[0m : 1.42929
[1mStep[0m  [90/106], [94mLoss[0m : 1.48076
[1mStep[0m  [100/106], [94mLoss[0m : 1.41902

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.438, [92mTest[0m: 2.476, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.35426
[1mStep[0m  [10/106], [94mLoss[0m : 1.49285
[1mStep[0m  [20/106], [94mLoss[0m : 1.36713
[1mStep[0m  [30/106], [94mLoss[0m : 1.22686
[1mStep[0m  [40/106], [94mLoss[0m : 1.43725
[1mStep[0m  [50/106], [94mLoss[0m : 1.29883
[1mStep[0m  [60/106], [94mLoss[0m : 1.58211
[1mStep[0m  [70/106], [94mLoss[0m : 1.47300
[1mStep[0m  [80/106], [94mLoss[0m : 1.20643
[1mStep[0m  [90/106], [94mLoss[0m : 1.56719
[1mStep[0m  [100/106], [94mLoss[0m : 1.30147

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.410, [92mTest[0m: 2.493, [96mlr[0m: 0.004545
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 26 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.463
====================================

Phase 2 - Evaluation MAE:  2.462559223175049
MAE score P1      2.407297
MAE score P2      2.462559
loss              1.409893
learning_rate      0.00505
batch_size             128
hidden_sizes         [300]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping        True
dropout                0.3
momentum               0.5
weight_decay         0.001
Name: 1, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/213], [94mLoss[0m : 11.00474
[1mStep[0m  [21/213], [94mLoss[0m : 7.46170
[1mStep[0m  [42/213], [94mLoss[0m : 4.74233
[1mStep[0m  [63/213], [94mLoss[0m : 3.88619
[1mStep[0m  [84/213], [94mLoss[0m : 3.07738
[1mStep[0m  [105/213], [94mLoss[0m : 3.03696
[1mStep[0m  [126/213], [94mLoss[0m : 2.67492
[1mStep[0m  [147/213], [94mLoss[0m : 2.50143
[1mStep[0m  [168/213], [94mLoss[0m : 2.47717
[1mStep[0m  [189/213], [94mLoss[0m : 2.70366
[1mStep[0m  [210/213], [94mLoss[0m : 2.95345

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.903, [92mTest[0m: 10.340, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.30823
[1mStep[0m  [21/213], [94mLoss[0m : 2.24995
[1mStep[0m  [42/213], [94mLoss[0m : 2.58517
[1mStep[0m  [63/213], [94mLoss[0m : 2.82897
[1mStep[0m  [84/213], [94mLoss[0m : 2.67461
[1mStep[0m  [105/213], [94mLoss[0m : 2.59873
[1mStep[0m  [126/213], [94mLoss[0m : 2.75931
[1mStep[0m  [147/213], [94mLoss[0m : 2.66498
[1mStep[0m  [168/213], [94mLoss[0m : 2.66366
[1mStep[0m  [189/213], [94mLoss[0m : 2.42416
[1mStep[0m  [210/213], [94mLoss[0m : 2.79632

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.683, [92mTest[0m: 2.476, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.55321
[1mStep[0m  [21/213], [94mLoss[0m : 2.52113
[1mStep[0m  [42/213], [94mLoss[0m : 2.65325
[1mStep[0m  [63/213], [94mLoss[0m : 2.53622
[1mStep[0m  [84/213], [94mLoss[0m : 2.53668
[1mStep[0m  [105/213], [94mLoss[0m : 2.54364
[1mStep[0m  [126/213], [94mLoss[0m : 2.84403
[1mStep[0m  [147/213], [94mLoss[0m : 2.76535
[1mStep[0m  [168/213], [94mLoss[0m : 3.10586
[1mStep[0m  [189/213], [94mLoss[0m : 2.69838
[1mStep[0m  [210/213], [94mLoss[0m : 2.82910

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.670, [92mTest[0m: 2.446, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.18469
[1mStep[0m  [21/213], [94mLoss[0m : 2.26527
[1mStep[0m  [42/213], [94mLoss[0m : 2.75974
[1mStep[0m  [63/213], [94mLoss[0m : 2.49465
[1mStep[0m  [84/213], [94mLoss[0m : 2.78756
[1mStep[0m  [105/213], [94mLoss[0m : 2.65721
[1mStep[0m  [126/213], [94mLoss[0m : 2.35709
[1mStep[0m  [147/213], [94mLoss[0m : 2.52376
[1mStep[0m  [168/213], [94mLoss[0m : 2.80156
[1mStep[0m  [189/213], [94mLoss[0m : 2.60935
[1mStep[0m  [210/213], [94mLoss[0m : 2.85119

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.644, [92mTest[0m: 2.435, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.50556
[1mStep[0m  [21/213], [94mLoss[0m : 2.80557
[1mStep[0m  [42/213], [94mLoss[0m : 2.44347
[1mStep[0m  [63/213], [94mLoss[0m : 2.96688
[1mStep[0m  [84/213], [94mLoss[0m : 2.81368
[1mStep[0m  [105/213], [94mLoss[0m : 2.77864
[1mStep[0m  [126/213], [94mLoss[0m : 2.37032
[1mStep[0m  [147/213], [94mLoss[0m : 2.65185
[1mStep[0m  [168/213], [94mLoss[0m : 2.51518
[1mStep[0m  [189/213], [94mLoss[0m : 2.85879
[1mStep[0m  [210/213], [94mLoss[0m : 2.77367

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.638, [92mTest[0m: 2.422, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.60793
[1mStep[0m  [21/213], [94mLoss[0m : 2.60058
[1mStep[0m  [42/213], [94mLoss[0m : 2.72396
[1mStep[0m  [63/213], [94mLoss[0m : 2.93305
[1mStep[0m  [84/213], [94mLoss[0m : 2.72699
[1mStep[0m  [105/213], [94mLoss[0m : 2.50437
[1mStep[0m  [126/213], [94mLoss[0m : 2.83608
[1mStep[0m  [147/213], [94mLoss[0m : 2.38908
[1mStep[0m  [168/213], [94mLoss[0m : 2.90734
[1mStep[0m  [189/213], [94mLoss[0m : 2.25301
[1mStep[0m  [210/213], [94mLoss[0m : 2.59783

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.621, [92mTest[0m: 2.429, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.37016
[1mStep[0m  [21/213], [94mLoss[0m : 2.57750
[1mStep[0m  [42/213], [94mLoss[0m : 2.40268
[1mStep[0m  [63/213], [94mLoss[0m : 2.60527
[1mStep[0m  [84/213], [94mLoss[0m : 3.11289
[1mStep[0m  [105/213], [94mLoss[0m : 2.51822
[1mStep[0m  [126/213], [94mLoss[0m : 2.67206
[1mStep[0m  [147/213], [94mLoss[0m : 3.10350
[1mStep[0m  [168/213], [94mLoss[0m : 2.79576
[1mStep[0m  [189/213], [94mLoss[0m : 2.41235
[1mStep[0m  [210/213], [94mLoss[0m : 2.79317

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.622, [92mTest[0m: 2.424, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.75241
[1mStep[0m  [21/213], [94mLoss[0m : 2.57368
[1mStep[0m  [42/213], [94mLoss[0m : 2.45703
[1mStep[0m  [63/213], [94mLoss[0m : 2.21364
[1mStep[0m  [84/213], [94mLoss[0m : 2.73197
[1mStep[0m  [105/213], [94mLoss[0m : 2.77923
[1mStep[0m  [126/213], [94mLoss[0m : 3.41868
[1mStep[0m  [147/213], [94mLoss[0m : 2.66031
[1mStep[0m  [168/213], [94mLoss[0m : 2.72502
[1mStep[0m  [189/213], [94mLoss[0m : 2.53566
[1mStep[0m  [210/213], [94mLoss[0m : 2.98522

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.605, [92mTest[0m: 2.413, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.23587
[1mStep[0m  [21/213], [94mLoss[0m : 2.39722
[1mStep[0m  [42/213], [94mLoss[0m : 2.49295
[1mStep[0m  [63/213], [94mLoss[0m : 2.38096
[1mStep[0m  [84/213], [94mLoss[0m : 2.47529
[1mStep[0m  [105/213], [94mLoss[0m : 2.71061
[1mStep[0m  [126/213], [94mLoss[0m : 2.34372
[1mStep[0m  [147/213], [94mLoss[0m : 2.80646
[1mStep[0m  [168/213], [94mLoss[0m : 2.47113
[1mStep[0m  [189/213], [94mLoss[0m : 2.44552
[1mStep[0m  [210/213], [94mLoss[0m : 2.52310

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.589, [92mTest[0m: 2.422, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.39981
[1mStep[0m  [21/213], [94mLoss[0m : 3.04616
[1mStep[0m  [42/213], [94mLoss[0m : 2.33847
[1mStep[0m  [63/213], [94mLoss[0m : 2.42569
[1mStep[0m  [84/213], [94mLoss[0m : 2.43346
[1mStep[0m  [105/213], [94mLoss[0m : 2.28587
[1mStep[0m  [126/213], [94mLoss[0m : 2.59282
[1mStep[0m  [147/213], [94mLoss[0m : 2.48295
[1mStep[0m  [168/213], [94mLoss[0m : 2.71941
[1mStep[0m  [189/213], [94mLoss[0m : 2.47348
[1mStep[0m  [210/213], [94mLoss[0m : 2.22147

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.611, [92mTest[0m: 2.416, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.43417
[1mStep[0m  [21/213], [94mLoss[0m : 2.59681
[1mStep[0m  [42/213], [94mLoss[0m : 2.79547
[1mStep[0m  [63/213], [94mLoss[0m : 2.62383
[1mStep[0m  [84/213], [94mLoss[0m : 2.89463
[1mStep[0m  [105/213], [94mLoss[0m : 2.79837
[1mStep[0m  [126/213], [94mLoss[0m : 2.43607
[1mStep[0m  [147/213], [94mLoss[0m : 3.09578
[1mStep[0m  [168/213], [94mLoss[0m : 3.04913
[1mStep[0m  [189/213], [94mLoss[0m : 2.47333
[1mStep[0m  [210/213], [94mLoss[0m : 2.94156

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.615, [92mTest[0m: 2.412, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.55449
[1mStep[0m  [21/213], [94mLoss[0m : 2.59024
[1mStep[0m  [42/213], [94mLoss[0m : 2.77843
[1mStep[0m  [63/213], [94mLoss[0m : 2.00640
[1mStep[0m  [84/213], [94mLoss[0m : 2.71573
[1mStep[0m  [105/213], [94mLoss[0m : 2.29280
[1mStep[0m  [126/213], [94mLoss[0m : 2.61100
[1mStep[0m  [147/213], [94mLoss[0m : 2.89015
[1mStep[0m  [168/213], [94mLoss[0m : 2.56975
[1mStep[0m  [189/213], [94mLoss[0m : 2.48446
[1mStep[0m  [210/213], [94mLoss[0m : 2.38776

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.594, [92mTest[0m: 2.410, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.39614
[1mStep[0m  [21/213], [94mLoss[0m : 2.33993
[1mStep[0m  [42/213], [94mLoss[0m : 2.68349
[1mStep[0m  [63/213], [94mLoss[0m : 2.81401
[1mStep[0m  [84/213], [94mLoss[0m : 2.66958
[1mStep[0m  [105/213], [94mLoss[0m : 2.47888
[1mStep[0m  [126/213], [94mLoss[0m : 2.60980
[1mStep[0m  [147/213], [94mLoss[0m : 2.32324
[1mStep[0m  [168/213], [94mLoss[0m : 2.33447
[1mStep[0m  [189/213], [94mLoss[0m : 2.17173
[1mStep[0m  [210/213], [94mLoss[0m : 2.69249

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.583, [92mTest[0m: 2.404, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.42368
[1mStep[0m  [21/213], [94mLoss[0m : 2.37250
[1mStep[0m  [42/213], [94mLoss[0m : 2.90639
[1mStep[0m  [63/213], [94mLoss[0m : 2.70704
[1mStep[0m  [84/213], [94mLoss[0m : 3.05217
[1mStep[0m  [105/213], [94mLoss[0m : 2.90755
[1mStep[0m  [126/213], [94mLoss[0m : 2.38537
[1mStep[0m  [147/213], [94mLoss[0m : 2.40218
[1mStep[0m  [168/213], [94mLoss[0m : 2.70097
[1mStep[0m  [189/213], [94mLoss[0m : 2.29597
[1mStep[0m  [210/213], [94mLoss[0m : 2.21944

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.594, [92mTest[0m: 2.406, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.54723
[1mStep[0m  [21/213], [94mLoss[0m : 2.48044
[1mStep[0m  [42/213], [94mLoss[0m : 2.00123
[1mStep[0m  [63/213], [94mLoss[0m : 2.64348
[1mStep[0m  [84/213], [94mLoss[0m : 2.25679
[1mStep[0m  [105/213], [94mLoss[0m : 2.57833
[1mStep[0m  [126/213], [94mLoss[0m : 2.45873
[1mStep[0m  [147/213], [94mLoss[0m : 2.96931
[1mStep[0m  [168/213], [94mLoss[0m : 2.41601
[1mStep[0m  [189/213], [94mLoss[0m : 2.34814
[1mStep[0m  [210/213], [94mLoss[0m : 2.40128

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.571, [92mTest[0m: 2.406, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.56610
[1mStep[0m  [21/213], [94mLoss[0m : 2.34374
[1mStep[0m  [42/213], [94mLoss[0m : 2.49791
[1mStep[0m  [63/213], [94mLoss[0m : 2.87473
[1mStep[0m  [84/213], [94mLoss[0m : 2.52659
[1mStep[0m  [105/213], [94mLoss[0m : 2.84714
[1mStep[0m  [126/213], [94mLoss[0m : 2.54059
[1mStep[0m  [147/213], [94mLoss[0m : 2.70984
[1mStep[0m  [168/213], [94mLoss[0m : 2.47726
[1mStep[0m  [189/213], [94mLoss[0m : 2.48541
[1mStep[0m  [210/213], [94mLoss[0m : 2.55996

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.568, [92mTest[0m: 2.408, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.73214
[1mStep[0m  [21/213], [94mLoss[0m : 2.16313
[1mStep[0m  [42/213], [94mLoss[0m : 2.68007
[1mStep[0m  [63/213], [94mLoss[0m : 3.18886
[1mStep[0m  [84/213], [94mLoss[0m : 2.58931
[1mStep[0m  [105/213], [94mLoss[0m : 3.33023
[1mStep[0m  [126/213], [94mLoss[0m : 2.26972
[1mStep[0m  [147/213], [94mLoss[0m : 2.85480
[1mStep[0m  [168/213], [94mLoss[0m : 2.43151
[1mStep[0m  [189/213], [94mLoss[0m : 2.07785
[1mStep[0m  [210/213], [94mLoss[0m : 2.15090

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.575, [92mTest[0m: 2.401, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.23822
[1mStep[0m  [21/213], [94mLoss[0m : 2.96185
[1mStep[0m  [42/213], [94mLoss[0m : 2.75370
[1mStep[0m  [63/213], [94mLoss[0m : 2.16226
[1mStep[0m  [84/213], [94mLoss[0m : 2.52286
[1mStep[0m  [105/213], [94mLoss[0m : 2.72556
[1mStep[0m  [126/213], [94mLoss[0m : 2.31858
[1mStep[0m  [147/213], [94mLoss[0m : 2.72242
[1mStep[0m  [168/213], [94mLoss[0m : 2.69308
[1mStep[0m  [189/213], [94mLoss[0m : 2.92139
[1mStep[0m  [210/213], [94mLoss[0m : 2.54229

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.580, [92mTest[0m: 2.410, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.93112
[1mStep[0m  [21/213], [94mLoss[0m : 2.48034
[1mStep[0m  [42/213], [94mLoss[0m : 2.22351
[1mStep[0m  [63/213], [94mLoss[0m : 2.43609
[1mStep[0m  [84/213], [94mLoss[0m : 2.52036
[1mStep[0m  [105/213], [94mLoss[0m : 2.57305
[1mStep[0m  [126/213], [94mLoss[0m : 2.27282
[1mStep[0m  [147/213], [94mLoss[0m : 2.33194
[1mStep[0m  [168/213], [94mLoss[0m : 2.88987
[1mStep[0m  [189/213], [94mLoss[0m : 2.58495
[1mStep[0m  [210/213], [94mLoss[0m : 2.53788

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.567, [92mTest[0m: 2.406, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.82890
[1mStep[0m  [21/213], [94mLoss[0m : 2.35372
[1mStep[0m  [42/213], [94mLoss[0m : 2.38078
[1mStep[0m  [63/213], [94mLoss[0m : 2.61261
[1mStep[0m  [84/213], [94mLoss[0m : 2.53303
[1mStep[0m  [105/213], [94mLoss[0m : 2.57776
[1mStep[0m  [126/213], [94mLoss[0m : 2.79840
[1mStep[0m  [147/213], [94mLoss[0m : 2.42056
[1mStep[0m  [168/213], [94mLoss[0m : 2.88671
[1mStep[0m  [189/213], [94mLoss[0m : 2.45660
[1mStep[0m  [210/213], [94mLoss[0m : 2.64552

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.563, [92mTest[0m: 2.407, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.77369
[1mStep[0m  [21/213], [94mLoss[0m : 2.81084
[1mStep[0m  [42/213], [94mLoss[0m : 2.40671
[1mStep[0m  [63/213], [94mLoss[0m : 3.07656
[1mStep[0m  [84/213], [94mLoss[0m : 2.93760
[1mStep[0m  [105/213], [94mLoss[0m : 2.65047
[1mStep[0m  [126/213], [94mLoss[0m : 2.33916
[1mStep[0m  [147/213], [94mLoss[0m : 2.63395
[1mStep[0m  [168/213], [94mLoss[0m : 2.31710
[1mStep[0m  [189/213], [94mLoss[0m : 2.45936
[1mStep[0m  [210/213], [94mLoss[0m : 1.96924

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.568, [92mTest[0m: 2.407, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.52456
[1mStep[0m  [21/213], [94mLoss[0m : 2.66494
[1mStep[0m  [42/213], [94mLoss[0m : 2.58550
[1mStep[0m  [63/213], [94mLoss[0m : 3.01525
[1mStep[0m  [84/213], [94mLoss[0m : 2.79627
[1mStep[0m  [105/213], [94mLoss[0m : 2.67631
[1mStep[0m  [126/213], [94mLoss[0m : 2.54819
[1mStep[0m  [147/213], [94mLoss[0m : 2.48341
[1mStep[0m  [168/213], [94mLoss[0m : 2.29397
[1mStep[0m  [189/213], [94mLoss[0m : 2.53273
[1mStep[0m  [210/213], [94mLoss[0m : 2.03166

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.575, [92mTest[0m: 2.402, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 3.00031
[1mStep[0m  [21/213], [94mLoss[0m : 2.89337
[1mStep[0m  [42/213], [94mLoss[0m : 2.55600
[1mStep[0m  [63/213], [94mLoss[0m : 2.61384
[1mStep[0m  [84/213], [94mLoss[0m : 2.60435
[1mStep[0m  [105/213], [94mLoss[0m : 2.69639
[1mStep[0m  [126/213], [94mLoss[0m : 2.74899
[1mStep[0m  [147/213], [94mLoss[0m : 2.43378
[1mStep[0m  [168/213], [94mLoss[0m : 2.81748
[1mStep[0m  [189/213], [94mLoss[0m : 2.90294
[1mStep[0m  [210/213], [94mLoss[0m : 2.27959

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.588, [92mTest[0m: 2.409, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.89377
[1mStep[0m  [21/213], [94mLoss[0m : 2.80635
[1mStep[0m  [42/213], [94mLoss[0m : 2.29695
[1mStep[0m  [63/213], [94mLoss[0m : 2.54145
[1mStep[0m  [84/213], [94mLoss[0m : 2.33625
[1mStep[0m  [105/213], [94mLoss[0m : 2.59783
[1mStep[0m  [126/213], [94mLoss[0m : 2.65362
[1mStep[0m  [147/213], [94mLoss[0m : 2.77236
[1mStep[0m  [168/213], [94mLoss[0m : 2.90465
[1mStep[0m  [189/213], [94mLoss[0m : 2.65199
[1mStep[0m  [210/213], [94mLoss[0m : 2.43116

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.557, [92mTest[0m: 2.402, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.93975
[1mStep[0m  [21/213], [94mLoss[0m : 2.51109
[1mStep[0m  [42/213], [94mLoss[0m : 2.44068
[1mStep[0m  [63/213], [94mLoss[0m : 2.73483
[1mStep[0m  [84/213], [94mLoss[0m : 2.29930
[1mStep[0m  [105/213], [94mLoss[0m : 2.70717
[1mStep[0m  [126/213], [94mLoss[0m : 2.99664
[1mStep[0m  [147/213], [94mLoss[0m : 2.46106
[1mStep[0m  [168/213], [94mLoss[0m : 2.52498
[1mStep[0m  [189/213], [94mLoss[0m : 2.92427
[1mStep[0m  [210/213], [94mLoss[0m : 2.82493

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.564, [92mTest[0m: 2.400, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.41178
[1mStep[0m  [21/213], [94mLoss[0m : 2.77882
[1mStep[0m  [42/213], [94mLoss[0m : 2.40287
[1mStep[0m  [63/213], [94mLoss[0m : 2.60766
[1mStep[0m  [84/213], [94mLoss[0m : 2.76631
[1mStep[0m  [105/213], [94mLoss[0m : 2.67432
[1mStep[0m  [126/213], [94mLoss[0m : 2.12289
[1mStep[0m  [147/213], [94mLoss[0m : 2.43914
[1mStep[0m  [168/213], [94mLoss[0m : 2.79047
[1mStep[0m  [189/213], [94mLoss[0m : 2.58067
[1mStep[0m  [210/213], [94mLoss[0m : 2.71861

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.560, [92mTest[0m: 2.404, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.08828
[1mStep[0m  [21/213], [94mLoss[0m : 2.79255
[1mStep[0m  [42/213], [94mLoss[0m : 2.54487
[1mStep[0m  [63/213], [94mLoss[0m : 2.70261
[1mStep[0m  [84/213], [94mLoss[0m : 2.21617
[1mStep[0m  [105/213], [94mLoss[0m : 2.31049
[1mStep[0m  [126/213], [94mLoss[0m : 2.75208
[1mStep[0m  [147/213], [94mLoss[0m : 2.76452
[1mStep[0m  [168/213], [94mLoss[0m : 3.07641
[1mStep[0m  [189/213], [94mLoss[0m : 2.58709
[1mStep[0m  [210/213], [94mLoss[0m : 2.72116

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.570, [92mTest[0m: 2.403, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.46979
[1mStep[0m  [21/213], [94mLoss[0m : 2.32050
[1mStep[0m  [42/213], [94mLoss[0m : 2.74171
[1mStep[0m  [63/213], [94mLoss[0m : 2.07159
[1mStep[0m  [84/213], [94mLoss[0m : 2.59230
[1mStep[0m  [105/213], [94mLoss[0m : 2.90848
[1mStep[0m  [126/213], [94mLoss[0m : 2.58202
[1mStep[0m  [147/213], [94mLoss[0m : 2.59721
[1mStep[0m  [168/213], [94mLoss[0m : 3.06908
[1mStep[0m  [189/213], [94mLoss[0m : 2.50872
[1mStep[0m  [210/213], [94mLoss[0m : 2.91016

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.573, [92mTest[0m: 2.416, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 3.01150
[1mStep[0m  [21/213], [94mLoss[0m : 2.08979
[1mStep[0m  [42/213], [94mLoss[0m : 2.35210
[1mStep[0m  [63/213], [94mLoss[0m : 2.50306
[1mStep[0m  [84/213], [94mLoss[0m : 2.47337
[1mStep[0m  [105/213], [94mLoss[0m : 2.73893
[1mStep[0m  [126/213], [94mLoss[0m : 2.70125
[1mStep[0m  [147/213], [94mLoss[0m : 2.18952
[1mStep[0m  [168/213], [94mLoss[0m : 2.77374
[1mStep[0m  [189/213], [94mLoss[0m : 2.33672
[1mStep[0m  [210/213], [94mLoss[0m : 2.04714

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.568, [92mTest[0m: 2.401, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.80974
[1mStep[0m  [21/213], [94mLoss[0m : 2.59399
[1mStep[0m  [42/213], [94mLoss[0m : 2.53022
[1mStep[0m  [63/213], [94mLoss[0m : 2.39656
[1mStep[0m  [84/213], [94mLoss[0m : 2.68007
[1mStep[0m  [105/213], [94mLoss[0m : 2.49552
[1mStep[0m  [126/213], [94mLoss[0m : 2.72952
[1mStep[0m  [147/213], [94mLoss[0m : 2.37536
[1mStep[0m  [168/213], [94mLoss[0m : 2.58586
[1mStep[0m  [189/213], [94mLoss[0m : 2.41305
[1mStep[0m  [210/213], [94mLoss[0m : 2.69251

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.567, [92mTest[0m: 2.402, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.409
====================================

Phase 1 - Evaluation MAE:  2.4090758303426347
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/213], [94mLoss[0m : 2.40989
[1mStep[0m  [21/213], [94mLoss[0m : 2.61682
[1mStep[0m  [42/213], [94mLoss[0m : 2.71338
[1mStep[0m  [63/213], [94mLoss[0m : 2.74875
[1mStep[0m  [84/213], [94mLoss[0m : 2.82458
[1mStep[0m  [105/213], [94mLoss[0m : 2.74858
[1mStep[0m  [126/213], [94mLoss[0m : 2.59243
[1mStep[0m  [147/213], [94mLoss[0m : 1.81896
[1mStep[0m  [168/213], [94mLoss[0m : 1.98361
[1mStep[0m  [189/213], [94mLoss[0m : 2.72997
[1mStep[0m  [210/213], [94mLoss[0m : 2.46060

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.568, [92mTest[0m: 2.409, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.88269
[1mStep[0m  [21/213], [94mLoss[0m : 2.61267
[1mStep[0m  [42/213], [94mLoss[0m : 2.83985
[1mStep[0m  [63/213], [94mLoss[0m : 2.16944
[1mStep[0m  [84/213], [94mLoss[0m : 3.22362
[1mStep[0m  [105/213], [94mLoss[0m : 2.59170
[1mStep[0m  [126/213], [94mLoss[0m : 2.93799
[1mStep[0m  [147/213], [94mLoss[0m : 2.62238
[1mStep[0m  [168/213], [94mLoss[0m : 2.55656
[1mStep[0m  [189/213], [94mLoss[0m : 2.75134
[1mStep[0m  [210/213], [94mLoss[0m : 2.81395

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.522, [92mTest[0m: 2.429, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.38185
[1mStep[0m  [21/213], [94mLoss[0m : 2.60505
[1mStep[0m  [42/213], [94mLoss[0m : 2.37067
[1mStep[0m  [63/213], [94mLoss[0m : 2.21832
[1mStep[0m  [84/213], [94mLoss[0m : 2.19177
[1mStep[0m  [105/213], [94mLoss[0m : 2.68834
[1mStep[0m  [126/213], [94mLoss[0m : 2.66842
[1mStep[0m  [147/213], [94mLoss[0m : 2.59002
[1mStep[0m  [168/213], [94mLoss[0m : 2.42669
[1mStep[0m  [189/213], [94mLoss[0m : 2.54965
[1mStep[0m  [210/213], [94mLoss[0m : 2.65233

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.434, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.32847
[1mStep[0m  [21/213], [94mLoss[0m : 2.26575
[1mStep[0m  [42/213], [94mLoss[0m : 2.50893
[1mStep[0m  [63/213], [94mLoss[0m : 2.43895
[1mStep[0m  [84/213], [94mLoss[0m : 2.16261
[1mStep[0m  [105/213], [94mLoss[0m : 2.34247
[1mStep[0m  [126/213], [94mLoss[0m : 2.35969
[1mStep[0m  [147/213], [94mLoss[0m : 2.31104
[1mStep[0m  [168/213], [94mLoss[0m : 2.55074
[1mStep[0m  [189/213], [94mLoss[0m : 2.24975
[1mStep[0m  [210/213], [94mLoss[0m : 3.07646

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.410, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.14214
[1mStep[0m  [21/213], [94mLoss[0m : 2.63412
[1mStep[0m  [42/213], [94mLoss[0m : 1.91628
[1mStep[0m  [63/213], [94mLoss[0m : 2.58347
[1mStep[0m  [84/213], [94mLoss[0m : 2.36300
[1mStep[0m  [105/213], [94mLoss[0m : 2.88114
[1mStep[0m  [126/213], [94mLoss[0m : 2.04623
[1mStep[0m  [147/213], [94mLoss[0m : 2.45235
[1mStep[0m  [168/213], [94mLoss[0m : 2.42338
[1mStep[0m  [189/213], [94mLoss[0m : 2.93100
[1mStep[0m  [210/213], [94mLoss[0m : 2.08886

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.421, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.60648
[1mStep[0m  [21/213], [94mLoss[0m : 2.15404
[1mStep[0m  [42/213], [94mLoss[0m : 2.37941
[1mStep[0m  [63/213], [94mLoss[0m : 2.26649
[1mStep[0m  [84/213], [94mLoss[0m : 1.94591
[1mStep[0m  [105/213], [94mLoss[0m : 2.76829
[1mStep[0m  [126/213], [94mLoss[0m : 2.34933
[1mStep[0m  [147/213], [94mLoss[0m : 2.83245
[1mStep[0m  [168/213], [94mLoss[0m : 2.44042
[1mStep[0m  [189/213], [94mLoss[0m : 2.23925
[1mStep[0m  [210/213], [94mLoss[0m : 2.29781

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.371, [92mTest[0m: 2.418, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.31026
[1mStep[0m  [21/213], [94mLoss[0m : 2.15309
[1mStep[0m  [42/213], [94mLoss[0m : 2.02903
[1mStep[0m  [63/213], [94mLoss[0m : 2.56350
[1mStep[0m  [84/213], [94mLoss[0m : 2.06888
[1mStep[0m  [105/213], [94mLoss[0m : 2.44622
[1mStep[0m  [126/213], [94mLoss[0m : 2.40106
[1mStep[0m  [147/213], [94mLoss[0m : 2.39241
[1mStep[0m  [168/213], [94mLoss[0m : 2.06941
[1mStep[0m  [189/213], [94mLoss[0m : 2.32459
[1mStep[0m  [210/213], [94mLoss[0m : 2.27890

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.341, [92mTest[0m: 2.416, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.22669
[1mStep[0m  [21/213], [94mLoss[0m : 2.53344
[1mStep[0m  [42/213], [94mLoss[0m : 2.28516
[1mStep[0m  [63/213], [94mLoss[0m : 1.94279
[1mStep[0m  [84/213], [94mLoss[0m : 2.81377
[1mStep[0m  [105/213], [94mLoss[0m : 2.19539
[1mStep[0m  [126/213], [94mLoss[0m : 2.61876
[1mStep[0m  [147/213], [94mLoss[0m : 2.39580
[1mStep[0m  [168/213], [94mLoss[0m : 2.16839
[1mStep[0m  [189/213], [94mLoss[0m : 2.36794
[1mStep[0m  [210/213], [94mLoss[0m : 2.35393

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.303, [92mTest[0m: 2.402, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.25943
[1mStep[0m  [21/213], [94mLoss[0m : 2.48048
[1mStep[0m  [42/213], [94mLoss[0m : 2.79999
[1mStep[0m  [63/213], [94mLoss[0m : 2.52876
[1mStep[0m  [84/213], [94mLoss[0m : 2.02939
[1mStep[0m  [105/213], [94mLoss[0m : 2.29033
[1mStep[0m  [126/213], [94mLoss[0m : 2.71896
[1mStep[0m  [147/213], [94mLoss[0m : 2.29810
[1mStep[0m  [168/213], [94mLoss[0m : 2.91368
[1mStep[0m  [189/213], [94mLoss[0m : 2.77428
[1mStep[0m  [210/213], [94mLoss[0m : 2.24566

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.255, [92mTest[0m: 2.376, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.10931
[1mStep[0m  [21/213], [94mLoss[0m : 2.29022
[1mStep[0m  [42/213], [94mLoss[0m : 2.57901
[1mStep[0m  [63/213], [94mLoss[0m : 2.07818
[1mStep[0m  [84/213], [94mLoss[0m : 2.12890
[1mStep[0m  [105/213], [94mLoss[0m : 2.13489
[1mStep[0m  [126/213], [94mLoss[0m : 2.05392
[1mStep[0m  [147/213], [94mLoss[0m : 2.28466
[1mStep[0m  [168/213], [94mLoss[0m : 2.13584
[1mStep[0m  [189/213], [94mLoss[0m : 2.20752
[1mStep[0m  [210/213], [94mLoss[0m : 2.31806

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.252, [92mTest[0m: 2.411, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.16730
[1mStep[0m  [21/213], [94mLoss[0m : 1.97673
[1mStep[0m  [42/213], [94mLoss[0m : 2.06957
[1mStep[0m  [63/213], [94mLoss[0m : 2.01019
[1mStep[0m  [84/213], [94mLoss[0m : 1.86874
[1mStep[0m  [105/213], [94mLoss[0m : 2.20241
[1mStep[0m  [126/213], [94mLoss[0m : 1.77998
[1mStep[0m  [147/213], [94mLoss[0m : 2.12849
[1mStep[0m  [168/213], [94mLoss[0m : 2.39522
[1mStep[0m  [189/213], [94mLoss[0m : 2.50897
[1mStep[0m  [210/213], [94mLoss[0m : 2.23198

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.218, [92mTest[0m: 2.470, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.04987
[1mStep[0m  [21/213], [94mLoss[0m : 1.95296
[1mStep[0m  [42/213], [94mLoss[0m : 2.63354
[1mStep[0m  [63/213], [94mLoss[0m : 1.89555
[1mStep[0m  [84/213], [94mLoss[0m : 2.14087
[1mStep[0m  [105/213], [94mLoss[0m : 2.02253
[1mStep[0m  [126/213], [94mLoss[0m : 2.33764
[1mStep[0m  [147/213], [94mLoss[0m : 2.32303
[1mStep[0m  [168/213], [94mLoss[0m : 2.20122
[1mStep[0m  [189/213], [94mLoss[0m : 2.24033
[1mStep[0m  [210/213], [94mLoss[0m : 2.10311

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.178, [92mTest[0m: 2.455, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.25457
[1mStep[0m  [21/213], [94mLoss[0m : 2.13520
[1mStep[0m  [42/213], [94mLoss[0m : 1.92299
[1mStep[0m  [63/213], [94mLoss[0m : 2.20355
[1mStep[0m  [84/213], [94mLoss[0m : 2.37121
[1mStep[0m  [105/213], [94mLoss[0m : 1.98788
[1mStep[0m  [126/213], [94mLoss[0m : 1.92009
[1mStep[0m  [147/213], [94mLoss[0m : 2.24804
[1mStep[0m  [168/213], [94mLoss[0m : 2.16648
[1mStep[0m  [189/213], [94mLoss[0m : 1.86220
[1mStep[0m  [210/213], [94mLoss[0m : 2.65186

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.160, [92mTest[0m: 2.398, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.99243
[1mStep[0m  [21/213], [94mLoss[0m : 2.22313
[1mStep[0m  [42/213], [94mLoss[0m : 1.92602
[1mStep[0m  [63/213], [94mLoss[0m : 2.54105
[1mStep[0m  [84/213], [94mLoss[0m : 2.20001
[1mStep[0m  [105/213], [94mLoss[0m : 2.56528
[1mStep[0m  [126/213], [94mLoss[0m : 2.35768
[1mStep[0m  [147/213], [94mLoss[0m : 2.25488
[1mStep[0m  [168/213], [94mLoss[0m : 2.14105
[1mStep[0m  [189/213], [94mLoss[0m : 2.12555
[1mStep[0m  [210/213], [94mLoss[0m : 2.42423

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.126, [92mTest[0m: 2.417, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.72696
[1mStep[0m  [21/213], [94mLoss[0m : 2.05854
[1mStep[0m  [42/213], [94mLoss[0m : 2.29903
[1mStep[0m  [63/213], [94mLoss[0m : 2.27983
[1mStep[0m  [84/213], [94mLoss[0m : 2.36213
[1mStep[0m  [105/213], [94mLoss[0m : 2.06660
[1mStep[0m  [126/213], [94mLoss[0m : 2.30169
[1mStep[0m  [147/213], [94mLoss[0m : 2.60735
[1mStep[0m  [168/213], [94mLoss[0m : 2.15394
[1mStep[0m  [189/213], [94mLoss[0m : 2.01128
[1mStep[0m  [210/213], [94mLoss[0m : 2.12745

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.116, [92mTest[0m: 2.431, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.21581
[1mStep[0m  [21/213], [94mLoss[0m : 1.86590
[1mStep[0m  [42/213], [94mLoss[0m : 2.75576
[1mStep[0m  [63/213], [94mLoss[0m : 2.11851
[1mStep[0m  [84/213], [94mLoss[0m : 1.91713
[1mStep[0m  [105/213], [94mLoss[0m : 2.15780
[1mStep[0m  [126/213], [94mLoss[0m : 1.74850
[1mStep[0m  [147/213], [94mLoss[0m : 2.15795
[1mStep[0m  [168/213], [94mLoss[0m : 2.13044
[1mStep[0m  [189/213], [94mLoss[0m : 1.84594
[1mStep[0m  [210/213], [94mLoss[0m : 1.98011

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.097, [92mTest[0m: 2.442, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.93413
[1mStep[0m  [21/213], [94mLoss[0m : 2.02355
[1mStep[0m  [42/213], [94mLoss[0m : 2.00312
[1mStep[0m  [63/213], [94mLoss[0m : 1.95172
[1mStep[0m  [84/213], [94mLoss[0m : 1.78592
[1mStep[0m  [105/213], [94mLoss[0m : 2.11872
[1mStep[0m  [126/213], [94mLoss[0m : 2.47164
[1mStep[0m  [147/213], [94mLoss[0m : 2.14259
[1mStep[0m  [168/213], [94mLoss[0m : 1.92799
[1mStep[0m  [189/213], [94mLoss[0m : 1.91981
[1mStep[0m  [210/213], [94mLoss[0m : 1.83472

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.093, [92mTest[0m: 2.440, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.14217
[1mStep[0m  [21/213], [94mLoss[0m : 1.81495
[1mStep[0m  [42/213], [94mLoss[0m : 2.39558
[1mStep[0m  [63/213], [94mLoss[0m : 1.97654
[1mStep[0m  [84/213], [94mLoss[0m : 2.35311
[1mStep[0m  [105/213], [94mLoss[0m : 1.92726
[1mStep[0m  [126/213], [94mLoss[0m : 1.85891
[1mStep[0m  [147/213], [94mLoss[0m : 2.28059
[1mStep[0m  [168/213], [94mLoss[0m : 2.24112
[1mStep[0m  [189/213], [94mLoss[0m : 1.65614
[1mStep[0m  [210/213], [94mLoss[0m : 2.11380

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.068, [92mTest[0m: 2.432, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.97488
[1mStep[0m  [21/213], [94mLoss[0m : 2.63808
[1mStep[0m  [42/213], [94mLoss[0m : 1.76155
[1mStep[0m  [63/213], [94mLoss[0m : 2.11353
[1mStep[0m  [84/213], [94mLoss[0m : 1.66947
[1mStep[0m  [105/213], [94mLoss[0m : 2.28462
[1mStep[0m  [126/213], [94mLoss[0m : 2.12033
[1mStep[0m  [147/213], [94mLoss[0m : 1.89445
[1mStep[0m  [168/213], [94mLoss[0m : 2.31282
[1mStep[0m  [189/213], [94mLoss[0m : 1.76106
[1mStep[0m  [210/213], [94mLoss[0m : 1.95942

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.050, [92mTest[0m: 2.460, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.73029
[1mStep[0m  [21/213], [94mLoss[0m : 1.89194
[1mStep[0m  [42/213], [94mLoss[0m : 2.03258
[1mStep[0m  [63/213], [94mLoss[0m : 2.07494
[1mStep[0m  [84/213], [94mLoss[0m : 2.52379
[1mStep[0m  [105/213], [94mLoss[0m : 2.11629
[1mStep[0m  [126/213], [94mLoss[0m : 1.92549
[1mStep[0m  [147/213], [94mLoss[0m : 2.31135
[1mStep[0m  [168/213], [94mLoss[0m : 2.36549
[1mStep[0m  [189/213], [94mLoss[0m : 2.31545
[1mStep[0m  [210/213], [94mLoss[0m : 1.93889

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.058, [92mTest[0m: 2.481, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.33415
[1mStep[0m  [21/213], [94mLoss[0m : 2.00769
[1mStep[0m  [42/213], [94mLoss[0m : 1.73267
[1mStep[0m  [63/213], [94mLoss[0m : 2.08513
[1mStep[0m  [84/213], [94mLoss[0m : 2.00540
[1mStep[0m  [105/213], [94mLoss[0m : 2.35328
[1mStep[0m  [126/213], [94mLoss[0m : 2.16066
[1mStep[0m  [147/213], [94mLoss[0m : 2.01104
[1mStep[0m  [168/213], [94mLoss[0m : 1.73624
[1mStep[0m  [189/213], [94mLoss[0m : 2.26413
[1mStep[0m  [210/213], [94mLoss[0m : 2.32422

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.020, [92mTest[0m: 2.434, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.01745
[1mStep[0m  [21/213], [94mLoss[0m : 1.63315
[1mStep[0m  [42/213], [94mLoss[0m : 1.83314
[1mStep[0m  [63/213], [94mLoss[0m : 2.11263
[1mStep[0m  [84/213], [94mLoss[0m : 1.77677
[1mStep[0m  [105/213], [94mLoss[0m : 1.90648
[1mStep[0m  [126/213], [94mLoss[0m : 2.26676
[1mStep[0m  [147/213], [94mLoss[0m : 1.89213
[1mStep[0m  [168/213], [94mLoss[0m : 1.98982
[1mStep[0m  [189/213], [94mLoss[0m : 1.53656
[1mStep[0m  [210/213], [94mLoss[0m : 2.01604

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.982, [92mTest[0m: 2.469, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.89113
[1mStep[0m  [21/213], [94mLoss[0m : 1.93092
[1mStep[0m  [42/213], [94mLoss[0m : 1.99260
[1mStep[0m  [63/213], [94mLoss[0m : 2.00080
[1mStep[0m  [84/213], [94mLoss[0m : 1.55385
[1mStep[0m  [105/213], [94mLoss[0m : 1.89624
[1mStep[0m  [126/213], [94mLoss[0m : 2.01072
[1mStep[0m  [147/213], [94mLoss[0m : 1.64615
[1mStep[0m  [168/213], [94mLoss[0m : 2.07698
[1mStep[0m  [189/213], [94mLoss[0m : 2.00243
[1mStep[0m  [210/213], [94mLoss[0m : 2.22585

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.987, [92mTest[0m: 2.490, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.82884
[1mStep[0m  [21/213], [94mLoss[0m : 2.04178
[1mStep[0m  [42/213], [94mLoss[0m : 2.01963
[1mStep[0m  [63/213], [94mLoss[0m : 2.09310
[1mStep[0m  [84/213], [94mLoss[0m : 1.79504
[1mStep[0m  [105/213], [94mLoss[0m : 1.75447
[1mStep[0m  [126/213], [94mLoss[0m : 2.07675
[1mStep[0m  [147/213], [94mLoss[0m : 1.79262
[1mStep[0m  [168/213], [94mLoss[0m : 2.13825
[1mStep[0m  [189/213], [94mLoss[0m : 2.34742
[1mStep[0m  [210/213], [94mLoss[0m : 2.00166

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.984, [92mTest[0m: 2.494, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.55819
[1mStep[0m  [21/213], [94mLoss[0m : 2.05618
[1mStep[0m  [42/213], [94mLoss[0m : 2.25779
[1mStep[0m  [63/213], [94mLoss[0m : 1.91022
[1mStep[0m  [84/213], [94mLoss[0m : 1.92704
[1mStep[0m  [105/213], [94mLoss[0m : 2.08813
[1mStep[0m  [126/213], [94mLoss[0m : 1.90569
[1mStep[0m  [147/213], [94mLoss[0m : 2.27017
[1mStep[0m  [168/213], [94mLoss[0m : 1.95953
[1mStep[0m  [189/213], [94mLoss[0m : 2.08399
[1mStep[0m  [210/213], [94mLoss[0m : 1.81363

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.977, [92mTest[0m: 2.436, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.71061
[1mStep[0m  [21/213], [94mLoss[0m : 2.06628
[1mStep[0m  [42/213], [94mLoss[0m : 2.26829
[1mStep[0m  [63/213], [94mLoss[0m : 1.79040
[1mStep[0m  [84/213], [94mLoss[0m : 1.94122
[1mStep[0m  [105/213], [94mLoss[0m : 1.89876
[1mStep[0m  [126/213], [94mLoss[0m : 1.60067
[1mStep[0m  [147/213], [94mLoss[0m : 1.95911
[1mStep[0m  [168/213], [94mLoss[0m : 2.13719
[1mStep[0m  [189/213], [94mLoss[0m : 1.86819
[1mStep[0m  [210/213], [94mLoss[0m : 2.02779

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.940, [92mTest[0m: 2.436, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.78648
[1mStep[0m  [21/213], [94mLoss[0m : 1.94512
[1mStep[0m  [42/213], [94mLoss[0m : 1.83200
[1mStep[0m  [63/213], [94mLoss[0m : 2.45511
[1mStep[0m  [84/213], [94mLoss[0m : 1.92632
[1mStep[0m  [105/213], [94mLoss[0m : 1.99370
[1mStep[0m  [126/213], [94mLoss[0m : 1.64206
[1mStep[0m  [147/213], [94mLoss[0m : 2.01538
[1mStep[0m  [168/213], [94mLoss[0m : 2.00373
[1mStep[0m  [189/213], [94mLoss[0m : 1.88684
[1mStep[0m  [210/213], [94mLoss[0m : 1.87527

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.950, [92mTest[0m: 2.463, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.14443
[1mStep[0m  [21/213], [94mLoss[0m : 1.58815
[1mStep[0m  [42/213], [94mLoss[0m : 1.90545
[1mStep[0m  [63/213], [94mLoss[0m : 2.12605
[1mStep[0m  [84/213], [94mLoss[0m : 1.90650
[1mStep[0m  [105/213], [94mLoss[0m : 1.95105
[1mStep[0m  [126/213], [94mLoss[0m : 1.89721
[1mStep[0m  [147/213], [94mLoss[0m : 1.67806
[1mStep[0m  [168/213], [94mLoss[0m : 1.99403
[1mStep[0m  [189/213], [94mLoss[0m : 1.90470
[1mStep[0m  [210/213], [94mLoss[0m : 2.11100

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.961, [92mTest[0m: 2.547, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.00473
[1mStep[0m  [21/213], [94mLoss[0m : 1.67668
[1mStep[0m  [42/213], [94mLoss[0m : 2.10114
[1mStep[0m  [63/213], [94mLoss[0m : 1.86774
[1mStep[0m  [84/213], [94mLoss[0m : 1.84073
[1mStep[0m  [105/213], [94mLoss[0m : 1.95279
[1mStep[0m  [126/213], [94mLoss[0m : 1.86360
[1mStep[0m  [147/213], [94mLoss[0m : 2.03336
[1mStep[0m  [168/213], [94mLoss[0m : 2.01828
[1mStep[0m  [189/213], [94mLoss[0m : 1.62805
[1mStep[0m  [210/213], [94mLoss[0m : 2.18955

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.939, [92mTest[0m: 2.460, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.81308
[1mStep[0m  [21/213], [94mLoss[0m : 1.63084
[1mStep[0m  [42/213], [94mLoss[0m : 1.95801
[1mStep[0m  [63/213], [94mLoss[0m : 2.17331
[1mStep[0m  [84/213], [94mLoss[0m : 1.43395
[1mStep[0m  [105/213], [94mLoss[0m : 1.79398
[1mStep[0m  [126/213], [94mLoss[0m : 1.84736
[1mStep[0m  [147/213], [94mLoss[0m : 1.69633
[1mStep[0m  [168/213], [94mLoss[0m : 2.14488
[1mStep[0m  [189/213], [94mLoss[0m : 2.06266
[1mStep[0m  [210/213], [94mLoss[0m : 2.22359

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.938, [92mTest[0m: 2.487, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.428
====================================

Phase 2 - Evaluation MAE:  2.4276625019199445
MAE score P1       2.409076
MAE score P2       2.427663
loss               1.937749
learning_rate       0.00505
batch_size               64
hidden_sizes      [200, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.5
weight_decay           0.01
Name: 2, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/106], [94mLoss[0m : 11.16156
[1mStep[0m  [10/106], [94mLoss[0m : 11.02228
[1mStep[0m  [20/106], [94mLoss[0m : 9.54075
[1mStep[0m  [30/106], [94mLoss[0m : 8.72754
[1mStep[0m  [40/106], [94mLoss[0m : 8.12288
[1mStep[0m  [50/106], [94mLoss[0m : 7.35822
[1mStep[0m  [60/106], [94mLoss[0m : 6.56841
[1mStep[0m  [70/106], [94mLoss[0m : 5.12323
[1mStep[0m  [80/106], [94mLoss[0m : 4.70348
[1mStep[0m  [90/106], [94mLoss[0m : 4.09707
[1mStep[0m  [100/106], [94mLoss[0m : 3.22863

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.973, [92mTest[0m: 11.057, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.76341
[1mStep[0m  [10/106], [94mLoss[0m : 2.60082
[1mStep[0m  [20/106], [94mLoss[0m : 2.85099
[1mStep[0m  [30/106], [94mLoss[0m : 2.72039
[1mStep[0m  [40/106], [94mLoss[0m : 2.63849
[1mStep[0m  [50/106], [94mLoss[0m : 2.46293
[1mStep[0m  [60/106], [94mLoss[0m : 2.59933
[1mStep[0m  [70/106], [94mLoss[0m : 2.54873
[1mStep[0m  [80/106], [94mLoss[0m : 2.77407
[1mStep[0m  [90/106], [94mLoss[0m : 2.54198
[1mStep[0m  [100/106], [94mLoss[0m : 2.73769

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.690, [92mTest[0m: 2.876, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.44220
[1mStep[0m  [10/106], [94mLoss[0m : 2.61795
[1mStep[0m  [20/106], [94mLoss[0m : 2.54062
[1mStep[0m  [30/106], [94mLoss[0m : 2.76399
[1mStep[0m  [40/106], [94mLoss[0m : 2.52594
[1mStep[0m  [50/106], [94mLoss[0m : 2.60020
[1mStep[0m  [60/106], [94mLoss[0m : 2.69145
[1mStep[0m  [70/106], [94mLoss[0m : 2.52146
[1mStep[0m  [80/106], [94mLoss[0m : 2.53008
[1mStep[0m  [90/106], [94mLoss[0m : 2.65583
[1mStep[0m  [100/106], [94mLoss[0m : 2.70763

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.558, [92mTest[0m: 2.509, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.47748
[1mStep[0m  [10/106], [94mLoss[0m : 2.13617
[1mStep[0m  [20/106], [94mLoss[0m : 2.72215
[1mStep[0m  [30/106], [94mLoss[0m : 2.56177
[1mStep[0m  [40/106], [94mLoss[0m : 2.40095
[1mStep[0m  [50/106], [94mLoss[0m : 2.37338
[1mStep[0m  [60/106], [94mLoss[0m : 2.41731
[1mStep[0m  [70/106], [94mLoss[0m : 2.33501
[1mStep[0m  [80/106], [94mLoss[0m : 2.76123
[1mStep[0m  [90/106], [94mLoss[0m : 2.66879
[1mStep[0m  [100/106], [94mLoss[0m : 2.61424

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.494, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.22958
[1mStep[0m  [10/106], [94mLoss[0m : 2.28047
[1mStep[0m  [20/106], [94mLoss[0m : 2.32805
[1mStep[0m  [30/106], [94mLoss[0m : 2.40993
[1mStep[0m  [40/106], [94mLoss[0m : 2.34011
[1mStep[0m  [50/106], [94mLoss[0m : 2.53561
[1mStep[0m  [60/106], [94mLoss[0m : 2.76160
[1mStep[0m  [70/106], [94mLoss[0m : 2.42102
[1mStep[0m  [80/106], [94mLoss[0m : 2.80478
[1mStep[0m  [90/106], [94mLoss[0m : 3.06132
[1mStep[0m  [100/106], [94mLoss[0m : 2.71062

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.419, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.46240
[1mStep[0m  [10/106], [94mLoss[0m : 2.26754
[1mStep[0m  [20/106], [94mLoss[0m : 2.32647
[1mStep[0m  [30/106], [94mLoss[0m : 2.36878
[1mStep[0m  [40/106], [94mLoss[0m : 2.49954
[1mStep[0m  [50/106], [94mLoss[0m : 2.50842
[1mStep[0m  [60/106], [94mLoss[0m : 2.29039
[1mStep[0m  [70/106], [94mLoss[0m : 2.54222
[1mStep[0m  [80/106], [94mLoss[0m : 2.85980
[1mStep[0m  [90/106], [94mLoss[0m : 2.47744
[1mStep[0m  [100/106], [94mLoss[0m : 2.41530

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.434, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.21851
[1mStep[0m  [10/106], [94mLoss[0m : 2.90130
[1mStep[0m  [20/106], [94mLoss[0m : 2.33246
[1mStep[0m  [30/106], [94mLoss[0m : 2.04434
[1mStep[0m  [40/106], [94mLoss[0m : 2.40106
[1mStep[0m  [50/106], [94mLoss[0m : 2.37089
[1mStep[0m  [60/106], [94mLoss[0m : 2.08730
[1mStep[0m  [70/106], [94mLoss[0m : 2.45446
[1mStep[0m  [80/106], [94mLoss[0m : 2.52054
[1mStep[0m  [90/106], [94mLoss[0m : 2.35752
[1mStep[0m  [100/106], [94mLoss[0m : 2.71406

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.441, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.52426
[1mStep[0m  [10/106], [94mLoss[0m : 2.55146
[1mStep[0m  [20/106], [94mLoss[0m : 2.24787
[1mStep[0m  [30/106], [94mLoss[0m : 2.51759
[1mStep[0m  [40/106], [94mLoss[0m : 2.61020
[1mStep[0m  [50/106], [94mLoss[0m : 2.28558
[1mStep[0m  [60/106], [94mLoss[0m : 2.45567
[1mStep[0m  [70/106], [94mLoss[0m : 2.11072
[1mStep[0m  [80/106], [94mLoss[0m : 2.26052
[1mStep[0m  [90/106], [94mLoss[0m : 2.80921
[1mStep[0m  [100/106], [94mLoss[0m : 2.72066

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.426, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.43141
[1mStep[0m  [10/106], [94mLoss[0m : 2.17007
[1mStep[0m  [20/106], [94mLoss[0m : 2.61575
[1mStep[0m  [30/106], [94mLoss[0m : 2.40550
[1mStep[0m  [40/106], [94mLoss[0m : 2.56363
[1mStep[0m  [50/106], [94mLoss[0m : 2.68892
[1mStep[0m  [60/106], [94mLoss[0m : 2.44743
[1mStep[0m  [70/106], [94mLoss[0m : 2.42498
[1mStep[0m  [80/106], [94mLoss[0m : 2.24284
[1mStep[0m  [90/106], [94mLoss[0m : 2.69277
[1mStep[0m  [100/106], [94mLoss[0m : 2.35316

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.404, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.32242
[1mStep[0m  [10/106], [94mLoss[0m : 2.55756
[1mStep[0m  [20/106], [94mLoss[0m : 2.41664
[1mStep[0m  [30/106], [94mLoss[0m : 2.20791
[1mStep[0m  [40/106], [94mLoss[0m : 2.76952
[1mStep[0m  [50/106], [94mLoss[0m : 2.39935
[1mStep[0m  [60/106], [94mLoss[0m : 2.33960
[1mStep[0m  [70/106], [94mLoss[0m : 2.51978
[1mStep[0m  [80/106], [94mLoss[0m : 2.30144
[1mStep[0m  [90/106], [94mLoss[0m : 2.49850
[1mStep[0m  [100/106], [94mLoss[0m : 2.58618

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.409, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.29060
[1mStep[0m  [10/106], [94mLoss[0m : 2.15350
[1mStep[0m  [20/106], [94mLoss[0m : 2.35341
[1mStep[0m  [30/106], [94mLoss[0m : 2.35573
[1mStep[0m  [40/106], [94mLoss[0m : 2.55037
[1mStep[0m  [50/106], [94mLoss[0m : 2.53352
[1mStep[0m  [60/106], [94mLoss[0m : 2.44895
[1mStep[0m  [70/106], [94mLoss[0m : 2.39399
[1mStep[0m  [80/106], [94mLoss[0m : 2.53271
[1mStep[0m  [90/106], [94mLoss[0m : 2.65724
[1mStep[0m  [100/106], [94mLoss[0m : 2.57475

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.387, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.27834
[1mStep[0m  [10/106], [94mLoss[0m : 2.90378
[1mStep[0m  [20/106], [94mLoss[0m : 2.44072
[1mStep[0m  [30/106], [94mLoss[0m : 2.61787
[1mStep[0m  [40/106], [94mLoss[0m : 2.39644
[1mStep[0m  [50/106], [94mLoss[0m : 2.63287
[1mStep[0m  [60/106], [94mLoss[0m : 2.50974
[1mStep[0m  [70/106], [94mLoss[0m : 2.56078
[1mStep[0m  [80/106], [94mLoss[0m : 2.43031
[1mStep[0m  [90/106], [94mLoss[0m : 2.55820
[1mStep[0m  [100/106], [94mLoss[0m : 2.49016

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.390, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.30938
[1mStep[0m  [10/106], [94mLoss[0m : 3.04269
[1mStep[0m  [20/106], [94mLoss[0m : 2.35598
[1mStep[0m  [30/106], [94mLoss[0m : 2.49046
[1mStep[0m  [40/106], [94mLoss[0m : 2.09317
[1mStep[0m  [50/106], [94mLoss[0m : 2.56660
[1mStep[0m  [60/106], [94mLoss[0m : 2.53315
[1mStep[0m  [70/106], [94mLoss[0m : 2.25806
[1mStep[0m  [80/106], [94mLoss[0m : 2.58520
[1mStep[0m  [90/106], [94mLoss[0m : 2.64326
[1mStep[0m  [100/106], [94mLoss[0m : 2.58115

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.396, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.35919
[1mStep[0m  [10/106], [94mLoss[0m : 2.47893
[1mStep[0m  [20/106], [94mLoss[0m : 2.28879
[1mStep[0m  [30/106], [94mLoss[0m : 2.75049
[1mStep[0m  [40/106], [94mLoss[0m : 2.50911
[1mStep[0m  [50/106], [94mLoss[0m : 2.37050
[1mStep[0m  [60/106], [94mLoss[0m : 2.37350
[1mStep[0m  [70/106], [94mLoss[0m : 2.15259
[1mStep[0m  [80/106], [94mLoss[0m : 2.03098
[1mStep[0m  [90/106], [94mLoss[0m : 2.30950
[1mStep[0m  [100/106], [94mLoss[0m : 2.66483

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.396, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.69795
[1mStep[0m  [10/106], [94mLoss[0m : 2.47570
[1mStep[0m  [20/106], [94mLoss[0m : 2.38774
[1mStep[0m  [30/106], [94mLoss[0m : 2.20280
[1mStep[0m  [40/106], [94mLoss[0m : 2.56326
[1mStep[0m  [50/106], [94mLoss[0m : 2.81502
[1mStep[0m  [60/106], [94mLoss[0m : 2.72158
[1mStep[0m  [70/106], [94mLoss[0m : 2.43470
[1mStep[0m  [80/106], [94mLoss[0m : 2.47470
[1mStep[0m  [90/106], [94mLoss[0m : 2.34638
[1mStep[0m  [100/106], [94mLoss[0m : 2.26415

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.392, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.29119
[1mStep[0m  [10/106], [94mLoss[0m : 2.29916
[1mStep[0m  [20/106], [94mLoss[0m : 2.20343
[1mStep[0m  [30/106], [94mLoss[0m : 2.34470
[1mStep[0m  [40/106], [94mLoss[0m : 2.17163
[1mStep[0m  [50/106], [94mLoss[0m : 2.26025
[1mStep[0m  [60/106], [94mLoss[0m : 2.48630
[1mStep[0m  [70/106], [94mLoss[0m : 2.36275
[1mStep[0m  [80/106], [94mLoss[0m : 2.76704
[1mStep[0m  [90/106], [94mLoss[0m : 2.12962
[1mStep[0m  [100/106], [94mLoss[0m : 2.44037

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.386, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.44831
[1mStep[0m  [10/106], [94mLoss[0m : 2.36602
[1mStep[0m  [20/106], [94mLoss[0m : 2.46813
[1mStep[0m  [30/106], [94mLoss[0m : 2.33412
[1mStep[0m  [40/106], [94mLoss[0m : 2.50453
[1mStep[0m  [50/106], [94mLoss[0m : 2.45094
[1mStep[0m  [60/106], [94mLoss[0m : 2.38224
[1mStep[0m  [70/106], [94mLoss[0m : 2.10891
[1mStep[0m  [80/106], [94mLoss[0m : 2.40508
[1mStep[0m  [90/106], [94mLoss[0m : 2.38985
[1mStep[0m  [100/106], [94mLoss[0m : 2.50456

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.397, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.02225
[1mStep[0m  [10/106], [94mLoss[0m : 2.52462
[1mStep[0m  [20/106], [94mLoss[0m : 2.50897
[1mStep[0m  [30/106], [94mLoss[0m : 2.47392
[1mStep[0m  [40/106], [94mLoss[0m : 2.69177
[1mStep[0m  [50/106], [94mLoss[0m : 2.23430
[1mStep[0m  [60/106], [94mLoss[0m : 2.73483
[1mStep[0m  [70/106], [94mLoss[0m : 2.21625
[1mStep[0m  [80/106], [94mLoss[0m : 2.68941
[1mStep[0m  [90/106], [94mLoss[0m : 2.43159
[1mStep[0m  [100/106], [94mLoss[0m : 2.36944

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.381, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.71087
[1mStep[0m  [10/106], [94mLoss[0m : 2.56905
[1mStep[0m  [20/106], [94mLoss[0m : 2.50750
[1mStep[0m  [30/106], [94mLoss[0m : 2.60910
[1mStep[0m  [40/106], [94mLoss[0m : 2.27162
[1mStep[0m  [50/106], [94mLoss[0m : 2.52739
[1mStep[0m  [60/106], [94mLoss[0m : 2.35966
[1mStep[0m  [70/106], [94mLoss[0m : 2.73953
[1mStep[0m  [80/106], [94mLoss[0m : 2.32510
[1mStep[0m  [90/106], [94mLoss[0m : 2.23471
[1mStep[0m  [100/106], [94mLoss[0m : 2.70770

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.398, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.54436
[1mStep[0m  [10/106], [94mLoss[0m : 2.62309
[1mStep[0m  [20/106], [94mLoss[0m : 2.44939
[1mStep[0m  [30/106], [94mLoss[0m : 2.53158
[1mStep[0m  [40/106], [94mLoss[0m : 2.27971
[1mStep[0m  [50/106], [94mLoss[0m : 2.42499
[1mStep[0m  [60/106], [94mLoss[0m : 2.61900
[1mStep[0m  [70/106], [94mLoss[0m : 2.69683
[1mStep[0m  [80/106], [94mLoss[0m : 2.39159
[1mStep[0m  [90/106], [94mLoss[0m : 2.04075
[1mStep[0m  [100/106], [94mLoss[0m : 2.23367

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.396, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.39143
[1mStep[0m  [10/106], [94mLoss[0m : 2.22858
[1mStep[0m  [20/106], [94mLoss[0m : 2.34568
[1mStep[0m  [30/106], [94mLoss[0m : 2.20575
[1mStep[0m  [40/106], [94mLoss[0m : 2.51403
[1mStep[0m  [50/106], [94mLoss[0m : 2.34599
[1mStep[0m  [60/106], [94mLoss[0m : 2.56304
[1mStep[0m  [70/106], [94mLoss[0m : 2.02903
[1mStep[0m  [80/106], [94mLoss[0m : 2.51637
[1mStep[0m  [90/106], [94mLoss[0m : 2.41491
[1mStep[0m  [100/106], [94mLoss[0m : 2.25888

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.388, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.17399
[1mStep[0m  [10/106], [94mLoss[0m : 2.63732
[1mStep[0m  [20/106], [94mLoss[0m : 2.46893
[1mStep[0m  [30/106], [94mLoss[0m : 2.32855
[1mStep[0m  [40/106], [94mLoss[0m : 2.76849
[1mStep[0m  [50/106], [94mLoss[0m : 2.56171
[1mStep[0m  [60/106], [94mLoss[0m : 2.32122
[1mStep[0m  [70/106], [94mLoss[0m : 2.23411
[1mStep[0m  [80/106], [94mLoss[0m : 2.40570
[1mStep[0m  [90/106], [94mLoss[0m : 2.74825
[1mStep[0m  [100/106], [94mLoss[0m : 2.13249

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.387, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.19114
[1mStep[0m  [10/106], [94mLoss[0m : 2.27143
[1mStep[0m  [20/106], [94mLoss[0m : 2.41210
[1mStep[0m  [30/106], [94mLoss[0m : 2.61038
[1mStep[0m  [40/106], [94mLoss[0m : 2.31077
[1mStep[0m  [50/106], [94mLoss[0m : 2.19403
[1mStep[0m  [60/106], [94mLoss[0m : 2.51805
[1mStep[0m  [70/106], [94mLoss[0m : 2.51401
[1mStep[0m  [80/106], [94mLoss[0m : 2.02087
[1mStep[0m  [90/106], [94mLoss[0m : 2.66031
[1mStep[0m  [100/106], [94mLoss[0m : 2.54514

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.391, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.54686
[1mStep[0m  [10/106], [94mLoss[0m : 2.46560
[1mStep[0m  [20/106], [94mLoss[0m : 2.52845
[1mStep[0m  [30/106], [94mLoss[0m : 2.51337
[1mStep[0m  [40/106], [94mLoss[0m : 2.28679
[1mStep[0m  [50/106], [94mLoss[0m : 2.55258
[1mStep[0m  [60/106], [94mLoss[0m : 2.25297
[1mStep[0m  [70/106], [94mLoss[0m : 2.20894
[1mStep[0m  [80/106], [94mLoss[0m : 2.47915
[1mStep[0m  [90/106], [94mLoss[0m : 2.53964
[1mStep[0m  [100/106], [94mLoss[0m : 2.81332

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.391, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.36128
[1mStep[0m  [10/106], [94mLoss[0m : 2.65031
[1mStep[0m  [20/106], [94mLoss[0m : 2.34301
[1mStep[0m  [30/106], [94mLoss[0m : 2.57420
[1mStep[0m  [40/106], [94mLoss[0m : 2.48630
[1mStep[0m  [50/106], [94mLoss[0m : 2.45621
[1mStep[0m  [60/106], [94mLoss[0m : 2.07948
[1mStep[0m  [70/106], [94mLoss[0m : 2.98111
[1mStep[0m  [80/106], [94mLoss[0m : 2.29227
[1mStep[0m  [90/106], [94mLoss[0m : 2.34016
[1mStep[0m  [100/106], [94mLoss[0m : 2.58146

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.406, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.37101
[1mStep[0m  [10/106], [94mLoss[0m : 2.58388
[1mStep[0m  [20/106], [94mLoss[0m : 2.38568
[1mStep[0m  [30/106], [94mLoss[0m : 2.38327
[1mStep[0m  [40/106], [94mLoss[0m : 2.61516
[1mStep[0m  [50/106], [94mLoss[0m : 2.25800
[1mStep[0m  [60/106], [94mLoss[0m : 2.28480
[1mStep[0m  [70/106], [94mLoss[0m : 2.46966
[1mStep[0m  [80/106], [94mLoss[0m : 2.50967
[1mStep[0m  [90/106], [94mLoss[0m : 2.34816
[1mStep[0m  [100/106], [94mLoss[0m : 2.22990

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.386, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.53527
[1mStep[0m  [10/106], [94mLoss[0m : 2.19110
[1mStep[0m  [20/106], [94mLoss[0m : 2.55684
[1mStep[0m  [30/106], [94mLoss[0m : 2.43790
[1mStep[0m  [40/106], [94mLoss[0m : 2.69951
[1mStep[0m  [50/106], [94mLoss[0m : 2.28364
[1mStep[0m  [60/106], [94mLoss[0m : 2.34710
[1mStep[0m  [70/106], [94mLoss[0m : 2.26209
[1mStep[0m  [80/106], [94mLoss[0m : 2.10777
[1mStep[0m  [90/106], [94mLoss[0m : 2.40486
[1mStep[0m  [100/106], [94mLoss[0m : 2.39857

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.408, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.53160
[1mStep[0m  [10/106], [94mLoss[0m : 2.20140
[1mStep[0m  [20/106], [94mLoss[0m : 2.29114
[1mStep[0m  [30/106], [94mLoss[0m : 2.51289
[1mStep[0m  [40/106], [94mLoss[0m : 2.54427
[1mStep[0m  [50/106], [94mLoss[0m : 2.30701
[1mStep[0m  [60/106], [94mLoss[0m : 2.19940
[1mStep[0m  [70/106], [94mLoss[0m : 2.88090
[1mStep[0m  [80/106], [94mLoss[0m : 2.41816
[1mStep[0m  [90/106], [94mLoss[0m : 2.25470
[1mStep[0m  [100/106], [94mLoss[0m : 2.77424

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.397, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.35238
[1mStep[0m  [10/106], [94mLoss[0m : 2.29594
[1mStep[0m  [20/106], [94mLoss[0m : 2.32494
[1mStep[0m  [30/106], [94mLoss[0m : 2.18665
[1mStep[0m  [40/106], [94mLoss[0m : 2.52680
[1mStep[0m  [50/106], [94mLoss[0m : 2.59637
[1mStep[0m  [60/106], [94mLoss[0m : 2.38123
[1mStep[0m  [70/106], [94mLoss[0m : 2.64902
[1mStep[0m  [80/106], [94mLoss[0m : 2.41305
[1mStep[0m  [90/106], [94mLoss[0m : 2.43400
[1mStep[0m  [100/106], [94mLoss[0m : 2.42702

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.394, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.71505
[1mStep[0m  [10/106], [94mLoss[0m : 2.31118
[1mStep[0m  [20/106], [94mLoss[0m : 2.48169
[1mStep[0m  [30/106], [94mLoss[0m : 2.33936
[1mStep[0m  [40/106], [94mLoss[0m : 2.31188
[1mStep[0m  [50/106], [94mLoss[0m : 2.36292
[1mStep[0m  [60/106], [94mLoss[0m : 2.32970
[1mStep[0m  [70/106], [94mLoss[0m : 2.71850
[1mStep[0m  [80/106], [94mLoss[0m : 2.64414
[1mStep[0m  [90/106], [94mLoss[0m : 2.59133
[1mStep[0m  [100/106], [94mLoss[0m : 2.51724

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.388, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.388
====================================

Phase 1 - Evaluation MAE:  2.3881824916263796
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/106], [94mLoss[0m : 2.43476
[1mStep[0m  [10/106], [94mLoss[0m : 2.03320
[1mStep[0m  [20/106], [94mLoss[0m : 2.69039
[1mStep[0m  [30/106], [94mLoss[0m : 2.40493
[1mStep[0m  [40/106], [94mLoss[0m : 2.50076
[1mStep[0m  [50/106], [94mLoss[0m : 2.45778
[1mStep[0m  [60/106], [94mLoss[0m : 2.55655
[1mStep[0m  [70/106], [94mLoss[0m : 2.27136
[1mStep[0m  [80/106], [94mLoss[0m : 2.24878
[1mStep[0m  [90/106], [94mLoss[0m : 2.42303
[1mStep[0m  [100/106], [94mLoss[0m : 2.71299

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.390, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.26959
[1mStep[0m  [10/106], [94mLoss[0m : 2.40522
[1mStep[0m  [20/106], [94mLoss[0m : 2.36978
[1mStep[0m  [30/106], [94mLoss[0m : 2.63944
[1mStep[0m  [40/106], [94mLoss[0m : 2.51085
[1mStep[0m  [50/106], [94mLoss[0m : 2.64426
[1mStep[0m  [60/106], [94mLoss[0m : 2.24593
[1mStep[0m  [70/106], [94mLoss[0m : 2.34925
[1mStep[0m  [80/106], [94mLoss[0m : 2.20814
[1mStep[0m  [90/106], [94mLoss[0m : 2.45182
[1mStep[0m  [100/106], [94mLoss[0m : 2.66236

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.336, [92mTest[0m: 2.393, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.02643
[1mStep[0m  [10/106], [94mLoss[0m : 2.06263
[1mStep[0m  [20/106], [94mLoss[0m : 2.36616
[1mStep[0m  [30/106], [94mLoss[0m : 2.25650
[1mStep[0m  [40/106], [94mLoss[0m : 2.16133
[1mStep[0m  [50/106], [94mLoss[0m : 1.90139
[1mStep[0m  [60/106], [94mLoss[0m : 2.08046
[1mStep[0m  [70/106], [94mLoss[0m : 1.92649
[1mStep[0m  [80/106], [94mLoss[0m : 2.52093
[1mStep[0m  [90/106], [94mLoss[0m : 2.12562
[1mStep[0m  [100/106], [94mLoss[0m : 2.40632

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.255, [92mTest[0m: 2.378, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.01766
[1mStep[0m  [10/106], [94mLoss[0m : 2.19108
[1mStep[0m  [20/106], [94mLoss[0m : 2.29501
[1mStep[0m  [30/106], [94mLoss[0m : 2.14585
[1mStep[0m  [40/106], [94mLoss[0m : 2.28048
[1mStep[0m  [50/106], [94mLoss[0m : 2.23279
[1mStep[0m  [60/106], [94mLoss[0m : 2.15213
[1mStep[0m  [70/106], [94mLoss[0m : 2.38015
[1mStep[0m  [80/106], [94mLoss[0m : 2.26523
[1mStep[0m  [90/106], [94mLoss[0m : 2.29038
[1mStep[0m  [100/106], [94mLoss[0m : 2.13035

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.181, [92mTest[0m: 2.380, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.98803
[1mStep[0m  [10/106], [94mLoss[0m : 1.97792
[1mStep[0m  [20/106], [94mLoss[0m : 2.08500
[1mStep[0m  [30/106], [94mLoss[0m : 2.14174
[1mStep[0m  [40/106], [94mLoss[0m : 2.03138
[1mStep[0m  [50/106], [94mLoss[0m : 1.80173
[1mStep[0m  [60/106], [94mLoss[0m : 2.46508
[1mStep[0m  [70/106], [94mLoss[0m : 2.13223
[1mStep[0m  [80/106], [94mLoss[0m : 2.16282
[1mStep[0m  [90/106], [94mLoss[0m : 2.15736
[1mStep[0m  [100/106], [94mLoss[0m : 2.43051

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.115, [92mTest[0m: 2.394, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.06520
[1mStep[0m  [10/106], [94mLoss[0m : 2.05187
[1mStep[0m  [20/106], [94mLoss[0m : 2.33920
[1mStep[0m  [30/106], [94mLoss[0m : 2.18558
[1mStep[0m  [40/106], [94mLoss[0m : 2.17695
[1mStep[0m  [50/106], [94mLoss[0m : 1.91010
[1mStep[0m  [60/106], [94mLoss[0m : 2.10703
[1mStep[0m  [70/106], [94mLoss[0m : 2.05037
[1mStep[0m  [80/106], [94mLoss[0m : 2.00405
[1mStep[0m  [90/106], [94mLoss[0m : 1.98065
[1mStep[0m  [100/106], [94mLoss[0m : 2.06137

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.067, [92mTest[0m: 2.385, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.90125
[1mStep[0m  [10/106], [94mLoss[0m : 1.93236
[1mStep[0m  [20/106], [94mLoss[0m : 2.17638
[1mStep[0m  [30/106], [94mLoss[0m : 2.18084
[1mStep[0m  [40/106], [94mLoss[0m : 2.08196
[1mStep[0m  [50/106], [94mLoss[0m : 1.88804
[1mStep[0m  [60/106], [94mLoss[0m : 2.03553
[1mStep[0m  [70/106], [94mLoss[0m : 2.00409
[1mStep[0m  [80/106], [94mLoss[0m : 2.15253
[1mStep[0m  [90/106], [94mLoss[0m : 1.90237
[1mStep[0m  [100/106], [94mLoss[0m : 1.98325

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.021, [92mTest[0m: 2.433, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.06614
[1mStep[0m  [10/106], [94mLoss[0m : 1.71741
[1mStep[0m  [20/106], [94mLoss[0m : 1.95162
[1mStep[0m  [30/106], [94mLoss[0m : 1.84880
[1mStep[0m  [40/106], [94mLoss[0m : 1.91685
[1mStep[0m  [50/106], [94mLoss[0m : 1.75846
[1mStep[0m  [60/106], [94mLoss[0m : 1.92676
[1mStep[0m  [70/106], [94mLoss[0m : 1.93503
[1mStep[0m  [80/106], [94mLoss[0m : 1.89061
[1mStep[0m  [90/106], [94mLoss[0m : 2.26997
[1mStep[0m  [100/106], [94mLoss[0m : 2.11236

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.986, [92mTest[0m: 2.500, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.89336
[1mStep[0m  [10/106], [94mLoss[0m : 2.06913
[1mStep[0m  [20/106], [94mLoss[0m : 2.26484
[1mStep[0m  [30/106], [94mLoss[0m : 1.84724
[1mStep[0m  [40/106], [94mLoss[0m : 1.88240
[1mStep[0m  [50/106], [94mLoss[0m : 2.34803
[1mStep[0m  [60/106], [94mLoss[0m : 1.79738
[1mStep[0m  [70/106], [94mLoss[0m : 1.79576
[1mStep[0m  [80/106], [94mLoss[0m : 1.87045
[1mStep[0m  [90/106], [94mLoss[0m : 2.08028
[1mStep[0m  [100/106], [94mLoss[0m : 1.99630

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.949, [92mTest[0m: 2.407, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.75230
[1mStep[0m  [10/106], [94mLoss[0m : 1.96186
[1mStep[0m  [20/106], [94mLoss[0m : 1.82009
[1mStep[0m  [30/106], [94mLoss[0m : 1.48028
[1mStep[0m  [40/106], [94mLoss[0m : 1.84656
[1mStep[0m  [50/106], [94mLoss[0m : 1.85609
[1mStep[0m  [60/106], [94mLoss[0m : 1.86424
[1mStep[0m  [70/106], [94mLoss[0m : 1.76710
[1mStep[0m  [80/106], [94mLoss[0m : 1.82543
[1mStep[0m  [90/106], [94mLoss[0m : 1.81054
[1mStep[0m  [100/106], [94mLoss[0m : 1.98446

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.918, [92mTest[0m: 2.422, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.91217
[1mStep[0m  [10/106], [94mLoss[0m : 1.77625
[1mStep[0m  [20/106], [94mLoss[0m : 1.79525
[1mStep[0m  [30/106], [94mLoss[0m : 1.85350
[1mStep[0m  [40/106], [94mLoss[0m : 2.16095
[1mStep[0m  [50/106], [94mLoss[0m : 2.12133
[1mStep[0m  [60/106], [94mLoss[0m : 1.95177
[1mStep[0m  [70/106], [94mLoss[0m : 1.94459
[1mStep[0m  [80/106], [94mLoss[0m : 1.89241
[1mStep[0m  [90/106], [94mLoss[0m : 1.90490
[1mStep[0m  [100/106], [94mLoss[0m : 2.12662

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.907, [92mTest[0m: 2.462, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.80778
[1mStep[0m  [10/106], [94mLoss[0m : 1.91674
[1mStep[0m  [20/106], [94mLoss[0m : 1.53069
[1mStep[0m  [30/106], [94mLoss[0m : 1.87814
[1mStep[0m  [40/106], [94mLoss[0m : 1.63508
[1mStep[0m  [50/106], [94mLoss[0m : 2.00559
[1mStep[0m  [60/106], [94mLoss[0m : 2.06403
[1mStep[0m  [70/106], [94mLoss[0m : 2.13298
[1mStep[0m  [80/106], [94mLoss[0m : 1.86900
[1mStep[0m  [90/106], [94mLoss[0m : 2.02975
[1mStep[0m  [100/106], [94mLoss[0m : 1.61869

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.863, [92mTest[0m: 2.466, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.62284
[1mStep[0m  [10/106], [94mLoss[0m : 1.82439
[1mStep[0m  [20/106], [94mLoss[0m : 1.62381
[1mStep[0m  [30/106], [94mLoss[0m : 1.74245
[1mStep[0m  [40/106], [94mLoss[0m : 1.79131
[1mStep[0m  [50/106], [94mLoss[0m : 1.76458
[1mStep[0m  [60/106], [94mLoss[0m : 1.82373
[1mStep[0m  [70/106], [94mLoss[0m : 2.01761
[1mStep[0m  [80/106], [94mLoss[0m : 2.14864
[1mStep[0m  [90/106], [94mLoss[0m : 1.76349
[1mStep[0m  [100/106], [94mLoss[0m : 1.84158

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.859, [92mTest[0m: 2.477, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.79830
[1mStep[0m  [10/106], [94mLoss[0m : 1.47594
[1mStep[0m  [20/106], [94mLoss[0m : 1.63374
[1mStep[0m  [30/106], [94mLoss[0m : 1.68447
[1mStep[0m  [40/106], [94mLoss[0m : 1.99253
[1mStep[0m  [50/106], [94mLoss[0m : 1.62725
[1mStep[0m  [60/106], [94mLoss[0m : 1.75567
[1mStep[0m  [70/106], [94mLoss[0m : 2.08588
[1mStep[0m  [80/106], [94mLoss[0m : 1.89136
[1mStep[0m  [90/106], [94mLoss[0m : 2.06420
[1mStep[0m  [100/106], [94mLoss[0m : 2.16389

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.817, [92mTest[0m: 2.488, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.78391
[1mStep[0m  [10/106], [94mLoss[0m : 1.80608
[1mStep[0m  [20/106], [94mLoss[0m : 1.59167
[1mStep[0m  [30/106], [94mLoss[0m : 1.75266
[1mStep[0m  [40/106], [94mLoss[0m : 1.85340
[1mStep[0m  [50/106], [94mLoss[0m : 1.54726
[1mStep[0m  [60/106], [94mLoss[0m : 1.96352
[1mStep[0m  [70/106], [94mLoss[0m : 1.81104
[1mStep[0m  [80/106], [94mLoss[0m : 1.99994
[1mStep[0m  [90/106], [94mLoss[0m : 1.94858
[1mStep[0m  [100/106], [94mLoss[0m : 1.88057

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.800, [92mTest[0m: 2.436, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.94155
[1mStep[0m  [10/106], [94mLoss[0m : 1.45649
[1mStep[0m  [20/106], [94mLoss[0m : 1.86879
[1mStep[0m  [30/106], [94mLoss[0m : 1.43458
[1mStep[0m  [40/106], [94mLoss[0m : 2.07530
[1mStep[0m  [50/106], [94mLoss[0m : 1.85913
[1mStep[0m  [60/106], [94mLoss[0m : 1.54698
[1mStep[0m  [70/106], [94mLoss[0m : 1.59030
[1mStep[0m  [80/106], [94mLoss[0m : 1.74800
[1mStep[0m  [90/106], [94mLoss[0m : 1.87047
[1mStep[0m  [100/106], [94mLoss[0m : 1.81889

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.784, [92mTest[0m: 2.469, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.63139
[1mStep[0m  [10/106], [94mLoss[0m : 1.77599
[1mStep[0m  [20/106], [94mLoss[0m : 1.45581
[1mStep[0m  [30/106], [94mLoss[0m : 1.73554
[1mStep[0m  [40/106], [94mLoss[0m : 1.59525
[1mStep[0m  [50/106], [94mLoss[0m : 1.78963
[1mStep[0m  [60/106], [94mLoss[0m : 1.57229
[1mStep[0m  [70/106], [94mLoss[0m : 1.60061
[1mStep[0m  [80/106], [94mLoss[0m : 1.54812
[1mStep[0m  [90/106], [94mLoss[0m : 1.57675
[1mStep[0m  [100/106], [94mLoss[0m : 1.62756

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.754, [92mTest[0m: 2.494, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.76751
[1mStep[0m  [10/106], [94mLoss[0m : 1.67370
[1mStep[0m  [20/106], [94mLoss[0m : 1.55745
[1mStep[0m  [30/106], [94mLoss[0m : 1.74270
[1mStep[0m  [40/106], [94mLoss[0m : 1.88716
[1mStep[0m  [50/106], [94mLoss[0m : 1.84348
[1mStep[0m  [60/106], [94mLoss[0m : 1.69149
[1mStep[0m  [70/106], [94mLoss[0m : 1.63652
[1mStep[0m  [80/106], [94mLoss[0m : 1.85114
[1mStep[0m  [90/106], [94mLoss[0m : 1.98436
[1mStep[0m  [100/106], [94mLoss[0m : 1.69297

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.751, [92mTest[0m: 2.475, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.60165
[1mStep[0m  [10/106], [94mLoss[0m : 1.59179
[1mStep[0m  [20/106], [94mLoss[0m : 1.63389
[1mStep[0m  [30/106], [94mLoss[0m : 1.59671
[1mStep[0m  [40/106], [94mLoss[0m : 1.74576
[1mStep[0m  [50/106], [94mLoss[0m : 1.59073
[1mStep[0m  [60/106], [94mLoss[0m : 1.61787
[1mStep[0m  [70/106], [94mLoss[0m : 1.75078
[1mStep[0m  [80/106], [94mLoss[0m : 1.78176
[1mStep[0m  [90/106], [94mLoss[0m : 1.95630
[1mStep[0m  [100/106], [94mLoss[0m : 1.74967

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.738, [92mTest[0m: 2.454, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.56820
[1mStep[0m  [10/106], [94mLoss[0m : 1.80007
[1mStep[0m  [20/106], [94mLoss[0m : 1.40360
[1mStep[0m  [30/106], [94mLoss[0m : 1.83937
[1mStep[0m  [40/106], [94mLoss[0m : 1.80901
[1mStep[0m  [50/106], [94mLoss[0m : 1.83208
[1mStep[0m  [60/106], [94mLoss[0m : 1.99999
[1mStep[0m  [70/106], [94mLoss[0m : 2.00115
[1mStep[0m  [80/106], [94mLoss[0m : 1.88753
[1mStep[0m  [90/106], [94mLoss[0m : 1.76196
[1mStep[0m  [100/106], [94mLoss[0m : 1.68399

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.731, [92mTest[0m: 2.564, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.49146
[1mStep[0m  [10/106], [94mLoss[0m : 1.42894
[1mStep[0m  [20/106], [94mLoss[0m : 1.36420
[1mStep[0m  [30/106], [94mLoss[0m : 1.83062
[1mStep[0m  [40/106], [94mLoss[0m : 1.86554
[1mStep[0m  [50/106], [94mLoss[0m : 1.93976
[1mStep[0m  [60/106], [94mLoss[0m : 1.84498
[1mStep[0m  [70/106], [94mLoss[0m : 1.85005
[1mStep[0m  [80/106], [94mLoss[0m : 1.60065
[1mStep[0m  [90/106], [94mLoss[0m : 1.65915
[1mStep[0m  [100/106], [94mLoss[0m : 1.85118

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.682, [92mTest[0m: 2.549, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.44499
[1mStep[0m  [10/106], [94mLoss[0m : 1.57757
[1mStep[0m  [20/106], [94mLoss[0m : 1.44442
[1mStep[0m  [30/106], [94mLoss[0m : 1.58136
[1mStep[0m  [40/106], [94mLoss[0m : 1.53230
[1mStep[0m  [50/106], [94mLoss[0m : 1.72212
[1mStep[0m  [60/106], [94mLoss[0m : 1.29530
[1mStep[0m  [70/106], [94mLoss[0m : 1.62157
[1mStep[0m  [80/106], [94mLoss[0m : 1.55091
[1mStep[0m  [90/106], [94mLoss[0m : 1.57545
[1mStep[0m  [100/106], [94mLoss[0m : 1.62579

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.650, [92mTest[0m: 2.469, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.57683
[1mStep[0m  [10/106], [94mLoss[0m : 1.61889
[1mStep[0m  [20/106], [94mLoss[0m : 1.64679
[1mStep[0m  [30/106], [94mLoss[0m : 1.43056
[1mStep[0m  [40/106], [94mLoss[0m : 1.69971
[1mStep[0m  [50/106], [94mLoss[0m : 1.74193
[1mStep[0m  [60/106], [94mLoss[0m : 1.66987
[1mStep[0m  [70/106], [94mLoss[0m : 1.75389
[1mStep[0m  [80/106], [94mLoss[0m : 1.58785
[1mStep[0m  [90/106], [94mLoss[0m : 1.46099
[1mStep[0m  [100/106], [94mLoss[0m : 1.63034

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.621, [92mTest[0m: 2.460, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.61750
[1mStep[0m  [10/106], [94mLoss[0m : 1.53678
[1mStep[0m  [20/106], [94mLoss[0m : 1.42165
[1mStep[0m  [30/106], [94mLoss[0m : 1.64885
[1mStep[0m  [40/106], [94mLoss[0m : 1.42571
[1mStep[0m  [50/106], [94mLoss[0m : 1.87860
[1mStep[0m  [60/106], [94mLoss[0m : 1.53793
[1mStep[0m  [70/106], [94mLoss[0m : 1.74328
[1mStep[0m  [80/106], [94mLoss[0m : 1.59403
[1mStep[0m  [90/106], [94mLoss[0m : 1.80416
[1mStep[0m  [100/106], [94mLoss[0m : 1.71547

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.634, [92mTest[0m: 2.535, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.73125
[1mStep[0m  [10/106], [94mLoss[0m : 1.60443
[1mStep[0m  [20/106], [94mLoss[0m : 1.44306
[1mStep[0m  [30/106], [94mLoss[0m : 1.53249
[1mStep[0m  [40/106], [94mLoss[0m : 1.66458
[1mStep[0m  [50/106], [94mLoss[0m : 1.51840
[1mStep[0m  [60/106], [94mLoss[0m : 1.56101
[1mStep[0m  [70/106], [94mLoss[0m : 1.47509
[1mStep[0m  [80/106], [94mLoss[0m : 1.44033
[1mStep[0m  [90/106], [94mLoss[0m : 1.63308
[1mStep[0m  [100/106], [94mLoss[0m : 1.62462

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.612, [92mTest[0m: 2.479, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.55741
[1mStep[0m  [10/106], [94mLoss[0m : 1.35280
[1mStep[0m  [20/106], [94mLoss[0m : 1.56378
[1mStep[0m  [30/106], [94mLoss[0m : 1.62450
[1mStep[0m  [40/106], [94mLoss[0m : 1.88595
[1mStep[0m  [50/106], [94mLoss[0m : 1.56719
[1mStep[0m  [60/106], [94mLoss[0m : 1.63159
[1mStep[0m  [70/106], [94mLoss[0m : 1.54630
[1mStep[0m  [80/106], [94mLoss[0m : 1.64871
[1mStep[0m  [90/106], [94mLoss[0m : 1.84924
[1mStep[0m  [100/106], [94mLoss[0m : 1.74806

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.599, [92mTest[0m: 2.563, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.48639
[1mStep[0m  [10/106], [94mLoss[0m : 1.51479
[1mStep[0m  [20/106], [94mLoss[0m : 1.47020
[1mStep[0m  [30/106], [94mLoss[0m : 1.37541
[1mStep[0m  [40/106], [94mLoss[0m : 1.52062
[1mStep[0m  [50/106], [94mLoss[0m : 1.42796
[1mStep[0m  [60/106], [94mLoss[0m : 1.75251
[1mStep[0m  [70/106], [94mLoss[0m : 1.72668
[1mStep[0m  [80/106], [94mLoss[0m : 1.65795
[1mStep[0m  [90/106], [94mLoss[0m : 1.58951
[1mStep[0m  [100/106], [94mLoss[0m : 1.69012

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.618, [92mTest[0m: 2.509, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.40941
[1mStep[0m  [10/106], [94mLoss[0m : 1.53307
[1mStep[0m  [20/106], [94mLoss[0m : 1.43134
[1mStep[0m  [30/106], [94mLoss[0m : 1.56227
[1mStep[0m  [40/106], [94mLoss[0m : 1.43644
[1mStep[0m  [50/106], [94mLoss[0m : 1.60686
[1mStep[0m  [60/106], [94mLoss[0m : 1.72316
[1mStep[0m  [70/106], [94mLoss[0m : 1.46421
[1mStep[0m  [80/106], [94mLoss[0m : 1.75937
[1mStep[0m  [90/106], [94mLoss[0m : 1.94342
[1mStep[0m  [100/106], [94mLoss[0m : 1.68384

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.568, [92mTest[0m: 2.530, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.59972
[1mStep[0m  [10/106], [94mLoss[0m : 1.37901
[1mStep[0m  [20/106], [94mLoss[0m : 1.59382
[1mStep[0m  [30/106], [94mLoss[0m : 1.64659
[1mStep[0m  [40/106], [94mLoss[0m : 1.38258
[1mStep[0m  [50/106], [94mLoss[0m : 1.73598
[1mStep[0m  [60/106], [94mLoss[0m : 1.55731
[1mStep[0m  [70/106], [94mLoss[0m : 1.71984
[1mStep[0m  [80/106], [94mLoss[0m : 1.43871
[1mStep[0m  [90/106], [94mLoss[0m : 1.67336
[1mStep[0m  [100/106], [94mLoss[0m : 1.38046

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.567, [92mTest[0m: 2.559, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.49619
[1mStep[0m  [10/106], [94mLoss[0m : 1.56042
[1mStep[0m  [20/106], [94mLoss[0m : 1.63585
[1mStep[0m  [30/106], [94mLoss[0m : 1.47998
[1mStep[0m  [40/106], [94mLoss[0m : 1.59475
[1mStep[0m  [50/106], [94mLoss[0m : 1.67571
[1mStep[0m  [60/106], [94mLoss[0m : 1.56043
[1mStep[0m  [70/106], [94mLoss[0m : 1.85271
[1mStep[0m  [80/106], [94mLoss[0m : 1.60200
[1mStep[0m  [90/106], [94mLoss[0m : 1.70922
[1mStep[0m  [100/106], [94mLoss[0m : 1.60919

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.568, [92mTest[0m: 2.520, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.505
====================================

Phase 2 - Evaluation MAE:  2.5048906803131104
MAE score P1      2.388182
MAE score P2      2.504891
loss              1.566523
learning_rate      0.00505
batch_size             128
hidden_sizes         [300]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.9
weight_decay          0.01
Name: 3, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/213], [94mLoss[0m : 11.51312
[1mStep[0m  [21/213], [94mLoss[0m : 9.06810
[1mStep[0m  [42/213], [94mLoss[0m : 9.41155
[1mStep[0m  [63/213], [94mLoss[0m : 7.02336
[1mStep[0m  [84/213], [94mLoss[0m : 5.50144
[1mStep[0m  [105/213], [94mLoss[0m : 3.56923
[1mStep[0m  [126/213], [94mLoss[0m : 3.00471
[1mStep[0m  [147/213], [94mLoss[0m : 3.61841
[1mStep[0m  [168/213], [94mLoss[0m : 2.48430
[1mStep[0m  [189/213], [94mLoss[0m : 2.57423
[1mStep[0m  [210/213], [94mLoss[0m : 2.76403

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 5.222, [92mTest[0m: 11.241, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.85015
[1mStep[0m  [21/213], [94mLoss[0m : 3.10304
[1mStep[0m  [42/213], [94mLoss[0m : 2.36535
[1mStep[0m  [63/213], [94mLoss[0m : 3.02697
[1mStep[0m  [84/213], [94mLoss[0m : 2.63907
[1mStep[0m  [105/213], [94mLoss[0m : 2.80511
[1mStep[0m  [126/213], [94mLoss[0m : 2.60234
[1mStep[0m  [147/213], [94mLoss[0m : 2.78181
[1mStep[0m  [168/213], [94mLoss[0m : 2.44313
[1mStep[0m  [189/213], [94mLoss[0m : 2.74034
[1mStep[0m  [210/213], [94mLoss[0m : 2.43248

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.644, [92mTest[0m: 2.524, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.60663
[1mStep[0m  [21/213], [94mLoss[0m : 3.03115
[1mStep[0m  [42/213], [94mLoss[0m : 2.62110
[1mStep[0m  [63/213], [94mLoss[0m : 2.46604
[1mStep[0m  [84/213], [94mLoss[0m : 2.31623
[1mStep[0m  [105/213], [94mLoss[0m : 2.65107
[1mStep[0m  [126/213], [94mLoss[0m : 2.53085
[1mStep[0m  [147/213], [94mLoss[0m : 2.44886
[1mStep[0m  [168/213], [94mLoss[0m : 2.74681
[1mStep[0m  [189/213], [94mLoss[0m : 2.12350
[1mStep[0m  [210/213], [94mLoss[0m : 2.40835

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.581, [92mTest[0m: 2.468, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.74961
[1mStep[0m  [21/213], [94mLoss[0m : 2.62987
[1mStep[0m  [42/213], [94mLoss[0m : 3.06326
[1mStep[0m  [63/213], [94mLoss[0m : 2.21270
[1mStep[0m  [84/213], [94mLoss[0m : 2.43902
[1mStep[0m  [105/213], [94mLoss[0m : 2.40624
[1mStep[0m  [126/213], [94mLoss[0m : 2.80279
[1mStep[0m  [147/213], [94mLoss[0m : 2.59666
[1mStep[0m  [168/213], [94mLoss[0m : 2.29430
[1mStep[0m  [189/213], [94mLoss[0m : 2.31630
[1mStep[0m  [210/213], [94mLoss[0m : 2.24463

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.575, [92mTest[0m: 2.440, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.76793
[1mStep[0m  [21/213], [94mLoss[0m : 2.35369
[1mStep[0m  [42/213], [94mLoss[0m : 2.44217
[1mStep[0m  [63/213], [94mLoss[0m : 3.07311
[1mStep[0m  [84/213], [94mLoss[0m : 2.62464
[1mStep[0m  [105/213], [94mLoss[0m : 2.79865
[1mStep[0m  [126/213], [94mLoss[0m : 2.46992
[1mStep[0m  [147/213], [94mLoss[0m : 2.67053
[1mStep[0m  [168/213], [94mLoss[0m : 2.53784
[1mStep[0m  [189/213], [94mLoss[0m : 2.74492
[1mStep[0m  [210/213], [94mLoss[0m : 2.93361

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.583, [92mTest[0m: 2.451, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.13922
[1mStep[0m  [21/213], [94mLoss[0m : 2.70184
[1mStep[0m  [42/213], [94mLoss[0m : 2.91638
[1mStep[0m  [63/213], [94mLoss[0m : 2.84215
[1mStep[0m  [84/213], [94mLoss[0m : 2.70999
[1mStep[0m  [105/213], [94mLoss[0m : 2.85084
[1mStep[0m  [126/213], [94mLoss[0m : 2.86074
[1mStep[0m  [147/213], [94mLoss[0m : 2.68839
[1mStep[0m  [168/213], [94mLoss[0m : 2.61303
[1mStep[0m  [189/213], [94mLoss[0m : 2.25018
[1mStep[0m  [210/213], [94mLoss[0m : 2.50181

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.549, [92mTest[0m: 2.429, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.59742
[1mStep[0m  [21/213], [94mLoss[0m : 2.67083
[1mStep[0m  [42/213], [94mLoss[0m : 2.55163
[1mStep[0m  [63/213], [94mLoss[0m : 2.26121
[1mStep[0m  [84/213], [94mLoss[0m : 2.67165
[1mStep[0m  [105/213], [94mLoss[0m : 2.45688
[1mStep[0m  [126/213], [94mLoss[0m : 2.64890
[1mStep[0m  [147/213], [94mLoss[0m : 2.15775
[1mStep[0m  [168/213], [94mLoss[0m : 2.17638
[1mStep[0m  [189/213], [94mLoss[0m : 2.59301
[1mStep[0m  [210/213], [94mLoss[0m : 2.68070

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.563, [92mTest[0m: 2.441, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.97828
[1mStep[0m  [21/213], [94mLoss[0m : 2.59270
[1mStep[0m  [42/213], [94mLoss[0m : 2.42410
[1mStep[0m  [63/213], [94mLoss[0m : 2.48634
[1mStep[0m  [84/213], [94mLoss[0m : 2.24913
[1mStep[0m  [105/213], [94mLoss[0m : 2.32978
[1mStep[0m  [126/213], [94mLoss[0m : 2.44953
[1mStep[0m  [147/213], [94mLoss[0m : 2.21356
[1mStep[0m  [168/213], [94mLoss[0m : 2.83487
[1mStep[0m  [189/213], [94mLoss[0m : 2.44318
[1mStep[0m  [210/213], [94mLoss[0m : 2.57583

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.555, [92mTest[0m: 2.420, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.55859
[1mStep[0m  [21/213], [94mLoss[0m : 2.59944
[1mStep[0m  [42/213], [94mLoss[0m : 2.92666
[1mStep[0m  [63/213], [94mLoss[0m : 2.32544
[1mStep[0m  [84/213], [94mLoss[0m : 2.73721
[1mStep[0m  [105/213], [94mLoss[0m : 2.78026
[1mStep[0m  [126/213], [94mLoss[0m : 2.65666
[1mStep[0m  [147/213], [94mLoss[0m : 2.49394
[1mStep[0m  [168/213], [94mLoss[0m : 2.36301
[1mStep[0m  [189/213], [94mLoss[0m : 2.43056
[1mStep[0m  [210/213], [94mLoss[0m : 2.59809

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.558, [92mTest[0m: 2.425, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.66755
[1mStep[0m  [21/213], [94mLoss[0m : 2.64214
[1mStep[0m  [42/213], [94mLoss[0m : 2.03243
[1mStep[0m  [63/213], [94mLoss[0m : 3.05844
[1mStep[0m  [84/213], [94mLoss[0m : 2.54184
[1mStep[0m  [105/213], [94mLoss[0m : 2.56153
[1mStep[0m  [126/213], [94mLoss[0m : 2.40807
[1mStep[0m  [147/213], [94mLoss[0m : 1.94134
[1mStep[0m  [168/213], [94mLoss[0m : 2.82513
[1mStep[0m  [189/213], [94mLoss[0m : 2.40990
[1mStep[0m  [210/213], [94mLoss[0m : 2.43240

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.571, [92mTest[0m: 2.418, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 3.13465
[1mStep[0m  [21/213], [94mLoss[0m : 2.16969
[1mStep[0m  [42/213], [94mLoss[0m : 2.36839
[1mStep[0m  [63/213], [94mLoss[0m : 2.82792
[1mStep[0m  [84/213], [94mLoss[0m : 2.31746
[1mStep[0m  [105/213], [94mLoss[0m : 2.50431
[1mStep[0m  [126/213], [94mLoss[0m : 2.59144
[1mStep[0m  [147/213], [94mLoss[0m : 2.94554
[1mStep[0m  [168/213], [94mLoss[0m : 2.22009
[1mStep[0m  [189/213], [94mLoss[0m : 2.59588
[1mStep[0m  [210/213], [94mLoss[0m : 2.71139

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.545, [92mTest[0m: 2.414, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.63925
[1mStep[0m  [21/213], [94mLoss[0m : 2.36783
[1mStep[0m  [42/213], [94mLoss[0m : 2.70767
[1mStep[0m  [63/213], [94mLoss[0m : 2.97949
[1mStep[0m  [84/213], [94mLoss[0m : 2.32969
[1mStep[0m  [105/213], [94mLoss[0m : 2.22605
[1mStep[0m  [126/213], [94mLoss[0m : 2.37674
[1mStep[0m  [147/213], [94mLoss[0m : 2.40146
[1mStep[0m  [168/213], [94mLoss[0m : 2.53916
[1mStep[0m  [189/213], [94mLoss[0m : 2.49227
[1mStep[0m  [210/213], [94mLoss[0m : 2.29309

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.550, [92mTest[0m: 2.424, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.94739
[1mStep[0m  [21/213], [94mLoss[0m : 2.62681
[1mStep[0m  [42/213], [94mLoss[0m : 2.75451
[1mStep[0m  [63/213], [94mLoss[0m : 3.09197
[1mStep[0m  [84/213], [94mLoss[0m : 2.73668
[1mStep[0m  [105/213], [94mLoss[0m : 2.73061
[1mStep[0m  [126/213], [94mLoss[0m : 2.66931
[1mStep[0m  [147/213], [94mLoss[0m : 2.84979
[1mStep[0m  [168/213], [94mLoss[0m : 2.61397
[1mStep[0m  [189/213], [94mLoss[0m : 2.43764
[1mStep[0m  [210/213], [94mLoss[0m : 2.35593

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.535, [92mTest[0m: 2.410, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.02359
[1mStep[0m  [21/213], [94mLoss[0m : 2.52004
[1mStep[0m  [42/213], [94mLoss[0m : 2.31969
[1mStep[0m  [63/213], [94mLoss[0m : 2.28017
[1mStep[0m  [84/213], [94mLoss[0m : 2.88705
[1mStep[0m  [105/213], [94mLoss[0m : 2.55340
[1mStep[0m  [126/213], [94mLoss[0m : 2.32155
[1mStep[0m  [147/213], [94mLoss[0m : 2.70108
[1mStep[0m  [168/213], [94mLoss[0m : 2.19503
[1mStep[0m  [189/213], [94mLoss[0m : 2.55783
[1mStep[0m  [210/213], [94mLoss[0m : 2.87264

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.422, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.43102
[1mStep[0m  [21/213], [94mLoss[0m : 2.71407
[1mStep[0m  [42/213], [94mLoss[0m : 2.69888
[1mStep[0m  [63/213], [94mLoss[0m : 2.64471
[1mStep[0m  [84/213], [94mLoss[0m : 2.47671
[1mStep[0m  [105/213], [94mLoss[0m : 2.33601
[1mStep[0m  [126/213], [94mLoss[0m : 2.53931
[1mStep[0m  [147/213], [94mLoss[0m : 2.51013
[1mStep[0m  [168/213], [94mLoss[0m : 2.80252
[1mStep[0m  [189/213], [94mLoss[0m : 2.18271
[1mStep[0m  [210/213], [94mLoss[0m : 2.69765

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.417, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.47553
[1mStep[0m  [21/213], [94mLoss[0m : 2.12373
[1mStep[0m  [42/213], [94mLoss[0m : 2.35027
[1mStep[0m  [63/213], [94mLoss[0m : 2.32402
[1mStep[0m  [84/213], [94mLoss[0m : 2.42456
[1mStep[0m  [105/213], [94mLoss[0m : 2.74378
[1mStep[0m  [126/213], [94mLoss[0m : 2.55204
[1mStep[0m  [147/213], [94mLoss[0m : 2.91780
[1mStep[0m  [168/213], [94mLoss[0m : 3.10414
[1mStep[0m  [189/213], [94mLoss[0m : 2.21570
[1mStep[0m  [210/213], [94mLoss[0m : 2.21168

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.536, [92mTest[0m: 2.412, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.44129
[1mStep[0m  [21/213], [94mLoss[0m : 3.00647
[1mStep[0m  [42/213], [94mLoss[0m : 2.63196
[1mStep[0m  [63/213], [94mLoss[0m : 2.08269
[1mStep[0m  [84/213], [94mLoss[0m : 2.66583
[1mStep[0m  [105/213], [94mLoss[0m : 2.66519
[1mStep[0m  [126/213], [94mLoss[0m : 2.54370
[1mStep[0m  [147/213], [94mLoss[0m : 2.29748
[1mStep[0m  [168/213], [94mLoss[0m : 2.31711
[1mStep[0m  [189/213], [94mLoss[0m : 2.71440
[1mStep[0m  [210/213], [94mLoss[0m : 2.39574

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.546, [92mTest[0m: 2.419, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.30662
[1mStep[0m  [21/213], [94mLoss[0m : 2.36942
[1mStep[0m  [42/213], [94mLoss[0m : 2.78213
[1mStep[0m  [63/213], [94mLoss[0m : 2.45609
[1mStep[0m  [84/213], [94mLoss[0m : 2.66801
[1mStep[0m  [105/213], [94mLoss[0m : 2.69092
[1mStep[0m  [126/213], [94mLoss[0m : 2.61689
[1mStep[0m  [147/213], [94mLoss[0m : 2.25141
[1mStep[0m  [168/213], [94mLoss[0m : 2.74596
[1mStep[0m  [189/213], [94mLoss[0m : 2.52270
[1mStep[0m  [210/213], [94mLoss[0m : 2.66830

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.553, [92mTest[0m: 2.413, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.65370
[1mStep[0m  [21/213], [94mLoss[0m : 2.52374
[1mStep[0m  [42/213], [94mLoss[0m : 2.27667
[1mStep[0m  [63/213], [94mLoss[0m : 2.28843
[1mStep[0m  [84/213], [94mLoss[0m : 2.52743
[1mStep[0m  [105/213], [94mLoss[0m : 2.63141
[1mStep[0m  [126/213], [94mLoss[0m : 2.31271
[1mStep[0m  [147/213], [94mLoss[0m : 2.33583
[1mStep[0m  [168/213], [94mLoss[0m : 2.36512
[1mStep[0m  [189/213], [94mLoss[0m : 2.48107
[1mStep[0m  [210/213], [94mLoss[0m : 2.44667

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.542, [92mTest[0m: 2.411, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.51593
[1mStep[0m  [21/213], [94mLoss[0m : 2.60264
[1mStep[0m  [42/213], [94mLoss[0m : 2.49455
[1mStep[0m  [63/213], [94mLoss[0m : 2.67905
[1mStep[0m  [84/213], [94mLoss[0m : 2.42965
[1mStep[0m  [105/213], [94mLoss[0m : 2.64340
[1mStep[0m  [126/213], [94mLoss[0m : 2.20486
[1mStep[0m  [147/213], [94mLoss[0m : 2.59082
[1mStep[0m  [168/213], [94mLoss[0m : 2.45925
[1mStep[0m  [189/213], [94mLoss[0m : 2.65740
[1mStep[0m  [210/213], [94mLoss[0m : 2.73787

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.409, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.33477
[1mStep[0m  [21/213], [94mLoss[0m : 2.63152
[1mStep[0m  [42/213], [94mLoss[0m : 2.76171
[1mStep[0m  [63/213], [94mLoss[0m : 1.99870
[1mStep[0m  [84/213], [94mLoss[0m : 2.62318
[1mStep[0m  [105/213], [94mLoss[0m : 2.74464
[1mStep[0m  [126/213], [94mLoss[0m : 2.38170
[1mStep[0m  [147/213], [94mLoss[0m : 2.41301
[1mStep[0m  [168/213], [94mLoss[0m : 2.90941
[1mStep[0m  [189/213], [94mLoss[0m : 2.72115
[1mStep[0m  [210/213], [94mLoss[0m : 2.60528

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.537, [92mTest[0m: 2.427, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.55825
[1mStep[0m  [21/213], [94mLoss[0m : 2.25793
[1mStep[0m  [42/213], [94mLoss[0m : 2.81279
[1mStep[0m  [63/213], [94mLoss[0m : 2.32467
[1mStep[0m  [84/213], [94mLoss[0m : 2.76768
[1mStep[0m  [105/213], [94mLoss[0m : 2.38747
[1mStep[0m  [126/213], [94mLoss[0m : 2.53968
[1mStep[0m  [147/213], [94mLoss[0m : 2.33901
[1mStep[0m  [168/213], [94mLoss[0m : 2.47830
[1mStep[0m  [189/213], [94mLoss[0m : 2.73609
[1mStep[0m  [210/213], [94mLoss[0m : 2.73522

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.538, [92mTest[0m: 2.413, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.11904
[1mStep[0m  [21/213], [94mLoss[0m : 2.36853
[1mStep[0m  [42/213], [94mLoss[0m : 2.44328
[1mStep[0m  [63/213], [94mLoss[0m : 2.55069
[1mStep[0m  [84/213], [94mLoss[0m : 2.27590
[1mStep[0m  [105/213], [94mLoss[0m : 2.17904
[1mStep[0m  [126/213], [94mLoss[0m : 2.34305
[1mStep[0m  [147/213], [94mLoss[0m : 2.71487
[1mStep[0m  [168/213], [94mLoss[0m : 2.40885
[1mStep[0m  [189/213], [94mLoss[0m : 2.72005
[1mStep[0m  [210/213], [94mLoss[0m : 2.33775

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.530, [92mTest[0m: 2.406, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.46715
[1mStep[0m  [21/213], [94mLoss[0m : 2.65725
[1mStep[0m  [42/213], [94mLoss[0m : 2.38021
[1mStep[0m  [63/213], [94mLoss[0m : 2.41321
[1mStep[0m  [84/213], [94mLoss[0m : 2.87725
[1mStep[0m  [105/213], [94mLoss[0m : 2.58560
[1mStep[0m  [126/213], [94mLoss[0m : 2.95354
[1mStep[0m  [147/213], [94mLoss[0m : 2.43888
[1mStep[0m  [168/213], [94mLoss[0m : 2.36745
[1mStep[0m  [189/213], [94mLoss[0m : 2.76256
[1mStep[0m  [210/213], [94mLoss[0m : 2.85007

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.529, [92mTest[0m: 2.413, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.69290
[1mStep[0m  [21/213], [94mLoss[0m : 2.30770
[1mStep[0m  [42/213], [94mLoss[0m : 2.01668
[1mStep[0m  [63/213], [94mLoss[0m : 2.60991
[1mStep[0m  [84/213], [94mLoss[0m : 2.59094
[1mStep[0m  [105/213], [94mLoss[0m : 2.30474
[1mStep[0m  [126/213], [94mLoss[0m : 2.43953
[1mStep[0m  [147/213], [94mLoss[0m : 2.75242
[1mStep[0m  [168/213], [94mLoss[0m : 2.46748
[1mStep[0m  [189/213], [94mLoss[0m : 2.38913
[1mStep[0m  [210/213], [94mLoss[0m : 1.89063

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.419, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.35587
[1mStep[0m  [21/213], [94mLoss[0m : 2.95829
[1mStep[0m  [42/213], [94mLoss[0m : 2.35508
[1mStep[0m  [63/213], [94mLoss[0m : 2.47327
[1mStep[0m  [84/213], [94mLoss[0m : 2.23837
[1mStep[0m  [105/213], [94mLoss[0m : 2.30638
[1mStep[0m  [126/213], [94mLoss[0m : 2.60490
[1mStep[0m  [147/213], [94mLoss[0m : 2.68370
[1mStep[0m  [168/213], [94mLoss[0m : 2.78861
[1mStep[0m  [189/213], [94mLoss[0m : 2.52969
[1mStep[0m  [210/213], [94mLoss[0m : 2.26943

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.515, [92mTest[0m: 2.409, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.62854
[1mStep[0m  [21/213], [94mLoss[0m : 2.17720
[1mStep[0m  [42/213], [94mLoss[0m : 2.43252
[1mStep[0m  [63/213], [94mLoss[0m : 2.34798
[1mStep[0m  [84/213], [94mLoss[0m : 2.45998
[1mStep[0m  [105/213], [94mLoss[0m : 1.90351
[1mStep[0m  [126/213], [94mLoss[0m : 2.75075
[1mStep[0m  [147/213], [94mLoss[0m : 2.90795
[1mStep[0m  [168/213], [94mLoss[0m : 2.42776
[1mStep[0m  [189/213], [94mLoss[0m : 2.56546
[1mStep[0m  [210/213], [94mLoss[0m : 2.78486

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.405, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.23986
[1mStep[0m  [21/213], [94mLoss[0m : 2.70533
[1mStep[0m  [42/213], [94mLoss[0m : 2.52550
[1mStep[0m  [63/213], [94mLoss[0m : 2.56632
[1mStep[0m  [84/213], [94mLoss[0m : 2.93881
[1mStep[0m  [105/213], [94mLoss[0m : 2.71556
[1mStep[0m  [126/213], [94mLoss[0m : 2.51996
[1mStep[0m  [147/213], [94mLoss[0m : 2.34411
[1mStep[0m  [168/213], [94mLoss[0m : 2.17998
[1mStep[0m  [189/213], [94mLoss[0m : 2.87053
[1mStep[0m  [210/213], [94mLoss[0m : 2.79253

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.421, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.43471
[1mStep[0m  [21/213], [94mLoss[0m : 2.59862
[1mStep[0m  [42/213], [94mLoss[0m : 2.29469
[1mStep[0m  [63/213], [94mLoss[0m : 2.57967
[1mStep[0m  [84/213], [94mLoss[0m : 2.35171
[1mStep[0m  [105/213], [94mLoss[0m : 2.31489
[1mStep[0m  [126/213], [94mLoss[0m : 2.46714
[1mStep[0m  [147/213], [94mLoss[0m : 2.67925
[1mStep[0m  [168/213], [94mLoss[0m : 2.53738
[1mStep[0m  [189/213], [94mLoss[0m : 2.69536
[1mStep[0m  [210/213], [94mLoss[0m : 2.45296

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.398, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.71029
[1mStep[0m  [21/213], [94mLoss[0m : 2.41695
[1mStep[0m  [42/213], [94mLoss[0m : 2.57345
[1mStep[0m  [63/213], [94mLoss[0m : 2.10264
[1mStep[0m  [84/213], [94mLoss[0m : 2.68920
[1mStep[0m  [105/213], [94mLoss[0m : 2.44142
[1mStep[0m  [126/213], [94mLoss[0m : 2.27810
[1mStep[0m  [147/213], [94mLoss[0m : 2.60759
[1mStep[0m  [168/213], [94mLoss[0m : 2.62672
[1mStep[0m  [189/213], [94mLoss[0m : 2.45346
[1mStep[0m  [210/213], [94mLoss[0m : 2.42224

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.401, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.395
====================================

Phase 1 - Evaluation MAE:  2.3954140870076306
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/213], [94mLoss[0m : 2.37781
[1mStep[0m  [21/213], [94mLoss[0m : 2.56242
[1mStep[0m  [42/213], [94mLoss[0m : 2.58220
[1mStep[0m  [63/213], [94mLoss[0m : 2.63446
[1mStep[0m  [84/213], [94mLoss[0m : 2.56056
[1mStep[0m  [105/213], [94mLoss[0m : 3.03036
[1mStep[0m  [126/213], [94mLoss[0m : 2.46597
[1mStep[0m  [147/213], [94mLoss[0m : 2.36474
[1mStep[0m  [168/213], [94mLoss[0m : 2.75277
[1mStep[0m  [189/213], [94mLoss[0m : 2.44420
[1mStep[0m  [210/213], [94mLoss[0m : 2.32740

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.531, [92mTest[0m: 2.404, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.28324
[1mStep[0m  [21/213], [94mLoss[0m : 2.30722
[1mStep[0m  [42/213], [94mLoss[0m : 2.44463
[1mStep[0m  [63/213], [94mLoss[0m : 2.55324
[1mStep[0m  [84/213], [94mLoss[0m : 2.32293
[1mStep[0m  [105/213], [94mLoss[0m : 2.39993
[1mStep[0m  [126/213], [94mLoss[0m : 2.74043
[1mStep[0m  [147/213], [94mLoss[0m : 2.59088
[1mStep[0m  [168/213], [94mLoss[0m : 2.67456
[1mStep[0m  [189/213], [94mLoss[0m : 2.87184
[1mStep[0m  [210/213], [94mLoss[0m : 2.68533

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.727, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.88656
[1mStep[0m  [21/213], [94mLoss[0m : 2.32410
[1mStep[0m  [42/213], [94mLoss[0m : 2.28696
[1mStep[0m  [63/213], [94mLoss[0m : 2.52193
[1mStep[0m  [84/213], [94mLoss[0m : 2.17139
[1mStep[0m  [105/213], [94mLoss[0m : 2.63444
[1mStep[0m  [126/213], [94mLoss[0m : 2.54858
[1mStep[0m  [147/213], [94mLoss[0m : 2.17610
[1mStep[0m  [168/213], [94mLoss[0m : 2.17627
[1mStep[0m  [189/213], [94mLoss[0m : 2.50675
[1mStep[0m  [210/213], [94mLoss[0m : 2.36715

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.724, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.38437
[1mStep[0m  [21/213], [94mLoss[0m : 3.30721
[1mStep[0m  [42/213], [94mLoss[0m : 2.05026
[1mStep[0m  [63/213], [94mLoss[0m : 2.37550
[1mStep[0m  [84/213], [94mLoss[0m : 2.36260
[1mStep[0m  [105/213], [94mLoss[0m : 2.10226
[1mStep[0m  [126/213], [94mLoss[0m : 2.44102
[1mStep[0m  [147/213], [94mLoss[0m : 2.67153
[1mStep[0m  [168/213], [94mLoss[0m : 2.90217
[1mStep[0m  [189/213], [94mLoss[0m : 2.59075
[1mStep[0m  [210/213], [94mLoss[0m : 2.29800

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.620, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.09457
[1mStep[0m  [21/213], [94mLoss[0m : 1.97029
[1mStep[0m  [42/213], [94mLoss[0m : 2.82188
[1mStep[0m  [63/213], [94mLoss[0m : 2.25517
[1mStep[0m  [84/213], [94mLoss[0m : 2.52324
[1mStep[0m  [105/213], [94mLoss[0m : 2.39573
[1mStep[0m  [126/213], [94mLoss[0m : 2.61645
[1mStep[0m  [147/213], [94mLoss[0m : 3.04075
[1mStep[0m  [168/213], [94mLoss[0m : 2.38049
[1mStep[0m  [189/213], [94mLoss[0m : 2.12938
[1mStep[0m  [210/213], [94mLoss[0m : 2.34424

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.374, [92mTest[0m: 2.632, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.49134
[1mStep[0m  [21/213], [94mLoss[0m : 2.10618
[1mStep[0m  [42/213], [94mLoss[0m : 2.14585
[1mStep[0m  [63/213], [94mLoss[0m : 2.21941
[1mStep[0m  [84/213], [94mLoss[0m : 2.51362
[1mStep[0m  [105/213], [94mLoss[0m : 2.23402
[1mStep[0m  [126/213], [94mLoss[0m : 2.22092
[1mStep[0m  [147/213], [94mLoss[0m : 2.03087
[1mStep[0m  [168/213], [94mLoss[0m : 2.22160
[1mStep[0m  [189/213], [94mLoss[0m : 2.35385
[1mStep[0m  [210/213], [94mLoss[0m : 2.49401

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.320, [92mTest[0m: 2.578, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.26014
[1mStep[0m  [21/213], [94mLoss[0m : 2.13797
[1mStep[0m  [42/213], [94mLoss[0m : 2.54402
[1mStep[0m  [63/213], [94mLoss[0m : 2.39577
[1mStep[0m  [84/213], [94mLoss[0m : 2.31316
[1mStep[0m  [105/213], [94mLoss[0m : 2.39199
[1mStep[0m  [126/213], [94mLoss[0m : 2.22348
[1mStep[0m  [147/213], [94mLoss[0m : 2.04606
[1mStep[0m  [168/213], [94mLoss[0m : 1.80874
[1mStep[0m  [189/213], [94mLoss[0m : 1.90053
[1mStep[0m  [210/213], [94mLoss[0m : 2.57392

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.277, [92mTest[0m: 2.517, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.39010
[1mStep[0m  [21/213], [94mLoss[0m : 2.15609
[1mStep[0m  [42/213], [94mLoss[0m : 1.85896
[1mStep[0m  [63/213], [94mLoss[0m : 2.51864
[1mStep[0m  [84/213], [94mLoss[0m : 2.27835
[1mStep[0m  [105/213], [94mLoss[0m : 2.12549
[1mStep[0m  [126/213], [94mLoss[0m : 2.64849
[1mStep[0m  [147/213], [94mLoss[0m : 2.05924
[1mStep[0m  [168/213], [94mLoss[0m : 2.18191
[1mStep[0m  [189/213], [94mLoss[0m : 2.49779
[1mStep[0m  [210/213], [94mLoss[0m : 2.06599

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.241, [92mTest[0m: 2.465, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.41866
[1mStep[0m  [21/213], [94mLoss[0m : 2.05324
[1mStep[0m  [42/213], [94mLoss[0m : 2.01677
[1mStep[0m  [63/213], [94mLoss[0m : 1.93253
[1mStep[0m  [84/213], [94mLoss[0m : 2.00726
[1mStep[0m  [105/213], [94mLoss[0m : 2.35608
[1mStep[0m  [126/213], [94mLoss[0m : 2.33167
[1mStep[0m  [147/213], [94mLoss[0m : 2.71867
[1mStep[0m  [168/213], [94mLoss[0m : 1.90002
[1mStep[0m  [189/213], [94mLoss[0m : 2.61628
[1mStep[0m  [210/213], [94mLoss[0m : 2.33736

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.181, [92mTest[0m: 2.441, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.61478
[1mStep[0m  [21/213], [94mLoss[0m : 2.48706
[1mStep[0m  [42/213], [94mLoss[0m : 1.83337
[1mStep[0m  [63/213], [94mLoss[0m : 2.48791
[1mStep[0m  [84/213], [94mLoss[0m : 2.13022
[1mStep[0m  [105/213], [94mLoss[0m : 2.26536
[1mStep[0m  [126/213], [94mLoss[0m : 2.06867
[1mStep[0m  [147/213], [94mLoss[0m : 2.14093
[1mStep[0m  [168/213], [94mLoss[0m : 2.39803
[1mStep[0m  [189/213], [94mLoss[0m : 2.08016
[1mStep[0m  [210/213], [94mLoss[0m : 2.12219

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.174, [92mTest[0m: 2.423, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.96801
[1mStep[0m  [21/213], [94mLoss[0m : 1.77945
[1mStep[0m  [42/213], [94mLoss[0m : 2.06297
[1mStep[0m  [63/213], [94mLoss[0m : 1.97697
[1mStep[0m  [84/213], [94mLoss[0m : 2.49686
[1mStep[0m  [105/213], [94mLoss[0m : 1.95610
[1mStep[0m  [126/213], [94mLoss[0m : 2.37539
[1mStep[0m  [147/213], [94mLoss[0m : 2.27807
[1mStep[0m  [168/213], [94mLoss[0m : 1.93044
[1mStep[0m  [189/213], [94mLoss[0m : 1.75506
[1mStep[0m  [210/213], [94mLoss[0m : 2.37327

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.124, [92mTest[0m: 2.437, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.09193
[1mStep[0m  [21/213], [94mLoss[0m : 1.91612
[1mStep[0m  [42/213], [94mLoss[0m : 2.31423
[1mStep[0m  [63/213], [94mLoss[0m : 2.07936
[1mStep[0m  [84/213], [94mLoss[0m : 2.07792
[1mStep[0m  [105/213], [94mLoss[0m : 1.84960
[1mStep[0m  [126/213], [94mLoss[0m : 2.33566
[1mStep[0m  [147/213], [94mLoss[0m : 2.31412
[1mStep[0m  [168/213], [94mLoss[0m : 2.31088
[1mStep[0m  [189/213], [94mLoss[0m : 2.32712
[1mStep[0m  [210/213], [94mLoss[0m : 2.17657

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.094, [92mTest[0m: 2.430, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.95727
[1mStep[0m  [21/213], [94mLoss[0m : 2.29924
[1mStep[0m  [42/213], [94mLoss[0m : 1.65388
[1mStep[0m  [63/213], [94mLoss[0m : 2.06264
[1mStep[0m  [84/213], [94mLoss[0m : 1.81232
[1mStep[0m  [105/213], [94mLoss[0m : 2.10278
[1mStep[0m  [126/213], [94mLoss[0m : 1.93790
[1mStep[0m  [147/213], [94mLoss[0m : 2.23709
[1mStep[0m  [168/213], [94mLoss[0m : 1.73767
[1mStep[0m  [189/213], [94mLoss[0m : 2.01018
[1mStep[0m  [210/213], [94mLoss[0m : 2.06237

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.048, [92mTest[0m: 2.446, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.97927
[1mStep[0m  [21/213], [94mLoss[0m : 1.51588
[1mStep[0m  [42/213], [94mLoss[0m : 2.20216
[1mStep[0m  [63/213], [94mLoss[0m : 1.83801
[1mStep[0m  [84/213], [94mLoss[0m : 1.98133
[1mStep[0m  [105/213], [94mLoss[0m : 1.75841
[1mStep[0m  [126/213], [94mLoss[0m : 2.14154
[1mStep[0m  [147/213], [94mLoss[0m : 2.30326
[1mStep[0m  [168/213], [94mLoss[0m : 2.01188
[1mStep[0m  [189/213], [94mLoss[0m : 2.32812
[1mStep[0m  [210/213], [94mLoss[0m : 2.16451

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.021, [92mTest[0m: 2.456, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.75722
[1mStep[0m  [21/213], [94mLoss[0m : 1.97892
[1mStep[0m  [42/213], [94mLoss[0m : 1.77854
[1mStep[0m  [63/213], [94mLoss[0m : 2.21289
[1mStep[0m  [84/213], [94mLoss[0m : 2.10573
[1mStep[0m  [105/213], [94mLoss[0m : 2.17240
[1mStep[0m  [126/213], [94mLoss[0m : 1.42737
[1mStep[0m  [147/213], [94mLoss[0m : 1.97516
[1mStep[0m  [168/213], [94mLoss[0m : 1.92549
[1mStep[0m  [189/213], [94mLoss[0m : 1.48689
[1mStep[0m  [210/213], [94mLoss[0m : 1.86037

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.983, [92mTest[0m: 2.451, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.63029
[1mStep[0m  [21/213], [94mLoss[0m : 1.99360
[1mStep[0m  [42/213], [94mLoss[0m : 1.97385
[1mStep[0m  [63/213], [94mLoss[0m : 1.82043
[1mStep[0m  [84/213], [94mLoss[0m : 2.15090
[1mStep[0m  [105/213], [94mLoss[0m : 2.04640
[1mStep[0m  [126/213], [94mLoss[0m : 1.72125
[1mStep[0m  [147/213], [94mLoss[0m : 1.96895
[1mStep[0m  [168/213], [94mLoss[0m : 1.95429
[1mStep[0m  [189/213], [94mLoss[0m : 2.26813
[1mStep[0m  [210/213], [94mLoss[0m : 2.09463

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.955, [92mTest[0m: 2.463, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.12018
[1mStep[0m  [21/213], [94mLoss[0m : 1.76546
[1mStep[0m  [42/213], [94mLoss[0m : 1.92321
[1mStep[0m  [63/213], [94mLoss[0m : 2.04724
[1mStep[0m  [84/213], [94mLoss[0m : 2.05004
[1mStep[0m  [105/213], [94mLoss[0m : 1.91726
[1mStep[0m  [126/213], [94mLoss[0m : 2.09358
[1mStep[0m  [147/213], [94mLoss[0m : 2.10469
[1mStep[0m  [168/213], [94mLoss[0m : 1.89806
[1mStep[0m  [189/213], [94mLoss[0m : 1.84475
[1mStep[0m  [210/213], [94mLoss[0m : 1.84823

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.927, [92mTest[0m: 2.457, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.97870
[1mStep[0m  [21/213], [94mLoss[0m : 1.96038
[1mStep[0m  [42/213], [94mLoss[0m : 1.99026
[1mStep[0m  [63/213], [94mLoss[0m : 1.96415
[1mStep[0m  [84/213], [94mLoss[0m : 1.86951
[1mStep[0m  [105/213], [94mLoss[0m : 1.73251
[1mStep[0m  [126/213], [94mLoss[0m : 1.60176
[1mStep[0m  [147/213], [94mLoss[0m : 1.92511
[1mStep[0m  [168/213], [94mLoss[0m : 1.77981
[1mStep[0m  [189/213], [94mLoss[0m : 1.95463
[1mStep[0m  [210/213], [94mLoss[0m : 2.06606

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.914, [92mTest[0m: 2.540, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.18136
[1mStep[0m  [21/213], [94mLoss[0m : 1.87194
[1mStep[0m  [42/213], [94mLoss[0m : 1.86822
[1mStep[0m  [63/213], [94mLoss[0m : 2.04037
[1mStep[0m  [84/213], [94mLoss[0m : 1.86357
[1mStep[0m  [105/213], [94mLoss[0m : 1.61752
[1mStep[0m  [126/213], [94mLoss[0m : 1.97180
[1mStep[0m  [147/213], [94mLoss[0m : 1.60824
[1mStep[0m  [168/213], [94mLoss[0m : 2.06287
[1mStep[0m  [189/213], [94mLoss[0m : 1.85782
[1mStep[0m  [210/213], [94mLoss[0m : 2.12001

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.858, [92mTest[0m: 2.517, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.77750
[1mStep[0m  [21/213], [94mLoss[0m : 2.06605
[1mStep[0m  [42/213], [94mLoss[0m : 1.95789
[1mStep[0m  [63/213], [94mLoss[0m : 2.05884
[1mStep[0m  [84/213], [94mLoss[0m : 1.84528
[1mStep[0m  [105/213], [94mLoss[0m : 1.81054
[1mStep[0m  [126/213], [94mLoss[0m : 1.91464
[1mStep[0m  [147/213], [94mLoss[0m : 1.84558
[1mStep[0m  [168/213], [94mLoss[0m : 2.01376
[1mStep[0m  [189/213], [94mLoss[0m : 1.93284
[1mStep[0m  [210/213], [94mLoss[0m : 1.81282

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.830, [92mTest[0m: 2.482, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.62944
[1mStep[0m  [21/213], [94mLoss[0m : 1.84694
[1mStep[0m  [42/213], [94mLoss[0m : 1.99160
[1mStep[0m  [63/213], [94mLoss[0m : 1.87074
[1mStep[0m  [84/213], [94mLoss[0m : 1.78546
[1mStep[0m  [105/213], [94mLoss[0m : 1.58359
[1mStep[0m  [126/213], [94mLoss[0m : 1.98032
[1mStep[0m  [147/213], [94mLoss[0m : 1.55853
[1mStep[0m  [168/213], [94mLoss[0m : 1.86913
[1mStep[0m  [189/213], [94mLoss[0m : 1.64217
[1mStep[0m  [210/213], [94mLoss[0m : 1.63378

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.828, [92mTest[0m: 2.541, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.77463
[1mStep[0m  [21/213], [94mLoss[0m : 1.72928
[1mStep[0m  [42/213], [94mLoss[0m : 1.64465
[1mStep[0m  [63/213], [94mLoss[0m : 1.37758
[1mStep[0m  [84/213], [94mLoss[0m : 1.74323
[1mStep[0m  [105/213], [94mLoss[0m : 1.69593
[1mStep[0m  [126/213], [94mLoss[0m : 1.92713
[1mStep[0m  [147/213], [94mLoss[0m : 1.85792
[1mStep[0m  [168/213], [94mLoss[0m : 1.70661
[1mStep[0m  [189/213], [94mLoss[0m : 1.80203
[1mStep[0m  [210/213], [94mLoss[0m : 2.06712

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.789, [92mTest[0m: 2.424, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.57448
[1mStep[0m  [21/213], [94mLoss[0m : 1.89894
[1mStep[0m  [42/213], [94mLoss[0m : 1.72078
[1mStep[0m  [63/213], [94mLoss[0m : 2.06510
[1mStep[0m  [84/213], [94mLoss[0m : 1.81633
[1mStep[0m  [105/213], [94mLoss[0m : 1.74090
[1mStep[0m  [126/213], [94mLoss[0m : 1.89219
[1mStep[0m  [147/213], [94mLoss[0m : 2.15871
[1mStep[0m  [168/213], [94mLoss[0m : 1.58705
[1mStep[0m  [189/213], [94mLoss[0m : 1.90674
[1mStep[0m  [210/213], [94mLoss[0m : 1.88384

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.774, [92mTest[0m: 2.514, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.14698
[1mStep[0m  [21/213], [94mLoss[0m : 1.91119
[1mStep[0m  [42/213], [94mLoss[0m : 1.82732
[1mStep[0m  [63/213], [94mLoss[0m : 1.70228
[1mStep[0m  [84/213], [94mLoss[0m : 1.65170
[1mStep[0m  [105/213], [94mLoss[0m : 1.66050
[1mStep[0m  [126/213], [94mLoss[0m : 1.53819
[1mStep[0m  [147/213], [94mLoss[0m : 1.80727
[1mStep[0m  [168/213], [94mLoss[0m : 1.74904
[1mStep[0m  [189/213], [94mLoss[0m : 1.94818
[1mStep[0m  [210/213], [94mLoss[0m : 1.55497

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.750, [92mTest[0m: 2.486, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.75429
[1mStep[0m  [21/213], [94mLoss[0m : 1.56520
[1mStep[0m  [42/213], [94mLoss[0m : 1.39798
[1mStep[0m  [63/213], [94mLoss[0m : 1.52808
[1mStep[0m  [84/213], [94mLoss[0m : 1.87701
[1mStep[0m  [105/213], [94mLoss[0m : 1.83378
[1mStep[0m  [126/213], [94mLoss[0m : 1.46830
[1mStep[0m  [147/213], [94mLoss[0m : 1.63616
[1mStep[0m  [168/213], [94mLoss[0m : 1.85936
[1mStep[0m  [189/213], [94mLoss[0m : 1.57140
[1mStep[0m  [210/213], [94mLoss[0m : 1.84243

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.732, [92mTest[0m: 2.581, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.41226
[1mStep[0m  [21/213], [94mLoss[0m : 1.51093
[1mStep[0m  [42/213], [94mLoss[0m : 1.84226
[1mStep[0m  [63/213], [94mLoss[0m : 1.56555
[1mStep[0m  [84/213], [94mLoss[0m : 1.71830
[1mStep[0m  [105/213], [94mLoss[0m : 1.79415
[1mStep[0m  [126/213], [94mLoss[0m : 1.63922
[1mStep[0m  [147/213], [94mLoss[0m : 1.94655
[1mStep[0m  [168/213], [94mLoss[0m : 1.78172
[1mStep[0m  [189/213], [94mLoss[0m : 1.58112
[1mStep[0m  [210/213], [94mLoss[0m : 1.84289

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.687, [92mTest[0m: 2.548, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.74336
[1mStep[0m  [21/213], [94mLoss[0m : 1.53913
[1mStep[0m  [42/213], [94mLoss[0m : 1.66360
[1mStep[0m  [63/213], [94mLoss[0m : 1.74583
[1mStep[0m  [84/213], [94mLoss[0m : 1.73417
[1mStep[0m  [105/213], [94mLoss[0m : 1.20995
[1mStep[0m  [126/213], [94mLoss[0m : 1.47281
[1mStep[0m  [147/213], [94mLoss[0m : 1.68879
[1mStep[0m  [168/213], [94mLoss[0m : 1.74970
[1mStep[0m  [189/213], [94mLoss[0m : 1.53914
[1mStep[0m  [210/213], [94mLoss[0m : 1.87787

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.684, [92mTest[0m: 2.448, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.91664
[1mStep[0m  [21/213], [94mLoss[0m : 1.38816
[1mStep[0m  [42/213], [94mLoss[0m : 1.71973
[1mStep[0m  [63/213], [94mLoss[0m : 1.37790
[1mStep[0m  [84/213], [94mLoss[0m : 1.41216
[1mStep[0m  [105/213], [94mLoss[0m : 1.45212
[1mStep[0m  [126/213], [94mLoss[0m : 1.31883
[1mStep[0m  [147/213], [94mLoss[0m : 1.35136
[1mStep[0m  [168/213], [94mLoss[0m : 1.41771
[1mStep[0m  [189/213], [94mLoss[0m : 1.59504
[1mStep[0m  [210/213], [94mLoss[0m : 1.61830

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.684, [92mTest[0m: 2.510, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.27442
[1mStep[0m  [21/213], [94mLoss[0m : 2.25601
[1mStep[0m  [42/213], [94mLoss[0m : 1.73732
[1mStep[0m  [63/213], [94mLoss[0m : 1.64109
[1mStep[0m  [84/213], [94mLoss[0m : 1.39545
[1mStep[0m  [105/213], [94mLoss[0m : 1.76267
[1mStep[0m  [126/213], [94mLoss[0m : 1.43576
[1mStep[0m  [147/213], [94mLoss[0m : 1.52629
[1mStep[0m  [168/213], [94mLoss[0m : 1.50166
[1mStep[0m  [189/213], [94mLoss[0m : 1.50775
[1mStep[0m  [210/213], [94mLoss[0m : 1.71745

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.675, [92mTest[0m: 2.467, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.44997
[1mStep[0m  [21/213], [94mLoss[0m : 1.58055
[1mStep[0m  [42/213], [94mLoss[0m : 1.76518
[1mStep[0m  [63/213], [94mLoss[0m : 1.74929
[1mStep[0m  [84/213], [94mLoss[0m : 1.46638
[1mStep[0m  [105/213], [94mLoss[0m : 1.61462
[1mStep[0m  [126/213], [94mLoss[0m : 1.58686
[1mStep[0m  [147/213], [94mLoss[0m : 1.68850
[1mStep[0m  [168/213], [94mLoss[0m : 1.64758
[1mStep[0m  [189/213], [94mLoss[0m : 1.78224
[1mStep[0m  [210/213], [94mLoss[0m : 1.84150

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.630, [92mTest[0m: 2.500, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.532
====================================

Phase 2 - Evaluation MAE:  2.531847502825395
MAE score P1       2.395414
MAE score P2       2.531848
loss               1.629931
learning_rate       0.00505
batch_size               64
hidden_sizes      [250, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.1
weight_decay          0.001
Name: 4, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/106], [94mLoss[0m : 10.65985
[1mStep[0m  [10/106], [94mLoss[0m : 3.54605
[1mStep[0m  [20/106], [94mLoss[0m : 2.75777
[1mStep[0m  [30/106], [94mLoss[0m : 2.39081
[1mStep[0m  [40/106], [94mLoss[0m : 2.41483
[1mStep[0m  [50/106], [94mLoss[0m : 2.72885
[1mStep[0m  [60/106], [94mLoss[0m : 2.84842
[1mStep[0m  [70/106], [94mLoss[0m : 2.48111
[1mStep[0m  [80/106], [94mLoss[0m : 2.49061
[1mStep[0m  [90/106], [94mLoss[0m : 2.41508
[1mStep[0m  [100/106], [94mLoss[0m : 2.64007

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.056, [92mTest[0m: 10.842, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.81656
[1mStep[0m  [10/106], [94mLoss[0m : 2.33500
[1mStep[0m  [20/106], [94mLoss[0m : 2.65475
[1mStep[0m  [30/106], [94mLoss[0m : 2.42913
[1mStep[0m  [40/106], [94mLoss[0m : 2.55081
[1mStep[0m  [50/106], [94mLoss[0m : 2.33886
[1mStep[0m  [60/106], [94mLoss[0m : 2.66485
[1mStep[0m  [70/106], [94mLoss[0m : 2.42922
[1mStep[0m  [80/106], [94mLoss[0m : 2.27315
[1mStep[0m  [90/106], [94mLoss[0m : 2.44041
[1mStep[0m  [100/106], [94mLoss[0m : 2.50407

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.476, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.29143
[1mStep[0m  [10/106], [94mLoss[0m : 2.32618
[1mStep[0m  [20/106], [94mLoss[0m : 2.77588
[1mStep[0m  [30/106], [94mLoss[0m : 2.23987
[1mStep[0m  [40/106], [94mLoss[0m : 2.56714
[1mStep[0m  [50/106], [94mLoss[0m : 2.29144
[1mStep[0m  [60/106], [94mLoss[0m : 2.48538
[1mStep[0m  [70/106], [94mLoss[0m : 2.37702
[1mStep[0m  [80/106], [94mLoss[0m : 2.27166
[1mStep[0m  [90/106], [94mLoss[0m : 2.60552
[1mStep[0m  [100/106], [94mLoss[0m : 2.26839

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.415, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.19804
[1mStep[0m  [10/106], [94mLoss[0m : 2.25291
[1mStep[0m  [20/106], [94mLoss[0m : 2.57272
[1mStep[0m  [30/106], [94mLoss[0m : 2.48959
[1mStep[0m  [40/106], [94mLoss[0m : 2.25817
[1mStep[0m  [50/106], [94mLoss[0m : 2.19368
[1mStep[0m  [60/106], [94mLoss[0m : 2.55259
[1mStep[0m  [70/106], [94mLoss[0m : 2.33394
[1mStep[0m  [80/106], [94mLoss[0m : 2.16150
[1mStep[0m  [90/106], [94mLoss[0m : 2.13163
[1mStep[0m  [100/106], [94mLoss[0m : 2.19902

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.410, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.12101
[1mStep[0m  [10/106], [94mLoss[0m : 2.40335
[1mStep[0m  [20/106], [94mLoss[0m : 2.78341
[1mStep[0m  [30/106], [94mLoss[0m : 2.43887
[1mStep[0m  [40/106], [94mLoss[0m : 2.28223
[1mStep[0m  [50/106], [94mLoss[0m : 2.02391
[1mStep[0m  [60/106], [94mLoss[0m : 2.33737
[1mStep[0m  [70/106], [94mLoss[0m : 2.35645
[1mStep[0m  [80/106], [94mLoss[0m : 2.24824
[1mStep[0m  [90/106], [94mLoss[0m : 2.53730
[1mStep[0m  [100/106], [94mLoss[0m : 2.41876

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.422, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.40622
[1mStep[0m  [10/106], [94mLoss[0m : 2.47291
[1mStep[0m  [20/106], [94mLoss[0m : 2.61096
[1mStep[0m  [30/106], [94mLoss[0m : 2.35927
[1mStep[0m  [40/106], [94mLoss[0m : 2.44659
[1mStep[0m  [50/106], [94mLoss[0m : 2.08588
[1mStep[0m  [60/106], [94mLoss[0m : 2.57943
[1mStep[0m  [70/106], [94mLoss[0m : 2.35713
[1mStep[0m  [80/106], [94mLoss[0m : 2.65455
[1mStep[0m  [90/106], [94mLoss[0m : 2.55315
[1mStep[0m  [100/106], [94mLoss[0m : 2.44371

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.400, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.56915
[1mStep[0m  [10/106], [94mLoss[0m : 2.58359
[1mStep[0m  [20/106], [94mLoss[0m : 2.53571
[1mStep[0m  [30/106], [94mLoss[0m : 2.44606
[1mStep[0m  [40/106], [94mLoss[0m : 2.25809
[1mStep[0m  [50/106], [94mLoss[0m : 2.54695
[1mStep[0m  [60/106], [94mLoss[0m : 2.50759
[1mStep[0m  [70/106], [94mLoss[0m : 2.49782
[1mStep[0m  [80/106], [94mLoss[0m : 2.27813
[1mStep[0m  [90/106], [94mLoss[0m : 2.34384
[1mStep[0m  [100/106], [94mLoss[0m : 2.53746

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.481, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.42185
[1mStep[0m  [10/106], [94mLoss[0m : 2.55833
[1mStep[0m  [20/106], [94mLoss[0m : 2.64276
[1mStep[0m  [30/106], [94mLoss[0m : 2.51798
[1mStep[0m  [40/106], [94mLoss[0m : 2.29233
[1mStep[0m  [50/106], [94mLoss[0m : 2.38712
[1mStep[0m  [60/106], [94mLoss[0m : 2.35399
[1mStep[0m  [70/106], [94mLoss[0m : 2.40938
[1mStep[0m  [80/106], [94mLoss[0m : 2.50364
[1mStep[0m  [90/106], [94mLoss[0m : 2.39449
[1mStep[0m  [100/106], [94mLoss[0m : 2.44682

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.393, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.22992
[1mStep[0m  [10/106], [94mLoss[0m : 2.52309
[1mStep[0m  [20/106], [94mLoss[0m : 2.45589
[1mStep[0m  [30/106], [94mLoss[0m : 2.39698
[1mStep[0m  [40/106], [94mLoss[0m : 2.39633
[1mStep[0m  [50/106], [94mLoss[0m : 2.73030
[1mStep[0m  [60/106], [94mLoss[0m : 2.62226
[1mStep[0m  [70/106], [94mLoss[0m : 2.36391
[1mStep[0m  [80/106], [94mLoss[0m : 2.18424
[1mStep[0m  [90/106], [94mLoss[0m : 2.47041
[1mStep[0m  [100/106], [94mLoss[0m : 2.13116

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.468, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.37399
[1mStep[0m  [10/106], [94mLoss[0m : 2.37903
[1mStep[0m  [20/106], [94mLoss[0m : 2.17933
[1mStep[0m  [30/106], [94mLoss[0m : 2.72110
[1mStep[0m  [40/106], [94mLoss[0m : 2.58925
[1mStep[0m  [50/106], [94mLoss[0m : 2.30275
[1mStep[0m  [60/106], [94mLoss[0m : 2.74882
[1mStep[0m  [70/106], [94mLoss[0m : 2.63357
[1mStep[0m  [80/106], [94mLoss[0m : 2.44718
[1mStep[0m  [90/106], [94mLoss[0m : 1.99630
[1mStep[0m  [100/106], [94mLoss[0m : 2.50015

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.426, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.45598
[1mStep[0m  [10/106], [94mLoss[0m : 2.53357
[1mStep[0m  [20/106], [94mLoss[0m : 2.16051
[1mStep[0m  [30/106], [94mLoss[0m : 2.51399
[1mStep[0m  [40/106], [94mLoss[0m : 2.16216
[1mStep[0m  [50/106], [94mLoss[0m : 2.30910
[1mStep[0m  [60/106], [94mLoss[0m : 2.44902
[1mStep[0m  [70/106], [94mLoss[0m : 2.51285
[1mStep[0m  [80/106], [94mLoss[0m : 2.36581
[1mStep[0m  [90/106], [94mLoss[0m : 2.04039
[1mStep[0m  [100/106], [94mLoss[0m : 2.34570

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.446, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.13455
[1mStep[0m  [10/106], [94mLoss[0m : 2.10859
[1mStep[0m  [20/106], [94mLoss[0m : 2.37587
[1mStep[0m  [30/106], [94mLoss[0m : 2.36376
[1mStep[0m  [40/106], [94mLoss[0m : 2.35947
[1mStep[0m  [50/106], [94mLoss[0m : 2.74775
[1mStep[0m  [60/106], [94mLoss[0m : 2.77443
[1mStep[0m  [70/106], [94mLoss[0m : 2.92654
[1mStep[0m  [80/106], [94mLoss[0m : 2.13830
[1mStep[0m  [90/106], [94mLoss[0m : 2.38667
[1mStep[0m  [100/106], [94mLoss[0m : 2.40199

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.468, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.37850
[1mStep[0m  [10/106], [94mLoss[0m : 2.20653
[1mStep[0m  [20/106], [94mLoss[0m : 2.52729
[1mStep[0m  [30/106], [94mLoss[0m : 2.46368
[1mStep[0m  [40/106], [94mLoss[0m : 2.43937
[1mStep[0m  [50/106], [94mLoss[0m : 2.60105
[1mStep[0m  [60/106], [94mLoss[0m : 2.47770
[1mStep[0m  [70/106], [94mLoss[0m : 2.29908
[1mStep[0m  [80/106], [94mLoss[0m : 2.53601
[1mStep[0m  [90/106], [94mLoss[0m : 2.68027
[1mStep[0m  [100/106], [94mLoss[0m : 2.60740

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.395, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.28569
[1mStep[0m  [10/106], [94mLoss[0m : 2.64806
[1mStep[0m  [20/106], [94mLoss[0m : 2.31878
[1mStep[0m  [30/106], [94mLoss[0m : 2.76875
[1mStep[0m  [40/106], [94mLoss[0m : 2.39905
[1mStep[0m  [50/106], [94mLoss[0m : 2.34428
[1mStep[0m  [60/106], [94mLoss[0m : 2.38955
[1mStep[0m  [70/106], [94mLoss[0m : 2.68384
[1mStep[0m  [80/106], [94mLoss[0m : 2.50768
[1mStep[0m  [90/106], [94mLoss[0m : 2.08057
[1mStep[0m  [100/106], [94mLoss[0m : 2.58659

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.468, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.67299
[1mStep[0m  [10/106], [94mLoss[0m : 2.35895
[1mStep[0m  [20/106], [94mLoss[0m : 2.29112
[1mStep[0m  [30/106], [94mLoss[0m : 2.23417
[1mStep[0m  [40/106], [94mLoss[0m : 2.53721
[1mStep[0m  [50/106], [94mLoss[0m : 2.53420
[1mStep[0m  [60/106], [94mLoss[0m : 2.95953
[1mStep[0m  [70/106], [94mLoss[0m : 2.56197
[1mStep[0m  [80/106], [94mLoss[0m : 2.50410
[1mStep[0m  [90/106], [94mLoss[0m : 2.28924
[1mStep[0m  [100/106], [94mLoss[0m : 2.39473

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.396, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.47261
[1mStep[0m  [10/106], [94mLoss[0m : 2.36509
[1mStep[0m  [20/106], [94mLoss[0m : 2.20567
[1mStep[0m  [30/106], [94mLoss[0m : 2.50413
[1mStep[0m  [40/106], [94mLoss[0m : 2.41267
[1mStep[0m  [50/106], [94mLoss[0m : 2.31651
[1mStep[0m  [60/106], [94mLoss[0m : 2.27558
[1mStep[0m  [70/106], [94mLoss[0m : 2.60608
[1mStep[0m  [80/106], [94mLoss[0m : 2.51022
[1mStep[0m  [90/106], [94mLoss[0m : 2.31692
[1mStep[0m  [100/106], [94mLoss[0m : 2.20806

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.390, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.29731
[1mStep[0m  [10/106], [94mLoss[0m : 2.32079
[1mStep[0m  [20/106], [94mLoss[0m : 2.46961
[1mStep[0m  [30/106], [94mLoss[0m : 2.76408
[1mStep[0m  [40/106], [94mLoss[0m : 2.30003
[1mStep[0m  [50/106], [94mLoss[0m : 2.45346
[1mStep[0m  [60/106], [94mLoss[0m : 2.40433
[1mStep[0m  [70/106], [94mLoss[0m : 2.22895
[1mStep[0m  [80/106], [94mLoss[0m : 2.32842
[1mStep[0m  [90/106], [94mLoss[0m : 2.08143
[1mStep[0m  [100/106], [94mLoss[0m : 2.57055

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.467, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.36615
[1mStep[0m  [10/106], [94mLoss[0m : 2.46599
[1mStep[0m  [20/106], [94mLoss[0m : 2.54857
[1mStep[0m  [30/106], [94mLoss[0m : 2.60027
[1mStep[0m  [40/106], [94mLoss[0m : 2.60538
[1mStep[0m  [50/106], [94mLoss[0m : 2.28464
[1mStep[0m  [60/106], [94mLoss[0m : 2.41304
[1mStep[0m  [70/106], [94mLoss[0m : 2.46543
[1mStep[0m  [80/106], [94mLoss[0m : 2.34309
[1mStep[0m  [90/106], [94mLoss[0m : 2.45786
[1mStep[0m  [100/106], [94mLoss[0m : 2.43746

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.383, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.30304
[1mStep[0m  [10/106], [94mLoss[0m : 2.38749
[1mStep[0m  [20/106], [94mLoss[0m : 2.46493
[1mStep[0m  [30/106], [94mLoss[0m : 2.43165
[1mStep[0m  [40/106], [94mLoss[0m : 2.29800
[1mStep[0m  [50/106], [94mLoss[0m : 2.45479
[1mStep[0m  [60/106], [94mLoss[0m : 2.28970
[1mStep[0m  [70/106], [94mLoss[0m : 2.42784
[1mStep[0m  [80/106], [94mLoss[0m : 2.80840
[1mStep[0m  [90/106], [94mLoss[0m : 2.29797
[1mStep[0m  [100/106], [94mLoss[0m : 2.35929

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.397, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.04174
[1mStep[0m  [10/106], [94mLoss[0m : 2.28210
[1mStep[0m  [20/106], [94mLoss[0m : 2.67289
[1mStep[0m  [30/106], [94mLoss[0m : 2.35283
[1mStep[0m  [40/106], [94mLoss[0m : 2.62364
[1mStep[0m  [50/106], [94mLoss[0m : 2.26344
[1mStep[0m  [60/106], [94mLoss[0m : 2.40751
[1mStep[0m  [70/106], [94mLoss[0m : 2.55712
[1mStep[0m  [80/106], [94mLoss[0m : 2.16346
[1mStep[0m  [90/106], [94mLoss[0m : 2.38965
[1mStep[0m  [100/106], [94mLoss[0m : 2.58656

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.446, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.65951
[1mStep[0m  [10/106], [94mLoss[0m : 2.83283
[1mStep[0m  [20/106], [94mLoss[0m : 2.29837
[1mStep[0m  [30/106], [94mLoss[0m : 2.30560
[1mStep[0m  [40/106], [94mLoss[0m : 2.49882
[1mStep[0m  [50/106], [94mLoss[0m : 2.14344
[1mStep[0m  [60/106], [94mLoss[0m : 2.40640
[1mStep[0m  [70/106], [94mLoss[0m : 2.09304
[1mStep[0m  [80/106], [94mLoss[0m : 2.41972
[1mStep[0m  [90/106], [94mLoss[0m : 2.21719
[1mStep[0m  [100/106], [94mLoss[0m : 2.47953

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.434, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.53325
[1mStep[0m  [10/106], [94mLoss[0m : 2.19509
[1mStep[0m  [20/106], [94mLoss[0m : 2.11483
[1mStep[0m  [30/106], [94mLoss[0m : 2.37341
[1mStep[0m  [40/106], [94mLoss[0m : 2.83249
[1mStep[0m  [50/106], [94mLoss[0m : 2.22988
[1mStep[0m  [60/106], [94mLoss[0m : 2.55540
[1mStep[0m  [70/106], [94mLoss[0m : 2.47470
[1mStep[0m  [80/106], [94mLoss[0m : 2.33624
[1mStep[0m  [90/106], [94mLoss[0m : 2.21923
[1mStep[0m  [100/106], [94mLoss[0m : 2.31544

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.392, [92mTest[0m: 2.386, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.30762
[1mStep[0m  [10/106], [94mLoss[0m : 2.34387
[1mStep[0m  [20/106], [94mLoss[0m : 2.33206
[1mStep[0m  [30/106], [94mLoss[0m : 2.06898
[1mStep[0m  [40/106], [94mLoss[0m : 2.24011
[1mStep[0m  [50/106], [94mLoss[0m : 2.49839
[1mStep[0m  [60/106], [94mLoss[0m : 2.40320
[1mStep[0m  [70/106], [94mLoss[0m : 2.25512
[1mStep[0m  [80/106], [94mLoss[0m : 2.30165
[1mStep[0m  [90/106], [94mLoss[0m : 2.12806
[1mStep[0m  [100/106], [94mLoss[0m : 2.35725

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.391, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.56570
[1mStep[0m  [10/106], [94mLoss[0m : 2.44051
[1mStep[0m  [20/106], [94mLoss[0m : 2.27211
[1mStep[0m  [30/106], [94mLoss[0m : 2.71631
[1mStep[0m  [40/106], [94mLoss[0m : 2.61832
[1mStep[0m  [50/106], [94mLoss[0m : 2.29131
[1mStep[0m  [60/106], [94mLoss[0m : 2.19711
[1mStep[0m  [70/106], [94mLoss[0m : 2.46524
[1mStep[0m  [80/106], [94mLoss[0m : 2.60758
[1mStep[0m  [90/106], [94mLoss[0m : 2.56207
[1mStep[0m  [100/106], [94mLoss[0m : 2.72173

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.382, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.12415
[1mStep[0m  [10/106], [94mLoss[0m : 2.49966
[1mStep[0m  [20/106], [94mLoss[0m : 2.27389
[1mStep[0m  [30/106], [94mLoss[0m : 2.18633
[1mStep[0m  [40/106], [94mLoss[0m : 2.42116
[1mStep[0m  [50/106], [94mLoss[0m : 2.50588
[1mStep[0m  [60/106], [94mLoss[0m : 2.33447
[1mStep[0m  [70/106], [94mLoss[0m : 2.66921
[1mStep[0m  [80/106], [94mLoss[0m : 2.45609
[1mStep[0m  [90/106], [94mLoss[0m : 2.27026
[1mStep[0m  [100/106], [94mLoss[0m : 2.25922

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.399, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.38407
[1mStep[0m  [10/106], [94mLoss[0m : 2.35259
[1mStep[0m  [20/106], [94mLoss[0m : 2.54916
[1mStep[0m  [30/106], [94mLoss[0m : 2.59676
[1mStep[0m  [40/106], [94mLoss[0m : 2.53120
[1mStep[0m  [50/106], [94mLoss[0m : 2.23321
[1mStep[0m  [60/106], [94mLoss[0m : 2.25649
[1mStep[0m  [70/106], [94mLoss[0m : 2.52280
[1mStep[0m  [80/106], [94mLoss[0m : 2.69184
[1mStep[0m  [90/106], [94mLoss[0m : 2.55101
[1mStep[0m  [100/106], [94mLoss[0m : 2.41153

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.399, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.29810
[1mStep[0m  [10/106], [94mLoss[0m : 2.50846
[1mStep[0m  [20/106], [94mLoss[0m : 2.40556
[1mStep[0m  [30/106], [94mLoss[0m : 2.34304
[1mStep[0m  [40/106], [94mLoss[0m : 2.06314
[1mStep[0m  [50/106], [94mLoss[0m : 2.75515
[1mStep[0m  [60/106], [94mLoss[0m : 2.50821
[1mStep[0m  [70/106], [94mLoss[0m : 2.37497
[1mStep[0m  [80/106], [94mLoss[0m : 2.45667
[1mStep[0m  [90/106], [94mLoss[0m : 2.19377
[1mStep[0m  [100/106], [94mLoss[0m : 2.19994

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.398, [92mTest[0m: 2.377, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.61235
[1mStep[0m  [10/106], [94mLoss[0m : 2.52596
[1mStep[0m  [20/106], [94mLoss[0m : 2.68696
[1mStep[0m  [30/106], [94mLoss[0m : 2.13924
[1mStep[0m  [40/106], [94mLoss[0m : 2.65878
[1mStep[0m  [50/106], [94mLoss[0m : 2.51623
[1mStep[0m  [60/106], [94mLoss[0m : 2.38340
[1mStep[0m  [70/106], [94mLoss[0m : 2.26634
[1mStep[0m  [80/106], [94mLoss[0m : 2.55957
[1mStep[0m  [90/106], [94mLoss[0m : 2.55993
[1mStep[0m  [100/106], [94mLoss[0m : 2.31213

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.381, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.37766
[1mStep[0m  [10/106], [94mLoss[0m : 2.46551
[1mStep[0m  [20/106], [94mLoss[0m : 2.55045
[1mStep[0m  [30/106], [94mLoss[0m : 1.97557
[1mStep[0m  [40/106], [94mLoss[0m : 2.62823
[1mStep[0m  [50/106], [94mLoss[0m : 2.32294
[1mStep[0m  [60/106], [94mLoss[0m : 2.41065
[1mStep[0m  [70/106], [94mLoss[0m : 2.23811
[1mStep[0m  [80/106], [94mLoss[0m : 2.29706
[1mStep[0m  [90/106], [94mLoss[0m : 2.40878
[1mStep[0m  [100/106], [94mLoss[0m : 2.35624

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.394, [92mTest[0m: 2.398, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.25817
[1mStep[0m  [10/106], [94mLoss[0m : 2.64071
[1mStep[0m  [20/106], [94mLoss[0m : 2.34818
[1mStep[0m  [30/106], [94mLoss[0m : 2.35498
[1mStep[0m  [40/106], [94mLoss[0m : 2.21572
[1mStep[0m  [50/106], [94mLoss[0m : 2.40450
[1mStep[0m  [60/106], [94mLoss[0m : 2.49512
[1mStep[0m  [70/106], [94mLoss[0m : 2.53224
[1mStep[0m  [80/106], [94mLoss[0m : 2.62239
[1mStep[0m  [90/106], [94mLoss[0m : 2.20268
[1mStep[0m  [100/106], [94mLoss[0m : 2.26368

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.394, [92mTest[0m: 2.384, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.397
====================================

Phase 1 - Evaluation MAE:  2.397040025243219
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/106], [94mLoss[0m : 2.61578
[1mStep[0m  [10/106], [94mLoss[0m : 2.18334
[1mStep[0m  [20/106], [94mLoss[0m : 2.43898
[1mStep[0m  [30/106], [94mLoss[0m : 2.23639
[1mStep[0m  [40/106], [94mLoss[0m : 2.23679
[1mStep[0m  [50/106], [94mLoss[0m : 2.40653
[1mStep[0m  [60/106], [94mLoss[0m : 2.39648
[1mStep[0m  [70/106], [94mLoss[0m : 2.41632
[1mStep[0m  [80/106], [94mLoss[0m : 2.67029
[1mStep[0m  [90/106], [94mLoss[0m : 2.25612
[1mStep[0m  [100/106], [94mLoss[0m : 2.43347

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.395, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.25815
[1mStep[0m  [10/106], [94mLoss[0m : 2.37277
[1mStep[0m  [20/106], [94mLoss[0m : 2.47124
[1mStep[0m  [30/106], [94mLoss[0m : 2.39812
[1mStep[0m  [40/106], [94mLoss[0m : 2.34562
[1mStep[0m  [50/106], [94mLoss[0m : 2.06992
[1mStep[0m  [60/106], [94mLoss[0m : 2.16655
[1mStep[0m  [70/106], [94mLoss[0m : 2.61228
[1mStep[0m  [80/106], [94mLoss[0m : 2.57758
[1mStep[0m  [90/106], [94mLoss[0m : 2.23378
[1mStep[0m  [100/106], [94mLoss[0m : 2.32460

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.309, [92mTest[0m: 2.440, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.13780
[1mStep[0m  [10/106], [94mLoss[0m : 1.68264
[1mStep[0m  [20/106], [94mLoss[0m : 1.86854
[1mStep[0m  [30/106], [94mLoss[0m : 2.29108
[1mStep[0m  [40/106], [94mLoss[0m : 2.00276
[1mStep[0m  [50/106], [94mLoss[0m : 2.74148
[1mStep[0m  [60/106], [94mLoss[0m : 2.36394
[1mStep[0m  [70/106], [94mLoss[0m : 2.18029
[1mStep[0m  [80/106], [94mLoss[0m : 2.06272
[1mStep[0m  [90/106], [94mLoss[0m : 2.22553
[1mStep[0m  [100/106], [94mLoss[0m : 2.33807

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.211, [92mTest[0m: 2.387, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.16295
[1mStep[0m  [10/106], [94mLoss[0m : 2.18300
[1mStep[0m  [20/106], [94mLoss[0m : 2.45787
[1mStep[0m  [30/106], [94mLoss[0m : 1.91058
[1mStep[0m  [40/106], [94mLoss[0m : 2.20692
[1mStep[0m  [50/106], [94mLoss[0m : 2.13586
[1mStep[0m  [60/106], [94mLoss[0m : 1.99875
[1mStep[0m  [70/106], [94mLoss[0m : 2.18545
[1mStep[0m  [80/106], [94mLoss[0m : 2.29437
[1mStep[0m  [90/106], [94mLoss[0m : 2.20953
[1mStep[0m  [100/106], [94mLoss[0m : 2.47639

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.146, [92mTest[0m: 2.371, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.94355
[1mStep[0m  [10/106], [94mLoss[0m : 2.26143
[1mStep[0m  [20/106], [94mLoss[0m : 1.62714
[1mStep[0m  [30/106], [94mLoss[0m : 2.01159
[1mStep[0m  [40/106], [94mLoss[0m : 2.04196
[1mStep[0m  [50/106], [94mLoss[0m : 2.27497
[1mStep[0m  [60/106], [94mLoss[0m : 2.29502
[1mStep[0m  [70/106], [94mLoss[0m : 2.10333
[1mStep[0m  [80/106], [94mLoss[0m : 2.31684
[1mStep[0m  [90/106], [94mLoss[0m : 1.97354
[1mStep[0m  [100/106], [94mLoss[0m : 1.92416

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.056, [92mTest[0m: 2.371, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.86319
[1mStep[0m  [10/106], [94mLoss[0m : 1.88893
[1mStep[0m  [20/106], [94mLoss[0m : 1.80535
[1mStep[0m  [30/106], [94mLoss[0m : 2.28945
[1mStep[0m  [40/106], [94mLoss[0m : 1.85651
[1mStep[0m  [50/106], [94mLoss[0m : 1.90079
[1mStep[0m  [60/106], [94mLoss[0m : 2.19722
[1mStep[0m  [70/106], [94mLoss[0m : 1.85411
[1mStep[0m  [80/106], [94mLoss[0m : 1.91547
[1mStep[0m  [90/106], [94mLoss[0m : 1.94159
[1mStep[0m  [100/106], [94mLoss[0m : 1.96492

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.020, [92mTest[0m: 2.402, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.89598
[1mStep[0m  [10/106], [94mLoss[0m : 2.08300
[1mStep[0m  [20/106], [94mLoss[0m : 1.71418
[1mStep[0m  [30/106], [94mLoss[0m : 1.86516
[1mStep[0m  [40/106], [94mLoss[0m : 1.89609
[1mStep[0m  [50/106], [94mLoss[0m : 1.74989
[1mStep[0m  [60/106], [94mLoss[0m : 1.89596
[1mStep[0m  [70/106], [94mLoss[0m : 2.13042
[1mStep[0m  [80/106], [94mLoss[0m : 2.03170
[1mStep[0m  [90/106], [94mLoss[0m : 1.98554
[1mStep[0m  [100/106], [94mLoss[0m : 1.78035

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.920, [92mTest[0m: 2.471, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.74541
[1mStep[0m  [10/106], [94mLoss[0m : 1.58959
[1mStep[0m  [20/106], [94mLoss[0m : 1.97400
[1mStep[0m  [30/106], [94mLoss[0m : 2.09079
[1mStep[0m  [40/106], [94mLoss[0m : 2.18005
[1mStep[0m  [50/106], [94mLoss[0m : 1.78637
[1mStep[0m  [60/106], [94mLoss[0m : 2.14120
[1mStep[0m  [70/106], [94mLoss[0m : 1.89566
[1mStep[0m  [80/106], [94mLoss[0m : 1.97972
[1mStep[0m  [90/106], [94mLoss[0m : 2.08790
[1mStep[0m  [100/106], [94mLoss[0m : 1.91014

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.894, [92mTest[0m: 2.447, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.94556
[1mStep[0m  [10/106], [94mLoss[0m : 1.73598
[1mStep[0m  [20/106], [94mLoss[0m : 1.86530
[1mStep[0m  [30/106], [94mLoss[0m : 1.94797
[1mStep[0m  [40/106], [94mLoss[0m : 1.78978
[1mStep[0m  [50/106], [94mLoss[0m : 1.74404
[1mStep[0m  [60/106], [94mLoss[0m : 1.87759
[1mStep[0m  [70/106], [94mLoss[0m : 2.01379
[1mStep[0m  [80/106], [94mLoss[0m : 2.00278
[1mStep[0m  [90/106], [94mLoss[0m : 1.90801
[1mStep[0m  [100/106], [94mLoss[0m : 1.81027

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.839, [92mTest[0m: 2.472, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.58676
[1mStep[0m  [10/106], [94mLoss[0m : 1.72717
[1mStep[0m  [20/106], [94mLoss[0m : 1.59742
[1mStep[0m  [30/106], [94mLoss[0m : 1.97589
[1mStep[0m  [40/106], [94mLoss[0m : 1.44978
[1mStep[0m  [50/106], [94mLoss[0m : 1.85363
[1mStep[0m  [60/106], [94mLoss[0m : 2.21125
[1mStep[0m  [70/106], [94mLoss[0m : 1.74444
[1mStep[0m  [80/106], [94mLoss[0m : 1.63590
[1mStep[0m  [90/106], [94mLoss[0m : 1.93691
[1mStep[0m  [100/106], [94mLoss[0m : 2.01056

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.778, [92mTest[0m: 2.428, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.55868
[1mStep[0m  [10/106], [94mLoss[0m : 1.68415
[1mStep[0m  [20/106], [94mLoss[0m : 1.70404
[1mStep[0m  [30/106], [94mLoss[0m : 1.75220
[1mStep[0m  [40/106], [94mLoss[0m : 1.40168
[1mStep[0m  [50/106], [94mLoss[0m : 1.43962
[1mStep[0m  [60/106], [94mLoss[0m : 1.84367
[1mStep[0m  [70/106], [94mLoss[0m : 1.85604
[1mStep[0m  [80/106], [94mLoss[0m : 1.68158
[1mStep[0m  [90/106], [94mLoss[0m : 1.94585
[1mStep[0m  [100/106], [94mLoss[0m : 1.95221

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.717, [92mTest[0m: 2.555, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.58310
[1mStep[0m  [10/106], [94mLoss[0m : 2.04248
[1mStep[0m  [20/106], [94mLoss[0m : 1.55948
[1mStep[0m  [30/106], [94mLoss[0m : 1.49254
[1mStep[0m  [40/106], [94mLoss[0m : 1.60647
[1mStep[0m  [50/106], [94mLoss[0m : 1.71346
[1mStep[0m  [60/106], [94mLoss[0m : 1.78801
[1mStep[0m  [70/106], [94mLoss[0m : 1.66408
[1mStep[0m  [80/106], [94mLoss[0m : 1.78794
[1mStep[0m  [90/106], [94mLoss[0m : 1.81698
[1mStep[0m  [100/106], [94mLoss[0m : 1.93500

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.703, [92mTest[0m: 2.502, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.87104
[1mStep[0m  [10/106], [94mLoss[0m : 1.67058
[1mStep[0m  [20/106], [94mLoss[0m : 1.75958
[1mStep[0m  [30/106], [94mLoss[0m : 1.56350
[1mStep[0m  [40/106], [94mLoss[0m : 1.55391
[1mStep[0m  [50/106], [94mLoss[0m : 1.56720
[1mStep[0m  [60/106], [94mLoss[0m : 1.76276
[1mStep[0m  [70/106], [94mLoss[0m : 1.67932
[1mStep[0m  [80/106], [94mLoss[0m : 1.77056
[1mStep[0m  [90/106], [94mLoss[0m : 1.89292
[1mStep[0m  [100/106], [94mLoss[0m : 1.85175

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.677, [92mTest[0m: 2.528, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.31666
[1mStep[0m  [10/106], [94mLoss[0m : 1.85468
[1mStep[0m  [20/106], [94mLoss[0m : 1.80334
[1mStep[0m  [30/106], [94mLoss[0m : 1.77257
[1mStep[0m  [40/106], [94mLoss[0m : 1.66541
[1mStep[0m  [50/106], [94mLoss[0m : 1.49833
[1mStep[0m  [60/106], [94mLoss[0m : 1.57645
[1mStep[0m  [70/106], [94mLoss[0m : 1.58130
[1mStep[0m  [80/106], [94mLoss[0m : 1.74530
[1mStep[0m  [90/106], [94mLoss[0m : 1.56506
[1mStep[0m  [100/106], [94mLoss[0m : 1.54572

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.620, [92mTest[0m: 2.533, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.63069
[1mStep[0m  [10/106], [94mLoss[0m : 1.45334
[1mStep[0m  [20/106], [94mLoss[0m : 1.39242
[1mStep[0m  [30/106], [94mLoss[0m : 1.63668
[1mStep[0m  [40/106], [94mLoss[0m : 1.62757
[1mStep[0m  [50/106], [94mLoss[0m : 1.60283
[1mStep[0m  [60/106], [94mLoss[0m : 1.64231
[1mStep[0m  [70/106], [94mLoss[0m : 1.80012
[1mStep[0m  [80/106], [94mLoss[0m : 1.65936
[1mStep[0m  [90/106], [94mLoss[0m : 1.41432
[1mStep[0m  [100/106], [94mLoss[0m : 1.70993

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.567, [92mTest[0m: 2.475, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.42065
[1mStep[0m  [10/106], [94mLoss[0m : 1.60285
[1mStep[0m  [20/106], [94mLoss[0m : 1.62980
[1mStep[0m  [30/106], [94mLoss[0m : 1.79710
[1mStep[0m  [40/106], [94mLoss[0m : 1.52520
[1mStep[0m  [50/106], [94mLoss[0m : 1.71580
[1mStep[0m  [60/106], [94mLoss[0m : 1.49430
[1mStep[0m  [70/106], [94mLoss[0m : 1.54025
[1mStep[0m  [80/106], [94mLoss[0m : 1.69829
[1mStep[0m  [90/106], [94mLoss[0m : 1.59063
[1mStep[0m  [100/106], [94mLoss[0m : 1.54189

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.538, [92mTest[0m: 2.463, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.45875
[1mStep[0m  [10/106], [94mLoss[0m : 1.41486
[1mStep[0m  [20/106], [94mLoss[0m : 1.42148
[1mStep[0m  [30/106], [94mLoss[0m : 1.18931
[1mStep[0m  [40/106], [94mLoss[0m : 1.22883
[1mStep[0m  [50/106], [94mLoss[0m : 1.51142
[1mStep[0m  [60/106], [94mLoss[0m : 1.52548
[1mStep[0m  [70/106], [94mLoss[0m : 1.56066
[1mStep[0m  [80/106], [94mLoss[0m : 1.47505
[1mStep[0m  [90/106], [94mLoss[0m : 1.38833
[1mStep[0m  [100/106], [94mLoss[0m : 1.62836

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.518, [92mTest[0m: 2.486, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.30117
[1mStep[0m  [10/106], [94mLoss[0m : 1.29622
[1mStep[0m  [20/106], [94mLoss[0m : 1.53098
[1mStep[0m  [30/106], [94mLoss[0m : 1.47711
[1mStep[0m  [40/106], [94mLoss[0m : 1.35937
[1mStep[0m  [50/106], [94mLoss[0m : 1.70588
[1mStep[0m  [60/106], [94mLoss[0m : 1.60983
[1mStep[0m  [70/106], [94mLoss[0m : 1.56349
[1mStep[0m  [80/106], [94mLoss[0m : 1.43605
[1mStep[0m  [90/106], [94mLoss[0m : 1.52224
[1mStep[0m  [100/106], [94mLoss[0m : 1.43940

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.479, [92mTest[0m: 2.491, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.35465
[1mStep[0m  [10/106], [94mLoss[0m : 1.72572
[1mStep[0m  [20/106], [94mLoss[0m : 1.53131
[1mStep[0m  [30/106], [94mLoss[0m : 1.46741
[1mStep[0m  [40/106], [94mLoss[0m : 1.62479
[1mStep[0m  [50/106], [94mLoss[0m : 1.40000
[1mStep[0m  [60/106], [94mLoss[0m : 1.42869
[1mStep[0m  [70/106], [94mLoss[0m : 1.74608
[1mStep[0m  [80/106], [94mLoss[0m : 1.53927
[1mStep[0m  [90/106], [94mLoss[0m : 1.55837
[1mStep[0m  [100/106], [94mLoss[0m : 1.43464

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.479, [92mTest[0m: 2.600, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.44846
[1mStep[0m  [10/106], [94mLoss[0m : 1.12016
[1mStep[0m  [20/106], [94mLoss[0m : 1.24138
[1mStep[0m  [30/106], [94mLoss[0m : 1.49169
[1mStep[0m  [40/106], [94mLoss[0m : 1.40951
[1mStep[0m  [50/106], [94mLoss[0m : 1.34072
[1mStep[0m  [60/106], [94mLoss[0m : 1.58995
[1mStep[0m  [70/106], [94mLoss[0m : 1.43532
[1mStep[0m  [80/106], [94mLoss[0m : 1.54210
[1mStep[0m  [90/106], [94mLoss[0m : 1.43399
[1mStep[0m  [100/106], [94mLoss[0m : 1.55850

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.423, [92mTest[0m: 2.580, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.30019
[1mStep[0m  [10/106], [94mLoss[0m : 1.29292
[1mStep[0m  [20/106], [94mLoss[0m : 1.24520
[1mStep[0m  [30/106], [94mLoss[0m : 1.38584
[1mStep[0m  [40/106], [94mLoss[0m : 1.37546
[1mStep[0m  [50/106], [94mLoss[0m : 1.56517
[1mStep[0m  [60/106], [94mLoss[0m : 1.42420
[1mStep[0m  [70/106], [94mLoss[0m : 1.10530
[1mStep[0m  [80/106], [94mLoss[0m : 1.31934
[1mStep[0m  [90/106], [94mLoss[0m : 1.40827
[1mStep[0m  [100/106], [94mLoss[0m : 1.56325

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.361, [92mTest[0m: 2.611, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.29501
[1mStep[0m  [10/106], [94mLoss[0m : 1.35579
[1mStep[0m  [20/106], [94mLoss[0m : 1.24605
[1mStep[0m  [30/106], [94mLoss[0m : 1.30060
[1mStep[0m  [40/106], [94mLoss[0m : 1.27566
[1mStep[0m  [50/106], [94mLoss[0m : 1.20907
[1mStep[0m  [60/106], [94mLoss[0m : 1.22944
[1mStep[0m  [70/106], [94mLoss[0m : 1.37564
[1mStep[0m  [80/106], [94mLoss[0m : 1.33121
[1mStep[0m  [90/106], [94mLoss[0m : 1.35327
[1mStep[0m  [100/106], [94mLoss[0m : 1.26143

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.334, [92mTest[0m: 2.502, [96mlr[0m: 0.004545
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 21 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.574
====================================

Phase 2 - Evaluation MAE:  2.5739686129228123
MAE score P1       2.39704
MAE score P2      2.573969
loss              1.333873
learning_rate      0.00505
batch_size             128
hidden_sizes         [300]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping        True
dropout                0.2
momentum               0.9
weight_decay        0.0001
Name: 5, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/213], [94mLoss[0m : 10.85663
[1mStep[0m  [21/213], [94mLoss[0m : 5.93384
[1mStep[0m  [42/213], [94mLoss[0m : 3.05336
[1mStep[0m  [63/213], [94mLoss[0m : 2.51103
[1mStep[0m  [84/213], [94mLoss[0m : 2.85549
[1mStep[0m  [105/213], [94mLoss[0m : 2.90372
[1mStep[0m  [126/213], [94mLoss[0m : 2.70143
[1mStep[0m  [147/213], [94mLoss[0m : 2.64003
[1mStep[0m  [168/213], [94mLoss[0m : 2.25035
[1mStep[0m  [189/213], [94mLoss[0m : 2.49979
[1mStep[0m  [210/213], [94mLoss[0m : 2.70645

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.342, [92mTest[0m: 10.810, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.58445
[1mStep[0m  [21/213], [94mLoss[0m : 2.37135
[1mStep[0m  [42/213], [94mLoss[0m : 2.47257
[1mStep[0m  [63/213], [94mLoss[0m : 2.70535
[1mStep[0m  [84/213], [94mLoss[0m : 2.75376
[1mStep[0m  [105/213], [94mLoss[0m : 3.13579
[1mStep[0m  [126/213], [94mLoss[0m : 2.65035
[1mStep[0m  [147/213], [94mLoss[0m : 2.35313
[1mStep[0m  [168/213], [94mLoss[0m : 2.48214
[1mStep[0m  [189/213], [94mLoss[0m : 3.01511
[1mStep[0m  [210/213], [94mLoss[0m : 2.40137

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.556, [92mTest[0m: 2.445, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.90283
[1mStep[0m  [21/213], [94mLoss[0m : 2.42271
[1mStep[0m  [42/213], [94mLoss[0m : 2.57085
[1mStep[0m  [63/213], [94mLoss[0m : 2.69882
[1mStep[0m  [84/213], [94mLoss[0m : 2.34636
[1mStep[0m  [105/213], [94mLoss[0m : 2.45503
[1mStep[0m  [126/213], [94mLoss[0m : 2.27718
[1mStep[0m  [147/213], [94mLoss[0m : 2.59066
[1mStep[0m  [168/213], [94mLoss[0m : 2.50875
[1mStep[0m  [189/213], [94mLoss[0m : 2.59964
[1mStep[0m  [210/213], [94mLoss[0m : 2.45243

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.526, [92mTest[0m: 2.443, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.38210
[1mStep[0m  [21/213], [94mLoss[0m : 2.13958
[1mStep[0m  [42/213], [94mLoss[0m : 2.56966
[1mStep[0m  [63/213], [94mLoss[0m : 2.26894
[1mStep[0m  [84/213], [94mLoss[0m : 2.17594
[1mStep[0m  [105/213], [94mLoss[0m : 2.83376
[1mStep[0m  [126/213], [94mLoss[0m : 2.72852
[1mStep[0m  [147/213], [94mLoss[0m : 2.18882
[1mStep[0m  [168/213], [94mLoss[0m : 1.96401
[1mStep[0m  [189/213], [94mLoss[0m : 2.20265
[1mStep[0m  [210/213], [94mLoss[0m : 2.41498

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.529, [92mTest[0m: 2.435, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.58554
[1mStep[0m  [21/213], [94mLoss[0m : 2.17742
[1mStep[0m  [42/213], [94mLoss[0m : 2.30012
[1mStep[0m  [63/213], [94mLoss[0m : 2.68844
[1mStep[0m  [84/213], [94mLoss[0m : 2.22585
[1mStep[0m  [105/213], [94mLoss[0m : 2.29695
[1mStep[0m  [126/213], [94mLoss[0m : 2.75140
[1mStep[0m  [147/213], [94mLoss[0m : 2.81033
[1mStep[0m  [168/213], [94mLoss[0m : 2.73686
[1mStep[0m  [189/213], [94mLoss[0m : 2.74771
[1mStep[0m  [210/213], [94mLoss[0m : 2.43919

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.417, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.21120
[1mStep[0m  [21/213], [94mLoss[0m : 2.47298
[1mStep[0m  [42/213], [94mLoss[0m : 2.70544
[1mStep[0m  [63/213], [94mLoss[0m : 2.59867
[1mStep[0m  [84/213], [94mLoss[0m : 2.56451
[1mStep[0m  [105/213], [94mLoss[0m : 2.27529
[1mStep[0m  [126/213], [94mLoss[0m : 2.83224
[1mStep[0m  [147/213], [94mLoss[0m : 3.15095
[1mStep[0m  [168/213], [94mLoss[0m : 3.22358
[1mStep[0m  [189/213], [94mLoss[0m : 2.57152
[1mStep[0m  [210/213], [94mLoss[0m : 2.70720

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.411, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.61555
[1mStep[0m  [21/213], [94mLoss[0m : 2.46734
[1mStep[0m  [42/213], [94mLoss[0m : 2.25351
[1mStep[0m  [63/213], [94mLoss[0m : 2.86488
[1mStep[0m  [84/213], [94mLoss[0m : 2.50130
[1mStep[0m  [105/213], [94mLoss[0m : 2.28073
[1mStep[0m  [126/213], [94mLoss[0m : 2.49786
[1mStep[0m  [147/213], [94mLoss[0m : 2.16090
[1mStep[0m  [168/213], [94mLoss[0m : 2.48498
[1mStep[0m  [189/213], [94mLoss[0m : 2.56613
[1mStep[0m  [210/213], [94mLoss[0m : 2.81752

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.418, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.59673
[1mStep[0m  [21/213], [94mLoss[0m : 2.94631
[1mStep[0m  [42/213], [94mLoss[0m : 2.62267
[1mStep[0m  [63/213], [94mLoss[0m : 2.45185
[1mStep[0m  [84/213], [94mLoss[0m : 2.50094
[1mStep[0m  [105/213], [94mLoss[0m : 2.64297
[1mStep[0m  [126/213], [94mLoss[0m : 2.44872
[1mStep[0m  [147/213], [94mLoss[0m : 2.53609
[1mStep[0m  [168/213], [94mLoss[0m : 2.90620
[1mStep[0m  [189/213], [94mLoss[0m : 2.21004
[1mStep[0m  [210/213], [94mLoss[0m : 2.50729

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.416, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.17573
[1mStep[0m  [21/213], [94mLoss[0m : 2.64080
[1mStep[0m  [42/213], [94mLoss[0m : 2.98575
[1mStep[0m  [63/213], [94mLoss[0m : 2.14729
[1mStep[0m  [84/213], [94mLoss[0m : 2.16589
[1mStep[0m  [105/213], [94mLoss[0m : 2.28873
[1mStep[0m  [126/213], [94mLoss[0m : 2.63775
[1mStep[0m  [147/213], [94mLoss[0m : 2.51662
[1mStep[0m  [168/213], [94mLoss[0m : 2.66103
[1mStep[0m  [189/213], [94mLoss[0m : 2.54143
[1mStep[0m  [210/213], [94mLoss[0m : 2.54644

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.405, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.62673
[1mStep[0m  [21/213], [94mLoss[0m : 2.06500
[1mStep[0m  [42/213], [94mLoss[0m : 2.27846
[1mStep[0m  [63/213], [94mLoss[0m : 2.75210
[1mStep[0m  [84/213], [94mLoss[0m : 3.01461
[1mStep[0m  [105/213], [94mLoss[0m : 2.92833
[1mStep[0m  [126/213], [94mLoss[0m : 2.81745
[1mStep[0m  [147/213], [94mLoss[0m : 2.84777
[1mStep[0m  [168/213], [94mLoss[0m : 3.12040
[1mStep[0m  [189/213], [94mLoss[0m : 2.74484
[1mStep[0m  [210/213], [94mLoss[0m : 2.61327

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.400, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.81575
[1mStep[0m  [21/213], [94mLoss[0m : 2.50891
[1mStep[0m  [42/213], [94mLoss[0m : 2.38789
[1mStep[0m  [63/213], [94mLoss[0m : 2.47224
[1mStep[0m  [84/213], [94mLoss[0m : 2.48326
[1mStep[0m  [105/213], [94mLoss[0m : 3.11632
[1mStep[0m  [126/213], [94mLoss[0m : 2.61482
[1mStep[0m  [147/213], [94mLoss[0m : 2.27416
[1mStep[0m  [168/213], [94mLoss[0m : 2.37342
[1mStep[0m  [189/213], [94mLoss[0m : 2.42760
[1mStep[0m  [210/213], [94mLoss[0m : 2.29843

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.399, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.92658
[1mStep[0m  [21/213], [94mLoss[0m : 2.57054
[1mStep[0m  [42/213], [94mLoss[0m : 2.52633
[1mStep[0m  [63/213], [94mLoss[0m : 2.58352
[1mStep[0m  [84/213], [94mLoss[0m : 2.33039
[1mStep[0m  [105/213], [94mLoss[0m : 2.29976
[1mStep[0m  [126/213], [94mLoss[0m : 2.78309
[1mStep[0m  [147/213], [94mLoss[0m : 2.69624
[1mStep[0m  [168/213], [94mLoss[0m : 2.16879
[1mStep[0m  [189/213], [94mLoss[0m : 2.83900
[1mStep[0m  [210/213], [94mLoss[0m : 2.30025

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.399, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.27149
[1mStep[0m  [21/213], [94mLoss[0m : 2.26594
[1mStep[0m  [42/213], [94mLoss[0m : 2.67077
[1mStep[0m  [63/213], [94mLoss[0m : 2.40977
[1mStep[0m  [84/213], [94mLoss[0m : 2.52295
[1mStep[0m  [105/213], [94mLoss[0m : 2.04269
[1mStep[0m  [126/213], [94mLoss[0m : 2.19347
[1mStep[0m  [147/213], [94mLoss[0m : 2.29162
[1mStep[0m  [168/213], [94mLoss[0m : 2.44829
[1mStep[0m  [189/213], [94mLoss[0m : 2.26239
[1mStep[0m  [210/213], [94mLoss[0m : 2.37643

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.400, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.37896
[1mStep[0m  [21/213], [94mLoss[0m : 2.69110
[1mStep[0m  [42/213], [94mLoss[0m : 2.50648
[1mStep[0m  [63/213], [94mLoss[0m : 2.58740
[1mStep[0m  [84/213], [94mLoss[0m : 2.44513
[1mStep[0m  [105/213], [94mLoss[0m : 2.54206
[1mStep[0m  [126/213], [94mLoss[0m : 2.38399
[1mStep[0m  [147/213], [94mLoss[0m : 2.46751
[1mStep[0m  [168/213], [94mLoss[0m : 2.66227
[1mStep[0m  [189/213], [94mLoss[0m : 2.20065
[1mStep[0m  [210/213], [94mLoss[0m : 2.43551

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.406, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.61159
[1mStep[0m  [21/213], [94mLoss[0m : 2.37991
[1mStep[0m  [42/213], [94mLoss[0m : 3.21395
[1mStep[0m  [63/213], [94mLoss[0m : 2.24585
[1mStep[0m  [84/213], [94mLoss[0m : 2.82173
[1mStep[0m  [105/213], [94mLoss[0m : 2.38782
[1mStep[0m  [126/213], [94mLoss[0m : 2.50781
[1mStep[0m  [147/213], [94mLoss[0m : 2.47854
[1mStep[0m  [168/213], [94mLoss[0m : 2.07281
[1mStep[0m  [189/213], [94mLoss[0m : 2.11513
[1mStep[0m  [210/213], [94mLoss[0m : 2.35732

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.407, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.22222
[1mStep[0m  [21/213], [94mLoss[0m : 2.74207
[1mStep[0m  [42/213], [94mLoss[0m : 2.19884
[1mStep[0m  [63/213], [94mLoss[0m : 2.54189
[1mStep[0m  [84/213], [94mLoss[0m : 2.21095
[1mStep[0m  [105/213], [94mLoss[0m : 2.24989
[1mStep[0m  [126/213], [94mLoss[0m : 2.62373
[1mStep[0m  [147/213], [94mLoss[0m : 2.28493
[1mStep[0m  [168/213], [94mLoss[0m : 2.31920
[1mStep[0m  [189/213], [94mLoss[0m : 2.44088
[1mStep[0m  [210/213], [94mLoss[0m : 2.63373

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.409, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.52270
[1mStep[0m  [21/213], [94mLoss[0m : 2.49681
[1mStep[0m  [42/213], [94mLoss[0m : 2.46257
[1mStep[0m  [63/213], [94mLoss[0m : 2.01887
[1mStep[0m  [84/213], [94mLoss[0m : 1.95463
[1mStep[0m  [105/213], [94mLoss[0m : 2.46694
[1mStep[0m  [126/213], [94mLoss[0m : 2.38482
[1mStep[0m  [147/213], [94mLoss[0m : 2.53883
[1mStep[0m  [168/213], [94mLoss[0m : 1.96857
[1mStep[0m  [189/213], [94mLoss[0m : 2.40587
[1mStep[0m  [210/213], [94mLoss[0m : 2.80078

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.390, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.62594
[1mStep[0m  [21/213], [94mLoss[0m : 2.13570
[1mStep[0m  [42/213], [94mLoss[0m : 2.39149
[1mStep[0m  [63/213], [94mLoss[0m : 2.22437
[1mStep[0m  [84/213], [94mLoss[0m : 2.55261
[1mStep[0m  [105/213], [94mLoss[0m : 2.62129
[1mStep[0m  [126/213], [94mLoss[0m : 2.35074
[1mStep[0m  [147/213], [94mLoss[0m : 2.62929
[1mStep[0m  [168/213], [94mLoss[0m : 2.89265
[1mStep[0m  [189/213], [94mLoss[0m : 2.51169
[1mStep[0m  [210/213], [94mLoss[0m : 2.27740

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.405, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.19255
[1mStep[0m  [21/213], [94mLoss[0m : 2.01599
[1mStep[0m  [42/213], [94mLoss[0m : 2.36546
[1mStep[0m  [63/213], [94mLoss[0m : 2.77689
[1mStep[0m  [84/213], [94mLoss[0m : 2.78996
[1mStep[0m  [105/213], [94mLoss[0m : 2.10173
[1mStep[0m  [126/213], [94mLoss[0m : 2.54240
[1mStep[0m  [147/213], [94mLoss[0m : 2.43694
[1mStep[0m  [168/213], [94mLoss[0m : 2.67370
[1mStep[0m  [189/213], [94mLoss[0m : 2.46327
[1mStep[0m  [210/213], [94mLoss[0m : 2.69848

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.410, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.70939
[1mStep[0m  [21/213], [94mLoss[0m : 2.23975
[1mStep[0m  [42/213], [94mLoss[0m : 2.72304
[1mStep[0m  [63/213], [94mLoss[0m : 2.40814
[1mStep[0m  [84/213], [94mLoss[0m : 2.42209
[1mStep[0m  [105/213], [94mLoss[0m : 2.59890
[1mStep[0m  [126/213], [94mLoss[0m : 2.29572
[1mStep[0m  [147/213], [94mLoss[0m : 2.36733
[1mStep[0m  [168/213], [94mLoss[0m : 2.65795
[1mStep[0m  [189/213], [94mLoss[0m : 2.55810
[1mStep[0m  [210/213], [94mLoss[0m : 2.48451

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.398, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.53015
[1mStep[0m  [21/213], [94mLoss[0m : 2.46280
[1mStep[0m  [42/213], [94mLoss[0m : 2.92401
[1mStep[0m  [63/213], [94mLoss[0m : 2.48136
[1mStep[0m  [84/213], [94mLoss[0m : 2.77750
[1mStep[0m  [105/213], [94mLoss[0m : 2.84780
[1mStep[0m  [126/213], [94mLoss[0m : 2.81854
[1mStep[0m  [147/213], [94mLoss[0m : 2.49209
[1mStep[0m  [168/213], [94mLoss[0m : 2.49475
[1mStep[0m  [189/213], [94mLoss[0m : 2.44386
[1mStep[0m  [210/213], [94mLoss[0m : 2.54037

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.414, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.46903
[1mStep[0m  [21/213], [94mLoss[0m : 2.25250
[1mStep[0m  [42/213], [94mLoss[0m : 2.73800
[1mStep[0m  [63/213], [94mLoss[0m : 2.67405
[1mStep[0m  [84/213], [94mLoss[0m : 2.75582
[1mStep[0m  [105/213], [94mLoss[0m : 2.55928
[1mStep[0m  [126/213], [94mLoss[0m : 2.42430
[1mStep[0m  [147/213], [94mLoss[0m : 2.14781
[1mStep[0m  [168/213], [94mLoss[0m : 2.44861
[1mStep[0m  [189/213], [94mLoss[0m : 2.76069
[1mStep[0m  [210/213], [94mLoss[0m : 2.48290

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.396, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.38561
[1mStep[0m  [21/213], [94mLoss[0m : 2.51648
[1mStep[0m  [42/213], [94mLoss[0m : 2.81017
[1mStep[0m  [63/213], [94mLoss[0m : 2.51656
[1mStep[0m  [84/213], [94mLoss[0m : 2.52335
[1mStep[0m  [105/213], [94mLoss[0m : 2.70148
[1mStep[0m  [126/213], [94mLoss[0m : 2.68838
[1mStep[0m  [147/213], [94mLoss[0m : 3.05853
[1mStep[0m  [168/213], [94mLoss[0m : 2.47447
[1mStep[0m  [189/213], [94mLoss[0m : 2.39034
[1mStep[0m  [210/213], [94mLoss[0m : 2.52612

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.401, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.91198
[1mStep[0m  [21/213], [94mLoss[0m : 2.44454
[1mStep[0m  [42/213], [94mLoss[0m : 2.19352
[1mStep[0m  [63/213], [94mLoss[0m : 2.34807
[1mStep[0m  [84/213], [94mLoss[0m : 3.06128
[1mStep[0m  [105/213], [94mLoss[0m : 2.25933
[1mStep[0m  [126/213], [94mLoss[0m : 2.21055
[1mStep[0m  [147/213], [94mLoss[0m : 2.70778
[1mStep[0m  [168/213], [94mLoss[0m : 2.17853
[1mStep[0m  [189/213], [94mLoss[0m : 2.33943
[1mStep[0m  [210/213], [94mLoss[0m : 2.47043

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.399, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.55305
[1mStep[0m  [21/213], [94mLoss[0m : 2.29045
[1mStep[0m  [42/213], [94mLoss[0m : 2.48367
[1mStep[0m  [63/213], [94mLoss[0m : 2.18107
[1mStep[0m  [84/213], [94mLoss[0m : 2.21080
[1mStep[0m  [105/213], [94mLoss[0m : 2.57341
[1mStep[0m  [126/213], [94mLoss[0m : 2.96508
[1mStep[0m  [147/213], [94mLoss[0m : 2.14023
[1mStep[0m  [168/213], [94mLoss[0m : 2.32330
[1mStep[0m  [189/213], [94mLoss[0m : 2.33594
[1mStep[0m  [210/213], [94mLoss[0m : 2.53671

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.400, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.50446
[1mStep[0m  [21/213], [94mLoss[0m : 2.72456
[1mStep[0m  [42/213], [94mLoss[0m : 2.43910
[1mStep[0m  [63/213], [94mLoss[0m : 2.74785
[1mStep[0m  [84/213], [94mLoss[0m : 2.77432
[1mStep[0m  [105/213], [94mLoss[0m : 2.25359
[1mStep[0m  [126/213], [94mLoss[0m : 2.33651
[1mStep[0m  [147/213], [94mLoss[0m : 2.56234
[1mStep[0m  [168/213], [94mLoss[0m : 2.31056
[1mStep[0m  [189/213], [94mLoss[0m : 2.36811
[1mStep[0m  [210/213], [94mLoss[0m : 2.44488

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.396, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.38081
[1mStep[0m  [21/213], [94mLoss[0m : 2.41891
[1mStep[0m  [42/213], [94mLoss[0m : 2.42418
[1mStep[0m  [63/213], [94mLoss[0m : 2.39850
[1mStep[0m  [84/213], [94mLoss[0m : 2.46913
[1mStep[0m  [105/213], [94mLoss[0m : 2.17908
[1mStep[0m  [126/213], [94mLoss[0m : 2.63431
[1mStep[0m  [147/213], [94mLoss[0m : 2.20434
[1mStep[0m  [168/213], [94mLoss[0m : 2.44433
[1mStep[0m  [189/213], [94mLoss[0m : 2.66052
[1mStep[0m  [210/213], [94mLoss[0m : 2.48398

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.402, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.09730
[1mStep[0m  [21/213], [94mLoss[0m : 2.40555
[1mStep[0m  [42/213], [94mLoss[0m : 2.92504
[1mStep[0m  [63/213], [94mLoss[0m : 2.37742
[1mStep[0m  [84/213], [94mLoss[0m : 2.31667
[1mStep[0m  [105/213], [94mLoss[0m : 2.53471
[1mStep[0m  [126/213], [94mLoss[0m : 2.46290
[1mStep[0m  [147/213], [94mLoss[0m : 2.54004
[1mStep[0m  [168/213], [94mLoss[0m : 2.29804
[1mStep[0m  [189/213], [94mLoss[0m : 2.22902
[1mStep[0m  [210/213], [94mLoss[0m : 2.52415

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.410, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.87908
[1mStep[0m  [21/213], [94mLoss[0m : 2.18790
[1mStep[0m  [42/213], [94mLoss[0m : 2.73064
[1mStep[0m  [63/213], [94mLoss[0m : 2.22837
[1mStep[0m  [84/213], [94mLoss[0m : 2.37803
[1mStep[0m  [105/213], [94mLoss[0m : 2.35971
[1mStep[0m  [126/213], [94mLoss[0m : 2.74852
[1mStep[0m  [147/213], [94mLoss[0m : 2.51527
[1mStep[0m  [168/213], [94mLoss[0m : 2.27855
[1mStep[0m  [189/213], [94mLoss[0m : 2.50900
[1mStep[0m  [210/213], [94mLoss[0m : 2.65630

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.408, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.53575
[1mStep[0m  [21/213], [94mLoss[0m : 3.04453
[1mStep[0m  [42/213], [94mLoss[0m : 2.74951
[1mStep[0m  [63/213], [94mLoss[0m : 2.32684
[1mStep[0m  [84/213], [94mLoss[0m : 2.33543
[1mStep[0m  [105/213], [94mLoss[0m : 2.36644
[1mStep[0m  [126/213], [94mLoss[0m : 2.51143
[1mStep[0m  [147/213], [94mLoss[0m : 2.32305
[1mStep[0m  [168/213], [94mLoss[0m : 2.36461
[1mStep[0m  [189/213], [94mLoss[0m : 2.28584
[1mStep[0m  [210/213], [94mLoss[0m : 2.16213

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.401, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.405
====================================

Phase 1 - Evaluation MAE:  2.4047917003901498
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/213], [94mLoss[0m : 2.67698
[1mStep[0m  [21/213], [94mLoss[0m : 2.39560
[1mStep[0m  [42/213], [94mLoss[0m : 2.58381
[1mStep[0m  [63/213], [94mLoss[0m : 2.61708
[1mStep[0m  [84/213], [94mLoss[0m : 2.25733
[1mStep[0m  [105/213], [94mLoss[0m : 1.99424
[1mStep[0m  [126/213], [94mLoss[0m : 2.21620
[1mStep[0m  [147/213], [94mLoss[0m : 2.07298
[1mStep[0m  [168/213], [94mLoss[0m : 2.52419
[1mStep[0m  [189/213], [94mLoss[0m : 2.07307
[1mStep[0m  [210/213], [94mLoss[0m : 2.84087

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.510, [92mTest[0m: 2.405, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.99552
[1mStep[0m  [21/213], [94mLoss[0m : 2.47273
[1mStep[0m  [42/213], [94mLoss[0m : 2.72647
[1mStep[0m  [63/213], [94mLoss[0m : 2.75096
[1mStep[0m  [84/213], [94mLoss[0m : 2.08095
[1mStep[0m  [105/213], [94mLoss[0m : 2.61051
[1mStep[0m  [126/213], [94mLoss[0m : 2.59465
[1mStep[0m  [147/213], [94mLoss[0m : 2.41373
[1mStep[0m  [168/213], [94mLoss[0m : 2.34995
[1mStep[0m  [189/213], [94mLoss[0m : 2.88279
[1mStep[0m  [210/213], [94mLoss[0m : 2.35247

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.421, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.75487
[1mStep[0m  [21/213], [94mLoss[0m : 2.49296
[1mStep[0m  [42/213], [94mLoss[0m : 2.32205
[1mStep[0m  [63/213], [94mLoss[0m : 2.37050
[1mStep[0m  [84/213], [94mLoss[0m : 2.65547
[1mStep[0m  [105/213], [94mLoss[0m : 2.67555
[1mStep[0m  [126/213], [94mLoss[0m : 2.18447
[1mStep[0m  [147/213], [94mLoss[0m : 2.48648
[1mStep[0m  [168/213], [94mLoss[0m : 2.41667
[1mStep[0m  [189/213], [94mLoss[0m : 2.51720
[1mStep[0m  [210/213], [94mLoss[0m : 2.42410

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.376, [92mTest[0m: 2.483, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.08472
[1mStep[0m  [21/213], [94mLoss[0m : 2.14121
[1mStep[0m  [42/213], [94mLoss[0m : 2.10378
[1mStep[0m  [63/213], [94mLoss[0m : 2.39779
[1mStep[0m  [84/213], [94mLoss[0m : 2.23267
[1mStep[0m  [105/213], [94mLoss[0m : 2.46707
[1mStep[0m  [126/213], [94mLoss[0m : 2.44284
[1mStep[0m  [147/213], [94mLoss[0m : 2.31301
[1mStep[0m  [168/213], [94mLoss[0m : 2.13169
[1mStep[0m  [189/213], [94mLoss[0m : 2.69191
[1mStep[0m  [210/213], [94mLoss[0m : 2.00679

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.326, [92mTest[0m: 2.385, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.50444
[1mStep[0m  [21/213], [94mLoss[0m : 2.23443
[1mStep[0m  [42/213], [94mLoss[0m : 2.25222
[1mStep[0m  [63/213], [94mLoss[0m : 2.21392
[1mStep[0m  [84/213], [94mLoss[0m : 2.41373
[1mStep[0m  [105/213], [94mLoss[0m : 2.37723
[1mStep[0m  [126/213], [94mLoss[0m : 2.33735
[1mStep[0m  [147/213], [94mLoss[0m : 2.41612
[1mStep[0m  [168/213], [94mLoss[0m : 2.12719
[1mStep[0m  [189/213], [94mLoss[0m : 2.27439
[1mStep[0m  [210/213], [94mLoss[0m : 2.08374

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.268, [92mTest[0m: 2.385, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.13574
[1mStep[0m  [21/213], [94mLoss[0m : 2.19923
[1mStep[0m  [42/213], [94mLoss[0m : 1.63416
[1mStep[0m  [63/213], [94mLoss[0m : 1.72374
[1mStep[0m  [84/213], [94mLoss[0m : 1.94583
[1mStep[0m  [105/213], [94mLoss[0m : 1.97342
[1mStep[0m  [126/213], [94mLoss[0m : 1.98932
[1mStep[0m  [147/213], [94mLoss[0m : 1.99228
[1mStep[0m  [168/213], [94mLoss[0m : 1.83048
[1mStep[0m  [189/213], [94mLoss[0m : 2.10511
[1mStep[0m  [210/213], [94mLoss[0m : 2.26292

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.225, [92mTest[0m: 2.410, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.43316
[1mStep[0m  [21/213], [94mLoss[0m : 2.13687
[1mStep[0m  [42/213], [94mLoss[0m : 2.43866
[1mStep[0m  [63/213], [94mLoss[0m : 1.92568
[1mStep[0m  [84/213], [94mLoss[0m : 1.99909
[1mStep[0m  [105/213], [94mLoss[0m : 1.89373
[1mStep[0m  [126/213], [94mLoss[0m : 2.36389
[1mStep[0m  [147/213], [94mLoss[0m : 2.16096
[1mStep[0m  [168/213], [94mLoss[0m : 2.46150
[1mStep[0m  [189/213], [94mLoss[0m : 2.39526
[1mStep[0m  [210/213], [94mLoss[0m : 2.24933

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.166, [92mTest[0m: 2.425, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.01341
[1mStep[0m  [21/213], [94mLoss[0m : 1.88519
[1mStep[0m  [42/213], [94mLoss[0m : 2.10006
[1mStep[0m  [63/213], [94mLoss[0m : 2.10660
[1mStep[0m  [84/213], [94mLoss[0m : 2.53680
[1mStep[0m  [105/213], [94mLoss[0m : 1.73266
[1mStep[0m  [126/213], [94mLoss[0m : 2.18934
[1mStep[0m  [147/213], [94mLoss[0m : 2.08922
[1mStep[0m  [168/213], [94mLoss[0m : 2.65907
[1mStep[0m  [189/213], [94mLoss[0m : 2.20602
[1mStep[0m  [210/213], [94mLoss[0m : 2.16873

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.117, [92mTest[0m: 2.398, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.02959
[1mStep[0m  [21/213], [94mLoss[0m : 1.79536
[1mStep[0m  [42/213], [94mLoss[0m : 2.25043
[1mStep[0m  [63/213], [94mLoss[0m : 1.95891
[1mStep[0m  [84/213], [94mLoss[0m : 1.72088
[1mStep[0m  [105/213], [94mLoss[0m : 2.25578
[1mStep[0m  [126/213], [94mLoss[0m : 1.89176
[1mStep[0m  [147/213], [94mLoss[0m : 2.27984
[1mStep[0m  [168/213], [94mLoss[0m : 2.07361
[1mStep[0m  [189/213], [94mLoss[0m : 2.22659
[1mStep[0m  [210/213], [94mLoss[0m : 2.24784

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.090, [92mTest[0m: 2.416, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.52998
[1mStep[0m  [21/213], [94mLoss[0m : 1.86311
[1mStep[0m  [42/213], [94mLoss[0m : 2.04615
[1mStep[0m  [63/213], [94mLoss[0m : 2.17206
[1mStep[0m  [84/213], [94mLoss[0m : 2.21392
[1mStep[0m  [105/213], [94mLoss[0m : 1.95436
[1mStep[0m  [126/213], [94mLoss[0m : 1.65273
[1mStep[0m  [147/213], [94mLoss[0m : 2.21042
[1mStep[0m  [168/213], [94mLoss[0m : 1.76982
[1mStep[0m  [189/213], [94mLoss[0m : 1.78236
[1mStep[0m  [210/213], [94mLoss[0m : 2.39246

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.057, [92mTest[0m: 2.381, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.76083
[1mStep[0m  [21/213], [94mLoss[0m : 2.03982
[1mStep[0m  [42/213], [94mLoss[0m : 1.66100
[1mStep[0m  [63/213], [94mLoss[0m : 1.71873
[1mStep[0m  [84/213], [94mLoss[0m : 2.12273
[1mStep[0m  [105/213], [94mLoss[0m : 1.88787
[1mStep[0m  [126/213], [94mLoss[0m : 1.84859
[1mStep[0m  [147/213], [94mLoss[0m : 2.39133
[1mStep[0m  [168/213], [94mLoss[0m : 1.93631
[1mStep[0m  [189/213], [94mLoss[0m : 2.37098
[1mStep[0m  [210/213], [94mLoss[0m : 2.15379

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.015, [92mTest[0m: 2.409, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.81697
[1mStep[0m  [21/213], [94mLoss[0m : 2.52328
[1mStep[0m  [42/213], [94mLoss[0m : 1.91621
[1mStep[0m  [63/213], [94mLoss[0m : 1.79713
[1mStep[0m  [84/213], [94mLoss[0m : 1.95369
[1mStep[0m  [105/213], [94mLoss[0m : 1.97916
[1mStep[0m  [126/213], [94mLoss[0m : 1.78354
[1mStep[0m  [147/213], [94mLoss[0m : 1.61335
[1mStep[0m  [168/213], [94mLoss[0m : 1.84809
[1mStep[0m  [189/213], [94mLoss[0m : 2.08731
[1mStep[0m  [210/213], [94mLoss[0m : 1.60608

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.976, [92mTest[0m: 2.407, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.96743
[1mStep[0m  [21/213], [94mLoss[0m : 2.08163
[1mStep[0m  [42/213], [94mLoss[0m : 2.06969
[1mStep[0m  [63/213], [94mLoss[0m : 2.62126
[1mStep[0m  [84/213], [94mLoss[0m : 1.74721
[1mStep[0m  [105/213], [94mLoss[0m : 1.99754
[1mStep[0m  [126/213], [94mLoss[0m : 2.32674
[1mStep[0m  [147/213], [94mLoss[0m : 1.81390
[1mStep[0m  [168/213], [94mLoss[0m : 2.00906
[1mStep[0m  [189/213], [94mLoss[0m : 2.08060
[1mStep[0m  [210/213], [94mLoss[0m : 2.33499

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.945, [92mTest[0m: 2.408, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.81804
[1mStep[0m  [21/213], [94mLoss[0m : 2.04570
[1mStep[0m  [42/213], [94mLoss[0m : 1.94921
[1mStep[0m  [63/213], [94mLoss[0m : 1.60328
[1mStep[0m  [84/213], [94mLoss[0m : 2.00144
[1mStep[0m  [105/213], [94mLoss[0m : 2.15688
[1mStep[0m  [126/213], [94mLoss[0m : 2.05427
[1mStep[0m  [147/213], [94mLoss[0m : 1.89291
[1mStep[0m  [168/213], [94mLoss[0m : 1.96177
[1mStep[0m  [189/213], [94mLoss[0m : 2.01134
[1mStep[0m  [210/213], [94mLoss[0m : 2.45053

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.937, [92mTest[0m: 2.430, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.28670
[1mStep[0m  [21/213], [94mLoss[0m : 2.01120
[1mStep[0m  [42/213], [94mLoss[0m : 1.96919
[1mStep[0m  [63/213], [94mLoss[0m : 1.80216
[1mStep[0m  [84/213], [94mLoss[0m : 2.03446
[1mStep[0m  [105/213], [94mLoss[0m : 2.00304
[1mStep[0m  [126/213], [94mLoss[0m : 2.03774
[1mStep[0m  [147/213], [94mLoss[0m : 2.32507
[1mStep[0m  [168/213], [94mLoss[0m : 1.87005
[1mStep[0m  [189/213], [94mLoss[0m : 2.40930
[1mStep[0m  [210/213], [94mLoss[0m : 2.28145

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.911, [92mTest[0m: 2.431, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.99121
[1mStep[0m  [21/213], [94mLoss[0m : 2.24532
[1mStep[0m  [42/213], [94mLoss[0m : 1.65304
[1mStep[0m  [63/213], [94mLoss[0m : 1.49985
[1mStep[0m  [84/213], [94mLoss[0m : 1.82063
[1mStep[0m  [105/213], [94mLoss[0m : 2.25821
[1mStep[0m  [126/213], [94mLoss[0m : 1.45925
[1mStep[0m  [147/213], [94mLoss[0m : 2.01365
[1mStep[0m  [168/213], [94mLoss[0m : 2.17520
[1mStep[0m  [189/213], [94mLoss[0m : 1.89497
[1mStep[0m  [210/213], [94mLoss[0m : 1.84945

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.894, [92mTest[0m: 2.483, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.11343
[1mStep[0m  [21/213], [94mLoss[0m : 1.68584
[1mStep[0m  [42/213], [94mLoss[0m : 1.96837
[1mStep[0m  [63/213], [94mLoss[0m : 1.85425
[1mStep[0m  [84/213], [94mLoss[0m : 1.58054
[1mStep[0m  [105/213], [94mLoss[0m : 1.94808
[1mStep[0m  [126/213], [94mLoss[0m : 1.75659
[1mStep[0m  [147/213], [94mLoss[0m : 2.04619
[1mStep[0m  [168/213], [94mLoss[0m : 1.94277
[1mStep[0m  [189/213], [94mLoss[0m : 1.73252
[1mStep[0m  [210/213], [94mLoss[0m : 1.57142

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.863, [92mTest[0m: 2.433, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.91348
[1mStep[0m  [21/213], [94mLoss[0m : 2.17710
[1mStep[0m  [42/213], [94mLoss[0m : 1.62482
[1mStep[0m  [63/213], [94mLoss[0m : 1.82530
[1mStep[0m  [84/213], [94mLoss[0m : 1.72476
[1mStep[0m  [105/213], [94mLoss[0m : 1.62754
[1mStep[0m  [126/213], [94mLoss[0m : 1.52737
[1mStep[0m  [147/213], [94mLoss[0m : 1.78672
[1mStep[0m  [168/213], [94mLoss[0m : 2.12124
[1mStep[0m  [189/213], [94mLoss[0m : 1.80750
[1mStep[0m  [210/213], [94mLoss[0m : 1.83651

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.845, [92mTest[0m: 2.424, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.70157
[1mStep[0m  [21/213], [94mLoss[0m : 1.99781
[1mStep[0m  [42/213], [94mLoss[0m : 1.66854
[1mStep[0m  [63/213], [94mLoss[0m : 1.74167
[1mStep[0m  [84/213], [94mLoss[0m : 2.00601
[1mStep[0m  [105/213], [94mLoss[0m : 1.78454
[1mStep[0m  [126/213], [94mLoss[0m : 1.76720
[1mStep[0m  [147/213], [94mLoss[0m : 1.81545
[1mStep[0m  [168/213], [94mLoss[0m : 1.59578
[1mStep[0m  [189/213], [94mLoss[0m : 2.14505
[1mStep[0m  [210/213], [94mLoss[0m : 2.34202

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.831, [92mTest[0m: 2.432, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.91856
[1mStep[0m  [21/213], [94mLoss[0m : 1.81760
[1mStep[0m  [42/213], [94mLoss[0m : 1.78366
[1mStep[0m  [63/213], [94mLoss[0m : 1.74716
[1mStep[0m  [84/213], [94mLoss[0m : 1.87494
[1mStep[0m  [105/213], [94mLoss[0m : 1.69753
[1mStep[0m  [126/213], [94mLoss[0m : 1.72126
[1mStep[0m  [147/213], [94mLoss[0m : 1.77027
[1mStep[0m  [168/213], [94mLoss[0m : 1.87357
[1mStep[0m  [189/213], [94mLoss[0m : 1.62667
[1mStep[0m  [210/213], [94mLoss[0m : 1.69075

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.812, [92mTest[0m: 2.414, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.87326
[1mStep[0m  [21/213], [94mLoss[0m : 1.74814
[1mStep[0m  [42/213], [94mLoss[0m : 2.06517
[1mStep[0m  [63/213], [94mLoss[0m : 1.82167
[1mStep[0m  [84/213], [94mLoss[0m : 1.76626
[1mStep[0m  [105/213], [94mLoss[0m : 1.62351
[1mStep[0m  [126/213], [94mLoss[0m : 2.07097
[1mStep[0m  [147/213], [94mLoss[0m : 1.87653
[1mStep[0m  [168/213], [94mLoss[0m : 1.60582
[1mStep[0m  [189/213], [94mLoss[0m : 1.69755
[1mStep[0m  [210/213], [94mLoss[0m : 1.91237

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.778, [92mTest[0m: 2.492, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.73174
[1mStep[0m  [21/213], [94mLoss[0m : 1.76943
[1mStep[0m  [42/213], [94mLoss[0m : 1.63152
[1mStep[0m  [63/213], [94mLoss[0m : 1.98698
[1mStep[0m  [84/213], [94mLoss[0m : 1.74615
[1mStep[0m  [105/213], [94mLoss[0m : 1.52273
[1mStep[0m  [126/213], [94mLoss[0m : 2.01304
[1mStep[0m  [147/213], [94mLoss[0m : 1.77890
[1mStep[0m  [168/213], [94mLoss[0m : 1.78188
[1mStep[0m  [189/213], [94mLoss[0m : 1.66443
[1mStep[0m  [210/213], [94mLoss[0m : 1.91084

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.762, [92mTest[0m: 2.438, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.90518
[1mStep[0m  [21/213], [94mLoss[0m : 1.97850
[1mStep[0m  [42/213], [94mLoss[0m : 1.46717
[1mStep[0m  [63/213], [94mLoss[0m : 1.74713
[1mStep[0m  [84/213], [94mLoss[0m : 1.69985
[1mStep[0m  [105/213], [94mLoss[0m : 1.67771
[1mStep[0m  [126/213], [94mLoss[0m : 1.72260
[1mStep[0m  [147/213], [94mLoss[0m : 1.59871
[1mStep[0m  [168/213], [94mLoss[0m : 1.78172
[1mStep[0m  [189/213], [94mLoss[0m : 1.71702
[1mStep[0m  [210/213], [94mLoss[0m : 1.73839

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.736, [92mTest[0m: 2.436, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.79128
[1mStep[0m  [21/213], [94mLoss[0m : 1.70947
[1mStep[0m  [42/213], [94mLoss[0m : 1.53733
[1mStep[0m  [63/213], [94mLoss[0m : 1.89504
[1mStep[0m  [84/213], [94mLoss[0m : 1.78740
[1mStep[0m  [105/213], [94mLoss[0m : 1.75546
[1mStep[0m  [126/213], [94mLoss[0m : 1.38160
[1mStep[0m  [147/213], [94mLoss[0m : 1.40674
[1mStep[0m  [168/213], [94mLoss[0m : 1.64104
[1mStep[0m  [189/213], [94mLoss[0m : 1.72618
[1mStep[0m  [210/213], [94mLoss[0m : 1.77716

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.751, [92mTest[0m: 2.489, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.91894
[1mStep[0m  [21/213], [94mLoss[0m : 1.39131
[1mStep[0m  [42/213], [94mLoss[0m : 1.94779
[1mStep[0m  [63/213], [94mLoss[0m : 2.00177
[1mStep[0m  [84/213], [94mLoss[0m : 2.01238
[1mStep[0m  [105/213], [94mLoss[0m : 1.67547
[1mStep[0m  [126/213], [94mLoss[0m : 1.44769
[1mStep[0m  [147/213], [94mLoss[0m : 1.98663
[1mStep[0m  [168/213], [94mLoss[0m : 1.82808
[1mStep[0m  [189/213], [94mLoss[0m : 1.71387
[1mStep[0m  [210/213], [94mLoss[0m : 2.02271

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.719, [92mTest[0m: 2.451, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.77798
[1mStep[0m  [21/213], [94mLoss[0m : 1.89827
[1mStep[0m  [42/213], [94mLoss[0m : 1.56573
[1mStep[0m  [63/213], [94mLoss[0m : 1.48362
[1mStep[0m  [84/213], [94mLoss[0m : 1.64937
[1mStep[0m  [105/213], [94mLoss[0m : 1.67155
[1mStep[0m  [126/213], [94mLoss[0m : 1.67301
[1mStep[0m  [147/213], [94mLoss[0m : 1.75749
[1mStep[0m  [168/213], [94mLoss[0m : 1.60066
[1mStep[0m  [189/213], [94mLoss[0m : 1.93531
[1mStep[0m  [210/213], [94mLoss[0m : 1.62776

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.717, [92mTest[0m: 2.507, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.60772
[1mStep[0m  [21/213], [94mLoss[0m : 1.56493
[1mStep[0m  [42/213], [94mLoss[0m : 1.34771
[1mStep[0m  [63/213], [94mLoss[0m : 1.81772
[1mStep[0m  [84/213], [94mLoss[0m : 1.54510
[1mStep[0m  [105/213], [94mLoss[0m : 1.90171
[1mStep[0m  [126/213], [94mLoss[0m : 1.96827
[1mStep[0m  [147/213], [94mLoss[0m : 1.49295
[1mStep[0m  [168/213], [94mLoss[0m : 1.49259
[1mStep[0m  [189/213], [94mLoss[0m : 1.71086
[1mStep[0m  [210/213], [94mLoss[0m : 1.84239

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.700, [92mTest[0m: 2.430, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.82765
[1mStep[0m  [21/213], [94mLoss[0m : 1.81109
[1mStep[0m  [42/213], [94mLoss[0m : 1.64698
[1mStep[0m  [63/213], [94mLoss[0m : 2.06220
[1mStep[0m  [84/213], [94mLoss[0m : 1.54126
[1mStep[0m  [105/213], [94mLoss[0m : 1.62545
[1mStep[0m  [126/213], [94mLoss[0m : 1.96864
[1mStep[0m  [147/213], [94mLoss[0m : 1.41318
[1mStep[0m  [168/213], [94mLoss[0m : 1.90075
[1mStep[0m  [189/213], [94mLoss[0m : 1.68813
[1mStep[0m  [210/213], [94mLoss[0m : 1.65444

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.664, [92mTest[0m: 2.443, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.92650
[1mStep[0m  [21/213], [94mLoss[0m : 1.82634
[1mStep[0m  [42/213], [94mLoss[0m : 1.49404
[1mStep[0m  [63/213], [94mLoss[0m : 1.82033
[1mStep[0m  [84/213], [94mLoss[0m : 1.89380
[1mStep[0m  [105/213], [94mLoss[0m : 1.68525
[1mStep[0m  [126/213], [94mLoss[0m : 1.53375
[1mStep[0m  [147/213], [94mLoss[0m : 1.87020
[1mStep[0m  [168/213], [94mLoss[0m : 1.73925
[1mStep[0m  [189/213], [94mLoss[0m : 1.72766
[1mStep[0m  [210/213], [94mLoss[0m : 1.78181

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.677, [92mTest[0m: 2.465, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.59951
[1mStep[0m  [21/213], [94mLoss[0m : 1.54664
[1mStep[0m  [42/213], [94mLoss[0m : 1.68346
[1mStep[0m  [63/213], [94mLoss[0m : 1.66068
[1mStep[0m  [84/213], [94mLoss[0m : 1.64717
[1mStep[0m  [105/213], [94mLoss[0m : 2.18676
[1mStep[0m  [126/213], [94mLoss[0m : 1.61382
[1mStep[0m  [147/213], [94mLoss[0m : 1.90601
[1mStep[0m  [168/213], [94mLoss[0m : 1.56869
[1mStep[0m  [189/213], [94mLoss[0m : 1.65061
[1mStep[0m  [210/213], [94mLoss[0m : 1.70377

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.675, [92mTest[0m: 2.458, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.545
====================================

Phase 2 - Evaluation MAE:  2.5450501981771216
MAE score P1      2.404792
MAE score P2       2.54505
loss              1.664009
learning_rate      0.00505
batch_size              64
hidden_sizes         [100]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.4
momentum               0.5
weight_decay          0.01
Name: 6, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/213], [94mLoss[0m : 10.54913
[1mStep[0m  [21/213], [94mLoss[0m : 3.19752
[1mStep[0m  [42/213], [94mLoss[0m : 3.03973
[1mStep[0m  [63/213], [94mLoss[0m : 3.06596
[1mStep[0m  [84/213], [94mLoss[0m : 2.81032
[1mStep[0m  [105/213], [94mLoss[0m : 2.49629
[1mStep[0m  [126/213], [94mLoss[0m : 2.54824
[1mStep[0m  [147/213], [94mLoss[0m : 2.67713
[1mStep[0m  [168/213], [94mLoss[0m : 2.61341
[1mStep[0m  [189/213], [94mLoss[0m : 3.18498
[1mStep[0m  [210/213], [94mLoss[0m : 2.92658

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.153, [92mTest[0m: 10.790, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.53868
[1mStep[0m  [21/213], [94mLoss[0m : 2.82488
[1mStep[0m  [42/213], [94mLoss[0m : 2.98126
[1mStep[0m  [63/213], [94mLoss[0m : 2.77892
[1mStep[0m  [84/213], [94mLoss[0m : 3.03309
[1mStep[0m  [105/213], [94mLoss[0m : 3.15331
[1mStep[0m  [126/213], [94mLoss[0m : 2.54266
[1mStep[0m  [147/213], [94mLoss[0m : 3.03634
[1mStep[0m  [168/213], [94mLoss[0m : 2.91948
[1mStep[0m  [189/213], [94mLoss[0m : 2.56205
[1mStep[0m  [210/213], [94mLoss[0m : 2.61229

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.560, [92mTest[0m: 2.512, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.42041
[1mStep[0m  [21/213], [94mLoss[0m : 2.70281
[1mStep[0m  [42/213], [94mLoss[0m : 2.42093
[1mStep[0m  [63/213], [94mLoss[0m : 2.33363
[1mStep[0m  [84/213], [94mLoss[0m : 2.30645
[1mStep[0m  [105/213], [94mLoss[0m : 2.49716
[1mStep[0m  [126/213], [94mLoss[0m : 3.02060
[1mStep[0m  [147/213], [94mLoss[0m : 2.61535
[1mStep[0m  [168/213], [94mLoss[0m : 2.67784
[1mStep[0m  [189/213], [94mLoss[0m : 2.05692
[1mStep[0m  [210/213], [94mLoss[0m : 2.04101

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.528, [92mTest[0m: 2.433, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.06360
[1mStep[0m  [21/213], [94mLoss[0m : 2.78937
[1mStep[0m  [42/213], [94mLoss[0m : 2.61468
[1mStep[0m  [63/213], [94mLoss[0m : 2.47587
[1mStep[0m  [84/213], [94mLoss[0m : 2.14477
[1mStep[0m  [105/213], [94mLoss[0m : 2.25159
[1mStep[0m  [126/213], [94mLoss[0m : 2.28823
[1mStep[0m  [147/213], [94mLoss[0m : 2.52454
[1mStep[0m  [168/213], [94mLoss[0m : 2.39820
[1mStep[0m  [189/213], [94mLoss[0m : 2.78335
[1mStep[0m  [210/213], [94mLoss[0m : 2.40963

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.399, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.26621
[1mStep[0m  [21/213], [94mLoss[0m : 2.63558
[1mStep[0m  [42/213], [94mLoss[0m : 2.85484
[1mStep[0m  [63/213], [94mLoss[0m : 2.67197
[1mStep[0m  [84/213], [94mLoss[0m : 2.43122
[1mStep[0m  [105/213], [94mLoss[0m : 2.54432
[1mStep[0m  [126/213], [94mLoss[0m : 2.57547
[1mStep[0m  [147/213], [94mLoss[0m : 2.03235
[1mStep[0m  [168/213], [94mLoss[0m : 2.39406
[1mStep[0m  [189/213], [94mLoss[0m : 2.24101
[1mStep[0m  [210/213], [94mLoss[0m : 3.17002

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.405, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.32691
[1mStep[0m  [21/213], [94mLoss[0m : 2.90104
[1mStep[0m  [42/213], [94mLoss[0m : 2.53976
[1mStep[0m  [63/213], [94mLoss[0m : 2.27136
[1mStep[0m  [84/213], [94mLoss[0m : 2.59571
[1mStep[0m  [105/213], [94mLoss[0m : 2.43797
[1mStep[0m  [126/213], [94mLoss[0m : 1.90520
[1mStep[0m  [147/213], [94mLoss[0m : 2.21835
[1mStep[0m  [168/213], [94mLoss[0m : 2.20758
[1mStep[0m  [189/213], [94mLoss[0m : 2.55585
[1mStep[0m  [210/213], [94mLoss[0m : 2.38872

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.365, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.58499
[1mStep[0m  [21/213], [94mLoss[0m : 2.10355
[1mStep[0m  [42/213], [94mLoss[0m : 2.53897
[1mStep[0m  [63/213], [94mLoss[0m : 2.74478
[1mStep[0m  [84/213], [94mLoss[0m : 2.78270
[1mStep[0m  [105/213], [94mLoss[0m : 2.18363
[1mStep[0m  [126/213], [94mLoss[0m : 2.87700
[1mStep[0m  [147/213], [94mLoss[0m : 2.57886
[1mStep[0m  [168/213], [94mLoss[0m : 2.42248
[1mStep[0m  [189/213], [94mLoss[0m : 2.63801
[1mStep[0m  [210/213], [94mLoss[0m : 2.58444

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.412, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.76320
[1mStep[0m  [21/213], [94mLoss[0m : 2.51514
[1mStep[0m  [42/213], [94mLoss[0m : 2.82986
[1mStep[0m  [63/213], [94mLoss[0m : 2.53376
[1mStep[0m  [84/213], [94mLoss[0m : 2.32601
[1mStep[0m  [105/213], [94mLoss[0m : 2.11253
[1mStep[0m  [126/213], [94mLoss[0m : 2.54081
[1mStep[0m  [147/213], [94mLoss[0m : 2.66600
[1mStep[0m  [168/213], [94mLoss[0m : 2.42202
[1mStep[0m  [189/213], [94mLoss[0m : 2.58595
[1mStep[0m  [210/213], [94mLoss[0m : 2.64416

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.362, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.51569
[1mStep[0m  [21/213], [94mLoss[0m : 2.48371
[1mStep[0m  [42/213], [94mLoss[0m : 2.69267
[1mStep[0m  [63/213], [94mLoss[0m : 2.47398
[1mStep[0m  [84/213], [94mLoss[0m : 2.68428
[1mStep[0m  [105/213], [94mLoss[0m : 2.38263
[1mStep[0m  [126/213], [94mLoss[0m : 1.77632
[1mStep[0m  [147/213], [94mLoss[0m : 2.54302
[1mStep[0m  [168/213], [94mLoss[0m : 2.61617
[1mStep[0m  [189/213], [94mLoss[0m : 2.61506
[1mStep[0m  [210/213], [94mLoss[0m : 2.21035

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.393, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.40798
[1mStep[0m  [21/213], [94mLoss[0m : 2.35701
[1mStep[0m  [42/213], [94mLoss[0m : 2.41035
[1mStep[0m  [63/213], [94mLoss[0m : 2.40450
[1mStep[0m  [84/213], [94mLoss[0m : 2.45069
[1mStep[0m  [105/213], [94mLoss[0m : 2.43384
[1mStep[0m  [126/213], [94mLoss[0m : 2.86557
[1mStep[0m  [147/213], [94mLoss[0m : 2.62130
[1mStep[0m  [168/213], [94mLoss[0m : 1.98973
[1mStep[0m  [189/213], [94mLoss[0m : 2.76023
[1mStep[0m  [210/213], [94mLoss[0m : 2.37912

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.374, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.50555
[1mStep[0m  [21/213], [94mLoss[0m : 2.45951
[1mStep[0m  [42/213], [94mLoss[0m : 2.22910
[1mStep[0m  [63/213], [94mLoss[0m : 2.17986
[1mStep[0m  [84/213], [94mLoss[0m : 2.57314
[1mStep[0m  [105/213], [94mLoss[0m : 2.50795
[1mStep[0m  [126/213], [94mLoss[0m : 2.29628
[1mStep[0m  [147/213], [94mLoss[0m : 2.30513
[1mStep[0m  [168/213], [94mLoss[0m : 2.19225
[1mStep[0m  [189/213], [94mLoss[0m : 2.28186
[1mStep[0m  [210/213], [94mLoss[0m : 1.95642

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.391, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.14979
[1mStep[0m  [21/213], [94mLoss[0m : 2.23626
[1mStep[0m  [42/213], [94mLoss[0m : 2.55812
[1mStep[0m  [63/213], [94mLoss[0m : 2.66198
[1mStep[0m  [84/213], [94mLoss[0m : 2.35515
[1mStep[0m  [105/213], [94mLoss[0m : 2.20810
[1mStep[0m  [126/213], [94mLoss[0m : 2.23832
[1mStep[0m  [147/213], [94mLoss[0m : 1.99522
[1mStep[0m  [168/213], [94mLoss[0m : 2.52708
[1mStep[0m  [189/213], [94mLoss[0m : 3.01286
[1mStep[0m  [210/213], [94mLoss[0m : 2.27651

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.357, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.33478
[1mStep[0m  [21/213], [94mLoss[0m : 2.41320
[1mStep[0m  [42/213], [94mLoss[0m : 2.68191
[1mStep[0m  [63/213], [94mLoss[0m : 2.23025
[1mStep[0m  [84/213], [94mLoss[0m : 2.45229
[1mStep[0m  [105/213], [94mLoss[0m : 2.27885
[1mStep[0m  [126/213], [94mLoss[0m : 2.35719
[1mStep[0m  [147/213], [94mLoss[0m : 2.15663
[1mStep[0m  [168/213], [94mLoss[0m : 3.06086
[1mStep[0m  [189/213], [94mLoss[0m : 2.75755
[1mStep[0m  [210/213], [94mLoss[0m : 2.19685

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.396, [92mTest[0m: 2.378, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.30036
[1mStep[0m  [21/213], [94mLoss[0m : 2.28473
[1mStep[0m  [42/213], [94mLoss[0m : 2.29359
[1mStep[0m  [63/213], [94mLoss[0m : 2.21072
[1mStep[0m  [84/213], [94mLoss[0m : 2.66830
[1mStep[0m  [105/213], [94mLoss[0m : 2.46466
[1mStep[0m  [126/213], [94mLoss[0m : 2.65932
[1mStep[0m  [147/213], [94mLoss[0m : 2.60483
[1mStep[0m  [168/213], [94mLoss[0m : 2.36235
[1mStep[0m  [189/213], [94mLoss[0m : 2.51544
[1mStep[0m  [210/213], [94mLoss[0m : 2.02638

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.387, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.33232
[1mStep[0m  [21/213], [94mLoss[0m : 2.20707
[1mStep[0m  [42/213], [94mLoss[0m : 2.36987
[1mStep[0m  [63/213], [94mLoss[0m : 2.32020
[1mStep[0m  [84/213], [94mLoss[0m : 2.14610
[1mStep[0m  [105/213], [94mLoss[0m : 2.17613
[1mStep[0m  [126/213], [94mLoss[0m : 2.35325
[1mStep[0m  [147/213], [94mLoss[0m : 2.64457
[1mStep[0m  [168/213], [94mLoss[0m : 1.94897
[1mStep[0m  [189/213], [94mLoss[0m : 2.27515
[1mStep[0m  [210/213], [94mLoss[0m : 2.46987

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.366, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.40130
[1mStep[0m  [21/213], [94mLoss[0m : 2.46370
[1mStep[0m  [42/213], [94mLoss[0m : 2.21127
[1mStep[0m  [63/213], [94mLoss[0m : 2.03789
[1mStep[0m  [84/213], [94mLoss[0m : 2.88771
[1mStep[0m  [105/213], [94mLoss[0m : 1.97662
[1mStep[0m  [126/213], [94mLoss[0m : 2.19038
[1mStep[0m  [147/213], [94mLoss[0m : 2.77657
[1mStep[0m  [168/213], [94mLoss[0m : 2.33392
[1mStep[0m  [189/213], [94mLoss[0m : 2.35468
[1mStep[0m  [210/213], [94mLoss[0m : 2.08875

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.358, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.28092
[1mStep[0m  [21/213], [94mLoss[0m : 2.40953
[1mStep[0m  [42/213], [94mLoss[0m : 2.67736
[1mStep[0m  [63/213], [94mLoss[0m : 2.06604
[1mStep[0m  [84/213], [94mLoss[0m : 2.63628
[1mStep[0m  [105/213], [94mLoss[0m : 2.18297
[1mStep[0m  [126/213], [94mLoss[0m : 2.08597
[1mStep[0m  [147/213], [94mLoss[0m : 2.48165
[1mStep[0m  [168/213], [94mLoss[0m : 2.62029
[1mStep[0m  [189/213], [94mLoss[0m : 2.75516
[1mStep[0m  [210/213], [94mLoss[0m : 2.45286

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.375, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.62950
[1mStep[0m  [21/213], [94mLoss[0m : 2.51534
[1mStep[0m  [42/213], [94mLoss[0m : 1.91996
[1mStep[0m  [63/213], [94mLoss[0m : 2.52220
[1mStep[0m  [84/213], [94mLoss[0m : 2.03831
[1mStep[0m  [105/213], [94mLoss[0m : 2.27436
[1mStep[0m  [126/213], [94mLoss[0m : 2.34056
[1mStep[0m  [147/213], [94mLoss[0m : 2.80008
[1mStep[0m  [168/213], [94mLoss[0m : 2.31680
[1mStep[0m  [189/213], [94mLoss[0m : 2.95809
[1mStep[0m  [210/213], [94mLoss[0m : 2.54822

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.391, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.49957
[1mStep[0m  [21/213], [94mLoss[0m : 2.33532
[1mStep[0m  [42/213], [94mLoss[0m : 2.46818
[1mStep[0m  [63/213], [94mLoss[0m : 2.04005
[1mStep[0m  [84/213], [94mLoss[0m : 2.28708
[1mStep[0m  [105/213], [94mLoss[0m : 2.23100
[1mStep[0m  [126/213], [94mLoss[0m : 2.23230
[1mStep[0m  [147/213], [94mLoss[0m : 2.34946
[1mStep[0m  [168/213], [94mLoss[0m : 2.36481
[1mStep[0m  [189/213], [94mLoss[0m : 2.20688
[1mStep[0m  [210/213], [94mLoss[0m : 2.38490

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.384, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.52134
[1mStep[0m  [21/213], [94mLoss[0m : 2.23253
[1mStep[0m  [42/213], [94mLoss[0m : 2.68661
[1mStep[0m  [63/213], [94mLoss[0m : 2.51037
[1mStep[0m  [84/213], [94mLoss[0m : 2.37432
[1mStep[0m  [105/213], [94mLoss[0m : 2.44957
[1mStep[0m  [126/213], [94mLoss[0m : 2.40161
[1mStep[0m  [147/213], [94mLoss[0m : 2.55992
[1mStep[0m  [168/213], [94mLoss[0m : 2.22522
[1mStep[0m  [189/213], [94mLoss[0m : 2.29011
[1mStep[0m  [210/213], [94mLoss[0m : 2.36576

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.365, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.52190
[1mStep[0m  [21/213], [94mLoss[0m : 2.10592
[1mStep[0m  [42/213], [94mLoss[0m : 2.16374
[1mStep[0m  [63/213], [94mLoss[0m : 2.46775
[1mStep[0m  [84/213], [94mLoss[0m : 2.28918
[1mStep[0m  [105/213], [94mLoss[0m : 2.32165
[1mStep[0m  [126/213], [94mLoss[0m : 2.33500
[1mStep[0m  [147/213], [94mLoss[0m : 2.62966
[1mStep[0m  [168/213], [94mLoss[0m : 2.38148
[1mStep[0m  [189/213], [94mLoss[0m : 2.10569
[1mStep[0m  [210/213], [94mLoss[0m : 2.55388

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.369, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.72569
[1mStep[0m  [21/213], [94mLoss[0m : 2.52914
[1mStep[0m  [42/213], [94mLoss[0m : 2.57511
[1mStep[0m  [63/213], [94mLoss[0m : 2.04008
[1mStep[0m  [84/213], [94mLoss[0m : 2.45964
[1mStep[0m  [105/213], [94mLoss[0m : 2.55410
[1mStep[0m  [126/213], [94mLoss[0m : 2.05784
[1mStep[0m  [147/213], [94mLoss[0m : 2.30861
[1mStep[0m  [168/213], [94mLoss[0m : 2.31927
[1mStep[0m  [189/213], [94mLoss[0m : 2.44399
[1mStep[0m  [210/213], [94mLoss[0m : 2.07414

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.394, [92mTest[0m: 2.347, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.04223
[1mStep[0m  [21/213], [94mLoss[0m : 2.56322
[1mStep[0m  [42/213], [94mLoss[0m : 2.17953
[1mStep[0m  [63/213], [94mLoss[0m : 2.41084
[1mStep[0m  [84/213], [94mLoss[0m : 2.25640
[1mStep[0m  [105/213], [94mLoss[0m : 2.48625
[1mStep[0m  [126/213], [94mLoss[0m : 2.50331
[1mStep[0m  [147/213], [94mLoss[0m : 2.33194
[1mStep[0m  [168/213], [94mLoss[0m : 2.14481
[1mStep[0m  [189/213], [94mLoss[0m : 2.34382
[1mStep[0m  [210/213], [94mLoss[0m : 2.38549

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.391, [92mTest[0m: 2.385, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.22720
[1mStep[0m  [21/213], [94mLoss[0m : 2.36621
[1mStep[0m  [42/213], [94mLoss[0m : 2.94465
[1mStep[0m  [63/213], [94mLoss[0m : 1.91079
[1mStep[0m  [84/213], [94mLoss[0m : 2.49999
[1mStep[0m  [105/213], [94mLoss[0m : 2.49492
[1mStep[0m  [126/213], [94mLoss[0m : 2.47727
[1mStep[0m  [147/213], [94mLoss[0m : 2.60928
[1mStep[0m  [168/213], [94mLoss[0m : 2.01889
[1mStep[0m  [189/213], [94mLoss[0m : 2.56645
[1mStep[0m  [210/213], [94mLoss[0m : 2.31608

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.395, [92mTest[0m: 2.347, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.07174
[1mStep[0m  [21/213], [94mLoss[0m : 2.48351
[1mStep[0m  [42/213], [94mLoss[0m : 2.74120
[1mStep[0m  [63/213], [94mLoss[0m : 2.37794
[1mStep[0m  [84/213], [94mLoss[0m : 2.41791
[1mStep[0m  [105/213], [94mLoss[0m : 2.42338
[1mStep[0m  [126/213], [94mLoss[0m : 2.96969
[1mStep[0m  [147/213], [94mLoss[0m : 2.42769
[1mStep[0m  [168/213], [94mLoss[0m : 2.42758
[1mStep[0m  [189/213], [94mLoss[0m : 1.99815
[1mStep[0m  [210/213], [94mLoss[0m : 2.83310

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.370, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.44516
[1mStep[0m  [21/213], [94mLoss[0m : 2.53170
[1mStep[0m  [42/213], [94mLoss[0m : 2.40029
[1mStep[0m  [63/213], [94mLoss[0m : 2.25329
[1mStep[0m  [84/213], [94mLoss[0m : 2.51274
[1mStep[0m  [105/213], [94mLoss[0m : 2.69458
[1mStep[0m  [126/213], [94mLoss[0m : 2.74707
[1mStep[0m  [147/213], [94mLoss[0m : 2.37419
[1mStep[0m  [168/213], [94mLoss[0m : 2.56401
[1mStep[0m  [189/213], [94mLoss[0m : 2.48946
[1mStep[0m  [210/213], [94mLoss[0m : 2.52344

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.401, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.46160
[1mStep[0m  [21/213], [94mLoss[0m : 2.50417
[1mStep[0m  [42/213], [94mLoss[0m : 2.35062
[1mStep[0m  [63/213], [94mLoss[0m : 2.60986
[1mStep[0m  [84/213], [94mLoss[0m : 2.58265
[1mStep[0m  [105/213], [94mLoss[0m : 2.40224
[1mStep[0m  [126/213], [94mLoss[0m : 1.95977
[1mStep[0m  [147/213], [94mLoss[0m : 2.94155
[1mStep[0m  [168/213], [94mLoss[0m : 2.81749
[1mStep[0m  [189/213], [94mLoss[0m : 2.22566
[1mStep[0m  [210/213], [94mLoss[0m : 2.04831

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.394, [92mTest[0m: 2.368, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.50179
[1mStep[0m  [21/213], [94mLoss[0m : 2.24724
[1mStep[0m  [42/213], [94mLoss[0m : 3.63699
[1mStep[0m  [63/213], [94mLoss[0m : 2.48295
[1mStep[0m  [84/213], [94mLoss[0m : 2.31825
[1mStep[0m  [105/213], [94mLoss[0m : 2.63447
[1mStep[0m  [126/213], [94mLoss[0m : 2.31887
[1mStep[0m  [147/213], [94mLoss[0m : 2.17494
[1mStep[0m  [168/213], [94mLoss[0m : 2.44191
[1mStep[0m  [189/213], [94mLoss[0m : 1.96195
[1mStep[0m  [210/213], [94mLoss[0m : 2.11036

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.363, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.50491
[1mStep[0m  [21/213], [94mLoss[0m : 2.26498
[1mStep[0m  [42/213], [94mLoss[0m : 2.43907
[1mStep[0m  [63/213], [94mLoss[0m : 2.57015
[1mStep[0m  [84/213], [94mLoss[0m : 2.39386
[1mStep[0m  [105/213], [94mLoss[0m : 2.59273
[1mStep[0m  [126/213], [94mLoss[0m : 2.01663
[1mStep[0m  [147/213], [94mLoss[0m : 2.62100
[1mStep[0m  [168/213], [94mLoss[0m : 2.89585
[1mStep[0m  [189/213], [94mLoss[0m : 2.53134
[1mStep[0m  [210/213], [94mLoss[0m : 2.49626

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.403, [92mTest[0m: 2.420, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.39398
[1mStep[0m  [21/213], [94mLoss[0m : 2.74282
[1mStep[0m  [42/213], [94mLoss[0m : 2.52116
[1mStep[0m  [63/213], [94mLoss[0m : 2.63762
[1mStep[0m  [84/213], [94mLoss[0m : 2.39821
[1mStep[0m  [105/213], [94mLoss[0m : 2.35542
[1mStep[0m  [126/213], [94mLoss[0m : 2.29589
[1mStep[0m  [147/213], [94mLoss[0m : 2.52057
[1mStep[0m  [168/213], [94mLoss[0m : 2.71851
[1mStep[0m  [189/213], [94mLoss[0m : 2.75167
[1mStep[0m  [210/213], [94mLoss[0m : 2.36298

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.384, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.399
====================================

Phase 1 - Evaluation MAE:  2.399348946112507
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/213], [94mLoss[0m : 2.58318
[1mStep[0m  [21/213], [94mLoss[0m : 2.08006
[1mStep[0m  [42/213], [94mLoss[0m : 2.87358
[1mStep[0m  [63/213], [94mLoss[0m : 2.73680
[1mStep[0m  [84/213], [94mLoss[0m : 2.52769
[1mStep[0m  [105/213], [94mLoss[0m : 2.89030
[1mStep[0m  [126/213], [94mLoss[0m : 2.42737
[1mStep[0m  [147/213], [94mLoss[0m : 2.45563
[1mStep[0m  [168/213], [94mLoss[0m : 2.49766
[1mStep[0m  [189/213], [94mLoss[0m : 2.51232
[1mStep[0m  [210/213], [94mLoss[0m : 2.57502

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.397, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.96604
[1mStep[0m  [21/213], [94mLoss[0m : 2.19620
[1mStep[0m  [42/213], [94mLoss[0m : 2.71471
[1mStep[0m  [63/213], [94mLoss[0m : 2.31077
[1mStep[0m  [84/213], [94mLoss[0m : 2.52667
[1mStep[0m  [105/213], [94mLoss[0m : 2.92045
[1mStep[0m  [126/213], [94mLoss[0m : 2.64764
[1mStep[0m  [147/213], [94mLoss[0m : 2.22491
[1mStep[0m  [168/213], [94mLoss[0m : 2.48209
[1mStep[0m  [189/213], [94mLoss[0m : 2.21428
[1mStep[0m  [210/213], [94mLoss[0m : 2.63291

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.368, [92mTest[0m: 2.395, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.42561
[1mStep[0m  [21/213], [94mLoss[0m : 1.95047
[1mStep[0m  [42/213], [94mLoss[0m : 2.40443
[1mStep[0m  [63/213], [94mLoss[0m : 2.63685
[1mStep[0m  [84/213], [94mLoss[0m : 2.61448
[1mStep[0m  [105/213], [94mLoss[0m : 2.44767
[1mStep[0m  [126/213], [94mLoss[0m : 2.48712
[1mStep[0m  [147/213], [94mLoss[0m : 2.54629
[1mStep[0m  [168/213], [94mLoss[0m : 2.67247
[1mStep[0m  [189/213], [94mLoss[0m : 2.50601
[1mStep[0m  [210/213], [94mLoss[0m : 2.32624

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.320, [92mTest[0m: 2.417, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.99960
[1mStep[0m  [21/213], [94mLoss[0m : 2.21919
[1mStep[0m  [42/213], [94mLoss[0m : 2.35210
[1mStep[0m  [63/213], [94mLoss[0m : 2.38044
[1mStep[0m  [84/213], [94mLoss[0m : 1.97864
[1mStep[0m  [105/213], [94mLoss[0m : 2.45104
[1mStep[0m  [126/213], [94mLoss[0m : 2.76250
[1mStep[0m  [147/213], [94mLoss[0m : 2.63167
[1mStep[0m  [168/213], [94mLoss[0m : 1.85849
[1mStep[0m  [189/213], [94mLoss[0m : 2.53593
[1mStep[0m  [210/213], [94mLoss[0m : 2.24298

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.271, [92mTest[0m: 2.419, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.99381
[1mStep[0m  [21/213], [94mLoss[0m : 2.54593
[1mStep[0m  [42/213], [94mLoss[0m : 2.32183
[1mStep[0m  [63/213], [94mLoss[0m : 2.44089
[1mStep[0m  [84/213], [94mLoss[0m : 2.29071
[1mStep[0m  [105/213], [94mLoss[0m : 2.04284
[1mStep[0m  [126/213], [94mLoss[0m : 2.30514
[1mStep[0m  [147/213], [94mLoss[0m : 2.12850
[1mStep[0m  [168/213], [94mLoss[0m : 2.07416
[1mStep[0m  [189/213], [94mLoss[0m : 2.37313
[1mStep[0m  [210/213], [94mLoss[0m : 2.86662

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.244, [92mTest[0m: 2.404, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.09068
[1mStep[0m  [21/213], [94mLoss[0m : 1.91396
[1mStep[0m  [42/213], [94mLoss[0m : 2.37605
[1mStep[0m  [63/213], [94mLoss[0m : 2.26718
[1mStep[0m  [84/213], [94mLoss[0m : 2.19221
[1mStep[0m  [105/213], [94mLoss[0m : 2.40741
[1mStep[0m  [126/213], [94mLoss[0m : 2.22220
[1mStep[0m  [147/213], [94mLoss[0m : 2.38097
[1mStep[0m  [168/213], [94mLoss[0m : 2.27597
[1mStep[0m  [189/213], [94mLoss[0m : 2.15910
[1mStep[0m  [210/213], [94mLoss[0m : 2.56291

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.211, [92mTest[0m: 2.390, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.20473
[1mStep[0m  [21/213], [94mLoss[0m : 2.24451
[1mStep[0m  [42/213], [94mLoss[0m : 2.31620
[1mStep[0m  [63/213], [94mLoss[0m : 2.38286
[1mStep[0m  [84/213], [94mLoss[0m : 2.15277
[1mStep[0m  [105/213], [94mLoss[0m : 2.54920
[1mStep[0m  [126/213], [94mLoss[0m : 2.31282
[1mStep[0m  [147/213], [94mLoss[0m : 2.56741
[1mStep[0m  [168/213], [94mLoss[0m : 2.15620
[1mStep[0m  [189/213], [94mLoss[0m : 2.09150
[1mStep[0m  [210/213], [94mLoss[0m : 1.94584

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.187, [92mTest[0m: 2.404, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.12854
[1mStep[0m  [21/213], [94mLoss[0m : 2.40698
[1mStep[0m  [42/213], [94mLoss[0m : 2.40290
[1mStep[0m  [63/213], [94mLoss[0m : 2.27454
[1mStep[0m  [84/213], [94mLoss[0m : 2.31004
[1mStep[0m  [105/213], [94mLoss[0m : 2.15185
[1mStep[0m  [126/213], [94mLoss[0m : 1.77128
[1mStep[0m  [147/213], [94mLoss[0m : 2.69779
[1mStep[0m  [168/213], [94mLoss[0m : 2.60511
[1mStep[0m  [189/213], [94mLoss[0m : 2.43318
[1mStep[0m  [210/213], [94mLoss[0m : 2.33199

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.183, [92mTest[0m: 2.424, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.75221
[1mStep[0m  [21/213], [94mLoss[0m : 1.84696
[1mStep[0m  [42/213], [94mLoss[0m : 1.90735
[1mStep[0m  [63/213], [94mLoss[0m : 2.13667
[1mStep[0m  [84/213], [94mLoss[0m : 2.18854
[1mStep[0m  [105/213], [94mLoss[0m : 2.12993
[1mStep[0m  [126/213], [94mLoss[0m : 2.26036
[1mStep[0m  [147/213], [94mLoss[0m : 2.25505
[1mStep[0m  [168/213], [94mLoss[0m : 1.99426
[1mStep[0m  [189/213], [94mLoss[0m : 2.20808
[1mStep[0m  [210/213], [94mLoss[0m : 1.86020

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.160, [92mTest[0m: 2.448, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.43560
[1mStep[0m  [21/213], [94mLoss[0m : 1.94035
[1mStep[0m  [42/213], [94mLoss[0m : 2.26291
[1mStep[0m  [63/213], [94mLoss[0m : 2.07492
[1mStep[0m  [84/213], [94mLoss[0m : 2.15626
[1mStep[0m  [105/213], [94mLoss[0m : 2.27301
[1mStep[0m  [126/213], [94mLoss[0m : 2.39730
[1mStep[0m  [147/213], [94mLoss[0m : 2.31732
[1mStep[0m  [168/213], [94mLoss[0m : 2.16597
[1mStep[0m  [189/213], [94mLoss[0m : 1.99682
[1mStep[0m  [210/213], [94mLoss[0m : 2.51691

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.154, [92mTest[0m: 2.428, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.22831
[1mStep[0m  [21/213], [94mLoss[0m : 1.99093
[1mStep[0m  [42/213], [94mLoss[0m : 2.46615
[1mStep[0m  [63/213], [94mLoss[0m : 2.54406
[1mStep[0m  [84/213], [94mLoss[0m : 1.70765
[1mStep[0m  [105/213], [94mLoss[0m : 1.97548
[1mStep[0m  [126/213], [94mLoss[0m : 2.37214
[1mStep[0m  [147/213], [94mLoss[0m : 2.55505
[1mStep[0m  [168/213], [94mLoss[0m : 2.59242
[1mStep[0m  [189/213], [94mLoss[0m : 2.14243
[1mStep[0m  [210/213], [94mLoss[0m : 1.98316

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.154, [92mTest[0m: 2.410, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.10170
[1mStep[0m  [21/213], [94mLoss[0m : 2.28381
[1mStep[0m  [42/213], [94mLoss[0m : 1.94524
[1mStep[0m  [63/213], [94mLoss[0m : 1.85221
[1mStep[0m  [84/213], [94mLoss[0m : 1.97757
[1mStep[0m  [105/213], [94mLoss[0m : 1.94607
[1mStep[0m  [126/213], [94mLoss[0m : 2.39417
[1mStep[0m  [147/213], [94mLoss[0m : 2.45473
[1mStep[0m  [168/213], [94mLoss[0m : 2.10222
[1mStep[0m  [189/213], [94mLoss[0m : 2.38427
[1mStep[0m  [210/213], [94mLoss[0m : 2.60564

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.132, [92mTest[0m: 2.423, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.73281
[1mStep[0m  [21/213], [94mLoss[0m : 1.73087
[1mStep[0m  [42/213], [94mLoss[0m : 2.04545
[1mStep[0m  [63/213], [94mLoss[0m : 2.12300
[1mStep[0m  [84/213], [94mLoss[0m : 2.28950
[1mStep[0m  [105/213], [94mLoss[0m : 1.83582
[1mStep[0m  [126/213], [94mLoss[0m : 2.20115
[1mStep[0m  [147/213], [94mLoss[0m : 2.08161
[1mStep[0m  [168/213], [94mLoss[0m : 2.02110
[1mStep[0m  [189/213], [94mLoss[0m : 2.00565
[1mStep[0m  [210/213], [94mLoss[0m : 2.21967

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.106, [92mTest[0m: 2.464, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.82400
[1mStep[0m  [21/213], [94mLoss[0m : 1.91936
[1mStep[0m  [42/213], [94mLoss[0m : 2.03811
[1mStep[0m  [63/213], [94mLoss[0m : 1.94992
[1mStep[0m  [84/213], [94mLoss[0m : 2.07071
[1mStep[0m  [105/213], [94mLoss[0m : 2.64801
[1mStep[0m  [126/213], [94mLoss[0m : 1.64818
[1mStep[0m  [147/213], [94mLoss[0m : 2.21193
[1mStep[0m  [168/213], [94mLoss[0m : 2.27173
[1mStep[0m  [189/213], [94mLoss[0m : 2.21814
[1mStep[0m  [210/213], [94mLoss[0m : 1.92229

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.108, [92mTest[0m: 2.448, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.80658
[1mStep[0m  [21/213], [94mLoss[0m : 2.22163
[1mStep[0m  [42/213], [94mLoss[0m : 1.76813
[1mStep[0m  [63/213], [94mLoss[0m : 2.04231
[1mStep[0m  [84/213], [94mLoss[0m : 2.00843
[1mStep[0m  [105/213], [94mLoss[0m : 2.43868
[1mStep[0m  [126/213], [94mLoss[0m : 2.12578
[1mStep[0m  [147/213], [94mLoss[0m : 2.36411
[1mStep[0m  [168/213], [94mLoss[0m : 2.02948
[1mStep[0m  [189/213], [94mLoss[0m : 2.14216
[1mStep[0m  [210/213], [94mLoss[0m : 1.92113

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.084, [92mTest[0m: 2.449, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.91863
[1mStep[0m  [21/213], [94mLoss[0m : 1.61797
[1mStep[0m  [42/213], [94mLoss[0m : 1.96040
[1mStep[0m  [63/213], [94mLoss[0m : 2.00488
[1mStep[0m  [84/213], [94mLoss[0m : 2.39792
[1mStep[0m  [105/213], [94mLoss[0m : 2.09298
[1mStep[0m  [126/213], [94mLoss[0m : 2.05517
[1mStep[0m  [147/213], [94mLoss[0m : 1.90203
[1mStep[0m  [168/213], [94mLoss[0m : 2.12616
[1mStep[0m  [189/213], [94mLoss[0m : 1.93287
[1mStep[0m  [210/213], [94mLoss[0m : 1.90542

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.094, [92mTest[0m: 2.464, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.02649
[1mStep[0m  [21/213], [94mLoss[0m : 1.95184
[1mStep[0m  [42/213], [94mLoss[0m : 2.28479
[1mStep[0m  [63/213], [94mLoss[0m : 2.12969
[1mStep[0m  [84/213], [94mLoss[0m : 2.24917
[1mStep[0m  [105/213], [94mLoss[0m : 2.15699
[1mStep[0m  [126/213], [94mLoss[0m : 2.06959
[1mStep[0m  [147/213], [94mLoss[0m : 2.31327
[1mStep[0m  [168/213], [94mLoss[0m : 2.29459
[1mStep[0m  [189/213], [94mLoss[0m : 2.17848
[1mStep[0m  [210/213], [94mLoss[0m : 2.18709

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.087, [92mTest[0m: 2.471, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.77506
[1mStep[0m  [21/213], [94mLoss[0m : 2.08298
[1mStep[0m  [42/213], [94mLoss[0m : 1.74438
[1mStep[0m  [63/213], [94mLoss[0m : 2.38154
[1mStep[0m  [84/213], [94mLoss[0m : 1.86098
[1mStep[0m  [105/213], [94mLoss[0m : 1.74655
[1mStep[0m  [126/213], [94mLoss[0m : 1.82459
[1mStep[0m  [147/213], [94mLoss[0m : 2.20839
[1mStep[0m  [168/213], [94mLoss[0m : 1.76849
[1mStep[0m  [189/213], [94mLoss[0m : 1.84668
[1mStep[0m  [210/213], [94mLoss[0m : 1.92715

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.068, [92mTest[0m: 2.420, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.22783
[1mStep[0m  [21/213], [94mLoss[0m : 1.93275
[1mStep[0m  [42/213], [94mLoss[0m : 2.10391
[1mStep[0m  [63/213], [94mLoss[0m : 1.81016
[1mStep[0m  [84/213], [94mLoss[0m : 1.94331
[1mStep[0m  [105/213], [94mLoss[0m : 2.03513
[1mStep[0m  [126/213], [94mLoss[0m : 2.54145
[1mStep[0m  [147/213], [94mLoss[0m : 2.04885
[1mStep[0m  [168/213], [94mLoss[0m : 1.67361
[1mStep[0m  [189/213], [94mLoss[0m : 2.43632
[1mStep[0m  [210/213], [94mLoss[0m : 1.84996

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.049, [92mTest[0m: 2.470, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.92569
[1mStep[0m  [21/213], [94mLoss[0m : 1.93879
[1mStep[0m  [42/213], [94mLoss[0m : 1.77725
[1mStep[0m  [63/213], [94mLoss[0m : 1.81417
[1mStep[0m  [84/213], [94mLoss[0m : 2.16606
[1mStep[0m  [105/213], [94mLoss[0m : 2.13740
[1mStep[0m  [126/213], [94mLoss[0m : 1.82820
[1mStep[0m  [147/213], [94mLoss[0m : 2.16498
[1mStep[0m  [168/213], [94mLoss[0m : 1.69416
[1mStep[0m  [189/213], [94mLoss[0m : 2.46091
[1mStep[0m  [210/213], [94mLoss[0m : 1.96493

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.045, [92mTest[0m: 2.470, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.74062
[1mStep[0m  [21/213], [94mLoss[0m : 1.76731
[1mStep[0m  [42/213], [94mLoss[0m : 1.97194
[1mStep[0m  [63/213], [94mLoss[0m : 1.74526
[1mStep[0m  [84/213], [94mLoss[0m : 1.78872
[1mStep[0m  [105/213], [94mLoss[0m : 1.94116
[1mStep[0m  [126/213], [94mLoss[0m : 1.98557
[1mStep[0m  [147/213], [94mLoss[0m : 1.94107
[1mStep[0m  [168/213], [94mLoss[0m : 1.90283
[1mStep[0m  [189/213], [94mLoss[0m : 2.34410
[1mStep[0m  [210/213], [94mLoss[0m : 2.25154

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.991, [92mTest[0m: 2.436, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.26239
[1mStep[0m  [21/213], [94mLoss[0m : 1.83373
[1mStep[0m  [42/213], [94mLoss[0m : 1.83715
[1mStep[0m  [63/213], [94mLoss[0m : 1.98843
[1mStep[0m  [84/213], [94mLoss[0m : 2.07414
[1mStep[0m  [105/213], [94mLoss[0m : 2.13954
[1mStep[0m  [126/213], [94mLoss[0m : 1.86401
[1mStep[0m  [147/213], [94mLoss[0m : 1.83474
[1mStep[0m  [168/213], [94mLoss[0m : 1.76063
[1mStep[0m  [189/213], [94mLoss[0m : 1.65389
[1mStep[0m  [210/213], [94mLoss[0m : 1.77193

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.978, [92mTest[0m: 2.485, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.03698
[1mStep[0m  [21/213], [94mLoss[0m : 1.79197
[1mStep[0m  [42/213], [94mLoss[0m : 2.13994
[1mStep[0m  [63/213], [94mLoss[0m : 2.00421
[1mStep[0m  [84/213], [94mLoss[0m : 1.62351
[1mStep[0m  [105/213], [94mLoss[0m : 1.92651
[1mStep[0m  [126/213], [94mLoss[0m : 1.83730
[1mStep[0m  [147/213], [94mLoss[0m : 1.71936
[1mStep[0m  [168/213], [94mLoss[0m : 2.23763
[1mStep[0m  [189/213], [94mLoss[0m : 2.23893
[1mStep[0m  [210/213], [94mLoss[0m : 2.06642

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.003, [92mTest[0m: 2.474, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.97172
[1mStep[0m  [21/213], [94mLoss[0m : 1.97517
[1mStep[0m  [42/213], [94mLoss[0m : 2.33863
[1mStep[0m  [63/213], [94mLoss[0m : 2.02013
[1mStep[0m  [84/213], [94mLoss[0m : 2.11296
[1mStep[0m  [105/213], [94mLoss[0m : 2.04853
[1mStep[0m  [126/213], [94mLoss[0m : 1.72102
[1mStep[0m  [147/213], [94mLoss[0m : 1.55578
[1mStep[0m  [168/213], [94mLoss[0m : 1.84860
[1mStep[0m  [189/213], [94mLoss[0m : 2.79273
[1mStep[0m  [210/213], [94mLoss[0m : 2.20652

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.978, [92mTest[0m: 2.467, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.26905
[1mStep[0m  [21/213], [94mLoss[0m : 1.63136
[1mStep[0m  [42/213], [94mLoss[0m : 2.00895
[1mStep[0m  [63/213], [94mLoss[0m : 1.74304
[1mStep[0m  [84/213], [94mLoss[0m : 2.15382
[1mStep[0m  [105/213], [94mLoss[0m : 2.00174
[1mStep[0m  [126/213], [94mLoss[0m : 1.92225
[1mStep[0m  [147/213], [94mLoss[0m : 2.00754
[1mStep[0m  [168/213], [94mLoss[0m : 2.05332
[1mStep[0m  [189/213], [94mLoss[0m : 2.03754
[1mStep[0m  [210/213], [94mLoss[0m : 1.94503

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.965, [92mTest[0m: 2.572, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.49393
[1mStep[0m  [21/213], [94mLoss[0m : 2.09223
[1mStep[0m  [42/213], [94mLoss[0m : 1.88053
[1mStep[0m  [63/213], [94mLoss[0m : 1.99202
[1mStep[0m  [84/213], [94mLoss[0m : 2.10032
[1mStep[0m  [105/213], [94mLoss[0m : 1.90049
[1mStep[0m  [126/213], [94mLoss[0m : 2.03069
[1mStep[0m  [147/213], [94mLoss[0m : 2.08765
[1mStep[0m  [168/213], [94mLoss[0m : 1.99085
[1mStep[0m  [189/213], [94mLoss[0m : 2.10044
[1mStep[0m  [210/213], [94mLoss[0m : 1.94693

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.966, [92mTest[0m: 2.518, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.80558
[1mStep[0m  [21/213], [94mLoss[0m : 1.64984
[1mStep[0m  [42/213], [94mLoss[0m : 1.61359
[1mStep[0m  [63/213], [94mLoss[0m : 1.88203
[1mStep[0m  [84/213], [94mLoss[0m : 1.96037
[1mStep[0m  [105/213], [94mLoss[0m : 2.10451
[1mStep[0m  [126/213], [94mLoss[0m : 2.22285
[1mStep[0m  [147/213], [94mLoss[0m : 2.19058
[1mStep[0m  [168/213], [94mLoss[0m : 1.67558
[1mStep[0m  [189/213], [94mLoss[0m : 1.87077
[1mStep[0m  [210/213], [94mLoss[0m : 1.95129

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.959, [92mTest[0m: 2.469, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.49919
[1mStep[0m  [21/213], [94mLoss[0m : 1.76211
[1mStep[0m  [42/213], [94mLoss[0m : 1.53089
[1mStep[0m  [63/213], [94mLoss[0m : 2.45225
[1mStep[0m  [84/213], [94mLoss[0m : 2.15651
[1mStep[0m  [105/213], [94mLoss[0m : 2.16305
[1mStep[0m  [126/213], [94mLoss[0m : 2.08402
[1mStep[0m  [147/213], [94mLoss[0m : 2.21413
[1mStep[0m  [168/213], [94mLoss[0m : 1.99689
[1mStep[0m  [189/213], [94mLoss[0m : 1.82672
[1mStep[0m  [210/213], [94mLoss[0m : 1.60914

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.957, [92mTest[0m: 2.478, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.58351
[1mStep[0m  [21/213], [94mLoss[0m : 1.83656
[1mStep[0m  [42/213], [94mLoss[0m : 1.54642
[1mStep[0m  [63/213], [94mLoss[0m : 2.02411
[1mStep[0m  [84/213], [94mLoss[0m : 1.87811
[1mStep[0m  [105/213], [94mLoss[0m : 1.97684
[1mStep[0m  [126/213], [94mLoss[0m : 2.13141
[1mStep[0m  [147/213], [94mLoss[0m : 1.90967
[1mStep[0m  [168/213], [94mLoss[0m : 1.81473
[1mStep[0m  [189/213], [94mLoss[0m : 1.97949
[1mStep[0m  [210/213], [94mLoss[0m : 1.86594

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.956, [92mTest[0m: 2.557, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.77778
[1mStep[0m  [21/213], [94mLoss[0m : 1.42890
[1mStep[0m  [42/213], [94mLoss[0m : 2.09169
[1mStep[0m  [63/213], [94mLoss[0m : 1.76234
[1mStep[0m  [84/213], [94mLoss[0m : 2.04538
[1mStep[0m  [105/213], [94mLoss[0m : 1.84795
[1mStep[0m  [126/213], [94mLoss[0m : 2.19635
[1mStep[0m  [147/213], [94mLoss[0m : 1.98068
[1mStep[0m  [168/213], [94mLoss[0m : 2.27466
[1mStep[0m  [189/213], [94mLoss[0m : 2.30814
[1mStep[0m  [210/213], [94mLoss[0m : 2.32009

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.955, [92mTest[0m: 2.527, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.505
====================================

Phase 2 - Evaluation MAE:  2.504933460703436
MAE score P1      2.399349
MAE score P2      2.504933
loss              1.954764
learning_rate      0.00505
batch_size              64
hidden_sizes         [100]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.9
weight_decay          0.01
Name: 7, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/213], [94mLoss[0m : 11.18210
[1mStep[0m  [21/213], [94mLoss[0m : 10.39471
[1mStep[0m  [42/213], [94mLoss[0m : 10.72690
[1mStep[0m  [63/213], [94mLoss[0m : 10.61661
[1mStep[0m  [84/213], [94mLoss[0m : 10.33934
[1mStep[0m  [105/213], [94mLoss[0m : 9.09980
[1mStep[0m  [126/213], [94mLoss[0m : 8.30363
[1mStep[0m  [147/213], [94mLoss[0m : 9.31984
[1mStep[0m  [168/213], [94mLoss[0m : 8.75905
[1mStep[0m  [189/213], [94mLoss[0m : 8.27839
[1mStep[0m  [210/213], [94mLoss[0m : 7.17550

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.444, [92mTest[0m: 10.783, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 7.04031
[1mStep[0m  [21/213], [94mLoss[0m : 6.26476
[1mStep[0m  [42/213], [94mLoss[0m : 6.64935
[1mStep[0m  [63/213], [94mLoss[0m : 5.81523
[1mStep[0m  [84/213], [94mLoss[0m : 5.10434
[1mStep[0m  [105/213], [94mLoss[0m : 5.36775
[1mStep[0m  [126/213], [94mLoss[0m : 4.62736
[1mStep[0m  [147/213], [94mLoss[0m : 4.09115
[1mStep[0m  [168/213], [94mLoss[0m : 3.44085
[1mStep[0m  [189/213], [94mLoss[0m : 2.91140
[1mStep[0m  [210/213], [94mLoss[0m : 3.93790

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 5.054, [92mTest[0m: 7.182, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.51965
[1mStep[0m  [21/213], [94mLoss[0m : 2.38075
[1mStep[0m  [42/213], [94mLoss[0m : 3.00719
[1mStep[0m  [63/213], [94mLoss[0m : 2.46040
[1mStep[0m  [84/213], [94mLoss[0m : 2.51178
[1mStep[0m  [105/213], [94mLoss[0m : 2.66690
[1mStep[0m  [126/213], [94mLoss[0m : 2.56525
[1mStep[0m  [147/213], [94mLoss[0m : 2.45562
[1mStep[0m  [168/213], [94mLoss[0m : 2.75900
[1mStep[0m  [189/213], [94mLoss[0m : 2.99816
[1mStep[0m  [210/213], [94mLoss[0m : 3.05539

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.727, [92mTest[0m: 2.748, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.83348
[1mStep[0m  [21/213], [94mLoss[0m : 2.81032
[1mStep[0m  [42/213], [94mLoss[0m : 2.66106
[1mStep[0m  [63/213], [94mLoss[0m : 2.68972
[1mStep[0m  [84/213], [94mLoss[0m : 2.25916
[1mStep[0m  [105/213], [94mLoss[0m : 2.45488
[1mStep[0m  [126/213], [94mLoss[0m : 2.94239
[1mStep[0m  [147/213], [94mLoss[0m : 2.57741
[1mStep[0m  [168/213], [94mLoss[0m : 2.68723
[1mStep[0m  [189/213], [94mLoss[0m : 2.68912
[1mStep[0m  [210/213], [94mLoss[0m : 2.74299

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.614, [92mTest[0m: 2.429, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.43961
[1mStep[0m  [21/213], [94mLoss[0m : 2.49379
[1mStep[0m  [42/213], [94mLoss[0m : 2.54276
[1mStep[0m  [63/213], [94mLoss[0m : 2.50265
[1mStep[0m  [84/213], [94mLoss[0m : 2.35288
[1mStep[0m  [105/213], [94mLoss[0m : 2.62038
[1mStep[0m  [126/213], [94mLoss[0m : 2.69001
[1mStep[0m  [147/213], [94mLoss[0m : 2.50831
[1mStep[0m  [168/213], [94mLoss[0m : 2.73257
[1mStep[0m  [189/213], [94mLoss[0m : 2.71036
[1mStep[0m  [210/213], [94mLoss[0m : 2.85776

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.582, [92mTest[0m: 2.416, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.73809
[1mStep[0m  [21/213], [94mLoss[0m : 2.87949
[1mStep[0m  [42/213], [94mLoss[0m : 2.72134
[1mStep[0m  [63/213], [94mLoss[0m : 2.61327
[1mStep[0m  [84/213], [94mLoss[0m : 2.20357
[1mStep[0m  [105/213], [94mLoss[0m : 2.28030
[1mStep[0m  [126/213], [94mLoss[0m : 2.87450
[1mStep[0m  [147/213], [94mLoss[0m : 3.18598
[1mStep[0m  [168/213], [94mLoss[0m : 2.39952
[1mStep[0m  [189/213], [94mLoss[0m : 2.37142
[1mStep[0m  [210/213], [94mLoss[0m : 2.38007

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.560, [92mTest[0m: 2.428, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.41753
[1mStep[0m  [21/213], [94mLoss[0m : 2.70524
[1mStep[0m  [42/213], [94mLoss[0m : 2.54530
[1mStep[0m  [63/213], [94mLoss[0m : 2.25311
[1mStep[0m  [84/213], [94mLoss[0m : 2.24515
[1mStep[0m  [105/213], [94mLoss[0m : 3.02277
[1mStep[0m  [126/213], [94mLoss[0m : 2.16624
[1mStep[0m  [147/213], [94mLoss[0m : 2.28591
[1mStep[0m  [168/213], [94mLoss[0m : 3.07484
[1mStep[0m  [189/213], [94mLoss[0m : 2.31700
[1mStep[0m  [210/213], [94mLoss[0m : 2.23271

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.538, [92mTest[0m: 2.415, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.36513
[1mStep[0m  [21/213], [94mLoss[0m : 2.87162
[1mStep[0m  [42/213], [94mLoss[0m : 2.38613
[1mStep[0m  [63/213], [94mLoss[0m : 2.19803
[1mStep[0m  [84/213], [94mLoss[0m : 2.67856
[1mStep[0m  [105/213], [94mLoss[0m : 2.78095
[1mStep[0m  [126/213], [94mLoss[0m : 2.28383
[1mStep[0m  [147/213], [94mLoss[0m : 2.37046
[1mStep[0m  [168/213], [94mLoss[0m : 2.46831
[1mStep[0m  [189/213], [94mLoss[0m : 2.72011
[1mStep[0m  [210/213], [94mLoss[0m : 2.51539

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.403, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.24535
[1mStep[0m  [21/213], [94mLoss[0m : 2.53910
[1mStep[0m  [42/213], [94mLoss[0m : 2.22083
[1mStep[0m  [63/213], [94mLoss[0m : 2.76147
[1mStep[0m  [84/213], [94mLoss[0m : 2.49231
[1mStep[0m  [105/213], [94mLoss[0m : 2.37153
[1mStep[0m  [126/213], [94mLoss[0m : 2.44289
[1mStep[0m  [147/213], [94mLoss[0m : 2.89522
[1mStep[0m  [168/213], [94mLoss[0m : 2.27551
[1mStep[0m  [189/213], [94mLoss[0m : 2.24788
[1mStep[0m  [210/213], [94mLoss[0m : 2.45364

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.391, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.70575
[1mStep[0m  [21/213], [94mLoss[0m : 2.50679
[1mStep[0m  [42/213], [94mLoss[0m : 2.48666
[1mStep[0m  [63/213], [94mLoss[0m : 2.51839
[1mStep[0m  [84/213], [94mLoss[0m : 2.51985
[1mStep[0m  [105/213], [94mLoss[0m : 2.51979
[1mStep[0m  [126/213], [94mLoss[0m : 2.37069
[1mStep[0m  [147/213], [94mLoss[0m : 2.68731
[1mStep[0m  [168/213], [94mLoss[0m : 2.82769
[1mStep[0m  [189/213], [94mLoss[0m : 2.77538
[1mStep[0m  [210/213], [94mLoss[0m : 2.09794

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.411, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.65509
[1mStep[0m  [21/213], [94mLoss[0m : 2.48409
[1mStep[0m  [42/213], [94mLoss[0m : 2.97697
[1mStep[0m  [63/213], [94mLoss[0m : 2.63212
[1mStep[0m  [84/213], [94mLoss[0m : 2.30905
[1mStep[0m  [105/213], [94mLoss[0m : 2.14659
[1mStep[0m  [126/213], [94mLoss[0m : 2.97348
[1mStep[0m  [147/213], [94mLoss[0m : 2.60731
[1mStep[0m  [168/213], [94mLoss[0m : 2.48463
[1mStep[0m  [189/213], [94mLoss[0m : 2.94927
[1mStep[0m  [210/213], [94mLoss[0m : 2.80788

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.381, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.33199
[1mStep[0m  [21/213], [94mLoss[0m : 2.51423
[1mStep[0m  [42/213], [94mLoss[0m : 2.14145
[1mStep[0m  [63/213], [94mLoss[0m : 2.96511
[1mStep[0m  [84/213], [94mLoss[0m : 2.86477
[1mStep[0m  [105/213], [94mLoss[0m : 2.70767
[1mStep[0m  [126/213], [94mLoss[0m : 2.22184
[1mStep[0m  [147/213], [94mLoss[0m : 2.41117
[1mStep[0m  [168/213], [94mLoss[0m : 2.60439
[1mStep[0m  [189/213], [94mLoss[0m : 2.52396
[1mStep[0m  [210/213], [94mLoss[0m : 2.47815

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.383, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.71892
[1mStep[0m  [21/213], [94mLoss[0m : 2.59240
[1mStep[0m  [42/213], [94mLoss[0m : 2.60790
[1mStep[0m  [63/213], [94mLoss[0m : 2.41475
[1mStep[0m  [84/213], [94mLoss[0m : 2.42677
[1mStep[0m  [105/213], [94mLoss[0m : 2.06394
[1mStep[0m  [126/213], [94mLoss[0m : 2.39458
[1mStep[0m  [147/213], [94mLoss[0m : 2.51545
[1mStep[0m  [168/213], [94mLoss[0m : 2.81463
[1mStep[0m  [189/213], [94mLoss[0m : 2.57240
[1mStep[0m  [210/213], [94mLoss[0m : 2.53761

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.397, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.36020
[1mStep[0m  [21/213], [94mLoss[0m : 2.58592
[1mStep[0m  [42/213], [94mLoss[0m : 2.63353
[1mStep[0m  [63/213], [94mLoss[0m : 2.54872
[1mStep[0m  [84/213], [94mLoss[0m : 2.77806
[1mStep[0m  [105/213], [94mLoss[0m : 2.55645
[1mStep[0m  [126/213], [94mLoss[0m : 2.58380
[1mStep[0m  [147/213], [94mLoss[0m : 2.23531
[1mStep[0m  [168/213], [94mLoss[0m : 2.45111
[1mStep[0m  [189/213], [94mLoss[0m : 2.37468
[1mStep[0m  [210/213], [94mLoss[0m : 2.70160

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.397, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.54465
[1mStep[0m  [21/213], [94mLoss[0m : 2.70395
[1mStep[0m  [42/213], [94mLoss[0m : 2.58284
[1mStep[0m  [63/213], [94mLoss[0m : 2.52557
[1mStep[0m  [84/213], [94mLoss[0m : 2.70741
[1mStep[0m  [105/213], [94mLoss[0m : 2.59705
[1mStep[0m  [126/213], [94mLoss[0m : 2.54836
[1mStep[0m  [147/213], [94mLoss[0m : 2.56564
[1mStep[0m  [168/213], [94mLoss[0m : 2.52376
[1mStep[0m  [189/213], [94mLoss[0m : 2.82961
[1mStep[0m  [210/213], [94mLoss[0m : 2.49542

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.379, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.31712
[1mStep[0m  [21/213], [94mLoss[0m : 2.25617
[1mStep[0m  [42/213], [94mLoss[0m : 2.38837
[1mStep[0m  [63/213], [94mLoss[0m : 2.48308
[1mStep[0m  [84/213], [94mLoss[0m : 2.13167
[1mStep[0m  [105/213], [94mLoss[0m : 2.87571
[1mStep[0m  [126/213], [94mLoss[0m : 2.57325
[1mStep[0m  [147/213], [94mLoss[0m : 2.44920
[1mStep[0m  [168/213], [94mLoss[0m : 2.61798
[1mStep[0m  [189/213], [94mLoss[0m : 2.59292
[1mStep[0m  [210/213], [94mLoss[0m : 2.23536

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.387, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.78284
[1mStep[0m  [21/213], [94mLoss[0m : 2.80010
[1mStep[0m  [42/213], [94mLoss[0m : 2.69317
[1mStep[0m  [63/213], [94mLoss[0m : 2.43937
[1mStep[0m  [84/213], [94mLoss[0m : 2.26405
[1mStep[0m  [105/213], [94mLoss[0m : 2.54421
[1mStep[0m  [126/213], [94mLoss[0m : 2.42441
[1mStep[0m  [147/213], [94mLoss[0m : 2.26210
[1mStep[0m  [168/213], [94mLoss[0m : 2.58656
[1mStep[0m  [189/213], [94mLoss[0m : 2.09103
[1mStep[0m  [210/213], [94mLoss[0m : 2.80074

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.375, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.22272
[1mStep[0m  [21/213], [94mLoss[0m : 2.32390
[1mStep[0m  [42/213], [94mLoss[0m : 2.59985
[1mStep[0m  [63/213], [94mLoss[0m : 2.35871
[1mStep[0m  [84/213], [94mLoss[0m : 2.45736
[1mStep[0m  [105/213], [94mLoss[0m : 2.42310
[1mStep[0m  [126/213], [94mLoss[0m : 2.84612
[1mStep[0m  [147/213], [94mLoss[0m : 2.45363
[1mStep[0m  [168/213], [94mLoss[0m : 2.51839
[1mStep[0m  [189/213], [94mLoss[0m : 2.60968
[1mStep[0m  [210/213], [94mLoss[0m : 2.37234

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.382, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.45727
[1mStep[0m  [21/213], [94mLoss[0m : 2.47425
[1mStep[0m  [42/213], [94mLoss[0m : 2.37549
[1mStep[0m  [63/213], [94mLoss[0m : 2.74745
[1mStep[0m  [84/213], [94mLoss[0m : 1.95209
[1mStep[0m  [105/213], [94mLoss[0m : 2.13888
[1mStep[0m  [126/213], [94mLoss[0m : 2.11451
[1mStep[0m  [147/213], [94mLoss[0m : 2.68193
[1mStep[0m  [168/213], [94mLoss[0m : 2.12916
[1mStep[0m  [189/213], [94mLoss[0m : 2.54707
[1mStep[0m  [210/213], [94mLoss[0m : 2.72557

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.378, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.21780
[1mStep[0m  [21/213], [94mLoss[0m : 2.75419
[1mStep[0m  [42/213], [94mLoss[0m : 2.45870
[1mStep[0m  [63/213], [94mLoss[0m : 2.31127
[1mStep[0m  [84/213], [94mLoss[0m : 2.56119
[1mStep[0m  [105/213], [94mLoss[0m : 2.35528
[1mStep[0m  [126/213], [94mLoss[0m : 2.15838
[1mStep[0m  [147/213], [94mLoss[0m : 2.50847
[1mStep[0m  [168/213], [94mLoss[0m : 2.57111
[1mStep[0m  [189/213], [94mLoss[0m : 2.59472
[1mStep[0m  [210/213], [94mLoss[0m : 2.30602

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.376, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.13378
[1mStep[0m  [21/213], [94mLoss[0m : 2.65322
[1mStep[0m  [42/213], [94mLoss[0m : 2.37382
[1mStep[0m  [63/213], [94mLoss[0m : 2.32470
[1mStep[0m  [84/213], [94mLoss[0m : 2.68966
[1mStep[0m  [105/213], [94mLoss[0m : 2.20299
[1mStep[0m  [126/213], [94mLoss[0m : 2.40386
[1mStep[0m  [147/213], [94mLoss[0m : 2.59814
[1mStep[0m  [168/213], [94mLoss[0m : 2.10155
[1mStep[0m  [189/213], [94mLoss[0m : 2.55203
[1mStep[0m  [210/213], [94mLoss[0m : 2.66750

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.376, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.42419
[1mStep[0m  [21/213], [94mLoss[0m : 2.41996
[1mStep[0m  [42/213], [94mLoss[0m : 2.52675
[1mStep[0m  [63/213], [94mLoss[0m : 2.76478
[1mStep[0m  [84/213], [94mLoss[0m : 2.53763
[1mStep[0m  [105/213], [94mLoss[0m : 2.45105
[1mStep[0m  [126/213], [94mLoss[0m : 2.64915
[1mStep[0m  [147/213], [94mLoss[0m : 2.23230
[1mStep[0m  [168/213], [94mLoss[0m : 2.51673
[1mStep[0m  [189/213], [94mLoss[0m : 2.35990
[1mStep[0m  [210/213], [94mLoss[0m : 2.54059

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.387, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.39032
[1mStep[0m  [21/213], [94mLoss[0m : 2.51419
[1mStep[0m  [42/213], [94mLoss[0m : 2.23221
[1mStep[0m  [63/213], [94mLoss[0m : 2.45902
[1mStep[0m  [84/213], [94mLoss[0m : 2.46864
[1mStep[0m  [105/213], [94mLoss[0m : 2.11336
[1mStep[0m  [126/213], [94mLoss[0m : 2.48502
[1mStep[0m  [147/213], [94mLoss[0m : 2.27392
[1mStep[0m  [168/213], [94mLoss[0m : 2.43559
[1mStep[0m  [189/213], [94mLoss[0m : 2.85488
[1mStep[0m  [210/213], [94mLoss[0m : 2.45164

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.399, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.22344
[1mStep[0m  [21/213], [94mLoss[0m : 2.67169
[1mStep[0m  [42/213], [94mLoss[0m : 2.51501
[1mStep[0m  [63/213], [94mLoss[0m : 2.29492
[1mStep[0m  [84/213], [94mLoss[0m : 2.45345
[1mStep[0m  [105/213], [94mLoss[0m : 2.34482
[1mStep[0m  [126/213], [94mLoss[0m : 2.54784
[1mStep[0m  [147/213], [94mLoss[0m : 2.30552
[1mStep[0m  [168/213], [94mLoss[0m : 2.09796
[1mStep[0m  [189/213], [94mLoss[0m : 2.57242
[1mStep[0m  [210/213], [94mLoss[0m : 2.55976

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.386, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.65160
[1mStep[0m  [21/213], [94mLoss[0m : 2.28673
[1mStep[0m  [42/213], [94mLoss[0m : 1.93381
[1mStep[0m  [63/213], [94mLoss[0m : 2.20662
[1mStep[0m  [84/213], [94mLoss[0m : 2.09432
[1mStep[0m  [105/213], [94mLoss[0m : 2.58948
[1mStep[0m  [126/213], [94mLoss[0m : 2.24731
[1mStep[0m  [147/213], [94mLoss[0m : 2.44785
[1mStep[0m  [168/213], [94mLoss[0m : 2.36764
[1mStep[0m  [189/213], [94mLoss[0m : 2.16027
[1mStep[0m  [210/213], [94mLoss[0m : 2.37869

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.386, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.17789
[1mStep[0m  [21/213], [94mLoss[0m : 2.52903
[1mStep[0m  [42/213], [94mLoss[0m : 2.65437
[1mStep[0m  [63/213], [94mLoss[0m : 2.38377
[1mStep[0m  [84/213], [94mLoss[0m : 2.69135
[1mStep[0m  [105/213], [94mLoss[0m : 2.26322
[1mStep[0m  [126/213], [94mLoss[0m : 2.48404
[1mStep[0m  [147/213], [94mLoss[0m : 2.34688
[1mStep[0m  [168/213], [94mLoss[0m : 2.36376
[1mStep[0m  [189/213], [94mLoss[0m : 2.18694
[1mStep[0m  [210/213], [94mLoss[0m : 2.27352

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.387, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.25675
[1mStep[0m  [21/213], [94mLoss[0m : 2.38087
[1mStep[0m  [42/213], [94mLoss[0m : 2.63224
[1mStep[0m  [63/213], [94mLoss[0m : 1.93205
[1mStep[0m  [84/213], [94mLoss[0m : 1.95952
[1mStep[0m  [105/213], [94mLoss[0m : 2.51697
[1mStep[0m  [126/213], [94mLoss[0m : 2.51887
[1mStep[0m  [147/213], [94mLoss[0m : 2.42500
[1mStep[0m  [168/213], [94mLoss[0m : 2.49779
[1mStep[0m  [189/213], [94mLoss[0m : 2.58951
[1mStep[0m  [210/213], [94mLoss[0m : 2.57573

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.373, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.37390
[1mStep[0m  [21/213], [94mLoss[0m : 2.53324
[1mStep[0m  [42/213], [94mLoss[0m : 2.44177
[1mStep[0m  [63/213], [94mLoss[0m : 2.44653
[1mStep[0m  [84/213], [94mLoss[0m : 2.22657
[1mStep[0m  [105/213], [94mLoss[0m : 2.67599
[1mStep[0m  [126/213], [94mLoss[0m : 2.42709
[1mStep[0m  [147/213], [94mLoss[0m : 2.23068
[1mStep[0m  [168/213], [94mLoss[0m : 2.51413
[1mStep[0m  [189/213], [94mLoss[0m : 2.02054
[1mStep[0m  [210/213], [94mLoss[0m : 2.62726

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.365, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.50076
[1mStep[0m  [21/213], [94mLoss[0m : 2.59355
[1mStep[0m  [42/213], [94mLoss[0m : 2.50295
[1mStep[0m  [63/213], [94mLoss[0m : 2.87552
[1mStep[0m  [84/213], [94mLoss[0m : 2.40597
[1mStep[0m  [105/213], [94mLoss[0m : 2.45476
[1mStep[0m  [126/213], [94mLoss[0m : 2.30433
[1mStep[0m  [147/213], [94mLoss[0m : 2.44687
[1mStep[0m  [168/213], [94mLoss[0m : 2.68207
[1mStep[0m  [189/213], [94mLoss[0m : 2.58585
[1mStep[0m  [210/213], [94mLoss[0m : 2.60476

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.371, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.20460
[1mStep[0m  [21/213], [94mLoss[0m : 2.13934
[1mStep[0m  [42/213], [94mLoss[0m : 2.31150
[1mStep[0m  [63/213], [94mLoss[0m : 2.37226
[1mStep[0m  [84/213], [94mLoss[0m : 2.24805
[1mStep[0m  [105/213], [94mLoss[0m : 2.42297
[1mStep[0m  [126/213], [94mLoss[0m : 2.70549
[1mStep[0m  [147/213], [94mLoss[0m : 2.72673
[1mStep[0m  [168/213], [94mLoss[0m : 2.53519
[1mStep[0m  [189/213], [94mLoss[0m : 2.14225
[1mStep[0m  [210/213], [94mLoss[0m : 2.56056

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.383, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.377
====================================

Phase 1 - Evaluation MAE:  2.3766448149141275
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/213], [94mLoss[0m : 2.35745
[1mStep[0m  [21/213], [94mLoss[0m : 2.69041
[1mStep[0m  [42/213], [94mLoss[0m : 2.51598
[1mStep[0m  [63/213], [94mLoss[0m : 2.44280
[1mStep[0m  [84/213], [94mLoss[0m : 2.49073
[1mStep[0m  [105/213], [94mLoss[0m : 2.41851
[1mStep[0m  [126/213], [94mLoss[0m : 2.22473
[1mStep[0m  [147/213], [94mLoss[0m : 2.22460
[1mStep[0m  [168/213], [94mLoss[0m : 2.65837
[1mStep[0m  [189/213], [94mLoss[0m : 2.41352
[1mStep[0m  [210/213], [94mLoss[0m : 2.57457

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.379, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.58938
[1mStep[0m  [21/213], [94mLoss[0m : 2.31556
[1mStep[0m  [42/213], [94mLoss[0m : 2.35218
[1mStep[0m  [63/213], [94mLoss[0m : 2.62346
[1mStep[0m  [84/213], [94mLoss[0m : 2.48269
[1mStep[0m  [105/213], [94mLoss[0m : 2.09554
[1mStep[0m  [126/213], [94mLoss[0m : 2.51475
[1mStep[0m  [147/213], [94mLoss[0m : 2.60712
[1mStep[0m  [168/213], [94mLoss[0m : 2.75282
[1mStep[0m  [189/213], [94mLoss[0m : 2.34836
[1mStep[0m  [210/213], [94mLoss[0m : 2.14919

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.393, [92mTest[0m: 2.372, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.00650
[1mStep[0m  [21/213], [94mLoss[0m : 2.49881
[1mStep[0m  [42/213], [94mLoss[0m : 2.50688
[1mStep[0m  [63/213], [94mLoss[0m : 2.30144
[1mStep[0m  [84/213], [94mLoss[0m : 1.99515
[1mStep[0m  [105/213], [94mLoss[0m : 2.44097
[1mStep[0m  [126/213], [94mLoss[0m : 2.28355
[1mStep[0m  [147/213], [94mLoss[0m : 2.46512
[1mStep[0m  [168/213], [94mLoss[0m : 2.57479
[1mStep[0m  [189/213], [94mLoss[0m : 2.26860
[1mStep[0m  [210/213], [94mLoss[0m : 2.78181

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.340, [92mTest[0m: 2.359, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.47616
[1mStep[0m  [21/213], [94mLoss[0m : 1.81798
[1mStep[0m  [42/213], [94mLoss[0m : 2.30392
[1mStep[0m  [63/213], [94mLoss[0m : 2.21202
[1mStep[0m  [84/213], [94mLoss[0m : 2.47518
[1mStep[0m  [105/213], [94mLoss[0m : 2.51863
[1mStep[0m  [126/213], [94mLoss[0m : 2.11334
[1mStep[0m  [147/213], [94mLoss[0m : 1.77101
[1mStep[0m  [168/213], [94mLoss[0m : 2.26172
[1mStep[0m  [189/213], [94mLoss[0m : 1.80720
[1mStep[0m  [210/213], [94mLoss[0m : 2.38122

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.263, [92mTest[0m: 2.368, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.04739
[1mStep[0m  [21/213], [94mLoss[0m : 1.97022
[1mStep[0m  [42/213], [94mLoss[0m : 2.23312
[1mStep[0m  [63/213], [94mLoss[0m : 1.96052
[1mStep[0m  [84/213], [94mLoss[0m : 2.18867
[1mStep[0m  [105/213], [94mLoss[0m : 2.14963
[1mStep[0m  [126/213], [94mLoss[0m : 2.22761
[1mStep[0m  [147/213], [94mLoss[0m : 2.02315
[1mStep[0m  [168/213], [94mLoss[0m : 2.37486
[1mStep[0m  [189/213], [94mLoss[0m : 2.12302
[1mStep[0m  [210/213], [94mLoss[0m : 2.34092

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.216, [92mTest[0m: 2.365, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.73444
[1mStep[0m  [21/213], [94mLoss[0m : 2.44059
[1mStep[0m  [42/213], [94mLoss[0m : 2.11019
[1mStep[0m  [63/213], [94mLoss[0m : 2.70652
[1mStep[0m  [84/213], [94mLoss[0m : 2.20706
[1mStep[0m  [105/213], [94mLoss[0m : 1.82283
[1mStep[0m  [126/213], [94mLoss[0m : 2.27643
[1mStep[0m  [147/213], [94mLoss[0m : 2.30961
[1mStep[0m  [168/213], [94mLoss[0m : 2.12686
[1mStep[0m  [189/213], [94mLoss[0m : 2.19421
[1mStep[0m  [210/213], [94mLoss[0m : 2.62599

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.170, [92mTest[0m: 2.368, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.78769
[1mStep[0m  [21/213], [94mLoss[0m : 1.94402
[1mStep[0m  [42/213], [94mLoss[0m : 1.86564
[1mStep[0m  [63/213], [94mLoss[0m : 1.85271
[1mStep[0m  [84/213], [94mLoss[0m : 2.52120
[1mStep[0m  [105/213], [94mLoss[0m : 2.06159
[1mStep[0m  [126/213], [94mLoss[0m : 2.34144
[1mStep[0m  [147/213], [94mLoss[0m : 1.82292
[1mStep[0m  [168/213], [94mLoss[0m : 2.17590
[1mStep[0m  [189/213], [94mLoss[0m : 1.80309
[1mStep[0m  [210/213], [94mLoss[0m : 2.53346

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.103, [92mTest[0m: 2.343, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.38590
[1mStep[0m  [21/213], [94mLoss[0m : 1.77977
[1mStep[0m  [42/213], [94mLoss[0m : 2.19730
[1mStep[0m  [63/213], [94mLoss[0m : 2.20757
[1mStep[0m  [84/213], [94mLoss[0m : 2.05918
[1mStep[0m  [105/213], [94mLoss[0m : 1.85749
[1mStep[0m  [126/213], [94mLoss[0m : 2.17463
[1mStep[0m  [147/213], [94mLoss[0m : 1.90300
[1mStep[0m  [168/213], [94mLoss[0m : 2.06663
[1mStep[0m  [189/213], [94mLoss[0m : 2.00150
[1mStep[0m  [210/213], [94mLoss[0m : 1.93139

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.068, [92mTest[0m: 2.428, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.99169
[1mStep[0m  [21/213], [94mLoss[0m : 1.72057
[1mStep[0m  [42/213], [94mLoss[0m : 1.81998
[1mStep[0m  [63/213], [94mLoss[0m : 1.61151
[1mStep[0m  [84/213], [94mLoss[0m : 2.13058
[1mStep[0m  [105/213], [94mLoss[0m : 2.06634
[1mStep[0m  [126/213], [94mLoss[0m : 2.12739
[1mStep[0m  [147/213], [94mLoss[0m : 2.24849
[1mStep[0m  [168/213], [94mLoss[0m : 1.81283
[1mStep[0m  [189/213], [94mLoss[0m : 1.86159
[1mStep[0m  [210/213], [94mLoss[0m : 2.03404

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.018, [92mTest[0m: 2.390, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.66042
[1mStep[0m  [21/213], [94mLoss[0m : 1.73361
[1mStep[0m  [42/213], [94mLoss[0m : 2.07862
[1mStep[0m  [63/213], [94mLoss[0m : 2.07199
[1mStep[0m  [84/213], [94mLoss[0m : 2.15054
[1mStep[0m  [105/213], [94mLoss[0m : 2.52852
[1mStep[0m  [126/213], [94mLoss[0m : 2.02198
[1mStep[0m  [147/213], [94mLoss[0m : 2.33140
[1mStep[0m  [168/213], [94mLoss[0m : 2.22453
[1mStep[0m  [189/213], [94mLoss[0m : 2.41932
[1mStep[0m  [210/213], [94mLoss[0m : 1.95462

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.985, [92mTest[0m: 2.393, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.65147
[1mStep[0m  [21/213], [94mLoss[0m : 2.07557
[1mStep[0m  [42/213], [94mLoss[0m : 1.84852
[1mStep[0m  [63/213], [94mLoss[0m : 2.05275
[1mStep[0m  [84/213], [94mLoss[0m : 1.86114
[1mStep[0m  [105/213], [94mLoss[0m : 1.97588
[1mStep[0m  [126/213], [94mLoss[0m : 1.97096
[1mStep[0m  [147/213], [94mLoss[0m : 2.21938
[1mStep[0m  [168/213], [94mLoss[0m : 1.66448
[1mStep[0m  [189/213], [94mLoss[0m : 2.13462
[1mStep[0m  [210/213], [94mLoss[0m : 1.34504

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.939, [92mTest[0m: 2.402, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.97156
[1mStep[0m  [21/213], [94mLoss[0m : 1.75940
[1mStep[0m  [42/213], [94mLoss[0m : 2.00410
[1mStep[0m  [63/213], [94mLoss[0m : 1.93743
[1mStep[0m  [84/213], [94mLoss[0m : 1.77954
[1mStep[0m  [105/213], [94mLoss[0m : 2.17732
[1mStep[0m  [126/213], [94mLoss[0m : 1.86020
[1mStep[0m  [147/213], [94mLoss[0m : 1.84547
[1mStep[0m  [168/213], [94mLoss[0m : 1.99758
[1mStep[0m  [189/213], [94mLoss[0m : 1.96912
[1mStep[0m  [210/213], [94mLoss[0m : 2.10031

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.937, [92mTest[0m: 2.391, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.36714
[1mStep[0m  [21/213], [94mLoss[0m : 2.11799
[1mStep[0m  [42/213], [94mLoss[0m : 1.89317
[1mStep[0m  [63/213], [94mLoss[0m : 1.77145
[1mStep[0m  [84/213], [94mLoss[0m : 1.81842
[1mStep[0m  [105/213], [94mLoss[0m : 1.95521
[1mStep[0m  [126/213], [94mLoss[0m : 1.93803
[1mStep[0m  [147/213], [94mLoss[0m : 1.79349
[1mStep[0m  [168/213], [94mLoss[0m : 1.76341
[1mStep[0m  [189/213], [94mLoss[0m : 1.99319
[1mStep[0m  [210/213], [94mLoss[0m : 1.91475

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.900, [92mTest[0m: 2.401, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.01388
[1mStep[0m  [21/213], [94mLoss[0m : 1.73984
[1mStep[0m  [42/213], [94mLoss[0m : 1.61170
[1mStep[0m  [63/213], [94mLoss[0m : 1.73225
[1mStep[0m  [84/213], [94mLoss[0m : 2.21985
[1mStep[0m  [105/213], [94mLoss[0m : 1.68467
[1mStep[0m  [126/213], [94mLoss[0m : 1.74715
[1mStep[0m  [147/213], [94mLoss[0m : 2.34830
[1mStep[0m  [168/213], [94mLoss[0m : 1.62343
[1mStep[0m  [189/213], [94mLoss[0m : 1.54937
[1mStep[0m  [210/213], [94mLoss[0m : 1.91826

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.853, [92mTest[0m: 2.433, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.47168
[1mStep[0m  [21/213], [94mLoss[0m : 1.88805
[1mStep[0m  [42/213], [94mLoss[0m : 2.18572
[1mStep[0m  [63/213], [94mLoss[0m : 1.70151
[1mStep[0m  [84/213], [94mLoss[0m : 1.59693
[1mStep[0m  [105/213], [94mLoss[0m : 1.83185
[1mStep[0m  [126/213], [94mLoss[0m : 1.61848
[1mStep[0m  [147/213], [94mLoss[0m : 2.16195
[1mStep[0m  [168/213], [94mLoss[0m : 2.01707
[1mStep[0m  [189/213], [94mLoss[0m : 1.78911
[1mStep[0m  [210/213], [94mLoss[0m : 1.98037

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.835, [92mTest[0m: 2.460, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.68018
[1mStep[0m  [21/213], [94mLoss[0m : 1.90330
[1mStep[0m  [42/213], [94mLoss[0m : 2.04616
[1mStep[0m  [63/213], [94mLoss[0m : 1.92028
[1mStep[0m  [84/213], [94mLoss[0m : 1.63475
[1mStep[0m  [105/213], [94mLoss[0m : 1.81438
[1mStep[0m  [126/213], [94mLoss[0m : 1.58844
[1mStep[0m  [147/213], [94mLoss[0m : 1.74365
[1mStep[0m  [168/213], [94mLoss[0m : 1.85397
[1mStep[0m  [189/213], [94mLoss[0m : 1.66688
[1mStep[0m  [210/213], [94mLoss[0m : 1.88035

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.803, [92mTest[0m: 2.423, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.87376
[1mStep[0m  [21/213], [94mLoss[0m : 1.64771
[1mStep[0m  [42/213], [94mLoss[0m : 1.96050
[1mStep[0m  [63/213], [94mLoss[0m : 1.50565
[1mStep[0m  [84/213], [94mLoss[0m : 1.97788
[1mStep[0m  [105/213], [94mLoss[0m : 1.78229
[1mStep[0m  [126/213], [94mLoss[0m : 2.01932
[1mStep[0m  [147/213], [94mLoss[0m : 1.52397
[1mStep[0m  [168/213], [94mLoss[0m : 1.82055
[1mStep[0m  [189/213], [94mLoss[0m : 1.61835
[1mStep[0m  [210/213], [94mLoss[0m : 1.83492

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.786, [92mTest[0m: 2.511, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.51147
[1mStep[0m  [21/213], [94mLoss[0m : 1.72537
[1mStep[0m  [42/213], [94mLoss[0m : 1.87375
[1mStep[0m  [63/213], [94mLoss[0m : 1.66810
[1mStep[0m  [84/213], [94mLoss[0m : 1.33337
[1mStep[0m  [105/213], [94mLoss[0m : 1.88152
[1mStep[0m  [126/213], [94mLoss[0m : 1.58409
[1mStep[0m  [147/213], [94mLoss[0m : 1.61924
[1mStep[0m  [168/213], [94mLoss[0m : 2.02686
[1mStep[0m  [189/213], [94mLoss[0m : 1.67310
[1mStep[0m  [210/213], [94mLoss[0m : 1.91073

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.763, [92mTest[0m: 2.457, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.58728
[1mStep[0m  [21/213], [94mLoss[0m : 1.78837
[1mStep[0m  [42/213], [94mLoss[0m : 1.53011
[1mStep[0m  [63/213], [94mLoss[0m : 1.45752
[1mStep[0m  [84/213], [94mLoss[0m : 1.63432
[1mStep[0m  [105/213], [94mLoss[0m : 1.68425
[1mStep[0m  [126/213], [94mLoss[0m : 1.97020
[1mStep[0m  [147/213], [94mLoss[0m : 1.71947
[1mStep[0m  [168/213], [94mLoss[0m : 1.71378
[1mStep[0m  [189/213], [94mLoss[0m : 1.72996
[1mStep[0m  [210/213], [94mLoss[0m : 2.09696

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.749, [92mTest[0m: 2.480, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.63486
[1mStep[0m  [21/213], [94mLoss[0m : 1.64804
[1mStep[0m  [42/213], [94mLoss[0m : 1.48413
[1mStep[0m  [63/213], [94mLoss[0m : 1.76867
[1mStep[0m  [84/213], [94mLoss[0m : 1.54583
[1mStep[0m  [105/213], [94mLoss[0m : 1.83146
[1mStep[0m  [126/213], [94mLoss[0m : 1.73837
[1mStep[0m  [147/213], [94mLoss[0m : 1.86296
[1mStep[0m  [168/213], [94mLoss[0m : 1.87139
[1mStep[0m  [189/213], [94mLoss[0m : 1.81468
[1mStep[0m  [210/213], [94mLoss[0m : 1.30100

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.720, [92mTest[0m: 2.465, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.71442
[1mStep[0m  [21/213], [94mLoss[0m : 2.05489
[1mStep[0m  [42/213], [94mLoss[0m : 1.99528
[1mStep[0m  [63/213], [94mLoss[0m : 1.33226
[1mStep[0m  [84/213], [94mLoss[0m : 1.53961
[1mStep[0m  [105/213], [94mLoss[0m : 1.87685
[1mStep[0m  [126/213], [94mLoss[0m : 1.57521
[1mStep[0m  [147/213], [94mLoss[0m : 1.65465
[1mStep[0m  [168/213], [94mLoss[0m : 1.86632
[1mStep[0m  [189/213], [94mLoss[0m : 1.51432
[1mStep[0m  [210/213], [94mLoss[0m : 1.77230

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.696, [92mTest[0m: 2.431, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.51280
[1mStep[0m  [21/213], [94mLoss[0m : 1.53091
[1mStep[0m  [42/213], [94mLoss[0m : 1.64844
[1mStep[0m  [63/213], [94mLoss[0m : 1.61417
[1mStep[0m  [84/213], [94mLoss[0m : 1.16394
[1mStep[0m  [105/213], [94mLoss[0m : 1.78824
[1mStep[0m  [126/213], [94mLoss[0m : 1.51390
[1mStep[0m  [147/213], [94mLoss[0m : 1.93027
[1mStep[0m  [168/213], [94mLoss[0m : 1.93154
[1mStep[0m  [189/213], [94mLoss[0m : 1.80423
[1mStep[0m  [210/213], [94mLoss[0m : 1.81227

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.669, [92mTest[0m: 2.488, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.38900
[1mStep[0m  [21/213], [94mLoss[0m : 1.48828
[1mStep[0m  [42/213], [94mLoss[0m : 1.79810
[1mStep[0m  [63/213], [94mLoss[0m : 1.38472
[1mStep[0m  [84/213], [94mLoss[0m : 1.63727
[1mStep[0m  [105/213], [94mLoss[0m : 1.45955
[1mStep[0m  [126/213], [94mLoss[0m : 1.40810
[1mStep[0m  [147/213], [94mLoss[0m : 1.52665
[1mStep[0m  [168/213], [94mLoss[0m : 1.62707
[1mStep[0m  [189/213], [94mLoss[0m : 1.52057
[1mStep[0m  [210/213], [94mLoss[0m : 1.54674

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.645, [92mTest[0m: 2.482, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.63403
[1mStep[0m  [21/213], [94mLoss[0m : 1.31841
[1mStep[0m  [42/213], [94mLoss[0m : 1.41184
[1mStep[0m  [63/213], [94mLoss[0m : 1.50821
[1mStep[0m  [84/213], [94mLoss[0m : 2.08807
[1mStep[0m  [105/213], [94mLoss[0m : 1.72847
[1mStep[0m  [126/213], [94mLoss[0m : 1.93897
[1mStep[0m  [147/213], [94mLoss[0m : 1.46952
[1mStep[0m  [168/213], [94mLoss[0m : 1.51033
[1mStep[0m  [189/213], [94mLoss[0m : 1.55228
[1mStep[0m  [210/213], [94mLoss[0m : 1.59241

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.644, [92mTest[0m: 2.459, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.81123
[1mStep[0m  [21/213], [94mLoss[0m : 1.58647
[1mStep[0m  [42/213], [94mLoss[0m : 1.74086
[1mStep[0m  [63/213], [94mLoss[0m : 1.73159
[1mStep[0m  [84/213], [94mLoss[0m : 1.72376
[1mStep[0m  [105/213], [94mLoss[0m : 1.61184
[1mStep[0m  [126/213], [94mLoss[0m : 1.71700
[1mStep[0m  [147/213], [94mLoss[0m : 1.53539
[1mStep[0m  [168/213], [94mLoss[0m : 1.18729
[1mStep[0m  [189/213], [94mLoss[0m : 1.48870
[1mStep[0m  [210/213], [94mLoss[0m : 1.81332

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.634, [92mTest[0m: 2.458, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.89451
[1mStep[0m  [21/213], [94mLoss[0m : 1.73861
[1mStep[0m  [42/213], [94mLoss[0m : 1.34161
[1mStep[0m  [63/213], [94mLoss[0m : 1.45928
[1mStep[0m  [84/213], [94mLoss[0m : 1.62205
[1mStep[0m  [105/213], [94mLoss[0m : 1.48484
[1mStep[0m  [126/213], [94mLoss[0m : 1.73856
[1mStep[0m  [147/213], [94mLoss[0m : 1.84973
[1mStep[0m  [168/213], [94mLoss[0m : 1.68393
[1mStep[0m  [189/213], [94mLoss[0m : 1.66624
[1mStep[0m  [210/213], [94mLoss[0m : 1.29798

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.592, [92mTest[0m: 2.510, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.48187
[1mStep[0m  [21/213], [94mLoss[0m : 1.55075
[1mStep[0m  [42/213], [94mLoss[0m : 1.63143
[1mStep[0m  [63/213], [94mLoss[0m : 1.44651
[1mStep[0m  [84/213], [94mLoss[0m : 1.79618
[1mStep[0m  [105/213], [94mLoss[0m : 1.70401
[1mStep[0m  [126/213], [94mLoss[0m : 1.54346
[1mStep[0m  [147/213], [94mLoss[0m : 1.56939
[1mStep[0m  [168/213], [94mLoss[0m : 1.33416
[1mStep[0m  [189/213], [94mLoss[0m : 1.69100
[1mStep[0m  [210/213], [94mLoss[0m : 1.57777

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.593, [92mTest[0m: 2.461, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.24446
[1mStep[0m  [21/213], [94mLoss[0m : 1.89766
[1mStep[0m  [42/213], [94mLoss[0m : 1.50609
[1mStep[0m  [63/213], [94mLoss[0m : 1.48860
[1mStep[0m  [84/213], [94mLoss[0m : 1.69618
[1mStep[0m  [105/213], [94mLoss[0m : 1.51827
[1mStep[0m  [126/213], [94mLoss[0m : 1.53083
[1mStep[0m  [147/213], [94mLoss[0m : 1.50809
[1mStep[0m  [168/213], [94mLoss[0m : 1.81457
[1mStep[0m  [189/213], [94mLoss[0m : 1.40331
[1mStep[0m  [210/213], [94mLoss[0m : 1.39938

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.586, [92mTest[0m: 2.532, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.77649
[1mStep[0m  [21/213], [94mLoss[0m : 1.50629
[1mStep[0m  [42/213], [94mLoss[0m : 1.69381
[1mStep[0m  [63/213], [94mLoss[0m : 1.65734
[1mStep[0m  [84/213], [94mLoss[0m : 1.68343
[1mStep[0m  [105/213], [94mLoss[0m : 1.49210
[1mStep[0m  [126/213], [94mLoss[0m : 1.50885
[1mStep[0m  [147/213], [94mLoss[0m : 1.70255
[1mStep[0m  [168/213], [94mLoss[0m : 1.42808
[1mStep[0m  [189/213], [94mLoss[0m : 1.51085
[1mStep[0m  [210/213], [94mLoss[0m : 1.85590

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.573, [92mTest[0m: 2.454, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.16103
[1mStep[0m  [21/213], [94mLoss[0m : 1.53030
[1mStep[0m  [42/213], [94mLoss[0m : 1.70663
[1mStep[0m  [63/213], [94mLoss[0m : 1.63937
[1mStep[0m  [84/213], [94mLoss[0m : 1.45110
[1mStep[0m  [105/213], [94mLoss[0m : 1.68078
[1mStep[0m  [126/213], [94mLoss[0m : 1.25837
[1mStep[0m  [147/213], [94mLoss[0m : 1.43267
[1mStep[0m  [168/213], [94mLoss[0m : 1.78262
[1mStep[0m  [189/213], [94mLoss[0m : 1.21983
[1mStep[0m  [210/213], [94mLoss[0m : 1.52762

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.565, [92mTest[0m: 2.476, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.465
====================================

Phase 2 - Evaluation MAE:  2.4648161777910196
MAE score P1       2.376645
MAE score P2       2.464816
loss               1.565225
learning_rate       0.00505
batch_size               64
hidden_sizes      [200, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.5
weight_decay           0.01
Name: 8, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/213], [94mLoss[0m : 10.89527
[1mStep[0m  [21/213], [94mLoss[0m : 8.42175
[1mStep[0m  [42/213], [94mLoss[0m : 7.04350
[1mStep[0m  [63/213], [94mLoss[0m : 4.91902
[1mStep[0m  [84/213], [94mLoss[0m : 4.08457
[1mStep[0m  [105/213], [94mLoss[0m : 2.87846
[1mStep[0m  [126/213], [94mLoss[0m : 2.83304
[1mStep[0m  [147/213], [94mLoss[0m : 2.89183
[1mStep[0m  [168/213], [94mLoss[0m : 2.77757
[1mStep[0m  [189/213], [94mLoss[0m : 2.61917
[1mStep[0m  [210/213], [94mLoss[0m : 2.78017

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.554, [92mTest[0m: 10.782, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 3.20282
[1mStep[0m  [21/213], [94mLoss[0m : 3.17463
[1mStep[0m  [42/213], [94mLoss[0m : 2.91325
[1mStep[0m  [63/213], [94mLoss[0m : 2.53491
[1mStep[0m  [84/213], [94mLoss[0m : 2.68036
[1mStep[0m  [105/213], [94mLoss[0m : 3.09844
[1mStep[0m  [126/213], [94mLoss[0m : 2.93780
[1mStep[0m  [147/213], [94mLoss[0m : 2.44568
[1mStep[0m  [168/213], [94mLoss[0m : 2.50825
[1mStep[0m  [189/213], [94mLoss[0m : 2.72468
[1mStep[0m  [210/213], [94mLoss[0m : 2.70730

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.689, [92mTest[0m: 2.608, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.44824
[1mStep[0m  [21/213], [94mLoss[0m : 2.65512
[1mStep[0m  [42/213], [94mLoss[0m : 2.64861
[1mStep[0m  [63/213], [94mLoss[0m : 2.64606
[1mStep[0m  [84/213], [94mLoss[0m : 3.02185
[1mStep[0m  [105/213], [94mLoss[0m : 2.82777
[1mStep[0m  [126/213], [94mLoss[0m : 2.71857
[1mStep[0m  [147/213], [94mLoss[0m : 2.44268
[1mStep[0m  [168/213], [94mLoss[0m : 2.58333
[1mStep[0m  [189/213], [94mLoss[0m : 2.77238
[1mStep[0m  [210/213], [94mLoss[0m : 2.81390

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.620, [92mTest[0m: 2.486, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.64583
[1mStep[0m  [21/213], [94mLoss[0m : 2.63337
[1mStep[0m  [42/213], [94mLoss[0m : 2.52948
[1mStep[0m  [63/213], [94mLoss[0m : 2.66934
[1mStep[0m  [84/213], [94mLoss[0m : 2.94345
[1mStep[0m  [105/213], [94mLoss[0m : 2.56361
[1mStep[0m  [126/213], [94mLoss[0m : 2.50232
[1mStep[0m  [147/213], [94mLoss[0m : 2.71926
[1mStep[0m  [168/213], [94mLoss[0m : 2.73205
[1mStep[0m  [189/213], [94mLoss[0m : 2.79854
[1mStep[0m  [210/213], [94mLoss[0m : 2.72148

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.567, [92mTest[0m: 2.453, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.24721
[1mStep[0m  [21/213], [94mLoss[0m : 2.12964
[1mStep[0m  [42/213], [94mLoss[0m : 2.56818
[1mStep[0m  [63/213], [94mLoss[0m : 2.31227
[1mStep[0m  [84/213], [94mLoss[0m : 2.40148
[1mStep[0m  [105/213], [94mLoss[0m : 2.70581
[1mStep[0m  [126/213], [94mLoss[0m : 3.00117
[1mStep[0m  [147/213], [94mLoss[0m : 2.31571
[1mStep[0m  [168/213], [94mLoss[0m : 2.41587
[1mStep[0m  [189/213], [94mLoss[0m : 2.69340
[1mStep[0m  [210/213], [94mLoss[0m : 2.37761

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.554, [92mTest[0m: 2.419, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.50651
[1mStep[0m  [21/213], [94mLoss[0m : 2.14354
[1mStep[0m  [42/213], [94mLoss[0m : 2.69775
[1mStep[0m  [63/213], [94mLoss[0m : 2.72505
[1mStep[0m  [84/213], [94mLoss[0m : 2.61293
[1mStep[0m  [105/213], [94mLoss[0m : 2.32761
[1mStep[0m  [126/213], [94mLoss[0m : 2.25428
[1mStep[0m  [147/213], [94mLoss[0m : 2.81136
[1mStep[0m  [168/213], [94mLoss[0m : 2.19868
[1mStep[0m  [189/213], [94mLoss[0m : 2.17428
[1mStep[0m  [210/213], [94mLoss[0m : 2.57688

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.546, [92mTest[0m: 2.442, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.53158
[1mStep[0m  [21/213], [94mLoss[0m : 2.97792
[1mStep[0m  [42/213], [94mLoss[0m : 3.16787
[1mStep[0m  [63/213], [94mLoss[0m : 2.54713
[1mStep[0m  [84/213], [94mLoss[0m : 2.49724
[1mStep[0m  [105/213], [94mLoss[0m : 2.10155
[1mStep[0m  [126/213], [94mLoss[0m : 2.51716
[1mStep[0m  [147/213], [94mLoss[0m : 2.24206
[1mStep[0m  [168/213], [94mLoss[0m : 2.64532
[1mStep[0m  [189/213], [94mLoss[0m : 2.32416
[1mStep[0m  [210/213], [94mLoss[0m : 2.58972

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.407, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.23861
[1mStep[0m  [21/213], [94mLoss[0m : 2.35339
[1mStep[0m  [42/213], [94mLoss[0m : 2.26929
[1mStep[0m  [63/213], [94mLoss[0m : 2.06543
[1mStep[0m  [84/213], [94mLoss[0m : 2.41704
[1mStep[0m  [105/213], [94mLoss[0m : 2.49177
[1mStep[0m  [126/213], [94mLoss[0m : 2.12539
[1mStep[0m  [147/213], [94mLoss[0m : 2.68509
[1mStep[0m  [168/213], [94mLoss[0m : 2.44832
[1mStep[0m  [189/213], [94mLoss[0m : 2.43714
[1mStep[0m  [210/213], [94mLoss[0m : 2.44853

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.374, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.83670
[1mStep[0m  [21/213], [94mLoss[0m : 2.54574
[1mStep[0m  [42/213], [94mLoss[0m : 2.61910
[1mStep[0m  [63/213], [94mLoss[0m : 2.41141
[1mStep[0m  [84/213], [94mLoss[0m : 2.55890
[1mStep[0m  [105/213], [94mLoss[0m : 2.49976
[1mStep[0m  [126/213], [94mLoss[0m : 2.75458
[1mStep[0m  [147/213], [94mLoss[0m : 2.79601
[1mStep[0m  [168/213], [94mLoss[0m : 2.71720
[1mStep[0m  [189/213], [94mLoss[0m : 2.76925
[1mStep[0m  [210/213], [94mLoss[0m : 2.41226

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.375, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.28936
[1mStep[0m  [21/213], [94mLoss[0m : 3.15273
[1mStep[0m  [42/213], [94mLoss[0m : 2.51686
[1mStep[0m  [63/213], [94mLoss[0m : 2.54626
[1mStep[0m  [84/213], [94mLoss[0m : 2.33897
[1mStep[0m  [105/213], [94mLoss[0m : 2.20796
[1mStep[0m  [126/213], [94mLoss[0m : 2.86062
[1mStep[0m  [147/213], [94mLoss[0m : 2.62991
[1mStep[0m  [168/213], [94mLoss[0m : 2.73571
[1mStep[0m  [189/213], [94mLoss[0m : 2.35424
[1mStep[0m  [210/213], [94mLoss[0m : 2.73946

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.404, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.81132
[1mStep[0m  [21/213], [94mLoss[0m : 2.78291
[1mStep[0m  [42/213], [94mLoss[0m : 2.52985
[1mStep[0m  [63/213], [94mLoss[0m : 2.47158
[1mStep[0m  [84/213], [94mLoss[0m : 2.45067
[1mStep[0m  [105/213], [94mLoss[0m : 2.66959
[1mStep[0m  [126/213], [94mLoss[0m : 2.30052
[1mStep[0m  [147/213], [94mLoss[0m : 2.32082
[1mStep[0m  [168/213], [94mLoss[0m : 2.49852
[1mStep[0m  [189/213], [94mLoss[0m : 2.47284
[1mStep[0m  [210/213], [94mLoss[0m : 2.40538

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.372, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.47385
[1mStep[0m  [21/213], [94mLoss[0m : 2.73083
[1mStep[0m  [42/213], [94mLoss[0m : 2.50371
[1mStep[0m  [63/213], [94mLoss[0m : 2.72858
[1mStep[0m  [84/213], [94mLoss[0m : 2.31003
[1mStep[0m  [105/213], [94mLoss[0m : 2.32647
[1mStep[0m  [126/213], [94mLoss[0m : 2.51530
[1mStep[0m  [147/213], [94mLoss[0m : 2.10410
[1mStep[0m  [168/213], [94mLoss[0m : 2.56273
[1mStep[0m  [189/213], [94mLoss[0m : 2.99808
[1mStep[0m  [210/213], [94mLoss[0m : 2.43123

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.352, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.69330
[1mStep[0m  [21/213], [94mLoss[0m : 2.58050
[1mStep[0m  [42/213], [94mLoss[0m : 2.36838
[1mStep[0m  [63/213], [94mLoss[0m : 1.87806
[1mStep[0m  [84/213], [94mLoss[0m : 2.35230
[1mStep[0m  [105/213], [94mLoss[0m : 2.55937
[1mStep[0m  [126/213], [94mLoss[0m : 2.45464
[1mStep[0m  [147/213], [94mLoss[0m : 2.30526
[1mStep[0m  [168/213], [94mLoss[0m : 2.59187
[1mStep[0m  [189/213], [94mLoss[0m : 2.32556
[1mStep[0m  [210/213], [94mLoss[0m : 1.88238

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.372, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.18938
[1mStep[0m  [21/213], [94mLoss[0m : 2.60888
[1mStep[0m  [42/213], [94mLoss[0m : 2.40118
[1mStep[0m  [63/213], [94mLoss[0m : 2.90240
[1mStep[0m  [84/213], [94mLoss[0m : 2.31232
[1mStep[0m  [105/213], [94mLoss[0m : 2.63226
[1mStep[0m  [126/213], [94mLoss[0m : 1.79192
[1mStep[0m  [147/213], [94mLoss[0m : 2.43914
[1mStep[0m  [168/213], [94mLoss[0m : 2.30700
[1mStep[0m  [189/213], [94mLoss[0m : 2.43942
[1mStep[0m  [210/213], [94mLoss[0m : 2.17636

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.422, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.46469
[1mStep[0m  [21/213], [94mLoss[0m : 2.72220
[1mStep[0m  [42/213], [94mLoss[0m : 2.47111
[1mStep[0m  [63/213], [94mLoss[0m : 2.39790
[1mStep[0m  [84/213], [94mLoss[0m : 2.03740
[1mStep[0m  [105/213], [94mLoss[0m : 2.43335
[1mStep[0m  [126/213], [94mLoss[0m : 2.30219
[1mStep[0m  [147/213], [94mLoss[0m : 2.34769
[1mStep[0m  [168/213], [94mLoss[0m : 2.45063
[1mStep[0m  [189/213], [94mLoss[0m : 2.06562
[1mStep[0m  [210/213], [94mLoss[0m : 2.60971

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.374, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.33636
[1mStep[0m  [21/213], [94mLoss[0m : 2.36535
[1mStep[0m  [42/213], [94mLoss[0m : 2.15182
[1mStep[0m  [63/213], [94mLoss[0m : 2.41105
[1mStep[0m  [84/213], [94mLoss[0m : 2.94077
[1mStep[0m  [105/213], [94mLoss[0m : 2.55699
[1mStep[0m  [126/213], [94mLoss[0m : 2.25576
[1mStep[0m  [147/213], [94mLoss[0m : 2.61496
[1mStep[0m  [168/213], [94mLoss[0m : 1.87371
[1mStep[0m  [189/213], [94mLoss[0m : 2.44827
[1mStep[0m  [210/213], [94mLoss[0m : 2.28242

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.361, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.48300
[1mStep[0m  [21/213], [94mLoss[0m : 2.10580
[1mStep[0m  [42/213], [94mLoss[0m : 2.41318
[1mStep[0m  [63/213], [94mLoss[0m : 2.37912
[1mStep[0m  [84/213], [94mLoss[0m : 2.35344
[1mStep[0m  [105/213], [94mLoss[0m : 2.50527
[1mStep[0m  [126/213], [94mLoss[0m : 2.88752
[1mStep[0m  [147/213], [94mLoss[0m : 2.32431
[1mStep[0m  [168/213], [94mLoss[0m : 2.25447
[1mStep[0m  [189/213], [94mLoss[0m : 2.27567
[1mStep[0m  [210/213], [94mLoss[0m : 2.43064

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.366, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.88963
[1mStep[0m  [21/213], [94mLoss[0m : 2.06196
[1mStep[0m  [42/213], [94mLoss[0m : 2.63173
[1mStep[0m  [63/213], [94mLoss[0m : 2.40371
[1mStep[0m  [84/213], [94mLoss[0m : 2.28558
[1mStep[0m  [105/213], [94mLoss[0m : 2.03763
[1mStep[0m  [126/213], [94mLoss[0m : 2.37278
[1mStep[0m  [147/213], [94mLoss[0m : 2.13793
[1mStep[0m  [168/213], [94mLoss[0m : 1.98817
[1mStep[0m  [189/213], [94mLoss[0m : 2.50328
[1mStep[0m  [210/213], [94mLoss[0m : 2.27812

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.344, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.30581
[1mStep[0m  [21/213], [94mLoss[0m : 2.58115
[1mStep[0m  [42/213], [94mLoss[0m : 2.14543
[1mStep[0m  [63/213], [94mLoss[0m : 2.16455
[1mStep[0m  [84/213], [94mLoss[0m : 2.41725
[1mStep[0m  [105/213], [94mLoss[0m : 2.54805
[1mStep[0m  [126/213], [94mLoss[0m : 2.61558
[1mStep[0m  [147/213], [94mLoss[0m : 2.69371
[1mStep[0m  [168/213], [94mLoss[0m : 2.22703
[1mStep[0m  [189/213], [94mLoss[0m : 2.24798
[1mStep[0m  [210/213], [94mLoss[0m : 2.42180

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.349, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.79702
[1mStep[0m  [21/213], [94mLoss[0m : 2.25583
[1mStep[0m  [42/213], [94mLoss[0m : 1.97932
[1mStep[0m  [63/213], [94mLoss[0m : 2.51622
[1mStep[0m  [84/213], [94mLoss[0m : 2.30010
[1mStep[0m  [105/213], [94mLoss[0m : 2.34563
[1mStep[0m  [126/213], [94mLoss[0m : 2.04715
[1mStep[0m  [147/213], [94mLoss[0m : 2.84871
[1mStep[0m  [168/213], [94mLoss[0m : 2.12648
[1mStep[0m  [189/213], [94mLoss[0m : 2.47415
[1mStep[0m  [210/213], [94mLoss[0m : 2.18076

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.338, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.44966
[1mStep[0m  [21/213], [94mLoss[0m : 2.70781
[1mStep[0m  [42/213], [94mLoss[0m : 2.52871
[1mStep[0m  [63/213], [94mLoss[0m : 2.72710
[1mStep[0m  [84/213], [94mLoss[0m : 2.77323
[1mStep[0m  [105/213], [94mLoss[0m : 2.40178
[1mStep[0m  [126/213], [94mLoss[0m : 2.48135
[1mStep[0m  [147/213], [94mLoss[0m : 2.64632
[1mStep[0m  [168/213], [94mLoss[0m : 2.45602
[1mStep[0m  [189/213], [94mLoss[0m : 2.17416
[1mStep[0m  [210/213], [94mLoss[0m : 2.45756

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.401, [92mTest[0m: 2.335, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.43087
[1mStep[0m  [21/213], [94mLoss[0m : 2.40105
[1mStep[0m  [42/213], [94mLoss[0m : 2.39643
[1mStep[0m  [63/213], [94mLoss[0m : 2.24144
[1mStep[0m  [84/213], [94mLoss[0m : 2.31254
[1mStep[0m  [105/213], [94mLoss[0m : 2.53614
[1mStep[0m  [126/213], [94mLoss[0m : 2.51645
[1mStep[0m  [147/213], [94mLoss[0m : 2.66125
[1mStep[0m  [168/213], [94mLoss[0m : 2.15381
[1mStep[0m  [189/213], [94mLoss[0m : 2.30341
[1mStep[0m  [210/213], [94mLoss[0m : 2.18280

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.384, [92mTest[0m: 2.345, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.58083
[1mStep[0m  [21/213], [94mLoss[0m : 2.75292
[1mStep[0m  [42/213], [94mLoss[0m : 2.25495
[1mStep[0m  [63/213], [94mLoss[0m : 2.24168
[1mStep[0m  [84/213], [94mLoss[0m : 2.55465
[1mStep[0m  [105/213], [94mLoss[0m : 2.64156
[1mStep[0m  [126/213], [94mLoss[0m : 2.11537
[1mStep[0m  [147/213], [94mLoss[0m : 2.53496
[1mStep[0m  [168/213], [94mLoss[0m : 2.38535
[1mStep[0m  [189/213], [94mLoss[0m : 2.49613
[1mStep[0m  [210/213], [94mLoss[0m : 2.47156

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.390, [92mTest[0m: 2.341, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.06757
[1mStep[0m  [21/213], [94mLoss[0m : 2.63976
[1mStep[0m  [42/213], [94mLoss[0m : 2.76227
[1mStep[0m  [63/213], [94mLoss[0m : 2.03260
[1mStep[0m  [84/213], [94mLoss[0m : 2.27436
[1mStep[0m  [105/213], [94mLoss[0m : 2.30439
[1mStep[0m  [126/213], [94mLoss[0m : 1.96568
[1mStep[0m  [147/213], [94mLoss[0m : 2.06909
[1mStep[0m  [168/213], [94mLoss[0m : 2.42500
[1mStep[0m  [189/213], [94mLoss[0m : 2.21864
[1mStep[0m  [210/213], [94mLoss[0m : 2.44157

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.379, [92mTest[0m: 2.345, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.11893
[1mStep[0m  [21/213], [94mLoss[0m : 2.24676
[1mStep[0m  [42/213], [94mLoss[0m : 2.22149
[1mStep[0m  [63/213], [94mLoss[0m : 2.08700
[1mStep[0m  [84/213], [94mLoss[0m : 2.63943
[1mStep[0m  [105/213], [94mLoss[0m : 2.45198
[1mStep[0m  [126/213], [94mLoss[0m : 2.51642
[1mStep[0m  [147/213], [94mLoss[0m : 2.39698
[1mStep[0m  [168/213], [94mLoss[0m : 2.28990
[1mStep[0m  [189/213], [94mLoss[0m : 2.87532
[1mStep[0m  [210/213], [94mLoss[0m : 2.40871

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.387, [92mTest[0m: 2.347, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.68569
[1mStep[0m  [21/213], [94mLoss[0m : 2.01345
[1mStep[0m  [42/213], [94mLoss[0m : 2.44151
[1mStep[0m  [63/213], [94mLoss[0m : 2.66986
[1mStep[0m  [84/213], [94mLoss[0m : 2.35308
[1mStep[0m  [105/213], [94mLoss[0m : 2.12267
[1mStep[0m  [126/213], [94mLoss[0m : 2.19298
[1mStep[0m  [147/213], [94mLoss[0m : 2.36018
[1mStep[0m  [168/213], [94mLoss[0m : 2.53193
[1mStep[0m  [189/213], [94mLoss[0m : 2.40143
[1mStep[0m  [210/213], [94mLoss[0m : 2.11367

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.367, [92mTest[0m: 2.335, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.70194
[1mStep[0m  [21/213], [94mLoss[0m : 2.06750
[1mStep[0m  [42/213], [94mLoss[0m : 2.01695
[1mStep[0m  [63/213], [94mLoss[0m : 2.20135
[1mStep[0m  [84/213], [94mLoss[0m : 2.35241
[1mStep[0m  [105/213], [94mLoss[0m : 2.82583
[1mStep[0m  [126/213], [94mLoss[0m : 1.95805
[1mStep[0m  [147/213], [94mLoss[0m : 2.13283
[1mStep[0m  [168/213], [94mLoss[0m : 2.51122
[1mStep[0m  [189/213], [94mLoss[0m : 2.24905
[1mStep[0m  [210/213], [94mLoss[0m : 2.38557

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.369, [92mTest[0m: 2.338, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.04803
[1mStep[0m  [21/213], [94mLoss[0m : 2.26201
[1mStep[0m  [42/213], [94mLoss[0m : 2.25133
[1mStep[0m  [63/213], [94mLoss[0m : 3.16509
[1mStep[0m  [84/213], [94mLoss[0m : 2.49248
[1mStep[0m  [105/213], [94mLoss[0m : 2.42535
[1mStep[0m  [126/213], [94mLoss[0m : 2.35121
[1mStep[0m  [147/213], [94mLoss[0m : 2.23280
[1mStep[0m  [168/213], [94mLoss[0m : 2.49683
[1mStep[0m  [189/213], [94mLoss[0m : 2.39642
[1mStep[0m  [210/213], [94mLoss[0m : 2.52561

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.372, [92mTest[0m: 2.319, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.70316
[1mStep[0m  [21/213], [94mLoss[0m : 2.63594
[1mStep[0m  [42/213], [94mLoss[0m : 2.10147
[1mStep[0m  [63/213], [94mLoss[0m : 2.49045
[1mStep[0m  [84/213], [94mLoss[0m : 1.89121
[1mStep[0m  [105/213], [94mLoss[0m : 2.20916
[1mStep[0m  [126/213], [94mLoss[0m : 2.22229
[1mStep[0m  [147/213], [94mLoss[0m : 2.35649
[1mStep[0m  [168/213], [94mLoss[0m : 2.69562
[1mStep[0m  [189/213], [94mLoss[0m : 2.36041
[1mStep[0m  [210/213], [94mLoss[0m : 2.17517

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.356, [92mTest[0m: 2.338, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.29058
[1mStep[0m  [21/213], [94mLoss[0m : 2.08973
[1mStep[0m  [42/213], [94mLoss[0m : 2.31243
[1mStep[0m  [63/213], [94mLoss[0m : 2.68646
[1mStep[0m  [84/213], [94mLoss[0m : 2.59770
[1mStep[0m  [105/213], [94mLoss[0m : 2.29199
[1mStep[0m  [126/213], [94mLoss[0m : 2.58864
[1mStep[0m  [147/213], [94mLoss[0m : 2.60419
[1mStep[0m  [168/213], [94mLoss[0m : 2.49848
[1mStep[0m  [189/213], [94mLoss[0m : 2.25140
[1mStep[0m  [210/213], [94mLoss[0m : 2.55263

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.365, [92mTest[0m: 2.344, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.318
====================================

Phase 1 - Evaluation MAE:  2.3181884165080087
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/213], [94mLoss[0m : 2.18127
[1mStep[0m  [21/213], [94mLoss[0m : 2.33580
[1mStep[0m  [42/213], [94mLoss[0m : 2.76694
[1mStep[0m  [63/213], [94mLoss[0m : 2.44736
[1mStep[0m  [84/213], [94mLoss[0m : 2.50681
[1mStep[0m  [105/213], [94mLoss[0m : 2.26193
[1mStep[0m  [126/213], [94mLoss[0m : 2.17123
[1mStep[0m  [147/213], [94mLoss[0m : 2.59988
[1mStep[0m  [168/213], [94mLoss[0m : 2.26285
[1mStep[0m  [189/213], [94mLoss[0m : 2.79466
[1mStep[0m  [210/213], [94mLoss[0m : 2.57047

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.324, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.17338
[1mStep[0m  [21/213], [94mLoss[0m : 2.44998
[1mStep[0m  [42/213], [94mLoss[0m : 2.58444
[1mStep[0m  [63/213], [94mLoss[0m : 2.44074
[1mStep[0m  [84/213], [94mLoss[0m : 2.50146
[1mStep[0m  [105/213], [94mLoss[0m : 2.19659
[1mStep[0m  [126/213], [94mLoss[0m : 2.84079
[1mStep[0m  [147/213], [94mLoss[0m : 2.34171
[1mStep[0m  [168/213], [94mLoss[0m : 2.66645
[1mStep[0m  [189/213], [94mLoss[0m : 2.48310
[1mStep[0m  [210/213], [94mLoss[0m : 2.32162

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.541, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.86857
[1mStep[0m  [21/213], [94mLoss[0m : 2.56804
[1mStep[0m  [42/213], [94mLoss[0m : 2.18414
[1mStep[0m  [63/213], [94mLoss[0m : 2.47070
[1mStep[0m  [84/213], [94mLoss[0m : 1.86948
[1mStep[0m  [105/213], [94mLoss[0m : 1.77371
[1mStep[0m  [126/213], [94mLoss[0m : 2.44983
[1mStep[0m  [147/213], [94mLoss[0m : 2.16514
[1mStep[0m  [168/213], [94mLoss[0m : 2.27204
[1mStep[0m  [189/213], [94mLoss[0m : 2.67215
[1mStep[0m  [210/213], [94mLoss[0m : 2.89237

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.345, [92mTest[0m: 2.377, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.73624
[1mStep[0m  [21/213], [94mLoss[0m : 2.05509
[1mStep[0m  [42/213], [94mLoss[0m : 2.53984
[1mStep[0m  [63/213], [94mLoss[0m : 2.43144
[1mStep[0m  [84/213], [94mLoss[0m : 1.88466
[1mStep[0m  [105/213], [94mLoss[0m : 2.08365
[1mStep[0m  [126/213], [94mLoss[0m : 1.68531
[1mStep[0m  [147/213], [94mLoss[0m : 2.42589
[1mStep[0m  [168/213], [94mLoss[0m : 1.96670
[1mStep[0m  [189/213], [94mLoss[0m : 2.31979
[1mStep[0m  [210/213], [94mLoss[0m : 2.32602

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.295, [92mTest[0m: 2.439, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.14632
[1mStep[0m  [21/213], [94mLoss[0m : 2.29521
[1mStep[0m  [42/213], [94mLoss[0m : 1.77482
[1mStep[0m  [63/213], [94mLoss[0m : 2.03196
[1mStep[0m  [84/213], [94mLoss[0m : 2.10791
[1mStep[0m  [105/213], [94mLoss[0m : 2.07889
[1mStep[0m  [126/213], [94mLoss[0m : 2.47636
[1mStep[0m  [147/213], [94mLoss[0m : 2.21971
[1mStep[0m  [168/213], [94mLoss[0m : 2.56483
[1mStep[0m  [189/213], [94mLoss[0m : 2.46047
[1mStep[0m  [210/213], [94mLoss[0m : 1.84766

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.239, [92mTest[0m: 2.366, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.99226
[1mStep[0m  [21/213], [94mLoss[0m : 2.11006
[1mStep[0m  [42/213], [94mLoss[0m : 1.73403
[1mStep[0m  [63/213], [94mLoss[0m : 2.08482
[1mStep[0m  [84/213], [94mLoss[0m : 2.25341
[1mStep[0m  [105/213], [94mLoss[0m : 2.65125
[1mStep[0m  [126/213], [94mLoss[0m : 2.81054
[1mStep[0m  [147/213], [94mLoss[0m : 2.10787
[1mStep[0m  [168/213], [94mLoss[0m : 2.21800
[1mStep[0m  [189/213], [94mLoss[0m : 2.55960
[1mStep[0m  [210/213], [94mLoss[0m : 1.65344

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.182, [92mTest[0m: 2.378, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.56078
[1mStep[0m  [21/213], [94mLoss[0m : 1.74913
[1mStep[0m  [42/213], [94mLoss[0m : 1.98164
[1mStep[0m  [63/213], [94mLoss[0m : 1.99478
[1mStep[0m  [84/213], [94mLoss[0m : 1.90752
[1mStep[0m  [105/213], [94mLoss[0m : 1.90849
[1mStep[0m  [126/213], [94mLoss[0m : 2.26300
[1mStep[0m  [147/213], [94mLoss[0m : 2.49914
[1mStep[0m  [168/213], [94mLoss[0m : 1.98326
[1mStep[0m  [189/213], [94mLoss[0m : 2.27651
[1mStep[0m  [210/213], [94mLoss[0m : 2.43243

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.123, [92mTest[0m: 2.353, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.74280
[1mStep[0m  [21/213], [94mLoss[0m : 1.90111
[1mStep[0m  [42/213], [94mLoss[0m : 2.06856
[1mStep[0m  [63/213], [94mLoss[0m : 2.21641
[1mStep[0m  [84/213], [94mLoss[0m : 2.27583
[1mStep[0m  [105/213], [94mLoss[0m : 2.12085
[1mStep[0m  [126/213], [94mLoss[0m : 2.06303
[1mStep[0m  [147/213], [94mLoss[0m : 1.69715
[1mStep[0m  [168/213], [94mLoss[0m : 1.82193
[1mStep[0m  [189/213], [94mLoss[0m : 1.69072
[1mStep[0m  [210/213], [94mLoss[0m : 2.47420

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.089, [92mTest[0m: 2.425, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.04122
[1mStep[0m  [21/213], [94mLoss[0m : 1.82121
[1mStep[0m  [42/213], [94mLoss[0m : 1.91062
[1mStep[0m  [63/213], [94mLoss[0m : 1.76506
[1mStep[0m  [84/213], [94mLoss[0m : 2.17715
[1mStep[0m  [105/213], [94mLoss[0m : 2.04036
[1mStep[0m  [126/213], [94mLoss[0m : 1.80001
[1mStep[0m  [147/213], [94mLoss[0m : 1.74114
[1mStep[0m  [168/213], [94mLoss[0m : 1.96681
[1mStep[0m  [189/213], [94mLoss[0m : 2.03802
[1mStep[0m  [210/213], [94mLoss[0m : 1.72743

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.031, [92mTest[0m: 2.363, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.17657
[1mStep[0m  [21/213], [94mLoss[0m : 2.10512
[1mStep[0m  [42/213], [94mLoss[0m : 2.02997
[1mStep[0m  [63/213], [94mLoss[0m : 1.56719
[1mStep[0m  [84/213], [94mLoss[0m : 1.69921
[1mStep[0m  [105/213], [94mLoss[0m : 2.35106
[1mStep[0m  [126/213], [94mLoss[0m : 2.16699
[1mStep[0m  [147/213], [94mLoss[0m : 2.03414
[1mStep[0m  [168/213], [94mLoss[0m : 1.86276
[1mStep[0m  [189/213], [94mLoss[0m : 2.15940
[1mStep[0m  [210/213], [94mLoss[0m : 1.85139

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.001, [92mTest[0m: 2.449, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.84430
[1mStep[0m  [21/213], [94mLoss[0m : 1.73569
[1mStep[0m  [42/213], [94mLoss[0m : 1.89507
[1mStep[0m  [63/213], [94mLoss[0m : 1.64492
[1mStep[0m  [84/213], [94mLoss[0m : 1.97406
[1mStep[0m  [105/213], [94mLoss[0m : 1.96668
[1mStep[0m  [126/213], [94mLoss[0m : 2.04801
[1mStep[0m  [147/213], [94mLoss[0m : 1.78082
[1mStep[0m  [168/213], [94mLoss[0m : 2.06325
[1mStep[0m  [189/213], [94mLoss[0m : 2.37822
[1mStep[0m  [210/213], [94mLoss[0m : 2.11852

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.960, [92mTest[0m: 2.475, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.87089
[1mStep[0m  [21/213], [94mLoss[0m : 2.47385
[1mStep[0m  [42/213], [94mLoss[0m : 2.37542
[1mStep[0m  [63/213], [94mLoss[0m : 1.78607
[1mStep[0m  [84/213], [94mLoss[0m : 2.09681
[1mStep[0m  [105/213], [94mLoss[0m : 2.03452
[1mStep[0m  [126/213], [94mLoss[0m : 1.83158
[1mStep[0m  [147/213], [94mLoss[0m : 1.76659
[1mStep[0m  [168/213], [94mLoss[0m : 2.03553
[1mStep[0m  [189/213], [94mLoss[0m : 1.62542
[1mStep[0m  [210/213], [94mLoss[0m : 2.10890

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.926, [92mTest[0m: 2.446, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.64078
[1mStep[0m  [21/213], [94mLoss[0m : 1.97312
[1mStep[0m  [42/213], [94mLoss[0m : 1.67853
[1mStep[0m  [63/213], [94mLoss[0m : 1.61749
[1mStep[0m  [84/213], [94mLoss[0m : 1.67943
[1mStep[0m  [105/213], [94mLoss[0m : 1.99146
[1mStep[0m  [126/213], [94mLoss[0m : 1.89440
[1mStep[0m  [147/213], [94mLoss[0m : 1.73924
[1mStep[0m  [168/213], [94mLoss[0m : 1.98163
[1mStep[0m  [189/213], [94mLoss[0m : 1.88725
[1mStep[0m  [210/213], [94mLoss[0m : 2.32668

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.897, [92mTest[0m: 2.436, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.45487
[1mStep[0m  [21/213], [94mLoss[0m : 1.55382
[1mStep[0m  [42/213], [94mLoss[0m : 1.93075
[1mStep[0m  [63/213], [94mLoss[0m : 1.87812
[1mStep[0m  [84/213], [94mLoss[0m : 1.64321
[1mStep[0m  [105/213], [94mLoss[0m : 1.56059
[1mStep[0m  [126/213], [94mLoss[0m : 1.92110
[1mStep[0m  [147/213], [94mLoss[0m : 1.92956
[1mStep[0m  [168/213], [94mLoss[0m : 1.81250
[1mStep[0m  [189/213], [94mLoss[0m : 2.04167
[1mStep[0m  [210/213], [94mLoss[0m : 1.91881

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.857, [92mTest[0m: 2.411, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.58129
[1mStep[0m  [21/213], [94mLoss[0m : 1.63618
[1mStep[0m  [42/213], [94mLoss[0m : 2.23273
[1mStep[0m  [63/213], [94mLoss[0m : 1.90980
[1mStep[0m  [84/213], [94mLoss[0m : 1.92097
[1mStep[0m  [105/213], [94mLoss[0m : 1.84576
[1mStep[0m  [126/213], [94mLoss[0m : 1.58911
[1mStep[0m  [147/213], [94mLoss[0m : 1.76482
[1mStep[0m  [168/213], [94mLoss[0m : 1.71569
[1mStep[0m  [189/213], [94mLoss[0m : 2.01108
[1mStep[0m  [210/213], [94mLoss[0m : 1.74239

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.822, [92mTest[0m: 2.486, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.66943
[1mStep[0m  [21/213], [94mLoss[0m : 1.77476
[1mStep[0m  [42/213], [94mLoss[0m : 1.83108
[1mStep[0m  [63/213], [94mLoss[0m : 1.87214
[1mStep[0m  [84/213], [94mLoss[0m : 1.65898
[1mStep[0m  [105/213], [94mLoss[0m : 1.93247
[1mStep[0m  [126/213], [94mLoss[0m : 1.74415
[1mStep[0m  [147/213], [94mLoss[0m : 1.77775
[1mStep[0m  [168/213], [94mLoss[0m : 2.44768
[1mStep[0m  [189/213], [94mLoss[0m : 1.87802
[1mStep[0m  [210/213], [94mLoss[0m : 1.78118

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.785, [92mTest[0m: 2.485, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.81662
[1mStep[0m  [21/213], [94mLoss[0m : 1.56034
[1mStep[0m  [42/213], [94mLoss[0m : 1.87936
[1mStep[0m  [63/213], [94mLoss[0m : 1.62725
[1mStep[0m  [84/213], [94mLoss[0m : 1.69542
[1mStep[0m  [105/213], [94mLoss[0m : 1.44651
[1mStep[0m  [126/213], [94mLoss[0m : 1.88523
[1mStep[0m  [147/213], [94mLoss[0m : 2.04755
[1mStep[0m  [168/213], [94mLoss[0m : 1.81112
[1mStep[0m  [189/213], [94mLoss[0m : 1.73452
[1mStep[0m  [210/213], [94mLoss[0m : 1.91383

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.770, [92mTest[0m: 2.455, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.65972
[1mStep[0m  [21/213], [94mLoss[0m : 1.62182
[1mStep[0m  [42/213], [94mLoss[0m : 1.63782
[1mStep[0m  [63/213], [94mLoss[0m : 1.60374
[1mStep[0m  [84/213], [94mLoss[0m : 1.44897
[1mStep[0m  [105/213], [94mLoss[0m : 1.91718
[1mStep[0m  [126/213], [94mLoss[0m : 1.71181
[1mStep[0m  [147/213], [94mLoss[0m : 1.58961
[1mStep[0m  [168/213], [94mLoss[0m : 1.75072
[1mStep[0m  [189/213], [94mLoss[0m : 1.52643
[1mStep[0m  [210/213], [94mLoss[0m : 1.72945

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.710, [92mTest[0m: 2.432, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.88634
[1mStep[0m  [21/213], [94mLoss[0m : 2.24095
[1mStep[0m  [42/213], [94mLoss[0m : 1.60322
[1mStep[0m  [63/213], [94mLoss[0m : 1.54189
[1mStep[0m  [84/213], [94mLoss[0m : 1.59220
[1mStep[0m  [105/213], [94mLoss[0m : 1.48280
[1mStep[0m  [126/213], [94mLoss[0m : 1.55442
[1mStep[0m  [147/213], [94mLoss[0m : 2.01350
[1mStep[0m  [168/213], [94mLoss[0m : 1.37460
[1mStep[0m  [189/213], [94mLoss[0m : 1.81590
[1mStep[0m  [210/213], [94mLoss[0m : 2.11839

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.706, [92mTest[0m: 2.464, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.64825
[1mStep[0m  [21/213], [94mLoss[0m : 1.63129
[1mStep[0m  [42/213], [94mLoss[0m : 1.71428
[1mStep[0m  [63/213], [94mLoss[0m : 1.58439
[1mStep[0m  [84/213], [94mLoss[0m : 1.72079
[1mStep[0m  [105/213], [94mLoss[0m : 1.46335
[1mStep[0m  [126/213], [94mLoss[0m : 1.70865
[1mStep[0m  [147/213], [94mLoss[0m : 1.47702
[1mStep[0m  [168/213], [94mLoss[0m : 1.46975
[1mStep[0m  [189/213], [94mLoss[0m : 1.85206
[1mStep[0m  [210/213], [94mLoss[0m : 1.63844

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.677, [92mTest[0m: 2.455, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.45700
[1mStep[0m  [21/213], [94mLoss[0m : 1.76214
[1mStep[0m  [42/213], [94mLoss[0m : 1.72927
[1mStep[0m  [63/213], [94mLoss[0m : 1.58989
[1mStep[0m  [84/213], [94mLoss[0m : 1.40561
[1mStep[0m  [105/213], [94mLoss[0m : 1.55330
[1mStep[0m  [126/213], [94mLoss[0m : 1.59487
[1mStep[0m  [147/213], [94mLoss[0m : 1.48774
[1mStep[0m  [168/213], [94mLoss[0m : 2.19146
[1mStep[0m  [189/213], [94mLoss[0m : 1.86268
[1mStep[0m  [210/213], [94mLoss[0m : 1.73695

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.626, [92mTest[0m: 2.440, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.34071
[1mStep[0m  [21/213], [94mLoss[0m : 1.41816
[1mStep[0m  [42/213], [94mLoss[0m : 1.43872
[1mStep[0m  [63/213], [94mLoss[0m : 1.48539
[1mStep[0m  [84/213], [94mLoss[0m : 1.54983
[1mStep[0m  [105/213], [94mLoss[0m : 1.46078
[1mStep[0m  [126/213], [94mLoss[0m : 1.43786
[1mStep[0m  [147/213], [94mLoss[0m : 1.41678
[1mStep[0m  [168/213], [94mLoss[0m : 1.69830
[1mStep[0m  [189/213], [94mLoss[0m : 1.50601
[1mStep[0m  [210/213], [94mLoss[0m : 1.55121

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.606, [92mTest[0m: 2.457, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.38889
[1mStep[0m  [21/213], [94mLoss[0m : 1.51147
[1mStep[0m  [42/213], [94mLoss[0m : 1.36616
[1mStep[0m  [63/213], [94mLoss[0m : 1.65538
[1mStep[0m  [84/213], [94mLoss[0m : 1.37318
[1mStep[0m  [105/213], [94mLoss[0m : 1.67250
[1mStep[0m  [126/213], [94mLoss[0m : 1.81597
[1mStep[0m  [147/213], [94mLoss[0m : 2.00990
[1mStep[0m  [168/213], [94mLoss[0m : 1.49184
[1mStep[0m  [189/213], [94mLoss[0m : 1.46587
[1mStep[0m  [210/213], [94mLoss[0m : 1.67758

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.570, [92mTest[0m: 2.454, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.56297
[1mStep[0m  [21/213], [94mLoss[0m : 1.52115
[1mStep[0m  [42/213], [94mLoss[0m : 1.55083
[1mStep[0m  [63/213], [94mLoss[0m : 1.44695
[1mStep[0m  [84/213], [94mLoss[0m : 1.76641
[1mStep[0m  [105/213], [94mLoss[0m : 1.26831
[1mStep[0m  [126/213], [94mLoss[0m : 1.62555
[1mStep[0m  [147/213], [94mLoss[0m : 1.62782
[1mStep[0m  [168/213], [94mLoss[0m : 1.58165
[1mStep[0m  [189/213], [94mLoss[0m : 1.82000
[1mStep[0m  [210/213], [94mLoss[0m : 1.96281

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.577, [92mTest[0m: 2.466, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.74235
[1mStep[0m  [21/213], [94mLoss[0m : 1.48243
[1mStep[0m  [42/213], [94mLoss[0m : 1.25040
[1mStep[0m  [63/213], [94mLoss[0m : 1.63408
[1mStep[0m  [84/213], [94mLoss[0m : 1.83838
[1mStep[0m  [105/213], [94mLoss[0m : 1.33170
[1mStep[0m  [126/213], [94mLoss[0m : 1.60389
[1mStep[0m  [147/213], [94mLoss[0m : 1.49463
[1mStep[0m  [168/213], [94mLoss[0m : 1.83406
[1mStep[0m  [189/213], [94mLoss[0m : 1.35104
[1mStep[0m  [210/213], [94mLoss[0m : 1.49077

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.534, [92mTest[0m: 2.465, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.56029
[1mStep[0m  [21/213], [94mLoss[0m : 1.60895
[1mStep[0m  [42/213], [94mLoss[0m : 1.69894
[1mStep[0m  [63/213], [94mLoss[0m : 1.57240
[1mStep[0m  [84/213], [94mLoss[0m : 1.44856
[1mStep[0m  [105/213], [94mLoss[0m : 1.59495
[1mStep[0m  [126/213], [94mLoss[0m : 1.64964
[1mStep[0m  [147/213], [94mLoss[0m : 1.59896
[1mStep[0m  [168/213], [94mLoss[0m : 1.43022
[1mStep[0m  [189/213], [94mLoss[0m : 1.46927
[1mStep[0m  [210/213], [94mLoss[0m : 1.36193

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.507, [92mTest[0m: 2.455, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.53331
[1mStep[0m  [21/213], [94mLoss[0m : 1.42895
[1mStep[0m  [42/213], [94mLoss[0m : 1.56659
[1mStep[0m  [63/213], [94mLoss[0m : 1.58878
[1mStep[0m  [84/213], [94mLoss[0m : 1.34613
[1mStep[0m  [105/213], [94mLoss[0m : 1.74889
[1mStep[0m  [126/213], [94mLoss[0m : 1.49012
[1mStep[0m  [147/213], [94mLoss[0m : 1.55477
[1mStep[0m  [168/213], [94mLoss[0m : 1.54390
[1mStep[0m  [189/213], [94mLoss[0m : 1.62278
[1mStep[0m  [210/213], [94mLoss[0m : 1.23676

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.516, [92mTest[0m: 2.462, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.52525
[1mStep[0m  [21/213], [94mLoss[0m : 1.36539
[1mStep[0m  [42/213], [94mLoss[0m : 1.55489
[1mStep[0m  [63/213], [94mLoss[0m : 1.57634
[1mStep[0m  [84/213], [94mLoss[0m : 1.33173
[1mStep[0m  [105/213], [94mLoss[0m : 1.40504
[1mStep[0m  [126/213], [94mLoss[0m : 1.66336
[1mStep[0m  [147/213], [94mLoss[0m : 1.43907
[1mStep[0m  [168/213], [94mLoss[0m : 1.83107
[1mStep[0m  [189/213], [94mLoss[0m : 1.46743
[1mStep[0m  [210/213], [94mLoss[0m : 1.32215

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.487, [92mTest[0m: 2.477, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.65887
[1mStep[0m  [21/213], [94mLoss[0m : 1.23458
[1mStep[0m  [42/213], [94mLoss[0m : 1.24809
[1mStep[0m  [63/213], [94mLoss[0m : 1.38971
[1mStep[0m  [84/213], [94mLoss[0m : 1.06129
[1mStep[0m  [105/213], [94mLoss[0m : 1.29108
[1mStep[0m  [126/213], [94mLoss[0m : 1.43529
[1mStep[0m  [147/213], [94mLoss[0m : 1.65128
[1mStep[0m  [168/213], [94mLoss[0m : 1.50820
[1mStep[0m  [189/213], [94mLoss[0m : 1.42804
[1mStep[0m  [210/213], [94mLoss[0m : 1.39012

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.469, [92mTest[0m: 2.466, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.50224
[1mStep[0m  [21/213], [94mLoss[0m : 1.46526
[1mStep[0m  [42/213], [94mLoss[0m : 1.22373
[1mStep[0m  [63/213], [94mLoss[0m : 1.30942
[1mStep[0m  [84/213], [94mLoss[0m : 1.41152
[1mStep[0m  [105/213], [94mLoss[0m : 1.44870
[1mStep[0m  [126/213], [94mLoss[0m : 1.30311
[1mStep[0m  [147/213], [94mLoss[0m : 1.27448
[1mStep[0m  [168/213], [94mLoss[0m : 1.48500
[1mStep[0m  [189/213], [94mLoss[0m : 1.40080
[1mStep[0m  [210/213], [94mLoss[0m : 1.33426

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.441, [92mTest[0m: 2.450, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.478
====================================

Phase 2 - Evaluation MAE:  2.4784989255779193
MAE score P1       2.318188
MAE score P2       2.478499
loss               1.441168
learning_rate       0.00505
batch_size               64
hidden_sizes      [250, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.5
weight_decay          0.001
Name: 9, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/213], [94mLoss[0m : 10.71835
[1mStep[0m  [21/213], [94mLoss[0m : 11.50310
[1mStep[0m  [42/213], [94mLoss[0m : 10.70438
[1mStep[0m  [63/213], [94mLoss[0m : 10.05253
[1mStep[0m  [84/213], [94mLoss[0m : 10.04145
[1mStep[0m  [105/213], [94mLoss[0m : 10.17740
[1mStep[0m  [126/213], [94mLoss[0m : 10.50800
[1mStep[0m  [147/213], [94mLoss[0m : 9.09964
[1mStep[0m  [168/213], [94mLoss[0m : 9.98553
[1mStep[0m  [189/213], [94mLoss[0m : 9.41928
[1mStep[0m  [210/213], [94mLoss[0m : 8.62510

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.066, [92mTest[0m: 10.789, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 9.24563
[1mStep[0m  [21/213], [94mLoss[0m : 9.20204
[1mStep[0m  [42/213], [94mLoss[0m : 8.20099
[1mStep[0m  [63/213], [94mLoss[0m : 8.76790
[1mStep[0m  [84/213], [94mLoss[0m : 9.17366
[1mStep[0m  [105/213], [94mLoss[0m : 7.26943
[1mStep[0m  [126/213], [94mLoss[0m : 7.49697
[1mStep[0m  [147/213], [94mLoss[0m : 7.25827
[1mStep[0m  [168/213], [94mLoss[0m : 6.82469
[1mStep[0m  [189/213], [94mLoss[0m : 6.93405
[1mStep[0m  [210/213], [94mLoss[0m : 6.36228

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 8.021, [92mTest[0m: 8.788, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 6.77783
[1mStep[0m  [21/213], [94mLoss[0m : 5.84643
[1mStep[0m  [42/213], [94mLoss[0m : 5.91909
[1mStep[0m  [63/213], [94mLoss[0m : 6.51640
[1mStep[0m  [84/213], [94mLoss[0m : 5.49480
[1mStep[0m  [105/213], [94mLoss[0m : 4.81966
[1mStep[0m  [126/213], [94mLoss[0m : 4.94552
[1mStep[0m  [147/213], [94mLoss[0m : 4.15095
[1mStep[0m  [168/213], [94mLoss[0m : 4.22161
[1mStep[0m  [189/213], [94mLoss[0m : 3.94871
[1mStep[0m  [210/213], [94mLoss[0m : 3.45293

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 5.094, [92mTest[0m: 5.704, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 3.95127
[1mStep[0m  [21/213], [94mLoss[0m : 4.08073
[1mStep[0m  [42/213], [94mLoss[0m : 3.55837
[1mStep[0m  [63/213], [94mLoss[0m : 2.76347
[1mStep[0m  [84/213], [94mLoss[0m : 2.78564
[1mStep[0m  [105/213], [94mLoss[0m : 2.71637
[1mStep[0m  [126/213], [94mLoss[0m : 2.63046
[1mStep[0m  [147/213], [94mLoss[0m : 2.50722
[1mStep[0m  [168/213], [94mLoss[0m : 3.52344
[1mStep[0m  [189/213], [94mLoss[0m : 2.66279
[1mStep[0m  [210/213], [94mLoss[0m : 2.20388

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.888, [92mTest[0m: 3.082, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.82030
[1mStep[0m  [21/213], [94mLoss[0m : 2.83964
[1mStep[0m  [42/213], [94mLoss[0m : 2.84865
[1mStep[0m  [63/213], [94mLoss[0m : 2.77460
[1mStep[0m  [84/213], [94mLoss[0m : 2.40377
[1mStep[0m  [105/213], [94mLoss[0m : 2.83431
[1mStep[0m  [126/213], [94mLoss[0m : 2.72810
[1mStep[0m  [147/213], [94mLoss[0m : 2.42914
[1mStep[0m  [168/213], [94mLoss[0m : 2.19009
[1mStep[0m  [189/213], [94mLoss[0m : 2.20790
[1mStep[0m  [210/213], [94mLoss[0m : 2.70560

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.630, [92mTest[0m: 2.467, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.63014
[1mStep[0m  [21/213], [94mLoss[0m : 2.78077
[1mStep[0m  [42/213], [94mLoss[0m : 2.66925
[1mStep[0m  [63/213], [94mLoss[0m : 2.80540
[1mStep[0m  [84/213], [94mLoss[0m : 2.54509
[1mStep[0m  [105/213], [94mLoss[0m : 2.56249
[1mStep[0m  [126/213], [94mLoss[0m : 2.62708
[1mStep[0m  [147/213], [94mLoss[0m : 3.03609
[1mStep[0m  [168/213], [94mLoss[0m : 2.47073
[1mStep[0m  [189/213], [94mLoss[0m : 2.62674
[1mStep[0m  [210/213], [94mLoss[0m : 2.04506

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.607, [92mTest[0m: 2.450, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.76486
[1mStep[0m  [21/213], [94mLoss[0m : 2.54093
[1mStep[0m  [42/213], [94mLoss[0m : 2.52865
[1mStep[0m  [63/213], [94mLoss[0m : 2.47962
[1mStep[0m  [84/213], [94mLoss[0m : 2.71773
[1mStep[0m  [105/213], [94mLoss[0m : 2.55632
[1mStep[0m  [126/213], [94mLoss[0m : 2.54952
[1mStep[0m  [147/213], [94mLoss[0m : 2.45552
[1mStep[0m  [168/213], [94mLoss[0m : 2.86950
[1mStep[0m  [189/213], [94mLoss[0m : 2.68910
[1mStep[0m  [210/213], [94mLoss[0m : 2.40915

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.577, [92mTest[0m: 2.421, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.28919
[1mStep[0m  [21/213], [94mLoss[0m : 2.74445
[1mStep[0m  [42/213], [94mLoss[0m : 2.52399
[1mStep[0m  [63/213], [94mLoss[0m : 2.48972
[1mStep[0m  [84/213], [94mLoss[0m : 2.80343
[1mStep[0m  [105/213], [94mLoss[0m : 2.73537
[1mStep[0m  [126/213], [94mLoss[0m : 2.70590
[1mStep[0m  [147/213], [94mLoss[0m : 2.88407
[1mStep[0m  [168/213], [94mLoss[0m : 2.51734
[1mStep[0m  [189/213], [94mLoss[0m : 2.64048
[1mStep[0m  [210/213], [94mLoss[0m : 2.68084

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.572, [92mTest[0m: 2.464, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.14125
[1mStep[0m  [21/213], [94mLoss[0m : 2.89456
[1mStep[0m  [42/213], [94mLoss[0m : 2.63160
[1mStep[0m  [63/213], [94mLoss[0m : 2.49500
[1mStep[0m  [84/213], [94mLoss[0m : 2.57301
[1mStep[0m  [105/213], [94mLoss[0m : 2.66467
[1mStep[0m  [126/213], [94mLoss[0m : 2.33426
[1mStep[0m  [147/213], [94mLoss[0m : 3.05801
[1mStep[0m  [168/213], [94mLoss[0m : 2.48799
[1mStep[0m  [189/213], [94mLoss[0m : 2.58825
[1mStep[0m  [210/213], [94mLoss[0m : 2.56992

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.557, [92mTest[0m: 2.425, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.93571
[1mStep[0m  [21/213], [94mLoss[0m : 2.58638
[1mStep[0m  [42/213], [94mLoss[0m : 2.24787
[1mStep[0m  [63/213], [94mLoss[0m : 2.23423
[1mStep[0m  [84/213], [94mLoss[0m : 2.39556
[1mStep[0m  [105/213], [94mLoss[0m : 2.54021
[1mStep[0m  [126/213], [94mLoss[0m : 2.62424
[1mStep[0m  [147/213], [94mLoss[0m : 2.21094
[1mStep[0m  [168/213], [94mLoss[0m : 2.46346
[1mStep[0m  [189/213], [94mLoss[0m : 2.52123
[1mStep[0m  [210/213], [94mLoss[0m : 2.24560

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.551, [92mTest[0m: 2.433, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.44847
[1mStep[0m  [21/213], [94mLoss[0m : 2.60673
[1mStep[0m  [42/213], [94mLoss[0m : 2.02197
[1mStep[0m  [63/213], [94mLoss[0m : 2.25122
[1mStep[0m  [84/213], [94mLoss[0m : 2.44424
[1mStep[0m  [105/213], [94mLoss[0m : 2.98487
[1mStep[0m  [126/213], [94mLoss[0m : 2.65853
[1mStep[0m  [147/213], [94mLoss[0m : 2.74935
[1mStep[0m  [168/213], [94mLoss[0m : 2.41237
[1mStep[0m  [189/213], [94mLoss[0m : 2.34906
[1mStep[0m  [210/213], [94mLoss[0m : 2.33975

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.526, [92mTest[0m: 2.414, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.29519
[1mStep[0m  [21/213], [94mLoss[0m : 2.49068
[1mStep[0m  [42/213], [94mLoss[0m : 2.52759
[1mStep[0m  [63/213], [94mLoss[0m : 2.68757
[1mStep[0m  [84/213], [94mLoss[0m : 2.22053
[1mStep[0m  [105/213], [94mLoss[0m : 2.43231
[1mStep[0m  [126/213], [94mLoss[0m : 2.54443
[1mStep[0m  [147/213], [94mLoss[0m : 2.45291
[1mStep[0m  [168/213], [94mLoss[0m : 2.36333
[1mStep[0m  [189/213], [94mLoss[0m : 2.61687
[1mStep[0m  [210/213], [94mLoss[0m : 2.85004

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.529, [92mTest[0m: 2.381, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.59212
[1mStep[0m  [21/213], [94mLoss[0m : 2.72276
[1mStep[0m  [42/213], [94mLoss[0m : 2.54384
[1mStep[0m  [63/213], [94mLoss[0m : 2.18385
[1mStep[0m  [84/213], [94mLoss[0m : 2.33063
[1mStep[0m  [105/213], [94mLoss[0m : 2.25213
[1mStep[0m  [126/213], [94mLoss[0m : 3.07230
[1mStep[0m  [147/213], [94mLoss[0m : 2.59702
[1mStep[0m  [168/213], [94mLoss[0m : 2.55886
[1mStep[0m  [189/213], [94mLoss[0m : 2.69606
[1mStep[0m  [210/213], [94mLoss[0m : 2.47213

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.515, [92mTest[0m: 2.419, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.24009
[1mStep[0m  [21/213], [94mLoss[0m : 2.56349
[1mStep[0m  [42/213], [94mLoss[0m : 2.25971
[1mStep[0m  [63/213], [94mLoss[0m : 2.75699
[1mStep[0m  [84/213], [94mLoss[0m : 2.43350
[1mStep[0m  [105/213], [94mLoss[0m : 2.59363
[1mStep[0m  [126/213], [94mLoss[0m : 2.82960
[1mStep[0m  [147/213], [94mLoss[0m : 2.25221
[1mStep[0m  [168/213], [94mLoss[0m : 2.96893
[1mStep[0m  [189/213], [94mLoss[0m : 2.43593
[1mStep[0m  [210/213], [94mLoss[0m : 2.29551

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.424, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.45284
[1mStep[0m  [21/213], [94mLoss[0m : 2.34419
[1mStep[0m  [42/213], [94mLoss[0m : 2.37198
[1mStep[0m  [63/213], [94mLoss[0m : 3.19135
[1mStep[0m  [84/213], [94mLoss[0m : 2.00959
[1mStep[0m  [105/213], [94mLoss[0m : 2.44542
[1mStep[0m  [126/213], [94mLoss[0m : 2.48600
[1mStep[0m  [147/213], [94mLoss[0m : 2.59352
[1mStep[0m  [168/213], [94mLoss[0m : 2.45678
[1mStep[0m  [189/213], [94mLoss[0m : 2.56534
[1mStep[0m  [210/213], [94mLoss[0m : 2.46668

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.380, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.95314
[1mStep[0m  [21/213], [94mLoss[0m : 2.65800
[1mStep[0m  [42/213], [94mLoss[0m : 2.11582
[1mStep[0m  [63/213], [94mLoss[0m : 2.32491
[1mStep[0m  [84/213], [94mLoss[0m : 2.44826
[1mStep[0m  [105/213], [94mLoss[0m : 2.42618
[1mStep[0m  [126/213], [94mLoss[0m : 2.70433
[1mStep[0m  [147/213], [94mLoss[0m : 2.93746
[1mStep[0m  [168/213], [94mLoss[0m : 2.43564
[1mStep[0m  [189/213], [94mLoss[0m : 2.38816
[1mStep[0m  [210/213], [94mLoss[0m : 2.41037

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.369, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.81348
[1mStep[0m  [21/213], [94mLoss[0m : 2.19377
[1mStep[0m  [42/213], [94mLoss[0m : 2.50059
[1mStep[0m  [63/213], [94mLoss[0m : 2.41709
[1mStep[0m  [84/213], [94mLoss[0m : 1.97549
[1mStep[0m  [105/213], [94mLoss[0m : 2.34118
[1mStep[0m  [126/213], [94mLoss[0m : 2.22790
[1mStep[0m  [147/213], [94mLoss[0m : 2.82946
[1mStep[0m  [168/213], [94mLoss[0m : 2.61427
[1mStep[0m  [189/213], [94mLoss[0m : 2.10587
[1mStep[0m  [210/213], [94mLoss[0m : 2.75916

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.375, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.34376
[1mStep[0m  [21/213], [94mLoss[0m : 2.45519
[1mStep[0m  [42/213], [94mLoss[0m : 2.46009
[1mStep[0m  [63/213], [94mLoss[0m : 2.40318
[1mStep[0m  [84/213], [94mLoss[0m : 2.74642
[1mStep[0m  [105/213], [94mLoss[0m : 2.20342
[1mStep[0m  [126/213], [94mLoss[0m : 2.66427
[1mStep[0m  [147/213], [94mLoss[0m : 2.75159
[1mStep[0m  [168/213], [94mLoss[0m : 2.90605
[1mStep[0m  [189/213], [94mLoss[0m : 2.18015
[1mStep[0m  [210/213], [94mLoss[0m : 2.20928

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.397, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 3.27806
[1mStep[0m  [21/213], [94mLoss[0m : 2.19496
[1mStep[0m  [42/213], [94mLoss[0m : 2.53449
[1mStep[0m  [63/213], [94mLoss[0m : 2.70397
[1mStep[0m  [84/213], [94mLoss[0m : 2.09393
[1mStep[0m  [105/213], [94mLoss[0m : 2.21179
[1mStep[0m  [126/213], [94mLoss[0m : 2.94810
[1mStep[0m  [147/213], [94mLoss[0m : 1.98168
[1mStep[0m  [168/213], [94mLoss[0m : 2.59232
[1mStep[0m  [189/213], [94mLoss[0m : 2.14781
[1mStep[0m  [210/213], [94mLoss[0m : 2.35068

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.355, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.62455
[1mStep[0m  [21/213], [94mLoss[0m : 2.46541
[1mStep[0m  [42/213], [94mLoss[0m : 2.50488
[1mStep[0m  [63/213], [94mLoss[0m : 2.21793
[1mStep[0m  [84/213], [94mLoss[0m : 2.92252
[1mStep[0m  [105/213], [94mLoss[0m : 2.42040
[1mStep[0m  [126/213], [94mLoss[0m : 2.39370
[1mStep[0m  [147/213], [94mLoss[0m : 2.48095
[1mStep[0m  [168/213], [94mLoss[0m : 2.35872
[1mStep[0m  [189/213], [94mLoss[0m : 2.29666
[1mStep[0m  [210/213], [94mLoss[0m : 2.54430

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.377, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.35368
[1mStep[0m  [21/213], [94mLoss[0m : 2.55251
[1mStep[0m  [42/213], [94mLoss[0m : 2.82148
[1mStep[0m  [63/213], [94mLoss[0m : 2.34009
[1mStep[0m  [84/213], [94mLoss[0m : 2.59018
[1mStep[0m  [105/213], [94mLoss[0m : 2.81346
[1mStep[0m  [126/213], [94mLoss[0m : 2.04275
[1mStep[0m  [147/213], [94mLoss[0m : 2.27116
[1mStep[0m  [168/213], [94mLoss[0m : 2.34860
[1mStep[0m  [189/213], [94mLoss[0m : 2.65096
[1mStep[0m  [210/213], [94mLoss[0m : 2.54325

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.389, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.11701
[1mStep[0m  [21/213], [94mLoss[0m : 2.57661
[1mStep[0m  [42/213], [94mLoss[0m : 2.51505
[1mStep[0m  [63/213], [94mLoss[0m : 2.39623
[1mStep[0m  [84/213], [94mLoss[0m : 2.63361
[1mStep[0m  [105/213], [94mLoss[0m : 2.23334
[1mStep[0m  [126/213], [94mLoss[0m : 2.64440
[1mStep[0m  [147/213], [94mLoss[0m : 2.49028
[1mStep[0m  [168/213], [94mLoss[0m : 2.40463
[1mStep[0m  [189/213], [94mLoss[0m : 2.81735
[1mStep[0m  [210/213], [94mLoss[0m : 2.50834

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.367, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.39389
[1mStep[0m  [21/213], [94mLoss[0m : 2.17620
[1mStep[0m  [42/213], [94mLoss[0m : 3.32411
[1mStep[0m  [63/213], [94mLoss[0m : 2.31088
[1mStep[0m  [84/213], [94mLoss[0m : 2.85645
[1mStep[0m  [105/213], [94mLoss[0m : 2.21765
[1mStep[0m  [126/213], [94mLoss[0m : 2.73529
[1mStep[0m  [147/213], [94mLoss[0m : 2.81950
[1mStep[0m  [168/213], [94mLoss[0m : 2.34119
[1mStep[0m  [189/213], [94mLoss[0m : 2.35349
[1mStep[0m  [210/213], [94mLoss[0m : 2.53720

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.387, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.50021
[1mStep[0m  [21/213], [94mLoss[0m : 2.79240
[1mStep[0m  [42/213], [94mLoss[0m : 2.28820
[1mStep[0m  [63/213], [94mLoss[0m : 2.36852
[1mStep[0m  [84/213], [94mLoss[0m : 2.35412
[1mStep[0m  [105/213], [94mLoss[0m : 3.24319
[1mStep[0m  [126/213], [94mLoss[0m : 2.69740
[1mStep[0m  [147/213], [94mLoss[0m : 2.27181
[1mStep[0m  [168/213], [94mLoss[0m : 2.27664
[1mStep[0m  [189/213], [94mLoss[0m : 2.01926
[1mStep[0m  [210/213], [94mLoss[0m : 2.49536

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.370, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.26300
[1mStep[0m  [21/213], [94mLoss[0m : 2.32276
[1mStep[0m  [42/213], [94mLoss[0m : 2.42499
[1mStep[0m  [63/213], [94mLoss[0m : 2.09031
[1mStep[0m  [84/213], [94mLoss[0m : 2.12626
[1mStep[0m  [105/213], [94mLoss[0m : 2.47100
[1mStep[0m  [126/213], [94mLoss[0m : 2.42095
[1mStep[0m  [147/213], [94mLoss[0m : 2.69920
[1mStep[0m  [168/213], [94mLoss[0m : 2.73086
[1mStep[0m  [189/213], [94mLoss[0m : 2.32387
[1mStep[0m  [210/213], [94mLoss[0m : 2.15332

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.380, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.43232
[1mStep[0m  [21/213], [94mLoss[0m : 2.47307
[1mStep[0m  [42/213], [94mLoss[0m : 2.37818
[1mStep[0m  [63/213], [94mLoss[0m : 2.43532
[1mStep[0m  [84/213], [94mLoss[0m : 2.20916
[1mStep[0m  [105/213], [94mLoss[0m : 2.58600
[1mStep[0m  [126/213], [94mLoss[0m : 2.31198
[1mStep[0m  [147/213], [94mLoss[0m : 2.67356
[1mStep[0m  [168/213], [94mLoss[0m : 2.04693
[1mStep[0m  [189/213], [94mLoss[0m : 2.39896
[1mStep[0m  [210/213], [94mLoss[0m : 2.67649

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.359, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.53528
[1mStep[0m  [21/213], [94mLoss[0m : 2.63840
[1mStep[0m  [42/213], [94mLoss[0m : 2.16307
[1mStep[0m  [63/213], [94mLoss[0m : 2.38413
[1mStep[0m  [84/213], [94mLoss[0m : 2.77303
[1mStep[0m  [105/213], [94mLoss[0m : 2.16238
[1mStep[0m  [126/213], [94mLoss[0m : 2.14307
[1mStep[0m  [147/213], [94mLoss[0m : 2.70442
[1mStep[0m  [168/213], [94mLoss[0m : 2.33892
[1mStep[0m  [189/213], [94mLoss[0m : 2.43368
[1mStep[0m  [210/213], [94mLoss[0m : 2.45313

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.376, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.21952
[1mStep[0m  [21/213], [94mLoss[0m : 2.45481
[1mStep[0m  [42/213], [94mLoss[0m : 2.61635
[1mStep[0m  [63/213], [94mLoss[0m : 2.21115
[1mStep[0m  [84/213], [94mLoss[0m : 2.59404
[1mStep[0m  [105/213], [94mLoss[0m : 2.62158
[1mStep[0m  [126/213], [94mLoss[0m : 2.28198
[1mStep[0m  [147/213], [94mLoss[0m : 2.73009
[1mStep[0m  [168/213], [94mLoss[0m : 2.74724
[1mStep[0m  [189/213], [94mLoss[0m : 2.08231
[1mStep[0m  [210/213], [94mLoss[0m : 2.08196

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.373, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.19135
[1mStep[0m  [21/213], [94mLoss[0m : 2.51251
[1mStep[0m  [42/213], [94mLoss[0m : 2.95060
[1mStep[0m  [63/213], [94mLoss[0m : 2.38756
[1mStep[0m  [84/213], [94mLoss[0m : 2.58151
[1mStep[0m  [105/213], [94mLoss[0m : 2.45154
[1mStep[0m  [126/213], [94mLoss[0m : 2.44980
[1mStep[0m  [147/213], [94mLoss[0m : 2.31421
[1mStep[0m  [168/213], [94mLoss[0m : 2.54373
[1mStep[0m  [189/213], [94mLoss[0m : 2.25475
[1mStep[0m  [210/213], [94mLoss[0m : 2.67101

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.373, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.70225
[1mStep[0m  [21/213], [94mLoss[0m : 2.45941
[1mStep[0m  [42/213], [94mLoss[0m : 2.84075
[1mStep[0m  [63/213], [94mLoss[0m : 2.40074
[1mStep[0m  [84/213], [94mLoss[0m : 2.11385
[1mStep[0m  [105/213], [94mLoss[0m : 2.39576
[1mStep[0m  [126/213], [94mLoss[0m : 2.28591
[1mStep[0m  [147/213], [94mLoss[0m : 2.60401
[1mStep[0m  [168/213], [94mLoss[0m : 2.30319
[1mStep[0m  [189/213], [94mLoss[0m : 2.29520
[1mStep[0m  [210/213], [94mLoss[0m : 2.59984

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.382, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.371
====================================

Phase 1 - Evaluation MAE:  2.3709271651393964
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/213], [94mLoss[0m : 2.50891
[1mStep[0m  [21/213], [94mLoss[0m : 2.90351
[1mStep[0m  [42/213], [94mLoss[0m : 2.19099
[1mStep[0m  [63/213], [94mLoss[0m : 2.54333
[1mStep[0m  [84/213], [94mLoss[0m : 2.56888
[1mStep[0m  [105/213], [94mLoss[0m : 2.85038
[1mStep[0m  [126/213], [94mLoss[0m : 2.63023
[1mStep[0m  [147/213], [94mLoss[0m : 3.02977
[1mStep[0m  [168/213], [94mLoss[0m : 2.59394
[1mStep[0m  [189/213], [94mLoss[0m : 2.41617
[1mStep[0m  [210/213], [94mLoss[0m : 2.58385

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.372, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.47718
[1mStep[0m  [21/213], [94mLoss[0m : 2.22728
[1mStep[0m  [42/213], [94mLoss[0m : 2.10410
[1mStep[0m  [63/213], [94mLoss[0m : 3.18837
[1mStep[0m  [84/213], [94mLoss[0m : 2.24671
[1mStep[0m  [105/213], [94mLoss[0m : 2.36047
[1mStep[0m  [126/213], [94mLoss[0m : 2.06361
[1mStep[0m  [147/213], [94mLoss[0m : 2.45618
[1mStep[0m  [168/213], [94mLoss[0m : 2.47281
[1mStep[0m  [189/213], [94mLoss[0m : 2.24482
[1mStep[0m  [210/213], [94mLoss[0m : 2.53721

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.559, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.41190
[1mStep[0m  [21/213], [94mLoss[0m : 2.31994
[1mStep[0m  [42/213], [94mLoss[0m : 2.13376
[1mStep[0m  [63/213], [94mLoss[0m : 2.55156
[1mStep[0m  [84/213], [94mLoss[0m : 2.51669
[1mStep[0m  [105/213], [94mLoss[0m : 2.47108
[1mStep[0m  [126/213], [94mLoss[0m : 2.01103
[1mStep[0m  [147/213], [94mLoss[0m : 2.49296
[1mStep[0m  [168/213], [94mLoss[0m : 2.73929
[1mStep[0m  [189/213], [94mLoss[0m : 2.25971
[1mStep[0m  [210/213], [94mLoss[0m : 2.88925

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.388, [92mTest[0m: 2.475, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.63831
[1mStep[0m  [21/213], [94mLoss[0m : 2.56650
[1mStep[0m  [42/213], [94mLoss[0m : 2.22180
[1mStep[0m  [63/213], [94mLoss[0m : 2.28247
[1mStep[0m  [84/213], [94mLoss[0m : 1.95588
[1mStep[0m  [105/213], [94mLoss[0m : 2.32630
[1mStep[0m  [126/213], [94mLoss[0m : 2.52868
[1mStep[0m  [147/213], [94mLoss[0m : 2.30017
[1mStep[0m  [168/213], [94mLoss[0m : 2.75829
[1mStep[0m  [189/213], [94mLoss[0m : 2.48862
[1mStep[0m  [210/213], [94mLoss[0m : 1.87896

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.320, [92mTest[0m: 2.383, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.64721
[1mStep[0m  [21/213], [94mLoss[0m : 2.29829
[1mStep[0m  [42/213], [94mLoss[0m : 2.71926
[1mStep[0m  [63/213], [94mLoss[0m : 2.20393
[1mStep[0m  [84/213], [94mLoss[0m : 2.20280
[1mStep[0m  [105/213], [94mLoss[0m : 2.30173
[1mStep[0m  [126/213], [94mLoss[0m : 2.11466
[1mStep[0m  [147/213], [94mLoss[0m : 2.09832
[1mStep[0m  [168/213], [94mLoss[0m : 2.35768
[1mStep[0m  [189/213], [94mLoss[0m : 2.40891
[1mStep[0m  [210/213], [94mLoss[0m : 2.03702

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.284, [92mTest[0m: 2.373, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.33225
[1mStep[0m  [21/213], [94mLoss[0m : 2.30446
[1mStep[0m  [42/213], [94mLoss[0m : 2.14488
[1mStep[0m  [63/213], [94mLoss[0m : 2.46221
[1mStep[0m  [84/213], [94mLoss[0m : 1.89350
[1mStep[0m  [105/213], [94mLoss[0m : 2.03139
[1mStep[0m  [126/213], [94mLoss[0m : 1.86958
[1mStep[0m  [147/213], [94mLoss[0m : 2.29624
[1mStep[0m  [168/213], [94mLoss[0m : 2.59423
[1mStep[0m  [189/213], [94mLoss[0m : 1.99387
[1mStep[0m  [210/213], [94mLoss[0m : 2.22017

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.226, [92mTest[0m: 2.386, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.90736
[1mStep[0m  [21/213], [94mLoss[0m : 2.40751
[1mStep[0m  [42/213], [94mLoss[0m : 2.34369
[1mStep[0m  [63/213], [94mLoss[0m : 2.14479
[1mStep[0m  [84/213], [94mLoss[0m : 2.00649
[1mStep[0m  [105/213], [94mLoss[0m : 1.88829
[1mStep[0m  [126/213], [94mLoss[0m : 1.91457
[1mStep[0m  [147/213], [94mLoss[0m : 2.13551
[1mStep[0m  [168/213], [94mLoss[0m : 2.09052
[1mStep[0m  [189/213], [94mLoss[0m : 2.28478
[1mStep[0m  [210/213], [94mLoss[0m : 2.27934

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.180, [92mTest[0m: 2.386, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.88002
[1mStep[0m  [21/213], [94mLoss[0m : 2.00454
[1mStep[0m  [42/213], [94mLoss[0m : 1.70158
[1mStep[0m  [63/213], [94mLoss[0m : 2.56278
[1mStep[0m  [84/213], [94mLoss[0m : 1.91229
[1mStep[0m  [105/213], [94mLoss[0m : 2.34874
[1mStep[0m  [126/213], [94mLoss[0m : 1.93375
[1mStep[0m  [147/213], [94mLoss[0m : 1.80891
[1mStep[0m  [168/213], [94mLoss[0m : 2.84600
[1mStep[0m  [189/213], [94mLoss[0m : 2.18538
[1mStep[0m  [210/213], [94mLoss[0m : 2.02507

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.128, [92mTest[0m: 2.418, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.21579
[1mStep[0m  [21/213], [94mLoss[0m : 1.90160
[1mStep[0m  [42/213], [94mLoss[0m : 2.02953
[1mStep[0m  [63/213], [94mLoss[0m : 2.23630
[1mStep[0m  [84/213], [94mLoss[0m : 2.08923
[1mStep[0m  [105/213], [94mLoss[0m : 2.21960
[1mStep[0m  [126/213], [94mLoss[0m : 2.43577
[1mStep[0m  [147/213], [94mLoss[0m : 1.96365
[1mStep[0m  [168/213], [94mLoss[0m : 1.83935
[1mStep[0m  [189/213], [94mLoss[0m : 2.21173
[1mStep[0m  [210/213], [94mLoss[0m : 2.52226

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.080, [92mTest[0m: 2.389, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.43396
[1mStep[0m  [21/213], [94mLoss[0m : 2.21733
[1mStep[0m  [42/213], [94mLoss[0m : 1.69165
[1mStep[0m  [63/213], [94mLoss[0m : 1.63523
[1mStep[0m  [84/213], [94mLoss[0m : 1.70421
[1mStep[0m  [105/213], [94mLoss[0m : 2.01174
[1mStep[0m  [126/213], [94mLoss[0m : 1.86610
[1mStep[0m  [147/213], [94mLoss[0m : 1.96275
[1mStep[0m  [168/213], [94mLoss[0m : 2.02007
[1mStep[0m  [189/213], [94mLoss[0m : 1.83456
[1mStep[0m  [210/213], [94mLoss[0m : 2.18194

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.015, [92mTest[0m: 2.396, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.53584
[1mStep[0m  [21/213], [94mLoss[0m : 2.04797
[1mStep[0m  [42/213], [94mLoss[0m : 2.03931
[1mStep[0m  [63/213], [94mLoss[0m : 2.31113
[1mStep[0m  [84/213], [94mLoss[0m : 2.06993
[1mStep[0m  [105/213], [94mLoss[0m : 1.89427
[1mStep[0m  [126/213], [94mLoss[0m : 1.71324
[1mStep[0m  [147/213], [94mLoss[0m : 2.36745
[1mStep[0m  [168/213], [94mLoss[0m : 1.76432
[1mStep[0m  [189/213], [94mLoss[0m : 2.15636
[1mStep[0m  [210/213], [94mLoss[0m : 2.39355

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.013, [92mTest[0m: 2.377, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.92568
[1mStep[0m  [21/213], [94mLoss[0m : 2.20714
[1mStep[0m  [42/213], [94mLoss[0m : 1.86578
[1mStep[0m  [63/213], [94mLoss[0m : 2.22585
[1mStep[0m  [84/213], [94mLoss[0m : 1.94749
[1mStep[0m  [105/213], [94mLoss[0m : 1.96940
[1mStep[0m  [126/213], [94mLoss[0m : 2.20076
[1mStep[0m  [147/213], [94mLoss[0m : 1.75879
[1mStep[0m  [168/213], [94mLoss[0m : 1.91655
[1mStep[0m  [189/213], [94mLoss[0m : 1.77534
[1mStep[0m  [210/213], [94mLoss[0m : 2.20609

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.959, [92mTest[0m: 2.401, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.93375
[1mStep[0m  [21/213], [94mLoss[0m : 1.81508
[1mStep[0m  [42/213], [94mLoss[0m : 1.96723
[1mStep[0m  [63/213], [94mLoss[0m : 1.88073
[1mStep[0m  [84/213], [94mLoss[0m : 2.14248
[1mStep[0m  [105/213], [94mLoss[0m : 1.78798
[1mStep[0m  [126/213], [94mLoss[0m : 1.98538
[1mStep[0m  [147/213], [94mLoss[0m : 1.77808
[1mStep[0m  [168/213], [94mLoss[0m : 1.72154
[1mStep[0m  [189/213], [94mLoss[0m : 1.62707
[1mStep[0m  [210/213], [94mLoss[0m : 1.80958

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.912, [92mTest[0m: 2.429, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.72730
[1mStep[0m  [21/213], [94mLoss[0m : 1.73434
[1mStep[0m  [42/213], [94mLoss[0m : 2.00769
[1mStep[0m  [63/213], [94mLoss[0m : 1.83608
[1mStep[0m  [84/213], [94mLoss[0m : 2.35712
[1mStep[0m  [105/213], [94mLoss[0m : 2.01749
[1mStep[0m  [126/213], [94mLoss[0m : 1.66043
[1mStep[0m  [147/213], [94mLoss[0m : 1.79589
[1mStep[0m  [168/213], [94mLoss[0m : 2.05745
[1mStep[0m  [189/213], [94mLoss[0m : 1.98089
[1mStep[0m  [210/213], [94mLoss[0m : 1.97803

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.889, [92mTest[0m: 2.431, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.47196
[1mStep[0m  [21/213], [94mLoss[0m : 2.00555
[1mStep[0m  [42/213], [94mLoss[0m : 1.80515
[1mStep[0m  [63/213], [94mLoss[0m : 1.86681
[1mStep[0m  [84/213], [94mLoss[0m : 1.95361
[1mStep[0m  [105/213], [94mLoss[0m : 2.01486
[1mStep[0m  [126/213], [94mLoss[0m : 2.16205
[1mStep[0m  [147/213], [94mLoss[0m : 1.51513
[1mStep[0m  [168/213], [94mLoss[0m : 2.01889
[1mStep[0m  [189/213], [94mLoss[0m : 1.41501
[1mStep[0m  [210/213], [94mLoss[0m : 1.98053

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.828, [92mTest[0m: 2.394, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.63055
[1mStep[0m  [21/213], [94mLoss[0m : 1.87529
[1mStep[0m  [42/213], [94mLoss[0m : 1.63955
[1mStep[0m  [63/213], [94mLoss[0m : 1.73307
[1mStep[0m  [84/213], [94mLoss[0m : 1.88373
[1mStep[0m  [105/213], [94mLoss[0m : 2.28149
[1mStep[0m  [126/213], [94mLoss[0m : 1.68164
[1mStep[0m  [147/213], [94mLoss[0m : 1.77009
[1mStep[0m  [168/213], [94mLoss[0m : 2.05311
[1mStep[0m  [189/213], [94mLoss[0m : 2.22815
[1mStep[0m  [210/213], [94mLoss[0m : 1.52081

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.822, [92mTest[0m: 2.444, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.61812
[1mStep[0m  [21/213], [94mLoss[0m : 2.01039
[1mStep[0m  [42/213], [94mLoss[0m : 1.78642
[1mStep[0m  [63/213], [94mLoss[0m : 2.35816
[1mStep[0m  [84/213], [94mLoss[0m : 1.66571
[1mStep[0m  [105/213], [94mLoss[0m : 1.76735
[1mStep[0m  [126/213], [94mLoss[0m : 1.53493
[1mStep[0m  [147/213], [94mLoss[0m : 1.47386
[1mStep[0m  [168/213], [94mLoss[0m : 1.55833
[1mStep[0m  [189/213], [94mLoss[0m : 2.15659
[1mStep[0m  [210/213], [94mLoss[0m : 1.70225

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.786, [92mTest[0m: 2.429, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.62294
[1mStep[0m  [21/213], [94mLoss[0m : 1.84345
[1mStep[0m  [42/213], [94mLoss[0m : 1.44579
[1mStep[0m  [63/213], [94mLoss[0m : 1.93371
[1mStep[0m  [84/213], [94mLoss[0m : 1.89452
[1mStep[0m  [105/213], [94mLoss[0m : 1.44614
[1mStep[0m  [126/213], [94mLoss[0m : 1.28450
[1mStep[0m  [147/213], [94mLoss[0m : 1.74793
[1mStep[0m  [168/213], [94mLoss[0m : 2.10152
[1mStep[0m  [189/213], [94mLoss[0m : 1.82533
[1mStep[0m  [210/213], [94mLoss[0m : 1.86951

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.759, [92mTest[0m: 2.444, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.91762
[1mStep[0m  [21/213], [94mLoss[0m : 1.54389
[1mStep[0m  [42/213], [94mLoss[0m : 1.70158
[1mStep[0m  [63/213], [94mLoss[0m : 1.66918
[1mStep[0m  [84/213], [94mLoss[0m : 1.76422
[1mStep[0m  [105/213], [94mLoss[0m : 1.42879
[1mStep[0m  [126/213], [94mLoss[0m : 1.81146
[1mStep[0m  [147/213], [94mLoss[0m : 1.76752
[1mStep[0m  [168/213], [94mLoss[0m : 1.60081
[1mStep[0m  [189/213], [94mLoss[0m : 1.65487
[1mStep[0m  [210/213], [94mLoss[0m : 1.79437

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.711, [92mTest[0m: 2.450, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.95811
[1mStep[0m  [21/213], [94mLoss[0m : 1.34589
[1mStep[0m  [42/213], [94mLoss[0m : 2.10746
[1mStep[0m  [63/213], [94mLoss[0m : 1.91121
[1mStep[0m  [84/213], [94mLoss[0m : 1.60966
[1mStep[0m  [105/213], [94mLoss[0m : 1.64314
[1mStep[0m  [126/213], [94mLoss[0m : 2.00251
[1mStep[0m  [147/213], [94mLoss[0m : 1.82643
[1mStep[0m  [168/213], [94mLoss[0m : 1.89912
[1mStep[0m  [189/213], [94mLoss[0m : 1.53671
[1mStep[0m  [210/213], [94mLoss[0m : 1.61521

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.703, [92mTest[0m: 2.447, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.60452
[1mStep[0m  [21/213], [94mLoss[0m : 1.54687
[1mStep[0m  [42/213], [94mLoss[0m : 1.57142
[1mStep[0m  [63/213], [94mLoss[0m : 1.51080
[1mStep[0m  [84/213], [94mLoss[0m : 1.31036
[1mStep[0m  [105/213], [94mLoss[0m : 1.54509
[1mStep[0m  [126/213], [94mLoss[0m : 1.52023
[1mStep[0m  [147/213], [94mLoss[0m : 1.57180
[1mStep[0m  [168/213], [94mLoss[0m : 1.52565
[1mStep[0m  [189/213], [94mLoss[0m : 1.83124
[1mStep[0m  [210/213], [94mLoss[0m : 1.34158

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.652, [92mTest[0m: 2.435, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.60622
[1mStep[0m  [21/213], [94mLoss[0m : 1.50798
[1mStep[0m  [42/213], [94mLoss[0m : 1.76992
[1mStep[0m  [63/213], [94mLoss[0m : 1.42683
[1mStep[0m  [84/213], [94mLoss[0m : 1.72616
[1mStep[0m  [105/213], [94mLoss[0m : 1.71684
[1mStep[0m  [126/213], [94mLoss[0m : 1.66906
[1mStep[0m  [147/213], [94mLoss[0m : 2.05333
[1mStep[0m  [168/213], [94mLoss[0m : 1.60306
[1mStep[0m  [189/213], [94mLoss[0m : 1.67842
[1mStep[0m  [210/213], [94mLoss[0m : 1.89818

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.637, [92mTest[0m: 2.438, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.38221
[1mStep[0m  [21/213], [94mLoss[0m : 1.55672
[1mStep[0m  [42/213], [94mLoss[0m : 1.66560
[1mStep[0m  [63/213], [94mLoss[0m : 1.78674
[1mStep[0m  [84/213], [94mLoss[0m : 1.47739
[1mStep[0m  [105/213], [94mLoss[0m : 2.02315
[1mStep[0m  [126/213], [94mLoss[0m : 1.82099
[1mStep[0m  [147/213], [94mLoss[0m : 1.35744
[1mStep[0m  [168/213], [94mLoss[0m : 1.50747
[1mStep[0m  [189/213], [94mLoss[0m : 1.66341
[1mStep[0m  [210/213], [94mLoss[0m : 1.57224

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.594, [92mTest[0m: 2.490, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.50670
[1mStep[0m  [21/213], [94mLoss[0m : 1.45587
[1mStep[0m  [42/213], [94mLoss[0m : 1.50502
[1mStep[0m  [63/213], [94mLoss[0m : 1.76615
[1mStep[0m  [84/213], [94mLoss[0m : 1.51899
[1mStep[0m  [105/213], [94mLoss[0m : 1.37843
[1mStep[0m  [126/213], [94mLoss[0m : 1.91316
[1mStep[0m  [147/213], [94mLoss[0m : 1.60385
[1mStep[0m  [168/213], [94mLoss[0m : 1.39672
[1mStep[0m  [189/213], [94mLoss[0m : 1.65958
[1mStep[0m  [210/213], [94mLoss[0m : 1.43975

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.582, [92mTest[0m: 2.397, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.46987
[1mStep[0m  [21/213], [94mLoss[0m : 1.24163
[1mStep[0m  [42/213], [94mLoss[0m : 1.43698
[1mStep[0m  [63/213], [94mLoss[0m : 1.64709
[1mStep[0m  [84/213], [94mLoss[0m : 1.44877
[1mStep[0m  [105/213], [94mLoss[0m : 1.54663
[1mStep[0m  [126/213], [94mLoss[0m : 1.39785
[1mStep[0m  [147/213], [94mLoss[0m : 1.68311
[1mStep[0m  [168/213], [94mLoss[0m : 1.34847
[1mStep[0m  [189/213], [94mLoss[0m : 1.66742
[1mStep[0m  [210/213], [94mLoss[0m : 1.86004

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.567, [92mTest[0m: 2.470, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.36243
[1mStep[0m  [21/213], [94mLoss[0m : 1.51561
[1mStep[0m  [42/213], [94mLoss[0m : 1.50969
[1mStep[0m  [63/213], [94mLoss[0m : 1.51125
[1mStep[0m  [84/213], [94mLoss[0m : 1.86331
[1mStep[0m  [105/213], [94mLoss[0m : 1.35493
[1mStep[0m  [126/213], [94mLoss[0m : 1.45330
[1mStep[0m  [147/213], [94mLoss[0m : 1.62527
[1mStep[0m  [168/213], [94mLoss[0m : 1.54903
[1mStep[0m  [189/213], [94mLoss[0m : 1.65484
[1mStep[0m  [210/213], [94mLoss[0m : 1.63568

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.529, [92mTest[0m: 2.563, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.35017
[1mStep[0m  [21/213], [94mLoss[0m : 2.02587
[1mStep[0m  [42/213], [94mLoss[0m : 1.76910
[1mStep[0m  [63/213], [94mLoss[0m : 1.50287
[1mStep[0m  [84/213], [94mLoss[0m : 1.38134
[1mStep[0m  [105/213], [94mLoss[0m : 2.22030
[1mStep[0m  [126/213], [94mLoss[0m : 1.50431
[1mStep[0m  [147/213], [94mLoss[0m : 1.44357
[1mStep[0m  [168/213], [94mLoss[0m : 1.66274
[1mStep[0m  [189/213], [94mLoss[0m : 1.69620
[1mStep[0m  [210/213], [94mLoss[0m : 1.53556

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.530, [92mTest[0m: 2.444, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.56622
[1mStep[0m  [21/213], [94mLoss[0m : 1.63435
[1mStep[0m  [42/213], [94mLoss[0m : 1.58949
[1mStep[0m  [63/213], [94mLoss[0m : 1.27760
[1mStep[0m  [84/213], [94mLoss[0m : 1.36333
[1mStep[0m  [105/213], [94mLoss[0m : 1.58876
[1mStep[0m  [126/213], [94mLoss[0m : 1.52921
[1mStep[0m  [147/213], [94mLoss[0m : 1.36671
[1mStep[0m  [168/213], [94mLoss[0m : 1.57334
[1mStep[0m  [189/213], [94mLoss[0m : 1.38604
[1mStep[0m  [210/213], [94mLoss[0m : 1.82591

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.517, [92mTest[0m: 2.519, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.67784
[1mStep[0m  [21/213], [94mLoss[0m : 1.51906
[1mStep[0m  [42/213], [94mLoss[0m : 1.96277
[1mStep[0m  [63/213], [94mLoss[0m : 1.36353
[1mStep[0m  [84/213], [94mLoss[0m : 1.38243
[1mStep[0m  [105/213], [94mLoss[0m : 1.67308
[1mStep[0m  [126/213], [94mLoss[0m : 1.48855
[1mStep[0m  [147/213], [94mLoss[0m : 1.65172
[1mStep[0m  [168/213], [94mLoss[0m : 1.51634
[1mStep[0m  [189/213], [94mLoss[0m : 1.42771
[1mStep[0m  [210/213], [94mLoss[0m : 1.65488

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.481, [92mTest[0m: 2.486, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.46512
[1mStep[0m  [21/213], [94mLoss[0m : 1.67419
[1mStep[0m  [42/213], [94mLoss[0m : 1.44297
[1mStep[0m  [63/213], [94mLoss[0m : 1.58619
[1mStep[0m  [84/213], [94mLoss[0m : 1.79272
[1mStep[0m  [105/213], [94mLoss[0m : 1.38657
[1mStep[0m  [126/213], [94mLoss[0m : 1.46602
[1mStep[0m  [147/213], [94mLoss[0m : 1.35029
[1mStep[0m  [168/213], [94mLoss[0m : 1.42289
[1mStep[0m  [189/213], [94mLoss[0m : 1.29148
[1mStep[0m  [210/213], [94mLoss[0m : 1.97657

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.469, [92mTest[0m: 2.468, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.510
====================================

Phase 2 - Evaluation MAE:  2.5104592948589683
MAE score P1        2.370927
MAE score P2        2.510459
loss                1.468563
learning_rate        0.00505
batch_size                64
hidden_sizes      [300, 100]
epochs                    30
activation              tanh
optimizer                sgd
early stopping         False
dropout                  0.2
momentum                 0.1
weight_decay           0.001
Name: 10, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/213], [94mLoss[0m : 10.94339
[1mStep[0m  [21/213], [94mLoss[0m : 7.62198
[1mStep[0m  [42/213], [94mLoss[0m : 3.93444
[1mStep[0m  [63/213], [94mLoss[0m : 2.82439
[1mStep[0m  [84/213], [94mLoss[0m : 2.59535
[1mStep[0m  [105/213], [94mLoss[0m : 3.21628
[1mStep[0m  [126/213], [94mLoss[0m : 2.49297
[1mStep[0m  [147/213], [94mLoss[0m : 3.32774
[1mStep[0m  [168/213], [94mLoss[0m : 2.60287
[1mStep[0m  [189/213], [94mLoss[0m : 2.76065
[1mStep[0m  [210/213], [94mLoss[0m : 2.65682

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.890, [92mTest[0m: 11.010, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.73858
[1mStep[0m  [21/213], [94mLoss[0m : 2.66295
[1mStep[0m  [42/213], [94mLoss[0m : 2.78371
[1mStep[0m  [63/213], [94mLoss[0m : 2.76301
[1mStep[0m  [84/213], [94mLoss[0m : 2.54901
[1mStep[0m  [105/213], [94mLoss[0m : 2.68216
[1mStep[0m  [126/213], [94mLoss[0m : 2.90088
[1mStep[0m  [147/213], [94mLoss[0m : 2.62051
[1mStep[0m  [168/213], [94mLoss[0m : 2.48324
[1mStep[0m  [189/213], [94mLoss[0m : 2.60706
[1mStep[0m  [210/213], [94mLoss[0m : 2.46753

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.733, [92mTest[0m: 2.514, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.51305
[1mStep[0m  [21/213], [94mLoss[0m : 2.79190
[1mStep[0m  [42/213], [94mLoss[0m : 2.46824
[1mStep[0m  [63/213], [94mLoss[0m : 2.09770
[1mStep[0m  [84/213], [94mLoss[0m : 2.48471
[1mStep[0m  [105/213], [94mLoss[0m : 2.85275
[1mStep[0m  [126/213], [94mLoss[0m : 2.48191
[1mStep[0m  [147/213], [94mLoss[0m : 2.71304
[1mStep[0m  [168/213], [94mLoss[0m : 2.46096
[1mStep[0m  [189/213], [94mLoss[0m : 2.88473
[1mStep[0m  [210/213], [94mLoss[0m : 2.65808

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.660, [92mTest[0m: 2.464, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.39326
[1mStep[0m  [21/213], [94mLoss[0m : 2.45485
[1mStep[0m  [42/213], [94mLoss[0m : 2.30826
[1mStep[0m  [63/213], [94mLoss[0m : 2.47437
[1mStep[0m  [84/213], [94mLoss[0m : 2.54260
[1mStep[0m  [105/213], [94mLoss[0m : 2.77599
[1mStep[0m  [126/213], [94mLoss[0m : 2.81878
[1mStep[0m  [147/213], [94mLoss[0m : 2.60044
[1mStep[0m  [168/213], [94mLoss[0m : 2.77821
[1mStep[0m  [189/213], [94mLoss[0m : 3.12291
[1mStep[0m  [210/213], [94mLoss[0m : 2.86679

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.657, [92mTest[0m: 2.462, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.25964
[1mStep[0m  [21/213], [94mLoss[0m : 2.85078
[1mStep[0m  [42/213], [94mLoss[0m : 2.54292
[1mStep[0m  [63/213], [94mLoss[0m : 2.90962
[1mStep[0m  [84/213], [94mLoss[0m : 2.66323
[1mStep[0m  [105/213], [94mLoss[0m : 2.68347
[1mStep[0m  [126/213], [94mLoss[0m : 2.92367
[1mStep[0m  [147/213], [94mLoss[0m : 2.59176
[1mStep[0m  [168/213], [94mLoss[0m : 2.48691
[1mStep[0m  [189/213], [94mLoss[0m : 2.85553
[1mStep[0m  [210/213], [94mLoss[0m : 2.50253

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.619, [92mTest[0m: 2.425, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.55314
[1mStep[0m  [21/213], [94mLoss[0m : 2.59979
[1mStep[0m  [42/213], [94mLoss[0m : 2.84720
[1mStep[0m  [63/213], [94mLoss[0m : 3.05162
[1mStep[0m  [84/213], [94mLoss[0m : 2.84283
[1mStep[0m  [105/213], [94mLoss[0m : 2.77876
[1mStep[0m  [126/213], [94mLoss[0m : 2.61345
[1mStep[0m  [147/213], [94mLoss[0m : 2.93783
[1mStep[0m  [168/213], [94mLoss[0m : 3.05249
[1mStep[0m  [189/213], [94mLoss[0m : 2.11188
[1mStep[0m  [210/213], [94mLoss[0m : 2.62892

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.616, [92mTest[0m: 2.389, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.23535
[1mStep[0m  [21/213], [94mLoss[0m : 2.36841
[1mStep[0m  [42/213], [94mLoss[0m : 2.24364
[1mStep[0m  [63/213], [94mLoss[0m : 2.38055
[1mStep[0m  [84/213], [94mLoss[0m : 2.48740
[1mStep[0m  [105/213], [94mLoss[0m : 2.68331
[1mStep[0m  [126/213], [94mLoss[0m : 2.93296
[1mStep[0m  [147/213], [94mLoss[0m : 2.69453
[1mStep[0m  [168/213], [94mLoss[0m : 2.70268
[1mStep[0m  [189/213], [94mLoss[0m : 2.46465
[1mStep[0m  [210/213], [94mLoss[0m : 2.42566

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.592, [92mTest[0m: 2.400, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.65657
[1mStep[0m  [21/213], [94mLoss[0m : 2.13557
[1mStep[0m  [42/213], [94mLoss[0m : 2.74087
[1mStep[0m  [63/213], [94mLoss[0m : 2.70189
[1mStep[0m  [84/213], [94mLoss[0m : 2.88713
[1mStep[0m  [105/213], [94mLoss[0m : 2.35562
[1mStep[0m  [126/213], [94mLoss[0m : 2.87294
[1mStep[0m  [147/213], [94mLoss[0m : 3.01998
[1mStep[0m  [168/213], [94mLoss[0m : 3.02497
[1mStep[0m  [189/213], [94mLoss[0m : 2.62282
[1mStep[0m  [210/213], [94mLoss[0m : 2.43277

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.583, [92mTest[0m: 2.380, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.26747
[1mStep[0m  [21/213], [94mLoss[0m : 2.39133
[1mStep[0m  [42/213], [94mLoss[0m : 2.07346
[1mStep[0m  [63/213], [94mLoss[0m : 2.23078
[1mStep[0m  [84/213], [94mLoss[0m : 2.89882
[1mStep[0m  [105/213], [94mLoss[0m : 2.31406
[1mStep[0m  [126/213], [94mLoss[0m : 2.38614
[1mStep[0m  [147/213], [94mLoss[0m : 2.57601
[1mStep[0m  [168/213], [94mLoss[0m : 2.54321
[1mStep[0m  [189/213], [94mLoss[0m : 2.33362
[1mStep[0m  [210/213], [94mLoss[0m : 2.53008

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.544, [92mTest[0m: 2.394, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.46477
[1mStep[0m  [21/213], [94mLoss[0m : 2.84132
[1mStep[0m  [42/213], [94mLoss[0m : 2.68121
[1mStep[0m  [63/213], [94mLoss[0m : 2.42499
[1mStep[0m  [84/213], [94mLoss[0m : 2.10769
[1mStep[0m  [105/213], [94mLoss[0m : 2.55802
[1mStep[0m  [126/213], [94mLoss[0m : 2.25465
[1mStep[0m  [147/213], [94mLoss[0m : 2.74684
[1mStep[0m  [168/213], [94mLoss[0m : 2.53793
[1mStep[0m  [189/213], [94mLoss[0m : 2.53835
[1mStep[0m  [210/213], [94mLoss[0m : 2.54602

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.540, [92mTest[0m: 2.354, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 3.07002
[1mStep[0m  [21/213], [94mLoss[0m : 2.73166
[1mStep[0m  [42/213], [94mLoss[0m : 2.66928
[1mStep[0m  [63/213], [94mLoss[0m : 2.82972
[1mStep[0m  [84/213], [94mLoss[0m : 2.56381
[1mStep[0m  [105/213], [94mLoss[0m : 2.41237
[1mStep[0m  [126/213], [94mLoss[0m : 2.50631
[1mStep[0m  [147/213], [94mLoss[0m : 3.00325
[1mStep[0m  [168/213], [94mLoss[0m : 2.34712
[1mStep[0m  [189/213], [94mLoss[0m : 2.34553
[1mStep[0m  [210/213], [94mLoss[0m : 2.56708

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.542, [92mTest[0m: 2.373, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.48140
[1mStep[0m  [21/213], [94mLoss[0m : 2.36456
[1mStep[0m  [42/213], [94mLoss[0m : 2.43238
[1mStep[0m  [63/213], [94mLoss[0m : 3.01541
[1mStep[0m  [84/213], [94mLoss[0m : 2.35234
[1mStep[0m  [105/213], [94mLoss[0m : 2.57560
[1mStep[0m  [126/213], [94mLoss[0m : 2.51439
[1mStep[0m  [147/213], [94mLoss[0m : 2.34201
[1mStep[0m  [168/213], [94mLoss[0m : 2.57834
[1mStep[0m  [189/213], [94mLoss[0m : 2.45349
[1mStep[0m  [210/213], [94mLoss[0m : 2.74649

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.370, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.83634
[1mStep[0m  [21/213], [94mLoss[0m : 2.06589
[1mStep[0m  [42/213], [94mLoss[0m : 2.29510
[1mStep[0m  [63/213], [94mLoss[0m : 2.53433
[1mStep[0m  [84/213], [94mLoss[0m : 2.52685
[1mStep[0m  [105/213], [94mLoss[0m : 2.59855
[1mStep[0m  [126/213], [94mLoss[0m : 2.34079
[1mStep[0m  [147/213], [94mLoss[0m : 2.56869
[1mStep[0m  [168/213], [94mLoss[0m : 2.47340
[1mStep[0m  [189/213], [94mLoss[0m : 2.38143
[1mStep[0m  [210/213], [94mLoss[0m : 2.19931

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.364, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.49302
[1mStep[0m  [21/213], [94mLoss[0m : 2.18669
[1mStep[0m  [42/213], [94mLoss[0m : 2.53842
[1mStep[0m  [63/213], [94mLoss[0m : 2.39653
[1mStep[0m  [84/213], [94mLoss[0m : 2.76598
[1mStep[0m  [105/213], [94mLoss[0m : 2.79893
[1mStep[0m  [126/213], [94mLoss[0m : 3.14055
[1mStep[0m  [147/213], [94mLoss[0m : 2.48488
[1mStep[0m  [168/213], [94mLoss[0m : 2.32685
[1mStep[0m  [189/213], [94mLoss[0m : 2.79239
[1mStep[0m  [210/213], [94mLoss[0m : 2.09690

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.356, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.43630
[1mStep[0m  [21/213], [94mLoss[0m : 2.63240
[1mStep[0m  [42/213], [94mLoss[0m : 2.76006
[1mStep[0m  [63/213], [94mLoss[0m : 2.16347
[1mStep[0m  [84/213], [94mLoss[0m : 2.85440
[1mStep[0m  [105/213], [94mLoss[0m : 2.94641
[1mStep[0m  [126/213], [94mLoss[0m : 3.02586
[1mStep[0m  [147/213], [94mLoss[0m : 2.58279
[1mStep[0m  [168/213], [94mLoss[0m : 2.43816
[1mStep[0m  [189/213], [94mLoss[0m : 2.31556
[1mStep[0m  [210/213], [94mLoss[0m : 2.49391

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.360, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.82053
[1mStep[0m  [21/213], [94mLoss[0m : 2.45410
[1mStep[0m  [42/213], [94mLoss[0m : 2.34197
[1mStep[0m  [63/213], [94mLoss[0m : 2.51170
[1mStep[0m  [84/213], [94mLoss[0m : 2.56325
[1mStep[0m  [105/213], [94mLoss[0m : 2.46644
[1mStep[0m  [126/213], [94mLoss[0m : 2.76904
[1mStep[0m  [147/213], [94mLoss[0m : 2.65777
[1mStep[0m  [168/213], [94mLoss[0m : 2.56374
[1mStep[0m  [189/213], [94mLoss[0m : 2.15223
[1mStep[0m  [210/213], [94mLoss[0m : 2.27078

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.349, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.49581
[1mStep[0m  [21/213], [94mLoss[0m : 2.45406
[1mStep[0m  [42/213], [94mLoss[0m : 2.13698
[1mStep[0m  [63/213], [94mLoss[0m : 2.03105
[1mStep[0m  [84/213], [94mLoss[0m : 2.35492
[1mStep[0m  [105/213], [94mLoss[0m : 2.20759
[1mStep[0m  [126/213], [94mLoss[0m : 2.36989
[1mStep[0m  [147/213], [94mLoss[0m : 2.45210
[1mStep[0m  [168/213], [94mLoss[0m : 2.30843
[1mStep[0m  [189/213], [94mLoss[0m : 2.67391
[1mStep[0m  [210/213], [94mLoss[0m : 2.28965

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.355, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.31272
[1mStep[0m  [21/213], [94mLoss[0m : 2.28881
[1mStep[0m  [42/213], [94mLoss[0m : 2.23985
[1mStep[0m  [63/213], [94mLoss[0m : 2.53937
[1mStep[0m  [84/213], [94mLoss[0m : 2.45549
[1mStep[0m  [105/213], [94mLoss[0m : 2.44097
[1mStep[0m  [126/213], [94mLoss[0m : 2.26544
[1mStep[0m  [147/213], [94mLoss[0m : 2.51066
[1mStep[0m  [168/213], [94mLoss[0m : 2.36983
[1mStep[0m  [189/213], [94mLoss[0m : 2.79317
[1mStep[0m  [210/213], [94mLoss[0m : 2.98518

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.340, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.43560
[1mStep[0m  [21/213], [94mLoss[0m : 2.50230
[1mStep[0m  [42/213], [94mLoss[0m : 2.68287
[1mStep[0m  [63/213], [94mLoss[0m : 2.41428
[1mStep[0m  [84/213], [94mLoss[0m : 2.27364
[1mStep[0m  [105/213], [94mLoss[0m : 2.35413
[1mStep[0m  [126/213], [94mLoss[0m : 2.22766
[1mStep[0m  [147/213], [94mLoss[0m : 2.71287
[1mStep[0m  [168/213], [94mLoss[0m : 2.15341
[1mStep[0m  [189/213], [94mLoss[0m : 2.60128
[1mStep[0m  [210/213], [94mLoss[0m : 2.16072

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.353, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.40397
[1mStep[0m  [21/213], [94mLoss[0m : 2.26448
[1mStep[0m  [42/213], [94mLoss[0m : 2.30930
[1mStep[0m  [63/213], [94mLoss[0m : 2.37032
[1mStep[0m  [84/213], [94mLoss[0m : 2.51602
[1mStep[0m  [105/213], [94mLoss[0m : 2.49951
[1mStep[0m  [126/213], [94mLoss[0m : 2.31666
[1mStep[0m  [147/213], [94mLoss[0m : 2.38753
[1mStep[0m  [168/213], [94mLoss[0m : 2.93704
[1mStep[0m  [189/213], [94mLoss[0m : 2.65653
[1mStep[0m  [210/213], [94mLoss[0m : 2.82324

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.350, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.45774
[1mStep[0m  [21/213], [94mLoss[0m : 2.18590
[1mStep[0m  [42/213], [94mLoss[0m : 2.72709
[1mStep[0m  [63/213], [94mLoss[0m : 2.00404
[1mStep[0m  [84/213], [94mLoss[0m : 2.37216
[1mStep[0m  [105/213], [94mLoss[0m : 2.60203
[1mStep[0m  [126/213], [94mLoss[0m : 2.27589
[1mStep[0m  [147/213], [94mLoss[0m : 2.57446
[1mStep[0m  [168/213], [94mLoss[0m : 2.03210
[1mStep[0m  [189/213], [94mLoss[0m : 2.26406
[1mStep[0m  [210/213], [94mLoss[0m : 2.64182

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.339, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.13888
[1mStep[0m  [21/213], [94mLoss[0m : 2.27317
[1mStep[0m  [42/213], [94mLoss[0m : 2.07939
[1mStep[0m  [63/213], [94mLoss[0m : 2.37466
[1mStep[0m  [84/213], [94mLoss[0m : 2.86960
[1mStep[0m  [105/213], [94mLoss[0m : 2.45606
[1mStep[0m  [126/213], [94mLoss[0m : 1.98119
[1mStep[0m  [147/213], [94mLoss[0m : 2.86080
[1mStep[0m  [168/213], [94mLoss[0m : 2.27617
[1mStep[0m  [189/213], [94mLoss[0m : 2.17093
[1mStep[0m  [210/213], [94mLoss[0m : 3.03600

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.356, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.36386
[1mStep[0m  [21/213], [94mLoss[0m : 2.13692
[1mStep[0m  [42/213], [94mLoss[0m : 2.88400
[1mStep[0m  [63/213], [94mLoss[0m : 2.43126
[1mStep[0m  [84/213], [94mLoss[0m : 2.94520
[1mStep[0m  [105/213], [94mLoss[0m : 2.11094
[1mStep[0m  [126/213], [94mLoss[0m : 2.67974
[1mStep[0m  [147/213], [94mLoss[0m : 2.72766
[1mStep[0m  [168/213], [94mLoss[0m : 2.45130
[1mStep[0m  [189/213], [94mLoss[0m : 2.36382
[1mStep[0m  [210/213], [94mLoss[0m : 2.64226

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.354, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.48521
[1mStep[0m  [21/213], [94mLoss[0m : 2.62947
[1mStep[0m  [42/213], [94mLoss[0m : 2.05637
[1mStep[0m  [63/213], [94mLoss[0m : 2.45565
[1mStep[0m  [84/213], [94mLoss[0m : 2.58960
[1mStep[0m  [105/213], [94mLoss[0m : 2.93109
[1mStep[0m  [126/213], [94mLoss[0m : 2.45560
[1mStep[0m  [147/213], [94mLoss[0m : 2.64373
[1mStep[0m  [168/213], [94mLoss[0m : 2.50960
[1mStep[0m  [189/213], [94mLoss[0m : 2.52293
[1mStep[0m  [210/213], [94mLoss[0m : 2.39063

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.336, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.39173
[1mStep[0m  [21/213], [94mLoss[0m : 2.52269
[1mStep[0m  [42/213], [94mLoss[0m : 2.04173
[1mStep[0m  [63/213], [94mLoss[0m : 2.35909
[1mStep[0m  [84/213], [94mLoss[0m : 2.50931
[1mStep[0m  [105/213], [94mLoss[0m : 2.57067
[1mStep[0m  [126/213], [94mLoss[0m : 3.07673
[1mStep[0m  [147/213], [94mLoss[0m : 2.34242
[1mStep[0m  [168/213], [94mLoss[0m : 2.54712
[1mStep[0m  [189/213], [94mLoss[0m : 2.31756
[1mStep[0m  [210/213], [94mLoss[0m : 2.55907

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.347, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.63035
[1mStep[0m  [21/213], [94mLoss[0m : 2.46311
[1mStep[0m  [42/213], [94mLoss[0m : 2.44006
[1mStep[0m  [63/213], [94mLoss[0m : 2.28048
[1mStep[0m  [84/213], [94mLoss[0m : 2.57680
[1mStep[0m  [105/213], [94mLoss[0m : 2.38427
[1mStep[0m  [126/213], [94mLoss[0m : 2.16012
[1mStep[0m  [147/213], [94mLoss[0m : 2.39583
[1mStep[0m  [168/213], [94mLoss[0m : 2.46709
[1mStep[0m  [189/213], [94mLoss[0m : 2.36157
[1mStep[0m  [210/213], [94mLoss[0m : 2.76066

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.334, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.18593
[1mStep[0m  [21/213], [94mLoss[0m : 2.84625
[1mStep[0m  [42/213], [94mLoss[0m : 2.70190
[1mStep[0m  [63/213], [94mLoss[0m : 2.55691
[1mStep[0m  [84/213], [94mLoss[0m : 2.78897
[1mStep[0m  [105/213], [94mLoss[0m : 2.13428
[1mStep[0m  [126/213], [94mLoss[0m : 2.38763
[1mStep[0m  [147/213], [94mLoss[0m : 2.34134
[1mStep[0m  [168/213], [94mLoss[0m : 2.11356
[1mStep[0m  [189/213], [94mLoss[0m : 2.45883
[1mStep[0m  [210/213], [94mLoss[0m : 2.45060

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.326, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.07173
[1mStep[0m  [21/213], [94mLoss[0m : 2.80608
[1mStep[0m  [42/213], [94mLoss[0m : 2.43639
[1mStep[0m  [63/213], [94mLoss[0m : 2.45481
[1mStep[0m  [84/213], [94mLoss[0m : 2.63046
[1mStep[0m  [105/213], [94mLoss[0m : 2.57705
[1mStep[0m  [126/213], [94mLoss[0m : 2.19031
[1mStep[0m  [147/213], [94mLoss[0m : 2.47227
[1mStep[0m  [168/213], [94mLoss[0m : 2.53943
[1mStep[0m  [189/213], [94mLoss[0m : 2.07233
[1mStep[0m  [210/213], [94mLoss[0m : 2.78426

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.340, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.65479
[1mStep[0m  [21/213], [94mLoss[0m : 2.45023
[1mStep[0m  [42/213], [94mLoss[0m : 2.34799
[1mStep[0m  [63/213], [94mLoss[0m : 2.73174
[1mStep[0m  [84/213], [94mLoss[0m : 2.47445
[1mStep[0m  [105/213], [94mLoss[0m : 2.08244
[1mStep[0m  [126/213], [94mLoss[0m : 2.41140
[1mStep[0m  [147/213], [94mLoss[0m : 2.11560
[1mStep[0m  [168/213], [94mLoss[0m : 2.36007
[1mStep[0m  [189/213], [94mLoss[0m : 2.46448
[1mStep[0m  [210/213], [94mLoss[0m : 2.67323

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.330, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.80878
[1mStep[0m  [21/213], [94mLoss[0m : 2.65644
[1mStep[0m  [42/213], [94mLoss[0m : 2.30235
[1mStep[0m  [63/213], [94mLoss[0m : 2.34524
[1mStep[0m  [84/213], [94mLoss[0m : 2.66808
[1mStep[0m  [105/213], [94mLoss[0m : 2.28536
[1mStep[0m  [126/213], [94mLoss[0m : 2.32676
[1mStep[0m  [147/213], [94mLoss[0m : 2.42134
[1mStep[0m  [168/213], [94mLoss[0m : 2.06482
[1mStep[0m  [189/213], [94mLoss[0m : 2.56665
[1mStep[0m  [210/213], [94mLoss[0m : 2.96294

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.324, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.333
====================================

Phase 1 - Evaluation MAE:  2.332536481461435
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/213], [94mLoss[0m : 2.53727
[1mStep[0m  [21/213], [94mLoss[0m : 1.75747
[1mStep[0m  [42/213], [94mLoss[0m : 2.45608
[1mStep[0m  [63/213], [94mLoss[0m : 2.24041
[1mStep[0m  [84/213], [94mLoss[0m : 2.67823
[1mStep[0m  [105/213], [94mLoss[0m : 2.43958
[1mStep[0m  [126/213], [94mLoss[0m : 2.22133
[1mStep[0m  [147/213], [94mLoss[0m : 2.58697
[1mStep[0m  [168/213], [94mLoss[0m : 2.74404
[1mStep[0m  [189/213], [94mLoss[0m : 2.74427
[1mStep[0m  [210/213], [94mLoss[0m : 2.64721

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.335, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.00746
[1mStep[0m  [21/213], [94mLoss[0m : 2.54050
[1mStep[0m  [42/213], [94mLoss[0m : 2.42243
[1mStep[0m  [63/213], [94mLoss[0m : 2.34476
[1mStep[0m  [84/213], [94mLoss[0m : 2.32247
[1mStep[0m  [105/213], [94mLoss[0m : 2.70998
[1mStep[0m  [126/213], [94mLoss[0m : 2.18644
[1mStep[0m  [147/213], [94mLoss[0m : 2.36101
[1mStep[0m  [168/213], [94mLoss[0m : 2.23426
[1mStep[0m  [189/213], [94mLoss[0m : 2.15482
[1mStep[0m  [210/213], [94mLoss[0m : 2.13648

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.713, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.44240
[1mStep[0m  [21/213], [94mLoss[0m : 2.32493
[1mStep[0m  [42/213], [94mLoss[0m : 2.50676
[1mStep[0m  [63/213], [94mLoss[0m : 2.61657
[1mStep[0m  [84/213], [94mLoss[0m : 2.67118
[1mStep[0m  [105/213], [94mLoss[0m : 2.00601
[1mStep[0m  [126/213], [94mLoss[0m : 2.01832
[1mStep[0m  [147/213], [94mLoss[0m : 2.08197
[1mStep[0m  [168/213], [94mLoss[0m : 2.48992
[1mStep[0m  [189/213], [94mLoss[0m : 2.95729
[1mStep[0m  [210/213], [94mLoss[0m : 2.15528

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.365, [92mTest[0m: 2.580, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.98724
[1mStep[0m  [21/213], [94mLoss[0m : 2.21052
[1mStep[0m  [42/213], [94mLoss[0m : 1.98465
[1mStep[0m  [63/213], [94mLoss[0m : 2.29619
[1mStep[0m  [84/213], [94mLoss[0m : 2.46657
[1mStep[0m  [105/213], [94mLoss[0m : 2.26011
[1mStep[0m  [126/213], [94mLoss[0m : 2.12418
[1mStep[0m  [147/213], [94mLoss[0m : 2.39217
[1mStep[0m  [168/213], [94mLoss[0m : 2.06016
[1mStep[0m  [189/213], [94mLoss[0m : 2.55055
[1mStep[0m  [210/213], [94mLoss[0m : 2.32769

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.284, [92mTest[0m: 2.439, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.53351
[1mStep[0m  [21/213], [94mLoss[0m : 1.88172
[1mStep[0m  [42/213], [94mLoss[0m : 2.30196
[1mStep[0m  [63/213], [94mLoss[0m : 2.24046
[1mStep[0m  [84/213], [94mLoss[0m : 2.10968
[1mStep[0m  [105/213], [94mLoss[0m : 2.21962
[1mStep[0m  [126/213], [94mLoss[0m : 1.95573
[1mStep[0m  [147/213], [94mLoss[0m : 2.80916
[1mStep[0m  [168/213], [94mLoss[0m : 2.28515
[1mStep[0m  [189/213], [94mLoss[0m : 2.35475
[1mStep[0m  [210/213], [94mLoss[0m : 1.82021

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.240, [92mTest[0m: 2.405, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.21538
[1mStep[0m  [21/213], [94mLoss[0m : 2.13479
[1mStep[0m  [42/213], [94mLoss[0m : 2.30235
[1mStep[0m  [63/213], [94mLoss[0m : 2.02271
[1mStep[0m  [84/213], [94mLoss[0m : 2.41773
[1mStep[0m  [105/213], [94mLoss[0m : 2.18596
[1mStep[0m  [126/213], [94mLoss[0m : 1.78501
[1mStep[0m  [147/213], [94mLoss[0m : 2.04119
[1mStep[0m  [168/213], [94mLoss[0m : 2.39887
[1mStep[0m  [189/213], [94mLoss[0m : 2.67901
[1mStep[0m  [210/213], [94mLoss[0m : 2.38397

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.200, [92mTest[0m: 2.407, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.24508
[1mStep[0m  [21/213], [94mLoss[0m : 2.23045
[1mStep[0m  [42/213], [94mLoss[0m : 1.84106
[1mStep[0m  [63/213], [94mLoss[0m : 2.67090
[1mStep[0m  [84/213], [94mLoss[0m : 2.67887
[1mStep[0m  [105/213], [94mLoss[0m : 1.96241
[1mStep[0m  [126/213], [94mLoss[0m : 2.36162
[1mStep[0m  [147/213], [94mLoss[0m : 2.00250
[1mStep[0m  [168/213], [94mLoss[0m : 2.34849
[1mStep[0m  [189/213], [94mLoss[0m : 2.33288
[1mStep[0m  [210/213], [94mLoss[0m : 2.73266

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.146, [92mTest[0m: 2.432, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.02978
[1mStep[0m  [21/213], [94mLoss[0m : 2.43736
[1mStep[0m  [42/213], [94mLoss[0m : 2.07906
[1mStep[0m  [63/213], [94mLoss[0m : 2.27629
[1mStep[0m  [84/213], [94mLoss[0m : 2.22684
[1mStep[0m  [105/213], [94mLoss[0m : 1.83590
[1mStep[0m  [126/213], [94mLoss[0m : 2.27166
[1mStep[0m  [147/213], [94mLoss[0m : 2.07021
[1mStep[0m  [168/213], [94mLoss[0m : 2.05190
[1mStep[0m  [189/213], [94mLoss[0m : 2.26190
[1mStep[0m  [210/213], [94mLoss[0m : 2.25751

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.076, [92mTest[0m: 2.412, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.82046
[1mStep[0m  [21/213], [94mLoss[0m : 1.68318
[1mStep[0m  [42/213], [94mLoss[0m : 1.70083
[1mStep[0m  [63/213], [94mLoss[0m : 2.00975
[1mStep[0m  [84/213], [94mLoss[0m : 2.20895
[1mStep[0m  [105/213], [94mLoss[0m : 2.17124
[1mStep[0m  [126/213], [94mLoss[0m : 2.13415
[1mStep[0m  [147/213], [94mLoss[0m : 2.42317
[1mStep[0m  [168/213], [94mLoss[0m : 2.28081
[1mStep[0m  [189/213], [94mLoss[0m : 2.25063
[1mStep[0m  [210/213], [94mLoss[0m : 1.95699

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.045, [92mTest[0m: 2.417, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.87804
[1mStep[0m  [21/213], [94mLoss[0m : 2.14914
[1mStep[0m  [42/213], [94mLoss[0m : 2.01295
[1mStep[0m  [63/213], [94mLoss[0m : 2.41919
[1mStep[0m  [84/213], [94mLoss[0m : 1.83367
[1mStep[0m  [105/213], [94mLoss[0m : 2.05842
[1mStep[0m  [126/213], [94mLoss[0m : 2.06049
[1mStep[0m  [147/213], [94mLoss[0m : 1.95083
[1mStep[0m  [168/213], [94mLoss[0m : 2.13307
[1mStep[0m  [189/213], [94mLoss[0m : 2.09862
[1mStep[0m  [210/213], [94mLoss[0m : 2.08192

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.013, [92mTest[0m: 2.411, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.83982
[1mStep[0m  [21/213], [94mLoss[0m : 1.97536
[1mStep[0m  [42/213], [94mLoss[0m : 2.28662
[1mStep[0m  [63/213], [94mLoss[0m : 1.77008
[1mStep[0m  [84/213], [94mLoss[0m : 1.57570
[1mStep[0m  [105/213], [94mLoss[0m : 1.75638
[1mStep[0m  [126/213], [94mLoss[0m : 1.92500
[1mStep[0m  [147/213], [94mLoss[0m : 2.08064
[1mStep[0m  [168/213], [94mLoss[0m : 1.91977
[1mStep[0m  [189/213], [94mLoss[0m : 2.23293
[1mStep[0m  [210/213], [94mLoss[0m : 2.07727

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.965, [92mTest[0m: 2.432, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.77660
[1mStep[0m  [21/213], [94mLoss[0m : 1.99140
[1mStep[0m  [42/213], [94mLoss[0m : 2.10187
[1mStep[0m  [63/213], [94mLoss[0m : 1.65261
[1mStep[0m  [84/213], [94mLoss[0m : 1.77823
[1mStep[0m  [105/213], [94mLoss[0m : 1.74706
[1mStep[0m  [126/213], [94mLoss[0m : 1.85522
[1mStep[0m  [147/213], [94mLoss[0m : 1.76997
[1mStep[0m  [168/213], [94mLoss[0m : 1.63115
[1mStep[0m  [189/213], [94mLoss[0m : 1.86114
[1mStep[0m  [210/213], [94mLoss[0m : 1.70042

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.908, [92mTest[0m: 2.420, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.93302
[1mStep[0m  [21/213], [94mLoss[0m : 1.83017
[1mStep[0m  [42/213], [94mLoss[0m : 1.99305
[1mStep[0m  [63/213], [94mLoss[0m : 2.09239
[1mStep[0m  [84/213], [94mLoss[0m : 1.86402
[1mStep[0m  [105/213], [94mLoss[0m : 2.15460
[1mStep[0m  [126/213], [94mLoss[0m : 2.13750
[1mStep[0m  [147/213], [94mLoss[0m : 1.94542
[1mStep[0m  [168/213], [94mLoss[0m : 1.93304
[1mStep[0m  [189/213], [94mLoss[0m : 1.59450
[1mStep[0m  [210/213], [94mLoss[0m : 1.91781

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.873, [92mTest[0m: 2.446, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.67141
[1mStep[0m  [21/213], [94mLoss[0m : 2.06574
[1mStep[0m  [42/213], [94mLoss[0m : 1.58052
[1mStep[0m  [63/213], [94mLoss[0m : 2.08501
[1mStep[0m  [84/213], [94mLoss[0m : 1.49704
[1mStep[0m  [105/213], [94mLoss[0m : 2.20596
[1mStep[0m  [126/213], [94mLoss[0m : 1.75684
[1mStep[0m  [147/213], [94mLoss[0m : 1.90876
[1mStep[0m  [168/213], [94mLoss[0m : 1.59114
[1mStep[0m  [189/213], [94mLoss[0m : 1.65943
[1mStep[0m  [210/213], [94mLoss[0m : 1.74872

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.845, [92mTest[0m: 2.460, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.71934
[1mStep[0m  [21/213], [94mLoss[0m : 1.51493
[1mStep[0m  [42/213], [94mLoss[0m : 1.60085
[1mStep[0m  [63/213], [94mLoss[0m : 1.91301
[1mStep[0m  [84/213], [94mLoss[0m : 1.45736
[1mStep[0m  [105/213], [94mLoss[0m : 1.65696
[1mStep[0m  [126/213], [94mLoss[0m : 1.96161
[1mStep[0m  [147/213], [94mLoss[0m : 1.87222
[1mStep[0m  [168/213], [94mLoss[0m : 1.73950
[1mStep[0m  [189/213], [94mLoss[0m : 1.48066
[1mStep[0m  [210/213], [94mLoss[0m : 1.80824

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.810, [92mTest[0m: 2.406, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.63631
[1mStep[0m  [21/213], [94mLoss[0m : 1.69460
[1mStep[0m  [42/213], [94mLoss[0m : 1.77674
[1mStep[0m  [63/213], [94mLoss[0m : 1.83693
[1mStep[0m  [84/213], [94mLoss[0m : 1.49807
[1mStep[0m  [105/213], [94mLoss[0m : 2.12128
[1mStep[0m  [126/213], [94mLoss[0m : 2.26601
[1mStep[0m  [147/213], [94mLoss[0m : 1.64495
[1mStep[0m  [168/213], [94mLoss[0m : 1.63577
[1mStep[0m  [189/213], [94mLoss[0m : 2.03717
[1mStep[0m  [210/213], [94mLoss[0m : 1.80291

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.772, [92mTest[0m: 2.426, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.65673
[1mStep[0m  [21/213], [94mLoss[0m : 1.74003
[1mStep[0m  [42/213], [94mLoss[0m : 1.58122
[1mStep[0m  [63/213], [94mLoss[0m : 1.98528
[1mStep[0m  [84/213], [94mLoss[0m : 1.82933
[1mStep[0m  [105/213], [94mLoss[0m : 1.61762
[1mStep[0m  [126/213], [94mLoss[0m : 1.78675
[1mStep[0m  [147/213], [94mLoss[0m : 1.63034
[1mStep[0m  [168/213], [94mLoss[0m : 1.94793
[1mStep[0m  [189/213], [94mLoss[0m : 1.51680
[1mStep[0m  [210/213], [94mLoss[0m : 1.72659

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.741, [92mTest[0m: 2.423, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.80867
[1mStep[0m  [21/213], [94mLoss[0m : 1.77054
[1mStep[0m  [42/213], [94mLoss[0m : 1.72461
[1mStep[0m  [63/213], [94mLoss[0m : 1.64260
[1mStep[0m  [84/213], [94mLoss[0m : 1.40290
[1mStep[0m  [105/213], [94mLoss[0m : 1.74952
[1mStep[0m  [126/213], [94mLoss[0m : 1.40486
[1mStep[0m  [147/213], [94mLoss[0m : 2.24951
[1mStep[0m  [168/213], [94mLoss[0m : 1.91041
[1mStep[0m  [189/213], [94mLoss[0m : 2.01152
[1mStep[0m  [210/213], [94mLoss[0m : 1.69114

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.708, [92mTest[0m: 2.418, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.37107
[1mStep[0m  [21/213], [94mLoss[0m : 1.64000
[1mStep[0m  [42/213], [94mLoss[0m : 1.72385
[1mStep[0m  [63/213], [94mLoss[0m : 1.57292
[1mStep[0m  [84/213], [94mLoss[0m : 1.70270
[1mStep[0m  [105/213], [94mLoss[0m : 1.43717
[1mStep[0m  [126/213], [94mLoss[0m : 1.83794
[1mStep[0m  [147/213], [94mLoss[0m : 1.58698
[1mStep[0m  [168/213], [94mLoss[0m : 2.04577
[1mStep[0m  [189/213], [94mLoss[0m : 1.98032
[1mStep[0m  [210/213], [94mLoss[0m : 2.14841

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.685, [92mTest[0m: 2.452, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.33809
[1mStep[0m  [21/213], [94mLoss[0m : 1.64429
[1mStep[0m  [42/213], [94mLoss[0m : 1.23217
[1mStep[0m  [63/213], [94mLoss[0m : 1.97934
[1mStep[0m  [84/213], [94mLoss[0m : 1.65580
[1mStep[0m  [105/213], [94mLoss[0m : 1.50542
[1mStep[0m  [126/213], [94mLoss[0m : 1.67693
[1mStep[0m  [147/213], [94mLoss[0m : 1.52138
[1mStep[0m  [168/213], [94mLoss[0m : 1.74967
[1mStep[0m  [189/213], [94mLoss[0m : 2.32580
[1mStep[0m  [210/213], [94mLoss[0m : 1.45369

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.636, [92mTest[0m: 2.446, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.56938
[1mStep[0m  [21/213], [94mLoss[0m : 1.36258
[1mStep[0m  [42/213], [94mLoss[0m : 1.42659
[1mStep[0m  [63/213], [94mLoss[0m : 1.69447
[1mStep[0m  [84/213], [94mLoss[0m : 1.52042
[1mStep[0m  [105/213], [94mLoss[0m : 1.32679
[1mStep[0m  [126/213], [94mLoss[0m : 1.91748
[1mStep[0m  [147/213], [94mLoss[0m : 1.86197
[1mStep[0m  [168/213], [94mLoss[0m : 1.83750
[1mStep[0m  [189/213], [94mLoss[0m : 1.65370
[1mStep[0m  [210/213], [94mLoss[0m : 1.21636

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.620, [92mTest[0m: 2.470, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.66954
[1mStep[0m  [21/213], [94mLoss[0m : 1.63065
[1mStep[0m  [42/213], [94mLoss[0m : 1.58232
[1mStep[0m  [63/213], [94mLoss[0m : 1.46751
[1mStep[0m  [84/213], [94mLoss[0m : 1.39913
[1mStep[0m  [105/213], [94mLoss[0m : 1.66614
[1mStep[0m  [126/213], [94mLoss[0m : 1.06961
[1mStep[0m  [147/213], [94mLoss[0m : 1.65148
[1mStep[0m  [168/213], [94mLoss[0m : 1.50933
[1mStep[0m  [189/213], [94mLoss[0m : 1.82213
[1mStep[0m  [210/213], [94mLoss[0m : 1.67848

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.589, [92mTest[0m: 2.425, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.85030
[1mStep[0m  [21/213], [94mLoss[0m : 1.58328
[1mStep[0m  [42/213], [94mLoss[0m : 1.49167
[1mStep[0m  [63/213], [94mLoss[0m : 1.49315
[1mStep[0m  [84/213], [94mLoss[0m : 1.34435
[1mStep[0m  [105/213], [94mLoss[0m : 1.40008
[1mStep[0m  [126/213], [94mLoss[0m : 1.56552
[1mStep[0m  [147/213], [94mLoss[0m : 1.45174
[1mStep[0m  [168/213], [94mLoss[0m : 1.71628
[1mStep[0m  [189/213], [94mLoss[0m : 1.43557
[1mStep[0m  [210/213], [94mLoss[0m : 1.87945

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.575, [92mTest[0m: 2.436, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.61539
[1mStep[0m  [21/213], [94mLoss[0m : 1.47284
[1mStep[0m  [42/213], [94mLoss[0m : 1.70753
[1mStep[0m  [63/213], [94mLoss[0m : 1.42188
[1mStep[0m  [84/213], [94mLoss[0m : 1.82982
[1mStep[0m  [105/213], [94mLoss[0m : 1.29633
[1mStep[0m  [126/213], [94mLoss[0m : 1.35225
[1mStep[0m  [147/213], [94mLoss[0m : 1.65087
[1mStep[0m  [168/213], [94mLoss[0m : 1.66356
[1mStep[0m  [189/213], [94mLoss[0m : 1.49890
[1mStep[0m  [210/213], [94mLoss[0m : 1.54469

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.543, [92mTest[0m: 2.487, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.69726
[1mStep[0m  [21/213], [94mLoss[0m : 1.23245
[1mStep[0m  [42/213], [94mLoss[0m : 1.68427
[1mStep[0m  [63/213], [94mLoss[0m : 1.28508
[1mStep[0m  [84/213], [94mLoss[0m : 1.59407
[1mStep[0m  [105/213], [94mLoss[0m : 1.58461
[1mStep[0m  [126/213], [94mLoss[0m : 1.51335
[1mStep[0m  [147/213], [94mLoss[0m : 1.28497
[1mStep[0m  [168/213], [94mLoss[0m : 1.44553
[1mStep[0m  [189/213], [94mLoss[0m : 1.51277
[1mStep[0m  [210/213], [94mLoss[0m : 1.60098

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.520, [92mTest[0m: 2.431, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.53887
[1mStep[0m  [21/213], [94mLoss[0m : 1.32149
[1mStep[0m  [42/213], [94mLoss[0m : 1.20553
[1mStep[0m  [63/213], [94mLoss[0m : 1.60602
[1mStep[0m  [84/213], [94mLoss[0m : 1.34611
[1mStep[0m  [105/213], [94mLoss[0m : 1.51882
[1mStep[0m  [126/213], [94mLoss[0m : 1.39652
[1mStep[0m  [147/213], [94mLoss[0m : 1.59009
[1mStep[0m  [168/213], [94mLoss[0m : 1.37912
[1mStep[0m  [189/213], [94mLoss[0m : 1.51227
[1mStep[0m  [210/213], [94mLoss[0m : 1.40006

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.507, [92mTest[0m: 2.426, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.53207
[1mStep[0m  [21/213], [94mLoss[0m : 1.33470
[1mStep[0m  [42/213], [94mLoss[0m : 1.91312
[1mStep[0m  [63/213], [94mLoss[0m : 1.38272
[1mStep[0m  [84/213], [94mLoss[0m : 1.61899
[1mStep[0m  [105/213], [94mLoss[0m : 1.46005
[1mStep[0m  [126/213], [94mLoss[0m : 1.38109
[1mStep[0m  [147/213], [94mLoss[0m : 1.40351
[1mStep[0m  [168/213], [94mLoss[0m : 1.39849
[1mStep[0m  [189/213], [94mLoss[0m : 1.38446
[1mStep[0m  [210/213], [94mLoss[0m : 1.55650

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.479, [92mTest[0m: 2.454, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.41151
[1mStep[0m  [21/213], [94mLoss[0m : 1.73961
[1mStep[0m  [42/213], [94mLoss[0m : 1.51564
[1mStep[0m  [63/213], [94mLoss[0m : 1.72412
[1mStep[0m  [84/213], [94mLoss[0m : 1.21326
[1mStep[0m  [105/213], [94mLoss[0m : 1.44512
[1mStep[0m  [126/213], [94mLoss[0m : 1.27751
[1mStep[0m  [147/213], [94mLoss[0m : 1.66531
[1mStep[0m  [168/213], [94mLoss[0m : 1.39222
[1mStep[0m  [189/213], [94mLoss[0m : 1.35077
[1mStep[0m  [210/213], [94mLoss[0m : 1.48483

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.464, [92mTest[0m: 2.440, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.39060
[1mStep[0m  [21/213], [94mLoss[0m : 1.68726
[1mStep[0m  [42/213], [94mLoss[0m : 1.62097
[1mStep[0m  [63/213], [94mLoss[0m : 1.39814
[1mStep[0m  [84/213], [94mLoss[0m : 1.77395
[1mStep[0m  [105/213], [94mLoss[0m : 1.37625
[1mStep[0m  [126/213], [94mLoss[0m : 1.56911
[1mStep[0m  [147/213], [94mLoss[0m : 1.33697
[1mStep[0m  [168/213], [94mLoss[0m : 1.48349
[1mStep[0m  [189/213], [94mLoss[0m : 1.35724
[1mStep[0m  [210/213], [94mLoss[0m : 1.43878

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.470, [92mTest[0m: 2.459, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.56776
[1mStep[0m  [21/213], [94mLoss[0m : 1.45920
[1mStep[0m  [42/213], [94mLoss[0m : 1.51117
[1mStep[0m  [63/213], [94mLoss[0m : 1.44996
[1mStep[0m  [84/213], [94mLoss[0m : 1.53882
[1mStep[0m  [105/213], [94mLoss[0m : 1.32835
[1mStep[0m  [126/213], [94mLoss[0m : 1.59172
[1mStep[0m  [147/213], [94mLoss[0m : 1.31727
[1mStep[0m  [168/213], [94mLoss[0m : 1.71809
[1mStep[0m  [189/213], [94mLoss[0m : 1.18424
[1mStep[0m  [210/213], [94mLoss[0m : 1.49047

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.444, [92mTest[0m: 2.520, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.451
====================================

Phase 2 - Evaluation MAE:  2.451430506301376
MAE score P1      2.332536
MAE score P2      2.451431
loss              1.444256
learning_rate      0.00505
batch_size              64
hidden_sizes         [100]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.4
momentum               0.5
weight_decay        0.0001
Name: 11, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/106], [94mLoss[0m : 10.87222
[1mStep[0m  [10/106], [94mLoss[0m : 6.85645
[1mStep[0m  [20/106], [94mLoss[0m : 4.34023
[1mStep[0m  [30/106], [94mLoss[0m : 2.73432
[1mStep[0m  [40/106], [94mLoss[0m : 2.54560
[1mStep[0m  [50/106], [94mLoss[0m : 2.50997
[1mStep[0m  [60/106], [94mLoss[0m : 2.64178
[1mStep[0m  [70/106], [94mLoss[0m : 2.66401
[1mStep[0m  [80/106], [94mLoss[0m : 2.74903
[1mStep[0m  [90/106], [94mLoss[0m : 2.91897
[1mStep[0m  [100/106], [94mLoss[0m : 2.59845

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.554, [92mTest[0m: 10.909, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.42698
[1mStep[0m  [10/106], [94mLoss[0m : 2.42820
[1mStep[0m  [20/106], [94mLoss[0m : 2.61357
[1mStep[0m  [30/106], [94mLoss[0m : 2.79106
[1mStep[0m  [40/106], [94mLoss[0m : 2.38899
[1mStep[0m  [50/106], [94mLoss[0m : 2.69842
[1mStep[0m  [60/106], [94mLoss[0m : 2.45923
[1mStep[0m  [70/106], [94mLoss[0m : 2.38322
[1mStep[0m  [80/106], [94mLoss[0m : 2.19930
[1mStep[0m  [90/106], [94mLoss[0m : 2.73276
[1mStep[0m  [100/106], [94mLoss[0m : 2.72274

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.849, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.37173
[1mStep[0m  [10/106], [94mLoss[0m : 2.56603
[1mStep[0m  [20/106], [94mLoss[0m : 2.31681
[1mStep[0m  [30/106], [94mLoss[0m : 2.39866
[1mStep[0m  [40/106], [94mLoss[0m : 2.62040
[1mStep[0m  [50/106], [94mLoss[0m : 2.38804
[1mStep[0m  [60/106], [94mLoss[0m : 2.48925
[1mStep[0m  [70/106], [94mLoss[0m : 2.35067
[1mStep[0m  [80/106], [94mLoss[0m : 2.41784
[1mStep[0m  [90/106], [94mLoss[0m : 2.43995
[1mStep[0m  [100/106], [94mLoss[0m : 2.52650

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.634, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.36237
[1mStep[0m  [10/106], [94mLoss[0m : 2.74935
[1mStep[0m  [20/106], [94mLoss[0m : 2.44344
[1mStep[0m  [30/106], [94mLoss[0m : 2.52451
[1mStep[0m  [40/106], [94mLoss[0m : 2.27640
[1mStep[0m  [50/106], [94mLoss[0m : 2.63079
[1mStep[0m  [60/106], [94mLoss[0m : 2.49140
[1mStep[0m  [70/106], [94mLoss[0m : 2.35294
[1mStep[0m  [80/106], [94mLoss[0m : 2.34844
[1mStep[0m  [90/106], [94mLoss[0m : 2.62287
[1mStep[0m  [100/106], [94mLoss[0m : 2.41935

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.624, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.75323
[1mStep[0m  [10/106], [94mLoss[0m : 2.35163
[1mStep[0m  [20/106], [94mLoss[0m : 2.53753
[1mStep[0m  [30/106], [94mLoss[0m : 2.62167
[1mStep[0m  [40/106], [94mLoss[0m : 2.17152
[1mStep[0m  [50/106], [94mLoss[0m : 2.62256
[1mStep[0m  [60/106], [94mLoss[0m : 2.55344
[1mStep[0m  [70/106], [94mLoss[0m : 2.47202
[1mStep[0m  [80/106], [94mLoss[0m : 2.87709
[1mStep[0m  [90/106], [94mLoss[0m : 2.26250
[1mStep[0m  [100/106], [94mLoss[0m : 2.34211

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.583, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.36715
[1mStep[0m  [10/106], [94mLoss[0m : 2.70266
[1mStep[0m  [20/106], [94mLoss[0m : 2.14363
[1mStep[0m  [30/106], [94mLoss[0m : 2.27975
[1mStep[0m  [40/106], [94mLoss[0m : 2.44244
[1mStep[0m  [50/106], [94mLoss[0m : 2.74597
[1mStep[0m  [60/106], [94mLoss[0m : 2.52772
[1mStep[0m  [70/106], [94mLoss[0m : 2.61934
[1mStep[0m  [80/106], [94mLoss[0m : 2.39870
[1mStep[0m  [90/106], [94mLoss[0m : 2.37619
[1mStep[0m  [100/106], [94mLoss[0m : 2.09985

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.579, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.54287
[1mStep[0m  [10/106], [94mLoss[0m : 2.30625
[1mStep[0m  [20/106], [94mLoss[0m : 2.88849
[1mStep[0m  [30/106], [94mLoss[0m : 2.50009
[1mStep[0m  [40/106], [94mLoss[0m : 2.49345
[1mStep[0m  [50/106], [94mLoss[0m : 2.37687
[1mStep[0m  [60/106], [94mLoss[0m : 2.49184
[1mStep[0m  [70/106], [94mLoss[0m : 2.16476
[1mStep[0m  [80/106], [94mLoss[0m : 2.29349
[1mStep[0m  [90/106], [94mLoss[0m : 2.63929
[1mStep[0m  [100/106], [94mLoss[0m : 2.22937

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.527, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.53605
[1mStep[0m  [10/106], [94mLoss[0m : 2.35081
[1mStep[0m  [20/106], [94mLoss[0m : 2.45545
[1mStep[0m  [30/106], [94mLoss[0m : 2.40160
[1mStep[0m  [40/106], [94mLoss[0m : 2.70749
[1mStep[0m  [50/106], [94mLoss[0m : 2.31925
[1mStep[0m  [60/106], [94mLoss[0m : 2.64149
[1mStep[0m  [70/106], [94mLoss[0m : 2.52105
[1mStep[0m  [80/106], [94mLoss[0m : 2.37013
[1mStep[0m  [90/106], [94mLoss[0m : 2.60450
[1mStep[0m  [100/106], [94mLoss[0m : 2.24208

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.565, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.40072
[1mStep[0m  [10/106], [94mLoss[0m : 1.98062
[1mStep[0m  [20/106], [94mLoss[0m : 2.23345
[1mStep[0m  [30/106], [94mLoss[0m : 2.27028
[1mStep[0m  [40/106], [94mLoss[0m : 2.32923
[1mStep[0m  [50/106], [94mLoss[0m : 2.46152
[1mStep[0m  [60/106], [94mLoss[0m : 2.60824
[1mStep[0m  [70/106], [94mLoss[0m : 2.14189
[1mStep[0m  [80/106], [94mLoss[0m : 2.09855
[1mStep[0m  [90/106], [94mLoss[0m : 2.50013
[1mStep[0m  [100/106], [94mLoss[0m : 2.19968

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.560, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.28458
[1mStep[0m  [10/106], [94mLoss[0m : 2.08456
[1mStep[0m  [20/106], [94mLoss[0m : 2.38344
[1mStep[0m  [30/106], [94mLoss[0m : 2.45505
[1mStep[0m  [40/106], [94mLoss[0m : 2.54413
[1mStep[0m  [50/106], [94mLoss[0m : 2.34121
[1mStep[0m  [60/106], [94mLoss[0m : 2.06576
[1mStep[0m  [70/106], [94mLoss[0m : 2.20545
[1mStep[0m  [80/106], [94mLoss[0m : 2.35598
[1mStep[0m  [90/106], [94mLoss[0m : 2.21462
[1mStep[0m  [100/106], [94mLoss[0m : 2.15248

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.390, [92mTest[0m: 2.541, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.43284
[1mStep[0m  [10/106], [94mLoss[0m : 2.19659
[1mStep[0m  [20/106], [94mLoss[0m : 2.65947
[1mStep[0m  [30/106], [94mLoss[0m : 2.40201
[1mStep[0m  [40/106], [94mLoss[0m : 2.57786
[1mStep[0m  [50/106], [94mLoss[0m : 2.47583
[1mStep[0m  [60/106], [94mLoss[0m : 2.48327
[1mStep[0m  [70/106], [94mLoss[0m : 2.36102
[1mStep[0m  [80/106], [94mLoss[0m : 2.21113
[1mStep[0m  [90/106], [94mLoss[0m : 2.47930
[1mStep[0m  [100/106], [94mLoss[0m : 2.21111

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.392, [92mTest[0m: 2.523, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.28923
[1mStep[0m  [10/106], [94mLoss[0m : 2.69694
[1mStep[0m  [20/106], [94mLoss[0m : 2.28914
[1mStep[0m  [30/106], [94mLoss[0m : 2.50765
[1mStep[0m  [40/106], [94mLoss[0m : 2.33800
[1mStep[0m  [50/106], [94mLoss[0m : 2.38622
[1mStep[0m  [60/106], [94mLoss[0m : 2.07001
[1mStep[0m  [70/106], [94mLoss[0m : 2.54683
[1mStep[0m  [80/106], [94mLoss[0m : 2.61129
[1mStep[0m  [90/106], [94mLoss[0m : 2.16977
[1mStep[0m  [100/106], [94mLoss[0m : 2.50085

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.393, [92mTest[0m: 2.504, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.10015
[1mStep[0m  [10/106], [94mLoss[0m : 2.81949
[1mStep[0m  [20/106], [94mLoss[0m : 2.39291
[1mStep[0m  [30/106], [94mLoss[0m : 2.19381
[1mStep[0m  [40/106], [94mLoss[0m : 2.41953
[1mStep[0m  [50/106], [94mLoss[0m : 2.37731
[1mStep[0m  [60/106], [94mLoss[0m : 2.21429
[1mStep[0m  [70/106], [94mLoss[0m : 2.48920
[1mStep[0m  [80/106], [94mLoss[0m : 2.47296
[1mStep[0m  [90/106], [94mLoss[0m : 1.99784
[1mStep[0m  [100/106], [94mLoss[0m : 2.41036

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.380, [92mTest[0m: 2.521, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.33206
[1mStep[0m  [10/106], [94mLoss[0m : 2.04273
[1mStep[0m  [20/106], [94mLoss[0m : 2.39865
[1mStep[0m  [30/106], [94mLoss[0m : 2.00846
[1mStep[0m  [40/106], [94mLoss[0m : 2.31593
[1mStep[0m  [50/106], [94mLoss[0m : 2.42442
[1mStep[0m  [60/106], [94mLoss[0m : 2.36588
[1mStep[0m  [70/106], [94mLoss[0m : 2.49375
[1mStep[0m  [80/106], [94mLoss[0m : 2.86637
[1mStep[0m  [90/106], [94mLoss[0m : 2.48265
[1mStep[0m  [100/106], [94mLoss[0m : 2.51667

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.385, [92mTest[0m: 2.488, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.36925
[1mStep[0m  [10/106], [94mLoss[0m : 2.32676
[1mStep[0m  [20/106], [94mLoss[0m : 2.40545
[1mStep[0m  [30/106], [94mLoss[0m : 2.08826
[1mStep[0m  [40/106], [94mLoss[0m : 2.36618
[1mStep[0m  [50/106], [94mLoss[0m : 2.14070
[1mStep[0m  [60/106], [94mLoss[0m : 1.99069
[1mStep[0m  [70/106], [94mLoss[0m : 2.63584
[1mStep[0m  [80/106], [94mLoss[0m : 2.31789
[1mStep[0m  [90/106], [94mLoss[0m : 2.02600
[1mStep[0m  [100/106], [94mLoss[0m : 2.31131

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.372, [92mTest[0m: 2.519, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.30552
[1mStep[0m  [10/106], [94mLoss[0m : 2.34486
[1mStep[0m  [20/106], [94mLoss[0m : 2.24567
[1mStep[0m  [30/106], [94mLoss[0m : 2.09915
[1mStep[0m  [40/106], [94mLoss[0m : 2.50623
[1mStep[0m  [50/106], [94mLoss[0m : 2.47202
[1mStep[0m  [60/106], [94mLoss[0m : 2.33989
[1mStep[0m  [70/106], [94mLoss[0m : 2.30324
[1mStep[0m  [80/106], [94mLoss[0m : 2.47998
[1mStep[0m  [90/106], [94mLoss[0m : 2.09962
[1mStep[0m  [100/106], [94mLoss[0m : 2.44933

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.380, [92mTest[0m: 2.467, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.38418
[1mStep[0m  [10/106], [94mLoss[0m : 2.28697
[1mStep[0m  [20/106], [94mLoss[0m : 2.37733
[1mStep[0m  [30/106], [94mLoss[0m : 2.12326
[1mStep[0m  [40/106], [94mLoss[0m : 2.34506
[1mStep[0m  [50/106], [94mLoss[0m : 2.41259
[1mStep[0m  [60/106], [94mLoss[0m : 2.50344
[1mStep[0m  [70/106], [94mLoss[0m : 2.35355
[1mStep[0m  [80/106], [94mLoss[0m : 2.25659
[1mStep[0m  [90/106], [94mLoss[0m : 2.40458
[1mStep[0m  [100/106], [94mLoss[0m : 2.51332

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.350, [92mTest[0m: 2.455, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.15445
[1mStep[0m  [10/106], [94mLoss[0m : 2.27495
[1mStep[0m  [20/106], [94mLoss[0m : 2.27677
[1mStep[0m  [30/106], [94mLoss[0m : 2.27002
[1mStep[0m  [40/106], [94mLoss[0m : 2.14905
[1mStep[0m  [50/106], [94mLoss[0m : 2.51127
[1mStep[0m  [60/106], [94mLoss[0m : 2.29466
[1mStep[0m  [70/106], [94mLoss[0m : 2.19629
[1mStep[0m  [80/106], [94mLoss[0m : 2.42683
[1mStep[0m  [90/106], [94mLoss[0m : 2.51559
[1mStep[0m  [100/106], [94mLoss[0m : 2.28207

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.353, [92mTest[0m: 2.488, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.54685
[1mStep[0m  [10/106], [94mLoss[0m : 2.21248
[1mStep[0m  [20/106], [94mLoss[0m : 2.19822
[1mStep[0m  [30/106], [94mLoss[0m : 2.53995
[1mStep[0m  [40/106], [94mLoss[0m : 2.47741
[1mStep[0m  [50/106], [94mLoss[0m : 2.61783
[1mStep[0m  [60/106], [94mLoss[0m : 2.31390
[1mStep[0m  [70/106], [94mLoss[0m : 2.30197
[1mStep[0m  [80/106], [94mLoss[0m : 2.47189
[1mStep[0m  [90/106], [94mLoss[0m : 2.47708
[1mStep[0m  [100/106], [94mLoss[0m : 2.64464

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.363, [92mTest[0m: 2.472, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.14235
[1mStep[0m  [10/106], [94mLoss[0m : 2.52641
[1mStep[0m  [20/106], [94mLoss[0m : 2.39439
[1mStep[0m  [30/106], [94mLoss[0m : 2.36488
[1mStep[0m  [40/106], [94mLoss[0m : 2.44183
[1mStep[0m  [50/106], [94mLoss[0m : 2.33190
[1mStep[0m  [60/106], [94mLoss[0m : 2.39299
[1mStep[0m  [70/106], [94mLoss[0m : 2.42273
[1mStep[0m  [80/106], [94mLoss[0m : 2.11463
[1mStep[0m  [90/106], [94mLoss[0m : 2.58326
[1mStep[0m  [100/106], [94mLoss[0m : 2.08688

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.356, [92mTest[0m: 2.438, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.29803
[1mStep[0m  [10/106], [94mLoss[0m : 2.67001
[1mStep[0m  [20/106], [94mLoss[0m : 2.42358
[1mStep[0m  [30/106], [94mLoss[0m : 2.33589
[1mStep[0m  [40/106], [94mLoss[0m : 2.43816
[1mStep[0m  [50/106], [94mLoss[0m : 2.45172
[1mStep[0m  [60/106], [94mLoss[0m : 2.50314
[1mStep[0m  [70/106], [94mLoss[0m : 2.27378
[1mStep[0m  [80/106], [94mLoss[0m : 2.21170
[1mStep[0m  [90/106], [94mLoss[0m : 2.46684
[1mStep[0m  [100/106], [94mLoss[0m : 2.54252

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.346, [92mTest[0m: 2.446, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.33049
[1mStep[0m  [10/106], [94mLoss[0m : 2.32964
[1mStep[0m  [20/106], [94mLoss[0m : 2.29196
[1mStep[0m  [30/106], [94mLoss[0m : 2.67477
[1mStep[0m  [40/106], [94mLoss[0m : 2.62403
[1mStep[0m  [50/106], [94mLoss[0m : 2.06294
[1mStep[0m  [60/106], [94mLoss[0m : 2.38973
[1mStep[0m  [70/106], [94mLoss[0m : 2.25504
[1mStep[0m  [80/106], [94mLoss[0m : 2.44854
[1mStep[0m  [90/106], [94mLoss[0m : 2.46920
[1mStep[0m  [100/106], [94mLoss[0m : 2.59217

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.347, [92mTest[0m: 2.479, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.59542
[1mStep[0m  [10/106], [94mLoss[0m : 2.42718
[1mStep[0m  [20/106], [94mLoss[0m : 2.35351
[1mStep[0m  [30/106], [94mLoss[0m : 2.31805
[1mStep[0m  [40/106], [94mLoss[0m : 2.50261
[1mStep[0m  [50/106], [94mLoss[0m : 2.30260
[1mStep[0m  [60/106], [94mLoss[0m : 2.41069
[1mStep[0m  [70/106], [94mLoss[0m : 2.25945
[1mStep[0m  [80/106], [94mLoss[0m : 2.16692
[1mStep[0m  [90/106], [94mLoss[0m : 2.53452
[1mStep[0m  [100/106], [94mLoss[0m : 2.48817

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.353, [92mTest[0m: 2.473, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.00271
[1mStep[0m  [10/106], [94mLoss[0m : 2.55213
[1mStep[0m  [20/106], [94mLoss[0m : 2.39143
[1mStep[0m  [30/106], [94mLoss[0m : 2.26613
[1mStep[0m  [40/106], [94mLoss[0m : 2.35189
[1mStep[0m  [50/106], [94mLoss[0m : 2.26114
[1mStep[0m  [60/106], [94mLoss[0m : 2.27381
[1mStep[0m  [70/106], [94mLoss[0m : 2.50303
[1mStep[0m  [80/106], [94mLoss[0m : 2.32669
[1mStep[0m  [90/106], [94mLoss[0m : 2.26648
[1mStep[0m  [100/106], [94mLoss[0m : 2.45870

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.340, [92mTest[0m: 2.472, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.08275
[1mStep[0m  [10/106], [94mLoss[0m : 2.34665
[1mStep[0m  [20/106], [94mLoss[0m : 2.11849
[1mStep[0m  [30/106], [94mLoss[0m : 1.99901
[1mStep[0m  [40/106], [94mLoss[0m : 2.30060
[1mStep[0m  [50/106], [94mLoss[0m : 2.43468
[1mStep[0m  [60/106], [94mLoss[0m : 2.47750
[1mStep[0m  [70/106], [94mLoss[0m : 2.49426
[1mStep[0m  [80/106], [94mLoss[0m : 2.50215
[1mStep[0m  [90/106], [94mLoss[0m : 2.48615
[1mStep[0m  [100/106], [94mLoss[0m : 2.55231

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.331, [92mTest[0m: 2.471, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.52184
[1mStep[0m  [10/106], [94mLoss[0m : 2.20012
[1mStep[0m  [20/106], [94mLoss[0m : 2.32047
[1mStep[0m  [30/106], [94mLoss[0m : 2.20489
[1mStep[0m  [40/106], [94mLoss[0m : 2.54645
[1mStep[0m  [50/106], [94mLoss[0m : 2.48520
[1mStep[0m  [60/106], [94mLoss[0m : 2.28483
[1mStep[0m  [70/106], [94mLoss[0m : 2.15385
[1mStep[0m  [80/106], [94mLoss[0m : 2.01302
[1mStep[0m  [90/106], [94mLoss[0m : 2.39922
[1mStep[0m  [100/106], [94mLoss[0m : 2.42794

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.332, [92mTest[0m: 2.442, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.13824
[1mStep[0m  [10/106], [94mLoss[0m : 2.27189
[1mStep[0m  [20/106], [94mLoss[0m : 2.39674
[1mStep[0m  [30/106], [94mLoss[0m : 2.24012
[1mStep[0m  [40/106], [94mLoss[0m : 2.22307
[1mStep[0m  [50/106], [94mLoss[0m : 2.62872
[1mStep[0m  [60/106], [94mLoss[0m : 2.50767
[1mStep[0m  [70/106], [94mLoss[0m : 2.31530
[1mStep[0m  [80/106], [94mLoss[0m : 2.33093
[1mStep[0m  [90/106], [94mLoss[0m : 2.32489
[1mStep[0m  [100/106], [94mLoss[0m : 2.06431

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.322, [92mTest[0m: 2.462, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.48476
[1mStep[0m  [10/106], [94mLoss[0m : 2.53563
[1mStep[0m  [20/106], [94mLoss[0m : 2.38035
[1mStep[0m  [30/106], [94mLoss[0m : 2.20140
[1mStep[0m  [40/106], [94mLoss[0m : 2.20028
[1mStep[0m  [50/106], [94mLoss[0m : 2.27880
[1mStep[0m  [60/106], [94mLoss[0m : 2.14016
[1mStep[0m  [70/106], [94mLoss[0m : 2.03317
[1mStep[0m  [80/106], [94mLoss[0m : 2.18500
[1mStep[0m  [90/106], [94mLoss[0m : 2.18972
[1mStep[0m  [100/106], [94mLoss[0m : 2.50369

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.322, [92mTest[0m: 2.468, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.50261
[1mStep[0m  [10/106], [94mLoss[0m : 2.48274
[1mStep[0m  [20/106], [94mLoss[0m : 2.27766
[1mStep[0m  [30/106], [94mLoss[0m : 2.24162
[1mStep[0m  [40/106], [94mLoss[0m : 2.15357
[1mStep[0m  [50/106], [94mLoss[0m : 2.32717
[1mStep[0m  [60/106], [94mLoss[0m : 2.35149
[1mStep[0m  [70/106], [94mLoss[0m : 2.24647
[1mStep[0m  [80/106], [94mLoss[0m : 2.23146
[1mStep[0m  [90/106], [94mLoss[0m : 2.26838
[1mStep[0m  [100/106], [94mLoss[0m : 2.39299

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.329, [92mTest[0m: 2.459, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.05512
[1mStep[0m  [10/106], [94mLoss[0m : 2.28765
[1mStep[0m  [20/106], [94mLoss[0m : 2.35592
[1mStep[0m  [30/106], [94mLoss[0m : 2.49397
[1mStep[0m  [40/106], [94mLoss[0m : 2.40755
[1mStep[0m  [50/106], [94mLoss[0m : 2.28248
[1mStep[0m  [60/106], [94mLoss[0m : 2.48121
[1mStep[0m  [70/106], [94mLoss[0m : 2.41560
[1mStep[0m  [80/106], [94mLoss[0m : 2.67766
[1mStep[0m  [90/106], [94mLoss[0m : 2.17867
[1mStep[0m  [100/106], [94mLoss[0m : 2.20511

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.326, [92mTest[0m: 2.425, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.432
====================================

Phase 1 - Evaluation MAE:  2.4323538159424403
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/106], [94mLoss[0m : 2.24496
[1mStep[0m  [10/106], [94mLoss[0m : 2.17166
[1mStep[0m  [20/106], [94mLoss[0m : 2.37983
[1mStep[0m  [30/106], [94mLoss[0m : 2.79909
[1mStep[0m  [40/106], [94mLoss[0m : 2.44563
[1mStep[0m  [50/106], [94mLoss[0m : 2.38041
[1mStep[0m  [60/106], [94mLoss[0m : 2.12446
[1mStep[0m  [70/106], [94mLoss[0m : 2.26716
[1mStep[0m  [80/106], [94mLoss[0m : 2.31640
[1mStep[0m  [90/106], [94mLoss[0m : 2.07975
[1mStep[0m  [100/106], [94mLoss[0m : 2.57522

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.429, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.03908
[1mStep[0m  [10/106], [94mLoss[0m : 2.11043
[1mStep[0m  [20/106], [94mLoss[0m : 2.21780
[1mStep[0m  [30/106], [94mLoss[0m : 2.42269
[1mStep[0m  [40/106], [94mLoss[0m : 2.25471
[1mStep[0m  [50/106], [94mLoss[0m : 2.25938
[1mStep[0m  [60/106], [94mLoss[0m : 2.42965
[1mStep[0m  [70/106], [94mLoss[0m : 2.37834
[1mStep[0m  [80/106], [94mLoss[0m : 2.21163
[1mStep[0m  [90/106], [94mLoss[0m : 2.31650
[1mStep[0m  [100/106], [94mLoss[0m : 2.15008

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.336, [92mTest[0m: 2.947, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.12842
[1mStep[0m  [10/106], [94mLoss[0m : 2.33636
[1mStep[0m  [20/106], [94mLoss[0m : 2.44524
[1mStep[0m  [30/106], [94mLoss[0m : 2.29277
[1mStep[0m  [40/106], [94mLoss[0m : 2.05005
[1mStep[0m  [50/106], [94mLoss[0m : 2.09716
[1mStep[0m  [60/106], [94mLoss[0m : 2.31015
[1mStep[0m  [70/106], [94mLoss[0m : 2.38258
[1mStep[0m  [80/106], [94mLoss[0m : 2.53740
[1mStep[0m  [90/106], [94mLoss[0m : 2.15810
[1mStep[0m  [100/106], [94mLoss[0m : 2.43996

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.267, [92mTest[0m: 2.418, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.25140
[1mStep[0m  [10/106], [94mLoss[0m : 1.99399
[1mStep[0m  [20/106], [94mLoss[0m : 2.18840
[1mStep[0m  [30/106], [94mLoss[0m : 2.23795
[1mStep[0m  [40/106], [94mLoss[0m : 2.33157
[1mStep[0m  [50/106], [94mLoss[0m : 1.99007
[1mStep[0m  [60/106], [94mLoss[0m : 2.18084
[1mStep[0m  [70/106], [94mLoss[0m : 2.16573
[1mStep[0m  [80/106], [94mLoss[0m : 2.22789
[1mStep[0m  [90/106], [94mLoss[0m : 2.26255
[1mStep[0m  [100/106], [94mLoss[0m : 2.17321

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.181, [92mTest[0m: 2.436, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.16786
[1mStep[0m  [10/106], [94mLoss[0m : 2.04859
[1mStep[0m  [20/106], [94mLoss[0m : 2.00483
[1mStep[0m  [30/106], [94mLoss[0m : 2.00747
[1mStep[0m  [40/106], [94mLoss[0m : 2.17033
[1mStep[0m  [50/106], [94mLoss[0m : 2.31085
[1mStep[0m  [60/106], [94mLoss[0m : 2.19058
[1mStep[0m  [70/106], [94mLoss[0m : 2.44838
[1mStep[0m  [80/106], [94mLoss[0m : 2.22991
[1mStep[0m  [90/106], [94mLoss[0m : 2.34085
[1mStep[0m  [100/106], [94mLoss[0m : 2.17696

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.138, [92mTest[0m: 2.481, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.20966
[1mStep[0m  [10/106], [94mLoss[0m : 1.97686
[1mStep[0m  [20/106], [94mLoss[0m : 2.12501
[1mStep[0m  [30/106], [94mLoss[0m : 2.41201
[1mStep[0m  [40/106], [94mLoss[0m : 2.28575
[1mStep[0m  [50/106], [94mLoss[0m : 1.95964
[1mStep[0m  [60/106], [94mLoss[0m : 2.15125
[1mStep[0m  [70/106], [94mLoss[0m : 1.99966
[1mStep[0m  [80/106], [94mLoss[0m : 2.00819
[1mStep[0m  [90/106], [94mLoss[0m : 2.29145
[1mStep[0m  [100/106], [94mLoss[0m : 2.23802

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.076, [92mTest[0m: 2.463, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.28936
[1mStep[0m  [10/106], [94mLoss[0m : 2.08287
[1mStep[0m  [20/106], [94mLoss[0m : 1.80271
[1mStep[0m  [30/106], [94mLoss[0m : 2.14737
[1mStep[0m  [40/106], [94mLoss[0m : 1.85702
[1mStep[0m  [50/106], [94mLoss[0m : 2.25216
[1mStep[0m  [60/106], [94mLoss[0m : 2.10040
[1mStep[0m  [70/106], [94mLoss[0m : 1.91291
[1mStep[0m  [80/106], [94mLoss[0m : 2.39518
[1mStep[0m  [90/106], [94mLoss[0m : 1.99052
[1mStep[0m  [100/106], [94mLoss[0m : 2.16286

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.019, [92mTest[0m: 2.515, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.08007
[1mStep[0m  [10/106], [94mLoss[0m : 1.88943
[1mStep[0m  [20/106], [94mLoss[0m : 1.83715
[1mStep[0m  [30/106], [94mLoss[0m : 1.99588
[1mStep[0m  [40/106], [94mLoss[0m : 1.83686
[1mStep[0m  [50/106], [94mLoss[0m : 2.11379
[1mStep[0m  [60/106], [94mLoss[0m : 1.97166
[1mStep[0m  [70/106], [94mLoss[0m : 1.91973
[1mStep[0m  [80/106], [94mLoss[0m : 1.94369
[1mStep[0m  [90/106], [94mLoss[0m : 1.98484
[1mStep[0m  [100/106], [94mLoss[0m : 1.86988

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.960, [92mTest[0m: 2.486, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.87645
[1mStep[0m  [10/106], [94mLoss[0m : 1.95470
[1mStep[0m  [20/106], [94mLoss[0m : 2.31034
[1mStep[0m  [30/106], [94mLoss[0m : 1.98924
[1mStep[0m  [40/106], [94mLoss[0m : 1.84537
[1mStep[0m  [50/106], [94mLoss[0m : 2.25185
[1mStep[0m  [60/106], [94mLoss[0m : 1.95540
[1mStep[0m  [70/106], [94mLoss[0m : 2.06220
[1mStep[0m  [80/106], [94mLoss[0m : 1.92935
[1mStep[0m  [90/106], [94mLoss[0m : 1.85514
[1mStep[0m  [100/106], [94mLoss[0m : 1.94268

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.927, [92mTest[0m: 2.510, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.90143
[1mStep[0m  [10/106], [94mLoss[0m : 1.80221
[1mStep[0m  [20/106], [94mLoss[0m : 2.12272
[1mStep[0m  [30/106], [94mLoss[0m : 1.72763
[1mStep[0m  [40/106], [94mLoss[0m : 1.92333
[1mStep[0m  [50/106], [94mLoss[0m : 1.92482
[1mStep[0m  [60/106], [94mLoss[0m : 2.02937
[1mStep[0m  [70/106], [94mLoss[0m : 1.96081
[1mStep[0m  [80/106], [94mLoss[0m : 1.98091
[1mStep[0m  [90/106], [94mLoss[0m : 1.69240
[1mStep[0m  [100/106], [94mLoss[0m : 2.06438

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.856, [92mTest[0m: 2.535, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.71436
[1mStep[0m  [10/106], [94mLoss[0m : 1.70855
[1mStep[0m  [20/106], [94mLoss[0m : 1.74665
[1mStep[0m  [30/106], [94mLoss[0m : 1.82159
[1mStep[0m  [40/106], [94mLoss[0m : 1.64023
[1mStep[0m  [50/106], [94mLoss[0m : 1.60246
[1mStep[0m  [60/106], [94mLoss[0m : 1.62591
[1mStep[0m  [70/106], [94mLoss[0m : 1.80084
[1mStep[0m  [80/106], [94mLoss[0m : 1.84671
[1mStep[0m  [90/106], [94mLoss[0m : 2.12041
[1mStep[0m  [100/106], [94mLoss[0m : 1.67308

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.805, [92mTest[0m: 2.548, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.78579
[1mStep[0m  [10/106], [94mLoss[0m : 1.84658
[1mStep[0m  [20/106], [94mLoss[0m : 1.54379
[1mStep[0m  [30/106], [94mLoss[0m : 1.67498
[1mStep[0m  [40/106], [94mLoss[0m : 1.61444
[1mStep[0m  [50/106], [94mLoss[0m : 1.72312
[1mStep[0m  [60/106], [94mLoss[0m : 1.49595
[1mStep[0m  [70/106], [94mLoss[0m : 1.44107
[1mStep[0m  [80/106], [94mLoss[0m : 2.08072
[1mStep[0m  [90/106], [94mLoss[0m : 1.73719
[1mStep[0m  [100/106], [94mLoss[0m : 1.68027

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.790, [92mTest[0m: 2.445, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.88920
[1mStep[0m  [10/106], [94mLoss[0m : 1.70606
[1mStep[0m  [20/106], [94mLoss[0m : 1.89403
[1mStep[0m  [30/106], [94mLoss[0m : 1.74175
[1mStep[0m  [40/106], [94mLoss[0m : 1.61593
[1mStep[0m  [50/106], [94mLoss[0m : 1.75925
[1mStep[0m  [60/106], [94mLoss[0m : 1.64684
[1mStep[0m  [70/106], [94mLoss[0m : 1.68135
[1mStep[0m  [80/106], [94mLoss[0m : 1.73158
[1mStep[0m  [90/106], [94mLoss[0m : 1.57713
[1mStep[0m  [100/106], [94mLoss[0m : 1.54389

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.708, [92mTest[0m: 2.574, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.77725
[1mStep[0m  [10/106], [94mLoss[0m : 1.75185
[1mStep[0m  [20/106], [94mLoss[0m : 1.69106
[1mStep[0m  [30/106], [94mLoss[0m : 1.90568
[1mStep[0m  [40/106], [94mLoss[0m : 1.44931
[1mStep[0m  [50/106], [94mLoss[0m : 1.80286
[1mStep[0m  [60/106], [94mLoss[0m : 1.85374
[1mStep[0m  [70/106], [94mLoss[0m : 1.60965
[1mStep[0m  [80/106], [94mLoss[0m : 1.98408
[1mStep[0m  [90/106], [94mLoss[0m : 1.64517
[1mStep[0m  [100/106], [94mLoss[0m : 1.88491

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.694, [92mTest[0m: 2.524, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.40285
[1mStep[0m  [10/106], [94mLoss[0m : 1.40763
[1mStep[0m  [20/106], [94mLoss[0m : 1.63780
[1mStep[0m  [30/106], [94mLoss[0m : 1.52060
[1mStep[0m  [40/106], [94mLoss[0m : 1.63267
[1mStep[0m  [50/106], [94mLoss[0m : 1.55725
[1mStep[0m  [60/106], [94mLoss[0m : 1.42151
[1mStep[0m  [70/106], [94mLoss[0m : 1.66173
[1mStep[0m  [80/106], [94mLoss[0m : 1.36985
[1mStep[0m  [90/106], [94mLoss[0m : 1.80514
[1mStep[0m  [100/106], [94mLoss[0m : 1.78888

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.655, [92mTest[0m: 2.520, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.71209
[1mStep[0m  [10/106], [94mLoss[0m : 1.53723
[1mStep[0m  [20/106], [94mLoss[0m : 1.62360
[1mStep[0m  [30/106], [94mLoss[0m : 1.54582
[1mStep[0m  [40/106], [94mLoss[0m : 1.59299
[1mStep[0m  [50/106], [94mLoss[0m : 1.57212
[1mStep[0m  [60/106], [94mLoss[0m : 1.42608
[1mStep[0m  [70/106], [94mLoss[0m : 1.59154
[1mStep[0m  [80/106], [94mLoss[0m : 1.69475
[1mStep[0m  [90/106], [94mLoss[0m : 1.63410
[1mStep[0m  [100/106], [94mLoss[0m : 1.55777

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.620, [92mTest[0m: 2.489, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.46356
[1mStep[0m  [10/106], [94mLoss[0m : 1.68513
[1mStep[0m  [20/106], [94mLoss[0m : 1.54624
[1mStep[0m  [30/106], [94mLoss[0m : 1.63684
[1mStep[0m  [40/106], [94mLoss[0m : 1.82859
[1mStep[0m  [50/106], [94mLoss[0m : 1.67306
[1mStep[0m  [60/106], [94mLoss[0m : 1.37117
[1mStep[0m  [70/106], [94mLoss[0m : 1.57562
[1mStep[0m  [80/106], [94mLoss[0m : 1.46555
[1mStep[0m  [90/106], [94mLoss[0m : 1.43510
[1mStep[0m  [100/106], [94mLoss[0m : 1.67126

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.579, [92mTest[0m: 2.484, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.42802
[1mStep[0m  [10/106], [94mLoss[0m : 1.57598
[1mStep[0m  [20/106], [94mLoss[0m : 1.80019
[1mStep[0m  [30/106], [94mLoss[0m : 1.46840
[1mStep[0m  [40/106], [94mLoss[0m : 1.60702
[1mStep[0m  [50/106], [94mLoss[0m : 1.71528
[1mStep[0m  [60/106], [94mLoss[0m : 1.35797
[1mStep[0m  [70/106], [94mLoss[0m : 1.37979
[1mStep[0m  [80/106], [94mLoss[0m : 1.80202
[1mStep[0m  [90/106], [94mLoss[0m : 1.67709
[1mStep[0m  [100/106], [94mLoss[0m : 1.48721

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.559, [92mTest[0m: 2.603, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.51457
[1mStep[0m  [10/106], [94mLoss[0m : 1.61435
[1mStep[0m  [20/106], [94mLoss[0m : 1.44296
[1mStep[0m  [30/106], [94mLoss[0m : 1.74790
[1mStep[0m  [40/106], [94mLoss[0m : 1.58979
[1mStep[0m  [50/106], [94mLoss[0m : 1.67102
[1mStep[0m  [60/106], [94mLoss[0m : 1.59458
[1mStep[0m  [70/106], [94mLoss[0m : 1.61726
[1mStep[0m  [80/106], [94mLoss[0m : 1.55963
[1mStep[0m  [90/106], [94mLoss[0m : 1.55537
[1mStep[0m  [100/106], [94mLoss[0m : 1.38712

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.537, [92mTest[0m: 2.548, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.54203
[1mStep[0m  [10/106], [94mLoss[0m : 1.45814
[1mStep[0m  [20/106], [94mLoss[0m : 1.28222
[1mStep[0m  [30/106], [94mLoss[0m : 1.82798
[1mStep[0m  [40/106], [94mLoss[0m : 1.56920
[1mStep[0m  [50/106], [94mLoss[0m : 1.52552
[1mStep[0m  [60/106], [94mLoss[0m : 1.77292
[1mStep[0m  [70/106], [94mLoss[0m : 1.50592
[1mStep[0m  [80/106], [94mLoss[0m : 1.67746
[1mStep[0m  [90/106], [94mLoss[0m : 1.55822
[1mStep[0m  [100/106], [94mLoss[0m : 1.35941

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.502, [92mTest[0m: 2.529, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.56773
[1mStep[0m  [10/106], [94mLoss[0m : 1.36982
[1mStep[0m  [20/106], [94mLoss[0m : 1.34555
[1mStep[0m  [30/106], [94mLoss[0m : 1.46970
[1mStep[0m  [40/106], [94mLoss[0m : 1.53300
[1mStep[0m  [50/106], [94mLoss[0m : 1.38826
[1mStep[0m  [60/106], [94mLoss[0m : 1.61947
[1mStep[0m  [70/106], [94mLoss[0m : 1.30533
[1mStep[0m  [80/106], [94mLoss[0m : 1.33789
[1mStep[0m  [90/106], [94mLoss[0m : 1.61463
[1mStep[0m  [100/106], [94mLoss[0m : 1.70531

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.488, [92mTest[0m: 2.478, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.53805
[1mStep[0m  [10/106], [94mLoss[0m : 1.36447
[1mStep[0m  [20/106], [94mLoss[0m : 1.58361
[1mStep[0m  [30/106], [94mLoss[0m : 1.44285
[1mStep[0m  [40/106], [94mLoss[0m : 1.47303
[1mStep[0m  [50/106], [94mLoss[0m : 1.52153
[1mStep[0m  [60/106], [94mLoss[0m : 1.35270
[1mStep[0m  [70/106], [94mLoss[0m : 1.43827
[1mStep[0m  [80/106], [94mLoss[0m : 1.50957
[1mStep[0m  [90/106], [94mLoss[0m : 1.39939
[1mStep[0m  [100/106], [94mLoss[0m : 1.49570

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.443, [92mTest[0m: 2.474, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.52502
[1mStep[0m  [10/106], [94mLoss[0m : 1.33672
[1mStep[0m  [20/106], [94mLoss[0m : 1.52335
[1mStep[0m  [30/106], [94mLoss[0m : 1.66826
[1mStep[0m  [40/106], [94mLoss[0m : 1.51954
[1mStep[0m  [50/106], [94mLoss[0m : 1.43753
[1mStep[0m  [60/106], [94mLoss[0m : 1.41682
[1mStep[0m  [70/106], [94mLoss[0m : 1.39520
[1mStep[0m  [80/106], [94mLoss[0m : 1.37339
[1mStep[0m  [90/106], [94mLoss[0m : 1.57429
[1mStep[0m  [100/106], [94mLoss[0m : 1.62872

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.427, [92mTest[0m: 2.517, [96mlr[0m: 0.004545
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 22 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.481
====================================

Phase 2 - Evaluation MAE:  2.480704536977804
MAE score P1      2.432354
MAE score P2      2.480705
loss                1.4273
learning_rate      0.00505
batch_size             128
hidden_sizes         [300]
epochs                  30
activation            relu
optimizer              sgd
early stopping        True
dropout                0.2
momentum               0.5
weight_decay        0.0001
Name: 12, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/106], [94mLoss[0m : 11.22414
[1mStep[0m  [10/106], [94mLoss[0m : 4.52183
[1mStep[0m  [20/106], [94mLoss[0m : 2.74307
[1mStep[0m  [30/106], [94mLoss[0m : 2.83649
[1mStep[0m  [40/106], [94mLoss[0m : 2.77369
[1mStep[0m  [50/106], [94mLoss[0m : 2.34028
[1mStep[0m  [60/106], [94mLoss[0m : 2.60372
[1mStep[0m  [70/106], [94mLoss[0m : 2.72833
[1mStep[0m  [80/106], [94mLoss[0m : 2.32184
[1mStep[0m  [90/106], [94mLoss[0m : 2.74215
[1mStep[0m  [100/106], [94mLoss[0m : 2.56055

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.238, [92mTest[0m: 10.792, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.54612
[1mStep[0m  [10/106], [94mLoss[0m : 2.48504
[1mStep[0m  [20/106], [94mLoss[0m : 2.30879
[1mStep[0m  [30/106], [94mLoss[0m : 2.52003
[1mStep[0m  [40/106], [94mLoss[0m : 2.49560
[1mStep[0m  [50/106], [94mLoss[0m : 2.69136
[1mStep[0m  [60/106], [94mLoss[0m : 2.64416
[1mStep[0m  [70/106], [94mLoss[0m : 2.64909
[1mStep[0m  [80/106], [94mLoss[0m : 2.52271
[1mStep[0m  [90/106], [94mLoss[0m : 2.57501
[1mStep[0m  [100/106], [94mLoss[0m : 2.64361

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.486, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.55200
[1mStep[0m  [10/106], [94mLoss[0m : 2.46321
[1mStep[0m  [20/106], [94mLoss[0m : 2.36974
[1mStep[0m  [30/106], [94mLoss[0m : 2.25163
[1mStep[0m  [40/106], [94mLoss[0m : 2.25872
[1mStep[0m  [50/106], [94mLoss[0m : 2.48801
[1mStep[0m  [60/106], [94mLoss[0m : 2.47263
[1mStep[0m  [70/106], [94mLoss[0m : 2.30730
[1mStep[0m  [80/106], [94mLoss[0m : 2.40222
[1mStep[0m  [90/106], [94mLoss[0m : 2.32049
[1mStep[0m  [100/106], [94mLoss[0m : 2.32114

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.463, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.86186
[1mStep[0m  [10/106], [94mLoss[0m : 2.98293
[1mStep[0m  [20/106], [94mLoss[0m : 2.48763
[1mStep[0m  [30/106], [94mLoss[0m : 2.69475
[1mStep[0m  [40/106], [94mLoss[0m : 2.39806
[1mStep[0m  [50/106], [94mLoss[0m : 2.65032
[1mStep[0m  [60/106], [94mLoss[0m : 2.57724
[1mStep[0m  [70/106], [94mLoss[0m : 2.39673
[1mStep[0m  [80/106], [94mLoss[0m : 2.67196
[1mStep[0m  [90/106], [94mLoss[0m : 2.31520
[1mStep[0m  [100/106], [94mLoss[0m : 2.56275

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.439, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.18789
[1mStep[0m  [10/106], [94mLoss[0m : 2.11053
[1mStep[0m  [20/106], [94mLoss[0m : 2.49779
[1mStep[0m  [30/106], [94mLoss[0m : 2.66788
[1mStep[0m  [40/106], [94mLoss[0m : 2.62917
[1mStep[0m  [50/106], [94mLoss[0m : 2.46511
[1mStep[0m  [60/106], [94mLoss[0m : 2.86232
[1mStep[0m  [70/106], [94mLoss[0m : 2.41771
[1mStep[0m  [80/106], [94mLoss[0m : 2.56508
[1mStep[0m  [90/106], [94mLoss[0m : 2.87471
[1mStep[0m  [100/106], [94mLoss[0m : 2.46798

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.454, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.50885
[1mStep[0m  [10/106], [94mLoss[0m : 2.78848
[1mStep[0m  [20/106], [94mLoss[0m : 2.57561
[1mStep[0m  [30/106], [94mLoss[0m : 2.30315
[1mStep[0m  [40/106], [94mLoss[0m : 2.70320
[1mStep[0m  [50/106], [94mLoss[0m : 2.30157
[1mStep[0m  [60/106], [94mLoss[0m : 2.48996
[1mStep[0m  [70/106], [94mLoss[0m : 2.36138
[1mStep[0m  [80/106], [94mLoss[0m : 2.42377
[1mStep[0m  [90/106], [94mLoss[0m : 2.74235
[1mStep[0m  [100/106], [94mLoss[0m : 2.46168

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.432, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.20553
[1mStep[0m  [10/106], [94mLoss[0m : 2.44156
[1mStep[0m  [20/106], [94mLoss[0m : 2.30129
[1mStep[0m  [30/106], [94mLoss[0m : 2.43648
[1mStep[0m  [40/106], [94mLoss[0m : 2.58210
[1mStep[0m  [50/106], [94mLoss[0m : 2.37289
[1mStep[0m  [60/106], [94mLoss[0m : 2.52399
[1mStep[0m  [70/106], [94mLoss[0m : 2.54429
[1mStep[0m  [80/106], [94mLoss[0m : 2.36464
[1mStep[0m  [90/106], [94mLoss[0m : 2.27838
[1mStep[0m  [100/106], [94mLoss[0m : 2.39378

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.422, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.70260
[1mStep[0m  [10/106], [94mLoss[0m : 2.40399
[1mStep[0m  [20/106], [94mLoss[0m : 2.45328
[1mStep[0m  [30/106], [94mLoss[0m : 2.26937
[1mStep[0m  [40/106], [94mLoss[0m : 2.85391
[1mStep[0m  [50/106], [94mLoss[0m : 2.19153
[1mStep[0m  [60/106], [94mLoss[0m : 2.16307
[1mStep[0m  [70/106], [94mLoss[0m : 2.68626
[1mStep[0m  [80/106], [94mLoss[0m : 2.23699
[1mStep[0m  [90/106], [94mLoss[0m : 2.41119
[1mStep[0m  [100/106], [94mLoss[0m : 2.52334

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.418, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.28166
[1mStep[0m  [10/106], [94mLoss[0m : 2.24915
[1mStep[0m  [20/106], [94mLoss[0m : 2.55491
[1mStep[0m  [30/106], [94mLoss[0m : 2.25644
[1mStep[0m  [40/106], [94mLoss[0m : 2.32979
[1mStep[0m  [50/106], [94mLoss[0m : 2.47079
[1mStep[0m  [60/106], [94mLoss[0m : 2.41837
[1mStep[0m  [70/106], [94mLoss[0m : 2.41481
[1mStep[0m  [80/106], [94mLoss[0m : 2.38245
[1mStep[0m  [90/106], [94mLoss[0m : 2.50086
[1mStep[0m  [100/106], [94mLoss[0m : 2.34805

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.425, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.16794
[1mStep[0m  [10/106], [94mLoss[0m : 2.63003
[1mStep[0m  [20/106], [94mLoss[0m : 2.49777
[1mStep[0m  [30/106], [94mLoss[0m : 2.36107
[1mStep[0m  [40/106], [94mLoss[0m : 2.15089
[1mStep[0m  [50/106], [94mLoss[0m : 2.33267
[1mStep[0m  [60/106], [94mLoss[0m : 2.51804
[1mStep[0m  [70/106], [94mLoss[0m : 2.38693
[1mStep[0m  [80/106], [94mLoss[0m : 2.49563
[1mStep[0m  [90/106], [94mLoss[0m : 2.27800
[1mStep[0m  [100/106], [94mLoss[0m : 2.18414

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.414, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.42540
[1mStep[0m  [10/106], [94mLoss[0m : 2.14872
[1mStep[0m  [20/106], [94mLoss[0m : 2.53332
[1mStep[0m  [30/106], [94mLoss[0m : 2.35613
[1mStep[0m  [40/106], [94mLoss[0m : 2.35575
[1mStep[0m  [50/106], [94mLoss[0m : 2.51714
[1mStep[0m  [60/106], [94mLoss[0m : 2.53131
[1mStep[0m  [70/106], [94mLoss[0m : 2.40431
[1mStep[0m  [80/106], [94mLoss[0m : 2.44088
[1mStep[0m  [90/106], [94mLoss[0m : 2.26211
[1mStep[0m  [100/106], [94mLoss[0m : 2.19960

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.411, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.63928
[1mStep[0m  [10/106], [94mLoss[0m : 2.70719
[1mStep[0m  [20/106], [94mLoss[0m : 2.60046
[1mStep[0m  [30/106], [94mLoss[0m : 2.63558
[1mStep[0m  [40/106], [94mLoss[0m : 2.37840
[1mStep[0m  [50/106], [94mLoss[0m : 2.22741
[1mStep[0m  [60/106], [94mLoss[0m : 2.25270
[1mStep[0m  [70/106], [94mLoss[0m : 2.33765
[1mStep[0m  [80/106], [94mLoss[0m : 2.27966
[1mStep[0m  [90/106], [94mLoss[0m : 2.40292
[1mStep[0m  [100/106], [94mLoss[0m : 2.50968

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.422, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.51316
[1mStep[0m  [10/106], [94mLoss[0m : 2.36703
[1mStep[0m  [20/106], [94mLoss[0m : 2.17187
[1mStep[0m  [30/106], [94mLoss[0m : 2.22330
[1mStep[0m  [40/106], [94mLoss[0m : 2.57990
[1mStep[0m  [50/106], [94mLoss[0m : 2.55930
[1mStep[0m  [60/106], [94mLoss[0m : 2.25252
[1mStep[0m  [70/106], [94mLoss[0m : 2.55634
[1mStep[0m  [80/106], [94mLoss[0m : 2.13643
[1mStep[0m  [90/106], [94mLoss[0m : 2.57204
[1mStep[0m  [100/106], [94mLoss[0m : 2.07064

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.417, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.60359
[1mStep[0m  [10/106], [94mLoss[0m : 2.43317
[1mStep[0m  [20/106], [94mLoss[0m : 2.46649
[1mStep[0m  [30/106], [94mLoss[0m : 2.34320
[1mStep[0m  [40/106], [94mLoss[0m : 2.50432
[1mStep[0m  [50/106], [94mLoss[0m : 2.20362
[1mStep[0m  [60/106], [94mLoss[0m : 2.55493
[1mStep[0m  [70/106], [94mLoss[0m : 2.30702
[1mStep[0m  [80/106], [94mLoss[0m : 2.39200
[1mStep[0m  [90/106], [94mLoss[0m : 2.54572
[1mStep[0m  [100/106], [94mLoss[0m : 2.33105

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.418, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.62088
[1mStep[0m  [10/106], [94mLoss[0m : 2.32212
[1mStep[0m  [20/106], [94mLoss[0m : 2.65418
[1mStep[0m  [30/106], [94mLoss[0m : 2.03508
[1mStep[0m  [40/106], [94mLoss[0m : 2.10339
[1mStep[0m  [50/106], [94mLoss[0m : 2.21722
[1mStep[0m  [60/106], [94mLoss[0m : 2.47150
[1mStep[0m  [70/106], [94mLoss[0m : 2.60987
[1mStep[0m  [80/106], [94mLoss[0m : 2.62453
[1mStep[0m  [90/106], [94mLoss[0m : 2.51831
[1mStep[0m  [100/106], [94mLoss[0m : 2.67465

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.412, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.42869
[1mStep[0m  [10/106], [94mLoss[0m : 2.42465
[1mStep[0m  [20/106], [94mLoss[0m : 2.60841
[1mStep[0m  [30/106], [94mLoss[0m : 2.36413
[1mStep[0m  [40/106], [94mLoss[0m : 2.31562
[1mStep[0m  [50/106], [94mLoss[0m : 2.43190
[1mStep[0m  [60/106], [94mLoss[0m : 2.50311
[1mStep[0m  [70/106], [94mLoss[0m : 2.63246
[1mStep[0m  [80/106], [94mLoss[0m : 2.32790
[1mStep[0m  [90/106], [94mLoss[0m : 2.07588
[1mStep[0m  [100/106], [94mLoss[0m : 2.58786

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.412, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.41847
[1mStep[0m  [10/106], [94mLoss[0m : 2.37794
[1mStep[0m  [20/106], [94mLoss[0m : 2.36100
[1mStep[0m  [30/106], [94mLoss[0m : 2.41315
[1mStep[0m  [40/106], [94mLoss[0m : 2.44488
[1mStep[0m  [50/106], [94mLoss[0m : 2.58378
[1mStep[0m  [60/106], [94mLoss[0m : 2.47099
[1mStep[0m  [70/106], [94mLoss[0m : 2.36428
[1mStep[0m  [80/106], [94mLoss[0m : 2.50410
[1mStep[0m  [90/106], [94mLoss[0m : 2.50655
[1mStep[0m  [100/106], [94mLoss[0m : 2.27324

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.414, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.30146
[1mStep[0m  [10/106], [94mLoss[0m : 2.29870
[1mStep[0m  [20/106], [94mLoss[0m : 2.41990
[1mStep[0m  [30/106], [94mLoss[0m : 2.32367
[1mStep[0m  [40/106], [94mLoss[0m : 2.59869
[1mStep[0m  [50/106], [94mLoss[0m : 2.57077
[1mStep[0m  [60/106], [94mLoss[0m : 2.29798
[1mStep[0m  [70/106], [94mLoss[0m : 2.44177
[1mStep[0m  [80/106], [94mLoss[0m : 2.23904
[1mStep[0m  [90/106], [94mLoss[0m : 2.18179
[1mStep[0m  [100/106], [94mLoss[0m : 2.34365

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.410, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.40689
[1mStep[0m  [10/106], [94mLoss[0m : 2.61554
[1mStep[0m  [20/106], [94mLoss[0m : 2.45472
[1mStep[0m  [30/106], [94mLoss[0m : 2.50125
[1mStep[0m  [40/106], [94mLoss[0m : 2.35168
[1mStep[0m  [50/106], [94mLoss[0m : 2.43077
[1mStep[0m  [60/106], [94mLoss[0m : 2.46129
[1mStep[0m  [70/106], [94mLoss[0m : 2.40012
[1mStep[0m  [80/106], [94mLoss[0m : 2.47714
[1mStep[0m  [90/106], [94mLoss[0m : 2.56519
[1mStep[0m  [100/106], [94mLoss[0m : 2.13229

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.426, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.38639
[1mStep[0m  [10/106], [94mLoss[0m : 2.40535
[1mStep[0m  [20/106], [94mLoss[0m : 2.44299
[1mStep[0m  [30/106], [94mLoss[0m : 2.28079
[1mStep[0m  [40/106], [94mLoss[0m : 2.35475
[1mStep[0m  [50/106], [94mLoss[0m : 2.49500
[1mStep[0m  [60/106], [94mLoss[0m : 2.24875
[1mStep[0m  [70/106], [94mLoss[0m : 2.48043
[1mStep[0m  [80/106], [94mLoss[0m : 2.67010
[1mStep[0m  [90/106], [94mLoss[0m : 2.49016
[1mStep[0m  [100/106], [94mLoss[0m : 2.54865

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.410, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.63556
[1mStep[0m  [10/106], [94mLoss[0m : 2.39281
[1mStep[0m  [20/106], [94mLoss[0m : 2.93527
[1mStep[0m  [30/106], [94mLoss[0m : 2.34036
[1mStep[0m  [40/106], [94mLoss[0m : 2.36411
[1mStep[0m  [50/106], [94mLoss[0m : 2.29142
[1mStep[0m  [60/106], [94mLoss[0m : 2.31969
[1mStep[0m  [70/106], [94mLoss[0m : 2.74705
[1mStep[0m  [80/106], [94mLoss[0m : 2.48280
[1mStep[0m  [90/106], [94mLoss[0m : 2.38721
[1mStep[0m  [100/106], [94mLoss[0m : 2.41783

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.404, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.24032
[1mStep[0m  [10/106], [94mLoss[0m : 2.39376
[1mStep[0m  [20/106], [94mLoss[0m : 2.38222
[1mStep[0m  [30/106], [94mLoss[0m : 2.23510
[1mStep[0m  [40/106], [94mLoss[0m : 2.04735
[1mStep[0m  [50/106], [94mLoss[0m : 2.14299
[1mStep[0m  [60/106], [94mLoss[0m : 2.48224
[1mStep[0m  [70/106], [94mLoss[0m : 2.21669
[1mStep[0m  [80/106], [94mLoss[0m : 2.40058
[1mStep[0m  [90/106], [94mLoss[0m : 2.49313
[1mStep[0m  [100/106], [94mLoss[0m : 2.77398

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.413, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.47609
[1mStep[0m  [10/106], [94mLoss[0m : 2.70535
[1mStep[0m  [20/106], [94mLoss[0m : 2.07510
[1mStep[0m  [30/106], [94mLoss[0m : 2.40670
[1mStep[0m  [40/106], [94mLoss[0m : 2.43171
[1mStep[0m  [50/106], [94mLoss[0m : 2.51248
[1mStep[0m  [60/106], [94mLoss[0m : 2.21061
[1mStep[0m  [70/106], [94mLoss[0m : 2.43624
[1mStep[0m  [80/106], [94mLoss[0m : 2.31715
[1mStep[0m  [90/106], [94mLoss[0m : 2.32198
[1mStep[0m  [100/106], [94mLoss[0m : 2.55365

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.398, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.56913
[1mStep[0m  [10/106], [94mLoss[0m : 2.44900
[1mStep[0m  [20/106], [94mLoss[0m : 2.31074
[1mStep[0m  [30/106], [94mLoss[0m : 2.30091
[1mStep[0m  [40/106], [94mLoss[0m : 2.08465
[1mStep[0m  [50/106], [94mLoss[0m : 2.45561
[1mStep[0m  [60/106], [94mLoss[0m : 2.43371
[1mStep[0m  [70/106], [94mLoss[0m : 2.52379
[1mStep[0m  [80/106], [94mLoss[0m : 2.22582
[1mStep[0m  [90/106], [94mLoss[0m : 2.30434
[1mStep[0m  [100/106], [94mLoss[0m : 2.29890

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.401, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.40721
[1mStep[0m  [10/106], [94mLoss[0m : 2.43363
[1mStep[0m  [20/106], [94mLoss[0m : 2.48913
[1mStep[0m  [30/106], [94mLoss[0m : 2.22165
[1mStep[0m  [40/106], [94mLoss[0m : 2.53308
[1mStep[0m  [50/106], [94mLoss[0m : 2.15669
[1mStep[0m  [60/106], [94mLoss[0m : 2.54425
[1mStep[0m  [70/106], [94mLoss[0m : 2.34937
[1mStep[0m  [80/106], [94mLoss[0m : 2.52977
[1mStep[0m  [90/106], [94mLoss[0m : 2.24009
[1mStep[0m  [100/106], [94mLoss[0m : 2.29661

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.407, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.37497
[1mStep[0m  [10/106], [94mLoss[0m : 2.29948
[1mStep[0m  [20/106], [94mLoss[0m : 2.41728
[1mStep[0m  [30/106], [94mLoss[0m : 2.29329
[1mStep[0m  [40/106], [94mLoss[0m : 2.43488
[1mStep[0m  [50/106], [94mLoss[0m : 2.38753
[1mStep[0m  [60/106], [94mLoss[0m : 2.50955
[1mStep[0m  [70/106], [94mLoss[0m : 2.59932
[1mStep[0m  [80/106], [94mLoss[0m : 2.25601
[1mStep[0m  [90/106], [94mLoss[0m : 2.19826
[1mStep[0m  [100/106], [94mLoss[0m : 2.18456

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.396, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.57304
[1mStep[0m  [10/106], [94mLoss[0m : 2.47218
[1mStep[0m  [20/106], [94mLoss[0m : 2.52178
[1mStep[0m  [30/106], [94mLoss[0m : 2.27559
[1mStep[0m  [40/106], [94mLoss[0m : 2.84291
[1mStep[0m  [50/106], [94mLoss[0m : 2.53817
[1mStep[0m  [60/106], [94mLoss[0m : 2.25452
[1mStep[0m  [70/106], [94mLoss[0m : 2.42737
[1mStep[0m  [80/106], [94mLoss[0m : 2.44219
[1mStep[0m  [90/106], [94mLoss[0m : 2.52503
[1mStep[0m  [100/106], [94mLoss[0m : 2.20215

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.398, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.57773
[1mStep[0m  [10/106], [94mLoss[0m : 2.46777
[1mStep[0m  [20/106], [94mLoss[0m : 2.51819
[1mStep[0m  [30/106], [94mLoss[0m : 2.87586
[1mStep[0m  [40/106], [94mLoss[0m : 2.52316
[1mStep[0m  [50/106], [94mLoss[0m : 2.26740
[1mStep[0m  [60/106], [94mLoss[0m : 2.37908
[1mStep[0m  [70/106], [94mLoss[0m : 2.20196
[1mStep[0m  [80/106], [94mLoss[0m : 2.52391
[1mStep[0m  [90/106], [94mLoss[0m : 2.49595
[1mStep[0m  [100/106], [94mLoss[0m : 2.72539

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.404, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.65043
[1mStep[0m  [10/106], [94mLoss[0m : 2.34442
[1mStep[0m  [20/106], [94mLoss[0m : 2.27826
[1mStep[0m  [30/106], [94mLoss[0m : 2.34033
[1mStep[0m  [40/106], [94mLoss[0m : 2.47605
[1mStep[0m  [50/106], [94mLoss[0m : 2.61059
[1mStep[0m  [60/106], [94mLoss[0m : 2.41712
[1mStep[0m  [70/106], [94mLoss[0m : 2.24332
[1mStep[0m  [80/106], [94mLoss[0m : 2.51446
[1mStep[0m  [90/106], [94mLoss[0m : 2.14658
[1mStep[0m  [100/106], [94mLoss[0m : 2.18512

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.412, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.42539
[1mStep[0m  [10/106], [94mLoss[0m : 2.39325
[1mStep[0m  [20/106], [94mLoss[0m : 2.34728
[1mStep[0m  [30/106], [94mLoss[0m : 2.30775
[1mStep[0m  [40/106], [94mLoss[0m : 2.37297
[1mStep[0m  [50/106], [94mLoss[0m : 2.59922
[1mStep[0m  [60/106], [94mLoss[0m : 2.25098
[1mStep[0m  [70/106], [94mLoss[0m : 2.32918
[1mStep[0m  [80/106], [94mLoss[0m : 2.18655
[1mStep[0m  [90/106], [94mLoss[0m : 2.34045
[1mStep[0m  [100/106], [94mLoss[0m : 2.33537

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.394, [92mTest[0m: 2.394, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.393
====================================

Phase 1 - Evaluation MAE:  2.392964043707218
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/106], [94mLoss[0m : 2.35828
[1mStep[0m  [10/106], [94mLoss[0m : 2.42390
[1mStep[0m  [20/106], [94mLoss[0m : 2.26582
[1mStep[0m  [30/106], [94mLoss[0m : 2.41319
[1mStep[0m  [40/106], [94mLoss[0m : 2.24801
[1mStep[0m  [50/106], [94mLoss[0m : 2.67889
[1mStep[0m  [60/106], [94mLoss[0m : 2.20479
[1mStep[0m  [70/106], [94mLoss[0m : 2.37642
[1mStep[0m  [80/106], [94mLoss[0m : 2.46530
[1mStep[0m  [90/106], [94mLoss[0m : 2.19477
[1mStep[0m  [100/106], [94mLoss[0m : 2.20873

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.385, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.55685
[1mStep[0m  [10/106], [94mLoss[0m : 2.34411
[1mStep[0m  [20/106], [94mLoss[0m : 2.34518
[1mStep[0m  [30/106], [94mLoss[0m : 2.43248
[1mStep[0m  [40/106], [94mLoss[0m : 2.31891
[1mStep[0m  [50/106], [94mLoss[0m : 2.58014
[1mStep[0m  [60/106], [94mLoss[0m : 2.11888
[1mStep[0m  [70/106], [94mLoss[0m : 2.00340
[1mStep[0m  [80/106], [94mLoss[0m : 2.38078
[1mStep[0m  [90/106], [94mLoss[0m : 2.30045
[1mStep[0m  [100/106], [94mLoss[0m : 2.32108

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.372, [92mTest[0m: 2.536, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.24346
[1mStep[0m  [10/106], [94mLoss[0m : 2.30645
[1mStep[0m  [20/106], [94mLoss[0m : 2.15329
[1mStep[0m  [30/106], [94mLoss[0m : 2.29370
[1mStep[0m  [40/106], [94mLoss[0m : 2.15916
[1mStep[0m  [50/106], [94mLoss[0m : 2.12450
[1mStep[0m  [60/106], [94mLoss[0m : 2.33716
[1mStep[0m  [70/106], [94mLoss[0m : 2.28724
[1mStep[0m  [80/106], [94mLoss[0m : 2.42794
[1mStep[0m  [90/106], [94mLoss[0m : 2.49717
[1mStep[0m  [100/106], [94mLoss[0m : 2.45471

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.329, [92mTest[0m: 2.574, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.32605
[1mStep[0m  [10/106], [94mLoss[0m : 2.16537
[1mStep[0m  [20/106], [94mLoss[0m : 2.39597
[1mStep[0m  [30/106], [94mLoss[0m : 2.45521
[1mStep[0m  [40/106], [94mLoss[0m : 2.38992
[1mStep[0m  [50/106], [94mLoss[0m : 2.26814
[1mStep[0m  [60/106], [94mLoss[0m : 2.27802
[1mStep[0m  [70/106], [94mLoss[0m : 2.22505
[1mStep[0m  [80/106], [94mLoss[0m : 2.25515
[1mStep[0m  [90/106], [94mLoss[0m : 2.46891
[1mStep[0m  [100/106], [94mLoss[0m : 2.11371

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.272, [92mTest[0m: 2.472, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.19493
[1mStep[0m  [10/106], [94mLoss[0m : 2.33255
[1mStep[0m  [20/106], [94mLoss[0m : 2.19098
[1mStep[0m  [30/106], [94mLoss[0m : 2.31765
[1mStep[0m  [40/106], [94mLoss[0m : 2.16520
[1mStep[0m  [50/106], [94mLoss[0m : 2.38226
[1mStep[0m  [60/106], [94mLoss[0m : 2.65218
[1mStep[0m  [70/106], [94mLoss[0m : 2.21911
[1mStep[0m  [80/106], [94mLoss[0m : 2.22479
[1mStep[0m  [90/106], [94mLoss[0m : 2.21115
[1mStep[0m  [100/106], [94mLoss[0m : 1.95274

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.218, [92mTest[0m: 2.673, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.98068
[1mStep[0m  [10/106], [94mLoss[0m : 2.20880
[1mStep[0m  [20/106], [94mLoss[0m : 2.10394
[1mStep[0m  [30/106], [94mLoss[0m : 2.43133
[1mStep[0m  [40/106], [94mLoss[0m : 1.95388
[1mStep[0m  [50/106], [94mLoss[0m : 1.95639
[1mStep[0m  [60/106], [94mLoss[0m : 2.29818
[1mStep[0m  [70/106], [94mLoss[0m : 2.21891
[1mStep[0m  [80/106], [94mLoss[0m : 2.08468
[1mStep[0m  [90/106], [94mLoss[0m : 1.92452
[1mStep[0m  [100/106], [94mLoss[0m : 2.03150

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.170, [92mTest[0m: 2.545, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.83370
[1mStep[0m  [10/106], [94mLoss[0m : 2.24596
[1mStep[0m  [20/106], [94mLoss[0m : 1.94535
[1mStep[0m  [30/106], [94mLoss[0m : 2.17592
[1mStep[0m  [40/106], [94mLoss[0m : 2.01357
[1mStep[0m  [50/106], [94mLoss[0m : 2.21080
[1mStep[0m  [60/106], [94mLoss[0m : 2.37969
[1mStep[0m  [70/106], [94mLoss[0m : 2.18447
[1mStep[0m  [80/106], [94mLoss[0m : 1.96297
[1mStep[0m  [90/106], [94mLoss[0m : 2.02474
[1mStep[0m  [100/106], [94mLoss[0m : 2.35239

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.093, [92mTest[0m: 2.398, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.36710
[1mStep[0m  [10/106], [94mLoss[0m : 2.05248
[1mStep[0m  [20/106], [94mLoss[0m : 1.97547
[1mStep[0m  [30/106], [94mLoss[0m : 2.03057
[1mStep[0m  [40/106], [94mLoss[0m : 1.86505
[1mStep[0m  [50/106], [94mLoss[0m : 1.88774
[1mStep[0m  [60/106], [94mLoss[0m : 1.97056
[1mStep[0m  [70/106], [94mLoss[0m : 2.15896
[1mStep[0m  [80/106], [94mLoss[0m : 2.38949
[1mStep[0m  [90/106], [94mLoss[0m : 1.93664
[1mStep[0m  [100/106], [94mLoss[0m : 1.89431

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.051, [92mTest[0m: 2.579, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.03639
[1mStep[0m  [10/106], [94mLoss[0m : 2.07835
[1mStep[0m  [20/106], [94mLoss[0m : 1.98462
[1mStep[0m  [30/106], [94mLoss[0m : 2.13574
[1mStep[0m  [40/106], [94mLoss[0m : 1.82328
[1mStep[0m  [50/106], [94mLoss[0m : 1.78334
[1mStep[0m  [60/106], [94mLoss[0m : 2.04928
[1mStep[0m  [70/106], [94mLoss[0m : 1.71122
[1mStep[0m  [80/106], [94mLoss[0m : 1.97723
[1mStep[0m  [90/106], [94mLoss[0m : 2.05187
[1mStep[0m  [100/106], [94mLoss[0m : 2.16527

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.984, [92mTest[0m: 2.486, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.87921
[1mStep[0m  [10/106], [94mLoss[0m : 1.96055
[1mStep[0m  [20/106], [94mLoss[0m : 1.98516
[1mStep[0m  [30/106], [94mLoss[0m : 1.86278
[1mStep[0m  [40/106], [94mLoss[0m : 1.95332
[1mStep[0m  [50/106], [94mLoss[0m : 2.18017
[1mStep[0m  [60/106], [94mLoss[0m : 1.84950
[1mStep[0m  [70/106], [94mLoss[0m : 1.69806
[1mStep[0m  [80/106], [94mLoss[0m : 2.01706
[1mStep[0m  [90/106], [94mLoss[0m : 1.87981
[1mStep[0m  [100/106], [94mLoss[0m : 1.96947

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.941, [92mTest[0m: 2.423, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.88482
[1mStep[0m  [10/106], [94mLoss[0m : 1.71177
[1mStep[0m  [20/106], [94mLoss[0m : 1.93606
[1mStep[0m  [30/106], [94mLoss[0m : 1.81542
[1mStep[0m  [40/106], [94mLoss[0m : 2.08788
[1mStep[0m  [50/106], [94mLoss[0m : 2.10078
[1mStep[0m  [60/106], [94mLoss[0m : 1.85335
[1mStep[0m  [70/106], [94mLoss[0m : 1.95903
[1mStep[0m  [80/106], [94mLoss[0m : 1.93706
[1mStep[0m  [90/106], [94mLoss[0m : 1.93978
[1mStep[0m  [100/106], [94mLoss[0m : 1.99045

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.885, [92mTest[0m: 2.435, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.66461
[1mStep[0m  [10/106], [94mLoss[0m : 1.63030
[1mStep[0m  [20/106], [94mLoss[0m : 2.02487
[1mStep[0m  [30/106], [94mLoss[0m : 1.72556
[1mStep[0m  [40/106], [94mLoss[0m : 1.67964
[1mStep[0m  [50/106], [94mLoss[0m : 1.78518
[1mStep[0m  [60/106], [94mLoss[0m : 1.85894
[1mStep[0m  [70/106], [94mLoss[0m : 1.68560
[1mStep[0m  [80/106], [94mLoss[0m : 2.09584
[1mStep[0m  [90/106], [94mLoss[0m : 1.89949
[1mStep[0m  [100/106], [94mLoss[0m : 2.13528

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.846, [92mTest[0m: 2.437, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.61249
[1mStep[0m  [10/106], [94mLoss[0m : 1.81318
[1mStep[0m  [20/106], [94mLoss[0m : 1.46826
[1mStep[0m  [30/106], [94mLoss[0m : 1.75986
[1mStep[0m  [40/106], [94mLoss[0m : 1.87410
[1mStep[0m  [50/106], [94mLoss[0m : 1.76312
[1mStep[0m  [60/106], [94mLoss[0m : 1.63228
[1mStep[0m  [70/106], [94mLoss[0m : 1.71831
[1mStep[0m  [80/106], [94mLoss[0m : 1.77236
[1mStep[0m  [90/106], [94mLoss[0m : 1.92978
[1mStep[0m  [100/106], [94mLoss[0m : 1.69890

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.806, [92mTest[0m: 2.440, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.81195
[1mStep[0m  [10/106], [94mLoss[0m : 1.65360
[1mStep[0m  [20/106], [94mLoss[0m : 1.69159
[1mStep[0m  [30/106], [94mLoss[0m : 1.61831
[1mStep[0m  [40/106], [94mLoss[0m : 1.65642
[1mStep[0m  [50/106], [94mLoss[0m : 1.91205
[1mStep[0m  [60/106], [94mLoss[0m : 1.58015
[1mStep[0m  [70/106], [94mLoss[0m : 1.90584
[1mStep[0m  [80/106], [94mLoss[0m : 1.76564
[1mStep[0m  [90/106], [94mLoss[0m : 1.87912
[1mStep[0m  [100/106], [94mLoss[0m : 1.70375

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.765, [92mTest[0m: 2.485, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.70679
[1mStep[0m  [10/106], [94mLoss[0m : 1.73962
[1mStep[0m  [20/106], [94mLoss[0m : 1.56367
[1mStep[0m  [30/106], [94mLoss[0m : 1.49891
[1mStep[0m  [40/106], [94mLoss[0m : 1.91038
[1mStep[0m  [50/106], [94mLoss[0m : 1.75280
[1mStep[0m  [60/106], [94mLoss[0m : 1.92416
[1mStep[0m  [70/106], [94mLoss[0m : 1.60891
[1mStep[0m  [80/106], [94mLoss[0m : 1.70572
[1mStep[0m  [90/106], [94mLoss[0m : 1.61860
[1mStep[0m  [100/106], [94mLoss[0m : 1.78598

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.729, [92mTest[0m: 2.437, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.52570
[1mStep[0m  [10/106], [94mLoss[0m : 1.54645
[1mStep[0m  [20/106], [94mLoss[0m : 1.54100
[1mStep[0m  [30/106], [94mLoss[0m : 1.77920
[1mStep[0m  [40/106], [94mLoss[0m : 1.59649
[1mStep[0m  [50/106], [94mLoss[0m : 1.70224
[1mStep[0m  [60/106], [94mLoss[0m : 1.91735
[1mStep[0m  [70/106], [94mLoss[0m : 1.66090
[1mStep[0m  [80/106], [94mLoss[0m : 1.81560
[1mStep[0m  [90/106], [94mLoss[0m : 1.72677
[1mStep[0m  [100/106], [94mLoss[0m : 1.64749

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.685, [92mTest[0m: 2.550, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.58372
[1mStep[0m  [10/106], [94mLoss[0m : 1.63384
[1mStep[0m  [20/106], [94mLoss[0m : 1.75370
[1mStep[0m  [30/106], [94mLoss[0m : 1.76599
[1mStep[0m  [40/106], [94mLoss[0m : 1.57981
[1mStep[0m  [50/106], [94mLoss[0m : 1.51598
[1mStep[0m  [60/106], [94mLoss[0m : 1.55051
[1mStep[0m  [70/106], [94mLoss[0m : 1.61553
[1mStep[0m  [80/106], [94mLoss[0m : 1.78251
[1mStep[0m  [90/106], [94mLoss[0m : 1.67825
[1mStep[0m  [100/106], [94mLoss[0m : 1.52453

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.654, [92mTest[0m: 2.554, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.69497
[1mStep[0m  [10/106], [94mLoss[0m : 1.61563
[1mStep[0m  [20/106], [94mLoss[0m : 1.51349
[1mStep[0m  [30/106], [94mLoss[0m : 1.67158
[1mStep[0m  [40/106], [94mLoss[0m : 1.69061
[1mStep[0m  [50/106], [94mLoss[0m : 1.55351
[1mStep[0m  [60/106], [94mLoss[0m : 1.64476
[1mStep[0m  [70/106], [94mLoss[0m : 1.62806
[1mStep[0m  [80/106], [94mLoss[0m : 1.62311
[1mStep[0m  [90/106], [94mLoss[0m : 1.64922
[1mStep[0m  [100/106], [94mLoss[0m : 1.53871

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.624, [92mTest[0m: 2.459, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.49600
[1mStep[0m  [10/106], [94mLoss[0m : 1.58007
[1mStep[0m  [20/106], [94mLoss[0m : 1.56298
[1mStep[0m  [30/106], [94mLoss[0m : 1.49413
[1mStep[0m  [40/106], [94mLoss[0m : 1.58927
[1mStep[0m  [50/106], [94mLoss[0m : 1.68185
[1mStep[0m  [60/106], [94mLoss[0m : 1.50743
[1mStep[0m  [70/106], [94mLoss[0m : 1.64552
[1mStep[0m  [80/106], [94mLoss[0m : 1.64179
[1mStep[0m  [90/106], [94mLoss[0m : 1.88569
[1mStep[0m  [100/106], [94mLoss[0m : 1.59274

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.602, [92mTest[0m: 2.449, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.71485
[1mStep[0m  [10/106], [94mLoss[0m : 1.26086
[1mStep[0m  [20/106], [94mLoss[0m : 1.47243
[1mStep[0m  [30/106], [94mLoss[0m : 1.72370
[1mStep[0m  [40/106], [94mLoss[0m : 1.45617
[1mStep[0m  [50/106], [94mLoss[0m : 1.59590
[1mStep[0m  [60/106], [94mLoss[0m : 1.50606
[1mStep[0m  [70/106], [94mLoss[0m : 1.67431
[1mStep[0m  [80/106], [94mLoss[0m : 1.52412
[1mStep[0m  [90/106], [94mLoss[0m : 1.35886
[1mStep[0m  [100/106], [94mLoss[0m : 1.50869

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.574, [92mTest[0m: 2.484, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.30068
[1mStep[0m  [10/106], [94mLoss[0m : 1.49515
[1mStep[0m  [20/106], [94mLoss[0m : 1.58420
[1mStep[0m  [30/106], [94mLoss[0m : 1.41317
[1mStep[0m  [40/106], [94mLoss[0m : 1.51352
[1mStep[0m  [50/106], [94mLoss[0m : 1.47181
[1mStep[0m  [60/106], [94mLoss[0m : 1.52982
[1mStep[0m  [70/106], [94mLoss[0m : 1.63016
[1mStep[0m  [80/106], [94mLoss[0m : 1.34365
[1mStep[0m  [90/106], [94mLoss[0m : 1.56878
[1mStep[0m  [100/106], [94mLoss[0m : 1.66323

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.512, [92mTest[0m: 2.607, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.29275
[1mStep[0m  [10/106], [94mLoss[0m : 1.54661
[1mStep[0m  [20/106], [94mLoss[0m : 1.54949
[1mStep[0m  [30/106], [94mLoss[0m : 1.39639
[1mStep[0m  [40/106], [94mLoss[0m : 1.43300
[1mStep[0m  [50/106], [94mLoss[0m : 1.76142
[1mStep[0m  [60/106], [94mLoss[0m : 1.54878
[1mStep[0m  [70/106], [94mLoss[0m : 1.63885
[1mStep[0m  [80/106], [94mLoss[0m : 1.60844
[1mStep[0m  [90/106], [94mLoss[0m : 1.52119
[1mStep[0m  [100/106], [94mLoss[0m : 1.42456

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.498, [92mTest[0m: 2.513, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.59592
[1mStep[0m  [10/106], [94mLoss[0m : 1.42575
[1mStep[0m  [20/106], [94mLoss[0m : 1.54261
[1mStep[0m  [30/106], [94mLoss[0m : 1.52112
[1mStep[0m  [40/106], [94mLoss[0m : 1.46049
[1mStep[0m  [50/106], [94mLoss[0m : 1.49617
[1mStep[0m  [60/106], [94mLoss[0m : 1.33855
[1mStep[0m  [70/106], [94mLoss[0m : 1.48260
[1mStep[0m  [80/106], [94mLoss[0m : 1.48562
[1mStep[0m  [90/106], [94mLoss[0m : 1.52955
[1mStep[0m  [100/106], [94mLoss[0m : 1.41591

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.468, [92mTest[0m: 2.660, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.35699
[1mStep[0m  [10/106], [94mLoss[0m : 1.53075
[1mStep[0m  [20/106], [94mLoss[0m : 1.62012
[1mStep[0m  [30/106], [94mLoss[0m : 1.52522
[1mStep[0m  [40/106], [94mLoss[0m : 1.36445
[1mStep[0m  [50/106], [94mLoss[0m : 1.61010
[1mStep[0m  [60/106], [94mLoss[0m : 1.67588
[1mStep[0m  [70/106], [94mLoss[0m : 1.36149
[1mStep[0m  [80/106], [94mLoss[0m : 1.64566
[1mStep[0m  [90/106], [94mLoss[0m : 1.48979
[1mStep[0m  [100/106], [94mLoss[0m : 1.65201

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.455, [92mTest[0m: 2.514, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.32627
[1mStep[0m  [10/106], [94mLoss[0m : 1.38433
[1mStep[0m  [20/106], [94mLoss[0m : 1.54306
[1mStep[0m  [30/106], [94mLoss[0m : 1.51847
[1mStep[0m  [40/106], [94mLoss[0m : 1.36775
[1mStep[0m  [50/106], [94mLoss[0m : 1.45228
[1mStep[0m  [60/106], [94mLoss[0m : 1.31145
[1mStep[0m  [70/106], [94mLoss[0m : 1.27488
[1mStep[0m  [80/106], [94mLoss[0m : 1.44511
[1mStep[0m  [90/106], [94mLoss[0m : 1.41210
[1mStep[0m  [100/106], [94mLoss[0m : 1.35675

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.428, [92mTest[0m: 2.464, [96mlr[0m: 0.004545
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 24 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.506
====================================

Phase 2 - Evaluation MAE:  2.505895529153212
MAE score P1      2.392964
MAE score P2      2.505896
loss              1.428289
learning_rate      0.00505
batch_size             128
hidden_sizes         [250]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping        True
dropout                0.2
momentum               0.5
weight_decay         0.001
Name: 13, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/106], [94mLoss[0m : 11.14795
[1mStep[0m  [10/106], [94mLoss[0m : 5.06463
[1mStep[0m  [20/106], [94mLoss[0m : 2.98432
[1mStep[0m  [30/106], [94mLoss[0m : 2.61778
[1mStep[0m  [40/106], [94mLoss[0m : 2.76042
[1mStep[0m  [50/106], [94mLoss[0m : 2.25867
[1mStep[0m  [60/106], [94mLoss[0m : 2.50100
[1mStep[0m  [70/106], [94mLoss[0m : 2.37104
[1mStep[0m  [80/106], [94mLoss[0m : 2.64449
[1mStep[0m  [90/106], [94mLoss[0m : 2.48060
[1mStep[0m  [100/106], [94mLoss[0m : 2.60568

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.259, [92mTest[0m: 10.551, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.46328
[1mStep[0m  [10/106], [94mLoss[0m : 2.46630
[1mStep[0m  [20/106], [94mLoss[0m : 2.17714
[1mStep[0m  [30/106], [94mLoss[0m : 2.20001
[1mStep[0m  [40/106], [94mLoss[0m : 2.87167
[1mStep[0m  [50/106], [94mLoss[0m : 2.38618
[1mStep[0m  [60/106], [94mLoss[0m : 2.53623
[1mStep[0m  [70/106], [94mLoss[0m : 2.27216
[1mStep[0m  [80/106], [94mLoss[0m : 2.72841
[1mStep[0m  [90/106], [94mLoss[0m : 2.81928
[1mStep[0m  [100/106], [94mLoss[0m : 2.60483

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.520, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.24465
[1mStep[0m  [10/106], [94mLoss[0m : 2.69817
[1mStep[0m  [20/106], [94mLoss[0m : 2.75800
[1mStep[0m  [30/106], [94mLoss[0m : 2.98987
[1mStep[0m  [40/106], [94mLoss[0m : 2.64909
[1mStep[0m  [50/106], [94mLoss[0m : 2.10660
[1mStep[0m  [60/106], [94mLoss[0m : 2.41723
[1mStep[0m  [70/106], [94mLoss[0m : 2.51603
[1mStep[0m  [80/106], [94mLoss[0m : 2.57879
[1mStep[0m  [90/106], [94mLoss[0m : 2.59253
[1mStep[0m  [100/106], [94mLoss[0m : 2.35900

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.468, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.66016
[1mStep[0m  [10/106], [94mLoss[0m : 2.29390
[1mStep[0m  [20/106], [94mLoss[0m : 2.61628
[1mStep[0m  [30/106], [94mLoss[0m : 2.58095
[1mStep[0m  [40/106], [94mLoss[0m : 2.63001
[1mStep[0m  [50/106], [94mLoss[0m : 2.42938
[1mStep[0m  [60/106], [94mLoss[0m : 2.73355
[1mStep[0m  [70/106], [94mLoss[0m : 2.77927
[1mStep[0m  [80/106], [94mLoss[0m : 2.53388
[1mStep[0m  [90/106], [94mLoss[0m : 2.49673
[1mStep[0m  [100/106], [94mLoss[0m : 2.24921

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.468, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.56236
[1mStep[0m  [10/106], [94mLoss[0m : 2.26897
[1mStep[0m  [20/106], [94mLoss[0m : 2.62758
[1mStep[0m  [30/106], [94mLoss[0m : 2.64329
[1mStep[0m  [40/106], [94mLoss[0m : 2.62144
[1mStep[0m  [50/106], [94mLoss[0m : 2.39161
[1mStep[0m  [60/106], [94mLoss[0m : 2.45004
[1mStep[0m  [70/106], [94mLoss[0m : 2.30630
[1mStep[0m  [80/106], [94mLoss[0m : 2.35187
[1mStep[0m  [90/106], [94mLoss[0m : 2.28225
[1mStep[0m  [100/106], [94mLoss[0m : 2.55533

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.445, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.41903
[1mStep[0m  [10/106], [94mLoss[0m : 2.17283
[1mStep[0m  [20/106], [94mLoss[0m : 2.20427
[1mStep[0m  [30/106], [94mLoss[0m : 2.53533
[1mStep[0m  [40/106], [94mLoss[0m : 2.39232
[1mStep[0m  [50/106], [94mLoss[0m : 2.77177
[1mStep[0m  [60/106], [94mLoss[0m : 2.46920
[1mStep[0m  [70/106], [94mLoss[0m : 2.43314
[1mStep[0m  [80/106], [94mLoss[0m : 2.48092
[1mStep[0m  [90/106], [94mLoss[0m : 2.41235
[1mStep[0m  [100/106], [94mLoss[0m : 2.06370

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.426, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.63916
[1mStep[0m  [10/106], [94mLoss[0m : 2.26717
[1mStep[0m  [20/106], [94mLoss[0m : 2.34668
[1mStep[0m  [30/106], [94mLoss[0m : 2.18589
[1mStep[0m  [40/106], [94mLoss[0m : 2.32147
[1mStep[0m  [50/106], [94mLoss[0m : 2.57405
[1mStep[0m  [60/106], [94mLoss[0m : 2.50095
[1mStep[0m  [70/106], [94mLoss[0m : 2.54821
[1mStep[0m  [80/106], [94mLoss[0m : 2.31920
[1mStep[0m  [90/106], [94mLoss[0m : 2.34810
[1mStep[0m  [100/106], [94mLoss[0m : 2.40834

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.437, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.57355
[1mStep[0m  [10/106], [94mLoss[0m : 2.17403
[1mStep[0m  [20/106], [94mLoss[0m : 2.84439
[1mStep[0m  [30/106], [94mLoss[0m : 2.31475
[1mStep[0m  [40/106], [94mLoss[0m : 2.82038
[1mStep[0m  [50/106], [94mLoss[0m : 2.40867
[1mStep[0m  [60/106], [94mLoss[0m : 2.54496
[1mStep[0m  [70/106], [94mLoss[0m : 2.68166
[1mStep[0m  [80/106], [94mLoss[0m : 2.15882
[1mStep[0m  [90/106], [94mLoss[0m : 2.51352
[1mStep[0m  [100/106], [94mLoss[0m : 2.58263

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.415, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.40348
[1mStep[0m  [10/106], [94mLoss[0m : 2.59205
[1mStep[0m  [20/106], [94mLoss[0m : 2.44103
[1mStep[0m  [30/106], [94mLoss[0m : 2.49837
[1mStep[0m  [40/106], [94mLoss[0m : 2.43857
[1mStep[0m  [50/106], [94mLoss[0m : 2.51230
[1mStep[0m  [60/106], [94mLoss[0m : 2.27811
[1mStep[0m  [70/106], [94mLoss[0m : 2.30661
[1mStep[0m  [80/106], [94mLoss[0m : 2.27434
[1mStep[0m  [90/106], [94mLoss[0m : 2.54822
[1mStep[0m  [100/106], [94mLoss[0m : 2.16740

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.427, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.22154
[1mStep[0m  [10/106], [94mLoss[0m : 2.54400
[1mStep[0m  [20/106], [94mLoss[0m : 2.36670
[1mStep[0m  [30/106], [94mLoss[0m : 2.19936
[1mStep[0m  [40/106], [94mLoss[0m : 2.56463
[1mStep[0m  [50/106], [94mLoss[0m : 2.58435
[1mStep[0m  [60/106], [94mLoss[0m : 2.43926
[1mStep[0m  [70/106], [94mLoss[0m : 2.44453
[1mStep[0m  [80/106], [94mLoss[0m : 2.38648
[1mStep[0m  [90/106], [94mLoss[0m : 2.65832
[1mStep[0m  [100/106], [94mLoss[0m : 2.61166

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.417, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.23534
[1mStep[0m  [10/106], [94mLoss[0m : 2.45366
[1mStep[0m  [20/106], [94mLoss[0m : 2.36482
[1mStep[0m  [30/106], [94mLoss[0m : 2.51916
[1mStep[0m  [40/106], [94mLoss[0m : 2.42300
[1mStep[0m  [50/106], [94mLoss[0m : 2.44403
[1mStep[0m  [60/106], [94mLoss[0m : 2.52775
[1mStep[0m  [70/106], [94mLoss[0m : 2.53129
[1mStep[0m  [80/106], [94mLoss[0m : 2.31065
[1mStep[0m  [90/106], [94mLoss[0m : 2.66853
[1mStep[0m  [100/106], [94mLoss[0m : 2.34675

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.427, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.50673
[1mStep[0m  [10/106], [94mLoss[0m : 2.67989
[1mStep[0m  [20/106], [94mLoss[0m : 2.46697
[1mStep[0m  [30/106], [94mLoss[0m : 2.40307
[1mStep[0m  [40/106], [94mLoss[0m : 2.42966
[1mStep[0m  [50/106], [94mLoss[0m : 2.49324
[1mStep[0m  [60/106], [94mLoss[0m : 2.18731
[1mStep[0m  [70/106], [94mLoss[0m : 2.30399
[1mStep[0m  [80/106], [94mLoss[0m : 2.42985
[1mStep[0m  [90/106], [94mLoss[0m : 2.28099
[1mStep[0m  [100/106], [94mLoss[0m : 2.24389

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.412, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.57510
[1mStep[0m  [10/106], [94mLoss[0m : 2.42569
[1mStep[0m  [20/106], [94mLoss[0m : 2.61441
[1mStep[0m  [30/106], [94mLoss[0m : 2.42015
[1mStep[0m  [40/106], [94mLoss[0m : 2.44533
[1mStep[0m  [50/106], [94mLoss[0m : 2.50511
[1mStep[0m  [60/106], [94mLoss[0m : 2.61884
[1mStep[0m  [70/106], [94mLoss[0m : 2.15516
[1mStep[0m  [80/106], [94mLoss[0m : 2.49865
[1mStep[0m  [90/106], [94mLoss[0m : 2.52442
[1mStep[0m  [100/106], [94mLoss[0m : 2.31893

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.428, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.15939
[1mStep[0m  [10/106], [94mLoss[0m : 2.37460
[1mStep[0m  [20/106], [94mLoss[0m : 2.74127
[1mStep[0m  [30/106], [94mLoss[0m : 2.35342
[1mStep[0m  [40/106], [94mLoss[0m : 2.50964
[1mStep[0m  [50/106], [94mLoss[0m : 2.41075
[1mStep[0m  [60/106], [94mLoss[0m : 2.44108
[1mStep[0m  [70/106], [94mLoss[0m : 2.36053
[1mStep[0m  [80/106], [94mLoss[0m : 2.34081
[1mStep[0m  [90/106], [94mLoss[0m : 2.51628
[1mStep[0m  [100/106], [94mLoss[0m : 2.42515

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.417, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.65205
[1mStep[0m  [10/106], [94mLoss[0m : 2.41060
[1mStep[0m  [20/106], [94mLoss[0m : 2.24579
[1mStep[0m  [30/106], [94mLoss[0m : 2.43914
[1mStep[0m  [40/106], [94mLoss[0m : 2.47254
[1mStep[0m  [50/106], [94mLoss[0m : 2.76339
[1mStep[0m  [60/106], [94mLoss[0m : 2.07874
[1mStep[0m  [70/106], [94mLoss[0m : 2.23624
[1mStep[0m  [80/106], [94mLoss[0m : 2.50654
[1mStep[0m  [90/106], [94mLoss[0m : 2.63152
[1mStep[0m  [100/106], [94mLoss[0m : 2.31187

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.412, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.43325
[1mStep[0m  [10/106], [94mLoss[0m : 2.24770
[1mStep[0m  [20/106], [94mLoss[0m : 2.42882
[1mStep[0m  [30/106], [94mLoss[0m : 2.26566
[1mStep[0m  [40/106], [94mLoss[0m : 2.35331
[1mStep[0m  [50/106], [94mLoss[0m : 2.48663
[1mStep[0m  [60/106], [94mLoss[0m : 2.82940
[1mStep[0m  [70/106], [94mLoss[0m : 2.15509
[1mStep[0m  [80/106], [94mLoss[0m : 2.39098
[1mStep[0m  [90/106], [94mLoss[0m : 2.58506
[1mStep[0m  [100/106], [94mLoss[0m : 2.74192

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.407, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.15899
[1mStep[0m  [10/106], [94mLoss[0m : 2.39778
[1mStep[0m  [20/106], [94mLoss[0m : 2.43717
[1mStep[0m  [30/106], [94mLoss[0m : 2.37970
[1mStep[0m  [40/106], [94mLoss[0m : 2.40325
[1mStep[0m  [50/106], [94mLoss[0m : 2.51893
[1mStep[0m  [60/106], [94mLoss[0m : 2.31070
[1mStep[0m  [70/106], [94mLoss[0m : 2.43980
[1mStep[0m  [80/106], [94mLoss[0m : 2.54868
[1mStep[0m  [90/106], [94mLoss[0m : 2.59390
[1mStep[0m  [100/106], [94mLoss[0m : 2.48110

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.414, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.27303
[1mStep[0m  [10/106], [94mLoss[0m : 2.32186
[1mStep[0m  [20/106], [94mLoss[0m : 2.33917
[1mStep[0m  [30/106], [94mLoss[0m : 2.44694
[1mStep[0m  [40/106], [94mLoss[0m : 2.43285
[1mStep[0m  [50/106], [94mLoss[0m : 2.37126
[1mStep[0m  [60/106], [94mLoss[0m : 2.14229
[1mStep[0m  [70/106], [94mLoss[0m : 2.63628
[1mStep[0m  [80/106], [94mLoss[0m : 2.48192
[1mStep[0m  [90/106], [94mLoss[0m : 2.25669
[1mStep[0m  [100/106], [94mLoss[0m : 2.41699

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.413, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.54880
[1mStep[0m  [10/106], [94mLoss[0m : 2.34300
[1mStep[0m  [20/106], [94mLoss[0m : 2.75836
[1mStep[0m  [30/106], [94mLoss[0m : 2.43404
[1mStep[0m  [40/106], [94mLoss[0m : 2.29096
[1mStep[0m  [50/106], [94mLoss[0m : 2.70320
[1mStep[0m  [60/106], [94mLoss[0m : 2.76210
[1mStep[0m  [70/106], [94mLoss[0m : 2.48506
[1mStep[0m  [80/106], [94mLoss[0m : 2.50995
[1mStep[0m  [90/106], [94mLoss[0m : 2.14123
[1mStep[0m  [100/106], [94mLoss[0m : 2.36959

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.417, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.39407
[1mStep[0m  [10/106], [94mLoss[0m : 2.62579
[1mStep[0m  [20/106], [94mLoss[0m : 2.45706
[1mStep[0m  [30/106], [94mLoss[0m : 2.57731
[1mStep[0m  [40/106], [94mLoss[0m : 2.43557
[1mStep[0m  [50/106], [94mLoss[0m : 2.72360
[1mStep[0m  [60/106], [94mLoss[0m : 2.47238
[1mStep[0m  [70/106], [94mLoss[0m : 2.44061
[1mStep[0m  [80/106], [94mLoss[0m : 2.84473
[1mStep[0m  [90/106], [94mLoss[0m : 2.34169
[1mStep[0m  [100/106], [94mLoss[0m : 2.32032

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.410, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.40130
[1mStep[0m  [10/106], [94mLoss[0m : 2.61817
[1mStep[0m  [20/106], [94mLoss[0m : 2.35438
[1mStep[0m  [30/106], [94mLoss[0m : 2.86108
[1mStep[0m  [40/106], [94mLoss[0m : 2.40121
[1mStep[0m  [50/106], [94mLoss[0m : 2.80106
[1mStep[0m  [60/106], [94mLoss[0m : 2.33542
[1mStep[0m  [70/106], [94mLoss[0m : 2.54567
[1mStep[0m  [80/106], [94mLoss[0m : 2.44874
[1mStep[0m  [90/106], [94mLoss[0m : 2.36850
[1mStep[0m  [100/106], [94mLoss[0m : 2.47132

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.409, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.60859
[1mStep[0m  [10/106], [94mLoss[0m : 2.29124
[1mStep[0m  [20/106], [94mLoss[0m : 2.29451
[1mStep[0m  [30/106], [94mLoss[0m : 2.20142
[1mStep[0m  [40/106], [94mLoss[0m : 2.62378
[1mStep[0m  [50/106], [94mLoss[0m : 2.50647
[1mStep[0m  [60/106], [94mLoss[0m : 2.80309
[1mStep[0m  [70/106], [94mLoss[0m : 2.48882
[1mStep[0m  [80/106], [94mLoss[0m : 2.43735
[1mStep[0m  [90/106], [94mLoss[0m : 2.69258
[1mStep[0m  [100/106], [94mLoss[0m : 2.50858

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.412, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.57320
[1mStep[0m  [10/106], [94mLoss[0m : 2.83867
[1mStep[0m  [20/106], [94mLoss[0m : 2.28726
[1mStep[0m  [30/106], [94mLoss[0m : 2.49108
[1mStep[0m  [40/106], [94mLoss[0m : 2.68037
[1mStep[0m  [50/106], [94mLoss[0m : 2.59568
[1mStep[0m  [60/106], [94mLoss[0m : 2.33612
[1mStep[0m  [70/106], [94mLoss[0m : 2.32355
[1mStep[0m  [80/106], [94mLoss[0m : 2.46539
[1mStep[0m  [90/106], [94mLoss[0m : 2.40626
[1mStep[0m  [100/106], [94mLoss[0m : 2.42291

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.401, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.45071
[1mStep[0m  [10/106], [94mLoss[0m : 2.39942
[1mStep[0m  [20/106], [94mLoss[0m : 2.36239
[1mStep[0m  [30/106], [94mLoss[0m : 2.51880
[1mStep[0m  [40/106], [94mLoss[0m : 2.53436
[1mStep[0m  [50/106], [94mLoss[0m : 2.79468
[1mStep[0m  [60/106], [94mLoss[0m : 2.65190
[1mStep[0m  [70/106], [94mLoss[0m : 2.56254
[1mStep[0m  [80/106], [94mLoss[0m : 2.25376
[1mStep[0m  [90/106], [94mLoss[0m : 2.22661
[1mStep[0m  [100/106], [94mLoss[0m : 2.63630

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.404, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.37079
[1mStep[0m  [10/106], [94mLoss[0m : 2.46795
[1mStep[0m  [20/106], [94mLoss[0m : 2.39351
[1mStep[0m  [30/106], [94mLoss[0m : 2.42325
[1mStep[0m  [40/106], [94mLoss[0m : 2.22164
[1mStep[0m  [50/106], [94mLoss[0m : 2.56188
[1mStep[0m  [60/106], [94mLoss[0m : 2.62146
[1mStep[0m  [70/106], [94mLoss[0m : 2.50224
[1mStep[0m  [80/106], [94mLoss[0m : 2.18406
[1mStep[0m  [90/106], [94mLoss[0m : 2.58544
[1mStep[0m  [100/106], [94mLoss[0m : 2.43199

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.402, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.38143
[1mStep[0m  [10/106], [94mLoss[0m : 2.58680
[1mStep[0m  [20/106], [94mLoss[0m : 2.28837
[1mStep[0m  [30/106], [94mLoss[0m : 2.45890
[1mStep[0m  [40/106], [94mLoss[0m : 2.52825
[1mStep[0m  [50/106], [94mLoss[0m : 2.64709
[1mStep[0m  [60/106], [94mLoss[0m : 2.36858
[1mStep[0m  [70/106], [94mLoss[0m : 2.51987
[1mStep[0m  [80/106], [94mLoss[0m : 2.64744
[1mStep[0m  [90/106], [94mLoss[0m : 2.33613
[1mStep[0m  [100/106], [94mLoss[0m : 2.20710

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.402, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.73452
[1mStep[0m  [10/106], [94mLoss[0m : 2.38417
[1mStep[0m  [20/106], [94mLoss[0m : 2.15587
[1mStep[0m  [30/106], [94mLoss[0m : 2.25123
[1mStep[0m  [40/106], [94mLoss[0m : 2.68004
[1mStep[0m  [50/106], [94mLoss[0m : 2.45668
[1mStep[0m  [60/106], [94mLoss[0m : 2.27134
[1mStep[0m  [70/106], [94mLoss[0m : 2.14189
[1mStep[0m  [80/106], [94mLoss[0m : 2.44725
[1mStep[0m  [90/106], [94mLoss[0m : 2.59555
[1mStep[0m  [100/106], [94mLoss[0m : 2.14153

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.401, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.51863
[1mStep[0m  [10/106], [94mLoss[0m : 2.40003
[1mStep[0m  [20/106], [94mLoss[0m : 2.62216
[1mStep[0m  [30/106], [94mLoss[0m : 2.49287
[1mStep[0m  [40/106], [94mLoss[0m : 2.74210
[1mStep[0m  [50/106], [94mLoss[0m : 2.38669
[1mStep[0m  [60/106], [94mLoss[0m : 2.43007
[1mStep[0m  [70/106], [94mLoss[0m : 2.08001
[1mStep[0m  [80/106], [94mLoss[0m : 2.53073
[1mStep[0m  [90/106], [94mLoss[0m : 2.30881
[1mStep[0m  [100/106], [94mLoss[0m : 2.29642

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.409, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.56064
[1mStep[0m  [10/106], [94mLoss[0m : 2.29519
[1mStep[0m  [20/106], [94mLoss[0m : 2.33650
[1mStep[0m  [30/106], [94mLoss[0m : 2.10937
[1mStep[0m  [40/106], [94mLoss[0m : 2.72837
[1mStep[0m  [50/106], [94mLoss[0m : 2.65368
[1mStep[0m  [60/106], [94mLoss[0m : 2.22350
[1mStep[0m  [70/106], [94mLoss[0m : 2.27852
[1mStep[0m  [80/106], [94mLoss[0m : 2.46572
[1mStep[0m  [90/106], [94mLoss[0m : 2.36787
[1mStep[0m  [100/106], [94mLoss[0m : 2.24404

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.398, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.39890
[1mStep[0m  [10/106], [94mLoss[0m : 2.23285
[1mStep[0m  [20/106], [94mLoss[0m : 2.29127
[1mStep[0m  [30/106], [94mLoss[0m : 2.25847
[1mStep[0m  [40/106], [94mLoss[0m : 2.52863
[1mStep[0m  [50/106], [94mLoss[0m : 2.45280
[1mStep[0m  [60/106], [94mLoss[0m : 2.30647
[1mStep[0m  [70/106], [94mLoss[0m : 2.42942
[1mStep[0m  [80/106], [94mLoss[0m : 2.49436
[1mStep[0m  [90/106], [94mLoss[0m : 2.25816
[1mStep[0m  [100/106], [94mLoss[0m : 2.37865

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.405, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.402
====================================

Phase 1 - Evaluation MAE:  2.402267707968658
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/106], [94mLoss[0m : 2.26774
[1mStep[0m  [10/106], [94mLoss[0m : 2.52987
[1mStep[0m  [20/106], [94mLoss[0m : 2.28150
[1mStep[0m  [30/106], [94mLoss[0m : 2.16565
[1mStep[0m  [40/106], [94mLoss[0m : 2.44624
[1mStep[0m  [50/106], [94mLoss[0m : 2.10137
[1mStep[0m  [60/106], [94mLoss[0m : 2.71072
[1mStep[0m  [70/106], [94mLoss[0m : 2.47838
[1mStep[0m  [80/106], [94mLoss[0m : 2.34229
[1mStep[0m  [90/106], [94mLoss[0m : 2.30188
[1mStep[0m  [100/106], [94mLoss[0m : 2.25374

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.404, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.55745
[1mStep[0m  [10/106], [94mLoss[0m : 2.32801
[1mStep[0m  [20/106], [94mLoss[0m : 2.66072
[1mStep[0m  [30/106], [94mLoss[0m : 2.69515
[1mStep[0m  [40/106], [94mLoss[0m : 2.48438
[1mStep[0m  [50/106], [94mLoss[0m : 2.63781
[1mStep[0m  [60/106], [94mLoss[0m : 2.20747
[1mStep[0m  [70/106], [94mLoss[0m : 2.52397
[1mStep[0m  [80/106], [94mLoss[0m : 1.98967
[1mStep[0m  [90/106], [94mLoss[0m : 2.30702
[1mStep[0m  [100/106], [94mLoss[0m : 2.20502

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.392, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.26696
[1mStep[0m  [10/106], [94mLoss[0m : 2.25281
[1mStep[0m  [20/106], [94mLoss[0m : 2.45599
[1mStep[0m  [30/106], [94mLoss[0m : 2.51669
[1mStep[0m  [40/106], [94mLoss[0m : 2.09970
[1mStep[0m  [50/106], [94mLoss[0m : 2.25751
[1mStep[0m  [60/106], [94mLoss[0m : 2.44418
[1mStep[0m  [70/106], [94mLoss[0m : 2.24759
[1mStep[0m  [80/106], [94mLoss[0m : 2.56788
[1mStep[0m  [90/106], [94mLoss[0m : 2.46228
[1mStep[0m  [100/106], [94mLoss[0m : 2.20678

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.374, [92mTest[0m: 2.441, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.06855
[1mStep[0m  [10/106], [94mLoss[0m : 2.28204
[1mStep[0m  [20/106], [94mLoss[0m : 2.56377
[1mStep[0m  [30/106], [94mLoss[0m : 2.32614
[1mStep[0m  [40/106], [94mLoss[0m : 2.19566
[1mStep[0m  [50/106], [94mLoss[0m : 2.56680
[1mStep[0m  [60/106], [94mLoss[0m : 2.12094
[1mStep[0m  [70/106], [94mLoss[0m : 2.18206
[1mStep[0m  [80/106], [94mLoss[0m : 2.03676
[1mStep[0m  [90/106], [94mLoss[0m : 2.29403
[1mStep[0m  [100/106], [94mLoss[0m : 2.26631

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.326, [92mTest[0m: 2.418, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.28726
[1mStep[0m  [10/106], [94mLoss[0m : 2.19983
[1mStep[0m  [20/106], [94mLoss[0m : 2.14334
[1mStep[0m  [30/106], [94mLoss[0m : 2.14103
[1mStep[0m  [40/106], [94mLoss[0m : 2.22819
[1mStep[0m  [50/106], [94mLoss[0m : 2.49792
[1mStep[0m  [60/106], [94mLoss[0m : 2.07882
[1mStep[0m  [70/106], [94mLoss[0m : 2.34390
[1mStep[0m  [80/106], [94mLoss[0m : 2.59778
[1mStep[0m  [90/106], [94mLoss[0m : 1.82783
[1mStep[0m  [100/106], [94mLoss[0m : 2.14535

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.273, [92mTest[0m: 2.432, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.07516
[1mStep[0m  [10/106], [94mLoss[0m : 2.25514
[1mStep[0m  [20/106], [94mLoss[0m : 2.03395
[1mStep[0m  [30/106], [94mLoss[0m : 2.02246
[1mStep[0m  [40/106], [94mLoss[0m : 2.15861
[1mStep[0m  [50/106], [94mLoss[0m : 2.38977
[1mStep[0m  [60/106], [94mLoss[0m : 2.09233
[1mStep[0m  [70/106], [94mLoss[0m : 2.13724
[1mStep[0m  [80/106], [94mLoss[0m : 2.37214
[1mStep[0m  [90/106], [94mLoss[0m : 2.19047
[1mStep[0m  [100/106], [94mLoss[0m : 2.34673

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.211, [92mTest[0m: 2.441, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.08909
[1mStep[0m  [10/106], [94mLoss[0m : 2.41384
[1mStep[0m  [20/106], [94mLoss[0m : 2.13532
[1mStep[0m  [30/106], [94mLoss[0m : 1.84179
[1mStep[0m  [40/106], [94mLoss[0m : 2.49724
[1mStep[0m  [50/106], [94mLoss[0m : 2.16728
[1mStep[0m  [60/106], [94mLoss[0m : 2.03062
[1mStep[0m  [70/106], [94mLoss[0m : 2.08751
[1mStep[0m  [80/106], [94mLoss[0m : 1.85004
[1mStep[0m  [90/106], [94mLoss[0m : 1.98383
[1mStep[0m  [100/106], [94mLoss[0m : 2.37133

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.187, [92mTest[0m: 2.428, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.62958
[1mStep[0m  [10/106], [94mLoss[0m : 2.01729
[1mStep[0m  [20/106], [94mLoss[0m : 2.16738
[1mStep[0m  [30/106], [94mLoss[0m : 2.24508
[1mStep[0m  [40/106], [94mLoss[0m : 1.93587
[1mStep[0m  [50/106], [94mLoss[0m : 2.20043
[1mStep[0m  [60/106], [94mLoss[0m : 2.27722
[1mStep[0m  [70/106], [94mLoss[0m : 2.09070
[1mStep[0m  [80/106], [94mLoss[0m : 1.95180
[1mStep[0m  [90/106], [94mLoss[0m : 2.16981
[1mStep[0m  [100/106], [94mLoss[0m : 2.19644

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.120, [92mTest[0m: 2.374, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.12954
[1mStep[0m  [10/106], [94mLoss[0m : 2.17814
[1mStep[0m  [20/106], [94mLoss[0m : 2.22989
[1mStep[0m  [30/106], [94mLoss[0m : 2.04456
[1mStep[0m  [40/106], [94mLoss[0m : 2.14887
[1mStep[0m  [50/106], [94mLoss[0m : 2.10441
[1mStep[0m  [60/106], [94mLoss[0m : 2.06584
[1mStep[0m  [70/106], [94mLoss[0m : 2.05608
[1mStep[0m  [80/106], [94mLoss[0m : 1.96032
[1mStep[0m  [90/106], [94mLoss[0m : 1.82660
[1mStep[0m  [100/106], [94mLoss[0m : 2.03342

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.071, [92mTest[0m: 2.421, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.01057
[1mStep[0m  [10/106], [94mLoss[0m : 1.79424
[1mStep[0m  [20/106], [94mLoss[0m : 2.23125
[1mStep[0m  [30/106], [94mLoss[0m : 1.93956
[1mStep[0m  [40/106], [94mLoss[0m : 2.14870
[1mStep[0m  [50/106], [94mLoss[0m : 1.99653
[1mStep[0m  [60/106], [94mLoss[0m : 1.95151
[1mStep[0m  [70/106], [94mLoss[0m : 2.04946
[1mStep[0m  [80/106], [94mLoss[0m : 2.08111
[1mStep[0m  [90/106], [94mLoss[0m : 1.73517
[1mStep[0m  [100/106], [94mLoss[0m : 1.83778

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.019, [92mTest[0m: 2.384, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.72242
[1mStep[0m  [10/106], [94mLoss[0m : 2.02474
[1mStep[0m  [20/106], [94mLoss[0m : 2.17404
[1mStep[0m  [30/106], [94mLoss[0m : 1.67362
[1mStep[0m  [40/106], [94mLoss[0m : 2.00810
[1mStep[0m  [50/106], [94mLoss[0m : 2.06142
[1mStep[0m  [60/106], [94mLoss[0m : 1.89782
[1mStep[0m  [70/106], [94mLoss[0m : 2.02740
[1mStep[0m  [80/106], [94mLoss[0m : 1.86692
[1mStep[0m  [90/106], [94mLoss[0m : 2.22591
[1mStep[0m  [100/106], [94mLoss[0m : 1.86194

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.985, [92mTest[0m: 2.373, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.95910
[1mStep[0m  [10/106], [94mLoss[0m : 1.85750
[1mStep[0m  [20/106], [94mLoss[0m : 1.92158
[1mStep[0m  [30/106], [94mLoss[0m : 1.98362
[1mStep[0m  [40/106], [94mLoss[0m : 2.02534
[1mStep[0m  [50/106], [94mLoss[0m : 1.82283
[1mStep[0m  [60/106], [94mLoss[0m : 1.91134
[1mStep[0m  [70/106], [94mLoss[0m : 1.86734
[1mStep[0m  [80/106], [94mLoss[0m : 1.91952
[1mStep[0m  [90/106], [94mLoss[0m : 2.25913
[1mStep[0m  [100/106], [94mLoss[0m : 1.74636

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.921, [92mTest[0m: 2.404, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.61360
[1mStep[0m  [10/106], [94mLoss[0m : 1.95463
[1mStep[0m  [20/106], [94mLoss[0m : 1.65375
[1mStep[0m  [30/106], [94mLoss[0m : 1.99637
[1mStep[0m  [40/106], [94mLoss[0m : 1.80302
[1mStep[0m  [50/106], [94mLoss[0m : 2.09142
[1mStep[0m  [60/106], [94mLoss[0m : 1.87920
[1mStep[0m  [70/106], [94mLoss[0m : 1.82838
[1mStep[0m  [80/106], [94mLoss[0m : 1.82108
[1mStep[0m  [90/106], [94mLoss[0m : 1.94705
[1mStep[0m  [100/106], [94mLoss[0m : 1.71141

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.888, [92mTest[0m: 2.405, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.85518
[1mStep[0m  [10/106], [94mLoss[0m : 1.85267
[1mStep[0m  [20/106], [94mLoss[0m : 1.54031
[1mStep[0m  [30/106], [94mLoss[0m : 1.73693
[1mStep[0m  [40/106], [94mLoss[0m : 1.64095
[1mStep[0m  [50/106], [94mLoss[0m : 1.87564
[1mStep[0m  [60/106], [94mLoss[0m : 1.86337
[1mStep[0m  [70/106], [94mLoss[0m : 1.80244
[1mStep[0m  [80/106], [94mLoss[0m : 1.80168
[1mStep[0m  [90/106], [94mLoss[0m : 1.68613
[1mStep[0m  [100/106], [94mLoss[0m : 1.98193

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.841, [92mTest[0m: 2.465, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.76288
[1mStep[0m  [10/106], [94mLoss[0m : 1.54609
[1mStep[0m  [20/106], [94mLoss[0m : 1.90585
[1mStep[0m  [30/106], [94mLoss[0m : 1.61899
[1mStep[0m  [40/106], [94mLoss[0m : 1.90289
[1mStep[0m  [50/106], [94mLoss[0m : 1.67169
[1mStep[0m  [60/106], [94mLoss[0m : 1.96074
[1mStep[0m  [70/106], [94mLoss[0m : 1.68627
[1mStep[0m  [80/106], [94mLoss[0m : 1.99479
[1mStep[0m  [90/106], [94mLoss[0m : 1.82841
[1mStep[0m  [100/106], [94mLoss[0m : 1.77490

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.816, [92mTest[0m: 2.428, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.73164
[1mStep[0m  [10/106], [94mLoss[0m : 1.87253
[1mStep[0m  [20/106], [94mLoss[0m : 2.04255
[1mStep[0m  [30/106], [94mLoss[0m : 1.96026
[1mStep[0m  [40/106], [94mLoss[0m : 1.69371
[1mStep[0m  [50/106], [94mLoss[0m : 1.75770
[1mStep[0m  [60/106], [94mLoss[0m : 1.71299
[1mStep[0m  [70/106], [94mLoss[0m : 1.85202
[1mStep[0m  [80/106], [94mLoss[0m : 1.82343
[1mStep[0m  [90/106], [94mLoss[0m : 1.88251
[1mStep[0m  [100/106], [94mLoss[0m : 2.02464

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.774, [92mTest[0m: 2.397, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.47782
[1mStep[0m  [10/106], [94mLoss[0m : 1.92478
[1mStep[0m  [20/106], [94mLoss[0m : 1.58154
[1mStep[0m  [30/106], [94mLoss[0m : 1.78228
[1mStep[0m  [40/106], [94mLoss[0m : 1.58767
[1mStep[0m  [50/106], [94mLoss[0m : 1.80170
[1mStep[0m  [60/106], [94mLoss[0m : 1.82139
[1mStep[0m  [70/106], [94mLoss[0m : 1.87739
[1mStep[0m  [80/106], [94mLoss[0m : 1.75055
[1mStep[0m  [90/106], [94mLoss[0m : 1.62261
[1mStep[0m  [100/106], [94mLoss[0m : 1.58928

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.745, [92mTest[0m: 2.412, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.62708
[1mStep[0m  [10/106], [94mLoss[0m : 1.67978
[1mStep[0m  [20/106], [94mLoss[0m : 1.55267
[1mStep[0m  [30/106], [94mLoss[0m : 1.85337
[1mStep[0m  [40/106], [94mLoss[0m : 1.57622
[1mStep[0m  [50/106], [94mLoss[0m : 1.68735
[1mStep[0m  [60/106], [94mLoss[0m : 1.74680
[1mStep[0m  [70/106], [94mLoss[0m : 1.85595
[1mStep[0m  [80/106], [94mLoss[0m : 2.02152
[1mStep[0m  [90/106], [94mLoss[0m : 1.71150
[1mStep[0m  [100/106], [94mLoss[0m : 1.77207

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.726, [92mTest[0m: 2.432, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.66213
[1mStep[0m  [10/106], [94mLoss[0m : 1.72805
[1mStep[0m  [20/106], [94mLoss[0m : 1.59847
[1mStep[0m  [30/106], [94mLoss[0m : 1.76006
[1mStep[0m  [40/106], [94mLoss[0m : 1.86853
[1mStep[0m  [50/106], [94mLoss[0m : 1.56193
[1mStep[0m  [60/106], [94mLoss[0m : 1.73138
[1mStep[0m  [70/106], [94mLoss[0m : 1.48872
[1mStep[0m  [80/106], [94mLoss[0m : 1.70770
[1mStep[0m  [90/106], [94mLoss[0m : 1.91831
[1mStep[0m  [100/106], [94mLoss[0m : 1.63620

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.698, [92mTest[0m: 2.446, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.34749
[1mStep[0m  [10/106], [94mLoss[0m : 1.74417
[1mStep[0m  [20/106], [94mLoss[0m : 1.54881
[1mStep[0m  [30/106], [94mLoss[0m : 1.68836
[1mStep[0m  [40/106], [94mLoss[0m : 1.62067
[1mStep[0m  [50/106], [94mLoss[0m : 1.65330
[1mStep[0m  [60/106], [94mLoss[0m : 1.72617
[1mStep[0m  [70/106], [94mLoss[0m : 1.66570
[1mStep[0m  [80/106], [94mLoss[0m : 1.88674
[1mStep[0m  [90/106], [94mLoss[0m : 1.82931
[1mStep[0m  [100/106], [94mLoss[0m : 1.79870

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.673, [92mTest[0m: 2.451, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.45124
[1mStep[0m  [10/106], [94mLoss[0m : 1.64578
[1mStep[0m  [20/106], [94mLoss[0m : 1.60820
[1mStep[0m  [30/106], [94mLoss[0m : 1.65120
[1mStep[0m  [40/106], [94mLoss[0m : 1.49352
[1mStep[0m  [50/106], [94mLoss[0m : 1.59630
[1mStep[0m  [60/106], [94mLoss[0m : 1.56506
[1mStep[0m  [70/106], [94mLoss[0m : 1.64458
[1mStep[0m  [80/106], [94mLoss[0m : 1.73399
[1mStep[0m  [90/106], [94mLoss[0m : 1.42078
[1mStep[0m  [100/106], [94mLoss[0m : 1.56861

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.629, [92mTest[0m: 2.433, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.41677
[1mStep[0m  [10/106], [94mLoss[0m : 1.74439
[1mStep[0m  [20/106], [94mLoss[0m : 1.61738
[1mStep[0m  [30/106], [94mLoss[0m : 1.46336
[1mStep[0m  [40/106], [94mLoss[0m : 1.55612
[1mStep[0m  [50/106], [94mLoss[0m : 1.76802
[1mStep[0m  [60/106], [94mLoss[0m : 1.69285
[1mStep[0m  [70/106], [94mLoss[0m : 1.63084
[1mStep[0m  [80/106], [94mLoss[0m : 1.51115
[1mStep[0m  [90/106], [94mLoss[0m : 1.59004
[1mStep[0m  [100/106], [94mLoss[0m : 1.34083

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.598, [92mTest[0m: 2.459, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.65578
[1mStep[0m  [10/106], [94mLoss[0m : 1.50483
[1mStep[0m  [20/106], [94mLoss[0m : 1.47983
[1mStep[0m  [30/106], [94mLoss[0m : 1.68935
[1mStep[0m  [40/106], [94mLoss[0m : 1.55986
[1mStep[0m  [50/106], [94mLoss[0m : 1.67624
[1mStep[0m  [60/106], [94mLoss[0m : 1.77523
[1mStep[0m  [70/106], [94mLoss[0m : 1.58660
[1mStep[0m  [80/106], [94mLoss[0m : 1.38362
[1mStep[0m  [90/106], [94mLoss[0m : 1.59334
[1mStep[0m  [100/106], [94mLoss[0m : 1.72827

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.566, [92mTest[0m: 2.452, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.44795
[1mStep[0m  [10/106], [94mLoss[0m : 1.43215
[1mStep[0m  [20/106], [94mLoss[0m : 1.47981
[1mStep[0m  [30/106], [94mLoss[0m : 1.46753
[1mStep[0m  [40/106], [94mLoss[0m : 1.58185
[1mStep[0m  [50/106], [94mLoss[0m : 1.55245
[1mStep[0m  [60/106], [94mLoss[0m : 1.55468
[1mStep[0m  [70/106], [94mLoss[0m : 1.48799
[1mStep[0m  [80/106], [94mLoss[0m : 1.91176
[1mStep[0m  [90/106], [94mLoss[0m : 1.56373
[1mStep[0m  [100/106], [94mLoss[0m : 1.62678

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.556, [92mTest[0m: 2.502, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.38431
[1mStep[0m  [10/106], [94mLoss[0m : 1.72777
[1mStep[0m  [20/106], [94mLoss[0m : 1.75290
[1mStep[0m  [30/106], [94mLoss[0m : 1.43508
[1mStep[0m  [40/106], [94mLoss[0m : 1.58668
[1mStep[0m  [50/106], [94mLoss[0m : 1.27425
[1mStep[0m  [60/106], [94mLoss[0m : 1.64490
[1mStep[0m  [70/106], [94mLoss[0m : 1.59433
[1mStep[0m  [80/106], [94mLoss[0m : 1.58518
[1mStep[0m  [90/106], [94mLoss[0m : 1.67046
[1mStep[0m  [100/106], [94mLoss[0m : 1.66722

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.545, [92mTest[0m: 2.434, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.43638
[1mStep[0m  [10/106], [94mLoss[0m : 1.55307
[1mStep[0m  [20/106], [94mLoss[0m : 1.75462
[1mStep[0m  [30/106], [94mLoss[0m : 1.48370
[1mStep[0m  [40/106], [94mLoss[0m : 1.55844
[1mStep[0m  [50/106], [94mLoss[0m : 1.38212
[1mStep[0m  [60/106], [94mLoss[0m : 1.37468
[1mStep[0m  [70/106], [94mLoss[0m : 1.74446
[1mStep[0m  [80/106], [94mLoss[0m : 1.52383
[1mStep[0m  [90/106], [94mLoss[0m : 1.48314
[1mStep[0m  [100/106], [94mLoss[0m : 1.52997

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.519, [92mTest[0m: 2.509, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.62099
[1mStep[0m  [10/106], [94mLoss[0m : 1.45620
[1mStep[0m  [20/106], [94mLoss[0m : 1.50921
[1mStep[0m  [30/106], [94mLoss[0m : 1.32236
[1mStep[0m  [40/106], [94mLoss[0m : 1.49314
[1mStep[0m  [50/106], [94mLoss[0m : 1.45830
[1mStep[0m  [60/106], [94mLoss[0m : 1.36936
[1mStep[0m  [70/106], [94mLoss[0m : 1.37124
[1mStep[0m  [80/106], [94mLoss[0m : 1.43110
[1mStep[0m  [90/106], [94mLoss[0m : 1.29897
[1mStep[0m  [100/106], [94mLoss[0m : 1.71452

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.509, [92mTest[0m: 2.492, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.58665
[1mStep[0m  [10/106], [94mLoss[0m : 1.51461
[1mStep[0m  [20/106], [94mLoss[0m : 1.44682
[1mStep[0m  [30/106], [94mLoss[0m : 1.40107
[1mStep[0m  [40/106], [94mLoss[0m : 1.45996
[1mStep[0m  [50/106], [94mLoss[0m : 1.44445
[1mStep[0m  [60/106], [94mLoss[0m : 1.69709
[1mStep[0m  [70/106], [94mLoss[0m : 1.62031
[1mStep[0m  [80/106], [94mLoss[0m : 1.62075
[1mStep[0m  [90/106], [94mLoss[0m : 1.50498
[1mStep[0m  [100/106], [94mLoss[0m : 1.57091

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.468, [92mTest[0m: 2.441, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.34852
[1mStep[0m  [10/106], [94mLoss[0m : 1.39405
[1mStep[0m  [20/106], [94mLoss[0m : 1.34338
[1mStep[0m  [30/106], [94mLoss[0m : 1.35538
[1mStep[0m  [40/106], [94mLoss[0m : 1.49799
[1mStep[0m  [50/106], [94mLoss[0m : 1.58905
[1mStep[0m  [60/106], [94mLoss[0m : 1.70018
[1mStep[0m  [70/106], [94mLoss[0m : 1.54169
[1mStep[0m  [80/106], [94mLoss[0m : 1.37638
[1mStep[0m  [90/106], [94mLoss[0m : 1.39411
[1mStep[0m  [100/106], [94mLoss[0m : 1.65166

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.466, [92mTest[0m: 2.584, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.57275
[1mStep[0m  [10/106], [94mLoss[0m : 1.38897
[1mStep[0m  [20/106], [94mLoss[0m : 1.37098
[1mStep[0m  [30/106], [94mLoss[0m : 1.41214
[1mStep[0m  [40/106], [94mLoss[0m : 1.29939
[1mStep[0m  [50/106], [94mLoss[0m : 1.40740
[1mStep[0m  [60/106], [94mLoss[0m : 1.70201
[1mStep[0m  [70/106], [94mLoss[0m : 1.35222
[1mStep[0m  [80/106], [94mLoss[0m : 1.65895
[1mStep[0m  [90/106], [94mLoss[0m : 1.57011
[1mStep[0m  [100/106], [94mLoss[0m : 1.45122

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.479, [92mTest[0m: 2.479, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.513
====================================

Phase 2 - Evaluation MAE:  2.512525589960926
MAE score P1      2.402268
MAE score P2      2.512526
loss               1.46622
learning_rate      0.00505
batch_size             128
hidden_sizes         [250]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.4
momentum               0.5
weight_decay          0.01
Name: 14, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/213], [94mLoss[0m : 10.78374
[1mStep[0m  [21/213], [94mLoss[0m : 10.09324
[1mStep[0m  [42/213], [94mLoss[0m : 7.56903
[1mStep[0m  [63/213], [94mLoss[0m : 5.90420
[1mStep[0m  [84/213], [94mLoss[0m : 4.01642
[1mStep[0m  [105/213], [94mLoss[0m : 2.92620
[1mStep[0m  [126/213], [94mLoss[0m : 2.98777
[1mStep[0m  [147/213], [94mLoss[0m : 3.19464
[1mStep[0m  [168/213], [94mLoss[0m : 2.67942
[1mStep[0m  [189/213], [94mLoss[0m : 3.07047
[1mStep[0m  [210/213], [94mLoss[0m : 2.84001

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.967, [92mTest[0m: 10.770, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.70126
[1mStep[0m  [21/213], [94mLoss[0m : 2.74821
[1mStep[0m  [42/213], [94mLoss[0m : 2.66090
[1mStep[0m  [63/213], [94mLoss[0m : 2.53731
[1mStep[0m  [84/213], [94mLoss[0m : 2.78723
[1mStep[0m  [105/213], [94mLoss[0m : 2.65844
[1mStep[0m  [126/213], [94mLoss[0m : 2.24299
[1mStep[0m  [147/213], [94mLoss[0m : 2.64087
[1mStep[0m  [168/213], [94mLoss[0m : 2.55078
[1mStep[0m  [189/213], [94mLoss[0m : 2.53321
[1mStep[0m  [210/213], [94mLoss[0m : 2.51190

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.621, [92mTest[0m: 2.578, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.78152
[1mStep[0m  [21/213], [94mLoss[0m : 2.41367
[1mStep[0m  [42/213], [94mLoss[0m : 3.23476
[1mStep[0m  [63/213], [94mLoss[0m : 2.61583
[1mStep[0m  [84/213], [94mLoss[0m : 2.56140
[1mStep[0m  [105/213], [94mLoss[0m : 2.64343
[1mStep[0m  [126/213], [94mLoss[0m : 2.88952
[1mStep[0m  [147/213], [94mLoss[0m : 2.55462
[1mStep[0m  [168/213], [94mLoss[0m : 2.16905
[1mStep[0m  [189/213], [94mLoss[0m : 2.87018
[1mStep[0m  [210/213], [94mLoss[0m : 2.39140

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.536, [92mTest[0m: 2.425, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.68559
[1mStep[0m  [21/213], [94mLoss[0m : 2.46911
[1mStep[0m  [42/213], [94mLoss[0m : 2.45514
[1mStep[0m  [63/213], [94mLoss[0m : 2.73362
[1mStep[0m  [84/213], [94mLoss[0m : 2.45875
[1mStep[0m  [105/213], [94mLoss[0m : 1.87080
[1mStep[0m  [126/213], [94mLoss[0m : 2.92374
[1mStep[0m  [147/213], [94mLoss[0m : 2.53422
[1mStep[0m  [168/213], [94mLoss[0m : 3.15874
[1mStep[0m  [189/213], [94mLoss[0m : 2.38962
[1mStep[0m  [210/213], [94mLoss[0m : 2.67754

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.413, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.50975
[1mStep[0m  [21/213], [94mLoss[0m : 2.43391
[1mStep[0m  [42/213], [94mLoss[0m : 2.08114
[1mStep[0m  [63/213], [94mLoss[0m : 2.29528
[1mStep[0m  [84/213], [94mLoss[0m : 2.93046
[1mStep[0m  [105/213], [94mLoss[0m : 2.70085
[1mStep[0m  [126/213], [94mLoss[0m : 2.33624
[1mStep[0m  [147/213], [94mLoss[0m : 2.73967
[1mStep[0m  [168/213], [94mLoss[0m : 2.67846
[1mStep[0m  [189/213], [94mLoss[0m : 2.36256
[1mStep[0m  [210/213], [94mLoss[0m : 2.71920

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.401, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.38897
[1mStep[0m  [21/213], [94mLoss[0m : 2.23948
[1mStep[0m  [42/213], [94mLoss[0m : 2.54717
[1mStep[0m  [63/213], [94mLoss[0m : 2.35048
[1mStep[0m  [84/213], [94mLoss[0m : 2.53162
[1mStep[0m  [105/213], [94mLoss[0m : 2.32348
[1mStep[0m  [126/213], [94mLoss[0m : 2.11969
[1mStep[0m  [147/213], [94mLoss[0m : 2.82350
[1mStep[0m  [168/213], [94mLoss[0m : 2.26745
[1mStep[0m  [189/213], [94mLoss[0m : 2.10396
[1mStep[0m  [210/213], [94mLoss[0m : 2.38303

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.415, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.61679
[1mStep[0m  [21/213], [94mLoss[0m : 2.60013
[1mStep[0m  [42/213], [94mLoss[0m : 2.65979
[1mStep[0m  [63/213], [94mLoss[0m : 2.07565
[1mStep[0m  [84/213], [94mLoss[0m : 2.46759
[1mStep[0m  [105/213], [94mLoss[0m : 2.39370
[1mStep[0m  [126/213], [94mLoss[0m : 2.31040
[1mStep[0m  [147/213], [94mLoss[0m : 2.66104
[1mStep[0m  [168/213], [94mLoss[0m : 2.86148
[1mStep[0m  [189/213], [94mLoss[0m : 2.44207
[1mStep[0m  [210/213], [94mLoss[0m : 2.69457

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.444, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.25013
[1mStep[0m  [21/213], [94mLoss[0m : 2.42159
[1mStep[0m  [42/213], [94mLoss[0m : 2.50921
[1mStep[0m  [63/213], [94mLoss[0m : 2.38937
[1mStep[0m  [84/213], [94mLoss[0m : 2.44357
[1mStep[0m  [105/213], [94mLoss[0m : 2.37179
[1mStep[0m  [126/213], [94mLoss[0m : 2.26968
[1mStep[0m  [147/213], [94mLoss[0m : 2.57774
[1mStep[0m  [168/213], [94mLoss[0m : 2.40461
[1mStep[0m  [189/213], [94mLoss[0m : 2.75897
[1mStep[0m  [210/213], [94mLoss[0m : 2.24679

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.399, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.08797
[1mStep[0m  [21/213], [94mLoss[0m : 2.34860
[1mStep[0m  [42/213], [94mLoss[0m : 2.25031
[1mStep[0m  [63/213], [94mLoss[0m : 2.12430
[1mStep[0m  [84/213], [94mLoss[0m : 2.55051
[1mStep[0m  [105/213], [94mLoss[0m : 2.21136
[1mStep[0m  [126/213], [94mLoss[0m : 2.45240
[1mStep[0m  [147/213], [94mLoss[0m : 2.24000
[1mStep[0m  [168/213], [94mLoss[0m : 2.64830
[1mStep[0m  [189/213], [94mLoss[0m : 2.87374
[1mStep[0m  [210/213], [94mLoss[0m : 2.50150

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.401, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.26978
[1mStep[0m  [21/213], [94mLoss[0m : 2.59575
[1mStep[0m  [42/213], [94mLoss[0m : 2.65439
[1mStep[0m  [63/213], [94mLoss[0m : 2.50729
[1mStep[0m  [84/213], [94mLoss[0m : 2.13179
[1mStep[0m  [105/213], [94mLoss[0m : 2.20337
[1mStep[0m  [126/213], [94mLoss[0m : 2.26016
[1mStep[0m  [147/213], [94mLoss[0m : 2.35199
[1mStep[0m  [168/213], [94mLoss[0m : 2.62255
[1mStep[0m  [189/213], [94mLoss[0m : 2.34983
[1mStep[0m  [210/213], [94mLoss[0m : 2.75868

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.401, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.53176
[1mStep[0m  [21/213], [94mLoss[0m : 2.82584
[1mStep[0m  [42/213], [94mLoss[0m : 2.56227
[1mStep[0m  [63/213], [94mLoss[0m : 2.43961
[1mStep[0m  [84/213], [94mLoss[0m : 2.13568
[1mStep[0m  [105/213], [94mLoss[0m : 2.39818
[1mStep[0m  [126/213], [94mLoss[0m : 2.65573
[1mStep[0m  [147/213], [94mLoss[0m : 2.34331
[1mStep[0m  [168/213], [94mLoss[0m : 2.31676
[1mStep[0m  [189/213], [94mLoss[0m : 2.21119
[1mStep[0m  [210/213], [94mLoss[0m : 2.67782

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.439, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.21739
[1mStep[0m  [21/213], [94mLoss[0m : 2.14250
[1mStep[0m  [42/213], [94mLoss[0m : 2.50267
[1mStep[0m  [63/213], [94mLoss[0m : 2.21050
[1mStep[0m  [84/213], [94mLoss[0m : 2.22834
[1mStep[0m  [105/213], [94mLoss[0m : 2.55573
[1mStep[0m  [126/213], [94mLoss[0m : 2.37997
[1mStep[0m  [147/213], [94mLoss[0m : 2.72895
[1mStep[0m  [168/213], [94mLoss[0m : 1.89901
[1mStep[0m  [189/213], [94mLoss[0m : 2.03624
[1mStep[0m  [210/213], [94mLoss[0m : 2.11510

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.413, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.61233
[1mStep[0m  [21/213], [94mLoss[0m : 2.16391
[1mStep[0m  [42/213], [94mLoss[0m : 2.44942
[1mStep[0m  [63/213], [94mLoss[0m : 2.02889
[1mStep[0m  [84/213], [94mLoss[0m : 2.38458
[1mStep[0m  [105/213], [94mLoss[0m : 2.55637
[1mStep[0m  [126/213], [94mLoss[0m : 2.42877
[1mStep[0m  [147/213], [94mLoss[0m : 2.53591
[1mStep[0m  [168/213], [94mLoss[0m : 2.34501
[1mStep[0m  [189/213], [94mLoss[0m : 2.04428
[1mStep[0m  [210/213], [94mLoss[0m : 2.55248

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.422, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.27860
[1mStep[0m  [21/213], [94mLoss[0m : 2.54634
[1mStep[0m  [42/213], [94mLoss[0m : 2.48039
[1mStep[0m  [63/213], [94mLoss[0m : 2.47416
[1mStep[0m  [84/213], [94mLoss[0m : 2.20275
[1mStep[0m  [105/213], [94mLoss[0m : 2.36121
[1mStep[0m  [126/213], [94mLoss[0m : 2.49632
[1mStep[0m  [147/213], [94mLoss[0m : 2.63495
[1mStep[0m  [168/213], [94mLoss[0m : 2.16073
[1mStep[0m  [189/213], [94mLoss[0m : 2.16369
[1mStep[0m  [210/213], [94mLoss[0m : 2.68500

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.439, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.54344
[1mStep[0m  [21/213], [94mLoss[0m : 2.47910
[1mStep[0m  [42/213], [94mLoss[0m : 2.40136
[1mStep[0m  [63/213], [94mLoss[0m : 2.03303
[1mStep[0m  [84/213], [94mLoss[0m : 2.29276
[1mStep[0m  [105/213], [94mLoss[0m : 2.17339
[1mStep[0m  [126/213], [94mLoss[0m : 2.30122
[1mStep[0m  [147/213], [94mLoss[0m : 2.36470
[1mStep[0m  [168/213], [94mLoss[0m : 2.35767
[1mStep[0m  [189/213], [94mLoss[0m : 1.75766
[1mStep[0m  [210/213], [94mLoss[0m : 2.45420

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.392, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.59455
[1mStep[0m  [21/213], [94mLoss[0m : 2.13064
[1mStep[0m  [42/213], [94mLoss[0m : 2.10055
[1mStep[0m  [63/213], [94mLoss[0m : 2.82245
[1mStep[0m  [84/213], [94mLoss[0m : 2.45757
[1mStep[0m  [105/213], [94mLoss[0m : 2.44697
[1mStep[0m  [126/213], [94mLoss[0m : 2.07930
[1mStep[0m  [147/213], [94mLoss[0m : 2.41433
[1mStep[0m  [168/213], [94mLoss[0m : 2.36218
[1mStep[0m  [189/213], [94mLoss[0m : 3.02439
[1mStep[0m  [210/213], [94mLoss[0m : 2.66454

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.416, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.20139
[1mStep[0m  [21/213], [94mLoss[0m : 2.49598
[1mStep[0m  [42/213], [94mLoss[0m : 2.66105
[1mStep[0m  [63/213], [94mLoss[0m : 2.74061
[1mStep[0m  [84/213], [94mLoss[0m : 2.41063
[1mStep[0m  [105/213], [94mLoss[0m : 1.93873
[1mStep[0m  [126/213], [94mLoss[0m : 2.10741
[1mStep[0m  [147/213], [94mLoss[0m : 2.70055
[1mStep[0m  [168/213], [94mLoss[0m : 2.39895
[1mStep[0m  [189/213], [94mLoss[0m : 2.42576
[1mStep[0m  [210/213], [94mLoss[0m : 2.56502

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.405, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.27539
[1mStep[0m  [21/213], [94mLoss[0m : 2.39630
[1mStep[0m  [42/213], [94mLoss[0m : 2.68707
[1mStep[0m  [63/213], [94mLoss[0m : 2.56342
[1mStep[0m  [84/213], [94mLoss[0m : 2.34675
[1mStep[0m  [105/213], [94mLoss[0m : 2.39489
[1mStep[0m  [126/213], [94mLoss[0m : 2.52481
[1mStep[0m  [147/213], [94mLoss[0m : 2.45907
[1mStep[0m  [168/213], [94mLoss[0m : 2.66289
[1mStep[0m  [189/213], [94mLoss[0m : 2.66702
[1mStep[0m  [210/213], [94mLoss[0m : 2.92945

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.398, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.27685
[1mStep[0m  [21/213], [94mLoss[0m : 2.60980
[1mStep[0m  [42/213], [94mLoss[0m : 2.46555
[1mStep[0m  [63/213], [94mLoss[0m : 2.48692
[1mStep[0m  [84/213], [94mLoss[0m : 2.39320
[1mStep[0m  [105/213], [94mLoss[0m : 2.47310
[1mStep[0m  [126/213], [94mLoss[0m : 2.50302
[1mStep[0m  [147/213], [94mLoss[0m : 2.31210
[1mStep[0m  [168/213], [94mLoss[0m : 2.32082
[1mStep[0m  [189/213], [94mLoss[0m : 2.33267
[1mStep[0m  [210/213], [94mLoss[0m : 3.08440

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.414, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.69184
[1mStep[0m  [21/213], [94mLoss[0m : 2.31422
[1mStep[0m  [42/213], [94mLoss[0m : 2.47055
[1mStep[0m  [63/213], [94mLoss[0m : 2.47209
[1mStep[0m  [84/213], [94mLoss[0m : 2.26418
[1mStep[0m  [105/213], [94mLoss[0m : 2.63346
[1mStep[0m  [126/213], [94mLoss[0m : 2.40408
[1mStep[0m  [147/213], [94mLoss[0m : 2.57037
[1mStep[0m  [168/213], [94mLoss[0m : 2.61119
[1mStep[0m  [189/213], [94mLoss[0m : 2.41712
[1mStep[0m  [210/213], [94mLoss[0m : 2.20726

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.404, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.99515
[1mStep[0m  [21/213], [94mLoss[0m : 2.16172
[1mStep[0m  [42/213], [94mLoss[0m : 2.31120
[1mStep[0m  [63/213], [94mLoss[0m : 2.75202
[1mStep[0m  [84/213], [94mLoss[0m : 2.73497
[1mStep[0m  [105/213], [94mLoss[0m : 2.85437
[1mStep[0m  [126/213], [94mLoss[0m : 2.60732
[1mStep[0m  [147/213], [94mLoss[0m : 2.46616
[1mStep[0m  [168/213], [94mLoss[0m : 1.99893
[1mStep[0m  [189/213], [94mLoss[0m : 2.20209
[1mStep[0m  [210/213], [94mLoss[0m : 2.36659

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.410, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.37459
[1mStep[0m  [21/213], [94mLoss[0m : 2.23237
[1mStep[0m  [42/213], [94mLoss[0m : 2.64657
[1mStep[0m  [63/213], [94mLoss[0m : 2.35591
[1mStep[0m  [84/213], [94mLoss[0m : 2.43342
[1mStep[0m  [105/213], [94mLoss[0m : 2.47709
[1mStep[0m  [126/213], [94mLoss[0m : 2.98550
[1mStep[0m  [147/213], [94mLoss[0m : 2.30301
[1mStep[0m  [168/213], [94mLoss[0m : 2.90529
[1mStep[0m  [189/213], [94mLoss[0m : 2.05459
[1mStep[0m  [210/213], [94mLoss[0m : 2.32667

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.400, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.44427
[1mStep[0m  [21/213], [94mLoss[0m : 2.69395
[1mStep[0m  [42/213], [94mLoss[0m : 2.10346
[1mStep[0m  [63/213], [94mLoss[0m : 2.66165
[1mStep[0m  [84/213], [94mLoss[0m : 2.44242
[1mStep[0m  [105/213], [94mLoss[0m : 2.47446
[1mStep[0m  [126/213], [94mLoss[0m : 2.69693
[1mStep[0m  [147/213], [94mLoss[0m : 2.23128
[1mStep[0m  [168/213], [94mLoss[0m : 2.64054
[1mStep[0m  [189/213], [94mLoss[0m : 2.73987
[1mStep[0m  [210/213], [94mLoss[0m : 2.41414

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.403, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.71389
[1mStep[0m  [21/213], [94mLoss[0m : 2.52697
[1mStep[0m  [42/213], [94mLoss[0m : 2.67069
[1mStep[0m  [63/213], [94mLoss[0m : 2.67048
[1mStep[0m  [84/213], [94mLoss[0m : 2.42732
[1mStep[0m  [105/213], [94mLoss[0m : 2.52849
[1mStep[0m  [126/213], [94mLoss[0m : 2.83838
[1mStep[0m  [147/213], [94mLoss[0m : 2.00994
[1mStep[0m  [168/213], [94mLoss[0m : 2.50256
[1mStep[0m  [189/213], [94mLoss[0m : 2.58929
[1mStep[0m  [210/213], [94mLoss[0m : 2.48235

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.413, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.81306
[1mStep[0m  [21/213], [94mLoss[0m : 2.43939
[1mStep[0m  [42/213], [94mLoss[0m : 2.33749
[1mStep[0m  [63/213], [94mLoss[0m : 2.28171
[1mStep[0m  [84/213], [94mLoss[0m : 2.58221
[1mStep[0m  [105/213], [94mLoss[0m : 2.04003
[1mStep[0m  [126/213], [94mLoss[0m : 2.30688
[1mStep[0m  [147/213], [94mLoss[0m : 2.70979
[1mStep[0m  [168/213], [94mLoss[0m : 2.80110
[1mStep[0m  [189/213], [94mLoss[0m : 2.39502
[1mStep[0m  [210/213], [94mLoss[0m : 2.40684

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.411, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.36918
[1mStep[0m  [21/213], [94mLoss[0m : 2.33803
[1mStep[0m  [42/213], [94mLoss[0m : 2.46020
[1mStep[0m  [63/213], [94mLoss[0m : 2.16043
[1mStep[0m  [84/213], [94mLoss[0m : 2.62515
[1mStep[0m  [105/213], [94mLoss[0m : 2.31852
[1mStep[0m  [126/213], [94mLoss[0m : 2.28613
[1mStep[0m  [147/213], [94mLoss[0m : 2.36840
[1mStep[0m  [168/213], [94mLoss[0m : 2.71973
[1mStep[0m  [189/213], [94mLoss[0m : 2.80141
[1mStep[0m  [210/213], [94mLoss[0m : 2.19898

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.405, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.16745
[1mStep[0m  [21/213], [94mLoss[0m : 2.26136
[1mStep[0m  [42/213], [94mLoss[0m : 2.12373
[1mStep[0m  [63/213], [94mLoss[0m : 2.81305
[1mStep[0m  [84/213], [94mLoss[0m : 2.34820
[1mStep[0m  [105/213], [94mLoss[0m : 2.42757
[1mStep[0m  [126/213], [94mLoss[0m : 2.03637
[1mStep[0m  [147/213], [94mLoss[0m : 2.41456
[1mStep[0m  [168/213], [94mLoss[0m : 2.75197
[1mStep[0m  [189/213], [94mLoss[0m : 2.60776
[1mStep[0m  [210/213], [94mLoss[0m : 2.75085

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.411, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.26455
[1mStep[0m  [21/213], [94mLoss[0m : 2.52482
[1mStep[0m  [42/213], [94mLoss[0m : 1.91979
[1mStep[0m  [63/213], [94mLoss[0m : 2.55187
[1mStep[0m  [84/213], [94mLoss[0m : 2.25499
[1mStep[0m  [105/213], [94mLoss[0m : 2.73102
[1mStep[0m  [126/213], [94mLoss[0m : 2.76144
[1mStep[0m  [147/213], [94mLoss[0m : 2.91653
[1mStep[0m  [168/213], [94mLoss[0m : 2.58599
[1mStep[0m  [189/213], [94mLoss[0m : 2.71879
[1mStep[0m  [210/213], [94mLoss[0m : 2.49115

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.400, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 3.06018
[1mStep[0m  [21/213], [94mLoss[0m : 2.56627
[1mStep[0m  [42/213], [94mLoss[0m : 2.24356
[1mStep[0m  [63/213], [94mLoss[0m : 2.36437
[1mStep[0m  [84/213], [94mLoss[0m : 1.94565
[1mStep[0m  [105/213], [94mLoss[0m : 2.23705
[1mStep[0m  [126/213], [94mLoss[0m : 1.74758
[1mStep[0m  [147/213], [94mLoss[0m : 2.38552
[1mStep[0m  [168/213], [94mLoss[0m : 2.28281
[1mStep[0m  [189/213], [94mLoss[0m : 2.12911
[1mStep[0m  [210/213], [94mLoss[0m : 2.82506

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.407, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.39446
[1mStep[0m  [21/213], [94mLoss[0m : 2.04131
[1mStep[0m  [42/213], [94mLoss[0m : 2.14965
[1mStep[0m  [63/213], [94mLoss[0m : 2.15043
[1mStep[0m  [84/213], [94mLoss[0m : 2.64646
[1mStep[0m  [105/213], [94mLoss[0m : 2.42305
[1mStep[0m  [126/213], [94mLoss[0m : 2.68415
[1mStep[0m  [147/213], [94mLoss[0m : 2.95771
[1mStep[0m  [168/213], [94mLoss[0m : 2.52247
[1mStep[0m  [189/213], [94mLoss[0m : 2.20670
[1mStep[0m  [210/213], [94mLoss[0m : 2.61010

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.417, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.411
====================================

Phase 1 - Evaluation MAE:  2.411341235322772
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/213], [94mLoss[0m : 2.68475
[1mStep[0m  [21/213], [94mLoss[0m : 2.50676
[1mStep[0m  [42/213], [94mLoss[0m : 2.18880
[1mStep[0m  [63/213], [94mLoss[0m : 2.68324
[1mStep[0m  [84/213], [94mLoss[0m : 2.22713
[1mStep[0m  [105/213], [94mLoss[0m : 2.64014
[1mStep[0m  [126/213], [94mLoss[0m : 2.29432
[1mStep[0m  [147/213], [94mLoss[0m : 2.17808
[1mStep[0m  [168/213], [94mLoss[0m : 2.36926
[1mStep[0m  [189/213], [94mLoss[0m : 2.89807
[1mStep[0m  [210/213], [94mLoss[0m : 2.85964

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.407, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.16076
[1mStep[0m  [21/213], [94mLoss[0m : 2.57227
[1mStep[0m  [42/213], [94mLoss[0m : 2.22313
[1mStep[0m  [63/213], [94mLoss[0m : 2.50580
[1mStep[0m  [84/213], [94mLoss[0m : 2.76698
[1mStep[0m  [105/213], [94mLoss[0m : 2.17892
[1mStep[0m  [126/213], [94mLoss[0m : 2.03021
[1mStep[0m  [147/213], [94mLoss[0m : 2.43160
[1mStep[0m  [168/213], [94mLoss[0m : 2.28826
[1mStep[0m  [189/213], [94mLoss[0m : 2.64502
[1mStep[0m  [210/213], [94mLoss[0m : 2.71392

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.375, [92mTest[0m: 2.404, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.45522
[1mStep[0m  [21/213], [94mLoss[0m : 2.05257
[1mStep[0m  [42/213], [94mLoss[0m : 2.15899
[1mStep[0m  [63/213], [94mLoss[0m : 2.01269
[1mStep[0m  [84/213], [94mLoss[0m : 2.30915
[1mStep[0m  [105/213], [94mLoss[0m : 2.27078
[1mStep[0m  [126/213], [94mLoss[0m : 2.31185
[1mStep[0m  [147/213], [94mLoss[0m : 2.32196
[1mStep[0m  [168/213], [94mLoss[0m : 2.14666
[1mStep[0m  [189/213], [94mLoss[0m : 2.35740
[1mStep[0m  [210/213], [94mLoss[0m : 2.18851

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.316, [92mTest[0m: 2.410, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.45129
[1mStep[0m  [21/213], [94mLoss[0m : 2.19314
[1mStep[0m  [42/213], [94mLoss[0m : 1.89142
[1mStep[0m  [63/213], [94mLoss[0m : 2.01325
[1mStep[0m  [84/213], [94mLoss[0m : 2.35363
[1mStep[0m  [105/213], [94mLoss[0m : 2.10106
[1mStep[0m  [126/213], [94mLoss[0m : 2.05658
[1mStep[0m  [147/213], [94mLoss[0m : 2.48796
[1mStep[0m  [168/213], [94mLoss[0m : 2.48906
[1mStep[0m  [189/213], [94mLoss[0m : 2.66646
[1mStep[0m  [210/213], [94mLoss[0m : 2.40423

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.265, [92mTest[0m: 2.407, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.32888
[1mStep[0m  [21/213], [94mLoss[0m : 2.09614
[1mStep[0m  [42/213], [94mLoss[0m : 1.93274
[1mStep[0m  [63/213], [94mLoss[0m : 2.49295
[1mStep[0m  [84/213], [94mLoss[0m : 2.61425
[1mStep[0m  [105/213], [94mLoss[0m : 2.44380
[1mStep[0m  [126/213], [94mLoss[0m : 2.24771
[1mStep[0m  [147/213], [94mLoss[0m : 1.99820
[1mStep[0m  [168/213], [94mLoss[0m : 2.05028
[1mStep[0m  [189/213], [94mLoss[0m : 2.05143
[1mStep[0m  [210/213], [94mLoss[0m : 2.32155

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.250, [92mTest[0m: 2.425, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.06422
[1mStep[0m  [21/213], [94mLoss[0m : 1.85891
[1mStep[0m  [42/213], [94mLoss[0m : 1.79568
[1mStep[0m  [63/213], [94mLoss[0m : 2.72164
[1mStep[0m  [84/213], [94mLoss[0m : 2.36028
[1mStep[0m  [105/213], [94mLoss[0m : 1.97532
[1mStep[0m  [126/213], [94mLoss[0m : 2.33373
[1mStep[0m  [147/213], [94mLoss[0m : 2.13096
[1mStep[0m  [168/213], [94mLoss[0m : 2.14862
[1mStep[0m  [189/213], [94mLoss[0m : 2.42946
[1mStep[0m  [210/213], [94mLoss[0m : 2.44466

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.214, [92mTest[0m: 2.403, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.33673
[1mStep[0m  [21/213], [94mLoss[0m : 2.06938
[1mStep[0m  [42/213], [94mLoss[0m : 1.96813
[1mStep[0m  [63/213], [94mLoss[0m : 2.49154
[1mStep[0m  [84/213], [94mLoss[0m : 2.22396
[1mStep[0m  [105/213], [94mLoss[0m : 1.67413
[1mStep[0m  [126/213], [94mLoss[0m : 2.61035
[1mStep[0m  [147/213], [94mLoss[0m : 2.17757
[1mStep[0m  [168/213], [94mLoss[0m : 2.17940
[1mStep[0m  [189/213], [94mLoss[0m : 2.16297
[1mStep[0m  [210/213], [94mLoss[0m : 2.33284

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.194, [92mTest[0m: 2.463, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.80044
[1mStep[0m  [21/213], [94mLoss[0m : 1.98800
[1mStep[0m  [42/213], [94mLoss[0m : 2.35534
[1mStep[0m  [63/213], [94mLoss[0m : 1.99307
[1mStep[0m  [84/213], [94mLoss[0m : 2.56685
[1mStep[0m  [105/213], [94mLoss[0m : 2.06994
[1mStep[0m  [126/213], [94mLoss[0m : 1.87173
[1mStep[0m  [147/213], [94mLoss[0m : 2.22271
[1mStep[0m  [168/213], [94mLoss[0m : 2.31281
[1mStep[0m  [189/213], [94mLoss[0m : 2.53980
[1mStep[0m  [210/213], [94mLoss[0m : 1.98870

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.160, [92mTest[0m: 2.444, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.92747
[1mStep[0m  [21/213], [94mLoss[0m : 2.31329
[1mStep[0m  [42/213], [94mLoss[0m : 2.16718
[1mStep[0m  [63/213], [94mLoss[0m : 1.78906
[1mStep[0m  [84/213], [94mLoss[0m : 2.09876
[1mStep[0m  [105/213], [94mLoss[0m : 1.96117
[1mStep[0m  [126/213], [94mLoss[0m : 2.01225
[1mStep[0m  [147/213], [94mLoss[0m : 2.21690
[1mStep[0m  [168/213], [94mLoss[0m : 2.52291
[1mStep[0m  [189/213], [94mLoss[0m : 2.37738
[1mStep[0m  [210/213], [94mLoss[0m : 2.52397

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.162, [92mTest[0m: 2.436, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.76288
[1mStep[0m  [21/213], [94mLoss[0m : 2.20811
[1mStep[0m  [42/213], [94mLoss[0m : 2.20716
[1mStep[0m  [63/213], [94mLoss[0m : 2.14607
[1mStep[0m  [84/213], [94mLoss[0m : 2.13711
[1mStep[0m  [105/213], [94mLoss[0m : 2.18013
[1mStep[0m  [126/213], [94mLoss[0m : 2.32768
[1mStep[0m  [147/213], [94mLoss[0m : 1.77356
[1mStep[0m  [168/213], [94mLoss[0m : 2.25947
[1mStep[0m  [189/213], [94mLoss[0m : 2.41963
[1mStep[0m  [210/213], [94mLoss[0m : 2.23584

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.150, [92mTest[0m: 2.399, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.08759
[1mStep[0m  [21/213], [94mLoss[0m : 1.88506
[1mStep[0m  [42/213], [94mLoss[0m : 2.35945
[1mStep[0m  [63/213], [94mLoss[0m : 2.07179
[1mStep[0m  [84/213], [94mLoss[0m : 1.64251
[1mStep[0m  [105/213], [94mLoss[0m : 2.22244
[1mStep[0m  [126/213], [94mLoss[0m : 1.97820
[1mStep[0m  [147/213], [94mLoss[0m : 2.21783
[1mStep[0m  [168/213], [94mLoss[0m : 2.48828
[1mStep[0m  [189/213], [94mLoss[0m : 2.01165
[1mStep[0m  [210/213], [94mLoss[0m : 2.60127

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.124, [92mTest[0m: 2.431, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.51344
[1mStep[0m  [21/213], [94mLoss[0m : 1.59223
[1mStep[0m  [42/213], [94mLoss[0m : 2.00671
[1mStep[0m  [63/213], [94mLoss[0m : 2.44587
[1mStep[0m  [84/213], [94mLoss[0m : 2.25410
[1mStep[0m  [105/213], [94mLoss[0m : 2.19711
[1mStep[0m  [126/213], [94mLoss[0m : 2.21767
[1mStep[0m  [147/213], [94mLoss[0m : 2.17876
[1mStep[0m  [168/213], [94mLoss[0m : 2.46827
[1mStep[0m  [189/213], [94mLoss[0m : 2.25332
[1mStep[0m  [210/213], [94mLoss[0m : 2.26083

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.108, [92mTest[0m: 2.435, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.88259
[1mStep[0m  [21/213], [94mLoss[0m : 1.97896
[1mStep[0m  [42/213], [94mLoss[0m : 1.63015
[1mStep[0m  [63/213], [94mLoss[0m : 2.31849
[1mStep[0m  [84/213], [94mLoss[0m : 1.91511
[1mStep[0m  [105/213], [94mLoss[0m : 2.03896
[1mStep[0m  [126/213], [94mLoss[0m : 2.18611
[1mStep[0m  [147/213], [94mLoss[0m : 1.56660
[1mStep[0m  [168/213], [94mLoss[0m : 2.36629
[1mStep[0m  [189/213], [94mLoss[0m : 1.74260
[1mStep[0m  [210/213], [94mLoss[0m : 1.98816

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.095, [92mTest[0m: 2.439, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.04575
[1mStep[0m  [21/213], [94mLoss[0m : 1.74827
[1mStep[0m  [42/213], [94mLoss[0m : 2.07857
[1mStep[0m  [63/213], [94mLoss[0m : 2.18670
[1mStep[0m  [84/213], [94mLoss[0m : 2.28317
[1mStep[0m  [105/213], [94mLoss[0m : 1.87329
[1mStep[0m  [126/213], [94mLoss[0m : 1.93782
[1mStep[0m  [147/213], [94mLoss[0m : 2.01323
[1mStep[0m  [168/213], [94mLoss[0m : 1.91837
[1mStep[0m  [189/213], [94mLoss[0m : 2.14648
[1mStep[0m  [210/213], [94mLoss[0m : 2.01741

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.079, [92mTest[0m: 2.428, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.87285
[1mStep[0m  [21/213], [94mLoss[0m : 1.99689
[1mStep[0m  [42/213], [94mLoss[0m : 1.83849
[1mStep[0m  [63/213], [94mLoss[0m : 1.96853
[1mStep[0m  [84/213], [94mLoss[0m : 1.95968
[1mStep[0m  [105/213], [94mLoss[0m : 1.85017
[1mStep[0m  [126/213], [94mLoss[0m : 2.21611
[1mStep[0m  [147/213], [94mLoss[0m : 2.08823
[1mStep[0m  [168/213], [94mLoss[0m : 2.07076
[1mStep[0m  [189/213], [94mLoss[0m : 2.29424
[1mStep[0m  [210/213], [94mLoss[0m : 2.19194

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.091, [92mTest[0m: 2.454, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.23697
[1mStep[0m  [21/213], [94mLoss[0m : 2.23562
[1mStep[0m  [42/213], [94mLoss[0m : 2.05020
[1mStep[0m  [63/213], [94mLoss[0m : 1.86192
[1mStep[0m  [84/213], [94mLoss[0m : 1.83053
[1mStep[0m  [105/213], [94mLoss[0m : 1.75819
[1mStep[0m  [126/213], [94mLoss[0m : 2.31577
[1mStep[0m  [147/213], [94mLoss[0m : 1.84832
[1mStep[0m  [168/213], [94mLoss[0m : 2.53497
[1mStep[0m  [189/213], [94mLoss[0m : 2.09067
[1mStep[0m  [210/213], [94mLoss[0m : 2.15030

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.069, [92mTest[0m: 2.425, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.87265
[1mStep[0m  [21/213], [94mLoss[0m : 2.18759
[1mStep[0m  [42/213], [94mLoss[0m : 1.82955
[1mStep[0m  [63/213], [94mLoss[0m : 1.88504
[1mStep[0m  [84/213], [94mLoss[0m : 1.90992
[1mStep[0m  [105/213], [94mLoss[0m : 2.19227
[1mStep[0m  [126/213], [94mLoss[0m : 1.77802
[1mStep[0m  [147/213], [94mLoss[0m : 1.85357
[1mStep[0m  [168/213], [94mLoss[0m : 1.86859
[1mStep[0m  [189/213], [94mLoss[0m : 2.55854
[1mStep[0m  [210/213], [94mLoss[0m : 2.36228

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.055, [92mTest[0m: 2.465, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.93536
[1mStep[0m  [21/213], [94mLoss[0m : 2.16983
[1mStep[0m  [42/213], [94mLoss[0m : 1.48279
[1mStep[0m  [63/213], [94mLoss[0m : 1.91190
[1mStep[0m  [84/213], [94mLoss[0m : 2.08877
[1mStep[0m  [105/213], [94mLoss[0m : 2.10928
[1mStep[0m  [126/213], [94mLoss[0m : 1.68069
[1mStep[0m  [147/213], [94mLoss[0m : 1.81139
[1mStep[0m  [168/213], [94mLoss[0m : 2.05640
[1mStep[0m  [189/213], [94mLoss[0m : 2.12849
[1mStep[0m  [210/213], [94mLoss[0m : 2.09214

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.057, [92mTest[0m: 2.455, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.86052
[1mStep[0m  [21/213], [94mLoss[0m : 2.06344
[1mStep[0m  [42/213], [94mLoss[0m : 1.80693
[1mStep[0m  [63/213], [94mLoss[0m : 2.35190
[1mStep[0m  [84/213], [94mLoss[0m : 1.70719
[1mStep[0m  [105/213], [94mLoss[0m : 2.00084
[1mStep[0m  [126/213], [94mLoss[0m : 2.11008
[1mStep[0m  [147/213], [94mLoss[0m : 2.09512
[1mStep[0m  [168/213], [94mLoss[0m : 2.09831
[1mStep[0m  [189/213], [94mLoss[0m : 1.90163
[1mStep[0m  [210/213], [94mLoss[0m : 1.94556

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.034, [92mTest[0m: 2.436, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.72616
[1mStep[0m  [21/213], [94mLoss[0m : 1.96374
[1mStep[0m  [42/213], [94mLoss[0m : 2.10626
[1mStep[0m  [63/213], [94mLoss[0m : 2.10668
[1mStep[0m  [84/213], [94mLoss[0m : 2.35826
[1mStep[0m  [105/213], [94mLoss[0m : 1.83020
[1mStep[0m  [126/213], [94mLoss[0m : 2.34404
[1mStep[0m  [147/213], [94mLoss[0m : 2.06786
[1mStep[0m  [168/213], [94mLoss[0m : 1.70182
[1mStep[0m  [189/213], [94mLoss[0m : 2.39615
[1mStep[0m  [210/213], [94mLoss[0m : 1.98371

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.033, [92mTest[0m: 2.462, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.03959
[1mStep[0m  [21/213], [94mLoss[0m : 1.76044
[1mStep[0m  [42/213], [94mLoss[0m : 2.14052
[1mStep[0m  [63/213], [94mLoss[0m : 1.99655
[1mStep[0m  [84/213], [94mLoss[0m : 1.96858
[1mStep[0m  [105/213], [94mLoss[0m : 2.15363
[1mStep[0m  [126/213], [94mLoss[0m : 2.33228
[1mStep[0m  [147/213], [94mLoss[0m : 1.86147
[1mStep[0m  [168/213], [94mLoss[0m : 1.84756
[1mStep[0m  [189/213], [94mLoss[0m : 1.85935
[1mStep[0m  [210/213], [94mLoss[0m : 1.99132

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.982, [92mTest[0m: 2.509, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.64083
[1mStep[0m  [21/213], [94mLoss[0m : 1.54587
[1mStep[0m  [42/213], [94mLoss[0m : 2.14091
[1mStep[0m  [63/213], [94mLoss[0m : 2.05831
[1mStep[0m  [84/213], [94mLoss[0m : 1.89376
[1mStep[0m  [105/213], [94mLoss[0m : 1.88708
[1mStep[0m  [126/213], [94mLoss[0m : 1.76982
[1mStep[0m  [147/213], [94mLoss[0m : 1.77716
[1mStep[0m  [168/213], [94mLoss[0m : 2.04738
[1mStep[0m  [189/213], [94mLoss[0m : 1.81176
[1mStep[0m  [210/213], [94mLoss[0m : 1.70953

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.963, [92mTest[0m: 2.485, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.67002
[1mStep[0m  [21/213], [94mLoss[0m : 1.85734
[1mStep[0m  [42/213], [94mLoss[0m : 1.63049
[1mStep[0m  [63/213], [94mLoss[0m : 1.80284
[1mStep[0m  [84/213], [94mLoss[0m : 2.19502
[1mStep[0m  [105/213], [94mLoss[0m : 1.85775
[1mStep[0m  [126/213], [94mLoss[0m : 1.93100
[1mStep[0m  [147/213], [94mLoss[0m : 1.49424
[1mStep[0m  [168/213], [94mLoss[0m : 1.93343
[1mStep[0m  [189/213], [94mLoss[0m : 1.83956
[1mStep[0m  [210/213], [94mLoss[0m : 1.88887

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.962, [92mTest[0m: 2.471, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.21184
[1mStep[0m  [21/213], [94mLoss[0m : 1.92169
[1mStep[0m  [42/213], [94mLoss[0m : 1.96219
[1mStep[0m  [63/213], [94mLoss[0m : 1.94434
[1mStep[0m  [84/213], [94mLoss[0m : 2.17377
[1mStep[0m  [105/213], [94mLoss[0m : 2.42637
[1mStep[0m  [126/213], [94mLoss[0m : 2.09188
[1mStep[0m  [147/213], [94mLoss[0m : 1.94647
[1mStep[0m  [168/213], [94mLoss[0m : 2.51282
[1mStep[0m  [189/213], [94mLoss[0m : 2.25344
[1mStep[0m  [210/213], [94mLoss[0m : 2.10428

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.964, [92mTest[0m: 2.474, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.98341
[1mStep[0m  [21/213], [94mLoss[0m : 2.21003
[1mStep[0m  [42/213], [94mLoss[0m : 2.04866
[1mStep[0m  [63/213], [94mLoss[0m : 1.83060
[1mStep[0m  [84/213], [94mLoss[0m : 1.94816
[1mStep[0m  [105/213], [94mLoss[0m : 2.01001
[1mStep[0m  [126/213], [94mLoss[0m : 1.81320
[1mStep[0m  [147/213], [94mLoss[0m : 1.92413
[1mStep[0m  [168/213], [94mLoss[0m : 1.60538
[1mStep[0m  [189/213], [94mLoss[0m : 1.80100
[1mStep[0m  [210/213], [94mLoss[0m : 1.80912

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.932, [92mTest[0m: 2.495, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.95667
[1mStep[0m  [21/213], [94mLoss[0m : 1.81932
[1mStep[0m  [42/213], [94mLoss[0m : 2.08078
[1mStep[0m  [63/213], [94mLoss[0m : 1.58954
[1mStep[0m  [84/213], [94mLoss[0m : 1.97236
[1mStep[0m  [105/213], [94mLoss[0m : 1.84408
[1mStep[0m  [126/213], [94mLoss[0m : 1.96204
[1mStep[0m  [147/213], [94mLoss[0m : 1.77490
[1mStep[0m  [168/213], [94mLoss[0m : 1.69272
[1mStep[0m  [189/213], [94mLoss[0m : 1.91493
[1mStep[0m  [210/213], [94mLoss[0m : 2.54659

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.950, [92mTest[0m: 2.501, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.86166
[1mStep[0m  [21/213], [94mLoss[0m : 1.57007
[1mStep[0m  [42/213], [94mLoss[0m : 2.12797
[1mStep[0m  [63/213], [94mLoss[0m : 1.93454
[1mStep[0m  [84/213], [94mLoss[0m : 1.88530
[1mStep[0m  [105/213], [94mLoss[0m : 1.91729
[1mStep[0m  [126/213], [94mLoss[0m : 1.93711
[1mStep[0m  [147/213], [94mLoss[0m : 1.75607
[1mStep[0m  [168/213], [94mLoss[0m : 2.27497
[1mStep[0m  [189/213], [94mLoss[0m : 1.79017
[1mStep[0m  [210/213], [94mLoss[0m : 1.77224

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.935, [92mTest[0m: 2.436, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.89353
[1mStep[0m  [21/213], [94mLoss[0m : 2.11741
[1mStep[0m  [42/213], [94mLoss[0m : 1.54138
[1mStep[0m  [63/213], [94mLoss[0m : 2.00626
[1mStep[0m  [84/213], [94mLoss[0m : 2.28088
[1mStep[0m  [105/213], [94mLoss[0m : 1.88553
[1mStep[0m  [126/213], [94mLoss[0m : 1.85429
[1mStep[0m  [147/213], [94mLoss[0m : 2.09239
[1mStep[0m  [168/213], [94mLoss[0m : 1.69824
[1mStep[0m  [189/213], [94mLoss[0m : 1.48668
[1mStep[0m  [210/213], [94mLoss[0m : 1.79172

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.937, [92mTest[0m: 2.506, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.05185
[1mStep[0m  [21/213], [94mLoss[0m : 2.13730
[1mStep[0m  [42/213], [94mLoss[0m : 1.97067
[1mStep[0m  [63/213], [94mLoss[0m : 1.87580
[1mStep[0m  [84/213], [94mLoss[0m : 1.66394
[1mStep[0m  [105/213], [94mLoss[0m : 1.96966
[1mStep[0m  [126/213], [94mLoss[0m : 1.76086
[1mStep[0m  [147/213], [94mLoss[0m : 2.42855
[1mStep[0m  [168/213], [94mLoss[0m : 2.07646
[1mStep[0m  [189/213], [94mLoss[0m : 1.63337
[1mStep[0m  [210/213], [94mLoss[0m : 2.00019

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.933, [92mTest[0m: 2.490, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.75629
[1mStep[0m  [21/213], [94mLoss[0m : 1.77594
[1mStep[0m  [42/213], [94mLoss[0m : 2.03722
[1mStep[0m  [63/213], [94mLoss[0m : 2.15300
[1mStep[0m  [84/213], [94mLoss[0m : 1.81734
[1mStep[0m  [105/213], [94mLoss[0m : 1.97312
[1mStep[0m  [126/213], [94mLoss[0m : 1.57883
[1mStep[0m  [147/213], [94mLoss[0m : 1.69094
[1mStep[0m  [168/213], [94mLoss[0m : 2.13453
[1mStep[0m  [189/213], [94mLoss[0m : 1.81766
[1mStep[0m  [210/213], [94mLoss[0m : 2.47853

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.921, [92mTest[0m: 2.509, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.461
====================================

Phase 2 - Evaluation MAE:  2.461014003123877
MAE score P1      2.411341
MAE score P2      2.461014
loss              1.921323
learning_rate      0.00505
batch_size              64
hidden_sizes         [250]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.9
weight_decay          0.01
Name: 15, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
