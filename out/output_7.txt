no change     /home/modelrep/sadiya/miniconda/condabin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda-env
no change     /home/modelrep/sadiya/miniconda/bin/activate
no change     /home/modelrep/sadiya/miniconda/bin/deactivate
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.sh
no change     /home/modelrep/sadiya/miniconda/etc/fish/conf.d/conda.fish
no change     /home/modelrep/sadiya/miniconda/shell/condabin/Conda.psm1
no change     /home/modelrep/sadiya/miniconda/shell/condabin/conda-hook.ps1
no change     /home/modelrep/sadiya/miniconda/lib/python3.10/site-packages/xontrib/conda.xsh
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.csh
no change     /home/modelrep/sadiya/.bashrc
No action taken.
-----------------
Process ID:  7
Number of Nodes:  1
Number of Array Tasks:  16
-----------------
Using device: cuda

AMD Instinct MI210
Memory Usage:
Allocated: 0.0 GB
Cached:    0.0 GB
theta
whole_spec
delta
alpha
beta
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/169], [94mLoss[0m : 10.61872
[1mStep[0m  [16/169], [94mLoss[0m : 4.72608
[1mStep[0m  [32/169], [94mLoss[0m : 2.68998
[1mStep[0m  [48/169], [94mLoss[0m : 2.11085
[1mStep[0m  [64/169], [94mLoss[0m : 2.88067
[1mStep[0m  [80/169], [94mLoss[0m : 2.82835
[1mStep[0m  [96/169], [94mLoss[0m : 2.88884
[1mStep[0m  [112/169], [94mLoss[0m : 2.87303
[1mStep[0m  [128/169], [94mLoss[0m : 2.42645
[1mStep[0m  [144/169], [94mLoss[0m : 2.97485
[1mStep[0m  [160/169], [94mLoss[0m : 2.37859

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.227, [92mTest[0m: 10.550, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.82295
[1mStep[0m  [16/169], [94mLoss[0m : 2.79716
[1mStep[0m  [32/169], [94mLoss[0m : 2.99033
[1mStep[0m  [48/169], [94mLoss[0m : 2.63974
[1mStep[0m  [64/169], [94mLoss[0m : 2.92140
[1mStep[0m  [80/169], [94mLoss[0m : 2.55618
[1mStep[0m  [96/169], [94mLoss[0m : 2.42765
[1mStep[0m  [112/169], [94mLoss[0m : 2.22662
[1mStep[0m  [128/169], [94mLoss[0m : 2.71234
[1mStep[0m  [144/169], [94mLoss[0m : 2.83360
[1mStep[0m  [160/169], [94mLoss[0m : 2.77187

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.582, [92mTest[0m: 2.356, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.10475
[1mStep[0m  [16/169], [94mLoss[0m : 2.09401
[1mStep[0m  [32/169], [94mLoss[0m : 2.84345
[1mStep[0m  [48/169], [94mLoss[0m : 2.18018
[1mStep[0m  [64/169], [94mLoss[0m : 2.98402
[1mStep[0m  [80/169], [94mLoss[0m : 2.94101
[1mStep[0m  [96/169], [94mLoss[0m : 2.60739
[1mStep[0m  [112/169], [94mLoss[0m : 2.38509
[1mStep[0m  [128/169], [94mLoss[0m : 2.62756
[1mStep[0m  [144/169], [94mLoss[0m : 2.63031
[1mStep[0m  [160/169], [94mLoss[0m : 2.58186

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.568, [92mTest[0m: 2.360, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.94841
[1mStep[0m  [16/169], [94mLoss[0m : 2.53191
[1mStep[0m  [32/169], [94mLoss[0m : 2.84748
[1mStep[0m  [48/169], [94mLoss[0m : 3.31743
[1mStep[0m  [64/169], [94mLoss[0m : 2.71480
[1mStep[0m  [80/169], [94mLoss[0m : 2.13395
[1mStep[0m  [96/169], [94mLoss[0m : 2.57211
[1mStep[0m  [112/169], [94mLoss[0m : 2.24011
[1mStep[0m  [128/169], [94mLoss[0m : 2.43954
[1mStep[0m  [144/169], [94mLoss[0m : 2.65188
[1mStep[0m  [160/169], [94mLoss[0m : 2.67975

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.567, [92mTest[0m: 2.341, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.36187
[1mStep[0m  [16/169], [94mLoss[0m : 2.41427
[1mStep[0m  [32/169], [94mLoss[0m : 2.34564
[1mStep[0m  [48/169], [94mLoss[0m : 2.63907
[1mStep[0m  [64/169], [94mLoss[0m : 2.51423
[1mStep[0m  [80/169], [94mLoss[0m : 2.72892
[1mStep[0m  [96/169], [94mLoss[0m : 2.90928
[1mStep[0m  [112/169], [94mLoss[0m : 3.14362
[1mStep[0m  [128/169], [94mLoss[0m : 2.60991
[1mStep[0m  [144/169], [94mLoss[0m : 2.52587
[1mStep[0m  [160/169], [94mLoss[0m : 1.92900

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.522, [92mTest[0m: 2.339, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.58193
[1mStep[0m  [16/169], [94mLoss[0m : 2.19398
[1mStep[0m  [32/169], [94mLoss[0m : 2.79142
[1mStep[0m  [48/169], [94mLoss[0m : 2.30403
[1mStep[0m  [64/169], [94mLoss[0m : 2.50454
[1mStep[0m  [80/169], [94mLoss[0m : 2.81160
[1mStep[0m  [96/169], [94mLoss[0m : 3.12914
[1mStep[0m  [112/169], [94mLoss[0m : 2.62995
[1mStep[0m  [128/169], [94mLoss[0m : 2.76682
[1mStep[0m  [144/169], [94mLoss[0m : 2.53929
[1mStep[0m  [160/169], [94mLoss[0m : 2.83444

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.334, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.55327
[1mStep[0m  [16/169], [94mLoss[0m : 2.67470
[1mStep[0m  [32/169], [94mLoss[0m : 2.18525
[1mStep[0m  [48/169], [94mLoss[0m : 2.40688
[1mStep[0m  [64/169], [94mLoss[0m : 2.46866
[1mStep[0m  [80/169], [94mLoss[0m : 2.98581
[1mStep[0m  [96/169], [94mLoss[0m : 2.80806
[1mStep[0m  [112/169], [94mLoss[0m : 2.42103
[1mStep[0m  [128/169], [94mLoss[0m : 2.24173
[1mStep[0m  [144/169], [94mLoss[0m : 2.43892
[1mStep[0m  [160/169], [94mLoss[0m : 2.17221

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.345, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.11842
[1mStep[0m  [16/169], [94mLoss[0m : 2.61581
[1mStep[0m  [32/169], [94mLoss[0m : 2.44544
[1mStep[0m  [48/169], [94mLoss[0m : 2.24401
[1mStep[0m  [64/169], [94mLoss[0m : 2.66287
[1mStep[0m  [80/169], [94mLoss[0m : 2.62586
[1mStep[0m  [96/169], [94mLoss[0m : 2.56031
[1mStep[0m  [112/169], [94mLoss[0m : 2.20330
[1mStep[0m  [128/169], [94mLoss[0m : 2.48961
[1mStep[0m  [144/169], [94mLoss[0m : 2.52704
[1mStep[0m  [160/169], [94mLoss[0m : 2.68108

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.326, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.42074
[1mStep[0m  [16/169], [94mLoss[0m : 3.08266
[1mStep[0m  [32/169], [94mLoss[0m : 2.59552
[1mStep[0m  [48/169], [94mLoss[0m : 2.62834
[1mStep[0m  [64/169], [94mLoss[0m : 2.43120
[1mStep[0m  [80/169], [94mLoss[0m : 2.33242
[1mStep[0m  [96/169], [94mLoss[0m : 2.48310
[1mStep[0m  [112/169], [94mLoss[0m : 2.18268
[1mStep[0m  [128/169], [94mLoss[0m : 3.02193
[1mStep[0m  [144/169], [94mLoss[0m : 2.71125
[1mStep[0m  [160/169], [94mLoss[0m : 2.17427

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.341, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.13191
[1mStep[0m  [16/169], [94mLoss[0m : 2.90603
[1mStep[0m  [32/169], [94mLoss[0m : 2.68422
[1mStep[0m  [48/169], [94mLoss[0m : 2.40118
[1mStep[0m  [64/169], [94mLoss[0m : 2.43685
[1mStep[0m  [80/169], [94mLoss[0m : 2.55746
[1mStep[0m  [96/169], [94mLoss[0m : 2.60041
[1mStep[0m  [112/169], [94mLoss[0m : 2.87714
[1mStep[0m  [128/169], [94mLoss[0m : 2.53248
[1mStep[0m  [144/169], [94mLoss[0m : 2.49924
[1mStep[0m  [160/169], [94mLoss[0m : 2.92392

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.341, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.68055
[1mStep[0m  [16/169], [94mLoss[0m : 2.63885
[1mStep[0m  [32/169], [94mLoss[0m : 2.45420
[1mStep[0m  [48/169], [94mLoss[0m : 2.29228
[1mStep[0m  [64/169], [94mLoss[0m : 2.78705
[1mStep[0m  [80/169], [94mLoss[0m : 2.09252
[1mStep[0m  [96/169], [94mLoss[0m : 2.52129
[1mStep[0m  [112/169], [94mLoss[0m : 2.52196
[1mStep[0m  [128/169], [94mLoss[0m : 2.54972
[1mStep[0m  [144/169], [94mLoss[0m : 2.68213
[1mStep[0m  [160/169], [94mLoss[0m : 2.43264

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.325, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.65389
[1mStep[0m  [16/169], [94mLoss[0m : 2.28345
[1mStep[0m  [32/169], [94mLoss[0m : 2.88615
[1mStep[0m  [48/169], [94mLoss[0m : 2.71033
[1mStep[0m  [64/169], [94mLoss[0m : 2.45196
[1mStep[0m  [80/169], [94mLoss[0m : 2.88651
[1mStep[0m  [96/169], [94mLoss[0m : 2.14008
[1mStep[0m  [112/169], [94mLoss[0m : 2.59180
[1mStep[0m  [128/169], [94mLoss[0m : 2.47583
[1mStep[0m  [144/169], [94mLoss[0m : 2.45718
[1mStep[0m  [160/169], [94mLoss[0m : 2.52045

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.331, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.32919
[1mStep[0m  [16/169], [94mLoss[0m : 2.59464
[1mStep[0m  [32/169], [94mLoss[0m : 2.72454
[1mStep[0m  [48/169], [94mLoss[0m : 2.87236
[1mStep[0m  [64/169], [94mLoss[0m : 2.73410
[1mStep[0m  [80/169], [94mLoss[0m : 2.44854
[1mStep[0m  [96/169], [94mLoss[0m : 2.92114
[1mStep[0m  [112/169], [94mLoss[0m : 2.46396
[1mStep[0m  [128/169], [94mLoss[0m : 2.18735
[1mStep[0m  [144/169], [94mLoss[0m : 2.64753
[1mStep[0m  [160/169], [94mLoss[0m : 2.37079

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.331, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.39600
[1mStep[0m  [16/169], [94mLoss[0m : 2.32476
[1mStep[0m  [32/169], [94mLoss[0m : 2.73887
[1mStep[0m  [48/169], [94mLoss[0m : 2.15226
[1mStep[0m  [64/169], [94mLoss[0m : 2.25076
[1mStep[0m  [80/169], [94mLoss[0m : 2.27636
[1mStep[0m  [96/169], [94mLoss[0m : 2.22002
[1mStep[0m  [112/169], [94mLoss[0m : 2.07492
[1mStep[0m  [128/169], [94mLoss[0m : 2.40266
[1mStep[0m  [144/169], [94mLoss[0m : 2.40387
[1mStep[0m  [160/169], [94mLoss[0m : 2.99387

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.333, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.45219
[1mStep[0m  [16/169], [94mLoss[0m : 2.72049
[1mStep[0m  [32/169], [94mLoss[0m : 1.80840
[1mStep[0m  [48/169], [94mLoss[0m : 2.83691
[1mStep[0m  [64/169], [94mLoss[0m : 2.44186
[1mStep[0m  [80/169], [94mLoss[0m : 2.21242
[1mStep[0m  [96/169], [94mLoss[0m : 2.35301
[1mStep[0m  [112/169], [94mLoss[0m : 2.65336
[1mStep[0m  [128/169], [94mLoss[0m : 2.35946
[1mStep[0m  [144/169], [94mLoss[0m : 2.34038
[1mStep[0m  [160/169], [94mLoss[0m : 2.82909

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.343, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.41776
[1mStep[0m  [16/169], [94mLoss[0m : 2.22502
[1mStep[0m  [32/169], [94mLoss[0m : 2.52062
[1mStep[0m  [48/169], [94mLoss[0m : 2.23562
[1mStep[0m  [64/169], [94mLoss[0m : 2.84636
[1mStep[0m  [80/169], [94mLoss[0m : 2.56987
[1mStep[0m  [96/169], [94mLoss[0m : 2.70825
[1mStep[0m  [112/169], [94mLoss[0m : 2.62707
[1mStep[0m  [128/169], [94mLoss[0m : 2.32463
[1mStep[0m  [144/169], [94mLoss[0m : 2.32267
[1mStep[0m  [160/169], [94mLoss[0m : 2.75879

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.331, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.43337
[1mStep[0m  [16/169], [94mLoss[0m : 2.94579
[1mStep[0m  [32/169], [94mLoss[0m : 2.64319
[1mStep[0m  [48/169], [94mLoss[0m : 1.98236
[1mStep[0m  [64/169], [94mLoss[0m : 2.08154
[1mStep[0m  [80/169], [94mLoss[0m : 2.44783
[1mStep[0m  [96/169], [94mLoss[0m : 2.12582
[1mStep[0m  [112/169], [94mLoss[0m : 2.31701
[1mStep[0m  [128/169], [94mLoss[0m : 2.64391
[1mStep[0m  [144/169], [94mLoss[0m : 2.30202
[1mStep[0m  [160/169], [94mLoss[0m : 2.84279

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.326, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.84431
[1mStep[0m  [16/169], [94mLoss[0m : 2.87193
[1mStep[0m  [32/169], [94mLoss[0m : 2.78062
[1mStep[0m  [48/169], [94mLoss[0m : 2.82813
[1mStep[0m  [64/169], [94mLoss[0m : 2.45603
[1mStep[0m  [80/169], [94mLoss[0m : 2.43269
[1mStep[0m  [96/169], [94mLoss[0m : 2.60706
[1mStep[0m  [112/169], [94mLoss[0m : 2.17192
[1mStep[0m  [128/169], [94mLoss[0m : 1.81304
[1mStep[0m  [144/169], [94mLoss[0m : 2.52059
[1mStep[0m  [160/169], [94mLoss[0m : 2.44943

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.321, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.71158
[1mStep[0m  [16/169], [94mLoss[0m : 2.60421
[1mStep[0m  [32/169], [94mLoss[0m : 2.09537
[1mStep[0m  [48/169], [94mLoss[0m : 2.30818
[1mStep[0m  [64/169], [94mLoss[0m : 2.59534
[1mStep[0m  [80/169], [94mLoss[0m : 2.71337
[1mStep[0m  [96/169], [94mLoss[0m : 2.45459
[1mStep[0m  [112/169], [94mLoss[0m : 2.76577
[1mStep[0m  [128/169], [94mLoss[0m : 2.57792
[1mStep[0m  [144/169], [94mLoss[0m : 2.47851
[1mStep[0m  [160/169], [94mLoss[0m : 2.66630

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.351, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.18983
[1mStep[0m  [16/169], [94mLoss[0m : 2.44052
[1mStep[0m  [32/169], [94mLoss[0m : 2.78006
[1mStep[0m  [48/169], [94mLoss[0m : 2.49709
[1mStep[0m  [64/169], [94mLoss[0m : 2.52782
[1mStep[0m  [80/169], [94mLoss[0m : 2.27942
[1mStep[0m  [96/169], [94mLoss[0m : 2.45730
[1mStep[0m  [112/169], [94mLoss[0m : 2.27408
[1mStep[0m  [128/169], [94mLoss[0m : 2.34982
[1mStep[0m  [144/169], [94mLoss[0m : 2.43377
[1mStep[0m  [160/169], [94mLoss[0m : 2.28169

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.338, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.38537
[1mStep[0m  [16/169], [94mLoss[0m : 2.71464
[1mStep[0m  [32/169], [94mLoss[0m : 2.37112
[1mStep[0m  [48/169], [94mLoss[0m : 2.18787
[1mStep[0m  [64/169], [94mLoss[0m : 2.35005
[1mStep[0m  [80/169], [94mLoss[0m : 2.40017
[1mStep[0m  [96/169], [94mLoss[0m : 2.64816
[1mStep[0m  [112/169], [94mLoss[0m : 1.95017
[1mStep[0m  [128/169], [94mLoss[0m : 2.31924
[1mStep[0m  [144/169], [94mLoss[0m : 2.49330
[1mStep[0m  [160/169], [94mLoss[0m : 2.14877

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.325, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.25664
[1mStep[0m  [16/169], [94mLoss[0m : 2.17157
[1mStep[0m  [32/169], [94mLoss[0m : 2.78130
[1mStep[0m  [48/169], [94mLoss[0m : 2.33061
[1mStep[0m  [64/169], [94mLoss[0m : 2.60517
[1mStep[0m  [80/169], [94mLoss[0m : 2.82106
[1mStep[0m  [96/169], [94mLoss[0m : 2.72737
[1mStep[0m  [112/169], [94mLoss[0m : 2.39264
[1mStep[0m  [128/169], [94mLoss[0m : 2.46410
[1mStep[0m  [144/169], [94mLoss[0m : 2.38447
[1mStep[0m  [160/169], [94mLoss[0m : 2.43977

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.337, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.32452
[1mStep[0m  [16/169], [94mLoss[0m : 2.48387
[1mStep[0m  [32/169], [94mLoss[0m : 2.31473
[1mStep[0m  [48/169], [94mLoss[0m : 2.84806
[1mStep[0m  [64/169], [94mLoss[0m : 2.51714
[1mStep[0m  [80/169], [94mLoss[0m : 2.39771
[1mStep[0m  [96/169], [94mLoss[0m : 2.39105
[1mStep[0m  [112/169], [94mLoss[0m : 2.45114
[1mStep[0m  [128/169], [94mLoss[0m : 2.27386
[1mStep[0m  [144/169], [94mLoss[0m : 2.60797
[1mStep[0m  [160/169], [94mLoss[0m : 2.33782

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.336, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.23981
[1mStep[0m  [16/169], [94mLoss[0m : 2.24481
[1mStep[0m  [32/169], [94mLoss[0m : 2.60517
[1mStep[0m  [48/169], [94mLoss[0m : 2.22861
[1mStep[0m  [64/169], [94mLoss[0m : 2.44760
[1mStep[0m  [80/169], [94mLoss[0m : 2.57517
[1mStep[0m  [96/169], [94mLoss[0m : 3.01162
[1mStep[0m  [112/169], [94mLoss[0m : 2.46406
[1mStep[0m  [128/169], [94mLoss[0m : 2.48489
[1mStep[0m  [144/169], [94mLoss[0m : 2.41998
[1mStep[0m  [160/169], [94mLoss[0m : 2.89325

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.334, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.24096
[1mStep[0m  [16/169], [94mLoss[0m : 2.71900
[1mStep[0m  [32/169], [94mLoss[0m : 2.41579
[1mStep[0m  [48/169], [94mLoss[0m : 1.96025
[1mStep[0m  [64/169], [94mLoss[0m : 2.42127
[1mStep[0m  [80/169], [94mLoss[0m : 2.35125
[1mStep[0m  [96/169], [94mLoss[0m : 2.37579
[1mStep[0m  [112/169], [94mLoss[0m : 2.53446
[1mStep[0m  [128/169], [94mLoss[0m : 2.86988
[1mStep[0m  [144/169], [94mLoss[0m : 2.38172
[1mStep[0m  [160/169], [94mLoss[0m : 2.60780

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.333, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.13699
[1mStep[0m  [16/169], [94mLoss[0m : 2.62588
[1mStep[0m  [32/169], [94mLoss[0m : 2.44495
[1mStep[0m  [48/169], [94mLoss[0m : 2.82283
[1mStep[0m  [64/169], [94mLoss[0m : 2.45511
[1mStep[0m  [80/169], [94mLoss[0m : 2.38251
[1mStep[0m  [96/169], [94mLoss[0m : 2.06077
[1mStep[0m  [112/169], [94mLoss[0m : 2.64881
[1mStep[0m  [128/169], [94mLoss[0m : 2.53900
[1mStep[0m  [144/169], [94mLoss[0m : 2.46088
[1mStep[0m  [160/169], [94mLoss[0m : 2.70407

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.344, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.59452
[1mStep[0m  [16/169], [94mLoss[0m : 2.14308
[1mStep[0m  [32/169], [94mLoss[0m : 2.82391
[1mStep[0m  [48/169], [94mLoss[0m : 2.26205
[1mStep[0m  [64/169], [94mLoss[0m : 2.56338
[1mStep[0m  [80/169], [94mLoss[0m : 2.33816
[1mStep[0m  [96/169], [94mLoss[0m : 2.18025
[1mStep[0m  [112/169], [94mLoss[0m : 2.32887
[1mStep[0m  [128/169], [94mLoss[0m : 2.27973
[1mStep[0m  [144/169], [94mLoss[0m : 2.77680
[1mStep[0m  [160/169], [94mLoss[0m : 2.37278

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.345, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.91948
[1mStep[0m  [16/169], [94mLoss[0m : 2.48873
[1mStep[0m  [32/169], [94mLoss[0m : 2.25076
[1mStep[0m  [48/169], [94mLoss[0m : 2.92398
[1mStep[0m  [64/169], [94mLoss[0m : 2.00584
[1mStep[0m  [80/169], [94mLoss[0m : 2.62308
[1mStep[0m  [96/169], [94mLoss[0m : 2.40724
[1mStep[0m  [112/169], [94mLoss[0m : 2.75948
[1mStep[0m  [128/169], [94mLoss[0m : 2.94734
[1mStep[0m  [144/169], [94mLoss[0m : 2.34635
[1mStep[0m  [160/169], [94mLoss[0m : 2.08660

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.331, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.19506
[1mStep[0m  [16/169], [94mLoss[0m : 2.42547
[1mStep[0m  [32/169], [94mLoss[0m : 2.46123
[1mStep[0m  [48/169], [94mLoss[0m : 2.40813
[1mStep[0m  [64/169], [94mLoss[0m : 2.84351
[1mStep[0m  [80/169], [94mLoss[0m : 2.64906
[1mStep[0m  [96/169], [94mLoss[0m : 2.28477
[1mStep[0m  [112/169], [94mLoss[0m : 2.51896
[1mStep[0m  [128/169], [94mLoss[0m : 2.44648
[1mStep[0m  [144/169], [94mLoss[0m : 2.37572
[1mStep[0m  [160/169], [94mLoss[0m : 2.60906

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.326, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.70013
[1mStep[0m  [16/169], [94mLoss[0m : 1.92315
[1mStep[0m  [32/169], [94mLoss[0m : 3.08594
[1mStep[0m  [48/169], [94mLoss[0m : 2.34205
[1mStep[0m  [64/169], [94mLoss[0m : 2.73113
[1mStep[0m  [80/169], [94mLoss[0m : 2.47679
[1mStep[0m  [96/169], [94mLoss[0m : 2.67273
[1mStep[0m  [112/169], [94mLoss[0m : 2.06358
[1mStep[0m  [128/169], [94mLoss[0m : 2.65967
[1mStep[0m  [144/169], [94mLoss[0m : 2.19667
[1mStep[0m  [160/169], [94mLoss[0m : 2.61715

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.338, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.334
====================================

Phase 1 - Evaluation MAE:  2.3337264508008957
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/169], [94mLoss[0m : 2.15734
[1mStep[0m  [16/169], [94mLoss[0m : 2.50345
[1mStep[0m  [32/169], [94mLoss[0m : 2.33818
[1mStep[0m  [48/169], [94mLoss[0m : 2.33241
[1mStep[0m  [64/169], [94mLoss[0m : 2.42927
[1mStep[0m  [80/169], [94mLoss[0m : 2.67055
[1mStep[0m  [96/169], [94mLoss[0m : 2.69978
[1mStep[0m  [112/169], [94mLoss[0m : 2.34962
[1mStep[0m  [128/169], [94mLoss[0m : 2.53228
[1mStep[0m  [144/169], [94mLoss[0m : 2.31783
[1mStep[0m  [160/169], [94mLoss[0m : 2.80364

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.329, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.21115
[1mStep[0m  [16/169], [94mLoss[0m : 2.41740
[1mStep[0m  [32/169], [94mLoss[0m : 2.72038
[1mStep[0m  [48/169], [94mLoss[0m : 2.11568
[1mStep[0m  [64/169], [94mLoss[0m : 2.49233
[1mStep[0m  [80/169], [94mLoss[0m : 2.15283
[1mStep[0m  [96/169], [94mLoss[0m : 2.12677
[1mStep[0m  [112/169], [94mLoss[0m : 2.25552
[1mStep[0m  [128/169], [94mLoss[0m : 2.44436
[1mStep[0m  [144/169], [94mLoss[0m : 2.52969
[1mStep[0m  [160/169], [94mLoss[0m : 2.37040

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.365, [92mTest[0m: 2.376, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.46921
[1mStep[0m  [16/169], [94mLoss[0m : 2.41166
[1mStep[0m  [32/169], [94mLoss[0m : 2.56509
[1mStep[0m  [48/169], [94mLoss[0m : 2.09033
[1mStep[0m  [64/169], [94mLoss[0m : 2.46237
[1mStep[0m  [80/169], [94mLoss[0m : 2.29711
[1mStep[0m  [96/169], [94mLoss[0m : 2.43526
[1mStep[0m  [112/169], [94mLoss[0m : 2.18528
[1mStep[0m  [128/169], [94mLoss[0m : 2.22036
[1mStep[0m  [144/169], [94mLoss[0m : 2.06604
[1mStep[0m  [160/169], [94mLoss[0m : 2.70345

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.314, [92mTest[0m: 2.400, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.28767
[1mStep[0m  [16/169], [94mLoss[0m : 2.13046
[1mStep[0m  [32/169], [94mLoss[0m : 2.03334
[1mStep[0m  [48/169], [94mLoss[0m : 1.88051
[1mStep[0m  [64/169], [94mLoss[0m : 2.53628
[1mStep[0m  [80/169], [94mLoss[0m : 1.94176
[1mStep[0m  [96/169], [94mLoss[0m : 2.02874
[1mStep[0m  [112/169], [94mLoss[0m : 2.32574
[1mStep[0m  [128/169], [94mLoss[0m : 2.82803
[1mStep[0m  [144/169], [94mLoss[0m : 2.15788
[1mStep[0m  [160/169], [94mLoss[0m : 2.20704

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.243, [92mTest[0m: 2.387, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.46661
[1mStep[0m  [16/169], [94mLoss[0m : 2.26032
[1mStep[0m  [32/169], [94mLoss[0m : 2.21210
[1mStep[0m  [48/169], [94mLoss[0m : 1.73943
[1mStep[0m  [64/169], [94mLoss[0m : 2.69285
[1mStep[0m  [80/169], [94mLoss[0m : 1.92844
[1mStep[0m  [96/169], [94mLoss[0m : 2.44832
[1mStep[0m  [112/169], [94mLoss[0m : 2.16974
[1mStep[0m  [128/169], [94mLoss[0m : 2.22943
[1mStep[0m  [144/169], [94mLoss[0m : 2.40287
[1mStep[0m  [160/169], [94mLoss[0m : 2.15378

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.196, [92mTest[0m: 2.356, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.83109
[1mStep[0m  [16/169], [94mLoss[0m : 1.78621
[1mStep[0m  [32/169], [94mLoss[0m : 1.85799
[1mStep[0m  [48/169], [94mLoss[0m : 1.98717
[1mStep[0m  [64/169], [94mLoss[0m : 2.29299
[1mStep[0m  [80/169], [94mLoss[0m : 1.89844
[1mStep[0m  [96/169], [94mLoss[0m : 2.15147
[1mStep[0m  [112/169], [94mLoss[0m : 2.51827
[1mStep[0m  [128/169], [94mLoss[0m : 2.21271
[1mStep[0m  [144/169], [94mLoss[0m : 2.18776
[1mStep[0m  [160/169], [94mLoss[0m : 2.26342

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.131, [92mTest[0m: 2.403, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.79124
[1mStep[0m  [16/169], [94mLoss[0m : 2.16306
[1mStep[0m  [32/169], [94mLoss[0m : 1.87898
[1mStep[0m  [48/169], [94mLoss[0m : 2.55349
[1mStep[0m  [64/169], [94mLoss[0m : 2.35331
[1mStep[0m  [80/169], [94mLoss[0m : 2.20509
[1mStep[0m  [96/169], [94mLoss[0m : 2.01123
[1mStep[0m  [112/169], [94mLoss[0m : 2.18831
[1mStep[0m  [128/169], [94mLoss[0m : 2.01209
[1mStep[0m  [144/169], [94mLoss[0m : 2.07969
[1mStep[0m  [160/169], [94mLoss[0m : 1.91980

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.068, [92mTest[0m: 2.394, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.09245
[1mStep[0m  [16/169], [94mLoss[0m : 2.18354
[1mStep[0m  [32/169], [94mLoss[0m : 2.07530
[1mStep[0m  [48/169], [94mLoss[0m : 2.22276
[1mStep[0m  [64/169], [94mLoss[0m : 1.74742
[1mStep[0m  [80/169], [94mLoss[0m : 2.18022
[1mStep[0m  [96/169], [94mLoss[0m : 2.29028
[1mStep[0m  [112/169], [94mLoss[0m : 2.08811
[1mStep[0m  [128/169], [94mLoss[0m : 2.26480
[1mStep[0m  [144/169], [94mLoss[0m : 2.18935
[1mStep[0m  [160/169], [94mLoss[0m : 1.90728

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.050, [92mTest[0m: 2.421, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.09427
[1mStep[0m  [16/169], [94mLoss[0m : 1.99410
[1mStep[0m  [32/169], [94mLoss[0m : 2.03627
[1mStep[0m  [48/169], [94mLoss[0m : 2.09067
[1mStep[0m  [64/169], [94mLoss[0m : 1.86094
[1mStep[0m  [80/169], [94mLoss[0m : 1.89338
[1mStep[0m  [96/169], [94mLoss[0m : 2.11922
[1mStep[0m  [112/169], [94mLoss[0m : 2.19285
[1mStep[0m  [128/169], [94mLoss[0m : 1.90305
[1mStep[0m  [144/169], [94mLoss[0m : 1.98545
[1mStep[0m  [160/169], [94mLoss[0m : 2.27952

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.992, [92mTest[0m: 2.409, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.02218
[1mStep[0m  [16/169], [94mLoss[0m : 1.80661
[1mStep[0m  [32/169], [94mLoss[0m : 1.90766
[1mStep[0m  [48/169], [94mLoss[0m : 1.83731
[1mStep[0m  [64/169], [94mLoss[0m : 1.72838
[1mStep[0m  [80/169], [94mLoss[0m : 1.91778
[1mStep[0m  [96/169], [94mLoss[0m : 1.62331
[1mStep[0m  [112/169], [94mLoss[0m : 1.59235
[1mStep[0m  [128/169], [94mLoss[0m : 2.21904
[1mStep[0m  [144/169], [94mLoss[0m : 1.66132
[1mStep[0m  [160/169], [94mLoss[0m : 2.37924

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.943, [92mTest[0m: 2.412, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.70700
[1mStep[0m  [16/169], [94mLoss[0m : 1.91106
[1mStep[0m  [32/169], [94mLoss[0m : 1.77621
[1mStep[0m  [48/169], [94mLoss[0m : 2.08702
[1mStep[0m  [64/169], [94mLoss[0m : 2.00567
[1mStep[0m  [80/169], [94mLoss[0m : 1.93731
[1mStep[0m  [96/169], [94mLoss[0m : 1.74930
[1mStep[0m  [112/169], [94mLoss[0m : 1.85827
[1mStep[0m  [128/169], [94mLoss[0m : 2.05689
[1mStep[0m  [144/169], [94mLoss[0m : 1.77273
[1mStep[0m  [160/169], [94mLoss[0m : 1.93838

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.918, [92mTest[0m: 2.468, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.71684
[1mStep[0m  [16/169], [94mLoss[0m : 2.07567
[1mStep[0m  [32/169], [94mLoss[0m : 2.25232
[1mStep[0m  [48/169], [94mLoss[0m : 1.96528
[1mStep[0m  [64/169], [94mLoss[0m : 2.20944
[1mStep[0m  [80/169], [94mLoss[0m : 1.58729
[1mStep[0m  [96/169], [94mLoss[0m : 1.46230
[1mStep[0m  [112/169], [94mLoss[0m : 1.95678
[1mStep[0m  [128/169], [94mLoss[0m : 2.15807
[1mStep[0m  [144/169], [94mLoss[0m : 1.96654
[1mStep[0m  [160/169], [94mLoss[0m : 1.81978

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.870, [92mTest[0m: 2.484, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.79479
[1mStep[0m  [16/169], [94mLoss[0m : 1.70565
[1mStep[0m  [32/169], [94mLoss[0m : 1.51953
[1mStep[0m  [48/169], [94mLoss[0m : 1.61425
[1mStep[0m  [64/169], [94mLoss[0m : 1.78884
[1mStep[0m  [80/169], [94mLoss[0m : 1.90416
[1mStep[0m  [96/169], [94mLoss[0m : 1.84396
[1mStep[0m  [112/169], [94mLoss[0m : 1.61531
[1mStep[0m  [128/169], [94mLoss[0m : 2.10510
[1mStep[0m  [144/169], [94mLoss[0m : 1.67429
[1mStep[0m  [160/169], [94mLoss[0m : 1.92170

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.836, [92mTest[0m: 2.416, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.27686
[1mStep[0m  [16/169], [94mLoss[0m : 1.85127
[1mStep[0m  [32/169], [94mLoss[0m : 1.71659
[1mStep[0m  [48/169], [94mLoss[0m : 1.53539
[1mStep[0m  [64/169], [94mLoss[0m : 1.64402
[1mStep[0m  [80/169], [94mLoss[0m : 1.77768
[1mStep[0m  [96/169], [94mLoss[0m : 1.74766
[1mStep[0m  [112/169], [94mLoss[0m : 1.85654
[1mStep[0m  [128/169], [94mLoss[0m : 1.83246
[1mStep[0m  [144/169], [94mLoss[0m : 1.95533
[1mStep[0m  [160/169], [94mLoss[0m : 1.70756

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.806, [92mTest[0m: 2.419, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.66270
[1mStep[0m  [16/169], [94mLoss[0m : 1.77716
[1mStep[0m  [32/169], [94mLoss[0m : 1.56394
[1mStep[0m  [48/169], [94mLoss[0m : 1.79983
[1mStep[0m  [64/169], [94mLoss[0m : 1.62768
[1mStep[0m  [80/169], [94mLoss[0m : 2.00069
[1mStep[0m  [96/169], [94mLoss[0m : 1.85632
[1mStep[0m  [112/169], [94mLoss[0m : 2.21550
[1mStep[0m  [128/169], [94mLoss[0m : 1.84249
[1mStep[0m  [144/169], [94mLoss[0m : 1.99076
[1mStep[0m  [160/169], [94mLoss[0m : 1.83351

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.795, [92mTest[0m: 2.505, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.67674
[1mStep[0m  [16/169], [94mLoss[0m : 1.95288
[1mStep[0m  [32/169], [94mLoss[0m : 1.72232
[1mStep[0m  [48/169], [94mLoss[0m : 1.50950
[1mStep[0m  [64/169], [94mLoss[0m : 1.41892
[1mStep[0m  [80/169], [94mLoss[0m : 1.57591
[1mStep[0m  [96/169], [94mLoss[0m : 1.62413
[1mStep[0m  [112/169], [94mLoss[0m : 1.50539
[1mStep[0m  [128/169], [94mLoss[0m : 2.27220
[1mStep[0m  [144/169], [94mLoss[0m : 1.87682
[1mStep[0m  [160/169], [94mLoss[0m : 1.45997

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.753, [92mTest[0m: 2.510, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.92323
[1mStep[0m  [16/169], [94mLoss[0m : 1.56885
[1mStep[0m  [32/169], [94mLoss[0m : 1.49037
[1mStep[0m  [48/169], [94mLoss[0m : 1.70190
[1mStep[0m  [64/169], [94mLoss[0m : 1.32163
[1mStep[0m  [80/169], [94mLoss[0m : 1.38352
[1mStep[0m  [96/169], [94mLoss[0m : 1.92000
[1mStep[0m  [112/169], [94mLoss[0m : 2.08148
[1mStep[0m  [128/169], [94mLoss[0m : 1.81648
[1mStep[0m  [144/169], [94mLoss[0m : 1.54257
[1mStep[0m  [160/169], [94mLoss[0m : 1.87208

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.717, [92mTest[0m: 2.453, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.62231
[1mStep[0m  [16/169], [94mLoss[0m : 1.76605
[1mStep[0m  [32/169], [94mLoss[0m : 1.60440
[1mStep[0m  [48/169], [94mLoss[0m : 1.60090
[1mStep[0m  [64/169], [94mLoss[0m : 1.48774
[1mStep[0m  [80/169], [94mLoss[0m : 1.87446
[1mStep[0m  [96/169], [94mLoss[0m : 1.52502
[1mStep[0m  [112/169], [94mLoss[0m : 2.04511
[1mStep[0m  [128/169], [94mLoss[0m : 1.62548
[1mStep[0m  [144/169], [94mLoss[0m : 1.97341
[1mStep[0m  [160/169], [94mLoss[0m : 1.95173

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.714, [92mTest[0m: 2.517, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.40991
[1mStep[0m  [16/169], [94mLoss[0m : 1.83314
[1mStep[0m  [32/169], [94mLoss[0m : 1.40590
[1mStep[0m  [48/169], [94mLoss[0m : 1.61220
[1mStep[0m  [64/169], [94mLoss[0m : 1.83166
[1mStep[0m  [80/169], [94mLoss[0m : 1.66392
[1mStep[0m  [96/169], [94mLoss[0m : 1.67242
[1mStep[0m  [112/169], [94mLoss[0m : 2.20970
[1mStep[0m  [128/169], [94mLoss[0m : 1.56566
[1mStep[0m  [144/169], [94mLoss[0m : 1.77207
[1mStep[0m  [160/169], [94mLoss[0m : 1.82900

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.706, [92mTest[0m: 2.506, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.62685
[1mStep[0m  [16/169], [94mLoss[0m : 1.60797
[1mStep[0m  [32/169], [94mLoss[0m : 1.71108
[1mStep[0m  [48/169], [94mLoss[0m : 1.49772
[1mStep[0m  [64/169], [94mLoss[0m : 1.42091
[1mStep[0m  [80/169], [94mLoss[0m : 1.74050
[1mStep[0m  [96/169], [94mLoss[0m : 1.74980
[1mStep[0m  [112/169], [94mLoss[0m : 1.41383
[1mStep[0m  [128/169], [94mLoss[0m : 1.64114
[1mStep[0m  [144/169], [94mLoss[0m : 1.50199
[1mStep[0m  [160/169], [94mLoss[0m : 1.71414

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.674, [92mTest[0m: 2.515, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.66969
[1mStep[0m  [16/169], [94mLoss[0m : 1.43675
[1mStep[0m  [32/169], [94mLoss[0m : 1.45026
[1mStep[0m  [48/169], [94mLoss[0m : 1.43404
[1mStep[0m  [64/169], [94mLoss[0m : 1.36963
[1mStep[0m  [80/169], [94mLoss[0m : 1.39598
[1mStep[0m  [96/169], [94mLoss[0m : 1.45718
[1mStep[0m  [112/169], [94mLoss[0m : 1.59276
[1mStep[0m  [128/169], [94mLoss[0m : 1.84560
[1mStep[0m  [144/169], [94mLoss[0m : 1.47632
[1mStep[0m  [160/169], [94mLoss[0m : 1.85572

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.595, [92mTest[0m: 2.538, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.41002
[1mStep[0m  [16/169], [94mLoss[0m : 1.37338
[1mStep[0m  [32/169], [94mLoss[0m : 1.41258
[1mStep[0m  [48/169], [94mLoss[0m : 1.38326
[1mStep[0m  [64/169], [94mLoss[0m : 1.58085
[1mStep[0m  [80/169], [94mLoss[0m : 1.68941
[1mStep[0m  [96/169], [94mLoss[0m : 1.48540
[1mStep[0m  [112/169], [94mLoss[0m : 1.40139
[1mStep[0m  [128/169], [94mLoss[0m : 1.90994
[1mStep[0m  [144/169], [94mLoss[0m : 1.55875
[1mStep[0m  [160/169], [94mLoss[0m : 1.22931

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.582, [92mTest[0m: 2.595, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.42486
[1mStep[0m  [16/169], [94mLoss[0m : 1.47641
[1mStep[0m  [32/169], [94mLoss[0m : 1.47696
[1mStep[0m  [48/169], [94mLoss[0m : 1.73396
[1mStep[0m  [64/169], [94mLoss[0m : 1.67609
[1mStep[0m  [80/169], [94mLoss[0m : 1.60889
[1mStep[0m  [96/169], [94mLoss[0m : 1.31363
[1mStep[0m  [112/169], [94mLoss[0m : 1.96086
[1mStep[0m  [128/169], [94mLoss[0m : 1.44183
[1mStep[0m  [144/169], [94mLoss[0m : 1.35375
[1mStep[0m  [160/169], [94mLoss[0m : 1.70702

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.584, [92mTest[0m: 2.469, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.32907
[1mStep[0m  [16/169], [94mLoss[0m : 1.38387
[1mStep[0m  [32/169], [94mLoss[0m : 1.49563
[1mStep[0m  [48/169], [94mLoss[0m : 1.47534
[1mStep[0m  [64/169], [94mLoss[0m : 1.47341
[1mStep[0m  [80/169], [94mLoss[0m : 1.98404
[1mStep[0m  [96/169], [94mLoss[0m : 1.59411
[1mStep[0m  [112/169], [94mLoss[0m : 1.42604
[1mStep[0m  [128/169], [94mLoss[0m : 1.57277
[1mStep[0m  [144/169], [94mLoss[0m : 1.39879
[1mStep[0m  [160/169], [94mLoss[0m : 1.67445

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.554, [92mTest[0m: 2.519, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.79794
[1mStep[0m  [16/169], [94mLoss[0m : 1.79730
[1mStep[0m  [32/169], [94mLoss[0m : 1.48467
[1mStep[0m  [48/169], [94mLoss[0m : 1.49407
[1mStep[0m  [64/169], [94mLoss[0m : 1.31881
[1mStep[0m  [80/169], [94mLoss[0m : 1.34263
[1mStep[0m  [96/169], [94mLoss[0m : 1.34739
[1mStep[0m  [112/169], [94mLoss[0m : 1.71300
[1mStep[0m  [128/169], [94mLoss[0m : 1.56249
[1mStep[0m  [144/169], [94mLoss[0m : 1.58837
[1mStep[0m  [160/169], [94mLoss[0m : 1.60851

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.534, [92mTest[0m: 2.533, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.46241
[1mStep[0m  [16/169], [94mLoss[0m : 1.53490
[1mStep[0m  [32/169], [94mLoss[0m : 1.62422
[1mStep[0m  [48/169], [94mLoss[0m : 1.29714
[1mStep[0m  [64/169], [94mLoss[0m : 1.58212
[1mStep[0m  [80/169], [94mLoss[0m : 1.35926
[1mStep[0m  [96/169], [94mLoss[0m : 1.60349
[1mStep[0m  [112/169], [94mLoss[0m : 1.58743
[1mStep[0m  [128/169], [94mLoss[0m : 1.71286
[1mStep[0m  [144/169], [94mLoss[0m : 1.22556
[1mStep[0m  [160/169], [94mLoss[0m : 1.50343

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.512, [92mTest[0m: 2.489, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.51150
[1mStep[0m  [16/169], [94mLoss[0m : 1.58980
[1mStep[0m  [32/169], [94mLoss[0m : 1.49553
[1mStep[0m  [48/169], [94mLoss[0m : 1.58316
[1mStep[0m  [64/169], [94mLoss[0m : 1.60368
[1mStep[0m  [80/169], [94mLoss[0m : 1.34191
[1mStep[0m  [96/169], [94mLoss[0m : 1.41849
[1mStep[0m  [112/169], [94mLoss[0m : 1.39277
[1mStep[0m  [128/169], [94mLoss[0m : 1.52697
[1mStep[0m  [144/169], [94mLoss[0m : 1.48090
[1mStep[0m  [160/169], [94mLoss[0m : 1.39015

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.501, [92mTest[0m: 2.563, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.39101
[1mStep[0m  [16/169], [94mLoss[0m : 1.31639
[1mStep[0m  [32/169], [94mLoss[0m : 1.73760
[1mStep[0m  [48/169], [94mLoss[0m : 1.31503
[1mStep[0m  [64/169], [94mLoss[0m : 1.59152
[1mStep[0m  [80/169], [94mLoss[0m : 1.31836
[1mStep[0m  [96/169], [94mLoss[0m : 1.57749
[1mStep[0m  [112/169], [94mLoss[0m : 1.26771
[1mStep[0m  [128/169], [94mLoss[0m : 1.64875
[1mStep[0m  [144/169], [94mLoss[0m : 1.83249
[1mStep[0m  [160/169], [94mLoss[0m : 1.29037

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.482, [92mTest[0m: 2.533, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.28904
[1mStep[0m  [16/169], [94mLoss[0m : 1.43178
[1mStep[0m  [32/169], [94mLoss[0m : 1.37133
[1mStep[0m  [48/169], [94mLoss[0m : 1.29272
[1mStep[0m  [64/169], [94mLoss[0m : 1.44287
[1mStep[0m  [80/169], [94mLoss[0m : 1.47076
[1mStep[0m  [96/169], [94mLoss[0m : 1.69315
[1mStep[0m  [112/169], [94mLoss[0m : 1.83532
[1mStep[0m  [128/169], [94mLoss[0m : 1.47970
[1mStep[0m  [144/169], [94mLoss[0m : 1.72278
[1mStep[0m  [160/169], [94mLoss[0m : 1.66225

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.489, [92mTest[0m: 2.515, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.40033
[1mStep[0m  [16/169], [94mLoss[0m : 1.58065
[1mStep[0m  [32/169], [94mLoss[0m : 1.45261
[1mStep[0m  [48/169], [94mLoss[0m : 1.65094
[1mStep[0m  [64/169], [94mLoss[0m : 1.86029
[1mStep[0m  [80/169], [94mLoss[0m : 1.50886
[1mStep[0m  [96/169], [94mLoss[0m : 1.55272
[1mStep[0m  [112/169], [94mLoss[0m : 1.59814
[1mStep[0m  [128/169], [94mLoss[0m : 1.24366
[1mStep[0m  [144/169], [94mLoss[0m : 2.10199
[1mStep[0m  [160/169], [94mLoss[0m : 1.47202

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.472, [92mTest[0m: 2.561, [96mlr[0m: 0.004545
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 29 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.493
====================================

Phase 2 - Evaluation MAE:  2.4929401108196805
MAE score P1       2.333726
MAE score P2        2.49294
loss               1.471803
learning_rate       0.00505
batch_size               64
hidden_sizes      [200, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping         True
dropout                 0.3
momentum                0.9
weight_decay          0.001
Name: 0, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 11.25614
[1mStep[0m  [8/84], [94mLoss[0m : 9.88629
[1mStep[0m  [16/84], [94mLoss[0m : 9.13197
[1mStep[0m  [24/84], [94mLoss[0m : 7.31305
[1mStep[0m  [32/84], [94mLoss[0m : 6.00518
[1mStep[0m  [40/84], [94mLoss[0m : 5.50261
[1mStep[0m  [48/84], [94mLoss[0m : 3.74989
[1mStep[0m  [56/84], [94mLoss[0m : 2.92037
[1mStep[0m  [64/84], [94mLoss[0m : 2.96731
[1mStep[0m  [72/84], [94mLoss[0m : 2.58416
[1mStep[0m  [80/84], [94mLoss[0m : 2.69272

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 5.747, [92mTest[0m: 11.035, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49820
[1mStep[0m  [8/84], [94mLoss[0m : 3.17745
[1mStep[0m  [16/84], [94mLoss[0m : 2.67122
[1mStep[0m  [24/84], [94mLoss[0m : 2.61332
[1mStep[0m  [32/84], [94mLoss[0m : 2.50400
[1mStep[0m  [40/84], [94mLoss[0m : 3.22036
[1mStep[0m  [48/84], [94mLoss[0m : 3.11134
[1mStep[0m  [56/84], [94mLoss[0m : 2.86826
[1mStep[0m  [64/84], [94mLoss[0m : 2.62789
[1mStep[0m  [72/84], [94mLoss[0m : 2.93522
[1mStep[0m  [80/84], [94mLoss[0m : 2.84405

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.781, [92mTest[0m: 2.947, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.09661
[1mStep[0m  [8/84], [94mLoss[0m : 2.69942
[1mStep[0m  [16/84], [94mLoss[0m : 2.67746
[1mStep[0m  [24/84], [94mLoss[0m : 2.47585
[1mStep[0m  [32/84], [94mLoss[0m : 2.85240
[1mStep[0m  [40/84], [94mLoss[0m : 2.76869
[1mStep[0m  [48/84], [94mLoss[0m : 2.86035
[1mStep[0m  [56/84], [94mLoss[0m : 2.90983
[1mStep[0m  [64/84], [94mLoss[0m : 2.43411
[1mStep[0m  [72/84], [94mLoss[0m : 2.43181
[1mStep[0m  [80/84], [94mLoss[0m : 2.98046

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.741, [92mTest[0m: 2.530, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.61773
[1mStep[0m  [8/84], [94mLoss[0m : 2.80261
[1mStep[0m  [16/84], [94mLoss[0m : 2.76813
[1mStep[0m  [24/84], [94mLoss[0m : 2.65925
[1mStep[0m  [32/84], [94mLoss[0m : 2.58295
[1mStep[0m  [40/84], [94mLoss[0m : 2.65213
[1mStep[0m  [48/84], [94mLoss[0m : 2.46464
[1mStep[0m  [56/84], [94mLoss[0m : 2.82815
[1mStep[0m  [64/84], [94mLoss[0m : 2.54359
[1mStep[0m  [72/84], [94mLoss[0m : 2.88563
[1mStep[0m  [80/84], [94mLoss[0m : 2.54076

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.668, [92mTest[0m: 2.496, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.61652
[1mStep[0m  [8/84], [94mLoss[0m : 2.66110
[1mStep[0m  [16/84], [94mLoss[0m : 2.79255
[1mStep[0m  [24/84], [94mLoss[0m : 3.04493
[1mStep[0m  [32/84], [94mLoss[0m : 2.40405
[1mStep[0m  [40/84], [94mLoss[0m : 2.82534
[1mStep[0m  [48/84], [94mLoss[0m : 2.31597
[1mStep[0m  [56/84], [94mLoss[0m : 2.95241
[1mStep[0m  [64/84], [94mLoss[0m : 2.62008
[1mStep[0m  [72/84], [94mLoss[0m : 2.82933
[1mStep[0m  [80/84], [94mLoss[0m : 2.88988

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.654, [92mTest[0m: 2.454, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59784
[1mStep[0m  [8/84], [94mLoss[0m : 2.58163
[1mStep[0m  [16/84], [94mLoss[0m : 2.37676
[1mStep[0m  [24/84], [94mLoss[0m : 2.43518
[1mStep[0m  [32/84], [94mLoss[0m : 2.91333
[1mStep[0m  [40/84], [94mLoss[0m : 2.44120
[1mStep[0m  [48/84], [94mLoss[0m : 2.37799
[1mStep[0m  [56/84], [94mLoss[0m : 2.40500
[1mStep[0m  [64/84], [94mLoss[0m : 2.54672
[1mStep[0m  [72/84], [94mLoss[0m : 2.27766
[1mStep[0m  [80/84], [94mLoss[0m : 2.75500

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.642, [92mTest[0m: 2.575, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50564
[1mStep[0m  [8/84], [94mLoss[0m : 2.37920
[1mStep[0m  [16/84], [94mLoss[0m : 2.76763
[1mStep[0m  [24/84], [94mLoss[0m : 2.89320
[1mStep[0m  [32/84], [94mLoss[0m : 2.59588
[1mStep[0m  [40/84], [94mLoss[0m : 2.52133
[1mStep[0m  [48/84], [94mLoss[0m : 2.68559
[1mStep[0m  [56/84], [94mLoss[0m : 2.30604
[1mStep[0m  [64/84], [94mLoss[0m : 2.34088
[1mStep[0m  [72/84], [94mLoss[0m : 2.65390
[1mStep[0m  [80/84], [94mLoss[0m : 2.68684

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.629, [92mTest[0m: 2.418, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52293
[1mStep[0m  [8/84], [94mLoss[0m : 2.66909
[1mStep[0m  [16/84], [94mLoss[0m : 2.80440
[1mStep[0m  [24/84], [94mLoss[0m : 2.92562
[1mStep[0m  [32/84], [94mLoss[0m : 2.47758
[1mStep[0m  [40/84], [94mLoss[0m : 2.57959
[1mStep[0m  [48/84], [94mLoss[0m : 2.37125
[1mStep[0m  [56/84], [94mLoss[0m : 2.60276
[1mStep[0m  [64/84], [94mLoss[0m : 2.90437
[1mStep[0m  [72/84], [94mLoss[0m : 2.74227
[1mStep[0m  [80/84], [94mLoss[0m : 2.25677

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.595, [92mTest[0m: 2.433, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53244
[1mStep[0m  [8/84], [94mLoss[0m : 2.65962
[1mStep[0m  [16/84], [94mLoss[0m : 2.33312
[1mStep[0m  [24/84], [94mLoss[0m : 2.20567
[1mStep[0m  [32/84], [94mLoss[0m : 2.33242
[1mStep[0m  [40/84], [94mLoss[0m : 2.37336
[1mStep[0m  [48/84], [94mLoss[0m : 2.55349
[1mStep[0m  [56/84], [94mLoss[0m : 2.44793
[1mStep[0m  [64/84], [94mLoss[0m : 2.66225
[1mStep[0m  [72/84], [94mLoss[0m : 2.50508
[1mStep[0m  [80/84], [94mLoss[0m : 2.45054

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.607, [92mTest[0m: 2.362, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54158
[1mStep[0m  [8/84], [94mLoss[0m : 2.74412
[1mStep[0m  [16/84], [94mLoss[0m : 2.56454
[1mStep[0m  [24/84], [94mLoss[0m : 2.58340
[1mStep[0m  [32/84], [94mLoss[0m : 2.66914
[1mStep[0m  [40/84], [94mLoss[0m : 2.81216
[1mStep[0m  [48/84], [94mLoss[0m : 2.52620
[1mStep[0m  [56/84], [94mLoss[0m : 2.63238
[1mStep[0m  [64/84], [94mLoss[0m : 2.21664
[1mStep[0m  [72/84], [94mLoss[0m : 2.64987
[1mStep[0m  [80/84], [94mLoss[0m : 2.59382

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.583, [92mTest[0m: 2.390, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55707
[1mStep[0m  [8/84], [94mLoss[0m : 2.73178
[1mStep[0m  [16/84], [94mLoss[0m : 2.46251
[1mStep[0m  [24/84], [94mLoss[0m : 2.60799
[1mStep[0m  [32/84], [94mLoss[0m : 2.42036
[1mStep[0m  [40/84], [94mLoss[0m : 2.66575
[1mStep[0m  [48/84], [94mLoss[0m : 2.57344
[1mStep[0m  [56/84], [94mLoss[0m : 2.60286
[1mStep[0m  [64/84], [94mLoss[0m : 2.64417
[1mStep[0m  [72/84], [94mLoss[0m : 2.57928
[1mStep[0m  [80/84], [94mLoss[0m : 2.69365

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.586, [92mTest[0m: 2.362, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.80785
[1mStep[0m  [8/84], [94mLoss[0m : 2.58882
[1mStep[0m  [16/84], [94mLoss[0m : 2.50918
[1mStep[0m  [24/84], [94mLoss[0m : 2.45441
[1mStep[0m  [32/84], [94mLoss[0m : 2.74559
[1mStep[0m  [40/84], [94mLoss[0m : 2.47818
[1mStep[0m  [48/84], [94mLoss[0m : 2.53498
[1mStep[0m  [56/84], [94mLoss[0m : 2.40621
[1mStep[0m  [64/84], [94mLoss[0m : 2.49902
[1mStep[0m  [72/84], [94mLoss[0m : 2.62592
[1mStep[0m  [80/84], [94mLoss[0m : 2.51783

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.564, [92mTest[0m: 2.426, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.81004
[1mStep[0m  [8/84], [94mLoss[0m : 2.56913
[1mStep[0m  [16/84], [94mLoss[0m : 2.29852
[1mStep[0m  [24/84], [94mLoss[0m : 2.52294
[1mStep[0m  [32/84], [94mLoss[0m : 2.81638
[1mStep[0m  [40/84], [94mLoss[0m : 2.31771
[1mStep[0m  [48/84], [94mLoss[0m : 2.40293
[1mStep[0m  [56/84], [94mLoss[0m : 2.53847
[1mStep[0m  [64/84], [94mLoss[0m : 2.66110
[1mStep[0m  [72/84], [94mLoss[0m : 2.48348
[1mStep[0m  [80/84], [94mLoss[0m : 2.67464

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.548, [92mTest[0m: 2.401, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.28413
[1mStep[0m  [8/84], [94mLoss[0m : 2.73011
[1mStep[0m  [16/84], [94mLoss[0m : 2.70582
[1mStep[0m  [24/84], [94mLoss[0m : 2.99892
[1mStep[0m  [32/84], [94mLoss[0m : 2.52488
[1mStep[0m  [40/84], [94mLoss[0m : 2.75677
[1mStep[0m  [48/84], [94mLoss[0m : 2.25891
[1mStep[0m  [56/84], [94mLoss[0m : 2.45184
[1mStep[0m  [64/84], [94mLoss[0m : 2.50767
[1mStep[0m  [72/84], [94mLoss[0m : 2.27681
[1mStep[0m  [80/84], [94mLoss[0m : 2.50382

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.560, [92mTest[0m: 2.388, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.65937
[1mStep[0m  [8/84], [94mLoss[0m : 2.92998
[1mStep[0m  [16/84], [94mLoss[0m : 2.38639
[1mStep[0m  [24/84], [94mLoss[0m : 2.46204
[1mStep[0m  [32/84], [94mLoss[0m : 2.75413
[1mStep[0m  [40/84], [94mLoss[0m : 2.64256
[1mStep[0m  [48/84], [94mLoss[0m : 2.41026
[1mStep[0m  [56/84], [94mLoss[0m : 2.50360
[1mStep[0m  [64/84], [94mLoss[0m : 2.53615
[1mStep[0m  [72/84], [94mLoss[0m : 2.91990
[1mStep[0m  [80/84], [94mLoss[0m : 2.29379

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.538, [92mTest[0m: 2.374, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.57742
[1mStep[0m  [8/84], [94mLoss[0m : 2.33238
[1mStep[0m  [16/84], [94mLoss[0m : 2.50687
[1mStep[0m  [24/84], [94mLoss[0m : 2.71821
[1mStep[0m  [32/84], [94mLoss[0m : 2.46539
[1mStep[0m  [40/84], [94mLoss[0m : 2.29259
[1mStep[0m  [48/84], [94mLoss[0m : 2.57435
[1mStep[0m  [56/84], [94mLoss[0m : 2.62809
[1mStep[0m  [64/84], [94mLoss[0m : 2.57928
[1mStep[0m  [72/84], [94mLoss[0m : 2.25326
[1mStep[0m  [80/84], [94mLoss[0m : 2.65807

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.531, [92mTest[0m: 2.386, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37705
[1mStep[0m  [8/84], [94mLoss[0m : 2.45401
[1mStep[0m  [16/84], [94mLoss[0m : 2.78739
[1mStep[0m  [24/84], [94mLoss[0m : 2.34953
[1mStep[0m  [32/84], [94mLoss[0m : 2.33873
[1mStep[0m  [40/84], [94mLoss[0m : 2.27684
[1mStep[0m  [48/84], [94mLoss[0m : 2.79834
[1mStep[0m  [56/84], [94mLoss[0m : 2.58972
[1mStep[0m  [64/84], [94mLoss[0m : 2.61256
[1mStep[0m  [72/84], [94mLoss[0m : 2.42717
[1mStep[0m  [80/84], [94mLoss[0m : 2.90583

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.528, [92mTest[0m: 2.332, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46682
[1mStep[0m  [8/84], [94mLoss[0m : 2.41657
[1mStep[0m  [16/84], [94mLoss[0m : 2.29307
[1mStep[0m  [24/84], [94mLoss[0m : 2.50017
[1mStep[0m  [32/84], [94mLoss[0m : 2.71914
[1mStep[0m  [40/84], [94mLoss[0m : 2.65963
[1mStep[0m  [48/84], [94mLoss[0m : 2.48883
[1mStep[0m  [56/84], [94mLoss[0m : 2.30118
[1mStep[0m  [64/84], [94mLoss[0m : 2.57265
[1mStep[0m  [72/84], [94mLoss[0m : 2.42979
[1mStep[0m  [80/84], [94mLoss[0m : 2.58249

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.337, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52179
[1mStep[0m  [8/84], [94mLoss[0m : 2.37945
[1mStep[0m  [16/84], [94mLoss[0m : 2.43818
[1mStep[0m  [24/84], [94mLoss[0m : 2.61586
[1mStep[0m  [32/84], [94mLoss[0m : 2.73116
[1mStep[0m  [40/84], [94mLoss[0m : 2.65996
[1mStep[0m  [48/84], [94mLoss[0m : 2.55270
[1mStep[0m  [56/84], [94mLoss[0m : 2.45831
[1mStep[0m  [64/84], [94mLoss[0m : 2.46419
[1mStep[0m  [72/84], [94mLoss[0m : 2.63577
[1mStep[0m  [80/84], [94mLoss[0m : 2.40318

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.345, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49655
[1mStep[0m  [8/84], [94mLoss[0m : 2.21963
[1mStep[0m  [16/84], [94mLoss[0m : 2.38070
[1mStep[0m  [24/84], [94mLoss[0m : 2.77292
[1mStep[0m  [32/84], [94mLoss[0m : 2.21268
[1mStep[0m  [40/84], [94mLoss[0m : 2.62452
[1mStep[0m  [48/84], [94mLoss[0m : 2.48729
[1mStep[0m  [56/84], [94mLoss[0m : 2.52666
[1mStep[0m  [64/84], [94mLoss[0m : 2.42139
[1mStep[0m  [72/84], [94mLoss[0m : 2.48505
[1mStep[0m  [80/84], [94mLoss[0m : 2.52496

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.329, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52635
[1mStep[0m  [8/84], [94mLoss[0m : 2.22289
[1mStep[0m  [16/84], [94mLoss[0m : 2.82072
[1mStep[0m  [24/84], [94mLoss[0m : 2.27307
[1mStep[0m  [32/84], [94mLoss[0m : 3.09456
[1mStep[0m  [40/84], [94mLoss[0m : 2.29719
[1mStep[0m  [48/84], [94mLoss[0m : 2.27583
[1mStep[0m  [56/84], [94mLoss[0m : 2.62087
[1mStep[0m  [64/84], [94mLoss[0m : 2.40098
[1mStep[0m  [72/84], [94mLoss[0m : 2.53902
[1mStep[0m  [80/84], [94mLoss[0m : 2.54041

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.327, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59809
[1mStep[0m  [8/84], [94mLoss[0m : 2.40980
[1mStep[0m  [16/84], [94mLoss[0m : 2.12146
[1mStep[0m  [24/84], [94mLoss[0m : 2.68576
[1mStep[0m  [32/84], [94mLoss[0m : 2.60283
[1mStep[0m  [40/84], [94mLoss[0m : 2.47088
[1mStep[0m  [48/84], [94mLoss[0m : 2.25023
[1mStep[0m  [56/84], [94mLoss[0m : 2.57271
[1mStep[0m  [64/84], [94mLoss[0m : 2.48731
[1mStep[0m  [72/84], [94mLoss[0m : 2.20935
[1mStep[0m  [80/84], [94mLoss[0m : 2.66837

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.347, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50063
[1mStep[0m  [8/84], [94mLoss[0m : 2.63408
[1mStep[0m  [16/84], [94mLoss[0m : 2.61066
[1mStep[0m  [24/84], [94mLoss[0m : 2.40962
[1mStep[0m  [32/84], [94mLoss[0m : 2.49408
[1mStep[0m  [40/84], [94mLoss[0m : 2.33166
[1mStep[0m  [48/84], [94mLoss[0m : 2.46094
[1mStep[0m  [56/84], [94mLoss[0m : 2.71476
[1mStep[0m  [64/84], [94mLoss[0m : 2.84642
[1mStep[0m  [72/84], [94mLoss[0m : 2.46305
[1mStep[0m  [80/84], [94mLoss[0m : 2.46685

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.524, [92mTest[0m: 2.331, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.72963
[1mStep[0m  [8/84], [94mLoss[0m : 2.59976
[1mStep[0m  [16/84], [94mLoss[0m : 2.41087
[1mStep[0m  [24/84], [94mLoss[0m : 2.60032
[1mStep[0m  [32/84], [94mLoss[0m : 2.34325
[1mStep[0m  [40/84], [94mLoss[0m : 2.40755
[1mStep[0m  [48/84], [94mLoss[0m : 2.66726
[1mStep[0m  [56/84], [94mLoss[0m : 2.38481
[1mStep[0m  [64/84], [94mLoss[0m : 2.49308
[1mStep[0m  [72/84], [94mLoss[0m : 2.53527
[1mStep[0m  [80/84], [94mLoss[0m : 2.38717

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.327, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48982
[1mStep[0m  [8/84], [94mLoss[0m : 2.54280
[1mStep[0m  [16/84], [94mLoss[0m : 2.56220
[1mStep[0m  [24/84], [94mLoss[0m : 2.58780
[1mStep[0m  [32/84], [94mLoss[0m : 2.74811
[1mStep[0m  [40/84], [94mLoss[0m : 2.52085
[1mStep[0m  [48/84], [94mLoss[0m : 2.70768
[1mStep[0m  [56/84], [94mLoss[0m : 2.51810
[1mStep[0m  [64/84], [94mLoss[0m : 2.27129
[1mStep[0m  [72/84], [94mLoss[0m : 2.55259
[1mStep[0m  [80/84], [94mLoss[0m : 2.27390

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.333, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.63197
[1mStep[0m  [8/84], [94mLoss[0m : 2.39998
[1mStep[0m  [16/84], [94mLoss[0m : 1.93051
[1mStep[0m  [24/84], [94mLoss[0m : 2.44384
[1mStep[0m  [32/84], [94mLoss[0m : 2.35585
[1mStep[0m  [40/84], [94mLoss[0m : 2.62182
[1mStep[0m  [48/84], [94mLoss[0m : 2.38282
[1mStep[0m  [56/84], [94mLoss[0m : 2.37680
[1mStep[0m  [64/84], [94mLoss[0m : 2.46582
[1mStep[0m  [72/84], [94mLoss[0m : 2.69776
[1mStep[0m  [80/84], [94mLoss[0m : 2.43797

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.340, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55329
[1mStep[0m  [8/84], [94mLoss[0m : 2.33643
[1mStep[0m  [16/84], [94mLoss[0m : 2.69971
[1mStep[0m  [24/84], [94mLoss[0m : 2.11282
[1mStep[0m  [32/84], [94mLoss[0m : 2.54591
[1mStep[0m  [40/84], [94mLoss[0m : 2.07772
[1mStep[0m  [48/84], [94mLoss[0m : 2.42720
[1mStep[0m  [56/84], [94mLoss[0m : 2.44443
[1mStep[0m  [64/84], [94mLoss[0m : 2.49454
[1mStep[0m  [72/84], [94mLoss[0m : 2.41869
[1mStep[0m  [80/84], [94mLoss[0m : 2.19741

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.313, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30928
[1mStep[0m  [8/84], [94mLoss[0m : 2.52341
[1mStep[0m  [16/84], [94mLoss[0m : 2.55812
[1mStep[0m  [24/84], [94mLoss[0m : 2.58350
[1mStep[0m  [32/84], [94mLoss[0m : 2.49794
[1mStep[0m  [40/84], [94mLoss[0m : 2.28407
[1mStep[0m  [48/84], [94mLoss[0m : 2.53101
[1mStep[0m  [56/84], [94mLoss[0m : 2.64525
[1mStep[0m  [64/84], [94mLoss[0m : 2.39570
[1mStep[0m  [72/84], [94mLoss[0m : 2.33866
[1mStep[0m  [80/84], [94mLoss[0m : 2.36967

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.323, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50940
[1mStep[0m  [8/84], [94mLoss[0m : 2.42223
[1mStep[0m  [16/84], [94mLoss[0m : 2.18581
[1mStep[0m  [24/84], [94mLoss[0m : 2.32868
[1mStep[0m  [32/84], [94mLoss[0m : 2.43097
[1mStep[0m  [40/84], [94mLoss[0m : 2.44531
[1mStep[0m  [48/84], [94mLoss[0m : 2.59519
[1mStep[0m  [56/84], [94mLoss[0m : 2.70050
[1mStep[0m  [64/84], [94mLoss[0m : 2.38992
[1mStep[0m  [72/84], [94mLoss[0m : 2.46445
[1mStep[0m  [80/84], [94mLoss[0m : 2.64226

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.326, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31866
[1mStep[0m  [8/84], [94mLoss[0m : 2.23017
[1mStep[0m  [16/84], [94mLoss[0m : 2.61862
[1mStep[0m  [24/84], [94mLoss[0m : 2.16257
[1mStep[0m  [32/84], [94mLoss[0m : 2.40861
[1mStep[0m  [40/84], [94mLoss[0m : 2.75712
[1mStep[0m  [48/84], [94mLoss[0m : 2.46109
[1mStep[0m  [56/84], [94mLoss[0m : 2.34924
[1mStep[0m  [64/84], [94mLoss[0m : 2.73283
[1mStep[0m  [72/84], [94mLoss[0m : 2.86461
[1mStep[0m  [80/84], [94mLoss[0m : 2.48482

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.299, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.307
====================================

Phase 1 - Evaluation MAE:  2.307215537343706
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 2.25123
[1mStep[0m  [8/84], [94mLoss[0m : 2.53773
[1mStep[0m  [16/84], [94mLoss[0m : 2.70253
[1mStep[0m  [24/84], [94mLoss[0m : 2.82526
[1mStep[0m  [32/84], [94mLoss[0m : 2.72095
[1mStep[0m  [40/84], [94mLoss[0m : 2.78133
[1mStep[0m  [48/84], [94mLoss[0m : 2.47040
[1mStep[0m  [56/84], [94mLoss[0m : 2.46695
[1mStep[0m  [64/84], [94mLoss[0m : 2.78818
[1mStep[0m  [72/84], [94mLoss[0m : 2.63481
[1mStep[0m  [80/84], [94mLoss[0m : 2.49643

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.584, [92mTest[0m: 2.309, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46911
[1mStep[0m  [8/84], [94mLoss[0m : 2.47903
[1mStep[0m  [16/84], [94mLoss[0m : 2.81209
[1mStep[0m  [24/84], [94mLoss[0m : 2.38778
[1mStep[0m  [32/84], [94mLoss[0m : 2.53469
[1mStep[0m  [40/84], [94mLoss[0m : 2.49587
[1mStep[0m  [48/84], [94mLoss[0m : 2.46585
[1mStep[0m  [56/84], [94mLoss[0m : 2.26368
[1mStep[0m  [64/84], [94mLoss[0m : 2.33173
[1mStep[0m  [72/84], [94mLoss[0m : 2.55259
[1mStep[0m  [80/84], [94mLoss[0m : 2.59583

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.743, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.80052
[1mStep[0m  [8/84], [94mLoss[0m : 2.39577
[1mStep[0m  [16/84], [94mLoss[0m : 2.23821
[1mStep[0m  [24/84], [94mLoss[0m : 2.31022
[1mStep[0m  [32/84], [94mLoss[0m : 2.29064
[1mStep[0m  [40/84], [94mLoss[0m : 2.33983
[1mStep[0m  [48/84], [94mLoss[0m : 2.51977
[1mStep[0m  [56/84], [94mLoss[0m : 2.16543
[1mStep[0m  [64/84], [94mLoss[0m : 2.26037
[1mStep[0m  [72/84], [94mLoss[0m : 2.59223
[1mStep[0m  [80/84], [94mLoss[0m : 2.17711

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.580, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.03893
[1mStep[0m  [8/84], [94mLoss[0m : 2.29438
[1mStep[0m  [16/84], [94mLoss[0m : 2.43923
[1mStep[0m  [24/84], [94mLoss[0m : 2.45612
[1mStep[0m  [32/84], [94mLoss[0m : 2.33162
[1mStep[0m  [40/84], [94mLoss[0m : 2.40421
[1mStep[0m  [48/84], [94mLoss[0m : 2.14448
[1mStep[0m  [56/84], [94mLoss[0m : 2.24885
[1mStep[0m  [64/84], [94mLoss[0m : 2.45983
[1mStep[0m  [72/84], [94mLoss[0m : 2.48445
[1mStep[0m  [80/84], [94mLoss[0m : 2.72834

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.369, [92mTest[0m: 2.448, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46900
[1mStep[0m  [8/84], [94mLoss[0m : 2.43557
[1mStep[0m  [16/84], [94mLoss[0m : 2.07597
[1mStep[0m  [24/84], [94mLoss[0m : 2.42730
[1mStep[0m  [32/84], [94mLoss[0m : 2.51303
[1mStep[0m  [40/84], [94mLoss[0m : 2.42953
[1mStep[0m  [48/84], [94mLoss[0m : 2.03143
[1mStep[0m  [56/84], [94mLoss[0m : 2.33992
[1mStep[0m  [64/84], [94mLoss[0m : 2.24204
[1mStep[0m  [72/84], [94mLoss[0m : 1.96289
[1mStep[0m  [80/84], [94mLoss[0m : 2.31074

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.299, [92mTest[0m: 2.515, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.21171
[1mStep[0m  [8/84], [94mLoss[0m : 2.22903
[1mStep[0m  [16/84], [94mLoss[0m : 2.13989
[1mStep[0m  [24/84], [94mLoss[0m : 2.23983
[1mStep[0m  [32/84], [94mLoss[0m : 2.66581
[1mStep[0m  [40/84], [94mLoss[0m : 2.23879
[1mStep[0m  [48/84], [94mLoss[0m : 2.32005
[1mStep[0m  [56/84], [94mLoss[0m : 2.21266
[1mStep[0m  [64/84], [94mLoss[0m : 2.29605
[1mStep[0m  [72/84], [94mLoss[0m : 2.51304
[1mStep[0m  [80/84], [94mLoss[0m : 2.31958

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.281, [92mTest[0m: 2.446, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.25062
[1mStep[0m  [8/84], [94mLoss[0m : 2.29380
[1mStep[0m  [16/84], [94mLoss[0m : 2.47786
[1mStep[0m  [24/84], [94mLoss[0m : 2.48198
[1mStep[0m  [32/84], [94mLoss[0m : 2.01495
[1mStep[0m  [40/84], [94mLoss[0m : 2.15677
[1mStep[0m  [48/84], [94mLoss[0m : 1.88547
[1mStep[0m  [56/84], [94mLoss[0m : 2.07936
[1mStep[0m  [64/84], [94mLoss[0m : 2.33800
[1mStep[0m  [72/84], [94mLoss[0m : 1.97371
[1mStep[0m  [80/84], [94mLoss[0m : 2.38702

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.212, [92mTest[0m: 2.411, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.06853
[1mStep[0m  [8/84], [94mLoss[0m : 2.09881
[1mStep[0m  [16/84], [94mLoss[0m : 1.95523
[1mStep[0m  [24/84], [94mLoss[0m : 2.08066
[1mStep[0m  [32/84], [94mLoss[0m : 2.30127
[1mStep[0m  [40/84], [94mLoss[0m : 2.27971
[1mStep[0m  [48/84], [94mLoss[0m : 2.07207
[1mStep[0m  [56/84], [94mLoss[0m : 2.06418
[1mStep[0m  [64/84], [94mLoss[0m : 1.91273
[1mStep[0m  [72/84], [94mLoss[0m : 1.92901
[1mStep[0m  [80/84], [94mLoss[0m : 2.50181

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.166, [92mTest[0m: 2.524, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.12270
[1mStep[0m  [8/84], [94mLoss[0m : 2.23336
[1mStep[0m  [16/84], [94mLoss[0m : 1.85445
[1mStep[0m  [24/84], [94mLoss[0m : 2.05196
[1mStep[0m  [32/84], [94mLoss[0m : 2.11061
[1mStep[0m  [40/84], [94mLoss[0m : 2.13249
[1mStep[0m  [48/84], [94mLoss[0m : 2.23684
[1mStep[0m  [56/84], [94mLoss[0m : 2.01003
[1mStep[0m  [64/84], [94mLoss[0m : 2.14045
[1mStep[0m  [72/84], [94mLoss[0m : 2.07947
[1mStep[0m  [80/84], [94mLoss[0m : 2.00608

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.120, [92mTest[0m: 2.458, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.88659
[1mStep[0m  [8/84], [94mLoss[0m : 2.34235
[1mStep[0m  [16/84], [94mLoss[0m : 2.23180
[1mStep[0m  [24/84], [94mLoss[0m : 2.00777
[1mStep[0m  [32/84], [94mLoss[0m : 2.35966
[1mStep[0m  [40/84], [94mLoss[0m : 2.50082
[1mStep[0m  [48/84], [94mLoss[0m : 2.11445
[1mStep[0m  [56/84], [94mLoss[0m : 2.22456
[1mStep[0m  [64/84], [94mLoss[0m : 2.18664
[1mStep[0m  [72/84], [94mLoss[0m : 1.94855
[1mStep[0m  [80/84], [94mLoss[0m : 1.90579

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.080, [92mTest[0m: 2.464, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.01659
[1mStep[0m  [8/84], [94mLoss[0m : 1.97283
[1mStep[0m  [16/84], [94mLoss[0m : 1.86393
[1mStep[0m  [24/84], [94mLoss[0m : 2.25044
[1mStep[0m  [32/84], [94mLoss[0m : 1.77878
[1mStep[0m  [40/84], [94mLoss[0m : 2.29502
[1mStep[0m  [48/84], [94mLoss[0m : 2.25624
[1mStep[0m  [56/84], [94mLoss[0m : 1.91564
[1mStep[0m  [64/84], [94mLoss[0m : 1.86373
[1mStep[0m  [72/84], [94mLoss[0m : 2.14925
[1mStep[0m  [80/84], [94mLoss[0m : 1.93823

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.047, [92mTest[0m: 2.438, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.16456
[1mStep[0m  [8/84], [94mLoss[0m : 1.83327
[1mStep[0m  [16/84], [94mLoss[0m : 2.10948
[1mStep[0m  [24/84], [94mLoss[0m : 1.83352
[1mStep[0m  [32/84], [94mLoss[0m : 1.89393
[1mStep[0m  [40/84], [94mLoss[0m : 2.14725
[1mStep[0m  [48/84], [94mLoss[0m : 2.05362
[1mStep[0m  [56/84], [94mLoss[0m : 1.97453
[1mStep[0m  [64/84], [94mLoss[0m : 1.67794
[1mStep[0m  [72/84], [94mLoss[0m : 2.15766
[1mStep[0m  [80/84], [94mLoss[0m : 1.89931

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.019, [92mTest[0m: 2.515, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.67751
[1mStep[0m  [8/84], [94mLoss[0m : 2.11783
[1mStep[0m  [16/84], [94mLoss[0m : 1.74182
[1mStep[0m  [24/84], [94mLoss[0m : 2.07330
[1mStep[0m  [32/84], [94mLoss[0m : 1.93952
[1mStep[0m  [40/84], [94mLoss[0m : 2.07326
[1mStep[0m  [48/84], [94mLoss[0m : 1.96195
[1mStep[0m  [56/84], [94mLoss[0m : 2.01591
[1mStep[0m  [64/84], [94mLoss[0m : 1.81213
[1mStep[0m  [72/84], [94mLoss[0m : 1.84690
[1mStep[0m  [80/84], [94mLoss[0m : 2.04068

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.951, [92mTest[0m: 2.438, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.93713
[1mStep[0m  [8/84], [94mLoss[0m : 1.97771
[1mStep[0m  [16/84], [94mLoss[0m : 2.03411
[1mStep[0m  [24/84], [94mLoss[0m : 2.10312
[1mStep[0m  [32/84], [94mLoss[0m : 1.74055
[1mStep[0m  [40/84], [94mLoss[0m : 1.79064
[1mStep[0m  [48/84], [94mLoss[0m : 1.93164
[1mStep[0m  [56/84], [94mLoss[0m : 2.02993
[1mStep[0m  [64/84], [94mLoss[0m : 2.00645
[1mStep[0m  [72/84], [94mLoss[0m : 1.94062
[1mStep[0m  [80/84], [94mLoss[0m : 2.19825

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.932, [92mTest[0m: 2.480, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.07081
[1mStep[0m  [8/84], [94mLoss[0m : 1.90688
[1mStep[0m  [16/84], [94mLoss[0m : 1.82688
[1mStep[0m  [24/84], [94mLoss[0m : 2.01468
[1mStep[0m  [32/84], [94mLoss[0m : 1.87265
[1mStep[0m  [40/84], [94mLoss[0m : 1.57981
[1mStep[0m  [48/84], [94mLoss[0m : 1.89482
[1mStep[0m  [56/84], [94mLoss[0m : 2.16907
[1mStep[0m  [64/84], [94mLoss[0m : 2.07716
[1mStep[0m  [72/84], [94mLoss[0m : 1.90225
[1mStep[0m  [80/84], [94mLoss[0m : 2.03053

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.891, [92mTest[0m: 2.477, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.82098
[1mStep[0m  [8/84], [94mLoss[0m : 1.78330
[1mStep[0m  [16/84], [94mLoss[0m : 1.73017
[1mStep[0m  [24/84], [94mLoss[0m : 1.88751
[1mStep[0m  [32/84], [94mLoss[0m : 1.56496
[1mStep[0m  [40/84], [94mLoss[0m : 1.89068
[1mStep[0m  [48/84], [94mLoss[0m : 1.92135
[1mStep[0m  [56/84], [94mLoss[0m : 2.02382
[1mStep[0m  [64/84], [94mLoss[0m : 2.11115
[1mStep[0m  [72/84], [94mLoss[0m : 2.02267
[1mStep[0m  [80/84], [94mLoss[0m : 1.80711

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.857, [92mTest[0m: 2.475, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.61540
[1mStep[0m  [8/84], [94mLoss[0m : 1.65049
[1mStep[0m  [16/84], [94mLoss[0m : 2.03999
[1mStep[0m  [24/84], [94mLoss[0m : 1.93764
[1mStep[0m  [32/84], [94mLoss[0m : 1.77738
[1mStep[0m  [40/84], [94mLoss[0m : 1.80348
[1mStep[0m  [48/84], [94mLoss[0m : 1.90298
[1mStep[0m  [56/84], [94mLoss[0m : 1.88711
[1mStep[0m  [64/84], [94mLoss[0m : 2.03813
[1mStep[0m  [72/84], [94mLoss[0m : 1.94616
[1mStep[0m  [80/84], [94mLoss[0m : 1.86691

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.837, [92mTest[0m: 2.448, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.77360
[1mStep[0m  [8/84], [94mLoss[0m : 1.87717
[1mStep[0m  [16/84], [94mLoss[0m : 1.65011
[1mStep[0m  [24/84], [94mLoss[0m : 2.05069
[1mStep[0m  [32/84], [94mLoss[0m : 1.77368
[1mStep[0m  [40/84], [94mLoss[0m : 1.82687
[1mStep[0m  [48/84], [94mLoss[0m : 1.68629
[1mStep[0m  [56/84], [94mLoss[0m : 1.80741
[1mStep[0m  [64/84], [94mLoss[0m : 1.83562
[1mStep[0m  [72/84], [94mLoss[0m : 1.95001
[1mStep[0m  [80/84], [94mLoss[0m : 1.84015

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.808, [92mTest[0m: 2.459, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.88772
[1mStep[0m  [8/84], [94mLoss[0m : 1.63229
[1mStep[0m  [16/84], [94mLoss[0m : 1.96771
[1mStep[0m  [24/84], [94mLoss[0m : 1.95163
[1mStep[0m  [32/84], [94mLoss[0m : 1.75964
[1mStep[0m  [40/84], [94mLoss[0m : 1.89958
[1mStep[0m  [48/84], [94mLoss[0m : 1.69071
[1mStep[0m  [56/84], [94mLoss[0m : 1.68021
[1mStep[0m  [64/84], [94mLoss[0m : 1.66519
[1mStep[0m  [72/84], [94mLoss[0m : 1.76522
[1mStep[0m  [80/84], [94mLoss[0m : 1.64891

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.776, [92mTest[0m: 2.462, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.77222
[1mStep[0m  [8/84], [94mLoss[0m : 1.57868
[1mStep[0m  [16/84], [94mLoss[0m : 1.92197
[1mStep[0m  [24/84], [94mLoss[0m : 1.71636
[1mStep[0m  [32/84], [94mLoss[0m : 1.82238
[1mStep[0m  [40/84], [94mLoss[0m : 1.86046
[1mStep[0m  [48/84], [94mLoss[0m : 2.00240
[1mStep[0m  [56/84], [94mLoss[0m : 2.02203
[1mStep[0m  [64/84], [94mLoss[0m : 1.70932
[1mStep[0m  [72/84], [94mLoss[0m : 1.62218
[1mStep[0m  [80/84], [94mLoss[0m : 1.63352

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.756, [92mTest[0m: 2.494, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.74895
[1mStep[0m  [8/84], [94mLoss[0m : 1.65388
[1mStep[0m  [16/84], [94mLoss[0m : 1.75142
[1mStep[0m  [24/84], [94mLoss[0m : 1.54656
[1mStep[0m  [32/84], [94mLoss[0m : 1.91484
[1mStep[0m  [40/84], [94mLoss[0m : 1.69322
[1mStep[0m  [48/84], [94mLoss[0m : 1.60055
[1mStep[0m  [56/84], [94mLoss[0m : 1.93175
[1mStep[0m  [64/84], [94mLoss[0m : 1.63254
[1mStep[0m  [72/84], [94mLoss[0m : 1.91432
[1mStep[0m  [80/84], [94mLoss[0m : 1.77199

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.718, [92mTest[0m: 2.498, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.68057
[1mStep[0m  [8/84], [94mLoss[0m : 1.58055
[1mStep[0m  [16/84], [94mLoss[0m : 1.60389
[1mStep[0m  [24/84], [94mLoss[0m : 1.77150
[1mStep[0m  [32/84], [94mLoss[0m : 1.82517
[1mStep[0m  [40/84], [94mLoss[0m : 1.72708
[1mStep[0m  [48/84], [94mLoss[0m : 1.80592
[1mStep[0m  [56/84], [94mLoss[0m : 1.81215
[1mStep[0m  [64/84], [94mLoss[0m : 1.51353
[1mStep[0m  [72/84], [94mLoss[0m : 1.81000
[1mStep[0m  [80/84], [94mLoss[0m : 1.60866

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.693, [92mTest[0m: 2.512, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.82545
[1mStep[0m  [8/84], [94mLoss[0m : 1.57104
[1mStep[0m  [16/84], [94mLoss[0m : 1.44743
[1mStep[0m  [24/84], [94mLoss[0m : 1.65534
[1mStep[0m  [32/84], [94mLoss[0m : 1.78082
[1mStep[0m  [40/84], [94mLoss[0m : 1.62288
[1mStep[0m  [48/84], [94mLoss[0m : 1.67498
[1mStep[0m  [56/84], [94mLoss[0m : 1.55407
[1mStep[0m  [64/84], [94mLoss[0m : 1.69539
[1mStep[0m  [72/84], [94mLoss[0m : 1.56624
[1mStep[0m  [80/84], [94mLoss[0m : 1.92690

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.653, [92mTest[0m: 2.517, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.41635
[1mStep[0m  [8/84], [94mLoss[0m : 1.58894
[1mStep[0m  [16/84], [94mLoss[0m : 1.75166
[1mStep[0m  [24/84], [94mLoss[0m : 1.70425
[1mStep[0m  [32/84], [94mLoss[0m : 1.66060
[1mStep[0m  [40/84], [94mLoss[0m : 1.43184
[1mStep[0m  [48/84], [94mLoss[0m : 1.64382
[1mStep[0m  [56/84], [94mLoss[0m : 1.54033
[1mStep[0m  [64/84], [94mLoss[0m : 1.38765
[1mStep[0m  [72/84], [94mLoss[0m : 1.55648
[1mStep[0m  [80/84], [94mLoss[0m : 1.64208

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.632, [92mTest[0m: 2.462, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.49238
[1mStep[0m  [8/84], [94mLoss[0m : 1.80376
[1mStep[0m  [16/84], [94mLoss[0m : 1.64783
[1mStep[0m  [24/84], [94mLoss[0m : 1.56476
[1mStep[0m  [32/84], [94mLoss[0m : 1.70296
[1mStep[0m  [40/84], [94mLoss[0m : 1.55080
[1mStep[0m  [48/84], [94mLoss[0m : 1.55362
[1mStep[0m  [56/84], [94mLoss[0m : 1.57915
[1mStep[0m  [64/84], [94mLoss[0m : 1.65899
[1mStep[0m  [72/84], [94mLoss[0m : 1.66298
[1mStep[0m  [80/84], [94mLoss[0m : 1.78068

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.596, [92mTest[0m: 2.455, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.52307
[1mStep[0m  [8/84], [94mLoss[0m : 1.76744
[1mStep[0m  [16/84], [94mLoss[0m : 1.57768
[1mStep[0m  [24/84], [94mLoss[0m : 1.55170
[1mStep[0m  [32/84], [94mLoss[0m : 1.51722
[1mStep[0m  [40/84], [94mLoss[0m : 1.69788
[1mStep[0m  [48/84], [94mLoss[0m : 1.63703
[1mStep[0m  [56/84], [94mLoss[0m : 1.73982
[1mStep[0m  [64/84], [94mLoss[0m : 1.55194
[1mStep[0m  [72/84], [94mLoss[0m : 1.61692
[1mStep[0m  [80/84], [94mLoss[0m : 1.64374

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.600, [92mTest[0m: 2.526, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.63526
[1mStep[0m  [8/84], [94mLoss[0m : 1.86846
[1mStep[0m  [16/84], [94mLoss[0m : 1.68546
[1mStep[0m  [24/84], [94mLoss[0m : 1.44911
[1mStep[0m  [32/84], [94mLoss[0m : 1.50238
[1mStep[0m  [40/84], [94mLoss[0m : 1.43396
[1mStep[0m  [48/84], [94mLoss[0m : 1.58812
[1mStep[0m  [56/84], [94mLoss[0m : 1.46429
[1mStep[0m  [64/84], [94mLoss[0m : 1.48462
[1mStep[0m  [72/84], [94mLoss[0m : 1.46009
[1mStep[0m  [80/84], [94mLoss[0m : 1.65169

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.571, [92mTest[0m: 2.470, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.41063
[1mStep[0m  [8/84], [94mLoss[0m : 1.44667
[1mStep[0m  [16/84], [94mLoss[0m : 1.60379
[1mStep[0m  [24/84], [94mLoss[0m : 1.66165
[1mStep[0m  [32/84], [94mLoss[0m : 1.48576
[1mStep[0m  [40/84], [94mLoss[0m : 1.44564
[1mStep[0m  [48/84], [94mLoss[0m : 1.49104
[1mStep[0m  [56/84], [94mLoss[0m : 1.33532
[1mStep[0m  [64/84], [94mLoss[0m : 1.61306
[1mStep[0m  [72/84], [94mLoss[0m : 1.82796
[1mStep[0m  [80/84], [94mLoss[0m : 1.70645

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.549, [92mTest[0m: 2.507, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.74989
[1mStep[0m  [8/84], [94mLoss[0m : 1.43736
[1mStep[0m  [16/84], [94mLoss[0m : 1.41595
[1mStep[0m  [24/84], [94mLoss[0m : 1.56468
[1mStep[0m  [32/84], [94mLoss[0m : 1.47708
[1mStep[0m  [40/84], [94mLoss[0m : 1.55037
[1mStep[0m  [48/84], [94mLoss[0m : 1.51840
[1mStep[0m  [56/84], [94mLoss[0m : 1.73219
[1mStep[0m  [64/84], [94mLoss[0m : 1.57206
[1mStep[0m  [72/84], [94mLoss[0m : 1.38778
[1mStep[0m  [80/84], [94mLoss[0m : 1.60809

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.530, [92mTest[0m: 2.481, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.45051
[1mStep[0m  [8/84], [94mLoss[0m : 1.60364
[1mStep[0m  [16/84], [94mLoss[0m : 1.44217
[1mStep[0m  [24/84], [94mLoss[0m : 1.48205
[1mStep[0m  [32/84], [94mLoss[0m : 1.31051
[1mStep[0m  [40/84], [94mLoss[0m : 1.61864
[1mStep[0m  [48/84], [94mLoss[0m : 1.58727
[1mStep[0m  [56/84], [94mLoss[0m : 1.83472
[1mStep[0m  [64/84], [94mLoss[0m : 1.52550
[1mStep[0m  [72/84], [94mLoss[0m : 1.62051
[1mStep[0m  [80/84], [94mLoss[0m : 1.39824

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.527, [92mTest[0m: 2.506, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.509
====================================

Phase 2 - Evaluation MAE:  2.5093593512262617
MAE score P1      2.307216
MAE score P2      2.509359
loss              1.527374
learning_rate      0.00505
batch_size             128
hidden_sizes         [100]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.4
momentum               0.5
weight_decay        0.0001
Name: 1, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/169], [94mLoss[0m : 10.71011
[1mStep[0m  [16/169], [94mLoss[0m : 9.62267
[1mStep[0m  [32/169], [94mLoss[0m : 9.17189
[1mStep[0m  [48/169], [94mLoss[0m : 8.31861
[1mStep[0m  [64/169], [94mLoss[0m : 6.96024
[1mStep[0m  [80/169], [94mLoss[0m : 6.46038
[1mStep[0m  [96/169], [94mLoss[0m : 5.99669
[1mStep[0m  [112/169], [94mLoss[0m : 5.77800
[1mStep[0m  [128/169], [94mLoss[0m : 4.37976
[1mStep[0m  [144/169], [94mLoss[0m : 4.04221
[1mStep[0m  [160/169], [94mLoss[0m : 3.66493

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.635, [92mTest[0m: 10.903, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.74127
[1mStep[0m  [16/169], [94mLoss[0m : 3.31493
[1mStep[0m  [32/169], [94mLoss[0m : 3.09214
[1mStep[0m  [48/169], [94mLoss[0m : 2.93643
[1mStep[0m  [64/169], [94mLoss[0m : 3.78643
[1mStep[0m  [80/169], [94mLoss[0m : 2.74423
[1mStep[0m  [96/169], [94mLoss[0m : 3.14623
[1mStep[0m  [112/169], [94mLoss[0m : 3.93372
[1mStep[0m  [128/169], [94mLoss[0m : 2.76463
[1mStep[0m  [144/169], [94mLoss[0m : 2.72781
[1mStep[0m  [160/169], [94mLoss[0m : 2.76183

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.135, [92mTest[0m: 4.433, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.85504
[1mStep[0m  [16/169], [94mLoss[0m : 3.77482
[1mStep[0m  [32/169], [94mLoss[0m : 3.41377
[1mStep[0m  [48/169], [94mLoss[0m : 2.90791
[1mStep[0m  [64/169], [94mLoss[0m : 3.24669
[1mStep[0m  [80/169], [94mLoss[0m : 3.01205
[1mStep[0m  [96/169], [94mLoss[0m : 3.06828
[1mStep[0m  [112/169], [94mLoss[0m : 2.94767
[1mStep[0m  [128/169], [94mLoss[0m : 2.28539
[1mStep[0m  [144/169], [94mLoss[0m : 3.00207
[1mStep[0m  [160/169], [94mLoss[0m : 3.34430

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.977, [92mTest[0m: 2.922, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.85206
[1mStep[0m  [16/169], [94mLoss[0m : 2.82764
[1mStep[0m  [32/169], [94mLoss[0m : 2.15741
[1mStep[0m  [48/169], [94mLoss[0m : 2.69869
[1mStep[0m  [64/169], [94mLoss[0m : 3.39550
[1mStep[0m  [80/169], [94mLoss[0m : 3.25977
[1mStep[0m  [96/169], [94mLoss[0m : 3.06046
[1mStep[0m  [112/169], [94mLoss[0m : 2.62988
[1mStep[0m  [128/169], [94mLoss[0m : 3.15868
[1mStep[0m  [144/169], [94mLoss[0m : 2.70846
[1mStep[0m  [160/169], [94mLoss[0m : 2.43159

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.914, [92mTest[0m: 2.781, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.81972
[1mStep[0m  [16/169], [94mLoss[0m : 2.63759
[1mStep[0m  [32/169], [94mLoss[0m : 2.99526
[1mStep[0m  [48/169], [94mLoss[0m : 2.86375
[1mStep[0m  [64/169], [94mLoss[0m : 3.54753
[1mStep[0m  [80/169], [94mLoss[0m : 3.28530
[1mStep[0m  [96/169], [94mLoss[0m : 2.49445
[1mStep[0m  [112/169], [94mLoss[0m : 2.82347
[1mStep[0m  [128/169], [94mLoss[0m : 2.50185
[1mStep[0m  [144/169], [94mLoss[0m : 3.35506
[1mStep[0m  [160/169], [94mLoss[0m : 2.57663

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.861, [92mTest[0m: 2.651, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.41252
[1mStep[0m  [16/169], [94mLoss[0m : 3.23948
[1mStep[0m  [32/169], [94mLoss[0m : 2.87415
[1mStep[0m  [48/169], [94mLoss[0m : 2.37897
[1mStep[0m  [64/169], [94mLoss[0m : 2.86134
[1mStep[0m  [80/169], [94mLoss[0m : 3.14293
[1mStep[0m  [96/169], [94mLoss[0m : 2.52472
[1mStep[0m  [112/169], [94mLoss[0m : 3.04738
[1mStep[0m  [128/169], [94mLoss[0m : 3.02583
[1mStep[0m  [144/169], [94mLoss[0m : 2.93209
[1mStep[0m  [160/169], [94mLoss[0m : 3.28990

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.843, [92mTest[0m: 2.533, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.94445
[1mStep[0m  [16/169], [94mLoss[0m : 3.23752
[1mStep[0m  [32/169], [94mLoss[0m : 2.58084
[1mStep[0m  [48/169], [94mLoss[0m : 2.94747
[1mStep[0m  [64/169], [94mLoss[0m : 3.17083
[1mStep[0m  [80/169], [94mLoss[0m : 2.59153
[1mStep[0m  [96/169], [94mLoss[0m : 2.73088
[1mStep[0m  [112/169], [94mLoss[0m : 3.09561
[1mStep[0m  [128/169], [94mLoss[0m : 3.08670
[1mStep[0m  [144/169], [94mLoss[0m : 2.97703
[1mStep[0m  [160/169], [94mLoss[0m : 2.78122

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.821, [92mTest[0m: 2.592, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.75786
[1mStep[0m  [16/169], [94mLoss[0m : 2.83422
[1mStep[0m  [32/169], [94mLoss[0m : 2.99509
[1mStep[0m  [48/169], [94mLoss[0m : 2.43093
[1mStep[0m  [64/169], [94mLoss[0m : 2.93443
[1mStep[0m  [80/169], [94mLoss[0m : 2.63374
[1mStep[0m  [96/169], [94mLoss[0m : 2.98109
[1mStep[0m  [112/169], [94mLoss[0m : 2.59786
[1mStep[0m  [128/169], [94mLoss[0m : 2.70315
[1mStep[0m  [144/169], [94mLoss[0m : 2.78401
[1mStep[0m  [160/169], [94mLoss[0m : 3.08268

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.772, [92mTest[0m: 2.459, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.95603
[1mStep[0m  [16/169], [94mLoss[0m : 2.50539
[1mStep[0m  [32/169], [94mLoss[0m : 2.84814
[1mStep[0m  [48/169], [94mLoss[0m : 2.26995
[1mStep[0m  [64/169], [94mLoss[0m : 3.05484
[1mStep[0m  [80/169], [94mLoss[0m : 2.24214
[1mStep[0m  [96/169], [94mLoss[0m : 2.52871
[1mStep[0m  [112/169], [94mLoss[0m : 2.39599
[1mStep[0m  [128/169], [94mLoss[0m : 3.34114
[1mStep[0m  [144/169], [94mLoss[0m : 2.83151
[1mStep[0m  [160/169], [94mLoss[0m : 2.89904

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.774, [92mTest[0m: 2.428, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.00661
[1mStep[0m  [16/169], [94mLoss[0m : 3.18289
[1mStep[0m  [32/169], [94mLoss[0m : 2.40071
[1mStep[0m  [48/169], [94mLoss[0m : 2.57797
[1mStep[0m  [64/169], [94mLoss[0m : 2.55405
[1mStep[0m  [80/169], [94mLoss[0m : 2.47014
[1mStep[0m  [96/169], [94mLoss[0m : 2.78069
[1mStep[0m  [112/169], [94mLoss[0m : 2.92394
[1mStep[0m  [128/169], [94mLoss[0m : 2.63930
[1mStep[0m  [144/169], [94mLoss[0m : 2.69989
[1mStep[0m  [160/169], [94mLoss[0m : 2.90064

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.766, [92mTest[0m: 2.425, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.66999
[1mStep[0m  [16/169], [94mLoss[0m : 2.65086
[1mStep[0m  [32/169], [94mLoss[0m : 2.63983
[1mStep[0m  [48/169], [94mLoss[0m : 2.80375
[1mStep[0m  [64/169], [94mLoss[0m : 3.21014
[1mStep[0m  [80/169], [94mLoss[0m : 2.40521
[1mStep[0m  [96/169], [94mLoss[0m : 2.86574
[1mStep[0m  [112/169], [94mLoss[0m : 2.92662
[1mStep[0m  [128/169], [94mLoss[0m : 2.20002
[1mStep[0m  [144/169], [94mLoss[0m : 2.90064
[1mStep[0m  [160/169], [94mLoss[0m : 2.72871

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.743, [92mTest[0m: 2.398, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.64288
[1mStep[0m  [16/169], [94mLoss[0m : 3.16173
[1mStep[0m  [32/169], [94mLoss[0m : 2.82564
[1mStep[0m  [48/169], [94mLoss[0m : 2.21096
[1mStep[0m  [64/169], [94mLoss[0m : 2.23542
[1mStep[0m  [80/169], [94mLoss[0m : 2.55077
[1mStep[0m  [96/169], [94mLoss[0m : 2.73179
[1mStep[0m  [112/169], [94mLoss[0m : 2.44977
[1mStep[0m  [128/169], [94mLoss[0m : 2.94340
[1mStep[0m  [144/169], [94mLoss[0m : 2.60550
[1mStep[0m  [160/169], [94mLoss[0m : 3.39523

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.727, [92mTest[0m: 2.409, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.71745
[1mStep[0m  [16/169], [94mLoss[0m : 2.84643
[1mStep[0m  [32/169], [94mLoss[0m : 2.81352
[1mStep[0m  [48/169], [94mLoss[0m : 2.43495
[1mStep[0m  [64/169], [94mLoss[0m : 2.30143
[1mStep[0m  [80/169], [94mLoss[0m : 2.94243
[1mStep[0m  [96/169], [94mLoss[0m : 2.51537
[1mStep[0m  [112/169], [94mLoss[0m : 2.83142
[1mStep[0m  [128/169], [94mLoss[0m : 2.41778
[1mStep[0m  [144/169], [94mLoss[0m : 2.99568
[1mStep[0m  [160/169], [94mLoss[0m : 2.61607

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.690, [92mTest[0m: 2.411, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.10799
[1mStep[0m  [16/169], [94mLoss[0m : 2.72322
[1mStep[0m  [32/169], [94mLoss[0m : 2.80250
[1mStep[0m  [48/169], [94mLoss[0m : 2.39765
[1mStep[0m  [64/169], [94mLoss[0m : 2.43922
[1mStep[0m  [80/169], [94mLoss[0m : 2.80507
[1mStep[0m  [96/169], [94mLoss[0m : 2.64409
[1mStep[0m  [112/169], [94mLoss[0m : 2.66666
[1mStep[0m  [128/169], [94mLoss[0m : 3.10113
[1mStep[0m  [144/169], [94mLoss[0m : 2.68583
[1mStep[0m  [160/169], [94mLoss[0m : 3.27963

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.687, [92mTest[0m: 2.359, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.74197
[1mStep[0m  [16/169], [94mLoss[0m : 2.69005
[1mStep[0m  [32/169], [94mLoss[0m : 2.77212
[1mStep[0m  [48/169], [94mLoss[0m : 2.58844
[1mStep[0m  [64/169], [94mLoss[0m : 2.99822
[1mStep[0m  [80/169], [94mLoss[0m : 2.39246
[1mStep[0m  [96/169], [94mLoss[0m : 2.17286
[1mStep[0m  [112/169], [94mLoss[0m : 2.61874
[1mStep[0m  [128/169], [94mLoss[0m : 2.67530
[1mStep[0m  [144/169], [94mLoss[0m : 2.63830
[1mStep[0m  [160/169], [94mLoss[0m : 2.67108

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.695, [92mTest[0m: 2.346, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.81549
[1mStep[0m  [16/169], [94mLoss[0m : 3.04560
[1mStep[0m  [32/169], [94mLoss[0m : 2.70643
[1mStep[0m  [48/169], [94mLoss[0m : 2.60449
[1mStep[0m  [64/169], [94mLoss[0m : 2.93649
[1mStep[0m  [80/169], [94mLoss[0m : 2.78580
[1mStep[0m  [96/169], [94mLoss[0m : 2.65641
[1mStep[0m  [112/169], [94mLoss[0m : 2.84587
[1mStep[0m  [128/169], [94mLoss[0m : 2.95317
[1mStep[0m  [144/169], [94mLoss[0m : 2.58483
[1mStep[0m  [160/169], [94mLoss[0m : 2.64184

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.666, [92mTest[0m: 2.373, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.75960
[1mStep[0m  [16/169], [94mLoss[0m : 2.66579
[1mStep[0m  [32/169], [94mLoss[0m : 2.98836
[1mStep[0m  [48/169], [94mLoss[0m : 2.69111
[1mStep[0m  [64/169], [94mLoss[0m : 2.87401
[1mStep[0m  [80/169], [94mLoss[0m : 3.55189
[1mStep[0m  [96/169], [94mLoss[0m : 2.37722
[1mStep[0m  [112/169], [94mLoss[0m : 1.94295
[1mStep[0m  [128/169], [94mLoss[0m : 2.28369
[1mStep[0m  [144/169], [94mLoss[0m : 2.58539
[1mStep[0m  [160/169], [94mLoss[0m : 2.43975

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.666, [92mTest[0m: 2.347, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.52412
[1mStep[0m  [16/169], [94mLoss[0m : 2.04111
[1mStep[0m  [32/169], [94mLoss[0m : 2.94974
[1mStep[0m  [48/169], [94mLoss[0m : 2.57173
[1mStep[0m  [64/169], [94mLoss[0m : 2.29855
[1mStep[0m  [80/169], [94mLoss[0m : 2.69760
[1mStep[0m  [96/169], [94mLoss[0m : 3.04624
[1mStep[0m  [112/169], [94mLoss[0m : 2.72674
[1mStep[0m  [128/169], [94mLoss[0m : 2.80690
[1mStep[0m  [144/169], [94mLoss[0m : 3.15947
[1mStep[0m  [160/169], [94mLoss[0m : 2.94081

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.639, [92mTest[0m: 2.332, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.86592
[1mStep[0m  [16/169], [94mLoss[0m : 1.91348
[1mStep[0m  [32/169], [94mLoss[0m : 2.97845
[1mStep[0m  [48/169], [94mLoss[0m : 2.70406
[1mStep[0m  [64/169], [94mLoss[0m : 2.55926
[1mStep[0m  [80/169], [94mLoss[0m : 3.23545
[1mStep[0m  [96/169], [94mLoss[0m : 2.66517
[1mStep[0m  [112/169], [94mLoss[0m : 2.76493
[1mStep[0m  [128/169], [94mLoss[0m : 2.74620
[1mStep[0m  [144/169], [94mLoss[0m : 2.94168
[1mStep[0m  [160/169], [94mLoss[0m : 2.84330

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.643, [92mTest[0m: 2.344, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.37665
[1mStep[0m  [16/169], [94mLoss[0m : 2.36015
[1mStep[0m  [32/169], [94mLoss[0m : 2.72829
[1mStep[0m  [48/169], [94mLoss[0m : 2.47430
[1mStep[0m  [64/169], [94mLoss[0m : 3.00790
[1mStep[0m  [80/169], [94mLoss[0m : 3.01997
[1mStep[0m  [96/169], [94mLoss[0m : 2.85822
[1mStep[0m  [112/169], [94mLoss[0m : 2.89238
[1mStep[0m  [128/169], [94mLoss[0m : 2.65297
[1mStep[0m  [144/169], [94mLoss[0m : 2.72538
[1mStep[0m  [160/169], [94mLoss[0m : 2.95896

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.640, [92mTest[0m: 2.363, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.66246
[1mStep[0m  [16/169], [94mLoss[0m : 2.72540
[1mStep[0m  [32/169], [94mLoss[0m : 2.86039
[1mStep[0m  [48/169], [94mLoss[0m : 2.59730
[1mStep[0m  [64/169], [94mLoss[0m : 2.89436
[1mStep[0m  [80/169], [94mLoss[0m : 2.51510
[1mStep[0m  [96/169], [94mLoss[0m : 2.61969
[1mStep[0m  [112/169], [94mLoss[0m : 2.60189
[1mStep[0m  [128/169], [94mLoss[0m : 2.53803
[1mStep[0m  [144/169], [94mLoss[0m : 2.83517
[1mStep[0m  [160/169], [94mLoss[0m : 2.02536

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.648, [92mTest[0m: 2.346, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.58750
[1mStep[0m  [16/169], [94mLoss[0m : 3.05917
[1mStep[0m  [32/169], [94mLoss[0m : 2.85252
[1mStep[0m  [48/169], [94mLoss[0m : 2.77958
[1mStep[0m  [64/169], [94mLoss[0m : 3.06063
[1mStep[0m  [80/169], [94mLoss[0m : 2.37685
[1mStep[0m  [96/169], [94mLoss[0m : 2.31302
[1mStep[0m  [112/169], [94mLoss[0m : 3.14040
[1mStep[0m  [128/169], [94mLoss[0m : 2.22345
[1mStep[0m  [144/169], [94mLoss[0m : 2.58127
[1mStep[0m  [160/169], [94mLoss[0m : 2.45076

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.632, [92mTest[0m: 2.340, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.30020
[1mStep[0m  [16/169], [94mLoss[0m : 2.76157
[1mStep[0m  [32/169], [94mLoss[0m : 2.47818
[1mStep[0m  [48/169], [94mLoss[0m : 2.36358
[1mStep[0m  [64/169], [94mLoss[0m : 2.14521
[1mStep[0m  [80/169], [94mLoss[0m : 2.62848
[1mStep[0m  [96/169], [94mLoss[0m : 2.75461
[1mStep[0m  [112/169], [94mLoss[0m : 2.67851
[1mStep[0m  [128/169], [94mLoss[0m : 2.64639
[1mStep[0m  [144/169], [94mLoss[0m : 2.76233
[1mStep[0m  [160/169], [94mLoss[0m : 2.36846

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.615, [92mTest[0m: 2.339, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.51881
[1mStep[0m  [16/169], [94mLoss[0m : 2.50644
[1mStep[0m  [32/169], [94mLoss[0m : 2.89607
[1mStep[0m  [48/169], [94mLoss[0m : 2.36408
[1mStep[0m  [64/169], [94mLoss[0m : 2.50376
[1mStep[0m  [80/169], [94mLoss[0m : 2.35866
[1mStep[0m  [96/169], [94mLoss[0m : 2.52937
[1mStep[0m  [112/169], [94mLoss[0m : 2.64651
[1mStep[0m  [128/169], [94mLoss[0m : 2.38546
[1mStep[0m  [144/169], [94mLoss[0m : 2.71903
[1mStep[0m  [160/169], [94mLoss[0m : 2.58611

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.609, [92mTest[0m: 2.335, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.42893
[1mStep[0m  [16/169], [94mLoss[0m : 2.54970
[1mStep[0m  [32/169], [94mLoss[0m : 2.73327
[1mStep[0m  [48/169], [94mLoss[0m : 2.85787
[1mStep[0m  [64/169], [94mLoss[0m : 2.62933
[1mStep[0m  [80/169], [94mLoss[0m : 2.57892
[1mStep[0m  [96/169], [94mLoss[0m : 2.26920
[1mStep[0m  [112/169], [94mLoss[0m : 2.52727
[1mStep[0m  [128/169], [94mLoss[0m : 2.71671
[1mStep[0m  [144/169], [94mLoss[0m : 3.07536
[1mStep[0m  [160/169], [94mLoss[0m : 2.58708

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.572, [92mTest[0m: 2.343, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.18253
[1mStep[0m  [16/169], [94mLoss[0m : 2.51797
[1mStep[0m  [32/169], [94mLoss[0m : 2.23172
[1mStep[0m  [48/169], [94mLoss[0m : 2.64949
[1mStep[0m  [64/169], [94mLoss[0m : 2.82839
[1mStep[0m  [80/169], [94mLoss[0m : 2.22288
[1mStep[0m  [96/169], [94mLoss[0m : 2.42142
[1mStep[0m  [112/169], [94mLoss[0m : 2.31979
[1mStep[0m  [128/169], [94mLoss[0m : 2.55560
[1mStep[0m  [144/169], [94mLoss[0m : 2.74024
[1mStep[0m  [160/169], [94mLoss[0m : 2.13813

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.605, [92mTest[0m: 2.346, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.72225
[1mStep[0m  [16/169], [94mLoss[0m : 2.74173
[1mStep[0m  [32/169], [94mLoss[0m : 2.33231
[1mStep[0m  [48/169], [94mLoss[0m : 2.59691
[1mStep[0m  [64/169], [94mLoss[0m : 2.55269
[1mStep[0m  [80/169], [94mLoss[0m : 2.95523
[1mStep[0m  [96/169], [94mLoss[0m : 2.63907
[1mStep[0m  [112/169], [94mLoss[0m : 2.45732
[1mStep[0m  [128/169], [94mLoss[0m : 2.14796
[1mStep[0m  [144/169], [94mLoss[0m : 2.37651
[1mStep[0m  [160/169], [94mLoss[0m : 2.76626

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.610, [92mTest[0m: 2.352, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.67273
[1mStep[0m  [16/169], [94mLoss[0m : 2.79789
[1mStep[0m  [32/169], [94mLoss[0m : 2.68019
[1mStep[0m  [48/169], [94mLoss[0m : 2.43593
[1mStep[0m  [64/169], [94mLoss[0m : 2.45301
[1mStep[0m  [80/169], [94mLoss[0m : 2.92984
[1mStep[0m  [96/169], [94mLoss[0m : 2.51120
[1mStep[0m  [112/169], [94mLoss[0m : 2.48930
[1mStep[0m  [128/169], [94mLoss[0m : 3.08883
[1mStep[0m  [144/169], [94mLoss[0m : 2.33107
[1mStep[0m  [160/169], [94mLoss[0m : 3.09109

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.596, [92mTest[0m: 2.329, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.80942
[1mStep[0m  [16/169], [94mLoss[0m : 2.30551
[1mStep[0m  [32/169], [94mLoss[0m : 2.79946
[1mStep[0m  [48/169], [94mLoss[0m : 1.97230
[1mStep[0m  [64/169], [94mLoss[0m : 2.72245
[1mStep[0m  [80/169], [94mLoss[0m : 2.75558
[1mStep[0m  [96/169], [94mLoss[0m : 2.87070
[1mStep[0m  [112/169], [94mLoss[0m : 2.61014
[1mStep[0m  [128/169], [94mLoss[0m : 2.62156
[1mStep[0m  [144/169], [94mLoss[0m : 2.74962
[1mStep[0m  [160/169], [94mLoss[0m : 2.81798

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.600, [92mTest[0m: 2.329, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.42832
[1mStep[0m  [16/169], [94mLoss[0m : 2.67132
[1mStep[0m  [32/169], [94mLoss[0m : 2.50768
[1mStep[0m  [48/169], [94mLoss[0m : 2.66210
[1mStep[0m  [64/169], [94mLoss[0m : 2.22573
[1mStep[0m  [80/169], [94mLoss[0m : 2.73240
[1mStep[0m  [96/169], [94mLoss[0m : 2.58094
[1mStep[0m  [112/169], [94mLoss[0m : 2.31941
[1mStep[0m  [128/169], [94mLoss[0m : 2.58766
[1mStep[0m  [144/169], [94mLoss[0m : 2.44260
[1mStep[0m  [160/169], [94mLoss[0m : 2.77251

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.575, [92mTest[0m: 2.330, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.317
====================================

Phase 1 - Evaluation MAE:  2.3166603595018387
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/169], [94mLoss[0m : 2.89897
[1mStep[0m  [16/169], [94mLoss[0m : 2.21141
[1mStep[0m  [32/169], [94mLoss[0m : 2.54679
[1mStep[0m  [48/169], [94mLoss[0m : 2.42483
[1mStep[0m  [64/169], [94mLoss[0m : 2.58880
[1mStep[0m  [80/169], [94mLoss[0m : 2.71866
[1mStep[0m  [96/169], [94mLoss[0m : 2.59825
[1mStep[0m  [112/169], [94mLoss[0m : 2.69943
[1mStep[0m  [128/169], [94mLoss[0m : 2.74639
[1mStep[0m  [144/169], [94mLoss[0m : 2.45158
[1mStep[0m  [160/169], [94mLoss[0m : 2.56808

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.665, [92mTest[0m: 2.313, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.77931
[1mStep[0m  [16/169], [94mLoss[0m : 2.56935
[1mStep[0m  [32/169], [94mLoss[0m : 2.83414
[1mStep[0m  [48/169], [94mLoss[0m : 2.54474
[1mStep[0m  [64/169], [94mLoss[0m : 2.91597
[1mStep[0m  [80/169], [94mLoss[0m : 2.77218
[1mStep[0m  [96/169], [94mLoss[0m : 2.77797
[1mStep[0m  [112/169], [94mLoss[0m : 2.78969
[1mStep[0m  [128/169], [94mLoss[0m : 2.96997
[1mStep[0m  [144/169], [94mLoss[0m : 2.79146
[1mStep[0m  [160/169], [94mLoss[0m : 2.02882

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.606, [92mTest[0m: 2.544, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.39174
[1mStep[0m  [16/169], [94mLoss[0m : 2.27653
[1mStep[0m  [32/169], [94mLoss[0m : 2.79388
[1mStep[0m  [48/169], [94mLoss[0m : 2.33196
[1mStep[0m  [64/169], [94mLoss[0m : 2.54488
[1mStep[0m  [80/169], [94mLoss[0m : 2.33798
[1mStep[0m  [96/169], [94mLoss[0m : 2.16829
[1mStep[0m  [112/169], [94mLoss[0m : 2.21950
[1mStep[0m  [128/169], [94mLoss[0m : 2.85153
[1mStep[0m  [144/169], [94mLoss[0m : 2.72192
[1mStep[0m  [160/169], [94mLoss[0m : 2.42619

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.541, [92mTest[0m: 2.402, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.13855
[1mStep[0m  [16/169], [94mLoss[0m : 2.53478
[1mStep[0m  [32/169], [94mLoss[0m : 2.52960
[1mStep[0m  [48/169], [94mLoss[0m : 2.35287
[1mStep[0m  [64/169], [94mLoss[0m : 2.25185
[1mStep[0m  [80/169], [94mLoss[0m : 2.91512
[1mStep[0m  [96/169], [94mLoss[0m : 2.40095
[1mStep[0m  [112/169], [94mLoss[0m : 2.51546
[1mStep[0m  [128/169], [94mLoss[0m : 2.28259
[1mStep[0m  [144/169], [94mLoss[0m : 2.23288
[1mStep[0m  [160/169], [94mLoss[0m : 3.00121

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.517, [92mTest[0m: 2.389, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.34606
[1mStep[0m  [16/169], [94mLoss[0m : 2.13967
[1mStep[0m  [32/169], [94mLoss[0m : 2.11889
[1mStep[0m  [48/169], [94mLoss[0m : 2.41102
[1mStep[0m  [64/169], [94mLoss[0m : 2.09039
[1mStep[0m  [80/169], [94mLoss[0m : 3.09409
[1mStep[0m  [96/169], [94mLoss[0m : 2.34494
[1mStep[0m  [112/169], [94mLoss[0m : 2.83608
[1mStep[0m  [128/169], [94mLoss[0m : 2.47438
[1mStep[0m  [144/169], [94mLoss[0m : 2.34284
[1mStep[0m  [160/169], [94mLoss[0m : 2.53796

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.397, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.32338
[1mStep[0m  [16/169], [94mLoss[0m : 2.16125
[1mStep[0m  [32/169], [94mLoss[0m : 2.62560
[1mStep[0m  [48/169], [94mLoss[0m : 2.73960
[1mStep[0m  [64/169], [94mLoss[0m : 2.71853
[1mStep[0m  [80/169], [94mLoss[0m : 2.56276
[1mStep[0m  [96/169], [94mLoss[0m : 2.41378
[1mStep[0m  [112/169], [94mLoss[0m : 2.72302
[1mStep[0m  [128/169], [94mLoss[0m : 2.22503
[1mStep[0m  [144/169], [94mLoss[0m : 2.41543
[1mStep[0m  [160/169], [94mLoss[0m : 2.37973

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.399, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.39784
[1mStep[0m  [16/169], [94mLoss[0m : 2.68807
[1mStep[0m  [32/169], [94mLoss[0m : 2.49035
[1mStep[0m  [48/169], [94mLoss[0m : 2.34713
[1mStep[0m  [64/169], [94mLoss[0m : 2.52823
[1mStep[0m  [80/169], [94mLoss[0m : 2.23711
[1mStep[0m  [96/169], [94mLoss[0m : 2.01215
[1mStep[0m  [112/169], [94mLoss[0m : 2.39585
[1mStep[0m  [128/169], [94mLoss[0m : 2.09914
[1mStep[0m  [144/169], [94mLoss[0m : 2.44327
[1mStep[0m  [160/169], [94mLoss[0m : 2.38465

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.388, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.20959
[1mStep[0m  [16/169], [94mLoss[0m : 2.45995
[1mStep[0m  [32/169], [94mLoss[0m : 2.40035
[1mStep[0m  [48/169], [94mLoss[0m : 2.48379
[1mStep[0m  [64/169], [94mLoss[0m : 2.44716
[1mStep[0m  [80/169], [94mLoss[0m : 2.18430
[1mStep[0m  [96/169], [94mLoss[0m : 2.47465
[1mStep[0m  [112/169], [94mLoss[0m : 2.30894
[1mStep[0m  [128/169], [94mLoss[0m : 2.65153
[1mStep[0m  [144/169], [94mLoss[0m : 2.17969
[1mStep[0m  [160/169], [94mLoss[0m : 2.77804

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.361, [92mTest[0m: 2.417, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.53426
[1mStep[0m  [16/169], [94mLoss[0m : 2.33705
[1mStep[0m  [32/169], [94mLoss[0m : 2.30076
[1mStep[0m  [48/169], [94mLoss[0m : 2.37407
[1mStep[0m  [64/169], [94mLoss[0m : 1.90580
[1mStep[0m  [80/169], [94mLoss[0m : 2.34824
[1mStep[0m  [96/169], [94mLoss[0m : 2.45966
[1mStep[0m  [112/169], [94mLoss[0m : 1.80121
[1mStep[0m  [128/169], [94mLoss[0m : 2.04598
[1mStep[0m  [144/169], [94mLoss[0m : 2.40407
[1mStep[0m  [160/169], [94mLoss[0m : 2.56534

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.347, [92mTest[0m: 2.433, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.09097
[1mStep[0m  [16/169], [94mLoss[0m : 2.01351
[1mStep[0m  [32/169], [94mLoss[0m : 2.61818
[1mStep[0m  [48/169], [94mLoss[0m : 2.31152
[1mStep[0m  [64/169], [94mLoss[0m : 1.94815
[1mStep[0m  [80/169], [94mLoss[0m : 2.18760
[1mStep[0m  [96/169], [94mLoss[0m : 1.93442
[1mStep[0m  [112/169], [94mLoss[0m : 2.38562
[1mStep[0m  [128/169], [94mLoss[0m : 2.12301
[1mStep[0m  [144/169], [94mLoss[0m : 2.43656
[1mStep[0m  [160/169], [94mLoss[0m : 1.94327

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.261, [92mTest[0m: 2.433, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.16728
[1mStep[0m  [16/169], [94mLoss[0m : 2.04497
[1mStep[0m  [32/169], [94mLoss[0m : 2.26461
[1mStep[0m  [48/169], [94mLoss[0m : 2.43910
[1mStep[0m  [64/169], [94mLoss[0m : 2.72946
[1mStep[0m  [80/169], [94mLoss[0m : 2.14642
[1mStep[0m  [96/169], [94mLoss[0m : 2.76387
[1mStep[0m  [112/169], [94mLoss[0m : 1.95498
[1mStep[0m  [128/169], [94mLoss[0m : 2.05744
[1mStep[0m  [144/169], [94mLoss[0m : 2.09840
[1mStep[0m  [160/169], [94mLoss[0m : 2.25844

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.235, [92mTest[0m: 2.512, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.32193
[1mStep[0m  [16/169], [94mLoss[0m : 2.30929
[1mStep[0m  [32/169], [94mLoss[0m : 2.57699
[1mStep[0m  [48/169], [94mLoss[0m : 2.37251
[1mStep[0m  [64/169], [94mLoss[0m : 2.12525
[1mStep[0m  [80/169], [94mLoss[0m : 2.48914
[1mStep[0m  [96/169], [94mLoss[0m : 2.29289
[1mStep[0m  [112/169], [94mLoss[0m : 2.12076
[1mStep[0m  [128/169], [94mLoss[0m : 1.93637
[1mStep[0m  [144/169], [94mLoss[0m : 2.53309
[1mStep[0m  [160/169], [94mLoss[0m : 2.43096

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.216, [92mTest[0m: 2.446, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.12482
[1mStep[0m  [16/169], [94mLoss[0m : 2.26557
[1mStep[0m  [32/169], [94mLoss[0m : 2.06604
[1mStep[0m  [48/169], [94mLoss[0m : 2.51506
[1mStep[0m  [64/169], [94mLoss[0m : 2.25101
[1mStep[0m  [80/169], [94mLoss[0m : 2.00093
[1mStep[0m  [96/169], [94mLoss[0m : 2.32376
[1mStep[0m  [112/169], [94mLoss[0m : 2.30883
[1mStep[0m  [128/169], [94mLoss[0m : 1.94919
[1mStep[0m  [144/169], [94mLoss[0m : 1.80837
[1mStep[0m  [160/169], [94mLoss[0m : 2.39613

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.168, [92mTest[0m: 2.430, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.22951
[1mStep[0m  [16/169], [94mLoss[0m : 2.44225
[1mStep[0m  [32/169], [94mLoss[0m : 1.96879
[1mStep[0m  [48/169], [94mLoss[0m : 2.24620
[1mStep[0m  [64/169], [94mLoss[0m : 1.99174
[1mStep[0m  [80/169], [94mLoss[0m : 2.35380
[1mStep[0m  [96/169], [94mLoss[0m : 2.12395
[1mStep[0m  [112/169], [94mLoss[0m : 2.59270
[1mStep[0m  [128/169], [94mLoss[0m : 2.21007
[1mStep[0m  [144/169], [94mLoss[0m : 2.21751
[1mStep[0m  [160/169], [94mLoss[0m : 2.06315

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.158, [92mTest[0m: 2.440, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.12133
[1mStep[0m  [16/169], [94mLoss[0m : 2.15247
[1mStep[0m  [32/169], [94mLoss[0m : 2.02087
[1mStep[0m  [48/169], [94mLoss[0m : 2.15600
[1mStep[0m  [64/169], [94mLoss[0m : 2.01135
[1mStep[0m  [80/169], [94mLoss[0m : 2.10682
[1mStep[0m  [96/169], [94mLoss[0m : 1.87561
[1mStep[0m  [112/169], [94mLoss[0m : 2.05544
[1mStep[0m  [128/169], [94mLoss[0m : 2.35294
[1mStep[0m  [144/169], [94mLoss[0m : 2.12041
[1mStep[0m  [160/169], [94mLoss[0m : 2.03108

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.130, [92mTest[0m: 2.442, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.88127
[1mStep[0m  [16/169], [94mLoss[0m : 2.27835
[1mStep[0m  [32/169], [94mLoss[0m : 1.92724
[1mStep[0m  [48/169], [94mLoss[0m : 1.72794
[1mStep[0m  [64/169], [94mLoss[0m : 1.74047
[1mStep[0m  [80/169], [94mLoss[0m : 2.25492
[1mStep[0m  [96/169], [94mLoss[0m : 2.25851
[1mStep[0m  [112/169], [94mLoss[0m : 1.71394
[1mStep[0m  [128/169], [94mLoss[0m : 2.47522
[1mStep[0m  [144/169], [94mLoss[0m : 2.17110
[1mStep[0m  [160/169], [94mLoss[0m : 2.10143

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.088, [92mTest[0m: 2.416, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.99894
[1mStep[0m  [16/169], [94mLoss[0m : 1.77571
[1mStep[0m  [32/169], [94mLoss[0m : 2.30466
[1mStep[0m  [48/169], [94mLoss[0m : 2.38554
[1mStep[0m  [64/169], [94mLoss[0m : 1.80671
[1mStep[0m  [80/169], [94mLoss[0m : 1.95817
[1mStep[0m  [96/169], [94mLoss[0m : 1.86317
[1mStep[0m  [112/169], [94mLoss[0m : 1.91508
[1mStep[0m  [128/169], [94mLoss[0m : 2.26737
[1mStep[0m  [144/169], [94mLoss[0m : 1.90013
[1mStep[0m  [160/169], [94mLoss[0m : 1.88317

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.092, [92mTest[0m: 2.502, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.07435
[1mStep[0m  [16/169], [94mLoss[0m : 1.73217
[1mStep[0m  [32/169], [94mLoss[0m : 2.06790
[1mStep[0m  [48/169], [94mLoss[0m : 1.91458
[1mStep[0m  [64/169], [94mLoss[0m : 1.90351
[1mStep[0m  [80/169], [94mLoss[0m : 2.89403
[1mStep[0m  [96/169], [94mLoss[0m : 2.10485
[1mStep[0m  [112/169], [94mLoss[0m : 2.08715
[1mStep[0m  [128/169], [94mLoss[0m : 1.88529
[1mStep[0m  [144/169], [94mLoss[0m : 1.88690
[1mStep[0m  [160/169], [94mLoss[0m : 2.16776

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.044, [92mTest[0m: 2.463, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.05965
[1mStep[0m  [16/169], [94mLoss[0m : 2.26410
[1mStep[0m  [32/169], [94mLoss[0m : 1.95469
[1mStep[0m  [48/169], [94mLoss[0m : 1.78012
[1mStep[0m  [64/169], [94mLoss[0m : 2.16965
[1mStep[0m  [80/169], [94mLoss[0m : 1.73088
[1mStep[0m  [96/169], [94mLoss[0m : 2.24532
[1mStep[0m  [112/169], [94mLoss[0m : 2.17754
[1mStep[0m  [128/169], [94mLoss[0m : 1.82821
[1mStep[0m  [144/169], [94mLoss[0m : 2.13048
[1mStep[0m  [160/169], [94mLoss[0m : 1.84412

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.991, [92mTest[0m: 2.482, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.83639
[1mStep[0m  [16/169], [94mLoss[0m : 1.92941
[1mStep[0m  [32/169], [94mLoss[0m : 1.95158
[1mStep[0m  [48/169], [94mLoss[0m : 1.68715
[1mStep[0m  [64/169], [94mLoss[0m : 1.87377
[1mStep[0m  [80/169], [94mLoss[0m : 1.94082
[1mStep[0m  [96/169], [94mLoss[0m : 2.60344
[1mStep[0m  [112/169], [94mLoss[0m : 2.18874
[1mStep[0m  [128/169], [94mLoss[0m : 1.87668
[1mStep[0m  [144/169], [94mLoss[0m : 1.85924
[1mStep[0m  [160/169], [94mLoss[0m : 2.00845

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.969, [92mTest[0m: 2.452, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.47722
[1mStep[0m  [16/169], [94mLoss[0m : 1.92591
[1mStep[0m  [32/169], [94mLoss[0m : 2.02028
[1mStep[0m  [48/169], [94mLoss[0m : 2.18851
[1mStep[0m  [64/169], [94mLoss[0m : 2.00231
[1mStep[0m  [80/169], [94mLoss[0m : 1.74085
[1mStep[0m  [96/169], [94mLoss[0m : 1.84964
[1mStep[0m  [112/169], [94mLoss[0m : 1.82135
[1mStep[0m  [128/169], [94mLoss[0m : 1.83950
[1mStep[0m  [144/169], [94mLoss[0m : 1.85031
[1mStep[0m  [160/169], [94mLoss[0m : 1.80016

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.940, [92mTest[0m: 2.479, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.93328
[1mStep[0m  [16/169], [94mLoss[0m : 1.82236
[1mStep[0m  [32/169], [94mLoss[0m : 1.59589
[1mStep[0m  [48/169], [94mLoss[0m : 1.97140
[1mStep[0m  [64/169], [94mLoss[0m : 1.82205
[1mStep[0m  [80/169], [94mLoss[0m : 2.02415
[1mStep[0m  [96/169], [94mLoss[0m : 2.02233
[1mStep[0m  [112/169], [94mLoss[0m : 1.78036
[1mStep[0m  [128/169], [94mLoss[0m : 2.17647
[1mStep[0m  [144/169], [94mLoss[0m : 1.70109
[1mStep[0m  [160/169], [94mLoss[0m : 1.67580

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.906, [92mTest[0m: 2.475, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.12279
[1mStep[0m  [16/169], [94mLoss[0m : 1.67080
[1mStep[0m  [32/169], [94mLoss[0m : 1.80835
[1mStep[0m  [48/169], [94mLoss[0m : 1.91078
[1mStep[0m  [64/169], [94mLoss[0m : 2.16181
[1mStep[0m  [80/169], [94mLoss[0m : 2.11287
[1mStep[0m  [96/169], [94mLoss[0m : 1.87964
[1mStep[0m  [112/169], [94mLoss[0m : 1.61867
[1mStep[0m  [128/169], [94mLoss[0m : 2.25681
[1mStep[0m  [144/169], [94mLoss[0m : 2.16861
[1mStep[0m  [160/169], [94mLoss[0m : 1.89499

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.892, [92mTest[0m: 2.505, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.64148
[1mStep[0m  [16/169], [94mLoss[0m : 1.63383
[1mStep[0m  [32/169], [94mLoss[0m : 1.97976
[1mStep[0m  [48/169], [94mLoss[0m : 1.90659
[1mStep[0m  [64/169], [94mLoss[0m : 1.58535
[1mStep[0m  [80/169], [94mLoss[0m : 1.74647
[1mStep[0m  [96/169], [94mLoss[0m : 1.65376
[1mStep[0m  [112/169], [94mLoss[0m : 1.80686
[1mStep[0m  [128/169], [94mLoss[0m : 1.76039
[1mStep[0m  [144/169], [94mLoss[0m : 1.67023
[1mStep[0m  [160/169], [94mLoss[0m : 2.09974

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.864, [92mTest[0m: 2.497, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.52477
[1mStep[0m  [16/169], [94mLoss[0m : 1.85453
[1mStep[0m  [32/169], [94mLoss[0m : 1.97969
[1mStep[0m  [48/169], [94mLoss[0m : 1.66014
[1mStep[0m  [64/169], [94mLoss[0m : 1.61329
[1mStep[0m  [80/169], [94mLoss[0m : 2.09741
[1mStep[0m  [96/169], [94mLoss[0m : 2.09903
[1mStep[0m  [112/169], [94mLoss[0m : 1.92250
[1mStep[0m  [128/169], [94mLoss[0m : 1.74266
[1mStep[0m  [144/169], [94mLoss[0m : 1.85053
[1mStep[0m  [160/169], [94mLoss[0m : 1.82463

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.852, [92mTest[0m: 2.494, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.13638
[1mStep[0m  [16/169], [94mLoss[0m : 1.71004
[1mStep[0m  [32/169], [94mLoss[0m : 2.17894
[1mStep[0m  [48/169], [94mLoss[0m : 1.88610
[1mStep[0m  [64/169], [94mLoss[0m : 1.72669
[1mStep[0m  [80/169], [94mLoss[0m : 1.80335
[1mStep[0m  [96/169], [94mLoss[0m : 1.63081
[1mStep[0m  [112/169], [94mLoss[0m : 1.54427
[1mStep[0m  [128/169], [94mLoss[0m : 2.20725
[1mStep[0m  [144/169], [94mLoss[0m : 1.79172
[1mStep[0m  [160/169], [94mLoss[0m : 1.68613

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.821, [92mTest[0m: 2.530, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.03086
[1mStep[0m  [16/169], [94mLoss[0m : 1.65535
[1mStep[0m  [32/169], [94mLoss[0m : 1.71805
[1mStep[0m  [48/169], [94mLoss[0m : 2.08989
[1mStep[0m  [64/169], [94mLoss[0m : 1.97578
[1mStep[0m  [80/169], [94mLoss[0m : 1.58484
[1mStep[0m  [96/169], [94mLoss[0m : 1.91241
[1mStep[0m  [112/169], [94mLoss[0m : 1.62283
[1mStep[0m  [128/169], [94mLoss[0m : 1.81761
[1mStep[0m  [144/169], [94mLoss[0m : 1.91833
[1mStep[0m  [160/169], [94mLoss[0m : 1.67625

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.796, [92mTest[0m: 2.536, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.99592
[1mStep[0m  [16/169], [94mLoss[0m : 1.75159
[1mStep[0m  [32/169], [94mLoss[0m : 2.08551
[1mStep[0m  [48/169], [94mLoss[0m : 1.49021
[1mStep[0m  [64/169], [94mLoss[0m : 1.94500
[1mStep[0m  [80/169], [94mLoss[0m : 1.60987
[1mStep[0m  [96/169], [94mLoss[0m : 1.79527
[1mStep[0m  [112/169], [94mLoss[0m : 1.80135
[1mStep[0m  [128/169], [94mLoss[0m : 2.03347
[1mStep[0m  [144/169], [94mLoss[0m : 1.77367
[1mStep[0m  [160/169], [94mLoss[0m : 1.92830

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.798, [92mTest[0m: 2.488, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.61637
[1mStep[0m  [16/169], [94mLoss[0m : 1.74467
[1mStep[0m  [32/169], [94mLoss[0m : 1.40330
[1mStep[0m  [48/169], [94mLoss[0m : 1.93084
[1mStep[0m  [64/169], [94mLoss[0m : 1.58342
[1mStep[0m  [80/169], [94mLoss[0m : 2.08993
[1mStep[0m  [96/169], [94mLoss[0m : 1.88299
[1mStep[0m  [112/169], [94mLoss[0m : 1.59025
[1mStep[0m  [128/169], [94mLoss[0m : 1.86186
[1mStep[0m  [144/169], [94mLoss[0m : 1.71759
[1mStep[0m  [160/169], [94mLoss[0m : 2.31918

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.771, [92mTest[0m: 2.487, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.99263
[1mStep[0m  [16/169], [94mLoss[0m : 1.94035
[1mStep[0m  [32/169], [94mLoss[0m : 1.90793
[1mStep[0m  [48/169], [94mLoss[0m : 1.70613
[1mStep[0m  [64/169], [94mLoss[0m : 2.34775
[1mStep[0m  [80/169], [94mLoss[0m : 1.82104
[1mStep[0m  [96/169], [94mLoss[0m : 1.63103
[1mStep[0m  [112/169], [94mLoss[0m : 2.18230
[1mStep[0m  [128/169], [94mLoss[0m : 1.82602
[1mStep[0m  [144/169], [94mLoss[0m : 1.73211
[1mStep[0m  [160/169], [94mLoss[0m : 2.03388

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.778, [92mTest[0m: 2.500, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.514
====================================

Phase 2 - Evaluation MAE:  2.5144905064787184
MAE score P1        2.31666
MAE score P2       2.514491
loss               1.771036
learning_rate       0.00505
batch_size               64
hidden_sizes      [200, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.1
weight_decay           0.01
Name: 2, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/169], [94mLoss[0m : 11.37514
[1mStep[0m  [16/169], [94mLoss[0m : 11.08703
[1mStep[0m  [32/169], [94mLoss[0m : 9.12801
[1mStep[0m  [48/169], [94mLoss[0m : 6.92971
[1mStep[0m  [64/169], [94mLoss[0m : 5.94870
[1mStep[0m  [80/169], [94mLoss[0m : 4.67159
[1mStep[0m  [96/169], [94mLoss[0m : 3.32756
[1mStep[0m  [112/169], [94mLoss[0m : 3.02015
[1mStep[0m  [128/169], [94mLoss[0m : 2.65192
[1mStep[0m  [144/169], [94mLoss[0m : 2.82935
[1mStep[0m  [160/169], [94mLoss[0m : 2.55612

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 5.503, [92mTest[0m: 11.025, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.11200
[1mStep[0m  [16/169], [94mLoss[0m : 2.64951
[1mStep[0m  [32/169], [94mLoss[0m : 3.69971
[1mStep[0m  [48/169], [94mLoss[0m : 3.29003
[1mStep[0m  [64/169], [94mLoss[0m : 2.34733
[1mStep[0m  [80/169], [94mLoss[0m : 2.49060
[1mStep[0m  [96/169], [94mLoss[0m : 2.61321
[1mStep[0m  [112/169], [94mLoss[0m : 2.56713
[1mStep[0m  [128/169], [94mLoss[0m : 2.67031
[1mStep[0m  [144/169], [94mLoss[0m : 2.42748
[1mStep[0m  [160/169], [94mLoss[0m : 3.08762

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.635, [92mTest[0m: 2.557, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.55732
[1mStep[0m  [16/169], [94mLoss[0m : 2.38695
[1mStep[0m  [32/169], [94mLoss[0m : 2.54449
[1mStep[0m  [48/169], [94mLoss[0m : 2.53128
[1mStep[0m  [64/169], [94mLoss[0m : 2.49230
[1mStep[0m  [80/169], [94mLoss[0m : 2.24735
[1mStep[0m  [96/169], [94mLoss[0m : 2.96259
[1mStep[0m  [112/169], [94mLoss[0m : 2.80072
[1mStep[0m  [128/169], [94mLoss[0m : 2.24852
[1mStep[0m  [144/169], [94mLoss[0m : 2.66346
[1mStep[0m  [160/169], [94mLoss[0m : 2.86885

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.433, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.39302
[1mStep[0m  [16/169], [94mLoss[0m : 2.09965
[1mStep[0m  [32/169], [94mLoss[0m : 2.57624
[1mStep[0m  [48/169], [94mLoss[0m : 2.65326
[1mStep[0m  [64/169], [94mLoss[0m : 2.81328
[1mStep[0m  [80/169], [94mLoss[0m : 2.44193
[1mStep[0m  [96/169], [94mLoss[0m : 2.32101
[1mStep[0m  [112/169], [94mLoss[0m : 2.66362
[1mStep[0m  [128/169], [94mLoss[0m : 2.52612
[1mStep[0m  [144/169], [94mLoss[0m : 2.59427
[1mStep[0m  [160/169], [94mLoss[0m : 2.21975

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.402, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.70409
[1mStep[0m  [16/169], [94mLoss[0m : 2.37012
[1mStep[0m  [32/169], [94mLoss[0m : 2.51550
[1mStep[0m  [48/169], [94mLoss[0m : 2.44267
[1mStep[0m  [64/169], [94mLoss[0m : 2.52940
[1mStep[0m  [80/169], [94mLoss[0m : 2.24412
[1mStep[0m  [96/169], [94mLoss[0m : 2.81104
[1mStep[0m  [112/169], [94mLoss[0m : 2.18515
[1mStep[0m  [128/169], [94mLoss[0m : 2.18898
[1mStep[0m  [144/169], [94mLoss[0m : 2.76384
[1mStep[0m  [160/169], [94mLoss[0m : 2.32156

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.390, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.91402
[1mStep[0m  [16/169], [94mLoss[0m : 2.57986
[1mStep[0m  [32/169], [94mLoss[0m : 2.48949
[1mStep[0m  [48/169], [94mLoss[0m : 2.70054
[1mStep[0m  [64/169], [94mLoss[0m : 2.63497
[1mStep[0m  [80/169], [94mLoss[0m : 2.73892
[1mStep[0m  [96/169], [94mLoss[0m : 2.45308
[1mStep[0m  [112/169], [94mLoss[0m : 2.30906
[1mStep[0m  [128/169], [94mLoss[0m : 2.74489
[1mStep[0m  [144/169], [94mLoss[0m : 2.70076
[1mStep[0m  [160/169], [94mLoss[0m : 2.71345

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.385, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.67714
[1mStep[0m  [16/169], [94mLoss[0m : 2.20058
[1mStep[0m  [32/169], [94mLoss[0m : 2.30571
[1mStep[0m  [48/169], [94mLoss[0m : 2.34459
[1mStep[0m  [64/169], [94mLoss[0m : 2.53303
[1mStep[0m  [80/169], [94mLoss[0m : 2.34216
[1mStep[0m  [96/169], [94mLoss[0m : 2.31013
[1mStep[0m  [112/169], [94mLoss[0m : 2.43247
[1mStep[0m  [128/169], [94mLoss[0m : 2.60311
[1mStep[0m  [144/169], [94mLoss[0m : 2.16887
[1mStep[0m  [160/169], [94mLoss[0m : 2.67327

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.357, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.24988
[1mStep[0m  [16/169], [94mLoss[0m : 2.60767
[1mStep[0m  [32/169], [94mLoss[0m : 2.62799
[1mStep[0m  [48/169], [94mLoss[0m : 2.68897
[1mStep[0m  [64/169], [94mLoss[0m : 2.24059
[1mStep[0m  [80/169], [94mLoss[0m : 2.68305
[1mStep[0m  [96/169], [94mLoss[0m : 2.43865
[1mStep[0m  [112/169], [94mLoss[0m : 2.70195
[1mStep[0m  [128/169], [94mLoss[0m : 2.49295
[1mStep[0m  [144/169], [94mLoss[0m : 2.33500
[1mStep[0m  [160/169], [94mLoss[0m : 2.54513

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.406, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.53470
[1mStep[0m  [16/169], [94mLoss[0m : 2.40409
[1mStep[0m  [32/169], [94mLoss[0m : 2.63355
[1mStep[0m  [48/169], [94mLoss[0m : 2.77054
[1mStep[0m  [64/169], [94mLoss[0m : 2.37454
[1mStep[0m  [80/169], [94mLoss[0m : 2.29787
[1mStep[0m  [96/169], [94mLoss[0m : 2.69882
[1mStep[0m  [112/169], [94mLoss[0m : 1.98396
[1mStep[0m  [128/169], [94mLoss[0m : 2.46692
[1mStep[0m  [144/169], [94mLoss[0m : 2.60836
[1mStep[0m  [160/169], [94mLoss[0m : 2.29697

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.378, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.75082
[1mStep[0m  [16/169], [94mLoss[0m : 2.54531
[1mStep[0m  [32/169], [94mLoss[0m : 2.54009
[1mStep[0m  [48/169], [94mLoss[0m : 2.03290
[1mStep[0m  [64/169], [94mLoss[0m : 2.72980
[1mStep[0m  [80/169], [94mLoss[0m : 2.25297
[1mStep[0m  [96/169], [94mLoss[0m : 2.54429
[1mStep[0m  [112/169], [94mLoss[0m : 2.55357
[1mStep[0m  [128/169], [94mLoss[0m : 2.85367
[1mStep[0m  [144/169], [94mLoss[0m : 2.66849
[1mStep[0m  [160/169], [94mLoss[0m : 2.56711

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.357, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.06512
[1mStep[0m  [16/169], [94mLoss[0m : 2.18900
[1mStep[0m  [32/169], [94mLoss[0m : 2.07563
[1mStep[0m  [48/169], [94mLoss[0m : 2.36940
[1mStep[0m  [64/169], [94mLoss[0m : 2.23885
[1mStep[0m  [80/169], [94mLoss[0m : 2.23059
[1mStep[0m  [96/169], [94mLoss[0m : 2.64404
[1mStep[0m  [112/169], [94mLoss[0m : 2.33139
[1mStep[0m  [128/169], [94mLoss[0m : 1.91874
[1mStep[0m  [144/169], [94mLoss[0m : 2.34958
[1mStep[0m  [160/169], [94mLoss[0m : 2.58756

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.336, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.89027
[1mStep[0m  [16/169], [94mLoss[0m : 2.56406
[1mStep[0m  [32/169], [94mLoss[0m : 2.03009
[1mStep[0m  [48/169], [94mLoss[0m : 2.34012
[1mStep[0m  [64/169], [94mLoss[0m : 2.48230
[1mStep[0m  [80/169], [94mLoss[0m : 2.57379
[1mStep[0m  [96/169], [94mLoss[0m : 2.76132
[1mStep[0m  [112/169], [94mLoss[0m : 2.65409
[1mStep[0m  [128/169], [94mLoss[0m : 2.13878
[1mStep[0m  [144/169], [94mLoss[0m : 2.42457
[1mStep[0m  [160/169], [94mLoss[0m : 2.09128

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.398, [92mTest[0m: 2.374, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.23102
[1mStep[0m  [16/169], [94mLoss[0m : 2.11287
[1mStep[0m  [32/169], [94mLoss[0m : 2.83285
[1mStep[0m  [48/169], [94mLoss[0m : 2.53733
[1mStep[0m  [64/169], [94mLoss[0m : 2.21074
[1mStep[0m  [80/169], [94mLoss[0m : 2.60466
[1mStep[0m  [96/169], [94mLoss[0m : 2.15386
[1mStep[0m  [112/169], [94mLoss[0m : 2.23769
[1mStep[0m  [128/169], [94mLoss[0m : 2.47417
[1mStep[0m  [144/169], [94mLoss[0m : 2.31434
[1mStep[0m  [160/169], [94mLoss[0m : 2.23676

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.401, [92mTest[0m: 2.377, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.50962
[1mStep[0m  [16/169], [94mLoss[0m : 2.31685
[1mStep[0m  [32/169], [94mLoss[0m : 2.62163
[1mStep[0m  [48/169], [94mLoss[0m : 2.33742
[1mStep[0m  [64/169], [94mLoss[0m : 2.60128
[1mStep[0m  [80/169], [94mLoss[0m : 2.62630
[1mStep[0m  [96/169], [94mLoss[0m : 2.50219
[1mStep[0m  [112/169], [94mLoss[0m : 2.16389
[1mStep[0m  [128/169], [94mLoss[0m : 2.19988
[1mStep[0m  [144/169], [94mLoss[0m : 2.35537
[1mStep[0m  [160/169], [94mLoss[0m : 2.82761

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.392, [92mTest[0m: 2.331, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.67100
[1mStep[0m  [16/169], [94mLoss[0m : 2.12296
[1mStep[0m  [32/169], [94mLoss[0m : 2.24598
[1mStep[0m  [48/169], [94mLoss[0m : 2.10842
[1mStep[0m  [64/169], [94mLoss[0m : 2.19393
[1mStep[0m  [80/169], [94mLoss[0m : 2.16290
[1mStep[0m  [96/169], [94mLoss[0m : 2.88012
[1mStep[0m  [112/169], [94mLoss[0m : 2.71795
[1mStep[0m  [128/169], [94mLoss[0m : 2.46347
[1mStep[0m  [144/169], [94mLoss[0m : 2.11033
[1mStep[0m  [160/169], [94mLoss[0m : 2.17349

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.363, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.23249
[1mStep[0m  [16/169], [94mLoss[0m : 2.29614
[1mStep[0m  [32/169], [94mLoss[0m : 2.23939
[1mStep[0m  [48/169], [94mLoss[0m : 2.50173
[1mStep[0m  [64/169], [94mLoss[0m : 2.54862
[1mStep[0m  [80/169], [94mLoss[0m : 2.08774
[1mStep[0m  [96/169], [94mLoss[0m : 2.11526
[1mStep[0m  [112/169], [94mLoss[0m : 2.36778
[1mStep[0m  [128/169], [94mLoss[0m : 2.82974
[1mStep[0m  [144/169], [94mLoss[0m : 2.28941
[1mStep[0m  [160/169], [94mLoss[0m : 2.82059

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.384, [92mTest[0m: 2.335, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.68940
[1mStep[0m  [16/169], [94mLoss[0m : 2.39990
[1mStep[0m  [32/169], [94mLoss[0m : 2.80573
[1mStep[0m  [48/169], [94mLoss[0m : 2.51314
[1mStep[0m  [64/169], [94mLoss[0m : 1.76312
[1mStep[0m  [80/169], [94mLoss[0m : 2.04006
[1mStep[0m  [96/169], [94mLoss[0m : 2.46010
[1mStep[0m  [112/169], [94mLoss[0m : 2.27703
[1mStep[0m  [128/169], [94mLoss[0m : 2.67779
[1mStep[0m  [144/169], [94mLoss[0m : 3.04797
[1mStep[0m  [160/169], [94mLoss[0m : 2.73938

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.388, [92mTest[0m: 2.359, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.72493
[1mStep[0m  [16/169], [94mLoss[0m : 2.39160
[1mStep[0m  [32/169], [94mLoss[0m : 2.47242
[1mStep[0m  [48/169], [94mLoss[0m : 2.50196
[1mStep[0m  [64/169], [94mLoss[0m : 2.76981
[1mStep[0m  [80/169], [94mLoss[0m : 2.62565
[1mStep[0m  [96/169], [94mLoss[0m : 2.49565
[1mStep[0m  [112/169], [94mLoss[0m : 2.08587
[1mStep[0m  [128/169], [94mLoss[0m : 2.59234
[1mStep[0m  [144/169], [94mLoss[0m : 2.13651
[1mStep[0m  [160/169], [94mLoss[0m : 2.50121

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.389, [92mTest[0m: 2.371, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.86906
[1mStep[0m  [16/169], [94mLoss[0m : 2.21664
[1mStep[0m  [32/169], [94mLoss[0m : 2.68141
[1mStep[0m  [48/169], [94mLoss[0m : 2.23720
[1mStep[0m  [64/169], [94mLoss[0m : 2.26787
[1mStep[0m  [80/169], [94mLoss[0m : 2.04824
[1mStep[0m  [96/169], [94mLoss[0m : 2.58692
[1mStep[0m  [112/169], [94mLoss[0m : 1.86754
[1mStep[0m  [128/169], [94mLoss[0m : 2.98748
[1mStep[0m  [144/169], [94mLoss[0m : 2.71994
[1mStep[0m  [160/169], [94mLoss[0m : 2.46826

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.394, [92mTest[0m: 2.400, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.39898
[1mStep[0m  [16/169], [94mLoss[0m : 2.21420
[1mStep[0m  [32/169], [94mLoss[0m : 2.69576
[1mStep[0m  [48/169], [94mLoss[0m : 2.33296
[1mStep[0m  [64/169], [94mLoss[0m : 2.32440
[1mStep[0m  [80/169], [94mLoss[0m : 2.12510
[1mStep[0m  [96/169], [94mLoss[0m : 2.26967
[1mStep[0m  [112/169], [94mLoss[0m : 2.37117
[1mStep[0m  [128/169], [94mLoss[0m : 2.49079
[1mStep[0m  [144/169], [94mLoss[0m : 2.24209
[1mStep[0m  [160/169], [94mLoss[0m : 2.27064

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.391, [92mTest[0m: 2.347, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.27705
[1mStep[0m  [16/169], [94mLoss[0m : 2.38090
[1mStep[0m  [32/169], [94mLoss[0m : 2.40418
[1mStep[0m  [48/169], [94mLoss[0m : 2.34909
[1mStep[0m  [64/169], [94mLoss[0m : 2.16135
[1mStep[0m  [80/169], [94mLoss[0m : 2.62848
[1mStep[0m  [96/169], [94mLoss[0m : 2.19804
[1mStep[0m  [112/169], [94mLoss[0m : 2.41385
[1mStep[0m  [128/169], [94mLoss[0m : 2.42335
[1mStep[0m  [144/169], [94mLoss[0m : 2.91776
[1mStep[0m  [160/169], [94mLoss[0m : 2.44101

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.393, [92mTest[0m: 2.340, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.92759
[1mStep[0m  [16/169], [94mLoss[0m : 2.44120
[1mStep[0m  [32/169], [94mLoss[0m : 2.50962
[1mStep[0m  [48/169], [94mLoss[0m : 2.12015
[1mStep[0m  [64/169], [94mLoss[0m : 2.45228
[1mStep[0m  [80/169], [94mLoss[0m : 1.98165
[1mStep[0m  [96/169], [94mLoss[0m : 2.66584
[1mStep[0m  [112/169], [94mLoss[0m : 2.17496
[1mStep[0m  [128/169], [94mLoss[0m : 2.36639
[1mStep[0m  [144/169], [94mLoss[0m : 2.54549
[1mStep[0m  [160/169], [94mLoss[0m : 2.58742

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.364, [92mTest[0m: 2.331, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.40567
[1mStep[0m  [16/169], [94mLoss[0m : 2.07480
[1mStep[0m  [32/169], [94mLoss[0m : 2.41929
[1mStep[0m  [48/169], [94mLoss[0m : 2.15278
[1mStep[0m  [64/169], [94mLoss[0m : 2.70096
[1mStep[0m  [80/169], [94mLoss[0m : 1.92722
[1mStep[0m  [96/169], [94mLoss[0m : 2.43064
[1mStep[0m  [112/169], [94mLoss[0m : 2.06593
[1mStep[0m  [128/169], [94mLoss[0m : 2.10616
[1mStep[0m  [144/169], [94mLoss[0m : 1.83242
[1mStep[0m  [160/169], [94mLoss[0m : 2.26010

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.374, [92mTest[0m: 2.337, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.48114
[1mStep[0m  [16/169], [94mLoss[0m : 2.30483
[1mStep[0m  [32/169], [94mLoss[0m : 2.59939
[1mStep[0m  [48/169], [94mLoss[0m : 2.39812
[1mStep[0m  [64/169], [94mLoss[0m : 2.80458
[1mStep[0m  [80/169], [94mLoss[0m : 2.41642
[1mStep[0m  [96/169], [94mLoss[0m : 1.84851
[1mStep[0m  [112/169], [94mLoss[0m : 2.44799
[1mStep[0m  [128/169], [94mLoss[0m : 2.38322
[1mStep[0m  [144/169], [94mLoss[0m : 2.25071
[1mStep[0m  [160/169], [94mLoss[0m : 2.26482

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.377, [92mTest[0m: 2.345, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.17857
[1mStep[0m  [16/169], [94mLoss[0m : 1.98621
[1mStep[0m  [32/169], [94mLoss[0m : 2.36250
[1mStep[0m  [48/169], [94mLoss[0m : 2.94719
[1mStep[0m  [64/169], [94mLoss[0m : 2.28801
[1mStep[0m  [80/169], [94mLoss[0m : 2.71559
[1mStep[0m  [96/169], [94mLoss[0m : 2.28496
[1mStep[0m  [112/169], [94mLoss[0m : 2.33783
[1mStep[0m  [128/169], [94mLoss[0m : 2.16387
[1mStep[0m  [144/169], [94mLoss[0m : 2.40248
[1mStep[0m  [160/169], [94mLoss[0m : 2.21763

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.373, [92mTest[0m: 2.330, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.47979
[1mStep[0m  [16/169], [94mLoss[0m : 2.16237
[1mStep[0m  [32/169], [94mLoss[0m : 1.99061
[1mStep[0m  [48/169], [94mLoss[0m : 2.43322
[1mStep[0m  [64/169], [94mLoss[0m : 2.75748
[1mStep[0m  [80/169], [94mLoss[0m : 2.48871
[1mStep[0m  [96/169], [94mLoss[0m : 2.45392
[1mStep[0m  [112/169], [94mLoss[0m : 2.78505
[1mStep[0m  [128/169], [94mLoss[0m : 2.28104
[1mStep[0m  [144/169], [94mLoss[0m : 2.66117
[1mStep[0m  [160/169], [94mLoss[0m : 2.55061

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.372, [92mTest[0m: 2.332, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.65622
[1mStep[0m  [16/169], [94mLoss[0m : 2.79612
[1mStep[0m  [32/169], [94mLoss[0m : 2.10505
[1mStep[0m  [48/169], [94mLoss[0m : 2.19234
[1mStep[0m  [64/169], [94mLoss[0m : 2.16087
[1mStep[0m  [80/169], [94mLoss[0m : 2.28680
[1mStep[0m  [96/169], [94mLoss[0m : 2.05398
[1mStep[0m  [112/169], [94mLoss[0m : 2.75967
[1mStep[0m  [128/169], [94mLoss[0m : 2.12500
[1mStep[0m  [144/169], [94mLoss[0m : 1.95797
[1mStep[0m  [160/169], [94mLoss[0m : 2.41504

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.375, [92mTest[0m: 2.330, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.07048
[1mStep[0m  [16/169], [94mLoss[0m : 2.27098
[1mStep[0m  [32/169], [94mLoss[0m : 2.84456
[1mStep[0m  [48/169], [94mLoss[0m : 2.18982
[1mStep[0m  [64/169], [94mLoss[0m : 2.10058
[1mStep[0m  [80/169], [94mLoss[0m : 2.58854
[1mStep[0m  [96/169], [94mLoss[0m : 1.94731
[1mStep[0m  [112/169], [94mLoss[0m : 2.24490
[1mStep[0m  [128/169], [94mLoss[0m : 2.50085
[1mStep[0m  [144/169], [94mLoss[0m : 2.48781
[1mStep[0m  [160/169], [94mLoss[0m : 2.42901

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.359, [92mTest[0m: 2.359, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.23393
[1mStep[0m  [16/169], [94mLoss[0m : 2.85339
[1mStep[0m  [32/169], [94mLoss[0m : 2.23730
[1mStep[0m  [48/169], [94mLoss[0m : 2.60495
[1mStep[0m  [64/169], [94mLoss[0m : 2.24059
[1mStep[0m  [80/169], [94mLoss[0m : 1.78057
[1mStep[0m  [96/169], [94mLoss[0m : 2.16849
[1mStep[0m  [112/169], [94mLoss[0m : 2.45420
[1mStep[0m  [128/169], [94mLoss[0m : 2.35096
[1mStep[0m  [144/169], [94mLoss[0m : 2.32434
[1mStep[0m  [160/169], [94mLoss[0m : 2.34748

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.371, [92mTest[0m: 2.368, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.29870
[1mStep[0m  [16/169], [94mLoss[0m : 2.77645
[1mStep[0m  [32/169], [94mLoss[0m : 2.37804
[1mStep[0m  [48/169], [94mLoss[0m : 2.13865
[1mStep[0m  [64/169], [94mLoss[0m : 2.24022
[1mStep[0m  [80/169], [94mLoss[0m : 2.29464
[1mStep[0m  [96/169], [94mLoss[0m : 2.79875
[1mStep[0m  [112/169], [94mLoss[0m : 2.80469
[1mStep[0m  [128/169], [94mLoss[0m : 2.36439
[1mStep[0m  [144/169], [94mLoss[0m : 2.59257
[1mStep[0m  [160/169], [94mLoss[0m : 2.28342

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.359, [92mTest[0m: 2.355, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.372
====================================

Phase 1 - Evaluation MAE:  2.372362854225295
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/169], [94mLoss[0m : 2.37826
[1mStep[0m  [16/169], [94mLoss[0m : 2.43358
[1mStep[0m  [32/169], [94mLoss[0m : 2.24357
[1mStep[0m  [48/169], [94mLoss[0m : 2.58156
[1mStep[0m  [64/169], [94mLoss[0m : 2.57403
[1mStep[0m  [80/169], [94mLoss[0m : 2.55615
[1mStep[0m  [96/169], [94mLoss[0m : 2.45714
[1mStep[0m  [112/169], [94mLoss[0m : 2.38709
[1mStep[0m  [128/169], [94mLoss[0m : 2.49556
[1mStep[0m  [144/169], [94mLoss[0m : 2.78441
[1mStep[0m  [160/169], [94mLoss[0m : 2.33360

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.369, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.81439
[1mStep[0m  [16/169], [94mLoss[0m : 2.27828
[1mStep[0m  [32/169], [94mLoss[0m : 2.06699
[1mStep[0m  [48/169], [94mLoss[0m : 2.32649
[1mStep[0m  [64/169], [94mLoss[0m : 2.12788
[1mStep[0m  [80/169], [94mLoss[0m : 2.18636
[1mStep[0m  [96/169], [94mLoss[0m : 2.75920
[1mStep[0m  [112/169], [94mLoss[0m : 2.64936
[1mStep[0m  [128/169], [94mLoss[0m : 2.47038
[1mStep[0m  [144/169], [94mLoss[0m : 1.98025
[1mStep[0m  [160/169], [94mLoss[0m : 2.49691

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.354, [92mTest[0m: 2.348, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.87437
[1mStep[0m  [16/169], [94mLoss[0m : 2.41127
[1mStep[0m  [32/169], [94mLoss[0m : 2.35019
[1mStep[0m  [48/169], [94mLoss[0m : 2.80982
[1mStep[0m  [64/169], [94mLoss[0m : 2.06011
[1mStep[0m  [80/169], [94mLoss[0m : 2.40652
[1mStep[0m  [96/169], [94mLoss[0m : 2.14791
[1mStep[0m  [112/169], [94mLoss[0m : 2.24943
[1mStep[0m  [128/169], [94mLoss[0m : 2.06741
[1mStep[0m  [144/169], [94mLoss[0m : 2.40161
[1mStep[0m  [160/169], [94mLoss[0m : 1.74697

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.244, [92mTest[0m: 2.390, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.78600
[1mStep[0m  [16/169], [94mLoss[0m : 2.15520
[1mStep[0m  [32/169], [94mLoss[0m : 1.50612
[1mStep[0m  [48/169], [94mLoss[0m : 2.12534
[1mStep[0m  [64/169], [94mLoss[0m : 2.18874
[1mStep[0m  [80/169], [94mLoss[0m : 2.32673
[1mStep[0m  [96/169], [94mLoss[0m : 1.95659
[1mStep[0m  [112/169], [94mLoss[0m : 2.01819
[1mStep[0m  [128/169], [94mLoss[0m : 2.27178
[1mStep[0m  [144/169], [94mLoss[0m : 2.48375
[1mStep[0m  [160/169], [94mLoss[0m : 2.75238

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.173, [92mTest[0m: 2.353, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.89597
[1mStep[0m  [16/169], [94mLoss[0m : 2.39664
[1mStep[0m  [32/169], [94mLoss[0m : 2.26961
[1mStep[0m  [48/169], [94mLoss[0m : 1.63809
[1mStep[0m  [64/169], [94mLoss[0m : 1.96484
[1mStep[0m  [80/169], [94mLoss[0m : 1.59217
[1mStep[0m  [96/169], [94mLoss[0m : 1.99200
[1mStep[0m  [112/169], [94mLoss[0m : 1.96624
[1mStep[0m  [128/169], [94mLoss[0m : 2.18944
[1mStep[0m  [144/169], [94mLoss[0m : 2.24392
[1mStep[0m  [160/169], [94mLoss[0m : 1.79965

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.092, [92mTest[0m: 2.448, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.03606
[1mStep[0m  [16/169], [94mLoss[0m : 2.15717
[1mStep[0m  [32/169], [94mLoss[0m : 1.77709
[1mStep[0m  [48/169], [94mLoss[0m : 2.18618
[1mStep[0m  [64/169], [94mLoss[0m : 2.49604
[1mStep[0m  [80/169], [94mLoss[0m : 2.05286
[1mStep[0m  [96/169], [94mLoss[0m : 1.92295
[1mStep[0m  [112/169], [94mLoss[0m : 1.98338
[1mStep[0m  [128/169], [94mLoss[0m : 2.05154
[1mStep[0m  [144/169], [94mLoss[0m : 2.76829
[1mStep[0m  [160/169], [94mLoss[0m : 1.99987

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.053, [92mTest[0m: 2.414, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.98979
[1mStep[0m  [16/169], [94mLoss[0m : 2.05714
[1mStep[0m  [32/169], [94mLoss[0m : 2.12810
[1mStep[0m  [48/169], [94mLoss[0m : 1.76476
[1mStep[0m  [64/169], [94mLoss[0m : 2.21797
[1mStep[0m  [80/169], [94mLoss[0m : 2.30746
[1mStep[0m  [96/169], [94mLoss[0m : 1.91256
[1mStep[0m  [112/169], [94mLoss[0m : 2.07228
[1mStep[0m  [128/169], [94mLoss[0m : 1.84166
[1mStep[0m  [144/169], [94mLoss[0m : 1.68252
[1mStep[0m  [160/169], [94mLoss[0m : 1.58225

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.975, [92mTest[0m: 2.417, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.85946
[1mStep[0m  [16/169], [94mLoss[0m : 1.76153
[1mStep[0m  [32/169], [94mLoss[0m : 1.63757
[1mStep[0m  [48/169], [94mLoss[0m : 1.79418
[1mStep[0m  [64/169], [94mLoss[0m : 1.67178
[1mStep[0m  [80/169], [94mLoss[0m : 2.05353
[1mStep[0m  [96/169], [94mLoss[0m : 2.05732
[1mStep[0m  [112/169], [94mLoss[0m : 1.75644
[1mStep[0m  [128/169], [94mLoss[0m : 1.50272
[1mStep[0m  [144/169], [94mLoss[0m : 1.94932
[1mStep[0m  [160/169], [94mLoss[0m : 2.23866

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.938, [92mTest[0m: 2.441, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.01352
[1mStep[0m  [16/169], [94mLoss[0m : 2.09026
[1mStep[0m  [32/169], [94mLoss[0m : 1.61511
[1mStep[0m  [48/169], [94mLoss[0m : 1.85842
[1mStep[0m  [64/169], [94mLoss[0m : 1.58943
[1mStep[0m  [80/169], [94mLoss[0m : 1.67519
[1mStep[0m  [96/169], [94mLoss[0m : 1.57051
[1mStep[0m  [112/169], [94mLoss[0m : 2.34857
[1mStep[0m  [128/169], [94mLoss[0m : 1.99931
[1mStep[0m  [144/169], [94mLoss[0m : 1.89493
[1mStep[0m  [160/169], [94mLoss[0m : 2.30066

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.903, [92mTest[0m: 2.482, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.50423
[1mStep[0m  [16/169], [94mLoss[0m : 2.10717
[1mStep[0m  [32/169], [94mLoss[0m : 1.70578
[1mStep[0m  [48/169], [94mLoss[0m : 1.64202
[1mStep[0m  [64/169], [94mLoss[0m : 1.52450
[1mStep[0m  [80/169], [94mLoss[0m : 2.41873
[1mStep[0m  [96/169], [94mLoss[0m : 1.59270
[1mStep[0m  [112/169], [94mLoss[0m : 1.99958
[1mStep[0m  [128/169], [94mLoss[0m : 1.49741
[1mStep[0m  [144/169], [94mLoss[0m : 1.78368
[1mStep[0m  [160/169], [94mLoss[0m : 1.85064

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.841, [92mTest[0m: 2.417, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.63785
[1mStep[0m  [16/169], [94mLoss[0m : 1.74029
[1mStep[0m  [32/169], [94mLoss[0m : 1.43011
[1mStep[0m  [48/169], [94mLoss[0m : 1.76962
[1mStep[0m  [64/169], [94mLoss[0m : 1.79294
[1mStep[0m  [80/169], [94mLoss[0m : 1.48033
[1mStep[0m  [96/169], [94mLoss[0m : 1.56780
[1mStep[0m  [112/169], [94mLoss[0m : 1.64713
[1mStep[0m  [128/169], [94mLoss[0m : 2.00935
[1mStep[0m  [144/169], [94mLoss[0m : 1.70096
[1mStep[0m  [160/169], [94mLoss[0m : 1.83183

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.821, [92mTest[0m: 2.474, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.90921
[1mStep[0m  [16/169], [94mLoss[0m : 1.52092
[1mStep[0m  [32/169], [94mLoss[0m : 1.58874
[1mStep[0m  [48/169], [94mLoss[0m : 1.65358
[1mStep[0m  [64/169], [94mLoss[0m : 1.89933
[1mStep[0m  [80/169], [94mLoss[0m : 1.98729
[1mStep[0m  [96/169], [94mLoss[0m : 1.79127
[1mStep[0m  [112/169], [94mLoss[0m : 1.60011
[1mStep[0m  [128/169], [94mLoss[0m : 1.49690
[1mStep[0m  [144/169], [94mLoss[0m : 2.27419
[1mStep[0m  [160/169], [94mLoss[0m : 1.92641

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.770, [92mTest[0m: 2.443, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.89853
[1mStep[0m  [16/169], [94mLoss[0m : 1.83007
[1mStep[0m  [32/169], [94mLoss[0m : 2.06807
[1mStep[0m  [48/169], [94mLoss[0m : 1.77164
[1mStep[0m  [64/169], [94mLoss[0m : 1.98624
[1mStep[0m  [80/169], [94mLoss[0m : 1.93054
[1mStep[0m  [96/169], [94mLoss[0m : 1.74303
[1mStep[0m  [112/169], [94mLoss[0m : 1.59251
[1mStep[0m  [128/169], [94mLoss[0m : 1.85807
[1mStep[0m  [144/169], [94mLoss[0m : 1.82202
[1mStep[0m  [160/169], [94mLoss[0m : 2.14436

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.731, [92mTest[0m: 2.514, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.40521
[1mStep[0m  [16/169], [94mLoss[0m : 1.90341
[1mStep[0m  [32/169], [94mLoss[0m : 1.62063
[1mStep[0m  [48/169], [94mLoss[0m : 1.65401
[1mStep[0m  [64/169], [94mLoss[0m : 1.30994
[1mStep[0m  [80/169], [94mLoss[0m : 1.46693
[1mStep[0m  [96/169], [94mLoss[0m : 1.73638
[1mStep[0m  [112/169], [94mLoss[0m : 1.46482
[1mStep[0m  [128/169], [94mLoss[0m : 1.77228
[1mStep[0m  [144/169], [94mLoss[0m : 1.66837
[1mStep[0m  [160/169], [94mLoss[0m : 1.69324

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.677, [92mTest[0m: 2.499, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.51292
[1mStep[0m  [16/169], [94mLoss[0m : 1.49730
[1mStep[0m  [32/169], [94mLoss[0m : 1.98899
[1mStep[0m  [48/169], [94mLoss[0m : 1.41808
[1mStep[0m  [64/169], [94mLoss[0m : 1.55295
[1mStep[0m  [80/169], [94mLoss[0m : 1.56045
[1mStep[0m  [96/169], [94mLoss[0m : 1.48918
[1mStep[0m  [112/169], [94mLoss[0m : 1.73427
[1mStep[0m  [128/169], [94mLoss[0m : 1.64527
[1mStep[0m  [144/169], [94mLoss[0m : 2.13501
[1mStep[0m  [160/169], [94mLoss[0m : 1.69790

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.659, [92mTest[0m: 2.451, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.25172
[1mStep[0m  [16/169], [94mLoss[0m : 1.59460
[1mStep[0m  [32/169], [94mLoss[0m : 1.41571
[1mStep[0m  [48/169], [94mLoss[0m : 1.18399
[1mStep[0m  [64/169], [94mLoss[0m : 1.50075
[1mStep[0m  [80/169], [94mLoss[0m : 1.81961
[1mStep[0m  [96/169], [94mLoss[0m : 1.59913
[1mStep[0m  [112/169], [94mLoss[0m : 1.63719
[1mStep[0m  [128/169], [94mLoss[0m : 1.54417
[1mStep[0m  [144/169], [94mLoss[0m : 1.59162
[1mStep[0m  [160/169], [94mLoss[0m : 1.96195

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.641, [92mTest[0m: 2.487, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.86193
[1mStep[0m  [16/169], [94mLoss[0m : 1.46398
[1mStep[0m  [32/169], [94mLoss[0m : 1.48481
[1mStep[0m  [48/169], [94mLoss[0m : 1.58376
[1mStep[0m  [64/169], [94mLoss[0m : 1.61757
[1mStep[0m  [80/169], [94mLoss[0m : 1.21820
[1mStep[0m  [96/169], [94mLoss[0m : 1.67147
[1mStep[0m  [112/169], [94mLoss[0m : 1.49157
[1mStep[0m  [128/169], [94mLoss[0m : 1.69095
[1mStep[0m  [144/169], [94mLoss[0m : 1.35097
[1mStep[0m  [160/169], [94mLoss[0m : 1.72997

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.593, [92mTest[0m: 2.483, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.38869
[1mStep[0m  [16/169], [94mLoss[0m : 1.47423
[1mStep[0m  [32/169], [94mLoss[0m : 1.71326
[1mStep[0m  [48/169], [94mLoss[0m : 1.34869
[1mStep[0m  [64/169], [94mLoss[0m : 1.41349
[1mStep[0m  [80/169], [94mLoss[0m : 1.38032
[1mStep[0m  [96/169], [94mLoss[0m : 1.91578
[1mStep[0m  [112/169], [94mLoss[0m : 1.69871
[1mStep[0m  [128/169], [94mLoss[0m : 1.43204
[1mStep[0m  [144/169], [94mLoss[0m : 1.78953
[1mStep[0m  [160/169], [94mLoss[0m : 1.64116

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.563, [92mTest[0m: 2.486, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.48673
[1mStep[0m  [16/169], [94mLoss[0m : 1.63911
[1mStep[0m  [32/169], [94mLoss[0m : 1.46624
[1mStep[0m  [48/169], [94mLoss[0m : 1.56000
[1mStep[0m  [64/169], [94mLoss[0m : 1.48243
[1mStep[0m  [80/169], [94mLoss[0m : 1.70735
[1mStep[0m  [96/169], [94mLoss[0m : 1.61862
[1mStep[0m  [112/169], [94mLoss[0m : 1.59323
[1mStep[0m  [128/169], [94mLoss[0m : 1.50932
[1mStep[0m  [144/169], [94mLoss[0m : 1.40049
[1mStep[0m  [160/169], [94mLoss[0m : 1.62868

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.579, [92mTest[0m: 2.511, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.30662
[1mStep[0m  [16/169], [94mLoss[0m : 1.70409
[1mStep[0m  [32/169], [94mLoss[0m : 1.36692
[1mStep[0m  [48/169], [94mLoss[0m : 1.79103
[1mStep[0m  [64/169], [94mLoss[0m : 1.40448
[1mStep[0m  [80/169], [94mLoss[0m : 1.54017
[1mStep[0m  [96/169], [94mLoss[0m : 1.31625
[1mStep[0m  [112/169], [94mLoss[0m : 1.63489
[1mStep[0m  [128/169], [94mLoss[0m : 1.57791
[1mStep[0m  [144/169], [94mLoss[0m : 1.51865
[1mStep[0m  [160/169], [94mLoss[0m : 1.82366

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.528, [92mTest[0m: 2.452, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.18587
[1mStep[0m  [16/169], [94mLoss[0m : 1.59572
[1mStep[0m  [32/169], [94mLoss[0m : 1.44143
[1mStep[0m  [48/169], [94mLoss[0m : 1.30016
[1mStep[0m  [64/169], [94mLoss[0m : 1.51407
[1mStep[0m  [80/169], [94mLoss[0m : 1.53686
[1mStep[0m  [96/169], [94mLoss[0m : 1.48102
[1mStep[0m  [112/169], [94mLoss[0m : 1.68410
[1mStep[0m  [128/169], [94mLoss[0m : 1.58986
[1mStep[0m  [144/169], [94mLoss[0m : 1.67398
[1mStep[0m  [160/169], [94mLoss[0m : 1.54581

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.473, [92mTest[0m: 2.447, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.24507
[1mStep[0m  [16/169], [94mLoss[0m : 1.44273
[1mStep[0m  [32/169], [94mLoss[0m : 1.29745
[1mStep[0m  [48/169], [94mLoss[0m : 1.62393
[1mStep[0m  [64/169], [94mLoss[0m : 1.32266
[1mStep[0m  [80/169], [94mLoss[0m : 1.40799
[1mStep[0m  [96/169], [94mLoss[0m : 1.12087
[1mStep[0m  [112/169], [94mLoss[0m : 1.73503
[1mStep[0m  [128/169], [94mLoss[0m : 1.55748
[1mStep[0m  [144/169], [94mLoss[0m : 1.22957
[1mStep[0m  [160/169], [94mLoss[0m : 1.36834

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.462, [92mTest[0m: 2.511, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.45404
[1mStep[0m  [16/169], [94mLoss[0m : 1.43074
[1mStep[0m  [32/169], [94mLoss[0m : 1.20614
[1mStep[0m  [48/169], [94mLoss[0m : 1.35448
[1mStep[0m  [64/169], [94mLoss[0m : 1.31057
[1mStep[0m  [80/169], [94mLoss[0m : 1.39891
[1mStep[0m  [96/169], [94mLoss[0m : 1.36057
[1mStep[0m  [112/169], [94mLoss[0m : 1.27205
[1mStep[0m  [128/169], [94mLoss[0m : 1.40829
[1mStep[0m  [144/169], [94mLoss[0m : 1.59465
[1mStep[0m  [160/169], [94mLoss[0m : 1.40022

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.413, [92mTest[0m: 2.460, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.09295
[1mStep[0m  [16/169], [94mLoss[0m : 1.59704
[1mStep[0m  [32/169], [94mLoss[0m : 1.30790
[1mStep[0m  [48/169], [94mLoss[0m : 1.33626
[1mStep[0m  [64/169], [94mLoss[0m : 1.48292
[1mStep[0m  [80/169], [94mLoss[0m : 1.40141
[1mStep[0m  [96/169], [94mLoss[0m : 1.55404
[1mStep[0m  [112/169], [94mLoss[0m : 1.46435
[1mStep[0m  [128/169], [94mLoss[0m : 1.49873
[1mStep[0m  [144/169], [94mLoss[0m : 1.62881
[1mStep[0m  [160/169], [94mLoss[0m : 1.76990

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.399, [92mTest[0m: 2.560, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.18276
[1mStep[0m  [16/169], [94mLoss[0m : 1.23639
[1mStep[0m  [32/169], [94mLoss[0m : 0.95013
[1mStep[0m  [48/169], [94mLoss[0m : 1.36861
[1mStep[0m  [64/169], [94mLoss[0m : 1.49867
[1mStep[0m  [80/169], [94mLoss[0m : 2.10673
[1mStep[0m  [96/169], [94mLoss[0m : 1.20844
[1mStep[0m  [112/169], [94mLoss[0m : 1.36421
[1mStep[0m  [128/169], [94mLoss[0m : 1.16692
[1mStep[0m  [144/169], [94mLoss[0m : 1.65450
[1mStep[0m  [160/169], [94mLoss[0m : 1.67267

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.396, [92mTest[0m: 2.484, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.50796
[1mStep[0m  [16/169], [94mLoss[0m : 1.14081
[1mStep[0m  [32/169], [94mLoss[0m : 1.32246
[1mStep[0m  [48/169], [94mLoss[0m : 1.45204
[1mStep[0m  [64/169], [94mLoss[0m : 1.25989
[1mStep[0m  [80/169], [94mLoss[0m : 1.31157
[1mStep[0m  [96/169], [94mLoss[0m : 1.27795
[1mStep[0m  [112/169], [94mLoss[0m : 1.18259
[1mStep[0m  [128/169], [94mLoss[0m : 1.12002
[1mStep[0m  [144/169], [94mLoss[0m : 1.05252
[1mStep[0m  [160/169], [94mLoss[0m : 1.34132

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.357, [92mTest[0m: 2.519, [96mlr[0m: 0.004545
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 25 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.571
====================================

Phase 2 - Evaluation MAE:  2.5710160859993527
MAE score P1      2.372363
MAE score P2      2.571016
loss              1.356873
learning_rate      0.00505
batch_size              64
hidden_sizes         [300]
epochs                  30
activation            tanh
optimizer              sgd
early stopping        True
dropout                0.2
momentum               0.9
weight_decay         0.001
Name: 3, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/169], [94mLoss[0m : 11.11175
[1mStep[0m  [16/169], [94mLoss[0m : 2.86853
[1mStep[0m  [32/169], [94mLoss[0m : 3.32266
[1mStep[0m  [48/169], [94mLoss[0m : 2.83005
[1mStep[0m  [64/169], [94mLoss[0m : 2.68338
[1mStep[0m  [80/169], [94mLoss[0m : 2.96916
[1mStep[0m  [96/169], [94mLoss[0m : 2.44231
[1mStep[0m  [112/169], [94mLoss[0m : 2.73846
[1mStep[0m  [128/169], [94mLoss[0m : 2.28701
[1mStep[0m  [144/169], [94mLoss[0m : 2.36186
[1mStep[0m  [160/169], [94mLoss[0m : 2.94469

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.113, [92mTest[0m: 11.265, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.19459
[1mStep[0m  [16/169], [94mLoss[0m : 2.71769
[1mStep[0m  [32/169], [94mLoss[0m : 2.32244
[1mStep[0m  [48/169], [94mLoss[0m : 2.45032
[1mStep[0m  [64/169], [94mLoss[0m : 2.44716
[1mStep[0m  [80/169], [94mLoss[0m : 2.62351
[1mStep[0m  [96/169], [94mLoss[0m : 2.52195
[1mStep[0m  [112/169], [94mLoss[0m : 2.44079
[1mStep[0m  [128/169], [94mLoss[0m : 2.71467
[1mStep[0m  [144/169], [94mLoss[0m : 2.27083
[1mStep[0m  [160/169], [94mLoss[0m : 2.54309

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.376, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.66988
[1mStep[0m  [16/169], [94mLoss[0m : 2.41916
[1mStep[0m  [32/169], [94mLoss[0m : 2.21004
[1mStep[0m  [48/169], [94mLoss[0m : 2.52280
[1mStep[0m  [64/169], [94mLoss[0m : 2.63499
[1mStep[0m  [80/169], [94mLoss[0m : 2.64194
[1mStep[0m  [96/169], [94mLoss[0m : 2.21933
[1mStep[0m  [112/169], [94mLoss[0m : 2.18787
[1mStep[0m  [128/169], [94mLoss[0m : 2.51646
[1mStep[0m  [144/169], [94mLoss[0m : 2.86073
[1mStep[0m  [160/169], [94mLoss[0m : 2.93067

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.338, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.39125
[1mStep[0m  [16/169], [94mLoss[0m : 2.56540
[1mStep[0m  [32/169], [94mLoss[0m : 2.62995
[1mStep[0m  [48/169], [94mLoss[0m : 2.17741
[1mStep[0m  [64/169], [94mLoss[0m : 2.64223
[1mStep[0m  [80/169], [94mLoss[0m : 2.58908
[1mStep[0m  [96/169], [94mLoss[0m : 2.23886
[1mStep[0m  [112/169], [94mLoss[0m : 2.52906
[1mStep[0m  [128/169], [94mLoss[0m : 2.94191
[1mStep[0m  [144/169], [94mLoss[0m : 2.44606
[1mStep[0m  [160/169], [94mLoss[0m : 2.44550

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.337, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.86183
[1mStep[0m  [16/169], [94mLoss[0m : 2.07756
[1mStep[0m  [32/169], [94mLoss[0m : 2.71018
[1mStep[0m  [48/169], [94mLoss[0m : 2.79441
[1mStep[0m  [64/169], [94mLoss[0m : 2.34568
[1mStep[0m  [80/169], [94mLoss[0m : 2.85374
[1mStep[0m  [96/169], [94mLoss[0m : 2.35088
[1mStep[0m  [112/169], [94mLoss[0m : 2.30339
[1mStep[0m  [128/169], [94mLoss[0m : 2.59880
[1mStep[0m  [144/169], [94mLoss[0m : 2.38016
[1mStep[0m  [160/169], [94mLoss[0m : 2.76281

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.359, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.63843
[1mStep[0m  [16/169], [94mLoss[0m : 2.95947
[1mStep[0m  [32/169], [94mLoss[0m : 2.62432
[1mStep[0m  [48/169], [94mLoss[0m : 2.53083
[1mStep[0m  [64/169], [94mLoss[0m : 2.75830
[1mStep[0m  [80/169], [94mLoss[0m : 2.63274
[1mStep[0m  [96/169], [94mLoss[0m : 2.54036
[1mStep[0m  [112/169], [94mLoss[0m : 3.02515
[1mStep[0m  [128/169], [94mLoss[0m : 2.22168
[1mStep[0m  [144/169], [94mLoss[0m : 2.41712
[1mStep[0m  [160/169], [94mLoss[0m : 2.45310

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.339, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.82100
[1mStep[0m  [16/169], [94mLoss[0m : 2.54871
[1mStep[0m  [32/169], [94mLoss[0m : 2.53833
[1mStep[0m  [48/169], [94mLoss[0m : 2.42944
[1mStep[0m  [64/169], [94mLoss[0m : 2.06847
[1mStep[0m  [80/169], [94mLoss[0m : 2.28937
[1mStep[0m  [96/169], [94mLoss[0m : 2.52438
[1mStep[0m  [112/169], [94mLoss[0m : 2.16905
[1mStep[0m  [128/169], [94mLoss[0m : 2.26162
[1mStep[0m  [144/169], [94mLoss[0m : 2.44845
[1mStep[0m  [160/169], [94mLoss[0m : 2.23224

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.332, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.09852
[1mStep[0m  [16/169], [94mLoss[0m : 2.22416
[1mStep[0m  [32/169], [94mLoss[0m : 2.75032
[1mStep[0m  [48/169], [94mLoss[0m : 2.28362
[1mStep[0m  [64/169], [94mLoss[0m : 2.73837
[1mStep[0m  [80/169], [94mLoss[0m : 2.38569
[1mStep[0m  [96/169], [94mLoss[0m : 2.32993
[1mStep[0m  [112/169], [94mLoss[0m : 2.26106
[1mStep[0m  [128/169], [94mLoss[0m : 2.65864
[1mStep[0m  [144/169], [94mLoss[0m : 2.24871
[1mStep[0m  [160/169], [94mLoss[0m : 2.57824

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.342, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.23522
[1mStep[0m  [16/169], [94mLoss[0m : 2.41010
[1mStep[0m  [32/169], [94mLoss[0m : 2.82509
[1mStep[0m  [48/169], [94mLoss[0m : 2.58520
[1mStep[0m  [64/169], [94mLoss[0m : 2.40393
[1mStep[0m  [80/169], [94mLoss[0m : 2.26824
[1mStep[0m  [96/169], [94mLoss[0m : 2.31557
[1mStep[0m  [112/169], [94mLoss[0m : 2.49926
[1mStep[0m  [128/169], [94mLoss[0m : 2.98614
[1mStep[0m  [144/169], [94mLoss[0m : 2.29776
[1mStep[0m  [160/169], [94mLoss[0m : 2.72088

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.339, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.75352
[1mStep[0m  [16/169], [94mLoss[0m : 2.39247
[1mStep[0m  [32/169], [94mLoss[0m : 2.42467
[1mStep[0m  [48/169], [94mLoss[0m : 2.14700
[1mStep[0m  [64/169], [94mLoss[0m : 2.06835
[1mStep[0m  [80/169], [94mLoss[0m : 2.82498
[1mStep[0m  [96/169], [94mLoss[0m : 2.84205
[1mStep[0m  [112/169], [94mLoss[0m : 2.43611
[1mStep[0m  [128/169], [94mLoss[0m : 2.29048
[1mStep[0m  [144/169], [94mLoss[0m : 2.28013
[1mStep[0m  [160/169], [94mLoss[0m : 2.57336

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.334, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.88779
[1mStep[0m  [16/169], [94mLoss[0m : 1.94819
[1mStep[0m  [32/169], [94mLoss[0m : 2.64349
[1mStep[0m  [48/169], [94mLoss[0m : 2.33085
[1mStep[0m  [64/169], [94mLoss[0m : 2.52294
[1mStep[0m  [80/169], [94mLoss[0m : 2.71152
[1mStep[0m  [96/169], [94mLoss[0m : 2.24761
[1mStep[0m  [112/169], [94mLoss[0m : 2.42148
[1mStep[0m  [128/169], [94mLoss[0m : 2.83667
[1mStep[0m  [144/169], [94mLoss[0m : 2.35737
[1mStep[0m  [160/169], [94mLoss[0m : 2.52004

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.327, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.84405
[1mStep[0m  [16/169], [94mLoss[0m : 2.14870
[1mStep[0m  [32/169], [94mLoss[0m : 2.24998
[1mStep[0m  [48/169], [94mLoss[0m : 2.30335
[1mStep[0m  [64/169], [94mLoss[0m : 2.36818
[1mStep[0m  [80/169], [94mLoss[0m : 2.09683
[1mStep[0m  [96/169], [94mLoss[0m : 2.18197
[1mStep[0m  [112/169], [94mLoss[0m : 2.40661
[1mStep[0m  [128/169], [94mLoss[0m : 2.45855
[1mStep[0m  [144/169], [94mLoss[0m : 2.45232
[1mStep[0m  [160/169], [94mLoss[0m : 2.78718

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.329, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.46860
[1mStep[0m  [16/169], [94mLoss[0m : 2.57209
[1mStep[0m  [32/169], [94mLoss[0m : 2.57767
[1mStep[0m  [48/169], [94mLoss[0m : 2.51636
[1mStep[0m  [64/169], [94mLoss[0m : 2.53208
[1mStep[0m  [80/169], [94mLoss[0m : 2.61347
[1mStep[0m  [96/169], [94mLoss[0m : 2.59350
[1mStep[0m  [112/169], [94mLoss[0m : 2.21424
[1mStep[0m  [128/169], [94mLoss[0m : 2.27453
[1mStep[0m  [144/169], [94mLoss[0m : 2.59003
[1mStep[0m  [160/169], [94mLoss[0m : 2.52383

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.331, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.38789
[1mStep[0m  [16/169], [94mLoss[0m : 2.18886
[1mStep[0m  [32/169], [94mLoss[0m : 2.20031
[1mStep[0m  [48/169], [94mLoss[0m : 2.45462
[1mStep[0m  [64/169], [94mLoss[0m : 2.52500
[1mStep[0m  [80/169], [94mLoss[0m : 2.57756
[1mStep[0m  [96/169], [94mLoss[0m : 2.67333
[1mStep[0m  [112/169], [94mLoss[0m : 2.33177
[1mStep[0m  [128/169], [94mLoss[0m : 2.59912
[1mStep[0m  [144/169], [94mLoss[0m : 2.58537
[1mStep[0m  [160/169], [94mLoss[0m : 2.43991

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.334, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.44897
[1mStep[0m  [16/169], [94mLoss[0m : 3.07358
[1mStep[0m  [32/169], [94mLoss[0m : 2.62564
[1mStep[0m  [48/169], [94mLoss[0m : 2.52160
[1mStep[0m  [64/169], [94mLoss[0m : 2.95831
[1mStep[0m  [80/169], [94mLoss[0m : 2.86414
[1mStep[0m  [96/169], [94mLoss[0m : 2.63012
[1mStep[0m  [112/169], [94mLoss[0m : 2.32673
[1mStep[0m  [128/169], [94mLoss[0m : 2.31760
[1mStep[0m  [144/169], [94mLoss[0m : 2.55806
[1mStep[0m  [160/169], [94mLoss[0m : 2.59022

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.342, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.41768
[1mStep[0m  [16/169], [94mLoss[0m : 2.26281
[1mStep[0m  [32/169], [94mLoss[0m : 2.24155
[1mStep[0m  [48/169], [94mLoss[0m : 2.10181
[1mStep[0m  [64/169], [94mLoss[0m : 3.11344
[1mStep[0m  [80/169], [94mLoss[0m : 2.37760
[1mStep[0m  [96/169], [94mLoss[0m : 2.73759
[1mStep[0m  [112/169], [94mLoss[0m : 2.62924
[1mStep[0m  [128/169], [94mLoss[0m : 2.25964
[1mStep[0m  [144/169], [94mLoss[0m : 2.38041
[1mStep[0m  [160/169], [94mLoss[0m : 2.22150

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.323, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.81820
[1mStep[0m  [16/169], [94mLoss[0m : 2.50193
[1mStep[0m  [32/169], [94mLoss[0m : 2.51572
[1mStep[0m  [48/169], [94mLoss[0m : 2.54325
[1mStep[0m  [64/169], [94mLoss[0m : 2.44470
[1mStep[0m  [80/169], [94mLoss[0m : 2.23628
[1mStep[0m  [96/169], [94mLoss[0m : 3.08321
[1mStep[0m  [112/169], [94mLoss[0m : 2.47858
[1mStep[0m  [128/169], [94mLoss[0m : 2.32421
[1mStep[0m  [144/169], [94mLoss[0m : 2.34310
[1mStep[0m  [160/169], [94mLoss[0m : 2.62442

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.325, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.61822
[1mStep[0m  [16/169], [94mLoss[0m : 2.84078
[1mStep[0m  [32/169], [94mLoss[0m : 2.20054
[1mStep[0m  [48/169], [94mLoss[0m : 2.37043
[1mStep[0m  [64/169], [94mLoss[0m : 2.57305
[1mStep[0m  [80/169], [94mLoss[0m : 2.23324
[1mStep[0m  [96/169], [94mLoss[0m : 2.33833
[1mStep[0m  [112/169], [94mLoss[0m : 2.16830
[1mStep[0m  [128/169], [94mLoss[0m : 2.74820
[1mStep[0m  [144/169], [94mLoss[0m : 2.39822
[1mStep[0m  [160/169], [94mLoss[0m : 2.54846

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.337, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.31155
[1mStep[0m  [16/169], [94mLoss[0m : 2.12040
[1mStep[0m  [32/169], [94mLoss[0m : 2.41440
[1mStep[0m  [48/169], [94mLoss[0m : 2.67222
[1mStep[0m  [64/169], [94mLoss[0m : 2.57683
[1mStep[0m  [80/169], [94mLoss[0m : 2.05629
[1mStep[0m  [96/169], [94mLoss[0m : 2.36645
[1mStep[0m  [112/169], [94mLoss[0m : 2.61033
[1mStep[0m  [128/169], [94mLoss[0m : 2.49761
[1mStep[0m  [144/169], [94mLoss[0m : 2.35110
[1mStep[0m  [160/169], [94mLoss[0m : 2.52357

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.328, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.16283
[1mStep[0m  [16/169], [94mLoss[0m : 2.73257
[1mStep[0m  [32/169], [94mLoss[0m : 2.38263
[1mStep[0m  [48/169], [94mLoss[0m : 2.55242
[1mStep[0m  [64/169], [94mLoss[0m : 2.46249
[1mStep[0m  [80/169], [94mLoss[0m : 2.17083
[1mStep[0m  [96/169], [94mLoss[0m : 2.38670
[1mStep[0m  [112/169], [94mLoss[0m : 2.35900
[1mStep[0m  [128/169], [94mLoss[0m : 2.54651
[1mStep[0m  [144/169], [94mLoss[0m : 2.55580
[1mStep[0m  [160/169], [94mLoss[0m : 2.98814

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.322, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.20878
[1mStep[0m  [16/169], [94mLoss[0m : 2.04688
[1mStep[0m  [32/169], [94mLoss[0m : 2.63216
[1mStep[0m  [48/169], [94mLoss[0m : 2.15834
[1mStep[0m  [64/169], [94mLoss[0m : 2.23340
[1mStep[0m  [80/169], [94mLoss[0m : 2.96579
[1mStep[0m  [96/169], [94mLoss[0m : 2.14328
[1mStep[0m  [112/169], [94mLoss[0m : 2.71441
[1mStep[0m  [128/169], [94mLoss[0m : 2.19376
[1mStep[0m  [144/169], [94mLoss[0m : 2.70221
[1mStep[0m  [160/169], [94mLoss[0m : 2.59523

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.329, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.28529
[1mStep[0m  [16/169], [94mLoss[0m : 1.96794
[1mStep[0m  [32/169], [94mLoss[0m : 2.23181
[1mStep[0m  [48/169], [94mLoss[0m : 2.08794
[1mStep[0m  [64/169], [94mLoss[0m : 2.66981
[1mStep[0m  [80/169], [94mLoss[0m : 2.65550
[1mStep[0m  [96/169], [94mLoss[0m : 2.17367
[1mStep[0m  [112/169], [94mLoss[0m : 2.35910
[1mStep[0m  [128/169], [94mLoss[0m : 3.00054
[1mStep[0m  [144/169], [94mLoss[0m : 2.41184
[1mStep[0m  [160/169], [94mLoss[0m : 2.67674

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.326, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.36045
[1mStep[0m  [16/169], [94mLoss[0m : 2.38537
[1mStep[0m  [32/169], [94mLoss[0m : 2.29780
[1mStep[0m  [48/169], [94mLoss[0m : 2.20039
[1mStep[0m  [64/169], [94mLoss[0m : 2.49417
[1mStep[0m  [80/169], [94mLoss[0m : 2.48636
[1mStep[0m  [96/169], [94mLoss[0m : 2.16747
[1mStep[0m  [112/169], [94mLoss[0m : 2.55757
[1mStep[0m  [128/169], [94mLoss[0m : 2.90224
[1mStep[0m  [144/169], [94mLoss[0m : 2.26423
[1mStep[0m  [160/169], [94mLoss[0m : 2.65294

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.344, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.73247
[1mStep[0m  [16/169], [94mLoss[0m : 2.27850
[1mStep[0m  [32/169], [94mLoss[0m : 2.59894
[1mStep[0m  [48/169], [94mLoss[0m : 2.85667
[1mStep[0m  [64/169], [94mLoss[0m : 2.64644
[1mStep[0m  [80/169], [94mLoss[0m : 2.31495
[1mStep[0m  [96/169], [94mLoss[0m : 2.15162
[1mStep[0m  [112/169], [94mLoss[0m : 2.14455
[1mStep[0m  [128/169], [94mLoss[0m : 2.29482
[1mStep[0m  [144/169], [94mLoss[0m : 2.13750
[1mStep[0m  [160/169], [94mLoss[0m : 3.01360

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.310, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.29452
[1mStep[0m  [16/169], [94mLoss[0m : 1.98005
[1mStep[0m  [32/169], [94mLoss[0m : 2.50106
[1mStep[0m  [48/169], [94mLoss[0m : 2.37933
[1mStep[0m  [64/169], [94mLoss[0m : 2.11849
[1mStep[0m  [80/169], [94mLoss[0m : 2.49913
[1mStep[0m  [96/169], [94mLoss[0m : 2.67699
[1mStep[0m  [112/169], [94mLoss[0m : 2.66604
[1mStep[0m  [128/169], [94mLoss[0m : 2.07545
[1mStep[0m  [144/169], [94mLoss[0m : 2.16778
[1mStep[0m  [160/169], [94mLoss[0m : 2.34598

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.327, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.26909
[1mStep[0m  [16/169], [94mLoss[0m : 2.44570
[1mStep[0m  [32/169], [94mLoss[0m : 2.32877
[1mStep[0m  [48/169], [94mLoss[0m : 2.32540
[1mStep[0m  [64/169], [94mLoss[0m : 2.05186
[1mStep[0m  [80/169], [94mLoss[0m : 2.15417
[1mStep[0m  [96/169], [94mLoss[0m : 2.15041
[1mStep[0m  [112/169], [94mLoss[0m : 2.29869
[1mStep[0m  [128/169], [94mLoss[0m : 2.32039
[1mStep[0m  [144/169], [94mLoss[0m : 2.25692
[1mStep[0m  [160/169], [94mLoss[0m : 2.12067

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.327, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.33904
[1mStep[0m  [16/169], [94mLoss[0m : 2.47702
[1mStep[0m  [32/169], [94mLoss[0m : 2.94014
[1mStep[0m  [48/169], [94mLoss[0m : 2.75401
[1mStep[0m  [64/169], [94mLoss[0m : 2.46111
[1mStep[0m  [80/169], [94mLoss[0m : 2.64259
[1mStep[0m  [96/169], [94mLoss[0m : 2.44889
[1mStep[0m  [112/169], [94mLoss[0m : 2.30818
[1mStep[0m  [128/169], [94mLoss[0m : 2.35717
[1mStep[0m  [144/169], [94mLoss[0m : 2.50941
[1mStep[0m  [160/169], [94mLoss[0m : 2.34388

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.326, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.80556
[1mStep[0m  [16/169], [94mLoss[0m : 2.21497
[1mStep[0m  [32/169], [94mLoss[0m : 2.40100
[1mStep[0m  [48/169], [94mLoss[0m : 2.37908
[1mStep[0m  [64/169], [94mLoss[0m : 2.75536
[1mStep[0m  [80/169], [94mLoss[0m : 2.84845
[1mStep[0m  [96/169], [94mLoss[0m : 2.34414
[1mStep[0m  [112/169], [94mLoss[0m : 2.43974
[1mStep[0m  [128/169], [94mLoss[0m : 2.68780
[1mStep[0m  [144/169], [94mLoss[0m : 2.47373
[1mStep[0m  [160/169], [94mLoss[0m : 1.95202

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.323, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.71050
[1mStep[0m  [16/169], [94mLoss[0m : 2.44798
[1mStep[0m  [32/169], [94mLoss[0m : 2.65025
[1mStep[0m  [48/169], [94mLoss[0m : 2.37571
[1mStep[0m  [64/169], [94mLoss[0m : 2.48335
[1mStep[0m  [80/169], [94mLoss[0m : 2.33953
[1mStep[0m  [96/169], [94mLoss[0m : 2.39921
[1mStep[0m  [112/169], [94mLoss[0m : 2.54425
[1mStep[0m  [128/169], [94mLoss[0m : 2.04292
[1mStep[0m  [144/169], [94mLoss[0m : 2.32098
[1mStep[0m  [160/169], [94mLoss[0m : 2.56837

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.331, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.40064
[1mStep[0m  [16/169], [94mLoss[0m : 2.54820
[1mStep[0m  [32/169], [94mLoss[0m : 2.00257
[1mStep[0m  [48/169], [94mLoss[0m : 1.93340
[1mStep[0m  [64/169], [94mLoss[0m : 2.92476
[1mStep[0m  [80/169], [94mLoss[0m : 2.47421
[1mStep[0m  [96/169], [94mLoss[0m : 2.28645
[1mStep[0m  [112/169], [94mLoss[0m : 2.03716
[1mStep[0m  [128/169], [94mLoss[0m : 2.76805
[1mStep[0m  [144/169], [94mLoss[0m : 2.71836
[1mStep[0m  [160/169], [94mLoss[0m : 2.42260

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.325, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.343
====================================

Phase 1 - Evaluation MAE:  2.3425885758229663
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/169], [94mLoss[0m : 2.60379
[1mStep[0m  [16/169], [94mLoss[0m : 2.45616
[1mStep[0m  [32/169], [94mLoss[0m : 2.26855
[1mStep[0m  [48/169], [94mLoss[0m : 2.58092
[1mStep[0m  [64/169], [94mLoss[0m : 2.59695
[1mStep[0m  [80/169], [94mLoss[0m : 2.10025
[1mStep[0m  [96/169], [94mLoss[0m : 2.50011
[1mStep[0m  [112/169], [94mLoss[0m : 2.86868
[1mStep[0m  [128/169], [94mLoss[0m : 2.23638
[1mStep[0m  [144/169], [94mLoss[0m : 2.98436
[1mStep[0m  [160/169], [94mLoss[0m : 2.57449

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.342, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.18542
[1mStep[0m  [16/169], [94mLoss[0m : 2.30613
[1mStep[0m  [32/169], [94mLoss[0m : 2.13427
[1mStep[0m  [48/169], [94mLoss[0m : 2.37964
[1mStep[0m  [64/169], [94mLoss[0m : 2.86571
[1mStep[0m  [80/169], [94mLoss[0m : 2.25249
[1mStep[0m  [96/169], [94mLoss[0m : 2.23842
[1mStep[0m  [112/169], [94mLoss[0m : 2.50052
[1mStep[0m  [128/169], [94mLoss[0m : 2.71761
[1mStep[0m  [144/169], [94mLoss[0m : 2.86085
[1mStep[0m  [160/169], [94mLoss[0m : 2.54671

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.363, [92mTest[0m: 2.465, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.51664
[1mStep[0m  [16/169], [94mLoss[0m : 1.92939
[1mStep[0m  [32/169], [94mLoss[0m : 2.41394
[1mStep[0m  [48/169], [94mLoss[0m : 2.25655
[1mStep[0m  [64/169], [94mLoss[0m : 2.44853
[1mStep[0m  [80/169], [94mLoss[0m : 2.17548
[1mStep[0m  [96/169], [94mLoss[0m : 1.74442
[1mStep[0m  [112/169], [94mLoss[0m : 1.99334
[1mStep[0m  [128/169], [94mLoss[0m : 2.10375
[1mStep[0m  [144/169], [94mLoss[0m : 1.89752
[1mStep[0m  [160/169], [94mLoss[0m : 2.23347

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.265, [92mTest[0m: 2.421, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.33899
[1mStep[0m  [16/169], [94mLoss[0m : 2.59030
[1mStep[0m  [32/169], [94mLoss[0m : 1.97151
[1mStep[0m  [48/169], [94mLoss[0m : 2.37548
[1mStep[0m  [64/169], [94mLoss[0m : 2.12853
[1mStep[0m  [80/169], [94mLoss[0m : 2.32233
[1mStep[0m  [96/169], [94mLoss[0m : 2.03961
[1mStep[0m  [112/169], [94mLoss[0m : 2.47188
[1mStep[0m  [128/169], [94mLoss[0m : 2.38170
[1mStep[0m  [144/169], [94mLoss[0m : 2.18645
[1mStep[0m  [160/169], [94mLoss[0m : 2.37135

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.211, [92mTest[0m: 2.399, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.98210
[1mStep[0m  [16/169], [94mLoss[0m : 2.17595
[1mStep[0m  [32/169], [94mLoss[0m : 2.65119
[1mStep[0m  [48/169], [94mLoss[0m : 2.12804
[1mStep[0m  [64/169], [94mLoss[0m : 2.05353
[1mStep[0m  [80/169], [94mLoss[0m : 2.23393
[1mStep[0m  [96/169], [94mLoss[0m : 1.79748
[1mStep[0m  [112/169], [94mLoss[0m : 2.28728
[1mStep[0m  [128/169], [94mLoss[0m : 2.07778
[1mStep[0m  [144/169], [94mLoss[0m : 2.10420
[1mStep[0m  [160/169], [94mLoss[0m : 2.34159

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.172, [92mTest[0m: 2.372, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.34579
[1mStep[0m  [16/169], [94mLoss[0m : 2.29916
[1mStep[0m  [32/169], [94mLoss[0m : 2.10685
[1mStep[0m  [48/169], [94mLoss[0m : 2.39095
[1mStep[0m  [64/169], [94mLoss[0m : 2.01362
[1mStep[0m  [80/169], [94mLoss[0m : 2.09474
[1mStep[0m  [96/169], [94mLoss[0m : 2.09901
[1mStep[0m  [112/169], [94mLoss[0m : 2.09503
[1mStep[0m  [128/169], [94mLoss[0m : 2.09314
[1mStep[0m  [144/169], [94mLoss[0m : 1.91025
[1mStep[0m  [160/169], [94mLoss[0m : 1.93331

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.094, [92mTest[0m: 2.388, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.17792
[1mStep[0m  [16/169], [94mLoss[0m : 1.95123
[1mStep[0m  [32/169], [94mLoss[0m : 1.65736
[1mStep[0m  [48/169], [94mLoss[0m : 2.00762
[1mStep[0m  [64/169], [94mLoss[0m : 2.22898
[1mStep[0m  [80/169], [94mLoss[0m : 2.04502
[1mStep[0m  [96/169], [94mLoss[0m : 1.84318
[1mStep[0m  [112/169], [94mLoss[0m : 1.90600
[1mStep[0m  [128/169], [94mLoss[0m : 2.11510
[1mStep[0m  [144/169], [94mLoss[0m : 2.39535
[1mStep[0m  [160/169], [94mLoss[0m : 1.87593

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.025, [92mTest[0m: 2.425, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.17035
[1mStep[0m  [16/169], [94mLoss[0m : 1.73058
[1mStep[0m  [32/169], [94mLoss[0m : 1.96040
[1mStep[0m  [48/169], [94mLoss[0m : 2.26392
[1mStep[0m  [64/169], [94mLoss[0m : 1.68940
[1mStep[0m  [80/169], [94mLoss[0m : 1.68540
[1mStep[0m  [96/169], [94mLoss[0m : 1.97792
[1mStep[0m  [112/169], [94mLoss[0m : 2.01719
[1mStep[0m  [128/169], [94mLoss[0m : 2.02449
[1mStep[0m  [144/169], [94mLoss[0m : 2.31421
[1mStep[0m  [160/169], [94mLoss[0m : 1.73060

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.993, [92mTest[0m: 2.399, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.00484
[1mStep[0m  [16/169], [94mLoss[0m : 1.72529
[1mStep[0m  [32/169], [94mLoss[0m : 1.86454
[1mStep[0m  [48/169], [94mLoss[0m : 1.98879
[1mStep[0m  [64/169], [94mLoss[0m : 1.67770
[1mStep[0m  [80/169], [94mLoss[0m : 1.71321
[1mStep[0m  [96/169], [94mLoss[0m : 1.63260
[1mStep[0m  [112/169], [94mLoss[0m : 1.91783
[1mStep[0m  [128/169], [94mLoss[0m : 1.92788
[1mStep[0m  [144/169], [94mLoss[0m : 2.11800
[1mStep[0m  [160/169], [94mLoss[0m : 2.04035

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.943, [92mTest[0m: 2.427, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.79913
[1mStep[0m  [16/169], [94mLoss[0m : 1.63662
[1mStep[0m  [32/169], [94mLoss[0m : 1.87123
[1mStep[0m  [48/169], [94mLoss[0m : 1.85993
[1mStep[0m  [64/169], [94mLoss[0m : 1.95013
[1mStep[0m  [80/169], [94mLoss[0m : 1.92079
[1mStep[0m  [96/169], [94mLoss[0m : 1.64993
[1mStep[0m  [112/169], [94mLoss[0m : 2.04173
[1mStep[0m  [128/169], [94mLoss[0m : 1.53219
[1mStep[0m  [144/169], [94mLoss[0m : 1.83123
[1mStep[0m  [160/169], [94mLoss[0m : 1.96590

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.909, [92mTest[0m: 2.477, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.87245
[1mStep[0m  [16/169], [94mLoss[0m : 1.96345
[1mStep[0m  [32/169], [94mLoss[0m : 1.68545
[1mStep[0m  [48/169], [94mLoss[0m : 1.65563
[1mStep[0m  [64/169], [94mLoss[0m : 1.92319
[1mStep[0m  [80/169], [94mLoss[0m : 1.77272
[1mStep[0m  [96/169], [94mLoss[0m : 2.03428
[1mStep[0m  [112/169], [94mLoss[0m : 2.01711
[1mStep[0m  [128/169], [94mLoss[0m : 1.99882
[1mStep[0m  [144/169], [94mLoss[0m : 1.95581
[1mStep[0m  [160/169], [94mLoss[0m : 2.23536

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.863, [92mTest[0m: 2.457, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.84572
[1mStep[0m  [16/169], [94mLoss[0m : 1.44201
[1mStep[0m  [32/169], [94mLoss[0m : 1.38457
[1mStep[0m  [48/169], [94mLoss[0m : 1.68879
[1mStep[0m  [64/169], [94mLoss[0m : 1.58623
[1mStep[0m  [80/169], [94mLoss[0m : 1.79295
[1mStep[0m  [96/169], [94mLoss[0m : 2.22034
[1mStep[0m  [112/169], [94mLoss[0m : 1.55425
[1mStep[0m  [128/169], [94mLoss[0m : 1.69640
[1mStep[0m  [144/169], [94mLoss[0m : 1.98478
[1mStep[0m  [160/169], [94mLoss[0m : 1.79771

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.795, [92mTest[0m: 2.509, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.65776
[1mStep[0m  [16/169], [94mLoss[0m : 1.78066
[1mStep[0m  [32/169], [94mLoss[0m : 1.82007
[1mStep[0m  [48/169], [94mLoss[0m : 1.76114
[1mStep[0m  [64/169], [94mLoss[0m : 1.74070
[1mStep[0m  [80/169], [94mLoss[0m : 1.98674
[1mStep[0m  [96/169], [94mLoss[0m : 2.41202
[1mStep[0m  [112/169], [94mLoss[0m : 1.83232
[1mStep[0m  [128/169], [94mLoss[0m : 1.63963
[1mStep[0m  [144/169], [94mLoss[0m : 1.63845
[1mStep[0m  [160/169], [94mLoss[0m : 1.71398

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.785, [92mTest[0m: 2.510, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.66827
[1mStep[0m  [16/169], [94mLoss[0m : 1.63867
[1mStep[0m  [32/169], [94mLoss[0m : 1.78820
[1mStep[0m  [48/169], [94mLoss[0m : 1.41101
[1mStep[0m  [64/169], [94mLoss[0m : 2.21531
[1mStep[0m  [80/169], [94mLoss[0m : 1.62781
[1mStep[0m  [96/169], [94mLoss[0m : 1.37579
[1mStep[0m  [112/169], [94mLoss[0m : 1.77675
[1mStep[0m  [128/169], [94mLoss[0m : 1.78177
[1mStep[0m  [144/169], [94mLoss[0m : 1.64673
[1mStep[0m  [160/169], [94mLoss[0m : 2.01503

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.744, [92mTest[0m: 2.476, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.61541
[1mStep[0m  [16/169], [94mLoss[0m : 1.51219
[1mStep[0m  [32/169], [94mLoss[0m : 1.73148
[1mStep[0m  [48/169], [94mLoss[0m : 1.55080
[1mStep[0m  [64/169], [94mLoss[0m : 1.54741
[1mStep[0m  [80/169], [94mLoss[0m : 1.49904
[1mStep[0m  [96/169], [94mLoss[0m : 1.53397
[1mStep[0m  [112/169], [94mLoss[0m : 1.73385
[1mStep[0m  [128/169], [94mLoss[0m : 1.74058
[1mStep[0m  [144/169], [94mLoss[0m : 1.72283
[1mStep[0m  [160/169], [94mLoss[0m : 1.66975

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.695, [92mTest[0m: 2.510, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.50048
[1mStep[0m  [16/169], [94mLoss[0m : 1.91053
[1mStep[0m  [32/169], [94mLoss[0m : 1.34285
[1mStep[0m  [48/169], [94mLoss[0m : 1.51535
[1mStep[0m  [64/169], [94mLoss[0m : 1.93803
[1mStep[0m  [80/169], [94mLoss[0m : 1.87726
[1mStep[0m  [96/169], [94mLoss[0m : 1.58650
[1mStep[0m  [112/169], [94mLoss[0m : 1.61335
[1mStep[0m  [128/169], [94mLoss[0m : 1.26475
[1mStep[0m  [144/169], [94mLoss[0m : 1.69348
[1mStep[0m  [160/169], [94mLoss[0m : 1.64422

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.677, [92mTest[0m: 2.507, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.95176
[1mStep[0m  [16/169], [94mLoss[0m : 1.59502
[1mStep[0m  [32/169], [94mLoss[0m : 1.65746
[1mStep[0m  [48/169], [94mLoss[0m : 1.73927
[1mStep[0m  [64/169], [94mLoss[0m : 1.71899
[1mStep[0m  [80/169], [94mLoss[0m : 1.63975
[1mStep[0m  [96/169], [94mLoss[0m : 1.56355
[1mStep[0m  [112/169], [94mLoss[0m : 1.53681
[1mStep[0m  [128/169], [94mLoss[0m : 1.87702
[1mStep[0m  [144/169], [94mLoss[0m : 1.76869
[1mStep[0m  [160/169], [94mLoss[0m : 2.11071

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.669, [92mTest[0m: 2.531, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.64474
[1mStep[0m  [16/169], [94mLoss[0m : 1.73466
[1mStep[0m  [32/169], [94mLoss[0m : 1.31329
[1mStep[0m  [48/169], [94mLoss[0m : 1.62499
[1mStep[0m  [64/169], [94mLoss[0m : 1.55812
[1mStep[0m  [80/169], [94mLoss[0m : 1.51183
[1mStep[0m  [96/169], [94mLoss[0m : 1.30026
[1mStep[0m  [112/169], [94mLoss[0m : 1.99626
[1mStep[0m  [128/169], [94mLoss[0m : 1.71675
[1mStep[0m  [144/169], [94mLoss[0m : 1.85271
[1mStep[0m  [160/169], [94mLoss[0m : 1.43598

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.631, [92mTest[0m: 2.513, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.98986
[1mStep[0m  [16/169], [94mLoss[0m : 1.50840
[1mStep[0m  [32/169], [94mLoss[0m : 1.40315
[1mStep[0m  [48/169], [94mLoss[0m : 1.41892
[1mStep[0m  [64/169], [94mLoss[0m : 1.43974
[1mStep[0m  [80/169], [94mLoss[0m : 1.61519
[1mStep[0m  [96/169], [94mLoss[0m : 1.50034
[1mStep[0m  [112/169], [94mLoss[0m : 1.50730
[1mStep[0m  [128/169], [94mLoss[0m : 1.85479
[1mStep[0m  [144/169], [94mLoss[0m : 1.81294
[1mStep[0m  [160/169], [94mLoss[0m : 1.52445

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.589, [92mTest[0m: 2.543, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.65856
[1mStep[0m  [16/169], [94mLoss[0m : 1.35598
[1mStep[0m  [32/169], [94mLoss[0m : 1.16776
[1mStep[0m  [48/169], [94mLoss[0m : 1.74687
[1mStep[0m  [64/169], [94mLoss[0m : 1.76035
[1mStep[0m  [80/169], [94mLoss[0m : 1.46200
[1mStep[0m  [96/169], [94mLoss[0m : 1.52309
[1mStep[0m  [112/169], [94mLoss[0m : 1.61648
[1mStep[0m  [128/169], [94mLoss[0m : 1.59746
[1mStep[0m  [144/169], [94mLoss[0m : 1.53073
[1mStep[0m  [160/169], [94mLoss[0m : 1.40607

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.551, [92mTest[0m: 2.530, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.95105
[1mStep[0m  [16/169], [94mLoss[0m : 1.13454
[1mStep[0m  [32/169], [94mLoss[0m : 1.57010
[1mStep[0m  [48/169], [94mLoss[0m : 1.69377
[1mStep[0m  [64/169], [94mLoss[0m : 1.45526
[1mStep[0m  [80/169], [94mLoss[0m : 1.54046
[1mStep[0m  [96/169], [94mLoss[0m : 1.51216
[1mStep[0m  [112/169], [94mLoss[0m : 1.69004
[1mStep[0m  [128/169], [94mLoss[0m : 1.51214
[1mStep[0m  [144/169], [94mLoss[0m : 1.24464
[1mStep[0m  [160/169], [94mLoss[0m : 1.52373

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.530, [92mTest[0m: 2.531, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.79520
[1mStep[0m  [16/169], [94mLoss[0m : 1.42385
[1mStep[0m  [32/169], [94mLoss[0m : 1.56598
[1mStep[0m  [48/169], [94mLoss[0m : 1.36247
[1mStep[0m  [64/169], [94mLoss[0m : 1.51058
[1mStep[0m  [80/169], [94mLoss[0m : 1.24978
[1mStep[0m  [96/169], [94mLoss[0m : 1.42982
[1mStep[0m  [112/169], [94mLoss[0m : 1.45773
[1mStep[0m  [128/169], [94mLoss[0m : 1.41039
[1mStep[0m  [144/169], [94mLoss[0m : 1.51479
[1mStep[0m  [160/169], [94mLoss[0m : 1.46151

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.483, [92mTest[0m: 2.450, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.04723
[1mStep[0m  [16/169], [94mLoss[0m : 1.28706
[1mStep[0m  [32/169], [94mLoss[0m : 1.32406
[1mStep[0m  [48/169], [94mLoss[0m : 1.57627
[1mStep[0m  [64/169], [94mLoss[0m : 1.30896
[1mStep[0m  [80/169], [94mLoss[0m : 1.36782
[1mStep[0m  [96/169], [94mLoss[0m : 1.32146
[1mStep[0m  [112/169], [94mLoss[0m : 1.38222
[1mStep[0m  [128/169], [94mLoss[0m : 1.36850
[1mStep[0m  [144/169], [94mLoss[0m : 1.62119
[1mStep[0m  [160/169], [94mLoss[0m : 1.30903

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.452, [92mTest[0m: 2.518, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.69002
[1mStep[0m  [16/169], [94mLoss[0m : 1.43833
[1mStep[0m  [32/169], [94mLoss[0m : 1.27705
[1mStep[0m  [48/169], [94mLoss[0m : 1.27423
[1mStep[0m  [64/169], [94mLoss[0m : 1.58996
[1mStep[0m  [80/169], [94mLoss[0m : 1.36938
[1mStep[0m  [96/169], [94mLoss[0m : 1.52810
[1mStep[0m  [112/169], [94mLoss[0m : 1.64855
[1mStep[0m  [128/169], [94mLoss[0m : 1.55684
[1mStep[0m  [144/169], [94mLoss[0m : 1.28245
[1mStep[0m  [160/169], [94mLoss[0m : 1.17604

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.450, [92mTest[0m: 2.473, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.71177
[1mStep[0m  [16/169], [94mLoss[0m : 1.72501
[1mStep[0m  [32/169], [94mLoss[0m : 1.31439
[1mStep[0m  [48/169], [94mLoss[0m : 1.29442
[1mStep[0m  [64/169], [94mLoss[0m : 1.32092
[1mStep[0m  [80/169], [94mLoss[0m : 1.39932
[1mStep[0m  [96/169], [94mLoss[0m : 1.02633
[1mStep[0m  [112/169], [94mLoss[0m : 1.50366
[1mStep[0m  [128/169], [94mLoss[0m : 1.43245
[1mStep[0m  [144/169], [94mLoss[0m : 1.33750
[1mStep[0m  [160/169], [94mLoss[0m : 1.70004

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.445, [92mTest[0m: 2.494, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.66925
[1mStep[0m  [16/169], [94mLoss[0m : 1.47237
[1mStep[0m  [32/169], [94mLoss[0m : 1.22377
[1mStep[0m  [48/169], [94mLoss[0m : 1.67414
[1mStep[0m  [64/169], [94mLoss[0m : 1.37371
[1mStep[0m  [80/169], [94mLoss[0m : 1.58490
[1mStep[0m  [96/169], [94mLoss[0m : 1.79777
[1mStep[0m  [112/169], [94mLoss[0m : 1.29391
[1mStep[0m  [128/169], [94mLoss[0m : 1.26458
[1mStep[0m  [144/169], [94mLoss[0m : 1.30175
[1mStep[0m  [160/169], [94mLoss[0m : 1.39945

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.413, [92mTest[0m: 2.480, [96mlr[0m: 0.004545
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 25 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.493
====================================

Phase 2 - Evaluation MAE:  2.492959384407316
MAE score P1        2.342589
MAE score P2        2.492959
loss                1.413268
learning_rate        0.00505
batch_size                64
hidden_sizes      [300, 100]
epochs                    30
activation           sigmoid
optimizer                sgd
early stopping          True
dropout                  0.3
momentum                 0.9
weight_decay          0.0001
Name: 4, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/169], [94mLoss[0m : 10.96907
[1mStep[0m  [16/169], [94mLoss[0m : 7.22059
[1mStep[0m  [32/169], [94mLoss[0m : 3.83241
[1mStep[0m  [48/169], [94mLoss[0m : 2.81283
[1mStep[0m  [64/169], [94mLoss[0m : 2.77511
[1mStep[0m  [80/169], [94mLoss[0m : 3.17972
[1mStep[0m  [96/169], [94mLoss[0m : 2.85222
[1mStep[0m  [112/169], [94mLoss[0m : 2.55423
[1mStep[0m  [128/169], [94mLoss[0m : 2.88185
[1mStep[0m  [144/169], [94mLoss[0m : 2.45689
[1mStep[0m  [160/169], [94mLoss[0m : 2.80660

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.632, [92mTest[0m: 10.937, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.08368
[1mStep[0m  [16/169], [94mLoss[0m : 2.98058
[1mStep[0m  [32/169], [94mLoss[0m : 2.58915
[1mStep[0m  [48/169], [94mLoss[0m : 2.38087
[1mStep[0m  [64/169], [94mLoss[0m : 2.82625
[1mStep[0m  [80/169], [94mLoss[0m : 2.20895
[1mStep[0m  [96/169], [94mLoss[0m : 2.66545
[1mStep[0m  [112/169], [94mLoss[0m : 2.26357
[1mStep[0m  [128/169], [94mLoss[0m : 2.87558
[1mStep[0m  [144/169], [94mLoss[0m : 2.63643
[1mStep[0m  [160/169], [94mLoss[0m : 2.35139

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.579, [92mTest[0m: 2.603, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.26594
[1mStep[0m  [16/169], [94mLoss[0m : 2.42759
[1mStep[0m  [32/169], [94mLoss[0m : 2.87469
[1mStep[0m  [48/169], [94mLoss[0m : 2.45805
[1mStep[0m  [64/169], [94mLoss[0m : 2.81319
[1mStep[0m  [80/169], [94mLoss[0m : 2.38388
[1mStep[0m  [96/169], [94mLoss[0m : 2.25196
[1mStep[0m  [112/169], [94mLoss[0m : 2.65446
[1mStep[0m  [128/169], [94mLoss[0m : 2.72976
[1mStep[0m  [144/169], [94mLoss[0m : 2.68060
[1mStep[0m  [160/169], [94mLoss[0m : 2.49582

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.546, [92mTest[0m: 2.527, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.37016
[1mStep[0m  [16/169], [94mLoss[0m : 2.31333
[1mStep[0m  [32/169], [94mLoss[0m : 2.83255
[1mStep[0m  [48/169], [94mLoss[0m : 2.72066
[1mStep[0m  [64/169], [94mLoss[0m : 2.47964
[1mStep[0m  [80/169], [94mLoss[0m : 2.32225
[1mStep[0m  [96/169], [94mLoss[0m : 2.50667
[1mStep[0m  [112/169], [94mLoss[0m : 2.80254
[1mStep[0m  [128/169], [94mLoss[0m : 2.45553
[1mStep[0m  [144/169], [94mLoss[0m : 2.24245
[1mStep[0m  [160/169], [94mLoss[0m : 2.74224

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.535, [92mTest[0m: 2.512, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.17144
[1mStep[0m  [16/169], [94mLoss[0m : 2.43710
[1mStep[0m  [32/169], [94mLoss[0m : 2.58844
[1mStep[0m  [48/169], [94mLoss[0m : 2.69094
[1mStep[0m  [64/169], [94mLoss[0m : 2.50693
[1mStep[0m  [80/169], [94mLoss[0m : 2.31711
[1mStep[0m  [96/169], [94mLoss[0m : 2.40279
[1mStep[0m  [112/169], [94mLoss[0m : 2.43671
[1mStep[0m  [128/169], [94mLoss[0m : 2.27997
[1mStep[0m  [144/169], [94mLoss[0m : 2.98027
[1mStep[0m  [160/169], [94mLoss[0m : 2.65326

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.535, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.34470
[1mStep[0m  [16/169], [94mLoss[0m : 2.64017
[1mStep[0m  [32/169], [94mLoss[0m : 2.08595
[1mStep[0m  [48/169], [94mLoss[0m : 2.61803
[1mStep[0m  [64/169], [94mLoss[0m : 2.79178
[1mStep[0m  [80/169], [94mLoss[0m : 2.74264
[1mStep[0m  [96/169], [94mLoss[0m : 2.74794
[1mStep[0m  [112/169], [94mLoss[0m : 2.25062
[1mStep[0m  [128/169], [94mLoss[0m : 2.42229
[1mStep[0m  [144/169], [94mLoss[0m : 2.45457
[1mStep[0m  [160/169], [94mLoss[0m : 3.12809

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.486, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.28601
[1mStep[0m  [16/169], [94mLoss[0m : 2.16605
[1mStep[0m  [32/169], [94mLoss[0m : 2.46004
[1mStep[0m  [48/169], [94mLoss[0m : 2.20463
[1mStep[0m  [64/169], [94mLoss[0m : 2.04588
[1mStep[0m  [80/169], [94mLoss[0m : 2.19090
[1mStep[0m  [96/169], [94mLoss[0m : 2.26280
[1mStep[0m  [112/169], [94mLoss[0m : 2.66750
[1mStep[0m  [128/169], [94mLoss[0m : 2.47932
[1mStep[0m  [144/169], [94mLoss[0m : 2.48650
[1mStep[0m  [160/169], [94mLoss[0m : 2.82987

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.482, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.25754
[1mStep[0m  [16/169], [94mLoss[0m : 2.59960
[1mStep[0m  [32/169], [94mLoss[0m : 2.67633
[1mStep[0m  [48/169], [94mLoss[0m : 2.18777
[1mStep[0m  [64/169], [94mLoss[0m : 2.44250
[1mStep[0m  [80/169], [94mLoss[0m : 2.71641
[1mStep[0m  [96/169], [94mLoss[0m : 2.23150
[1mStep[0m  [112/169], [94mLoss[0m : 2.55727
[1mStep[0m  [128/169], [94mLoss[0m : 2.53447
[1mStep[0m  [144/169], [94mLoss[0m : 2.54142
[1mStep[0m  [160/169], [94mLoss[0m : 2.91204

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.466, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.01094
[1mStep[0m  [16/169], [94mLoss[0m : 2.74424
[1mStep[0m  [32/169], [94mLoss[0m : 2.38130
[1mStep[0m  [48/169], [94mLoss[0m : 1.94077
[1mStep[0m  [64/169], [94mLoss[0m : 2.57796
[1mStep[0m  [80/169], [94mLoss[0m : 2.36132
[1mStep[0m  [96/169], [94mLoss[0m : 2.06608
[1mStep[0m  [112/169], [94mLoss[0m : 2.55229
[1mStep[0m  [128/169], [94mLoss[0m : 2.36872
[1mStep[0m  [144/169], [94mLoss[0m : 2.42330
[1mStep[0m  [160/169], [94mLoss[0m : 2.44160

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.492, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.72078
[1mStep[0m  [16/169], [94mLoss[0m : 2.75378
[1mStep[0m  [32/169], [94mLoss[0m : 2.83430
[1mStep[0m  [48/169], [94mLoss[0m : 2.54288
[1mStep[0m  [64/169], [94mLoss[0m : 2.31651
[1mStep[0m  [80/169], [94mLoss[0m : 2.42816
[1mStep[0m  [96/169], [94mLoss[0m : 2.41332
[1mStep[0m  [112/169], [94mLoss[0m : 2.37224
[1mStep[0m  [128/169], [94mLoss[0m : 2.42584
[1mStep[0m  [144/169], [94mLoss[0m : 2.42189
[1mStep[0m  [160/169], [94mLoss[0m : 2.81500

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.454, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.76308
[1mStep[0m  [16/169], [94mLoss[0m : 2.14909
[1mStep[0m  [32/169], [94mLoss[0m : 2.42720
[1mStep[0m  [48/169], [94mLoss[0m : 2.88704
[1mStep[0m  [64/169], [94mLoss[0m : 2.43871
[1mStep[0m  [80/169], [94mLoss[0m : 1.84369
[1mStep[0m  [96/169], [94mLoss[0m : 2.29654
[1mStep[0m  [112/169], [94mLoss[0m : 2.04596
[1mStep[0m  [128/169], [94mLoss[0m : 2.35621
[1mStep[0m  [144/169], [94mLoss[0m : 2.34513
[1mStep[0m  [160/169], [94mLoss[0m : 2.55534

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.474, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.87704
[1mStep[0m  [16/169], [94mLoss[0m : 2.55130
[1mStep[0m  [32/169], [94mLoss[0m : 2.46270
[1mStep[0m  [48/169], [94mLoss[0m : 2.57373
[1mStep[0m  [64/169], [94mLoss[0m : 2.40476
[1mStep[0m  [80/169], [94mLoss[0m : 2.13042
[1mStep[0m  [96/169], [94mLoss[0m : 2.31513
[1mStep[0m  [112/169], [94mLoss[0m : 2.46179
[1mStep[0m  [128/169], [94mLoss[0m : 2.38222
[1mStep[0m  [144/169], [94mLoss[0m : 2.67745
[1mStep[0m  [160/169], [94mLoss[0m : 2.80919

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.434, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.25539
[1mStep[0m  [16/169], [94mLoss[0m : 2.54288
[1mStep[0m  [32/169], [94mLoss[0m : 2.18865
[1mStep[0m  [48/169], [94mLoss[0m : 2.97010
[1mStep[0m  [64/169], [94mLoss[0m : 2.58632
[1mStep[0m  [80/169], [94mLoss[0m : 2.29770
[1mStep[0m  [96/169], [94mLoss[0m : 2.69553
[1mStep[0m  [112/169], [94mLoss[0m : 2.22921
[1mStep[0m  [128/169], [94mLoss[0m : 2.05199
[1mStep[0m  [144/169], [94mLoss[0m : 2.68109
[1mStep[0m  [160/169], [94mLoss[0m : 2.41805

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.415, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.48274
[1mStep[0m  [16/169], [94mLoss[0m : 2.72025
[1mStep[0m  [32/169], [94mLoss[0m : 2.26571
[1mStep[0m  [48/169], [94mLoss[0m : 2.29863
[1mStep[0m  [64/169], [94mLoss[0m : 2.65884
[1mStep[0m  [80/169], [94mLoss[0m : 2.33057
[1mStep[0m  [96/169], [94mLoss[0m : 2.16282
[1mStep[0m  [112/169], [94mLoss[0m : 2.83580
[1mStep[0m  [128/169], [94mLoss[0m : 2.59395
[1mStep[0m  [144/169], [94mLoss[0m : 2.35587
[1mStep[0m  [160/169], [94mLoss[0m : 2.60649

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.463, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.54712
[1mStep[0m  [16/169], [94mLoss[0m : 2.40556
[1mStep[0m  [32/169], [94mLoss[0m : 2.71987
[1mStep[0m  [48/169], [94mLoss[0m : 3.05623
[1mStep[0m  [64/169], [94mLoss[0m : 2.28391
[1mStep[0m  [80/169], [94mLoss[0m : 2.27248
[1mStep[0m  [96/169], [94mLoss[0m : 2.71229
[1mStep[0m  [112/169], [94mLoss[0m : 2.54335
[1mStep[0m  [128/169], [94mLoss[0m : 2.37014
[1mStep[0m  [144/169], [94mLoss[0m : 2.35216
[1mStep[0m  [160/169], [94mLoss[0m : 2.59126

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.440, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.34875
[1mStep[0m  [16/169], [94mLoss[0m : 2.67994
[1mStep[0m  [32/169], [94mLoss[0m : 2.93779
[1mStep[0m  [48/169], [94mLoss[0m : 2.11285
[1mStep[0m  [64/169], [94mLoss[0m : 2.04692
[1mStep[0m  [80/169], [94mLoss[0m : 2.45230
[1mStep[0m  [96/169], [94mLoss[0m : 2.06249
[1mStep[0m  [112/169], [94mLoss[0m : 2.32005
[1mStep[0m  [128/169], [94mLoss[0m : 2.29968
[1mStep[0m  [144/169], [94mLoss[0m : 2.56703
[1mStep[0m  [160/169], [94mLoss[0m : 2.39247

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.514, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.17495
[1mStep[0m  [16/169], [94mLoss[0m : 2.76423
[1mStep[0m  [32/169], [94mLoss[0m : 2.30252
[1mStep[0m  [48/169], [94mLoss[0m : 2.57825
[1mStep[0m  [64/169], [94mLoss[0m : 2.45085
[1mStep[0m  [80/169], [94mLoss[0m : 2.06626
[1mStep[0m  [96/169], [94mLoss[0m : 2.77971
[1mStep[0m  [112/169], [94mLoss[0m : 2.52092
[1mStep[0m  [128/169], [94mLoss[0m : 2.85086
[1mStep[0m  [144/169], [94mLoss[0m : 2.10536
[1mStep[0m  [160/169], [94mLoss[0m : 2.81382

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.417, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.18542
[1mStep[0m  [16/169], [94mLoss[0m : 2.32648
[1mStep[0m  [32/169], [94mLoss[0m : 2.39568
[1mStep[0m  [48/169], [94mLoss[0m : 2.24292
[1mStep[0m  [64/169], [94mLoss[0m : 2.63966
[1mStep[0m  [80/169], [94mLoss[0m : 2.40602
[1mStep[0m  [96/169], [94mLoss[0m : 2.78466
[1mStep[0m  [112/169], [94mLoss[0m : 2.17384
[1mStep[0m  [128/169], [94mLoss[0m : 2.41256
[1mStep[0m  [144/169], [94mLoss[0m : 2.23570
[1mStep[0m  [160/169], [94mLoss[0m : 2.31685

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.409, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.50924
[1mStep[0m  [16/169], [94mLoss[0m : 2.34270
[1mStep[0m  [32/169], [94mLoss[0m : 2.54610
[1mStep[0m  [48/169], [94mLoss[0m : 2.37312
[1mStep[0m  [64/169], [94mLoss[0m : 2.46825
[1mStep[0m  [80/169], [94mLoss[0m : 2.26390
[1mStep[0m  [96/169], [94mLoss[0m : 2.54112
[1mStep[0m  [112/169], [94mLoss[0m : 2.40368
[1mStep[0m  [128/169], [94mLoss[0m : 2.40925
[1mStep[0m  [144/169], [94mLoss[0m : 2.19614
[1mStep[0m  [160/169], [94mLoss[0m : 2.51000

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.403, [92mTest[0m: 2.366, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.09575
[1mStep[0m  [16/169], [94mLoss[0m : 2.35680
[1mStep[0m  [32/169], [94mLoss[0m : 2.10848
[1mStep[0m  [48/169], [94mLoss[0m : 2.91738
[1mStep[0m  [64/169], [94mLoss[0m : 1.93194
[1mStep[0m  [80/169], [94mLoss[0m : 2.82275
[1mStep[0m  [96/169], [94mLoss[0m : 2.57529
[1mStep[0m  [112/169], [94mLoss[0m : 2.72683
[1mStep[0m  [128/169], [94mLoss[0m : 2.55104
[1mStep[0m  [144/169], [94mLoss[0m : 2.24631
[1mStep[0m  [160/169], [94mLoss[0m : 2.36239

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.459, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.78250
[1mStep[0m  [16/169], [94mLoss[0m : 2.55296
[1mStep[0m  [32/169], [94mLoss[0m : 2.47242
[1mStep[0m  [48/169], [94mLoss[0m : 2.61236
[1mStep[0m  [64/169], [94mLoss[0m : 2.52647
[1mStep[0m  [80/169], [94mLoss[0m : 2.36753
[1mStep[0m  [96/169], [94mLoss[0m : 2.57026
[1mStep[0m  [112/169], [94mLoss[0m : 2.64237
[1mStep[0m  [128/169], [94mLoss[0m : 2.55590
[1mStep[0m  [144/169], [94mLoss[0m : 2.27232
[1mStep[0m  [160/169], [94mLoss[0m : 2.63077

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.429, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.91690
[1mStep[0m  [16/169], [94mLoss[0m : 2.24191
[1mStep[0m  [32/169], [94mLoss[0m : 2.23392
[1mStep[0m  [48/169], [94mLoss[0m : 2.91419
[1mStep[0m  [64/169], [94mLoss[0m : 2.48682
[1mStep[0m  [80/169], [94mLoss[0m : 2.70317
[1mStep[0m  [96/169], [94mLoss[0m : 2.20991
[1mStep[0m  [112/169], [94mLoss[0m : 2.80887
[1mStep[0m  [128/169], [94mLoss[0m : 2.07911
[1mStep[0m  [144/169], [94mLoss[0m : 2.63410
[1mStep[0m  [160/169], [94mLoss[0m : 2.47650

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.395, [92mTest[0m: 2.435, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.44554
[1mStep[0m  [16/169], [94mLoss[0m : 2.40097
[1mStep[0m  [32/169], [94mLoss[0m : 2.38890
[1mStep[0m  [48/169], [94mLoss[0m : 2.25469
[1mStep[0m  [64/169], [94mLoss[0m : 2.82791
[1mStep[0m  [80/169], [94mLoss[0m : 2.75990
[1mStep[0m  [96/169], [94mLoss[0m : 1.92441
[1mStep[0m  [112/169], [94mLoss[0m : 2.26936
[1mStep[0m  [128/169], [94mLoss[0m : 2.41627
[1mStep[0m  [144/169], [94mLoss[0m : 2.46831
[1mStep[0m  [160/169], [94mLoss[0m : 1.87274

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.388, [92mTest[0m: 2.383, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.76850
[1mStep[0m  [16/169], [94mLoss[0m : 2.39748
[1mStep[0m  [32/169], [94mLoss[0m : 2.54843
[1mStep[0m  [48/169], [94mLoss[0m : 2.31011
[1mStep[0m  [64/169], [94mLoss[0m : 2.53975
[1mStep[0m  [80/169], [94mLoss[0m : 2.19734
[1mStep[0m  [96/169], [94mLoss[0m : 2.24815
[1mStep[0m  [112/169], [94mLoss[0m : 2.45410
[1mStep[0m  [128/169], [94mLoss[0m : 2.81281
[1mStep[0m  [144/169], [94mLoss[0m : 2.40707
[1mStep[0m  [160/169], [94mLoss[0m : 2.56949

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.392, [92mTest[0m: 2.388, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.92475
[1mStep[0m  [16/169], [94mLoss[0m : 2.54204
[1mStep[0m  [32/169], [94mLoss[0m : 2.08991
[1mStep[0m  [48/169], [94mLoss[0m : 1.89927
[1mStep[0m  [64/169], [94mLoss[0m : 2.65723
[1mStep[0m  [80/169], [94mLoss[0m : 2.76666
[1mStep[0m  [96/169], [94mLoss[0m : 2.67476
[1mStep[0m  [112/169], [94mLoss[0m : 1.81854
[1mStep[0m  [128/169], [94mLoss[0m : 2.15646
[1mStep[0m  [144/169], [94mLoss[0m : 2.37923
[1mStep[0m  [160/169], [94mLoss[0m : 2.35678

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.380, [92mTest[0m: 2.402, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.10915
[1mStep[0m  [16/169], [94mLoss[0m : 2.33730
[1mStep[0m  [32/169], [94mLoss[0m : 2.31147
[1mStep[0m  [48/169], [94mLoss[0m : 2.67669
[1mStep[0m  [64/169], [94mLoss[0m : 2.40032
[1mStep[0m  [80/169], [94mLoss[0m : 2.40819
[1mStep[0m  [96/169], [94mLoss[0m : 2.30958
[1mStep[0m  [112/169], [94mLoss[0m : 2.62264
[1mStep[0m  [128/169], [94mLoss[0m : 2.15943
[1mStep[0m  [144/169], [94mLoss[0m : 2.44602
[1mStep[0m  [160/169], [94mLoss[0m : 2.44068

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.379, [92mTest[0m: 2.343, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.40037
[1mStep[0m  [16/169], [94mLoss[0m : 2.91851
[1mStep[0m  [32/169], [94mLoss[0m : 2.53611
[1mStep[0m  [48/169], [94mLoss[0m : 2.03828
[1mStep[0m  [64/169], [94mLoss[0m : 2.87591
[1mStep[0m  [80/169], [94mLoss[0m : 2.50205
[1mStep[0m  [96/169], [94mLoss[0m : 2.32745
[1mStep[0m  [112/169], [94mLoss[0m : 2.23886
[1mStep[0m  [128/169], [94mLoss[0m : 2.52566
[1mStep[0m  [144/169], [94mLoss[0m : 2.27657
[1mStep[0m  [160/169], [94mLoss[0m : 2.61611

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.386, [92mTest[0m: 2.386, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.62211
[1mStep[0m  [16/169], [94mLoss[0m : 2.04304
[1mStep[0m  [32/169], [94mLoss[0m : 2.63923
[1mStep[0m  [48/169], [94mLoss[0m : 2.49255
[1mStep[0m  [64/169], [94mLoss[0m : 2.16319
[1mStep[0m  [80/169], [94mLoss[0m : 2.24411
[1mStep[0m  [96/169], [94mLoss[0m : 2.64731
[1mStep[0m  [112/169], [94mLoss[0m : 2.29341
[1mStep[0m  [128/169], [94mLoss[0m : 2.33054
[1mStep[0m  [144/169], [94mLoss[0m : 2.18126
[1mStep[0m  [160/169], [94mLoss[0m : 2.16352

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.382, [92mTest[0m: 2.363, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.49692
[1mStep[0m  [16/169], [94mLoss[0m : 2.67583
[1mStep[0m  [32/169], [94mLoss[0m : 2.44489
[1mStep[0m  [48/169], [94mLoss[0m : 2.28332
[1mStep[0m  [64/169], [94mLoss[0m : 2.46510
[1mStep[0m  [80/169], [94mLoss[0m : 2.80791
[1mStep[0m  [96/169], [94mLoss[0m : 2.57802
[1mStep[0m  [112/169], [94mLoss[0m : 2.20220
[1mStep[0m  [128/169], [94mLoss[0m : 2.39809
[1mStep[0m  [144/169], [94mLoss[0m : 2.38789
[1mStep[0m  [160/169], [94mLoss[0m : 2.48822

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.369, [92mTest[0m: 2.342, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.33215
[1mStep[0m  [16/169], [94mLoss[0m : 2.55210
[1mStep[0m  [32/169], [94mLoss[0m : 2.23568
[1mStep[0m  [48/169], [94mLoss[0m : 2.38271
[1mStep[0m  [64/169], [94mLoss[0m : 1.86511
[1mStep[0m  [80/169], [94mLoss[0m : 2.66543
[1mStep[0m  [96/169], [94mLoss[0m : 2.68061
[1mStep[0m  [112/169], [94mLoss[0m : 2.76883
[1mStep[0m  [128/169], [94mLoss[0m : 2.45568
[1mStep[0m  [144/169], [94mLoss[0m : 2.04499
[1mStep[0m  [160/169], [94mLoss[0m : 2.58061

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.369, [92mTest[0m: 2.379, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.336
====================================

Phase 1 - Evaluation MAE:  2.336167722940445
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/169], [94mLoss[0m : 2.32311
[1mStep[0m  [16/169], [94mLoss[0m : 2.72574
[1mStep[0m  [32/169], [94mLoss[0m : 3.00062
[1mStep[0m  [48/169], [94mLoss[0m : 2.48518
[1mStep[0m  [64/169], [94mLoss[0m : 2.54194
[1mStep[0m  [80/169], [94mLoss[0m : 2.45455
[1mStep[0m  [96/169], [94mLoss[0m : 2.80996
[1mStep[0m  [112/169], [94mLoss[0m : 2.46262
[1mStep[0m  [128/169], [94mLoss[0m : 2.28243
[1mStep[0m  [144/169], [94mLoss[0m : 2.73894
[1mStep[0m  [160/169], [94mLoss[0m : 2.24198

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.336, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.61592
[1mStep[0m  [16/169], [94mLoss[0m : 2.47018
[1mStep[0m  [32/169], [94mLoss[0m : 2.43030
[1mStep[0m  [48/169], [94mLoss[0m : 2.67790
[1mStep[0m  [64/169], [94mLoss[0m : 2.22928
[1mStep[0m  [80/169], [94mLoss[0m : 2.41575
[1mStep[0m  [96/169], [94mLoss[0m : 2.65092
[1mStep[0m  [112/169], [94mLoss[0m : 2.24291
[1mStep[0m  [128/169], [94mLoss[0m : 2.24485
[1mStep[0m  [144/169], [94mLoss[0m : 2.48539
[1mStep[0m  [160/169], [94mLoss[0m : 2.01746

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.385, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.48134
[1mStep[0m  [16/169], [94mLoss[0m : 2.42765
[1mStep[0m  [32/169], [94mLoss[0m : 2.77834
[1mStep[0m  [48/169], [94mLoss[0m : 1.99815
[1mStep[0m  [64/169], [94mLoss[0m : 2.41990
[1mStep[0m  [80/169], [94mLoss[0m : 2.22873
[1mStep[0m  [96/169], [94mLoss[0m : 1.86800
[1mStep[0m  [112/169], [94mLoss[0m : 2.39230
[1mStep[0m  [128/169], [94mLoss[0m : 2.03335
[1mStep[0m  [144/169], [94mLoss[0m : 2.20572
[1mStep[0m  [160/169], [94mLoss[0m : 2.60224

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.323, [92mTest[0m: 2.386, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.75875
[1mStep[0m  [16/169], [94mLoss[0m : 2.07763
[1mStep[0m  [32/169], [94mLoss[0m : 2.48641
[1mStep[0m  [48/169], [94mLoss[0m : 2.62245
[1mStep[0m  [64/169], [94mLoss[0m : 2.39498
[1mStep[0m  [80/169], [94mLoss[0m : 2.13498
[1mStep[0m  [96/169], [94mLoss[0m : 2.09102
[1mStep[0m  [112/169], [94mLoss[0m : 1.94191
[1mStep[0m  [128/169], [94mLoss[0m : 2.11854
[1mStep[0m  [144/169], [94mLoss[0m : 2.49394
[1mStep[0m  [160/169], [94mLoss[0m : 2.04794

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.244, [92mTest[0m: 2.346, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.36227
[1mStep[0m  [16/169], [94mLoss[0m : 2.02381
[1mStep[0m  [32/169], [94mLoss[0m : 2.29957
[1mStep[0m  [48/169], [94mLoss[0m : 1.97526
[1mStep[0m  [64/169], [94mLoss[0m : 2.13618
[1mStep[0m  [80/169], [94mLoss[0m : 2.20878
[1mStep[0m  [96/169], [94mLoss[0m : 1.93896
[1mStep[0m  [112/169], [94mLoss[0m : 2.23111
[1mStep[0m  [128/169], [94mLoss[0m : 2.09241
[1mStep[0m  [144/169], [94mLoss[0m : 2.59679
[1mStep[0m  [160/169], [94mLoss[0m : 2.46196

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.198, [92mTest[0m: 2.392, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.08802
[1mStep[0m  [16/169], [94mLoss[0m : 2.15822
[1mStep[0m  [32/169], [94mLoss[0m : 2.10251
[1mStep[0m  [48/169], [94mLoss[0m : 2.04542
[1mStep[0m  [64/169], [94mLoss[0m : 2.16961
[1mStep[0m  [80/169], [94mLoss[0m : 2.16084
[1mStep[0m  [96/169], [94mLoss[0m : 1.94799
[1mStep[0m  [112/169], [94mLoss[0m : 1.90570
[1mStep[0m  [128/169], [94mLoss[0m : 2.29173
[1mStep[0m  [144/169], [94mLoss[0m : 2.55633
[1mStep[0m  [160/169], [94mLoss[0m : 1.73901

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.148, [92mTest[0m: 2.451, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.16176
[1mStep[0m  [16/169], [94mLoss[0m : 1.89224
[1mStep[0m  [32/169], [94mLoss[0m : 1.97010
[1mStep[0m  [48/169], [94mLoss[0m : 2.15856
[1mStep[0m  [64/169], [94mLoss[0m : 1.63291
[1mStep[0m  [80/169], [94mLoss[0m : 1.98094
[1mStep[0m  [96/169], [94mLoss[0m : 2.47957
[1mStep[0m  [112/169], [94mLoss[0m : 2.07663
[1mStep[0m  [128/169], [94mLoss[0m : 2.08363
[1mStep[0m  [144/169], [94mLoss[0m : 1.70773
[1mStep[0m  [160/169], [94mLoss[0m : 2.32001

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.106, [92mTest[0m: 2.445, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.89054
[1mStep[0m  [16/169], [94mLoss[0m : 2.04721
[1mStep[0m  [32/169], [94mLoss[0m : 2.03621
[1mStep[0m  [48/169], [94mLoss[0m : 1.82778
[1mStep[0m  [64/169], [94mLoss[0m : 2.30975
[1mStep[0m  [80/169], [94mLoss[0m : 1.94133
[1mStep[0m  [96/169], [94mLoss[0m : 1.87368
[1mStep[0m  [112/169], [94mLoss[0m : 2.16849
[1mStep[0m  [128/169], [94mLoss[0m : 1.71083
[1mStep[0m  [144/169], [94mLoss[0m : 1.88894
[1mStep[0m  [160/169], [94mLoss[0m : 2.29691

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.056, [92mTest[0m: 2.413, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.53412
[1mStep[0m  [16/169], [94mLoss[0m : 1.82571
[1mStep[0m  [32/169], [94mLoss[0m : 2.13937
[1mStep[0m  [48/169], [94mLoss[0m : 2.08801
[1mStep[0m  [64/169], [94mLoss[0m : 2.34712
[1mStep[0m  [80/169], [94mLoss[0m : 1.84365
[1mStep[0m  [96/169], [94mLoss[0m : 2.35121
[1mStep[0m  [112/169], [94mLoss[0m : 2.03249
[1mStep[0m  [128/169], [94mLoss[0m : 1.69171
[1mStep[0m  [144/169], [94mLoss[0m : 1.81445
[1mStep[0m  [160/169], [94mLoss[0m : 2.10901

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.006, [92mTest[0m: 2.453, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.19824
[1mStep[0m  [16/169], [94mLoss[0m : 2.01551
[1mStep[0m  [32/169], [94mLoss[0m : 2.04917
[1mStep[0m  [48/169], [94mLoss[0m : 2.17502
[1mStep[0m  [64/169], [94mLoss[0m : 1.98255
[1mStep[0m  [80/169], [94mLoss[0m : 1.95548
[1mStep[0m  [96/169], [94mLoss[0m : 2.37816
[1mStep[0m  [112/169], [94mLoss[0m : 1.90067
[1mStep[0m  [128/169], [94mLoss[0m : 2.50663
[1mStep[0m  [144/169], [94mLoss[0m : 1.81269
[1mStep[0m  [160/169], [94mLoss[0m : 1.77991

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.961, [92mTest[0m: 2.506, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.78962
[1mStep[0m  [16/169], [94mLoss[0m : 1.92254
[1mStep[0m  [32/169], [94mLoss[0m : 2.09929
[1mStep[0m  [48/169], [94mLoss[0m : 2.11225
[1mStep[0m  [64/169], [94mLoss[0m : 1.51172
[1mStep[0m  [80/169], [94mLoss[0m : 1.78721
[1mStep[0m  [96/169], [94mLoss[0m : 1.62058
[1mStep[0m  [112/169], [94mLoss[0m : 1.99474
[1mStep[0m  [128/169], [94mLoss[0m : 1.95404
[1mStep[0m  [144/169], [94mLoss[0m : 2.13985
[1mStep[0m  [160/169], [94mLoss[0m : 2.11668

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.927, [92mTest[0m: 2.467, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.77241
[1mStep[0m  [16/169], [94mLoss[0m : 1.78985
[1mStep[0m  [32/169], [94mLoss[0m : 1.73484
[1mStep[0m  [48/169], [94mLoss[0m : 1.69021
[1mStep[0m  [64/169], [94mLoss[0m : 1.65246
[1mStep[0m  [80/169], [94mLoss[0m : 2.05391
[1mStep[0m  [96/169], [94mLoss[0m : 1.89251
[1mStep[0m  [112/169], [94mLoss[0m : 1.95032
[1mStep[0m  [128/169], [94mLoss[0m : 1.77932
[1mStep[0m  [144/169], [94mLoss[0m : 2.08124
[1mStep[0m  [160/169], [94mLoss[0m : 1.86347

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.890, [92mTest[0m: 2.456, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.54259
[1mStep[0m  [16/169], [94mLoss[0m : 1.86092
[1mStep[0m  [32/169], [94mLoss[0m : 1.83609
[1mStep[0m  [48/169], [94mLoss[0m : 1.72802
[1mStep[0m  [64/169], [94mLoss[0m : 2.08438
[1mStep[0m  [80/169], [94mLoss[0m : 1.93242
[1mStep[0m  [96/169], [94mLoss[0m : 2.14413
[1mStep[0m  [112/169], [94mLoss[0m : 1.65214
[1mStep[0m  [128/169], [94mLoss[0m : 2.03870
[1mStep[0m  [144/169], [94mLoss[0m : 1.69026
[1mStep[0m  [160/169], [94mLoss[0m : 1.57472

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.867, [92mTest[0m: 2.531, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.98200
[1mStep[0m  [16/169], [94mLoss[0m : 1.80023
[1mStep[0m  [32/169], [94mLoss[0m : 1.74320
[1mStep[0m  [48/169], [94mLoss[0m : 1.97964
[1mStep[0m  [64/169], [94mLoss[0m : 1.53256
[1mStep[0m  [80/169], [94mLoss[0m : 1.81249
[1mStep[0m  [96/169], [94mLoss[0m : 1.85098
[1mStep[0m  [112/169], [94mLoss[0m : 1.95519
[1mStep[0m  [128/169], [94mLoss[0m : 1.58924
[1mStep[0m  [144/169], [94mLoss[0m : 1.87616
[1mStep[0m  [160/169], [94mLoss[0m : 2.03359

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.812, [92mTest[0m: 2.438, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.78969
[1mStep[0m  [16/169], [94mLoss[0m : 1.61677
[1mStep[0m  [32/169], [94mLoss[0m : 1.78062
[1mStep[0m  [48/169], [94mLoss[0m : 1.80775
[1mStep[0m  [64/169], [94mLoss[0m : 1.66384
[1mStep[0m  [80/169], [94mLoss[0m : 1.68063
[1mStep[0m  [96/169], [94mLoss[0m : 1.63003
[1mStep[0m  [112/169], [94mLoss[0m : 2.33409
[1mStep[0m  [128/169], [94mLoss[0m : 1.87777
[1mStep[0m  [144/169], [94mLoss[0m : 1.62987
[1mStep[0m  [160/169], [94mLoss[0m : 2.06111

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.779, [92mTest[0m: 2.533, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.81241
[1mStep[0m  [16/169], [94mLoss[0m : 1.51204
[1mStep[0m  [32/169], [94mLoss[0m : 2.01888
[1mStep[0m  [48/169], [94mLoss[0m : 1.77315
[1mStep[0m  [64/169], [94mLoss[0m : 1.72684
[1mStep[0m  [80/169], [94mLoss[0m : 1.83528
[1mStep[0m  [96/169], [94mLoss[0m : 1.72991
[1mStep[0m  [112/169], [94mLoss[0m : 1.53345
[1mStep[0m  [128/169], [94mLoss[0m : 2.28490
[1mStep[0m  [144/169], [94mLoss[0m : 1.34583
[1mStep[0m  [160/169], [94mLoss[0m : 1.80521

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.756, [92mTest[0m: 2.555, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.61807
[1mStep[0m  [16/169], [94mLoss[0m : 1.83514
[1mStep[0m  [32/169], [94mLoss[0m : 1.80925
[1mStep[0m  [48/169], [94mLoss[0m : 2.02391
[1mStep[0m  [64/169], [94mLoss[0m : 1.51504
[1mStep[0m  [80/169], [94mLoss[0m : 1.83063
[1mStep[0m  [96/169], [94mLoss[0m : 1.57061
[1mStep[0m  [112/169], [94mLoss[0m : 1.76636
[1mStep[0m  [128/169], [94mLoss[0m : 1.74202
[1mStep[0m  [144/169], [94mLoss[0m : 2.21974
[1mStep[0m  [160/169], [94mLoss[0m : 1.79619

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.745, [92mTest[0m: 2.485, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.64838
[1mStep[0m  [16/169], [94mLoss[0m : 1.39854
[1mStep[0m  [32/169], [94mLoss[0m : 2.00361
[1mStep[0m  [48/169], [94mLoss[0m : 1.80095
[1mStep[0m  [64/169], [94mLoss[0m : 1.76198
[1mStep[0m  [80/169], [94mLoss[0m : 1.50647
[1mStep[0m  [96/169], [94mLoss[0m : 1.52873
[1mStep[0m  [112/169], [94mLoss[0m : 1.80550
[1mStep[0m  [128/169], [94mLoss[0m : 1.22804
[1mStep[0m  [144/169], [94mLoss[0m : 1.98738
[1mStep[0m  [160/169], [94mLoss[0m : 2.01987

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.700, [92mTest[0m: 2.477, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.59103
[1mStep[0m  [16/169], [94mLoss[0m : 1.32034
[1mStep[0m  [32/169], [94mLoss[0m : 1.37468
[1mStep[0m  [48/169], [94mLoss[0m : 1.89244
[1mStep[0m  [64/169], [94mLoss[0m : 1.73411
[1mStep[0m  [80/169], [94mLoss[0m : 1.37045
[1mStep[0m  [96/169], [94mLoss[0m : 1.57190
[1mStep[0m  [112/169], [94mLoss[0m : 1.44363
[1mStep[0m  [128/169], [94mLoss[0m : 1.97488
[1mStep[0m  [144/169], [94mLoss[0m : 1.47714
[1mStep[0m  [160/169], [94mLoss[0m : 1.55146

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.671, [92mTest[0m: 2.467, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.50429
[1mStep[0m  [16/169], [94mLoss[0m : 1.57924
[1mStep[0m  [32/169], [94mLoss[0m : 1.54986
[1mStep[0m  [48/169], [94mLoss[0m : 1.93145
[1mStep[0m  [64/169], [94mLoss[0m : 1.56927
[1mStep[0m  [80/169], [94mLoss[0m : 1.56488
[1mStep[0m  [96/169], [94mLoss[0m : 1.44797
[1mStep[0m  [112/169], [94mLoss[0m : 1.61440
[1mStep[0m  [128/169], [94mLoss[0m : 1.43977
[1mStep[0m  [144/169], [94mLoss[0m : 1.68848
[1mStep[0m  [160/169], [94mLoss[0m : 1.66484

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.653, [92mTest[0m: 2.514, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.34248
[1mStep[0m  [16/169], [94mLoss[0m : 1.59605
[1mStep[0m  [32/169], [94mLoss[0m : 1.31926
[1mStep[0m  [48/169], [94mLoss[0m : 1.77107
[1mStep[0m  [64/169], [94mLoss[0m : 2.21669
[1mStep[0m  [80/169], [94mLoss[0m : 1.29261
[1mStep[0m  [96/169], [94mLoss[0m : 1.54525
[1mStep[0m  [112/169], [94mLoss[0m : 1.44860
[1mStep[0m  [128/169], [94mLoss[0m : 1.20726
[1mStep[0m  [144/169], [94mLoss[0m : 1.28319
[1mStep[0m  [160/169], [94mLoss[0m : 1.48503

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.604, [92mTest[0m: 2.526, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.36917
[1mStep[0m  [16/169], [94mLoss[0m : 1.74148
[1mStep[0m  [32/169], [94mLoss[0m : 1.52128
[1mStep[0m  [48/169], [94mLoss[0m : 1.92136
[1mStep[0m  [64/169], [94mLoss[0m : 1.65124
[1mStep[0m  [80/169], [94mLoss[0m : 1.65866
[1mStep[0m  [96/169], [94mLoss[0m : 1.68547
[1mStep[0m  [112/169], [94mLoss[0m : 1.62032
[1mStep[0m  [128/169], [94mLoss[0m : 1.76166
[1mStep[0m  [144/169], [94mLoss[0m : 1.46557
[1mStep[0m  [160/169], [94mLoss[0m : 1.55479

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.608, [92mTest[0m: 2.536, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.38245
[1mStep[0m  [16/169], [94mLoss[0m : 1.68643
[1mStep[0m  [32/169], [94mLoss[0m : 1.36117
[1mStep[0m  [48/169], [94mLoss[0m : 1.68563
[1mStep[0m  [64/169], [94mLoss[0m : 1.82927
[1mStep[0m  [80/169], [94mLoss[0m : 1.35016
[1mStep[0m  [96/169], [94mLoss[0m : 1.72665
[1mStep[0m  [112/169], [94mLoss[0m : 1.79984
[1mStep[0m  [128/169], [94mLoss[0m : 1.28203
[1mStep[0m  [144/169], [94mLoss[0m : 1.43510
[1mStep[0m  [160/169], [94mLoss[0m : 1.60432

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.562, [92mTest[0m: 2.495, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.74525
[1mStep[0m  [16/169], [94mLoss[0m : 1.85092
[1mStep[0m  [32/169], [94mLoss[0m : 1.61391
[1mStep[0m  [48/169], [94mLoss[0m : 1.33595
[1mStep[0m  [64/169], [94mLoss[0m : 1.43164
[1mStep[0m  [80/169], [94mLoss[0m : 1.54081
[1mStep[0m  [96/169], [94mLoss[0m : 1.33863
[1mStep[0m  [112/169], [94mLoss[0m : 1.45244
[1mStep[0m  [128/169], [94mLoss[0m : 1.56192
[1mStep[0m  [144/169], [94mLoss[0m : 1.51392
[1mStep[0m  [160/169], [94mLoss[0m : 1.33102

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.552, [92mTest[0m: 2.513, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.31766
[1mStep[0m  [16/169], [94mLoss[0m : 1.60272
[1mStep[0m  [32/169], [94mLoss[0m : 1.50674
[1mStep[0m  [48/169], [94mLoss[0m : 1.51957
[1mStep[0m  [64/169], [94mLoss[0m : 1.52736
[1mStep[0m  [80/169], [94mLoss[0m : 1.54022
[1mStep[0m  [96/169], [94mLoss[0m : 1.37227
[1mStep[0m  [112/169], [94mLoss[0m : 1.32683
[1mStep[0m  [128/169], [94mLoss[0m : 1.52973
[1mStep[0m  [144/169], [94mLoss[0m : 1.51178
[1mStep[0m  [160/169], [94mLoss[0m : 1.70354

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.548, [92mTest[0m: 2.513, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.17210
[1mStep[0m  [16/169], [94mLoss[0m : 1.42929
[1mStep[0m  [32/169], [94mLoss[0m : 1.84232
[1mStep[0m  [48/169], [94mLoss[0m : 1.44633
[1mStep[0m  [64/169], [94mLoss[0m : 1.55377
[1mStep[0m  [80/169], [94mLoss[0m : 1.56679
[1mStep[0m  [96/169], [94mLoss[0m : 1.44996
[1mStep[0m  [112/169], [94mLoss[0m : 1.38975
[1mStep[0m  [128/169], [94mLoss[0m : 1.44894
[1mStep[0m  [144/169], [94mLoss[0m : 1.39984
[1mStep[0m  [160/169], [94mLoss[0m : 1.86113

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.530, [92mTest[0m: 2.476, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.25696
[1mStep[0m  [16/169], [94mLoss[0m : 1.42389
[1mStep[0m  [32/169], [94mLoss[0m : 1.71779
[1mStep[0m  [48/169], [94mLoss[0m : 1.40509
[1mStep[0m  [64/169], [94mLoss[0m : 1.44163
[1mStep[0m  [80/169], [94mLoss[0m : 1.65576
[1mStep[0m  [96/169], [94mLoss[0m : 1.60340
[1mStep[0m  [112/169], [94mLoss[0m : 1.76889
[1mStep[0m  [128/169], [94mLoss[0m : 1.52514
[1mStep[0m  [144/169], [94mLoss[0m : 1.57927
[1mStep[0m  [160/169], [94mLoss[0m : 1.46702

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.507, [92mTest[0m: 2.514, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.26014
[1mStep[0m  [16/169], [94mLoss[0m : 1.45534
[1mStep[0m  [32/169], [94mLoss[0m : 1.52072
[1mStep[0m  [48/169], [94mLoss[0m : 1.40123
[1mStep[0m  [64/169], [94mLoss[0m : 1.52878
[1mStep[0m  [80/169], [94mLoss[0m : 1.42842
[1mStep[0m  [96/169], [94mLoss[0m : 1.55283
[1mStep[0m  [112/169], [94mLoss[0m : 1.45131
[1mStep[0m  [128/169], [94mLoss[0m : 1.41676
[1mStep[0m  [144/169], [94mLoss[0m : 1.34430
[1mStep[0m  [160/169], [94mLoss[0m : 1.20397

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.491, [92mTest[0m: 2.516, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.42885
[1mStep[0m  [16/169], [94mLoss[0m : 1.32223
[1mStep[0m  [32/169], [94mLoss[0m : 2.06156
[1mStep[0m  [48/169], [94mLoss[0m : 1.60967
[1mStep[0m  [64/169], [94mLoss[0m : 1.57098
[1mStep[0m  [80/169], [94mLoss[0m : 1.51948
[1mStep[0m  [96/169], [94mLoss[0m : 1.64643
[1mStep[0m  [112/169], [94mLoss[0m : 1.17796
[1mStep[0m  [128/169], [94mLoss[0m : 1.44365
[1mStep[0m  [144/169], [94mLoss[0m : 1.29398
[1mStep[0m  [160/169], [94mLoss[0m : 1.44423

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.479, [92mTest[0m: 2.558, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.40993
[1mStep[0m  [16/169], [94mLoss[0m : 1.29771
[1mStep[0m  [32/169], [94mLoss[0m : 1.44675
[1mStep[0m  [48/169], [94mLoss[0m : 1.32616
[1mStep[0m  [64/169], [94mLoss[0m : 1.35466
[1mStep[0m  [80/169], [94mLoss[0m : 1.52795
[1mStep[0m  [96/169], [94mLoss[0m : 1.69555
[1mStep[0m  [112/169], [94mLoss[0m : 1.31559
[1mStep[0m  [128/169], [94mLoss[0m : 1.48063
[1mStep[0m  [144/169], [94mLoss[0m : 1.61681
[1mStep[0m  [160/169], [94mLoss[0m : 1.65911

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.490, [92mTest[0m: 2.509, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.507
====================================

Phase 2 - Evaluation MAE:  2.5067498769078935
MAE score P1      2.336168
MAE score P2       2.50675
loss              1.479305
learning_rate      0.00505
batch_size              64
hidden_sizes         [300]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.4
momentum               0.1
weight_decay          0.01
Name: 5, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/169], [94mLoss[0m : 11.34532
[1mStep[0m  [16/169], [94mLoss[0m : 10.03864
[1mStep[0m  [32/169], [94mLoss[0m : 9.61624
[1mStep[0m  [48/169], [94mLoss[0m : 10.68400
[1mStep[0m  [64/169], [94mLoss[0m : 9.75852
[1mStep[0m  [80/169], [94mLoss[0m : 10.31692
[1mStep[0m  [96/169], [94mLoss[0m : 9.16503
[1mStep[0m  [112/169], [94mLoss[0m : 9.04001
[1mStep[0m  [128/169], [94mLoss[0m : 8.94800
[1mStep[0m  [144/169], [94mLoss[0m : 8.57848
[1mStep[0m  [160/169], [94mLoss[0m : 7.64385

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.478, [92mTest[0m: 10.907, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 8.23575
[1mStep[0m  [16/169], [94mLoss[0m : 7.39720
[1mStep[0m  [32/169], [94mLoss[0m : 7.57061
[1mStep[0m  [48/169], [94mLoss[0m : 6.74299
[1mStep[0m  [64/169], [94mLoss[0m : 5.97488
[1mStep[0m  [80/169], [94mLoss[0m : 6.83042
[1mStep[0m  [96/169], [94mLoss[0m : 5.90469
[1mStep[0m  [112/169], [94mLoss[0m : 4.80949
[1mStep[0m  [128/169], [94mLoss[0m : 5.06122
[1mStep[0m  [144/169], [94mLoss[0m : 5.08230
[1mStep[0m  [160/169], [94mLoss[0m : 4.44423

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 5.924, [92mTest[0m: 7.029, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 4.14069
[1mStep[0m  [16/169], [94mLoss[0m : 3.42319
[1mStep[0m  [32/169], [94mLoss[0m : 3.93616
[1mStep[0m  [48/169], [94mLoss[0m : 3.37655
[1mStep[0m  [64/169], [94mLoss[0m : 3.74660
[1mStep[0m  [80/169], [94mLoss[0m : 3.60955
[1mStep[0m  [96/169], [94mLoss[0m : 3.61465
[1mStep[0m  [112/169], [94mLoss[0m : 2.96722
[1mStep[0m  [128/169], [94mLoss[0m : 3.10900
[1mStep[0m  [144/169], [94mLoss[0m : 2.77505
[1mStep[0m  [160/169], [94mLoss[0m : 2.76657

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.418, [92mTest[0m: 3.598, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.91680
[1mStep[0m  [16/169], [94mLoss[0m : 2.72662
[1mStep[0m  [32/169], [94mLoss[0m : 2.96855
[1mStep[0m  [48/169], [94mLoss[0m : 2.81417
[1mStep[0m  [64/169], [94mLoss[0m : 2.99276
[1mStep[0m  [80/169], [94mLoss[0m : 2.60044
[1mStep[0m  [96/169], [94mLoss[0m : 2.69360
[1mStep[0m  [112/169], [94mLoss[0m : 2.34006
[1mStep[0m  [128/169], [94mLoss[0m : 2.69374
[1mStep[0m  [144/169], [94mLoss[0m : 2.67131
[1mStep[0m  [160/169], [94mLoss[0m : 2.85548

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.796, [92mTest[0m: 2.432, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.61109
[1mStep[0m  [16/169], [94mLoss[0m : 3.03436
[1mStep[0m  [32/169], [94mLoss[0m : 2.55779
[1mStep[0m  [48/169], [94mLoss[0m : 3.07297
[1mStep[0m  [64/169], [94mLoss[0m : 2.93129
[1mStep[0m  [80/169], [94mLoss[0m : 3.22982
[1mStep[0m  [96/169], [94mLoss[0m : 3.15732
[1mStep[0m  [112/169], [94mLoss[0m : 3.09922
[1mStep[0m  [128/169], [94mLoss[0m : 2.41140
[1mStep[0m  [144/169], [94mLoss[0m : 2.61281
[1mStep[0m  [160/169], [94mLoss[0m : 2.63648

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.758, [92mTest[0m: 2.354, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.43620
[1mStep[0m  [16/169], [94mLoss[0m : 2.66835
[1mStep[0m  [32/169], [94mLoss[0m : 2.56883
[1mStep[0m  [48/169], [94mLoss[0m : 2.77842
[1mStep[0m  [64/169], [94mLoss[0m : 2.53485
[1mStep[0m  [80/169], [94mLoss[0m : 2.04333
[1mStep[0m  [96/169], [94mLoss[0m : 2.57176
[1mStep[0m  [112/169], [94mLoss[0m : 2.91608
[1mStep[0m  [128/169], [94mLoss[0m : 3.10447
[1mStep[0m  [144/169], [94mLoss[0m : 2.59810
[1mStep[0m  [160/169], [94mLoss[0m : 2.30005

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.674, [92mTest[0m: 2.339, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.66376
[1mStep[0m  [16/169], [94mLoss[0m : 2.41143
[1mStep[0m  [32/169], [94mLoss[0m : 3.07789
[1mStep[0m  [48/169], [94mLoss[0m : 2.49831
[1mStep[0m  [64/169], [94mLoss[0m : 2.14112
[1mStep[0m  [80/169], [94mLoss[0m : 2.79687
[1mStep[0m  [96/169], [94mLoss[0m : 2.87729
[1mStep[0m  [112/169], [94mLoss[0m : 2.51143
[1mStep[0m  [128/169], [94mLoss[0m : 2.60768
[1mStep[0m  [144/169], [94mLoss[0m : 2.91485
[1mStep[0m  [160/169], [94mLoss[0m : 3.02764

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.662, [92mTest[0m: 2.350, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.74662
[1mStep[0m  [16/169], [94mLoss[0m : 2.90621
[1mStep[0m  [32/169], [94mLoss[0m : 2.90534
[1mStep[0m  [48/169], [94mLoss[0m : 2.62764
[1mStep[0m  [64/169], [94mLoss[0m : 3.16869
[1mStep[0m  [80/169], [94mLoss[0m : 2.73402
[1mStep[0m  [96/169], [94mLoss[0m : 2.35838
[1mStep[0m  [112/169], [94mLoss[0m : 2.28656
[1mStep[0m  [128/169], [94mLoss[0m : 2.74477
[1mStep[0m  [144/169], [94mLoss[0m : 2.29940
[1mStep[0m  [160/169], [94mLoss[0m : 3.23670

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.623, [92mTest[0m: 2.331, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.86411
[1mStep[0m  [16/169], [94mLoss[0m : 2.33358
[1mStep[0m  [32/169], [94mLoss[0m : 1.94238
[1mStep[0m  [48/169], [94mLoss[0m : 2.42027
[1mStep[0m  [64/169], [94mLoss[0m : 2.43345
[1mStep[0m  [80/169], [94mLoss[0m : 2.92796
[1mStep[0m  [96/169], [94mLoss[0m : 2.59133
[1mStep[0m  [112/169], [94mLoss[0m : 2.19326
[1mStep[0m  [128/169], [94mLoss[0m : 2.84794
[1mStep[0m  [144/169], [94mLoss[0m : 2.85244
[1mStep[0m  [160/169], [94mLoss[0m : 2.43692

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.615, [92mTest[0m: 2.355, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.46684
[1mStep[0m  [16/169], [94mLoss[0m : 2.63805
[1mStep[0m  [32/169], [94mLoss[0m : 2.32342
[1mStep[0m  [48/169], [94mLoss[0m : 2.57077
[1mStep[0m  [64/169], [94mLoss[0m : 2.49569
[1mStep[0m  [80/169], [94mLoss[0m : 2.83509
[1mStep[0m  [96/169], [94mLoss[0m : 2.09989
[1mStep[0m  [112/169], [94mLoss[0m : 2.32324
[1mStep[0m  [128/169], [94mLoss[0m : 2.40911
[1mStep[0m  [144/169], [94mLoss[0m : 2.88549
[1mStep[0m  [160/169], [94mLoss[0m : 2.87320

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.571, [92mTest[0m: 2.351, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.11920
[1mStep[0m  [16/169], [94mLoss[0m : 2.67331
[1mStep[0m  [32/169], [94mLoss[0m : 2.91255
[1mStep[0m  [48/169], [94mLoss[0m : 2.67032
[1mStep[0m  [64/169], [94mLoss[0m : 2.18286
[1mStep[0m  [80/169], [94mLoss[0m : 2.56803
[1mStep[0m  [96/169], [94mLoss[0m : 2.65358
[1mStep[0m  [112/169], [94mLoss[0m : 2.64393
[1mStep[0m  [128/169], [94mLoss[0m : 2.66933
[1mStep[0m  [144/169], [94mLoss[0m : 2.29794
[1mStep[0m  [160/169], [94mLoss[0m : 2.42891

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.578, [92mTest[0m: 2.355, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.75125
[1mStep[0m  [16/169], [94mLoss[0m : 2.38443
[1mStep[0m  [32/169], [94mLoss[0m : 2.32814
[1mStep[0m  [48/169], [94mLoss[0m : 2.42068
[1mStep[0m  [64/169], [94mLoss[0m : 2.26015
[1mStep[0m  [80/169], [94mLoss[0m : 2.13620
[1mStep[0m  [96/169], [94mLoss[0m : 2.54209
[1mStep[0m  [112/169], [94mLoss[0m : 2.77443
[1mStep[0m  [128/169], [94mLoss[0m : 2.26299
[1mStep[0m  [144/169], [94mLoss[0m : 2.41351
[1mStep[0m  [160/169], [94mLoss[0m : 2.08998

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.564, [92mTest[0m: 2.331, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.25053
[1mStep[0m  [16/169], [94mLoss[0m : 2.57045
[1mStep[0m  [32/169], [94mLoss[0m : 2.43978
[1mStep[0m  [48/169], [94mLoss[0m : 2.58372
[1mStep[0m  [64/169], [94mLoss[0m : 2.88261
[1mStep[0m  [80/169], [94mLoss[0m : 2.47343
[1mStep[0m  [96/169], [94mLoss[0m : 2.42252
[1mStep[0m  [112/169], [94mLoss[0m : 2.49298
[1mStep[0m  [128/169], [94mLoss[0m : 2.73056
[1mStep[0m  [144/169], [94mLoss[0m : 2.61357
[1mStep[0m  [160/169], [94mLoss[0m : 2.32568

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.558, [92mTest[0m: 2.330, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.36724
[1mStep[0m  [16/169], [94mLoss[0m : 2.56879
[1mStep[0m  [32/169], [94mLoss[0m : 2.34019
[1mStep[0m  [48/169], [94mLoss[0m : 2.10013
[1mStep[0m  [64/169], [94mLoss[0m : 2.41297
[1mStep[0m  [80/169], [94mLoss[0m : 2.81063
[1mStep[0m  [96/169], [94mLoss[0m : 2.61136
[1mStep[0m  [112/169], [94mLoss[0m : 2.45185
[1mStep[0m  [128/169], [94mLoss[0m : 2.72699
[1mStep[0m  [144/169], [94mLoss[0m : 2.19066
[1mStep[0m  [160/169], [94mLoss[0m : 2.57038

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.546, [92mTest[0m: 2.348, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.93965
[1mStep[0m  [16/169], [94mLoss[0m : 2.69971
[1mStep[0m  [32/169], [94mLoss[0m : 2.01922
[1mStep[0m  [48/169], [94mLoss[0m : 3.04968
[1mStep[0m  [64/169], [94mLoss[0m : 2.93406
[1mStep[0m  [80/169], [94mLoss[0m : 2.66930
[1mStep[0m  [96/169], [94mLoss[0m : 2.39404
[1mStep[0m  [112/169], [94mLoss[0m : 2.79762
[1mStep[0m  [128/169], [94mLoss[0m : 2.35778
[1mStep[0m  [144/169], [94mLoss[0m : 2.64260
[1mStep[0m  [160/169], [94mLoss[0m : 2.26565

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.335, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.13737
[1mStep[0m  [16/169], [94mLoss[0m : 3.01014
[1mStep[0m  [32/169], [94mLoss[0m : 2.62462
[1mStep[0m  [48/169], [94mLoss[0m : 2.54101
[1mStep[0m  [64/169], [94mLoss[0m : 2.24870
[1mStep[0m  [80/169], [94mLoss[0m : 2.39841
[1mStep[0m  [96/169], [94mLoss[0m : 2.58315
[1mStep[0m  [112/169], [94mLoss[0m : 2.38460
[1mStep[0m  [128/169], [94mLoss[0m : 2.64202
[1mStep[0m  [144/169], [94mLoss[0m : 2.78843
[1mStep[0m  [160/169], [94mLoss[0m : 2.49635

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.336, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.68897
[1mStep[0m  [16/169], [94mLoss[0m : 3.02412
[1mStep[0m  [32/169], [94mLoss[0m : 2.55735
[1mStep[0m  [48/169], [94mLoss[0m : 2.28028
[1mStep[0m  [64/169], [94mLoss[0m : 2.53864
[1mStep[0m  [80/169], [94mLoss[0m : 2.53163
[1mStep[0m  [96/169], [94mLoss[0m : 2.08204
[1mStep[0m  [112/169], [94mLoss[0m : 2.58944
[1mStep[0m  [128/169], [94mLoss[0m : 2.64902
[1mStep[0m  [144/169], [94mLoss[0m : 2.56674
[1mStep[0m  [160/169], [94mLoss[0m : 2.81880

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.320, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.58088
[1mStep[0m  [16/169], [94mLoss[0m : 2.52764
[1mStep[0m  [32/169], [94mLoss[0m : 2.19373
[1mStep[0m  [48/169], [94mLoss[0m : 1.98770
[1mStep[0m  [64/169], [94mLoss[0m : 2.42101
[1mStep[0m  [80/169], [94mLoss[0m : 2.46155
[1mStep[0m  [96/169], [94mLoss[0m : 2.69460
[1mStep[0m  [112/169], [94mLoss[0m : 2.80924
[1mStep[0m  [128/169], [94mLoss[0m : 2.82593
[1mStep[0m  [144/169], [94mLoss[0m : 2.63999
[1mStep[0m  [160/169], [94mLoss[0m : 2.23402

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.334, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.45574
[1mStep[0m  [16/169], [94mLoss[0m : 2.32657
[1mStep[0m  [32/169], [94mLoss[0m : 2.36060
[1mStep[0m  [48/169], [94mLoss[0m : 2.63261
[1mStep[0m  [64/169], [94mLoss[0m : 2.30804
[1mStep[0m  [80/169], [94mLoss[0m : 2.89274
[1mStep[0m  [96/169], [94mLoss[0m : 2.85856
[1mStep[0m  [112/169], [94mLoss[0m : 2.56237
[1mStep[0m  [128/169], [94mLoss[0m : 2.23174
[1mStep[0m  [144/169], [94mLoss[0m : 2.33187
[1mStep[0m  [160/169], [94mLoss[0m : 1.96043

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.344, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.17251
[1mStep[0m  [16/169], [94mLoss[0m : 2.35422
[1mStep[0m  [32/169], [94mLoss[0m : 2.46669
[1mStep[0m  [48/169], [94mLoss[0m : 2.27939
[1mStep[0m  [64/169], [94mLoss[0m : 2.83156
[1mStep[0m  [80/169], [94mLoss[0m : 3.07984
[1mStep[0m  [96/169], [94mLoss[0m : 2.28419
[1mStep[0m  [112/169], [94mLoss[0m : 2.39576
[1mStep[0m  [128/169], [94mLoss[0m : 2.75777
[1mStep[0m  [144/169], [94mLoss[0m : 2.91486
[1mStep[0m  [160/169], [94mLoss[0m : 2.42632

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.335, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.36503
[1mStep[0m  [16/169], [94mLoss[0m : 2.39741
[1mStep[0m  [32/169], [94mLoss[0m : 2.67297
[1mStep[0m  [48/169], [94mLoss[0m : 2.33383
[1mStep[0m  [64/169], [94mLoss[0m : 2.75235
[1mStep[0m  [80/169], [94mLoss[0m : 2.07842
[1mStep[0m  [96/169], [94mLoss[0m : 2.36814
[1mStep[0m  [112/169], [94mLoss[0m : 2.00048
[1mStep[0m  [128/169], [94mLoss[0m : 2.45592
[1mStep[0m  [144/169], [94mLoss[0m : 2.18269
[1mStep[0m  [160/169], [94mLoss[0m : 2.44901

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.336, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.21786
[1mStep[0m  [16/169], [94mLoss[0m : 2.61117
[1mStep[0m  [32/169], [94mLoss[0m : 2.36782
[1mStep[0m  [48/169], [94mLoss[0m : 2.63344
[1mStep[0m  [64/169], [94mLoss[0m : 2.22345
[1mStep[0m  [80/169], [94mLoss[0m : 2.56397
[1mStep[0m  [96/169], [94mLoss[0m : 2.55001
[1mStep[0m  [112/169], [94mLoss[0m : 2.53570
[1mStep[0m  [128/169], [94mLoss[0m : 2.70198
[1mStep[0m  [144/169], [94mLoss[0m : 2.31529
[1mStep[0m  [160/169], [94mLoss[0m : 2.48240

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.340, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.50500
[1mStep[0m  [16/169], [94mLoss[0m : 2.25404
[1mStep[0m  [32/169], [94mLoss[0m : 2.20586
[1mStep[0m  [48/169], [94mLoss[0m : 2.55313
[1mStep[0m  [64/169], [94mLoss[0m : 2.53432
[1mStep[0m  [80/169], [94mLoss[0m : 2.34357
[1mStep[0m  [96/169], [94mLoss[0m : 2.36616
[1mStep[0m  [112/169], [94mLoss[0m : 2.40736
[1mStep[0m  [128/169], [94mLoss[0m : 2.70989
[1mStep[0m  [144/169], [94mLoss[0m : 2.68610
[1mStep[0m  [160/169], [94mLoss[0m : 2.63276

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.342, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.42323
[1mStep[0m  [16/169], [94mLoss[0m : 3.13108
[1mStep[0m  [32/169], [94mLoss[0m : 2.83872
[1mStep[0m  [48/169], [94mLoss[0m : 2.16825
[1mStep[0m  [64/169], [94mLoss[0m : 2.06837
[1mStep[0m  [80/169], [94mLoss[0m : 2.20519
[1mStep[0m  [96/169], [94mLoss[0m : 2.69931
[1mStep[0m  [112/169], [94mLoss[0m : 2.19574
[1mStep[0m  [128/169], [94mLoss[0m : 2.21997
[1mStep[0m  [144/169], [94mLoss[0m : 2.40718
[1mStep[0m  [160/169], [94mLoss[0m : 3.13305

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.332, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.85562
[1mStep[0m  [16/169], [94mLoss[0m : 2.69271
[1mStep[0m  [32/169], [94mLoss[0m : 2.41710
[1mStep[0m  [48/169], [94mLoss[0m : 2.48694
[1mStep[0m  [64/169], [94mLoss[0m : 2.24461
[1mStep[0m  [80/169], [94mLoss[0m : 2.19572
[1mStep[0m  [96/169], [94mLoss[0m : 2.57817
[1mStep[0m  [112/169], [94mLoss[0m : 2.46892
[1mStep[0m  [128/169], [94mLoss[0m : 2.55880
[1mStep[0m  [144/169], [94mLoss[0m : 2.72710
[1mStep[0m  [160/169], [94mLoss[0m : 2.43418

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.341, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.26645
[1mStep[0m  [16/169], [94mLoss[0m : 2.46716
[1mStep[0m  [32/169], [94mLoss[0m : 2.42081
[1mStep[0m  [48/169], [94mLoss[0m : 2.45306
[1mStep[0m  [64/169], [94mLoss[0m : 2.90546
[1mStep[0m  [80/169], [94mLoss[0m : 2.72272
[1mStep[0m  [96/169], [94mLoss[0m : 2.20857
[1mStep[0m  [112/169], [94mLoss[0m : 2.39412
[1mStep[0m  [128/169], [94mLoss[0m : 2.29460
[1mStep[0m  [144/169], [94mLoss[0m : 2.73296
[1mStep[0m  [160/169], [94mLoss[0m : 2.04999

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.325, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.23334
[1mStep[0m  [16/169], [94mLoss[0m : 2.22805
[1mStep[0m  [32/169], [94mLoss[0m : 2.34946
[1mStep[0m  [48/169], [94mLoss[0m : 2.40859
[1mStep[0m  [64/169], [94mLoss[0m : 2.91230
[1mStep[0m  [80/169], [94mLoss[0m : 2.30895
[1mStep[0m  [96/169], [94mLoss[0m : 2.73996
[1mStep[0m  [112/169], [94mLoss[0m : 2.34173
[1mStep[0m  [128/169], [94mLoss[0m : 2.49029
[1mStep[0m  [144/169], [94mLoss[0m : 2.61399
[1mStep[0m  [160/169], [94mLoss[0m : 2.90223

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.358, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.50362
[1mStep[0m  [16/169], [94mLoss[0m : 2.34019
[1mStep[0m  [32/169], [94mLoss[0m : 2.84213
[1mStep[0m  [48/169], [94mLoss[0m : 2.02148
[1mStep[0m  [64/169], [94mLoss[0m : 2.49536
[1mStep[0m  [80/169], [94mLoss[0m : 2.07994
[1mStep[0m  [96/169], [94mLoss[0m : 2.34430
[1mStep[0m  [112/169], [94mLoss[0m : 2.00454
[1mStep[0m  [128/169], [94mLoss[0m : 2.95525
[1mStep[0m  [144/169], [94mLoss[0m : 2.20447
[1mStep[0m  [160/169], [94mLoss[0m : 2.82042

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.336, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.16753
[1mStep[0m  [16/169], [94mLoss[0m : 2.57281
[1mStep[0m  [32/169], [94mLoss[0m : 2.27577
[1mStep[0m  [48/169], [94mLoss[0m : 2.15957
[1mStep[0m  [64/169], [94mLoss[0m : 2.85778
[1mStep[0m  [80/169], [94mLoss[0m : 2.57636
[1mStep[0m  [96/169], [94mLoss[0m : 2.07869
[1mStep[0m  [112/169], [94mLoss[0m : 2.58430
[1mStep[0m  [128/169], [94mLoss[0m : 2.48802
[1mStep[0m  [144/169], [94mLoss[0m : 2.78862
[1mStep[0m  [160/169], [94mLoss[0m : 2.22165

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.325, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.19874
[1mStep[0m  [16/169], [94mLoss[0m : 2.56343
[1mStep[0m  [32/169], [94mLoss[0m : 2.23317
[1mStep[0m  [48/169], [94mLoss[0m : 2.67538
[1mStep[0m  [64/169], [94mLoss[0m : 2.81416
[1mStep[0m  [80/169], [94mLoss[0m : 2.29100
[1mStep[0m  [96/169], [94mLoss[0m : 2.54842
[1mStep[0m  [112/169], [94mLoss[0m : 2.20361
[1mStep[0m  [128/169], [94mLoss[0m : 2.36996
[1mStep[0m  [144/169], [94mLoss[0m : 2.45828
[1mStep[0m  [160/169], [94mLoss[0m : 2.49950

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.329, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.342
====================================

Phase 1 - Evaluation MAE:  2.3422152187143053
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/169], [94mLoss[0m : 2.06952
[1mStep[0m  [16/169], [94mLoss[0m : 2.31274
[1mStep[0m  [32/169], [94mLoss[0m : 2.56636
[1mStep[0m  [48/169], [94mLoss[0m : 2.16882
[1mStep[0m  [64/169], [94mLoss[0m : 2.32058
[1mStep[0m  [80/169], [94mLoss[0m : 2.28985
[1mStep[0m  [96/169], [94mLoss[0m : 2.45775
[1mStep[0m  [112/169], [94mLoss[0m : 2.39044
[1mStep[0m  [128/169], [94mLoss[0m : 2.86349
[1mStep[0m  [144/169], [94mLoss[0m : 2.42743
[1mStep[0m  [160/169], [94mLoss[0m : 2.34237

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.341, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.33782
[1mStep[0m  [16/169], [94mLoss[0m : 2.24774
[1mStep[0m  [32/169], [94mLoss[0m : 2.58524
[1mStep[0m  [48/169], [94mLoss[0m : 2.52309
[1mStep[0m  [64/169], [94mLoss[0m : 3.08463
[1mStep[0m  [80/169], [94mLoss[0m : 2.52011
[1mStep[0m  [96/169], [94mLoss[0m : 1.83178
[1mStep[0m  [112/169], [94mLoss[0m : 2.24578
[1mStep[0m  [128/169], [94mLoss[0m : 1.98067
[1mStep[0m  [144/169], [94mLoss[0m : 2.12899
[1mStep[0m  [160/169], [94mLoss[0m : 2.25367

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.392, [92mTest[0m: 2.365, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.72825
[1mStep[0m  [16/169], [94mLoss[0m : 1.95384
[1mStep[0m  [32/169], [94mLoss[0m : 2.70122
[1mStep[0m  [48/169], [94mLoss[0m : 2.39430
[1mStep[0m  [64/169], [94mLoss[0m : 2.61510
[1mStep[0m  [80/169], [94mLoss[0m : 2.19658
[1mStep[0m  [96/169], [94mLoss[0m : 2.34592
[1mStep[0m  [112/169], [94mLoss[0m : 2.43456
[1mStep[0m  [128/169], [94mLoss[0m : 2.19153
[1mStep[0m  [144/169], [94mLoss[0m : 2.06823
[1mStep[0m  [160/169], [94mLoss[0m : 1.54650

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.322, [92mTest[0m: 2.353, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.40594
[1mStep[0m  [16/169], [94mLoss[0m : 2.55042
[1mStep[0m  [32/169], [94mLoss[0m : 1.92845
[1mStep[0m  [48/169], [94mLoss[0m : 2.02313
[1mStep[0m  [64/169], [94mLoss[0m : 2.20554
[1mStep[0m  [80/169], [94mLoss[0m : 2.49499
[1mStep[0m  [96/169], [94mLoss[0m : 2.41130
[1mStep[0m  [112/169], [94mLoss[0m : 2.22029
[1mStep[0m  [128/169], [94mLoss[0m : 2.48704
[1mStep[0m  [144/169], [94mLoss[0m : 2.85425
[1mStep[0m  [160/169], [94mLoss[0m : 2.11496

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.282, [92mTest[0m: 2.354, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.50878
[1mStep[0m  [16/169], [94mLoss[0m : 1.69690
[1mStep[0m  [32/169], [94mLoss[0m : 2.28157
[1mStep[0m  [48/169], [94mLoss[0m : 2.01663
[1mStep[0m  [64/169], [94mLoss[0m : 2.30721
[1mStep[0m  [80/169], [94mLoss[0m : 2.35194
[1mStep[0m  [96/169], [94mLoss[0m : 2.09736
[1mStep[0m  [112/169], [94mLoss[0m : 2.46024
[1mStep[0m  [128/169], [94mLoss[0m : 2.35982
[1mStep[0m  [144/169], [94mLoss[0m : 2.03646
[1mStep[0m  [160/169], [94mLoss[0m : 2.22824

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.215, [92mTest[0m: 2.373, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.11948
[1mStep[0m  [16/169], [94mLoss[0m : 2.17308
[1mStep[0m  [32/169], [94mLoss[0m : 2.16729
[1mStep[0m  [48/169], [94mLoss[0m : 2.47161
[1mStep[0m  [64/169], [94mLoss[0m : 2.02494
[1mStep[0m  [80/169], [94mLoss[0m : 2.48663
[1mStep[0m  [96/169], [94mLoss[0m : 2.10321
[1mStep[0m  [112/169], [94mLoss[0m : 2.30454
[1mStep[0m  [128/169], [94mLoss[0m : 2.30191
[1mStep[0m  [144/169], [94mLoss[0m : 2.05445
[1mStep[0m  [160/169], [94mLoss[0m : 2.09935

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.164, [92mTest[0m: 2.404, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.89364
[1mStep[0m  [16/169], [94mLoss[0m : 2.06243
[1mStep[0m  [32/169], [94mLoss[0m : 2.25443
[1mStep[0m  [48/169], [94mLoss[0m : 1.93032
[1mStep[0m  [64/169], [94mLoss[0m : 2.21814
[1mStep[0m  [80/169], [94mLoss[0m : 2.34832
[1mStep[0m  [96/169], [94mLoss[0m : 2.04879
[1mStep[0m  [112/169], [94mLoss[0m : 2.36759
[1mStep[0m  [128/169], [94mLoss[0m : 1.77851
[1mStep[0m  [144/169], [94mLoss[0m : 2.29930
[1mStep[0m  [160/169], [94mLoss[0m : 1.84263

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.104, [92mTest[0m: 2.387, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.90895
[1mStep[0m  [16/169], [94mLoss[0m : 2.05423
[1mStep[0m  [32/169], [94mLoss[0m : 2.10218
[1mStep[0m  [48/169], [94mLoss[0m : 2.63678
[1mStep[0m  [64/169], [94mLoss[0m : 2.55847
[1mStep[0m  [80/169], [94mLoss[0m : 2.05168
[1mStep[0m  [96/169], [94mLoss[0m : 2.30524
[1mStep[0m  [112/169], [94mLoss[0m : 2.09068
[1mStep[0m  [128/169], [94mLoss[0m : 1.58965
[1mStep[0m  [144/169], [94mLoss[0m : 1.95418
[1mStep[0m  [160/169], [94mLoss[0m : 2.35027

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.049, [92mTest[0m: 2.437, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.12728
[1mStep[0m  [16/169], [94mLoss[0m : 2.13666
[1mStep[0m  [32/169], [94mLoss[0m : 1.81922
[1mStep[0m  [48/169], [94mLoss[0m : 2.35096
[1mStep[0m  [64/169], [94mLoss[0m : 1.88957
[1mStep[0m  [80/169], [94mLoss[0m : 1.95090
[1mStep[0m  [96/169], [94mLoss[0m : 1.99468
[1mStep[0m  [112/169], [94mLoss[0m : 1.77296
[1mStep[0m  [128/169], [94mLoss[0m : 1.44031
[1mStep[0m  [144/169], [94mLoss[0m : 2.36560
[1mStep[0m  [160/169], [94mLoss[0m : 2.03007

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.001, [92mTest[0m: 2.430, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.16008
[1mStep[0m  [16/169], [94mLoss[0m : 1.50064
[1mStep[0m  [32/169], [94mLoss[0m : 1.89655
[1mStep[0m  [48/169], [94mLoss[0m : 1.86881
[1mStep[0m  [64/169], [94mLoss[0m : 2.00462
[1mStep[0m  [80/169], [94mLoss[0m : 2.11397
[1mStep[0m  [96/169], [94mLoss[0m : 1.98596
[1mStep[0m  [112/169], [94mLoss[0m : 1.85651
[1mStep[0m  [128/169], [94mLoss[0m : 2.15052
[1mStep[0m  [144/169], [94mLoss[0m : 1.97907
[1mStep[0m  [160/169], [94mLoss[0m : 1.62023

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.959, [92mTest[0m: 2.421, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.09776
[1mStep[0m  [16/169], [94mLoss[0m : 1.77257
[1mStep[0m  [32/169], [94mLoss[0m : 1.92617
[1mStep[0m  [48/169], [94mLoss[0m : 2.17760
[1mStep[0m  [64/169], [94mLoss[0m : 1.98308
[1mStep[0m  [80/169], [94mLoss[0m : 2.07002
[1mStep[0m  [96/169], [94mLoss[0m : 2.08373
[1mStep[0m  [112/169], [94mLoss[0m : 2.34725
[1mStep[0m  [128/169], [94mLoss[0m : 2.30604
[1mStep[0m  [144/169], [94mLoss[0m : 1.59064
[1mStep[0m  [160/169], [94mLoss[0m : 1.89647

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.909, [92mTest[0m: 2.431, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.75517
[1mStep[0m  [16/169], [94mLoss[0m : 1.52301
[1mStep[0m  [32/169], [94mLoss[0m : 1.83629
[1mStep[0m  [48/169], [94mLoss[0m : 1.90850
[1mStep[0m  [64/169], [94mLoss[0m : 1.69035
[1mStep[0m  [80/169], [94mLoss[0m : 1.76438
[1mStep[0m  [96/169], [94mLoss[0m : 1.73779
[1mStep[0m  [112/169], [94mLoss[0m : 1.65714
[1mStep[0m  [128/169], [94mLoss[0m : 1.88517
[1mStep[0m  [144/169], [94mLoss[0m : 2.19583
[1mStep[0m  [160/169], [94mLoss[0m : 1.72498

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.867, [92mTest[0m: 2.439, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.66989
[1mStep[0m  [16/169], [94mLoss[0m : 2.43337
[1mStep[0m  [32/169], [94mLoss[0m : 1.68110
[1mStep[0m  [48/169], [94mLoss[0m : 1.45367
[1mStep[0m  [64/169], [94mLoss[0m : 1.79558
[1mStep[0m  [80/169], [94mLoss[0m : 1.71127
[1mStep[0m  [96/169], [94mLoss[0m : 1.61061
[1mStep[0m  [112/169], [94mLoss[0m : 1.98165
[1mStep[0m  [128/169], [94mLoss[0m : 1.72871
[1mStep[0m  [144/169], [94mLoss[0m : 1.91717
[1mStep[0m  [160/169], [94mLoss[0m : 1.83744

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.831, [92mTest[0m: 2.427, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.76386
[1mStep[0m  [16/169], [94mLoss[0m : 1.46503
[1mStep[0m  [32/169], [94mLoss[0m : 1.64761
[1mStep[0m  [48/169], [94mLoss[0m : 1.84530
[1mStep[0m  [64/169], [94mLoss[0m : 1.49761
[1mStep[0m  [80/169], [94mLoss[0m : 2.23880
[1mStep[0m  [96/169], [94mLoss[0m : 1.81383
[1mStep[0m  [112/169], [94mLoss[0m : 1.55123
[1mStep[0m  [128/169], [94mLoss[0m : 1.62007
[1mStep[0m  [144/169], [94mLoss[0m : 1.88524
[1mStep[0m  [160/169], [94mLoss[0m : 1.60671

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.791, [92mTest[0m: 2.443, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.62428
[1mStep[0m  [16/169], [94mLoss[0m : 1.93780
[1mStep[0m  [32/169], [94mLoss[0m : 1.68235
[1mStep[0m  [48/169], [94mLoss[0m : 1.95446
[1mStep[0m  [64/169], [94mLoss[0m : 1.64400
[1mStep[0m  [80/169], [94mLoss[0m : 1.79366
[1mStep[0m  [96/169], [94mLoss[0m : 1.84347
[1mStep[0m  [112/169], [94mLoss[0m : 1.84967
[1mStep[0m  [128/169], [94mLoss[0m : 2.09593
[1mStep[0m  [144/169], [94mLoss[0m : 1.71004
[1mStep[0m  [160/169], [94mLoss[0m : 2.08188

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.746, [92mTest[0m: 2.501, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.84786
[1mStep[0m  [16/169], [94mLoss[0m : 2.04087
[1mStep[0m  [32/169], [94mLoss[0m : 1.31252
[1mStep[0m  [48/169], [94mLoss[0m : 1.58373
[1mStep[0m  [64/169], [94mLoss[0m : 1.63083
[1mStep[0m  [80/169], [94mLoss[0m : 1.71841
[1mStep[0m  [96/169], [94mLoss[0m : 2.03169
[1mStep[0m  [112/169], [94mLoss[0m : 1.95602
[1mStep[0m  [128/169], [94mLoss[0m : 1.81197
[1mStep[0m  [144/169], [94mLoss[0m : 1.71231
[1mStep[0m  [160/169], [94mLoss[0m : 1.74070

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.717, [92mTest[0m: 2.487, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.93919
[1mStep[0m  [16/169], [94mLoss[0m : 1.32765
[1mStep[0m  [32/169], [94mLoss[0m : 1.93865
[1mStep[0m  [48/169], [94mLoss[0m : 1.80748
[1mStep[0m  [64/169], [94mLoss[0m : 1.47325
[1mStep[0m  [80/169], [94mLoss[0m : 1.53421
[1mStep[0m  [96/169], [94mLoss[0m : 1.71877
[1mStep[0m  [112/169], [94mLoss[0m : 1.88731
[1mStep[0m  [128/169], [94mLoss[0m : 1.75847
[1mStep[0m  [144/169], [94mLoss[0m : 2.36226
[1mStep[0m  [160/169], [94mLoss[0m : 1.96772

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.692, [92mTest[0m: 2.486, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.70377
[1mStep[0m  [16/169], [94mLoss[0m : 1.66532
[1mStep[0m  [32/169], [94mLoss[0m : 1.82424
[1mStep[0m  [48/169], [94mLoss[0m : 1.75358
[1mStep[0m  [64/169], [94mLoss[0m : 1.61368
[1mStep[0m  [80/169], [94mLoss[0m : 1.96189
[1mStep[0m  [96/169], [94mLoss[0m : 1.49424
[1mStep[0m  [112/169], [94mLoss[0m : 1.95369
[1mStep[0m  [128/169], [94mLoss[0m : 1.78210
[1mStep[0m  [144/169], [94mLoss[0m : 1.57177
[1mStep[0m  [160/169], [94mLoss[0m : 1.56553

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.654, [92mTest[0m: 2.485, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.68000
[1mStep[0m  [16/169], [94mLoss[0m : 1.19211
[1mStep[0m  [32/169], [94mLoss[0m : 1.65398
[1mStep[0m  [48/169], [94mLoss[0m : 1.54201
[1mStep[0m  [64/169], [94mLoss[0m : 1.22470
[1mStep[0m  [80/169], [94mLoss[0m : 1.77531
[1mStep[0m  [96/169], [94mLoss[0m : 1.85342
[1mStep[0m  [112/169], [94mLoss[0m : 1.54350
[1mStep[0m  [128/169], [94mLoss[0m : 1.70328
[1mStep[0m  [144/169], [94mLoss[0m : 1.59897
[1mStep[0m  [160/169], [94mLoss[0m : 1.68893

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.613, [92mTest[0m: 2.459, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.57646
[1mStep[0m  [16/169], [94mLoss[0m : 1.35461
[1mStep[0m  [32/169], [94mLoss[0m : 1.36320
[1mStep[0m  [48/169], [94mLoss[0m : 1.84735
[1mStep[0m  [64/169], [94mLoss[0m : 1.65975
[1mStep[0m  [80/169], [94mLoss[0m : 1.69671
[1mStep[0m  [96/169], [94mLoss[0m : 1.89377
[1mStep[0m  [112/169], [94mLoss[0m : 1.59188
[1mStep[0m  [128/169], [94mLoss[0m : 1.51160
[1mStep[0m  [144/169], [94mLoss[0m : 1.73005
[1mStep[0m  [160/169], [94mLoss[0m : 1.77245

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.602, [92mTest[0m: 2.484, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.55181
[1mStep[0m  [16/169], [94mLoss[0m : 1.49017
[1mStep[0m  [32/169], [94mLoss[0m : 1.61928
[1mStep[0m  [48/169], [94mLoss[0m : 1.56287
[1mStep[0m  [64/169], [94mLoss[0m : 1.62535
[1mStep[0m  [80/169], [94mLoss[0m : 1.49601
[1mStep[0m  [96/169], [94mLoss[0m : 1.63432
[1mStep[0m  [112/169], [94mLoss[0m : 1.84304
[1mStep[0m  [128/169], [94mLoss[0m : 1.61771
[1mStep[0m  [144/169], [94mLoss[0m : 1.38426
[1mStep[0m  [160/169], [94mLoss[0m : 1.44195

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.568, [92mTest[0m: 2.475, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.39463
[1mStep[0m  [16/169], [94mLoss[0m : 1.46516
[1mStep[0m  [32/169], [94mLoss[0m : 1.45146
[1mStep[0m  [48/169], [94mLoss[0m : 1.59377
[1mStep[0m  [64/169], [94mLoss[0m : 1.32065
[1mStep[0m  [80/169], [94mLoss[0m : 1.77858
[1mStep[0m  [96/169], [94mLoss[0m : 1.40083
[1mStep[0m  [112/169], [94mLoss[0m : 1.70337
[1mStep[0m  [128/169], [94mLoss[0m : 1.50242
[1mStep[0m  [144/169], [94mLoss[0m : 1.61047
[1mStep[0m  [160/169], [94mLoss[0m : 1.17471

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.535, [92mTest[0m: 2.481, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.29312
[1mStep[0m  [16/169], [94mLoss[0m : 1.28690
[1mStep[0m  [32/169], [94mLoss[0m : 1.19049
[1mStep[0m  [48/169], [94mLoss[0m : 1.54820
[1mStep[0m  [64/169], [94mLoss[0m : 1.47032
[1mStep[0m  [80/169], [94mLoss[0m : 1.54484
[1mStep[0m  [96/169], [94mLoss[0m : 1.55758
[1mStep[0m  [112/169], [94mLoss[0m : 1.73239
[1mStep[0m  [128/169], [94mLoss[0m : 1.62326
[1mStep[0m  [144/169], [94mLoss[0m : 1.72364
[1mStep[0m  [160/169], [94mLoss[0m : 1.32012

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.528, [92mTest[0m: 2.473, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.35037
[1mStep[0m  [16/169], [94mLoss[0m : 1.65383
[1mStep[0m  [32/169], [94mLoss[0m : 1.34666
[1mStep[0m  [48/169], [94mLoss[0m : 1.41468
[1mStep[0m  [64/169], [94mLoss[0m : 1.49135
[1mStep[0m  [80/169], [94mLoss[0m : 1.89677
[1mStep[0m  [96/169], [94mLoss[0m : 1.57564
[1mStep[0m  [112/169], [94mLoss[0m : 1.35127
[1mStep[0m  [128/169], [94mLoss[0m : 1.16150
[1mStep[0m  [144/169], [94mLoss[0m : 1.63468
[1mStep[0m  [160/169], [94mLoss[0m : 1.42842

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.495, [92mTest[0m: 2.449, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.40918
[1mStep[0m  [16/169], [94mLoss[0m : 1.51743
[1mStep[0m  [32/169], [94mLoss[0m : 1.34996
[1mStep[0m  [48/169], [94mLoss[0m : 1.66408
[1mStep[0m  [64/169], [94mLoss[0m : 1.42488
[1mStep[0m  [80/169], [94mLoss[0m : 1.59657
[1mStep[0m  [96/169], [94mLoss[0m : 1.30171
[1mStep[0m  [112/169], [94mLoss[0m : 1.64867
[1mStep[0m  [128/169], [94mLoss[0m : 1.22813
[1mStep[0m  [144/169], [94mLoss[0m : 1.82030
[1mStep[0m  [160/169], [94mLoss[0m : 1.67887

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.461, [92mTest[0m: 2.463, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.60964
[1mStep[0m  [16/169], [94mLoss[0m : 1.39729
[1mStep[0m  [32/169], [94mLoss[0m : 1.13653
[1mStep[0m  [48/169], [94mLoss[0m : 1.72591
[1mStep[0m  [64/169], [94mLoss[0m : 1.42462
[1mStep[0m  [80/169], [94mLoss[0m : 1.41642
[1mStep[0m  [96/169], [94mLoss[0m : 1.56187
[1mStep[0m  [112/169], [94mLoss[0m : 1.52138
[1mStep[0m  [128/169], [94mLoss[0m : 1.31242
[1mStep[0m  [144/169], [94mLoss[0m : 1.50655
[1mStep[0m  [160/169], [94mLoss[0m : 1.84537

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.458, [92mTest[0m: 2.536, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.54946
[1mStep[0m  [16/169], [94mLoss[0m : 1.18834
[1mStep[0m  [32/169], [94mLoss[0m : 1.37912
[1mStep[0m  [48/169], [94mLoss[0m : 1.43558
[1mStep[0m  [64/169], [94mLoss[0m : 1.59417
[1mStep[0m  [80/169], [94mLoss[0m : 1.57426
[1mStep[0m  [96/169], [94mLoss[0m : 1.55363
[1mStep[0m  [112/169], [94mLoss[0m : 1.22579
[1mStep[0m  [128/169], [94mLoss[0m : 1.48911
[1mStep[0m  [144/169], [94mLoss[0m : 1.36958
[1mStep[0m  [160/169], [94mLoss[0m : 1.34779

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.419, [92mTest[0m: 2.477, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.50690
[1mStep[0m  [16/169], [94mLoss[0m : 1.47625
[1mStep[0m  [32/169], [94mLoss[0m : 1.40183
[1mStep[0m  [48/169], [94mLoss[0m : 1.36458
[1mStep[0m  [64/169], [94mLoss[0m : 1.32757
[1mStep[0m  [80/169], [94mLoss[0m : 0.99354
[1mStep[0m  [96/169], [94mLoss[0m : 1.38574
[1mStep[0m  [112/169], [94mLoss[0m : 1.70832
[1mStep[0m  [128/169], [94mLoss[0m : 1.64657
[1mStep[0m  [144/169], [94mLoss[0m : 1.56410
[1mStep[0m  [160/169], [94mLoss[0m : 1.42390

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.421, [92mTest[0m: 2.536, [96mlr[0m: 0.004545
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 27 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.512
====================================

Phase 2 - Evaluation MAE:  2.511539305959429
MAE score P1      2.342215
MAE score P2      2.511539
loss              1.419209
learning_rate      0.00505
batch_size              64
hidden_sizes         [100]
epochs                  30
activation            tanh
optimizer              sgd
early stopping        True
dropout                0.4
momentum               0.5
weight_decay        0.0001
Name: 6, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/169], [94mLoss[0m : 11.16604
[1mStep[0m  [16/169], [94mLoss[0m : 7.36840
[1mStep[0m  [32/169], [94mLoss[0m : 3.18798
[1mStep[0m  [48/169], [94mLoss[0m : 2.80082
[1mStep[0m  [64/169], [94mLoss[0m : 3.27281
[1mStep[0m  [80/169], [94mLoss[0m : 3.29390
[1mStep[0m  [96/169], [94mLoss[0m : 2.99462
[1mStep[0m  [112/169], [94mLoss[0m : 2.53156
[1mStep[0m  [128/169], [94mLoss[0m : 3.56622
[1mStep[0m  [144/169], [94mLoss[0m : 2.97236
[1mStep[0m  [160/169], [94mLoss[0m : 2.59848

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.650, [92mTest[0m: 10.984, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.58377
[1mStep[0m  [16/169], [94mLoss[0m : 2.64624
[1mStep[0m  [32/169], [94mLoss[0m : 2.31586
[1mStep[0m  [48/169], [94mLoss[0m : 3.00819
[1mStep[0m  [64/169], [94mLoss[0m : 2.66342
[1mStep[0m  [80/169], [94mLoss[0m : 2.78248
[1mStep[0m  [96/169], [94mLoss[0m : 2.42751
[1mStep[0m  [112/169], [94mLoss[0m : 2.47889
[1mStep[0m  [128/169], [94mLoss[0m : 2.35361
[1mStep[0m  [144/169], [94mLoss[0m : 3.27816
[1mStep[0m  [160/169], [94mLoss[0m : 2.76643

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.718, [92mTest[0m: 2.433, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.98380
[1mStep[0m  [16/169], [94mLoss[0m : 2.08619
[1mStep[0m  [32/169], [94mLoss[0m : 2.54024
[1mStep[0m  [48/169], [94mLoss[0m : 2.71355
[1mStep[0m  [64/169], [94mLoss[0m : 2.64161
[1mStep[0m  [80/169], [94mLoss[0m : 2.20449
[1mStep[0m  [96/169], [94mLoss[0m : 2.42925
[1mStep[0m  [112/169], [94mLoss[0m : 2.21300
[1mStep[0m  [128/169], [94mLoss[0m : 2.54189
[1mStep[0m  [144/169], [94mLoss[0m : 2.60594
[1mStep[0m  [160/169], [94mLoss[0m : 2.57871

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.604, [92mTest[0m: 2.391, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.77787
[1mStep[0m  [16/169], [94mLoss[0m : 2.45310
[1mStep[0m  [32/169], [94mLoss[0m : 2.76367
[1mStep[0m  [48/169], [94mLoss[0m : 2.48592
[1mStep[0m  [64/169], [94mLoss[0m : 2.47782
[1mStep[0m  [80/169], [94mLoss[0m : 3.03381
[1mStep[0m  [96/169], [94mLoss[0m : 2.26404
[1mStep[0m  [112/169], [94mLoss[0m : 2.59371
[1mStep[0m  [128/169], [94mLoss[0m : 2.75618
[1mStep[0m  [144/169], [94mLoss[0m : 2.42108
[1mStep[0m  [160/169], [94mLoss[0m : 2.21157

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.610, [92mTest[0m: 2.408, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.56598
[1mStep[0m  [16/169], [94mLoss[0m : 2.41684
[1mStep[0m  [32/169], [94mLoss[0m : 2.37307
[1mStep[0m  [48/169], [94mLoss[0m : 2.54345
[1mStep[0m  [64/169], [94mLoss[0m : 2.52608
[1mStep[0m  [80/169], [94mLoss[0m : 2.72157
[1mStep[0m  [96/169], [94mLoss[0m : 2.85756
[1mStep[0m  [112/169], [94mLoss[0m : 2.66153
[1mStep[0m  [128/169], [94mLoss[0m : 2.54632
[1mStep[0m  [144/169], [94mLoss[0m : 2.68893
[1mStep[0m  [160/169], [94mLoss[0m : 2.47805

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.526, [92mTest[0m: 2.418, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.20788
[1mStep[0m  [16/169], [94mLoss[0m : 2.60217
[1mStep[0m  [32/169], [94mLoss[0m : 2.62987
[1mStep[0m  [48/169], [94mLoss[0m : 2.85009
[1mStep[0m  [64/169], [94mLoss[0m : 2.65007
[1mStep[0m  [80/169], [94mLoss[0m : 2.48912
[1mStep[0m  [96/169], [94mLoss[0m : 2.82213
[1mStep[0m  [112/169], [94mLoss[0m : 3.01840
[1mStep[0m  [128/169], [94mLoss[0m : 2.37842
[1mStep[0m  [144/169], [94mLoss[0m : 2.35202
[1mStep[0m  [160/169], [94mLoss[0m : 2.22683

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.402, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.34466
[1mStep[0m  [16/169], [94mLoss[0m : 3.08315
[1mStep[0m  [32/169], [94mLoss[0m : 2.61447
[1mStep[0m  [48/169], [94mLoss[0m : 2.31422
[1mStep[0m  [64/169], [94mLoss[0m : 2.10454
[1mStep[0m  [80/169], [94mLoss[0m : 2.54024
[1mStep[0m  [96/169], [94mLoss[0m : 2.19141
[1mStep[0m  [112/169], [94mLoss[0m : 1.96991
[1mStep[0m  [128/169], [94mLoss[0m : 2.51682
[1mStep[0m  [144/169], [94mLoss[0m : 2.58401
[1mStep[0m  [160/169], [94mLoss[0m : 2.63221

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.524, [92mTest[0m: 2.325, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.62856
[1mStep[0m  [16/169], [94mLoss[0m : 2.46309
[1mStep[0m  [32/169], [94mLoss[0m : 2.47165
[1mStep[0m  [48/169], [94mLoss[0m : 2.36413
[1mStep[0m  [64/169], [94mLoss[0m : 2.63079
[1mStep[0m  [80/169], [94mLoss[0m : 2.37614
[1mStep[0m  [96/169], [94mLoss[0m : 2.76430
[1mStep[0m  [112/169], [94mLoss[0m : 2.46623
[1mStep[0m  [128/169], [94mLoss[0m : 2.46184
[1mStep[0m  [144/169], [94mLoss[0m : 2.16236
[1mStep[0m  [160/169], [94mLoss[0m : 2.67285

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.342, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.42549
[1mStep[0m  [16/169], [94mLoss[0m : 2.38601
[1mStep[0m  [32/169], [94mLoss[0m : 2.22067
[1mStep[0m  [48/169], [94mLoss[0m : 2.23388
[1mStep[0m  [64/169], [94mLoss[0m : 2.30186
[1mStep[0m  [80/169], [94mLoss[0m : 2.74795
[1mStep[0m  [96/169], [94mLoss[0m : 2.85464
[1mStep[0m  [112/169], [94mLoss[0m : 2.89730
[1mStep[0m  [128/169], [94mLoss[0m : 2.30403
[1mStep[0m  [144/169], [94mLoss[0m : 2.50839
[1mStep[0m  [160/169], [94mLoss[0m : 2.55126

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.343, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.45564
[1mStep[0m  [16/169], [94mLoss[0m : 3.19821
[1mStep[0m  [32/169], [94mLoss[0m : 3.12347
[1mStep[0m  [48/169], [94mLoss[0m : 2.79501
[1mStep[0m  [64/169], [94mLoss[0m : 2.20399
[1mStep[0m  [80/169], [94mLoss[0m : 2.12443
[1mStep[0m  [96/169], [94mLoss[0m : 2.29572
[1mStep[0m  [112/169], [94mLoss[0m : 2.76731
[1mStep[0m  [128/169], [94mLoss[0m : 2.44834
[1mStep[0m  [144/169], [94mLoss[0m : 2.58179
[1mStep[0m  [160/169], [94mLoss[0m : 2.63338

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.344, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.53865
[1mStep[0m  [16/169], [94mLoss[0m : 2.19598
[1mStep[0m  [32/169], [94mLoss[0m : 2.52957
[1mStep[0m  [48/169], [94mLoss[0m : 2.57140
[1mStep[0m  [64/169], [94mLoss[0m : 2.34154
[1mStep[0m  [80/169], [94mLoss[0m : 2.48190
[1mStep[0m  [96/169], [94mLoss[0m : 2.19921
[1mStep[0m  [112/169], [94mLoss[0m : 2.19865
[1mStep[0m  [128/169], [94mLoss[0m : 2.65548
[1mStep[0m  [144/169], [94mLoss[0m : 2.74385
[1mStep[0m  [160/169], [94mLoss[0m : 2.29362

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.316, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.45558
[1mStep[0m  [16/169], [94mLoss[0m : 2.63671
[1mStep[0m  [32/169], [94mLoss[0m : 2.14813
[1mStep[0m  [48/169], [94mLoss[0m : 2.25120
[1mStep[0m  [64/169], [94mLoss[0m : 2.63091
[1mStep[0m  [80/169], [94mLoss[0m : 2.69935
[1mStep[0m  [96/169], [94mLoss[0m : 2.62307
[1mStep[0m  [112/169], [94mLoss[0m : 2.31398
[1mStep[0m  [128/169], [94mLoss[0m : 2.72101
[1mStep[0m  [144/169], [94mLoss[0m : 2.00320
[1mStep[0m  [160/169], [94mLoss[0m : 2.74942

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.336, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.21163
[1mStep[0m  [16/169], [94mLoss[0m : 2.66061
[1mStep[0m  [32/169], [94mLoss[0m : 2.40221
[1mStep[0m  [48/169], [94mLoss[0m : 2.42301
[1mStep[0m  [64/169], [94mLoss[0m : 2.34090
[1mStep[0m  [80/169], [94mLoss[0m : 2.40008
[1mStep[0m  [96/169], [94mLoss[0m : 2.39824
[1mStep[0m  [112/169], [94mLoss[0m : 2.13016
[1mStep[0m  [128/169], [94mLoss[0m : 2.12542
[1mStep[0m  [144/169], [94mLoss[0m : 2.78317
[1mStep[0m  [160/169], [94mLoss[0m : 2.28813

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.326, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.43038
[1mStep[0m  [16/169], [94mLoss[0m : 2.40704
[1mStep[0m  [32/169], [94mLoss[0m : 2.31755
[1mStep[0m  [48/169], [94mLoss[0m : 2.63702
[1mStep[0m  [64/169], [94mLoss[0m : 2.88940
[1mStep[0m  [80/169], [94mLoss[0m : 2.36405
[1mStep[0m  [96/169], [94mLoss[0m : 2.58398
[1mStep[0m  [112/169], [94mLoss[0m : 2.72667
[1mStep[0m  [128/169], [94mLoss[0m : 2.18943
[1mStep[0m  [144/169], [94mLoss[0m : 1.62851
[1mStep[0m  [160/169], [94mLoss[0m : 2.27805

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.310, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.38093
[1mStep[0m  [16/169], [94mLoss[0m : 2.45867
[1mStep[0m  [32/169], [94mLoss[0m : 2.36814
[1mStep[0m  [48/169], [94mLoss[0m : 2.31107
[1mStep[0m  [64/169], [94mLoss[0m : 2.23879
[1mStep[0m  [80/169], [94mLoss[0m : 2.75192
[1mStep[0m  [96/169], [94mLoss[0m : 2.42326
[1mStep[0m  [112/169], [94mLoss[0m : 2.51265
[1mStep[0m  [128/169], [94mLoss[0m : 2.30311
[1mStep[0m  [144/169], [94mLoss[0m : 2.77424
[1mStep[0m  [160/169], [94mLoss[0m : 2.31065

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.345, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.56051
[1mStep[0m  [16/169], [94mLoss[0m : 2.40302
[1mStep[0m  [32/169], [94mLoss[0m : 2.70164
[1mStep[0m  [48/169], [94mLoss[0m : 2.29398
[1mStep[0m  [64/169], [94mLoss[0m : 2.42076
[1mStep[0m  [80/169], [94mLoss[0m : 2.46994
[1mStep[0m  [96/169], [94mLoss[0m : 2.75130
[1mStep[0m  [112/169], [94mLoss[0m : 2.91908
[1mStep[0m  [128/169], [94mLoss[0m : 2.27484
[1mStep[0m  [144/169], [94mLoss[0m : 2.50083
[1mStep[0m  [160/169], [94mLoss[0m : 2.60719

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.315, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.63085
[1mStep[0m  [16/169], [94mLoss[0m : 2.60805
[1mStep[0m  [32/169], [94mLoss[0m : 2.51166
[1mStep[0m  [48/169], [94mLoss[0m : 1.92842
[1mStep[0m  [64/169], [94mLoss[0m : 2.31171
[1mStep[0m  [80/169], [94mLoss[0m : 2.36888
[1mStep[0m  [96/169], [94mLoss[0m : 2.72899
[1mStep[0m  [112/169], [94mLoss[0m : 2.54566
[1mStep[0m  [128/169], [94mLoss[0m : 2.36765
[1mStep[0m  [144/169], [94mLoss[0m : 2.32273
[1mStep[0m  [160/169], [94mLoss[0m : 1.92853

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.301, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.22426
[1mStep[0m  [16/169], [94mLoss[0m : 2.73964
[1mStep[0m  [32/169], [94mLoss[0m : 2.09934
[1mStep[0m  [48/169], [94mLoss[0m : 2.39929
[1mStep[0m  [64/169], [94mLoss[0m : 2.21794
[1mStep[0m  [80/169], [94mLoss[0m : 2.32031
[1mStep[0m  [96/169], [94mLoss[0m : 2.56891
[1mStep[0m  [112/169], [94mLoss[0m : 2.55612
[1mStep[0m  [128/169], [94mLoss[0m : 2.15067
[1mStep[0m  [144/169], [94mLoss[0m : 2.20833
[1mStep[0m  [160/169], [94mLoss[0m : 2.59966

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.293, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.53095
[1mStep[0m  [16/169], [94mLoss[0m : 2.03731
[1mStep[0m  [32/169], [94mLoss[0m : 2.75376
[1mStep[0m  [48/169], [94mLoss[0m : 2.62447
[1mStep[0m  [64/169], [94mLoss[0m : 2.11845
[1mStep[0m  [80/169], [94mLoss[0m : 2.69101
[1mStep[0m  [96/169], [94mLoss[0m : 2.18571
[1mStep[0m  [112/169], [94mLoss[0m : 2.53848
[1mStep[0m  [128/169], [94mLoss[0m : 2.38989
[1mStep[0m  [144/169], [94mLoss[0m : 2.08876
[1mStep[0m  [160/169], [94mLoss[0m : 2.45429

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.323, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.89415
[1mStep[0m  [16/169], [94mLoss[0m : 2.13220
[1mStep[0m  [32/169], [94mLoss[0m : 2.30877
[1mStep[0m  [48/169], [94mLoss[0m : 2.29726
[1mStep[0m  [64/169], [94mLoss[0m : 2.35830
[1mStep[0m  [80/169], [94mLoss[0m : 2.32142
[1mStep[0m  [96/169], [94mLoss[0m : 2.32838
[1mStep[0m  [112/169], [94mLoss[0m : 2.33436
[1mStep[0m  [128/169], [94mLoss[0m : 2.22462
[1mStep[0m  [144/169], [94mLoss[0m : 2.39602
[1mStep[0m  [160/169], [94mLoss[0m : 2.80459

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.343, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.18465
[1mStep[0m  [16/169], [94mLoss[0m : 2.32733
[1mStep[0m  [32/169], [94mLoss[0m : 2.64803
[1mStep[0m  [48/169], [94mLoss[0m : 1.63276
[1mStep[0m  [64/169], [94mLoss[0m : 2.86025
[1mStep[0m  [80/169], [94mLoss[0m : 2.17339
[1mStep[0m  [96/169], [94mLoss[0m : 2.64589
[1mStep[0m  [112/169], [94mLoss[0m : 2.58537
[1mStep[0m  [128/169], [94mLoss[0m : 2.63506
[1mStep[0m  [144/169], [94mLoss[0m : 2.19731
[1mStep[0m  [160/169], [94mLoss[0m : 2.51036

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.334, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.23400
[1mStep[0m  [16/169], [94mLoss[0m : 2.26843
[1mStep[0m  [32/169], [94mLoss[0m : 2.09202
[1mStep[0m  [48/169], [94mLoss[0m : 2.36502
[1mStep[0m  [64/169], [94mLoss[0m : 2.13414
[1mStep[0m  [80/169], [94mLoss[0m : 2.52512
[1mStep[0m  [96/169], [94mLoss[0m : 2.38147
[1mStep[0m  [112/169], [94mLoss[0m : 2.47490
[1mStep[0m  [128/169], [94mLoss[0m : 2.55541
[1mStep[0m  [144/169], [94mLoss[0m : 2.17745
[1mStep[0m  [160/169], [94mLoss[0m : 2.50184

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.291, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.82104
[1mStep[0m  [16/169], [94mLoss[0m : 2.53207
[1mStep[0m  [32/169], [94mLoss[0m : 2.48885
[1mStep[0m  [48/169], [94mLoss[0m : 2.22972
[1mStep[0m  [64/169], [94mLoss[0m : 2.25460
[1mStep[0m  [80/169], [94mLoss[0m : 2.23667
[1mStep[0m  [96/169], [94mLoss[0m : 2.42113
[1mStep[0m  [112/169], [94mLoss[0m : 2.38163
[1mStep[0m  [128/169], [94mLoss[0m : 2.56075
[1mStep[0m  [144/169], [94mLoss[0m : 2.71451
[1mStep[0m  [160/169], [94mLoss[0m : 2.11713

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.403, [92mTest[0m: 2.312, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.33625
[1mStep[0m  [16/169], [94mLoss[0m : 2.27394
[1mStep[0m  [32/169], [94mLoss[0m : 2.83841
[1mStep[0m  [48/169], [94mLoss[0m : 2.68505
[1mStep[0m  [64/169], [94mLoss[0m : 2.40310
[1mStep[0m  [80/169], [94mLoss[0m : 2.09124
[1mStep[0m  [96/169], [94mLoss[0m : 2.35371
[1mStep[0m  [112/169], [94mLoss[0m : 2.07836
[1mStep[0m  [128/169], [94mLoss[0m : 2.28177
[1mStep[0m  [144/169], [94mLoss[0m : 2.93074
[1mStep[0m  [160/169], [94mLoss[0m : 2.40170

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.310, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.98927
[1mStep[0m  [16/169], [94mLoss[0m : 2.46842
[1mStep[0m  [32/169], [94mLoss[0m : 2.07235
[1mStep[0m  [48/169], [94mLoss[0m : 2.31720
[1mStep[0m  [64/169], [94mLoss[0m : 2.53271
[1mStep[0m  [80/169], [94mLoss[0m : 2.38502
[1mStep[0m  [96/169], [94mLoss[0m : 2.06239
[1mStep[0m  [112/169], [94mLoss[0m : 2.26460
[1mStep[0m  [128/169], [94mLoss[0m : 2.25066
[1mStep[0m  [144/169], [94mLoss[0m : 2.39407
[1mStep[0m  [160/169], [94mLoss[0m : 2.55356

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.302, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.35723
[1mStep[0m  [16/169], [94mLoss[0m : 2.55678
[1mStep[0m  [32/169], [94mLoss[0m : 2.07106
[1mStep[0m  [48/169], [94mLoss[0m : 2.36415
[1mStep[0m  [64/169], [94mLoss[0m : 2.32421
[1mStep[0m  [80/169], [94mLoss[0m : 2.71211
[1mStep[0m  [96/169], [94mLoss[0m : 2.25127
[1mStep[0m  [112/169], [94mLoss[0m : 2.53076
[1mStep[0m  [128/169], [94mLoss[0m : 2.42442
[1mStep[0m  [144/169], [94mLoss[0m : 2.43865
[1mStep[0m  [160/169], [94mLoss[0m : 2.35511

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.396, [92mTest[0m: 2.350, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.88413
[1mStep[0m  [16/169], [94mLoss[0m : 2.55417
[1mStep[0m  [32/169], [94mLoss[0m : 2.29548
[1mStep[0m  [48/169], [94mLoss[0m : 2.73182
[1mStep[0m  [64/169], [94mLoss[0m : 2.58388
[1mStep[0m  [80/169], [94mLoss[0m : 2.37684
[1mStep[0m  [96/169], [94mLoss[0m : 2.40330
[1mStep[0m  [112/169], [94mLoss[0m : 2.48659
[1mStep[0m  [128/169], [94mLoss[0m : 2.10131
[1mStep[0m  [144/169], [94mLoss[0m : 2.45961
[1mStep[0m  [160/169], [94mLoss[0m : 2.16527

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.318, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.78957
[1mStep[0m  [16/169], [94mLoss[0m : 2.46665
[1mStep[0m  [32/169], [94mLoss[0m : 2.28940
[1mStep[0m  [48/169], [94mLoss[0m : 2.26576
[1mStep[0m  [64/169], [94mLoss[0m : 2.23285
[1mStep[0m  [80/169], [94mLoss[0m : 2.48298
[1mStep[0m  [96/169], [94mLoss[0m : 2.38687
[1mStep[0m  [112/169], [94mLoss[0m : 2.00561
[1mStep[0m  [128/169], [94mLoss[0m : 2.82565
[1mStep[0m  [144/169], [94mLoss[0m : 2.76806
[1mStep[0m  [160/169], [94mLoss[0m : 2.77597

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.289, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.61968
[1mStep[0m  [16/169], [94mLoss[0m : 2.32648
[1mStep[0m  [32/169], [94mLoss[0m : 2.08224
[1mStep[0m  [48/169], [94mLoss[0m : 2.56466
[1mStep[0m  [64/169], [94mLoss[0m : 2.17875
[1mStep[0m  [80/169], [94mLoss[0m : 2.43854
[1mStep[0m  [96/169], [94mLoss[0m : 2.21235
[1mStep[0m  [112/169], [94mLoss[0m : 2.21089
[1mStep[0m  [128/169], [94mLoss[0m : 2.12207
[1mStep[0m  [144/169], [94mLoss[0m : 2.30357
[1mStep[0m  [160/169], [94mLoss[0m : 1.95406

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.351, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.70670
[1mStep[0m  [16/169], [94mLoss[0m : 2.26125
[1mStep[0m  [32/169], [94mLoss[0m : 2.71566
[1mStep[0m  [48/169], [94mLoss[0m : 2.25599
[1mStep[0m  [64/169], [94mLoss[0m : 2.35086
[1mStep[0m  [80/169], [94mLoss[0m : 2.50237
[1mStep[0m  [96/169], [94mLoss[0m : 2.33643
[1mStep[0m  [112/169], [94mLoss[0m : 2.64873
[1mStep[0m  [128/169], [94mLoss[0m : 2.50995
[1mStep[0m  [144/169], [94mLoss[0m : 2.63364
[1mStep[0m  [160/169], [94mLoss[0m : 2.31876

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.390, [92mTest[0m: 2.327, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.296
====================================

Phase 1 - Evaluation MAE:  2.296140213097845
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/169], [94mLoss[0m : 2.30906
[1mStep[0m  [16/169], [94mLoss[0m : 2.58080
[1mStep[0m  [32/169], [94mLoss[0m : 2.17075
[1mStep[0m  [48/169], [94mLoss[0m : 2.92821
[1mStep[0m  [64/169], [94mLoss[0m : 2.16530
[1mStep[0m  [80/169], [94mLoss[0m : 2.26677
[1mStep[0m  [96/169], [94mLoss[0m : 2.94111
[1mStep[0m  [112/169], [94mLoss[0m : 3.28271
[1mStep[0m  [128/169], [94mLoss[0m : 2.43888
[1mStep[0m  [144/169], [94mLoss[0m : 2.09297
[1mStep[0m  [160/169], [94mLoss[0m : 2.71187

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.303, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.63362
[1mStep[0m  [16/169], [94mLoss[0m : 2.38642
[1mStep[0m  [32/169], [94mLoss[0m : 2.34436
[1mStep[0m  [48/169], [94mLoss[0m : 2.45966
[1mStep[0m  [64/169], [94mLoss[0m : 2.39227
[1mStep[0m  [80/169], [94mLoss[0m : 2.37959
[1mStep[0m  [96/169], [94mLoss[0m : 2.67200
[1mStep[0m  [112/169], [94mLoss[0m : 2.87761
[1mStep[0m  [128/169], [94mLoss[0m : 2.21187
[1mStep[0m  [144/169], [94mLoss[0m : 2.30703
[1mStep[0m  [160/169], [94mLoss[0m : 2.15803

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.385, [92mTest[0m: 2.371, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.52358
[1mStep[0m  [16/169], [94mLoss[0m : 2.10781
[1mStep[0m  [32/169], [94mLoss[0m : 2.41693
[1mStep[0m  [48/169], [94mLoss[0m : 1.93813
[1mStep[0m  [64/169], [94mLoss[0m : 1.75481
[1mStep[0m  [80/169], [94mLoss[0m : 2.17677
[1mStep[0m  [96/169], [94mLoss[0m : 2.32136
[1mStep[0m  [112/169], [94mLoss[0m : 2.63927
[1mStep[0m  [128/169], [94mLoss[0m : 2.65425
[1mStep[0m  [144/169], [94mLoss[0m : 2.13466
[1mStep[0m  [160/169], [94mLoss[0m : 2.16176

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.311, [92mTest[0m: 2.328, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.02621
[1mStep[0m  [16/169], [94mLoss[0m : 1.93671
[1mStep[0m  [32/169], [94mLoss[0m : 2.54917
[1mStep[0m  [48/169], [94mLoss[0m : 2.25263
[1mStep[0m  [64/169], [94mLoss[0m : 2.31571
[1mStep[0m  [80/169], [94mLoss[0m : 2.42319
[1mStep[0m  [96/169], [94mLoss[0m : 2.28374
[1mStep[0m  [112/169], [94mLoss[0m : 2.30721
[1mStep[0m  [128/169], [94mLoss[0m : 2.52718
[1mStep[0m  [144/169], [94mLoss[0m : 2.17543
[1mStep[0m  [160/169], [94mLoss[0m : 2.55075

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.276, [92mTest[0m: 2.401, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.32226
[1mStep[0m  [16/169], [94mLoss[0m : 2.07590
[1mStep[0m  [32/169], [94mLoss[0m : 2.07256
[1mStep[0m  [48/169], [94mLoss[0m : 2.03817
[1mStep[0m  [64/169], [94mLoss[0m : 1.90585
[1mStep[0m  [80/169], [94mLoss[0m : 2.20438
[1mStep[0m  [96/169], [94mLoss[0m : 2.31134
[1mStep[0m  [112/169], [94mLoss[0m : 2.18825
[1mStep[0m  [128/169], [94mLoss[0m : 2.31050
[1mStep[0m  [144/169], [94mLoss[0m : 2.75096
[1mStep[0m  [160/169], [94mLoss[0m : 2.06192

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.237, [92mTest[0m: 2.356, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.35810
[1mStep[0m  [16/169], [94mLoss[0m : 2.45142
[1mStep[0m  [32/169], [94mLoss[0m : 2.23665
[1mStep[0m  [48/169], [94mLoss[0m : 2.32170
[1mStep[0m  [64/169], [94mLoss[0m : 1.75597
[1mStep[0m  [80/169], [94mLoss[0m : 2.08696
[1mStep[0m  [96/169], [94mLoss[0m : 2.09524
[1mStep[0m  [112/169], [94mLoss[0m : 2.54750
[1mStep[0m  [128/169], [94mLoss[0m : 1.91336
[1mStep[0m  [144/169], [94mLoss[0m : 2.08703
[1mStep[0m  [160/169], [94mLoss[0m : 2.60056

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.228, [92mTest[0m: 2.378, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.24488
[1mStep[0m  [16/169], [94mLoss[0m : 2.04227
[1mStep[0m  [32/169], [94mLoss[0m : 2.18605
[1mStep[0m  [48/169], [94mLoss[0m : 2.46594
[1mStep[0m  [64/169], [94mLoss[0m : 2.50573
[1mStep[0m  [80/169], [94mLoss[0m : 2.03533
[1mStep[0m  [96/169], [94mLoss[0m : 2.16549
[1mStep[0m  [112/169], [94mLoss[0m : 2.42328
[1mStep[0m  [128/169], [94mLoss[0m : 2.05703
[1mStep[0m  [144/169], [94mLoss[0m : 2.33404
[1mStep[0m  [160/169], [94mLoss[0m : 2.19411

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.186, [92mTest[0m: 2.385, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.84175
[1mStep[0m  [16/169], [94mLoss[0m : 2.20380
[1mStep[0m  [32/169], [94mLoss[0m : 2.49191
[1mStep[0m  [48/169], [94mLoss[0m : 2.09195
[1mStep[0m  [64/169], [94mLoss[0m : 2.62672
[1mStep[0m  [80/169], [94mLoss[0m : 1.92527
[1mStep[0m  [96/169], [94mLoss[0m : 2.04191
[1mStep[0m  [112/169], [94mLoss[0m : 1.78448
[1mStep[0m  [128/169], [94mLoss[0m : 2.05408
[1mStep[0m  [144/169], [94mLoss[0m : 2.19019
[1mStep[0m  [160/169], [94mLoss[0m : 2.04425

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.181, [92mTest[0m: 2.404, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.17441
[1mStep[0m  [16/169], [94mLoss[0m : 2.35838
[1mStep[0m  [32/169], [94mLoss[0m : 1.79747
[1mStep[0m  [48/169], [94mLoss[0m : 1.88469
[1mStep[0m  [64/169], [94mLoss[0m : 2.56036
[1mStep[0m  [80/169], [94mLoss[0m : 2.31127
[1mStep[0m  [96/169], [94mLoss[0m : 2.39115
[1mStep[0m  [112/169], [94mLoss[0m : 2.29648
[1mStep[0m  [128/169], [94mLoss[0m : 2.01149
[1mStep[0m  [144/169], [94mLoss[0m : 2.14633
[1mStep[0m  [160/169], [94mLoss[0m : 2.23592

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.160, [92mTest[0m: 2.382, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.44597
[1mStep[0m  [16/169], [94mLoss[0m : 1.90671
[1mStep[0m  [32/169], [94mLoss[0m : 2.28982
[1mStep[0m  [48/169], [94mLoss[0m : 2.26886
[1mStep[0m  [64/169], [94mLoss[0m : 1.88353
[1mStep[0m  [80/169], [94mLoss[0m : 2.47503
[1mStep[0m  [96/169], [94mLoss[0m : 1.99394
[1mStep[0m  [112/169], [94mLoss[0m : 2.13586
[1mStep[0m  [128/169], [94mLoss[0m : 2.41749
[1mStep[0m  [144/169], [94mLoss[0m : 2.22762
[1mStep[0m  [160/169], [94mLoss[0m : 2.27833

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.142, [92mTest[0m: 2.469, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.07349
[1mStep[0m  [16/169], [94mLoss[0m : 2.36803
[1mStep[0m  [32/169], [94mLoss[0m : 1.88668
[1mStep[0m  [48/169], [94mLoss[0m : 2.12452
[1mStep[0m  [64/169], [94mLoss[0m : 1.82108
[1mStep[0m  [80/169], [94mLoss[0m : 2.22865
[1mStep[0m  [96/169], [94mLoss[0m : 2.13434
[1mStep[0m  [112/169], [94mLoss[0m : 2.36596
[1mStep[0m  [128/169], [94mLoss[0m : 2.42916
[1mStep[0m  [144/169], [94mLoss[0m : 1.91986
[1mStep[0m  [160/169], [94mLoss[0m : 2.05562

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.124, [92mTest[0m: 2.408, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.45442
[1mStep[0m  [16/169], [94mLoss[0m : 1.71158
[1mStep[0m  [32/169], [94mLoss[0m : 2.19592
[1mStep[0m  [48/169], [94mLoss[0m : 2.20471
[1mStep[0m  [64/169], [94mLoss[0m : 2.44653
[1mStep[0m  [80/169], [94mLoss[0m : 2.37940
[1mStep[0m  [96/169], [94mLoss[0m : 1.94749
[1mStep[0m  [112/169], [94mLoss[0m : 2.12036
[1mStep[0m  [128/169], [94mLoss[0m : 2.22097
[1mStep[0m  [144/169], [94mLoss[0m : 2.13229
[1mStep[0m  [160/169], [94mLoss[0m : 2.06409

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.116, [92mTest[0m: 2.418, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.72400
[1mStep[0m  [16/169], [94mLoss[0m : 1.71140
[1mStep[0m  [32/169], [94mLoss[0m : 2.13044
[1mStep[0m  [48/169], [94mLoss[0m : 1.70707
[1mStep[0m  [64/169], [94mLoss[0m : 2.12948
[1mStep[0m  [80/169], [94mLoss[0m : 1.89176
[1mStep[0m  [96/169], [94mLoss[0m : 2.14710
[1mStep[0m  [112/169], [94mLoss[0m : 2.10389
[1mStep[0m  [128/169], [94mLoss[0m : 2.13408
[1mStep[0m  [144/169], [94mLoss[0m : 1.66433
[1mStep[0m  [160/169], [94mLoss[0m : 2.60607

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.104, [92mTest[0m: 2.422, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.19215
[1mStep[0m  [16/169], [94mLoss[0m : 1.63516
[1mStep[0m  [32/169], [94mLoss[0m : 2.29230
[1mStep[0m  [48/169], [94mLoss[0m : 2.09226
[1mStep[0m  [64/169], [94mLoss[0m : 2.09509
[1mStep[0m  [80/169], [94mLoss[0m : 1.95678
[1mStep[0m  [96/169], [94mLoss[0m : 2.22708
[1mStep[0m  [112/169], [94mLoss[0m : 1.86379
[1mStep[0m  [128/169], [94mLoss[0m : 2.73155
[1mStep[0m  [144/169], [94mLoss[0m : 1.95754
[1mStep[0m  [160/169], [94mLoss[0m : 2.47883

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.090, [92mTest[0m: 2.426, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.16345
[1mStep[0m  [16/169], [94mLoss[0m : 1.95050
[1mStep[0m  [32/169], [94mLoss[0m : 2.42303
[1mStep[0m  [48/169], [94mLoss[0m : 2.22658
[1mStep[0m  [64/169], [94mLoss[0m : 1.70104
[1mStep[0m  [80/169], [94mLoss[0m : 2.22697
[1mStep[0m  [96/169], [94mLoss[0m : 1.76329
[1mStep[0m  [112/169], [94mLoss[0m : 2.07605
[1mStep[0m  [128/169], [94mLoss[0m : 2.07785
[1mStep[0m  [144/169], [94mLoss[0m : 1.79910
[1mStep[0m  [160/169], [94mLoss[0m : 2.35737

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.085, [92mTest[0m: 2.459, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.96042
[1mStep[0m  [16/169], [94mLoss[0m : 2.13661
[1mStep[0m  [32/169], [94mLoss[0m : 2.10831
[1mStep[0m  [48/169], [94mLoss[0m : 1.89812
[1mStep[0m  [64/169], [94mLoss[0m : 2.22271
[1mStep[0m  [80/169], [94mLoss[0m : 2.10750
[1mStep[0m  [96/169], [94mLoss[0m : 2.31064
[1mStep[0m  [112/169], [94mLoss[0m : 2.34319
[1mStep[0m  [128/169], [94mLoss[0m : 2.03775
[1mStep[0m  [144/169], [94mLoss[0m : 2.08683
[1mStep[0m  [160/169], [94mLoss[0m : 2.42264

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.095, [92mTest[0m: 2.404, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.05193
[1mStep[0m  [16/169], [94mLoss[0m : 2.35023
[1mStep[0m  [32/169], [94mLoss[0m : 1.76354
[1mStep[0m  [48/169], [94mLoss[0m : 2.06934
[1mStep[0m  [64/169], [94mLoss[0m : 1.98835
[1mStep[0m  [80/169], [94mLoss[0m : 2.03275
[1mStep[0m  [96/169], [94mLoss[0m : 1.96905
[1mStep[0m  [112/169], [94mLoss[0m : 1.89104
[1mStep[0m  [128/169], [94mLoss[0m : 2.17466
[1mStep[0m  [144/169], [94mLoss[0m : 1.79412
[1mStep[0m  [160/169], [94mLoss[0m : 2.31511

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.051, [92mTest[0m: 2.453, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.81322
[1mStep[0m  [16/169], [94mLoss[0m : 1.83263
[1mStep[0m  [32/169], [94mLoss[0m : 2.13879
[1mStep[0m  [48/169], [94mLoss[0m : 1.99380
[1mStep[0m  [64/169], [94mLoss[0m : 1.90523
[1mStep[0m  [80/169], [94mLoss[0m : 2.26369
[1mStep[0m  [96/169], [94mLoss[0m : 1.91137
[1mStep[0m  [112/169], [94mLoss[0m : 2.37403
[1mStep[0m  [128/169], [94mLoss[0m : 2.61796
[1mStep[0m  [144/169], [94mLoss[0m : 2.11150
[1mStep[0m  [160/169], [94mLoss[0m : 2.07259

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.088, [92mTest[0m: 2.415, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.16592
[1mStep[0m  [16/169], [94mLoss[0m : 2.05071
[1mStep[0m  [32/169], [94mLoss[0m : 2.16401
[1mStep[0m  [48/169], [94mLoss[0m : 2.12804
[1mStep[0m  [64/169], [94mLoss[0m : 2.05141
[1mStep[0m  [80/169], [94mLoss[0m : 2.08424
[1mStep[0m  [96/169], [94mLoss[0m : 1.77100
[1mStep[0m  [112/169], [94mLoss[0m : 2.00005
[1mStep[0m  [128/169], [94mLoss[0m : 2.06479
[1mStep[0m  [144/169], [94mLoss[0m : 2.20026
[1mStep[0m  [160/169], [94mLoss[0m : 1.84311

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.051, [92mTest[0m: 2.415, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.86976
[1mStep[0m  [16/169], [94mLoss[0m : 1.82141
[1mStep[0m  [32/169], [94mLoss[0m : 2.01068
[1mStep[0m  [48/169], [94mLoss[0m : 2.12033
[1mStep[0m  [64/169], [94mLoss[0m : 2.15442
[1mStep[0m  [80/169], [94mLoss[0m : 1.83715
[1mStep[0m  [96/169], [94mLoss[0m : 2.30048
[1mStep[0m  [112/169], [94mLoss[0m : 1.88763
[1mStep[0m  [128/169], [94mLoss[0m : 2.14945
[1mStep[0m  [144/169], [94mLoss[0m : 1.95885
[1mStep[0m  [160/169], [94mLoss[0m : 2.22740

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.050, [92mTest[0m: 2.435, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.76577
[1mStep[0m  [16/169], [94mLoss[0m : 1.84753
[1mStep[0m  [32/169], [94mLoss[0m : 2.29614
[1mStep[0m  [48/169], [94mLoss[0m : 1.87017
[1mStep[0m  [64/169], [94mLoss[0m : 1.69615
[1mStep[0m  [80/169], [94mLoss[0m : 2.29426
[1mStep[0m  [96/169], [94mLoss[0m : 2.08953
[1mStep[0m  [112/169], [94mLoss[0m : 1.84956
[1mStep[0m  [128/169], [94mLoss[0m : 1.58831
[1mStep[0m  [144/169], [94mLoss[0m : 2.61652
[1mStep[0m  [160/169], [94mLoss[0m : 1.93660

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.996, [92mTest[0m: 2.436, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.22107
[1mStep[0m  [16/169], [94mLoss[0m : 1.77807
[1mStep[0m  [32/169], [94mLoss[0m : 2.14499
[1mStep[0m  [48/169], [94mLoss[0m : 1.79933
[1mStep[0m  [64/169], [94mLoss[0m : 2.12041
[1mStep[0m  [80/169], [94mLoss[0m : 1.84744
[1mStep[0m  [96/169], [94mLoss[0m : 1.87017
[1mStep[0m  [112/169], [94mLoss[0m : 1.84786
[1mStep[0m  [128/169], [94mLoss[0m : 2.11657
[1mStep[0m  [144/169], [94mLoss[0m : 2.27741
[1mStep[0m  [160/169], [94mLoss[0m : 2.02538

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.982, [92mTest[0m: 2.503, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.86176
[1mStep[0m  [16/169], [94mLoss[0m : 1.84072
[1mStep[0m  [32/169], [94mLoss[0m : 1.67270
[1mStep[0m  [48/169], [94mLoss[0m : 1.55407
[1mStep[0m  [64/169], [94mLoss[0m : 2.17353
[1mStep[0m  [80/169], [94mLoss[0m : 2.06484
[1mStep[0m  [96/169], [94mLoss[0m : 2.31890
[1mStep[0m  [112/169], [94mLoss[0m : 1.90694
[1mStep[0m  [128/169], [94mLoss[0m : 2.02195
[1mStep[0m  [144/169], [94mLoss[0m : 2.14522
[1mStep[0m  [160/169], [94mLoss[0m : 1.94583

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.976, [92mTest[0m: 2.449, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.94487
[1mStep[0m  [16/169], [94mLoss[0m : 1.95732
[1mStep[0m  [32/169], [94mLoss[0m : 1.79233
[1mStep[0m  [48/169], [94mLoss[0m : 1.78460
[1mStep[0m  [64/169], [94mLoss[0m : 2.17815
[1mStep[0m  [80/169], [94mLoss[0m : 1.93491
[1mStep[0m  [96/169], [94mLoss[0m : 2.26186
[1mStep[0m  [112/169], [94mLoss[0m : 2.13486
[1mStep[0m  [128/169], [94mLoss[0m : 2.39823
[1mStep[0m  [144/169], [94mLoss[0m : 2.22213
[1mStep[0m  [160/169], [94mLoss[0m : 1.71330

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.984, [92mTest[0m: 2.432, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.27493
[1mStep[0m  [16/169], [94mLoss[0m : 1.76562
[1mStep[0m  [32/169], [94mLoss[0m : 2.10489
[1mStep[0m  [48/169], [94mLoss[0m : 2.16103
[1mStep[0m  [64/169], [94mLoss[0m : 1.80143
[1mStep[0m  [80/169], [94mLoss[0m : 1.42208
[1mStep[0m  [96/169], [94mLoss[0m : 1.58197
[1mStep[0m  [112/169], [94mLoss[0m : 2.03395
[1mStep[0m  [128/169], [94mLoss[0m : 1.81183
[1mStep[0m  [144/169], [94mLoss[0m : 1.92683
[1mStep[0m  [160/169], [94mLoss[0m : 1.90501

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.953, [92mTest[0m: 2.444, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.02679
[1mStep[0m  [16/169], [94mLoss[0m : 1.50659
[1mStep[0m  [32/169], [94mLoss[0m : 2.28500
[1mStep[0m  [48/169], [94mLoss[0m : 1.81189
[1mStep[0m  [64/169], [94mLoss[0m : 2.02486
[1mStep[0m  [80/169], [94mLoss[0m : 1.72288
[1mStep[0m  [96/169], [94mLoss[0m : 2.37535
[1mStep[0m  [112/169], [94mLoss[0m : 2.01786
[1mStep[0m  [128/169], [94mLoss[0m : 1.94211
[1mStep[0m  [144/169], [94mLoss[0m : 2.19713
[1mStep[0m  [160/169], [94mLoss[0m : 1.73612

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.959, [92mTest[0m: 2.464, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.05952
[1mStep[0m  [16/169], [94mLoss[0m : 1.76726
[1mStep[0m  [32/169], [94mLoss[0m : 1.74929
[1mStep[0m  [48/169], [94mLoss[0m : 1.96931
[1mStep[0m  [64/169], [94mLoss[0m : 1.74947
[1mStep[0m  [80/169], [94mLoss[0m : 2.13662
[1mStep[0m  [96/169], [94mLoss[0m : 2.11898
[1mStep[0m  [112/169], [94mLoss[0m : 2.07364
[1mStep[0m  [128/169], [94mLoss[0m : 1.53120
[1mStep[0m  [144/169], [94mLoss[0m : 1.95944
[1mStep[0m  [160/169], [94mLoss[0m : 1.96019

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.937, [92mTest[0m: 2.485, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.78230
[1mStep[0m  [16/169], [94mLoss[0m : 1.64523
[1mStep[0m  [32/169], [94mLoss[0m : 1.78575
[1mStep[0m  [48/169], [94mLoss[0m : 1.63444
[1mStep[0m  [64/169], [94mLoss[0m : 1.79641
[1mStep[0m  [80/169], [94mLoss[0m : 2.03383
[1mStep[0m  [96/169], [94mLoss[0m : 1.79064
[1mStep[0m  [112/169], [94mLoss[0m : 2.27409
[1mStep[0m  [128/169], [94mLoss[0m : 1.97175
[1mStep[0m  [144/169], [94mLoss[0m : 2.16810
[1mStep[0m  [160/169], [94mLoss[0m : 1.73387

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.947, [92mTest[0m: 2.477, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.96707
[1mStep[0m  [16/169], [94mLoss[0m : 1.64644
[1mStep[0m  [32/169], [94mLoss[0m : 1.92473
[1mStep[0m  [48/169], [94mLoss[0m : 1.80486
[1mStep[0m  [64/169], [94mLoss[0m : 2.28299
[1mStep[0m  [80/169], [94mLoss[0m : 2.09241
[1mStep[0m  [96/169], [94mLoss[0m : 1.91088
[1mStep[0m  [112/169], [94mLoss[0m : 2.27887
[1mStep[0m  [128/169], [94mLoss[0m : 1.95433
[1mStep[0m  [144/169], [94mLoss[0m : 2.03922
[1mStep[0m  [160/169], [94mLoss[0m : 1.92156

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.939, [92mTest[0m: 2.494, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.72614
[1mStep[0m  [16/169], [94mLoss[0m : 1.69459
[1mStep[0m  [32/169], [94mLoss[0m : 1.72184
[1mStep[0m  [48/169], [94mLoss[0m : 1.83246
[1mStep[0m  [64/169], [94mLoss[0m : 2.01469
[1mStep[0m  [80/169], [94mLoss[0m : 1.60937
[1mStep[0m  [96/169], [94mLoss[0m : 1.77968
[1mStep[0m  [112/169], [94mLoss[0m : 2.19985
[1mStep[0m  [128/169], [94mLoss[0m : 1.69401
[1mStep[0m  [144/169], [94mLoss[0m : 1.65381
[1mStep[0m  [160/169], [94mLoss[0m : 1.88336

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.930, [92mTest[0m: 2.498, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.456
====================================

Phase 2 - Evaluation MAE:  2.455655485391617
MAE score P1        2.29614
MAE score P2       2.455655
loss               1.929796
learning_rate       0.00505
batch_size               64
hidden_sizes      [200, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.9
weight_decay           0.01
Name: 7, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 11.41542
[1mStep[0m  [8/84], [94mLoss[0m : 6.45321
[1mStep[0m  [16/84], [94mLoss[0m : 2.75528
[1mStep[0m  [24/84], [94mLoss[0m : 3.44199
[1mStep[0m  [32/84], [94mLoss[0m : 2.86998
[1mStep[0m  [40/84], [94mLoss[0m : 2.68210
[1mStep[0m  [48/84], [94mLoss[0m : 2.70776
[1mStep[0m  [56/84], [94mLoss[0m : 2.48145
[1mStep[0m  [64/84], [94mLoss[0m : 2.36277
[1mStep[0m  [72/84], [94mLoss[0m : 2.46966
[1mStep[0m  [80/84], [94mLoss[0m : 2.77883

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.570, [92mTest[0m: 10.990, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.82130
[1mStep[0m  [8/84], [94mLoss[0m : 2.76330
[1mStep[0m  [16/84], [94mLoss[0m : 2.50604
[1mStep[0m  [24/84], [94mLoss[0m : 2.18686
[1mStep[0m  [32/84], [94mLoss[0m : 2.79495
[1mStep[0m  [40/84], [94mLoss[0m : 2.49470
[1mStep[0m  [48/84], [94mLoss[0m : 2.74359
[1mStep[0m  [56/84], [94mLoss[0m : 2.28987
[1mStep[0m  [64/84], [94mLoss[0m : 2.47962
[1mStep[0m  [72/84], [94mLoss[0m : 2.80317
[1mStep[0m  [80/84], [94mLoss[0m : 2.45007

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.341, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.74427
[1mStep[0m  [8/84], [94mLoss[0m : 2.48590
[1mStep[0m  [16/84], [94mLoss[0m : 2.74908
[1mStep[0m  [24/84], [94mLoss[0m : 2.63722
[1mStep[0m  [32/84], [94mLoss[0m : 2.53131
[1mStep[0m  [40/84], [94mLoss[0m : 2.62007
[1mStep[0m  [48/84], [94mLoss[0m : 2.60208
[1mStep[0m  [56/84], [94mLoss[0m : 2.06704
[1mStep[0m  [64/84], [94mLoss[0m : 2.84311
[1mStep[0m  [72/84], [94mLoss[0m : 2.21833
[1mStep[0m  [80/84], [94mLoss[0m : 2.51245

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.330, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.32483
[1mStep[0m  [8/84], [94mLoss[0m : 2.61318
[1mStep[0m  [16/84], [94mLoss[0m : 2.81318
[1mStep[0m  [24/84], [94mLoss[0m : 2.36573
[1mStep[0m  [32/84], [94mLoss[0m : 2.41806
[1mStep[0m  [40/84], [94mLoss[0m : 2.60558
[1mStep[0m  [48/84], [94mLoss[0m : 2.69389
[1mStep[0m  [56/84], [94mLoss[0m : 2.41955
[1mStep[0m  [64/84], [94mLoss[0m : 2.54114
[1mStep[0m  [72/84], [94mLoss[0m : 2.46835
[1mStep[0m  [80/84], [94mLoss[0m : 2.37269

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.335, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.63194
[1mStep[0m  [8/84], [94mLoss[0m : 2.18616
[1mStep[0m  [16/84], [94mLoss[0m : 2.30330
[1mStep[0m  [24/84], [94mLoss[0m : 2.16675
[1mStep[0m  [32/84], [94mLoss[0m : 2.27135
[1mStep[0m  [40/84], [94mLoss[0m : 2.75903
[1mStep[0m  [48/84], [94mLoss[0m : 2.44015
[1mStep[0m  [56/84], [94mLoss[0m : 2.55429
[1mStep[0m  [64/84], [94mLoss[0m : 2.16892
[1mStep[0m  [72/84], [94mLoss[0m : 2.47265
[1mStep[0m  [80/84], [94mLoss[0m : 2.57050

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.338, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35578
[1mStep[0m  [8/84], [94mLoss[0m : 2.42173
[1mStep[0m  [16/84], [94mLoss[0m : 2.49553
[1mStep[0m  [24/84], [94mLoss[0m : 2.49123
[1mStep[0m  [32/84], [94mLoss[0m : 2.31479
[1mStep[0m  [40/84], [94mLoss[0m : 2.48369
[1mStep[0m  [48/84], [94mLoss[0m : 2.51467
[1mStep[0m  [56/84], [94mLoss[0m : 2.45005
[1mStep[0m  [64/84], [94mLoss[0m : 2.38496
[1mStep[0m  [72/84], [94mLoss[0m : 2.64158
[1mStep[0m  [80/84], [94mLoss[0m : 2.72775

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.322, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43473
[1mStep[0m  [8/84], [94mLoss[0m : 2.63365
[1mStep[0m  [16/84], [94mLoss[0m : 2.31760
[1mStep[0m  [24/84], [94mLoss[0m : 2.42654
[1mStep[0m  [32/84], [94mLoss[0m : 2.08928
[1mStep[0m  [40/84], [94mLoss[0m : 2.51686
[1mStep[0m  [48/84], [94mLoss[0m : 2.01980
[1mStep[0m  [56/84], [94mLoss[0m : 2.07906
[1mStep[0m  [64/84], [94mLoss[0m : 2.50178
[1mStep[0m  [72/84], [94mLoss[0m : 2.51767
[1mStep[0m  [80/84], [94mLoss[0m : 2.26461

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.328, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27855
[1mStep[0m  [8/84], [94mLoss[0m : 2.49137
[1mStep[0m  [16/84], [94mLoss[0m : 2.47883
[1mStep[0m  [24/84], [94mLoss[0m : 2.40185
[1mStep[0m  [32/84], [94mLoss[0m : 2.57422
[1mStep[0m  [40/84], [94mLoss[0m : 2.86229
[1mStep[0m  [48/84], [94mLoss[0m : 2.62385
[1mStep[0m  [56/84], [94mLoss[0m : 2.61705
[1mStep[0m  [64/84], [94mLoss[0m : 2.35782
[1mStep[0m  [72/84], [94mLoss[0m : 2.44619
[1mStep[0m  [80/84], [94mLoss[0m : 2.42481

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.334, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.64421
[1mStep[0m  [8/84], [94mLoss[0m : 2.47733
[1mStep[0m  [16/84], [94mLoss[0m : 2.37388
[1mStep[0m  [24/84], [94mLoss[0m : 2.52169
[1mStep[0m  [32/84], [94mLoss[0m : 2.64297
[1mStep[0m  [40/84], [94mLoss[0m : 2.51480
[1mStep[0m  [48/84], [94mLoss[0m : 2.54676
[1mStep[0m  [56/84], [94mLoss[0m : 2.45929
[1mStep[0m  [64/84], [94mLoss[0m : 2.24333
[1mStep[0m  [72/84], [94mLoss[0m : 2.65586
[1mStep[0m  [80/84], [94mLoss[0m : 2.23203

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.334, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31349
[1mStep[0m  [8/84], [94mLoss[0m : 2.37034
[1mStep[0m  [16/84], [94mLoss[0m : 2.43715
[1mStep[0m  [24/84], [94mLoss[0m : 2.36098
[1mStep[0m  [32/84], [94mLoss[0m : 2.43748
[1mStep[0m  [40/84], [94mLoss[0m : 2.21298
[1mStep[0m  [48/84], [94mLoss[0m : 2.51235
[1mStep[0m  [56/84], [94mLoss[0m : 2.41161
[1mStep[0m  [64/84], [94mLoss[0m : 2.54939
[1mStep[0m  [72/84], [94mLoss[0m : 2.29317
[1mStep[0m  [80/84], [94mLoss[0m : 2.59438

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.342, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43304
[1mStep[0m  [8/84], [94mLoss[0m : 2.48001
[1mStep[0m  [16/84], [94mLoss[0m : 2.10950
[1mStep[0m  [24/84], [94mLoss[0m : 2.14176
[1mStep[0m  [32/84], [94mLoss[0m : 2.36320
[1mStep[0m  [40/84], [94mLoss[0m : 2.52802
[1mStep[0m  [48/84], [94mLoss[0m : 2.65780
[1mStep[0m  [56/84], [94mLoss[0m : 2.51948
[1mStep[0m  [64/84], [94mLoss[0m : 2.43520
[1mStep[0m  [72/84], [94mLoss[0m : 2.36173
[1mStep[0m  [80/84], [94mLoss[0m : 2.49151

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.333, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.61357
[1mStep[0m  [8/84], [94mLoss[0m : 2.60843
[1mStep[0m  [16/84], [94mLoss[0m : 2.36286
[1mStep[0m  [24/84], [94mLoss[0m : 2.75281
[1mStep[0m  [32/84], [94mLoss[0m : 2.36951
[1mStep[0m  [40/84], [94mLoss[0m : 2.39174
[1mStep[0m  [48/84], [94mLoss[0m : 2.10056
[1mStep[0m  [56/84], [94mLoss[0m : 2.33768
[1mStep[0m  [64/84], [94mLoss[0m : 2.55295
[1mStep[0m  [72/84], [94mLoss[0m : 2.33498
[1mStep[0m  [80/84], [94mLoss[0m : 2.49639

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.338, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.15495
[1mStep[0m  [8/84], [94mLoss[0m : 2.51715
[1mStep[0m  [16/84], [94mLoss[0m : 2.58508
[1mStep[0m  [24/84], [94mLoss[0m : 2.52488
[1mStep[0m  [32/84], [94mLoss[0m : 2.42594
[1mStep[0m  [40/84], [94mLoss[0m : 2.62778
[1mStep[0m  [48/84], [94mLoss[0m : 2.48066
[1mStep[0m  [56/84], [94mLoss[0m : 2.53874
[1mStep[0m  [64/84], [94mLoss[0m : 2.31040
[1mStep[0m  [72/84], [94mLoss[0m : 2.35833
[1mStep[0m  [80/84], [94mLoss[0m : 2.31987

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.328, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27248
[1mStep[0m  [8/84], [94mLoss[0m : 2.17387
[1mStep[0m  [16/84], [94mLoss[0m : 2.77239
[1mStep[0m  [24/84], [94mLoss[0m : 2.44849
[1mStep[0m  [32/84], [94mLoss[0m : 2.27490
[1mStep[0m  [40/84], [94mLoss[0m : 2.69592
[1mStep[0m  [48/84], [94mLoss[0m : 2.42293
[1mStep[0m  [56/84], [94mLoss[0m : 2.78615
[1mStep[0m  [64/84], [94mLoss[0m : 2.44435
[1mStep[0m  [72/84], [94mLoss[0m : 2.44353
[1mStep[0m  [80/84], [94mLoss[0m : 2.13281

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.330, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43181
[1mStep[0m  [8/84], [94mLoss[0m : 2.33716
[1mStep[0m  [16/84], [94mLoss[0m : 2.76801
[1mStep[0m  [24/84], [94mLoss[0m : 2.29816
[1mStep[0m  [32/84], [94mLoss[0m : 2.53712
[1mStep[0m  [40/84], [94mLoss[0m : 2.34835
[1mStep[0m  [48/84], [94mLoss[0m : 2.42453
[1mStep[0m  [56/84], [94mLoss[0m : 2.76769
[1mStep[0m  [64/84], [94mLoss[0m : 2.43979
[1mStep[0m  [72/84], [94mLoss[0m : 2.18298
[1mStep[0m  [80/84], [94mLoss[0m : 2.40163

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.330, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54458
[1mStep[0m  [8/84], [94mLoss[0m : 2.18919
[1mStep[0m  [16/84], [94mLoss[0m : 2.29311
[1mStep[0m  [24/84], [94mLoss[0m : 2.26644
[1mStep[0m  [32/84], [94mLoss[0m : 2.60799
[1mStep[0m  [40/84], [94mLoss[0m : 2.41382
[1mStep[0m  [48/84], [94mLoss[0m : 2.25526
[1mStep[0m  [56/84], [94mLoss[0m : 2.09049
[1mStep[0m  [64/84], [94mLoss[0m : 2.55818
[1mStep[0m  [72/84], [94mLoss[0m : 2.70946
[1mStep[0m  [80/84], [94mLoss[0m : 2.32718

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.329, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.22810
[1mStep[0m  [8/84], [94mLoss[0m : 2.22189
[1mStep[0m  [16/84], [94mLoss[0m : 2.21052
[1mStep[0m  [24/84], [94mLoss[0m : 2.53255
[1mStep[0m  [32/84], [94mLoss[0m : 2.49921
[1mStep[0m  [40/84], [94mLoss[0m : 2.51937
[1mStep[0m  [48/84], [94mLoss[0m : 2.33302
[1mStep[0m  [56/84], [94mLoss[0m : 2.44066
[1mStep[0m  [64/84], [94mLoss[0m : 2.50320
[1mStep[0m  [72/84], [94mLoss[0m : 2.49730
[1mStep[0m  [80/84], [94mLoss[0m : 2.35252

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.336, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52319
[1mStep[0m  [8/84], [94mLoss[0m : 2.17863
[1mStep[0m  [16/84], [94mLoss[0m : 2.60068
[1mStep[0m  [24/84], [94mLoss[0m : 2.31350
[1mStep[0m  [32/84], [94mLoss[0m : 2.30677
[1mStep[0m  [40/84], [94mLoss[0m : 2.65260
[1mStep[0m  [48/84], [94mLoss[0m : 2.51686
[1mStep[0m  [56/84], [94mLoss[0m : 2.30033
[1mStep[0m  [64/84], [94mLoss[0m : 2.24704
[1mStep[0m  [72/84], [94mLoss[0m : 2.70531
[1mStep[0m  [80/84], [94mLoss[0m : 2.70022

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.334, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33931
[1mStep[0m  [8/84], [94mLoss[0m : 2.29351
[1mStep[0m  [16/84], [94mLoss[0m : 2.29023
[1mStep[0m  [24/84], [94mLoss[0m : 2.76632
[1mStep[0m  [32/84], [94mLoss[0m : 2.50187
[1mStep[0m  [40/84], [94mLoss[0m : 2.10659
[1mStep[0m  [48/84], [94mLoss[0m : 2.28901
[1mStep[0m  [56/84], [94mLoss[0m : 2.42543
[1mStep[0m  [64/84], [94mLoss[0m : 2.50329
[1mStep[0m  [72/84], [94mLoss[0m : 2.61507
[1mStep[0m  [80/84], [94mLoss[0m : 2.54667

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.336, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43320
[1mStep[0m  [8/84], [94mLoss[0m : 2.54458
[1mStep[0m  [16/84], [94mLoss[0m : 2.44430
[1mStep[0m  [24/84], [94mLoss[0m : 2.76309
[1mStep[0m  [32/84], [94mLoss[0m : 2.37012
[1mStep[0m  [40/84], [94mLoss[0m : 2.15377
[1mStep[0m  [48/84], [94mLoss[0m : 2.39544
[1mStep[0m  [56/84], [94mLoss[0m : 2.55467
[1mStep[0m  [64/84], [94mLoss[0m : 2.32003
[1mStep[0m  [72/84], [94mLoss[0m : 2.36678
[1mStep[0m  [80/84], [94mLoss[0m : 2.76985

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.350, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.39163
[1mStep[0m  [8/84], [94mLoss[0m : 2.32304
[1mStep[0m  [16/84], [94mLoss[0m : 2.39636
[1mStep[0m  [24/84], [94mLoss[0m : 2.57631
[1mStep[0m  [32/84], [94mLoss[0m : 2.70063
[1mStep[0m  [40/84], [94mLoss[0m : 2.86510
[1mStep[0m  [48/84], [94mLoss[0m : 2.18925
[1mStep[0m  [56/84], [94mLoss[0m : 2.68302
[1mStep[0m  [64/84], [94mLoss[0m : 2.72102
[1mStep[0m  [72/84], [94mLoss[0m : 2.42885
[1mStep[0m  [80/84], [94mLoss[0m : 2.57646

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.344, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59037
[1mStep[0m  [8/84], [94mLoss[0m : 2.54977
[1mStep[0m  [16/84], [94mLoss[0m : 2.29691
[1mStep[0m  [24/84], [94mLoss[0m : 2.64795
[1mStep[0m  [32/84], [94mLoss[0m : 2.56881
[1mStep[0m  [40/84], [94mLoss[0m : 2.36698
[1mStep[0m  [48/84], [94mLoss[0m : 2.55578
[1mStep[0m  [56/84], [94mLoss[0m : 2.35632
[1mStep[0m  [64/84], [94mLoss[0m : 2.29465
[1mStep[0m  [72/84], [94mLoss[0m : 2.56086
[1mStep[0m  [80/84], [94mLoss[0m : 2.33524

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.331, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.21319
[1mStep[0m  [8/84], [94mLoss[0m : 2.27614
[1mStep[0m  [16/84], [94mLoss[0m : 2.25113
[1mStep[0m  [24/84], [94mLoss[0m : 2.37640
[1mStep[0m  [32/84], [94mLoss[0m : 2.25614
[1mStep[0m  [40/84], [94mLoss[0m : 2.60332
[1mStep[0m  [48/84], [94mLoss[0m : 2.54464
[1mStep[0m  [56/84], [94mLoss[0m : 2.33835
[1mStep[0m  [64/84], [94mLoss[0m : 2.55151
[1mStep[0m  [72/84], [94mLoss[0m : 2.44278
[1mStep[0m  [80/84], [94mLoss[0m : 2.42899

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.336, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.51408
[1mStep[0m  [8/84], [94mLoss[0m : 2.27082
[1mStep[0m  [16/84], [94mLoss[0m : 2.05948
[1mStep[0m  [24/84], [94mLoss[0m : 2.48406
[1mStep[0m  [32/84], [94mLoss[0m : 2.43519
[1mStep[0m  [40/84], [94mLoss[0m : 2.72892
[1mStep[0m  [48/84], [94mLoss[0m : 2.70562
[1mStep[0m  [56/84], [94mLoss[0m : 2.23349
[1mStep[0m  [64/84], [94mLoss[0m : 2.11999
[1mStep[0m  [72/84], [94mLoss[0m : 2.54399
[1mStep[0m  [80/84], [94mLoss[0m : 2.44136

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.340, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41749
[1mStep[0m  [8/84], [94mLoss[0m : 2.45592
[1mStep[0m  [16/84], [94mLoss[0m : 2.58735
[1mStep[0m  [24/84], [94mLoss[0m : 2.41646
[1mStep[0m  [32/84], [94mLoss[0m : 2.49637
[1mStep[0m  [40/84], [94mLoss[0m : 2.52610
[1mStep[0m  [48/84], [94mLoss[0m : 2.35441
[1mStep[0m  [56/84], [94mLoss[0m : 2.42402
[1mStep[0m  [64/84], [94mLoss[0m : 2.26530
[1mStep[0m  [72/84], [94mLoss[0m : 2.14578
[1mStep[0m  [80/84], [94mLoss[0m : 2.17112

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.342, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43605
[1mStep[0m  [8/84], [94mLoss[0m : 2.36779
[1mStep[0m  [16/84], [94mLoss[0m : 2.56879
[1mStep[0m  [24/84], [94mLoss[0m : 2.43560
[1mStep[0m  [32/84], [94mLoss[0m : 2.41476
[1mStep[0m  [40/84], [94mLoss[0m : 2.09851
[1mStep[0m  [48/84], [94mLoss[0m : 2.52640
[1mStep[0m  [56/84], [94mLoss[0m : 2.47896
[1mStep[0m  [64/84], [94mLoss[0m : 2.37052
[1mStep[0m  [72/84], [94mLoss[0m : 2.43173
[1mStep[0m  [80/84], [94mLoss[0m : 2.35314

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.346, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31808
[1mStep[0m  [8/84], [94mLoss[0m : 2.40672
[1mStep[0m  [16/84], [94mLoss[0m : 2.09813
[1mStep[0m  [24/84], [94mLoss[0m : 2.26050
[1mStep[0m  [32/84], [94mLoss[0m : 2.47181
[1mStep[0m  [40/84], [94mLoss[0m : 2.40656
[1mStep[0m  [48/84], [94mLoss[0m : 2.78508
[1mStep[0m  [56/84], [94mLoss[0m : 2.59373
[1mStep[0m  [64/84], [94mLoss[0m : 2.27307
[1mStep[0m  [72/84], [94mLoss[0m : 2.28964
[1mStep[0m  [80/84], [94mLoss[0m : 2.47400

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.347, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.25980
[1mStep[0m  [8/84], [94mLoss[0m : 2.37672
[1mStep[0m  [16/84], [94mLoss[0m : 2.62096
[1mStep[0m  [24/84], [94mLoss[0m : 2.40473
[1mStep[0m  [32/84], [94mLoss[0m : 2.47465
[1mStep[0m  [40/84], [94mLoss[0m : 2.46486
[1mStep[0m  [48/84], [94mLoss[0m : 2.36795
[1mStep[0m  [56/84], [94mLoss[0m : 2.18226
[1mStep[0m  [64/84], [94mLoss[0m : 2.35983
[1mStep[0m  [72/84], [94mLoss[0m : 2.59785
[1mStep[0m  [80/84], [94mLoss[0m : 2.69372

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.344, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26596
[1mStep[0m  [8/84], [94mLoss[0m : 2.20800
[1mStep[0m  [16/84], [94mLoss[0m : 2.72216
[1mStep[0m  [24/84], [94mLoss[0m : 2.30482
[1mStep[0m  [32/84], [94mLoss[0m : 2.20781
[1mStep[0m  [40/84], [94mLoss[0m : 2.50131
[1mStep[0m  [48/84], [94mLoss[0m : 2.55241
[1mStep[0m  [56/84], [94mLoss[0m : 2.43810
[1mStep[0m  [64/84], [94mLoss[0m : 2.69929
[1mStep[0m  [72/84], [94mLoss[0m : 2.28772
[1mStep[0m  [80/84], [94mLoss[0m : 2.38337

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.348, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.08746
[1mStep[0m  [8/84], [94mLoss[0m : 2.68228
[1mStep[0m  [16/84], [94mLoss[0m : 2.39823
[1mStep[0m  [24/84], [94mLoss[0m : 2.65135
[1mStep[0m  [32/84], [94mLoss[0m : 2.51552
[1mStep[0m  [40/84], [94mLoss[0m : 2.24480
[1mStep[0m  [48/84], [94mLoss[0m : 2.60358
[1mStep[0m  [56/84], [94mLoss[0m : 2.47306
[1mStep[0m  [64/84], [94mLoss[0m : 2.63654
[1mStep[0m  [72/84], [94mLoss[0m : 2.73973
[1mStep[0m  [80/84], [94mLoss[0m : 2.66253

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.354, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.341
====================================

Phase 1 - Evaluation MAE:  2.341407741819109
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 2.58021
[1mStep[0m  [8/84], [94mLoss[0m : 2.46295
[1mStep[0m  [16/84], [94mLoss[0m : 2.45074
[1mStep[0m  [24/84], [94mLoss[0m : 2.30490
[1mStep[0m  [32/84], [94mLoss[0m : 2.44140
[1mStep[0m  [40/84], [94mLoss[0m : 2.41527
[1mStep[0m  [48/84], [94mLoss[0m : 2.31272
[1mStep[0m  [56/84], [94mLoss[0m : 2.39142
[1mStep[0m  [64/84], [94mLoss[0m : 2.71073
[1mStep[0m  [72/84], [94mLoss[0m : 2.60077
[1mStep[0m  [80/84], [94mLoss[0m : 2.22496

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.349, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41508
[1mStep[0m  [8/84], [94mLoss[0m : 2.64656
[1mStep[0m  [16/84], [94mLoss[0m : 2.65395
[1mStep[0m  [24/84], [94mLoss[0m : 2.33094
[1mStep[0m  [32/84], [94mLoss[0m : 2.43956
[1mStep[0m  [40/84], [94mLoss[0m : 2.47773
[1mStep[0m  [48/84], [94mLoss[0m : 2.70621
[1mStep[0m  [56/84], [94mLoss[0m : 1.89144
[1mStep[0m  [64/84], [94mLoss[0m : 2.66548
[1mStep[0m  [72/84], [94mLoss[0m : 2.40641
[1mStep[0m  [80/84], [94mLoss[0m : 2.73364

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.378, [92mTest[0m: 2.352, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.09309
[1mStep[0m  [8/84], [94mLoss[0m : 2.25260
[1mStep[0m  [16/84], [94mLoss[0m : 2.37457
[1mStep[0m  [24/84], [94mLoss[0m : 2.34069
[1mStep[0m  [32/84], [94mLoss[0m : 2.18989
[1mStep[0m  [40/84], [94mLoss[0m : 2.14220
[1mStep[0m  [48/84], [94mLoss[0m : 2.53643
[1mStep[0m  [56/84], [94mLoss[0m : 2.13977
[1mStep[0m  [64/84], [94mLoss[0m : 2.17517
[1mStep[0m  [72/84], [94mLoss[0m : 2.40632
[1mStep[0m  [80/84], [94mLoss[0m : 2.37734

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.298, [92mTest[0m: 2.380, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.02127
[1mStep[0m  [8/84], [94mLoss[0m : 2.28735
[1mStep[0m  [16/84], [94mLoss[0m : 2.40617
[1mStep[0m  [24/84], [94mLoss[0m : 2.26057
[1mStep[0m  [32/84], [94mLoss[0m : 2.57951
[1mStep[0m  [40/84], [94mLoss[0m : 2.43609
[1mStep[0m  [48/84], [94mLoss[0m : 2.41439
[1mStep[0m  [56/84], [94mLoss[0m : 2.07239
[1mStep[0m  [64/84], [94mLoss[0m : 2.29612
[1mStep[0m  [72/84], [94mLoss[0m : 2.29064
[1mStep[0m  [80/84], [94mLoss[0m : 2.23780

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.243, [92mTest[0m: 2.367, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.14818
[1mStep[0m  [8/84], [94mLoss[0m : 2.07077
[1mStep[0m  [16/84], [94mLoss[0m : 1.72605
[1mStep[0m  [24/84], [94mLoss[0m : 2.35095
[1mStep[0m  [32/84], [94mLoss[0m : 2.15135
[1mStep[0m  [40/84], [94mLoss[0m : 1.94387
[1mStep[0m  [48/84], [94mLoss[0m : 2.17984
[1mStep[0m  [56/84], [94mLoss[0m : 2.12693
[1mStep[0m  [64/84], [94mLoss[0m : 2.06694
[1mStep[0m  [72/84], [94mLoss[0m : 2.05719
[1mStep[0m  [80/84], [94mLoss[0m : 2.53388

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.164, [92mTest[0m: 2.374, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.06314
[1mStep[0m  [8/84], [94mLoss[0m : 2.01812
[1mStep[0m  [16/84], [94mLoss[0m : 2.27308
[1mStep[0m  [24/84], [94mLoss[0m : 2.27648
[1mStep[0m  [32/84], [94mLoss[0m : 2.30190
[1mStep[0m  [40/84], [94mLoss[0m : 2.03854
[1mStep[0m  [48/84], [94mLoss[0m : 2.07686
[1mStep[0m  [56/84], [94mLoss[0m : 2.15712
[1mStep[0m  [64/84], [94mLoss[0m : 1.67322
[1mStep[0m  [72/84], [94mLoss[0m : 2.12854
[1mStep[0m  [80/84], [94mLoss[0m : 2.04221

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.117, [92mTest[0m: 2.402, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.58960
[1mStep[0m  [8/84], [94mLoss[0m : 2.26636
[1mStep[0m  [16/84], [94mLoss[0m : 1.99967
[1mStep[0m  [24/84], [94mLoss[0m : 2.06969
[1mStep[0m  [32/84], [94mLoss[0m : 2.39312
[1mStep[0m  [40/84], [94mLoss[0m : 2.12758
[1mStep[0m  [48/84], [94mLoss[0m : 2.00004
[1mStep[0m  [56/84], [94mLoss[0m : 2.00593
[1mStep[0m  [64/84], [94mLoss[0m : 2.31000
[1mStep[0m  [72/84], [94mLoss[0m : 2.41377
[1mStep[0m  [80/84], [94mLoss[0m : 2.12963

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.071, [92mTest[0m: 2.424, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.04797
[1mStep[0m  [8/84], [94mLoss[0m : 1.90860
[1mStep[0m  [16/84], [94mLoss[0m : 2.13741
[1mStep[0m  [24/84], [94mLoss[0m : 1.80962
[1mStep[0m  [32/84], [94mLoss[0m : 2.09434
[1mStep[0m  [40/84], [94mLoss[0m : 2.08564
[1mStep[0m  [48/84], [94mLoss[0m : 2.17360
[1mStep[0m  [56/84], [94mLoss[0m : 2.05152
[1mStep[0m  [64/84], [94mLoss[0m : 2.12598
[1mStep[0m  [72/84], [94mLoss[0m : 1.94522
[1mStep[0m  [80/84], [94mLoss[0m : 2.07693

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.030, [92mTest[0m: 2.376, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.72563
[1mStep[0m  [8/84], [94mLoss[0m : 1.93183
[1mStep[0m  [16/84], [94mLoss[0m : 2.19474
[1mStep[0m  [24/84], [94mLoss[0m : 1.80629
[1mStep[0m  [32/84], [94mLoss[0m : 1.64036
[1mStep[0m  [40/84], [94mLoss[0m : 1.87676
[1mStep[0m  [48/84], [94mLoss[0m : 1.94425
[1mStep[0m  [56/84], [94mLoss[0m : 2.16215
[1mStep[0m  [64/84], [94mLoss[0m : 2.02743
[1mStep[0m  [72/84], [94mLoss[0m : 2.18710
[1mStep[0m  [80/84], [94mLoss[0m : 2.19908

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.984, [92mTest[0m: 2.439, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.07574
[1mStep[0m  [8/84], [94mLoss[0m : 2.10615
[1mStep[0m  [16/84], [94mLoss[0m : 1.85932
[1mStep[0m  [24/84], [94mLoss[0m : 1.83709
[1mStep[0m  [32/84], [94mLoss[0m : 1.74769
[1mStep[0m  [40/84], [94mLoss[0m : 1.99646
[1mStep[0m  [48/84], [94mLoss[0m : 1.79509
[1mStep[0m  [56/84], [94mLoss[0m : 1.83622
[1mStep[0m  [64/84], [94mLoss[0m : 1.87971
[1mStep[0m  [72/84], [94mLoss[0m : 1.93222
[1mStep[0m  [80/84], [94mLoss[0m : 1.76760

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.950, [92mTest[0m: 2.419, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.82378
[1mStep[0m  [8/84], [94mLoss[0m : 1.98991
[1mStep[0m  [16/84], [94mLoss[0m : 2.05379
[1mStep[0m  [24/84], [94mLoss[0m : 1.72925
[1mStep[0m  [32/84], [94mLoss[0m : 2.00083
[1mStep[0m  [40/84], [94mLoss[0m : 1.79951
[1mStep[0m  [48/84], [94mLoss[0m : 2.25137
[1mStep[0m  [56/84], [94mLoss[0m : 1.94525
[1mStep[0m  [64/84], [94mLoss[0m : 1.80045
[1mStep[0m  [72/84], [94mLoss[0m : 2.02504
[1mStep[0m  [80/84], [94mLoss[0m : 2.18246

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.921, [92mTest[0m: 2.438, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.78741
[1mStep[0m  [8/84], [94mLoss[0m : 1.88282
[1mStep[0m  [16/84], [94mLoss[0m : 1.74413
[1mStep[0m  [24/84], [94mLoss[0m : 1.79174
[1mStep[0m  [32/84], [94mLoss[0m : 1.80261
[1mStep[0m  [40/84], [94mLoss[0m : 1.89094
[1mStep[0m  [48/84], [94mLoss[0m : 1.93974
[1mStep[0m  [56/84], [94mLoss[0m : 1.92138
[1mStep[0m  [64/84], [94mLoss[0m : 1.88287
[1mStep[0m  [72/84], [94mLoss[0m : 1.80345
[1mStep[0m  [80/84], [94mLoss[0m : 2.13853

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.889, [92mTest[0m: 2.448, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.82737
[1mStep[0m  [8/84], [94mLoss[0m : 1.86398
[1mStep[0m  [16/84], [94mLoss[0m : 1.95716
[1mStep[0m  [24/84], [94mLoss[0m : 1.69749
[1mStep[0m  [32/84], [94mLoss[0m : 1.76835
[1mStep[0m  [40/84], [94mLoss[0m : 1.82670
[1mStep[0m  [48/84], [94mLoss[0m : 2.01095
[1mStep[0m  [56/84], [94mLoss[0m : 1.84428
[1mStep[0m  [64/84], [94mLoss[0m : 2.08038
[1mStep[0m  [72/84], [94mLoss[0m : 2.04889
[1mStep[0m  [80/84], [94mLoss[0m : 2.03728

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.876, [92mTest[0m: 2.466, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.75021
[1mStep[0m  [8/84], [94mLoss[0m : 2.07189
[1mStep[0m  [16/84], [94mLoss[0m : 1.81046
[1mStep[0m  [24/84], [94mLoss[0m : 1.73861
[1mStep[0m  [32/84], [94mLoss[0m : 1.86538
[1mStep[0m  [40/84], [94mLoss[0m : 1.78992
[1mStep[0m  [48/84], [94mLoss[0m : 1.60200
[1mStep[0m  [56/84], [94mLoss[0m : 1.94137
[1mStep[0m  [64/84], [94mLoss[0m : 2.27870
[1mStep[0m  [72/84], [94mLoss[0m : 1.85687
[1mStep[0m  [80/84], [94mLoss[0m : 1.77415

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.856, [92mTest[0m: 2.438, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.65032
[1mStep[0m  [8/84], [94mLoss[0m : 1.45046
[1mStep[0m  [16/84], [94mLoss[0m : 1.73485
[1mStep[0m  [24/84], [94mLoss[0m : 1.99430
[1mStep[0m  [32/84], [94mLoss[0m : 1.89293
[1mStep[0m  [40/84], [94mLoss[0m : 1.64485
[1mStep[0m  [48/84], [94mLoss[0m : 2.09324
[1mStep[0m  [56/84], [94mLoss[0m : 1.72562
[1mStep[0m  [64/84], [94mLoss[0m : 1.74041
[1mStep[0m  [72/84], [94mLoss[0m : 2.13498
[1mStep[0m  [80/84], [94mLoss[0m : 2.20447

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.843, [92mTest[0m: 2.501, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.77772
[1mStep[0m  [8/84], [94mLoss[0m : 1.78630
[1mStep[0m  [16/84], [94mLoss[0m : 1.82602
[1mStep[0m  [24/84], [94mLoss[0m : 1.65445
[1mStep[0m  [32/84], [94mLoss[0m : 1.95633
[1mStep[0m  [40/84], [94mLoss[0m : 1.92585
[1mStep[0m  [48/84], [94mLoss[0m : 1.62829
[1mStep[0m  [56/84], [94mLoss[0m : 1.95514
[1mStep[0m  [64/84], [94mLoss[0m : 1.74465
[1mStep[0m  [72/84], [94mLoss[0m : 1.87481
[1mStep[0m  [80/84], [94mLoss[0m : 1.82854

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.812, [92mTest[0m: 2.500, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.66383
[1mStep[0m  [8/84], [94mLoss[0m : 1.84284
[1mStep[0m  [16/84], [94mLoss[0m : 1.72879
[1mStep[0m  [24/84], [94mLoss[0m : 1.83926
[1mStep[0m  [32/84], [94mLoss[0m : 1.86215
[1mStep[0m  [40/84], [94mLoss[0m : 1.89061
[1mStep[0m  [48/84], [94mLoss[0m : 1.60863
[1mStep[0m  [56/84], [94mLoss[0m : 1.74411
[1mStep[0m  [64/84], [94mLoss[0m : 1.71661
[1mStep[0m  [72/84], [94mLoss[0m : 1.68423
[1mStep[0m  [80/84], [94mLoss[0m : 1.89765

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.795, [92mTest[0m: 2.432, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.77427
[1mStep[0m  [8/84], [94mLoss[0m : 1.66834
[1mStep[0m  [16/84], [94mLoss[0m : 1.79727
[1mStep[0m  [24/84], [94mLoss[0m : 1.67862
[1mStep[0m  [32/84], [94mLoss[0m : 1.87060
[1mStep[0m  [40/84], [94mLoss[0m : 1.79537
[1mStep[0m  [48/84], [94mLoss[0m : 1.90827
[1mStep[0m  [56/84], [94mLoss[0m : 2.03176
[1mStep[0m  [64/84], [94mLoss[0m : 1.91382
[1mStep[0m  [72/84], [94mLoss[0m : 1.87603
[1mStep[0m  [80/84], [94mLoss[0m : 1.62547

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.795, [92mTest[0m: 2.511, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.57202
[1mStep[0m  [8/84], [94mLoss[0m : 1.82782
[1mStep[0m  [16/84], [94mLoss[0m : 1.67755
[1mStep[0m  [24/84], [94mLoss[0m : 1.85139
[1mStep[0m  [32/84], [94mLoss[0m : 1.65667
[1mStep[0m  [40/84], [94mLoss[0m : 2.11788
[1mStep[0m  [48/84], [94mLoss[0m : 1.63847
[1mStep[0m  [56/84], [94mLoss[0m : 1.76971
[1mStep[0m  [64/84], [94mLoss[0m : 1.98316
[1mStep[0m  [72/84], [94mLoss[0m : 1.75505
[1mStep[0m  [80/84], [94mLoss[0m : 1.95732

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.759, [92mTest[0m: 2.467, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.77093
[1mStep[0m  [8/84], [94mLoss[0m : 1.63574
[1mStep[0m  [16/84], [94mLoss[0m : 1.62765
[1mStep[0m  [24/84], [94mLoss[0m : 2.01919
[1mStep[0m  [32/84], [94mLoss[0m : 1.65973
[1mStep[0m  [40/84], [94mLoss[0m : 1.62308
[1mStep[0m  [48/84], [94mLoss[0m : 1.67254
[1mStep[0m  [56/84], [94mLoss[0m : 1.74778
[1mStep[0m  [64/84], [94mLoss[0m : 1.69443
[1mStep[0m  [72/84], [94mLoss[0m : 1.99430
[1mStep[0m  [80/84], [94mLoss[0m : 1.94989

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.753, [92mTest[0m: 2.499, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.88741
[1mStep[0m  [8/84], [94mLoss[0m : 1.71306
[1mStep[0m  [16/84], [94mLoss[0m : 1.64146
[1mStep[0m  [24/84], [94mLoss[0m : 1.70913
[1mStep[0m  [32/84], [94mLoss[0m : 1.88739
[1mStep[0m  [40/84], [94mLoss[0m : 1.89287
[1mStep[0m  [48/84], [94mLoss[0m : 1.62274
[1mStep[0m  [56/84], [94mLoss[0m : 1.54228
[1mStep[0m  [64/84], [94mLoss[0m : 1.74096
[1mStep[0m  [72/84], [94mLoss[0m : 1.80385
[1mStep[0m  [80/84], [94mLoss[0m : 1.47232

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.711, [92mTest[0m: 2.489, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.57383
[1mStep[0m  [8/84], [94mLoss[0m : 1.70086
[1mStep[0m  [16/84], [94mLoss[0m : 1.46156
[1mStep[0m  [24/84], [94mLoss[0m : 1.58999
[1mStep[0m  [32/84], [94mLoss[0m : 1.54848
[1mStep[0m  [40/84], [94mLoss[0m : 1.84005
[1mStep[0m  [48/84], [94mLoss[0m : 1.76569
[1mStep[0m  [56/84], [94mLoss[0m : 1.46900
[1mStep[0m  [64/84], [94mLoss[0m : 1.82196
[1mStep[0m  [72/84], [94mLoss[0m : 2.21437
[1mStep[0m  [80/84], [94mLoss[0m : 1.64686

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.671, [92mTest[0m: 2.456, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.72581
[1mStep[0m  [8/84], [94mLoss[0m : 1.72875
[1mStep[0m  [16/84], [94mLoss[0m : 1.61906
[1mStep[0m  [24/84], [94mLoss[0m : 1.75839
[1mStep[0m  [32/84], [94mLoss[0m : 1.87370
[1mStep[0m  [40/84], [94mLoss[0m : 1.81493
[1mStep[0m  [48/84], [94mLoss[0m : 1.51850
[1mStep[0m  [56/84], [94mLoss[0m : 1.62037
[1mStep[0m  [64/84], [94mLoss[0m : 1.62898
[1mStep[0m  [72/84], [94mLoss[0m : 1.68580
[1mStep[0m  [80/84], [94mLoss[0m : 1.55928

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.676, [92mTest[0m: 2.489, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.41771
[1mStep[0m  [8/84], [94mLoss[0m : 1.71998
[1mStep[0m  [16/84], [94mLoss[0m : 1.55986
[1mStep[0m  [24/84], [94mLoss[0m : 1.40909
[1mStep[0m  [32/84], [94mLoss[0m : 1.45660
[1mStep[0m  [40/84], [94mLoss[0m : 1.55991
[1mStep[0m  [48/84], [94mLoss[0m : 1.54005
[1mStep[0m  [56/84], [94mLoss[0m : 1.68481
[1mStep[0m  [64/84], [94mLoss[0m : 1.84219
[1mStep[0m  [72/84], [94mLoss[0m : 1.53848
[1mStep[0m  [80/84], [94mLoss[0m : 1.74044

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.656, [92mTest[0m: 2.548, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.65318
[1mStep[0m  [8/84], [94mLoss[0m : 1.53611
[1mStep[0m  [16/84], [94mLoss[0m : 1.47754
[1mStep[0m  [24/84], [94mLoss[0m : 1.61536
[1mStep[0m  [32/84], [94mLoss[0m : 1.47075
[1mStep[0m  [40/84], [94mLoss[0m : 1.61812
[1mStep[0m  [48/84], [94mLoss[0m : 1.52049
[1mStep[0m  [56/84], [94mLoss[0m : 1.86644
[1mStep[0m  [64/84], [94mLoss[0m : 1.83187
[1mStep[0m  [72/84], [94mLoss[0m : 1.55375
[1mStep[0m  [80/84], [94mLoss[0m : 1.75819

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.667, [92mTest[0m: 2.463, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.48937
[1mStep[0m  [8/84], [94mLoss[0m : 1.59947
[1mStep[0m  [16/84], [94mLoss[0m : 1.68397
[1mStep[0m  [24/84], [94mLoss[0m : 1.55209
[1mStep[0m  [32/84], [94mLoss[0m : 1.35827
[1mStep[0m  [40/84], [94mLoss[0m : 1.72243
[1mStep[0m  [48/84], [94mLoss[0m : 1.68182
[1mStep[0m  [56/84], [94mLoss[0m : 1.69574
[1mStep[0m  [64/84], [94mLoss[0m : 1.56548
[1mStep[0m  [72/84], [94mLoss[0m : 1.64540
[1mStep[0m  [80/84], [94mLoss[0m : 1.70955

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.641, [92mTest[0m: 2.484, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.43645
[1mStep[0m  [8/84], [94mLoss[0m : 1.71781
[1mStep[0m  [16/84], [94mLoss[0m : 1.63513
[1mStep[0m  [24/84], [94mLoss[0m : 1.53677
[1mStep[0m  [32/84], [94mLoss[0m : 1.63571
[1mStep[0m  [40/84], [94mLoss[0m : 1.52978
[1mStep[0m  [48/84], [94mLoss[0m : 1.73026
[1mStep[0m  [56/84], [94mLoss[0m : 1.52978
[1mStep[0m  [64/84], [94mLoss[0m : 1.55661
[1mStep[0m  [72/84], [94mLoss[0m : 1.70174
[1mStep[0m  [80/84], [94mLoss[0m : 2.00698

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.625, [92mTest[0m: 2.489, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.42092
[1mStep[0m  [8/84], [94mLoss[0m : 1.64076
[1mStep[0m  [16/84], [94mLoss[0m : 1.59092
[1mStep[0m  [24/84], [94mLoss[0m : 1.56356
[1mStep[0m  [32/84], [94mLoss[0m : 1.72870
[1mStep[0m  [40/84], [94mLoss[0m : 1.74561
[1mStep[0m  [48/84], [94mLoss[0m : 1.57838
[1mStep[0m  [56/84], [94mLoss[0m : 1.61110
[1mStep[0m  [64/84], [94mLoss[0m : 1.66092
[1mStep[0m  [72/84], [94mLoss[0m : 1.63634
[1mStep[0m  [80/84], [94mLoss[0m : 1.52791

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.611, [92mTest[0m: 2.444, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.77297
[1mStep[0m  [8/84], [94mLoss[0m : 1.53602
[1mStep[0m  [16/84], [94mLoss[0m : 1.40761
[1mStep[0m  [24/84], [94mLoss[0m : 1.49074
[1mStep[0m  [32/84], [94mLoss[0m : 1.45973
[1mStep[0m  [40/84], [94mLoss[0m : 1.62521
[1mStep[0m  [48/84], [94mLoss[0m : 1.52194
[1mStep[0m  [56/84], [94mLoss[0m : 1.79833
[1mStep[0m  [64/84], [94mLoss[0m : 1.72060
[1mStep[0m  [72/84], [94mLoss[0m : 1.76162
[1mStep[0m  [80/84], [94mLoss[0m : 1.84311

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.628, [92mTest[0m: 2.497, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.76301
[1mStep[0m  [8/84], [94mLoss[0m : 1.56357
[1mStep[0m  [16/84], [94mLoss[0m : 1.59945
[1mStep[0m  [24/84], [94mLoss[0m : 1.79549
[1mStep[0m  [32/84], [94mLoss[0m : 1.64070
[1mStep[0m  [40/84], [94mLoss[0m : 1.63294
[1mStep[0m  [48/84], [94mLoss[0m : 1.52537
[1mStep[0m  [56/84], [94mLoss[0m : 1.38695
[1mStep[0m  [64/84], [94mLoss[0m : 1.58587
[1mStep[0m  [72/84], [94mLoss[0m : 1.57592
[1mStep[0m  [80/84], [94mLoss[0m : 1.92736

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.599, [92mTest[0m: 2.506, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.505
====================================

Phase 2 - Evaluation MAE:  2.50474180494036
MAE score P1      2.341408
MAE score P2      2.504742
loss              1.599264
learning_rate      0.00505
batch_size             128
hidden_sizes         [100]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.9
weight_decay          0.01
Name: 8, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/339], [94mLoss[0m : 9.56660
[1mStep[0m  [33/339], [94mLoss[0m : 3.04259
[1mStep[0m  [66/339], [94mLoss[0m : 2.16209
[1mStep[0m  [99/339], [94mLoss[0m : 2.53041
[1mStep[0m  [132/339], [94mLoss[0m : 2.18538
[1mStep[0m  [165/339], [94mLoss[0m : 1.95977
[1mStep[0m  [198/339], [94mLoss[0m : 2.25769
[1mStep[0m  [231/339], [94mLoss[0m : 1.96844
[1mStep[0m  [264/339], [94mLoss[0m : 2.80503
[1mStep[0m  [297/339], [94mLoss[0m : 2.66336
[1mStep[0m  [330/339], [94mLoss[0m : 1.67128

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.936, [92mTest[0m: 11.249, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.86280
[1mStep[0m  [33/339], [94mLoss[0m : 2.33618
[1mStep[0m  [66/339], [94mLoss[0m : 1.99434
[1mStep[0m  [99/339], [94mLoss[0m : 2.75127
[1mStep[0m  [132/339], [94mLoss[0m : 2.19546
[1mStep[0m  [165/339], [94mLoss[0m : 2.65535
[1mStep[0m  [198/339], [94mLoss[0m : 2.84064
[1mStep[0m  [231/339], [94mLoss[0m : 2.56979
[1mStep[0m  [264/339], [94mLoss[0m : 2.42466
[1mStep[0m  [297/339], [94mLoss[0m : 3.05475
[1mStep[0m  [330/339], [94mLoss[0m : 2.73990

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.529, [92mTest[0m: 2.367, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.82492
[1mStep[0m  [33/339], [94mLoss[0m : 2.35786
[1mStep[0m  [66/339], [94mLoss[0m : 2.88899
[1mStep[0m  [99/339], [94mLoss[0m : 2.03168
[1mStep[0m  [132/339], [94mLoss[0m : 2.85162
[1mStep[0m  [165/339], [94mLoss[0m : 2.13421
[1mStep[0m  [198/339], [94mLoss[0m : 2.22798
[1mStep[0m  [231/339], [94mLoss[0m : 3.03587
[1mStep[0m  [264/339], [94mLoss[0m : 2.60472
[1mStep[0m  [297/339], [94mLoss[0m : 2.15329
[1mStep[0m  [330/339], [94mLoss[0m : 2.26463

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.353, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.04273
[1mStep[0m  [33/339], [94mLoss[0m : 2.17413
[1mStep[0m  [66/339], [94mLoss[0m : 2.34352
[1mStep[0m  [99/339], [94mLoss[0m : 3.15397
[1mStep[0m  [132/339], [94mLoss[0m : 2.91638
[1mStep[0m  [165/339], [94mLoss[0m : 2.21287
[1mStep[0m  [198/339], [94mLoss[0m : 2.35819
[1mStep[0m  [231/339], [94mLoss[0m : 2.27005
[1mStep[0m  [264/339], [94mLoss[0m : 2.60870
[1mStep[0m  [297/339], [94mLoss[0m : 2.16037
[1mStep[0m  [330/339], [94mLoss[0m : 2.27594

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.379, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.11577
[1mStep[0m  [33/339], [94mLoss[0m : 2.06537
[1mStep[0m  [66/339], [94mLoss[0m : 2.55364
[1mStep[0m  [99/339], [94mLoss[0m : 2.46548
[1mStep[0m  [132/339], [94mLoss[0m : 2.83760
[1mStep[0m  [165/339], [94mLoss[0m : 3.04570
[1mStep[0m  [198/339], [94mLoss[0m : 3.10825
[1mStep[0m  [231/339], [94mLoss[0m : 2.24649
[1mStep[0m  [264/339], [94mLoss[0m : 3.02334
[1mStep[0m  [297/339], [94mLoss[0m : 2.51935
[1mStep[0m  [330/339], [94mLoss[0m : 2.60752

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.355, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.06756
[1mStep[0m  [33/339], [94mLoss[0m : 2.69342
[1mStep[0m  [66/339], [94mLoss[0m : 2.31286
[1mStep[0m  [99/339], [94mLoss[0m : 3.40621
[1mStep[0m  [132/339], [94mLoss[0m : 2.49555
[1mStep[0m  [165/339], [94mLoss[0m : 2.41183
[1mStep[0m  [198/339], [94mLoss[0m : 2.39034
[1mStep[0m  [231/339], [94mLoss[0m : 2.31464
[1mStep[0m  [264/339], [94mLoss[0m : 1.97086
[1mStep[0m  [297/339], [94mLoss[0m : 2.27053
[1mStep[0m  [330/339], [94mLoss[0m : 2.82676

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.356, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.72630
[1mStep[0m  [33/339], [94mLoss[0m : 2.21897
[1mStep[0m  [66/339], [94mLoss[0m : 2.29234
[1mStep[0m  [99/339], [94mLoss[0m : 2.63411
[1mStep[0m  [132/339], [94mLoss[0m : 3.33338
[1mStep[0m  [165/339], [94mLoss[0m : 2.30472
[1mStep[0m  [198/339], [94mLoss[0m : 2.60108
[1mStep[0m  [231/339], [94mLoss[0m : 2.02874
[1mStep[0m  [264/339], [94mLoss[0m : 2.58760
[1mStep[0m  [297/339], [94mLoss[0m : 3.26020
[1mStep[0m  [330/339], [94mLoss[0m : 2.79419

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.336, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.96593
[1mStep[0m  [33/339], [94mLoss[0m : 2.19600
[1mStep[0m  [66/339], [94mLoss[0m : 2.11953
[1mStep[0m  [99/339], [94mLoss[0m : 2.83097
[1mStep[0m  [132/339], [94mLoss[0m : 2.64539
[1mStep[0m  [165/339], [94mLoss[0m : 3.03743
[1mStep[0m  [198/339], [94mLoss[0m : 2.54370
[1mStep[0m  [231/339], [94mLoss[0m : 3.24732
[1mStep[0m  [264/339], [94mLoss[0m : 3.15407
[1mStep[0m  [297/339], [94mLoss[0m : 2.20553
[1mStep[0m  [330/339], [94mLoss[0m : 2.38089

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.343, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.36026
[1mStep[0m  [33/339], [94mLoss[0m : 3.04945
[1mStep[0m  [66/339], [94mLoss[0m : 2.14922
[1mStep[0m  [99/339], [94mLoss[0m : 1.73529
[1mStep[0m  [132/339], [94mLoss[0m : 2.41217
[1mStep[0m  [165/339], [94mLoss[0m : 2.60739
[1mStep[0m  [198/339], [94mLoss[0m : 2.23925
[1mStep[0m  [231/339], [94mLoss[0m : 2.74627
[1mStep[0m  [264/339], [94mLoss[0m : 2.46840
[1mStep[0m  [297/339], [94mLoss[0m : 2.35073
[1mStep[0m  [330/339], [94mLoss[0m : 2.33373

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.336, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.98877
[1mStep[0m  [33/339], [94mLoss[0m : 2.05490
[1mStep[0m  [66/339], [94mLoss[0m : 2.11411
[1mStep[0m  [99/339], [94mLoss[0m : 2.20453
[1mStep[0m  [132/339], [94mLoss[0m : 2.13175
[1mStep[0m  [165/339], [94mLoss[0m : 2.12857
[1mStep[0m  [198/339], [94mLoss[0m : 2.66287
[1mStep[0m  [231/339], [94mLoss[0m : 3.06927
[1mStep[0m  [264/339], [94mLoss[0m : 2.18752
[1mStep[0m  [297/339], [94mLoss[0m : 2.38726
[1mStep[0m  [330/339], [94mLoss[0m : 2.56141

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.338, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.66383
[1mStep[0m  [33/339], [94mLoss[0m : 2.35492
[1mStep[0m  [66/339], [94mLoss[0m : 2.72371
[1mStep[0m  [99/339], [94mLoss[0m : 2.26285
[1mStep[0m  [132/339], [94mLoss[0m : 2.97354
[1mStep[0m  [165/339], [94mLoss[0m : 1.84260
[1mStep[0m  [198/339], [94mLoss[0m : 2.03331
[1mStep[0m  [231/339], [94mLoss[0m : 2.40151
[1mStep[0m  [264/339], [94mLoss[0m : 2.10675
[1mStep[0m  [297/339], [94mLoss[0m : 2.52597
[1mStep[0m  [330/339], [94mLoss[0m : 2.51595

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.364, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.33728
[1mStep[0m  [33/339], [94mLoss[0m : 2.21562
[1mStep[0m  [66/339], [94mLoss[0m : 3.11648
[1mStep[0m  [99/339], [94mLoss[0m : 2.53191
[1mStep[0m  [132/339], [94mLoss[0m : 2.65112
[1mStep[0m  [165/339], [94mLoss[0m : 2.61570
[1mStep[0m  [198/339], [94mLoss[0m : 2.00483
[1mStep[0m  [231/339], [94mLoss[0m : 2.42729
[1mStep[0m  [264/339], [94mLoss[0m : 1.95022
[1mStep[0m  [297/339], [94mLoss[0m : 3.61416
[1mStep[0m  [330/339], [94mLoss[0m : 2.04261

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.340, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.25794
[1mStep[0m  [33/339], [94mLoss[0m : 2.47626
[1mStep[0m  [66/339], [94mLoss[0m : 2.05804
[1mStep[0m  [99/339], [94mLoss[0m : 1.94862
[1mStep[0m  [132/339], [94mLoss[0m : 2.82208
[1mStep[0m  [165/339], [94mLoss[0m : 2.67022
[1mStep[0m  [198/339], [94mLoss[0m : 2.65501
[1mStep[0m  [231/339], [94mLoss[0m : 2.25415
[1mStep[0m  [264/339], [94mLoss[0m : 2.66982
[1mStep[0m  [297/339], [94mLoss[0m : 3.12500
[1mStep[0m  [330/339], [94mLoss[0m : 2.60218

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.345, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.94190
[1mStep[0m  [33/339], [94mLoss[0m : 2.54127
[1mStep[0m  [66/339], [94mLoss[0m : 3.14057
[1mStep[0m  [99/339], [94mLoss[0m : 2.90863
[1mStep[0m  [132/339], [94mLoss[0m : 2.23246
[1mStep[0m  [165/339], [94mLoss[0m : 2.62886
[1mStep[0m  [198/339], [94mLoss[0m : 1.76968
[1mStep[0m  [231/339], [94mLoss[0m : 2.63653
[1mStep[0m  [264/339], [94mLoss[0m : 2.48672
[1mStep[0m  [297/339], [94mLoss[0m : 2.48099
[1mStep[0m  [330/339], [94mLoss[0m : 1.95118

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.365, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.96399
[1mStep[0m  [33/339], [94mLoss[0m : 1.72253
[1mStep[0m  [66/339], [94mLoss[0m : 1.84517
[1mStep[0m  [99/339], [94mLoss[0m : 2.64732
[1mStep[0m  [132/339], [94mLoss[0m : 2.64586
[1mStep[0m  [165/339], [94mLoss[0m : 3.00618
[1mStep[0m  [198/339], [94mLoss[0m : 2.11858
[1mStep[0m  [231/339], [94mLoss[0m : 2.27651
[1mStep[0m  [264/339], [94mLoss[0m : 2.22215
[1mStep[0m  [297/339], [94mLoss[0m : 2.22142
[1mStep[0m  [330/339], [94mLoss[0m : 2.64478

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.326, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.61898
[1mStep[0m  [33/339], [94mLoss[0m : 2.23857
[1mStep[0m  [66/339], [94mLoss[0m : 2.61383
[1mStep[0m  [99/339], [94mLoss[0m : 2.28371
[1mStep[0m  [132/339], [94mLoss[0m : 2.66936
[1mStep[0m  [165/339], [94mLoss[0m : 2.60333
[1mStep[0m  [198/339], [94mLoss[0m : 2.26465
[1mStep[0m  [231/339], [94mLoss[0m : 2.27153
[1mStep[0m  [264/339], [94mLoss[0m : 2.47711
[1mStep[0m  [297/339], [94mLoss[0m : 2.42839
[1mStep[0m  [330/339], [94mLoss[0m : 2.16089

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.346, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.41920
[1mStep[0m  [33/339], [94mLoss[0m : 1.93687
[1mStep[0m  [66/339], [94mLoss[0m : 2.01207
[1mStep[0m  [99/339], [94mLoss[0m : 2.33497
[1mStep[0m  [132/339], [94mLoss[0m : 2.57326
[1mStep[0m  [165/339], [94mLoss[0m : 2.30339
[1mStep[0m  [198/339], [94mLoss[0m : 2.92234
[1mStep[0m  [231/339], [94mLoss[0m : 2.22531
[1mStep[0m  [264/339], [94mLoss[0m : 2.45115
[1mStep[0m  [297/339], [94mLoss[0m : 2.36443
[1mStep[0m  [330/339], [94mLoss[0m : 2.48074

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.341, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.92853
[1mStep[0m  [33/339], [94mLoss[0m : 2.46517
[1mStep[0m  [66/339], [94mLoss[0m : 2.34889
[1mStep[0m  [99/339], [94mLoss[0m : 3.02307
[1mStep[0m  [132/339], [94mLoss[0m : 2.04750
[1mStep[0m  [165/339], [94mLoss[0m : 2.33896
[1mStep[0m  [198/339], [94mLoss[0m : 2.96318
[1mStep[0m  [231/339], [94mLoss[0m : 2.46219
[1mStep[0m  [264/339], [94mLoss[0m : 1.98986
[1mStep[0m  [297/339], [94mLoss[0m : 1.56344
[1mStep[0m  [330/339], [94mLoss[0m : 2.68573

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.331, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.71741
[1mStep[0m  [33/339], [94mLoss[0m : 2.22426
[1mStep[0m  [66/339], [94mLoss[0m : 2.54916
[1mStep[0m  [99/339], [94mLoss[0m : 2.32848
[1mStep[0m  [132/339], [94mLoss[0m : 2.86173
[1mStep[0m  [165/339], [94mLoss[0m : 2.63051
[1mStep[0m  [198/339], [94mLoss[0m : 2.78238
[1mStep[0m  [231/339], [94mLoss[0m : 2.74935
[1mStep[0m  [264/339], [94mLoss[0m : 2.68261
[1mStep[0m  [297/339], [94mLoss[0m : 3.11599
[1mStep[0m  [330/339], [94mLoss[0m : 2.39550

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.325, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.16226
[1mStep[0m  [33/339], [94mLoss[0m : 2.76093
[1mStep[0m  [66/339], [94mLoss[0m : 3.12176
[1mStep[0m  [99/339], [94mLoss[0m : 3.07903
[1mStep[0m  [132/339], [94mLoss[0m : 3.06937
[1mStep[0m  [165/339], [94mLoss[0m : 2.32411
[1mStep[0m  [198/339], [94mLoss[0m : 2.55427
[1mStep[0m  [231/339], [94mLoss[0m : 2.63302
[1mStep[0m  [264/339], [94mLoss[0m : 2.11169
[1mStep[0m  [297/339], [94mLoss[0m : 2.04886
[1mStep[0m  [330/339], [94mLoss[0m : 2.49545

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.346, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.62079
[1mStep[0m  [33/339], [94mLoss[0m : 1.67542
[1mStep[0m  [66/339], [94mLoss[0m : 2.80802
[1mStep[0m  [99/339], [94mLoss[0m : 2.46789
[1mStep[0m  [132/339], [94mLoss[0m : 1.71739
[1mStep[0m  [165/339], [94mLoss[0m : 2.57978
[1mStep[0m  [198/339], [94mLoss[0m : 1.70869
[1mStep[0m  [231/339], [94mLoss[0m : 2.72942
[1mStep[0m  [264/339], [94mLoss[0m : 2.07363
[1mStep[0m  [297/339], [94mLoss[0m : 2.50384
[1mStep[0m  [330/339], [94mLoss[0m : 2.69627

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.325, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.46296
[1mStep[0m  [33/339], [94mLoss[0m : 2.56546
[1mStep[0m  [66/339], [94mLoss[0m : 2.90762
[1mStep[0m  [99/339], [94mLoss[0m : 2.07924
[1mStep[0m  [132/339], [94mLoss[0m : 2.72151
[1mStep[0m  [165/339], [94mLoss[0m : 2.57162
[1mStep[0m  [198/339], [94mLoss[0m : 2.96832
[1mStep[0m  [231/339], [94mLoss[0m : 2.28081
[1mStep[0m  [264/339], [94mLoss[0m : 2.05692
[1mStep[0m  [297/339], [94mLoss[0m : 2.70724
[1mStep[0m  [330/339], [94mLoss[0m : 3.07087

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.331, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.24876
[1mStep[0m  [33/339], [94mLoss[0m : 2.68346
[1mStep[0m  [66/339], [94mLoss[0m : 2.33567
[1mStep[0m  [99/339], [94mLoss[0m : 2.30891
[1mStep[0m  [132/339], [94mLoss[0m : 2.41539
[1mStep[0m  [165/339], [94mLoss[0m : 3.10172
[1mStep[0m  [198/339], [94mLoss[0m : 2.04546
[1mStep[0m  [231/339], [94mLoss[0m : 2.62736
[1mStep[0m  [264/339], [94mLoss[0m : 2.15481
[1mStep[0m  [297/339], [94mLoss[0m : 2.53273
[1mStep[0m  [330/339], [94mLoss[0m : 2.68077

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.320, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.60418
[1mStep[0m  [33/339], [94mLoss[0m : 2.86047
[1mStep[0m  [66/339], [94mLoss[0m : 1.63877
[1mStep[0m  [99/339], [94mLoss[0m : 2.70182
[1mStep[0m  [132/339], [94mLoss[0m : 2.24417
[1mStep[0m  [165/339], [94mLoss[0m : 2.00044
[1mStep[0m  [198/339], [94mLoss[0m : 2.75008
[1mStep[0m  [231/339], [94mLoss[0m : 2.03308
[1mStep[0m  [264/339], [94mLoss[0m : 2.12689
[1mStep[0m  [297/339], [94mLoss[0m : 2.88413
[1mStep[0m  [330/339], [94mLoss[0m : 2.36180

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.326, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.41891
[1mStep[0m  [33/339], [94mLoss[0m : 2.64564
[1mStep[0m  [66/339], [94mLoss[0m : 2.54655
[1mStep[0m  [99/339], [94mLoss[0m : 2.76850
[1mStep[0m  [132/339], [94mLoss[0m : 2.28913
[1mStep[0m  [165/339], [94mLoss[0m : 2.45560
[1mStep[0m  [198/339], [94mLoss[0m : 2.42078
[1mStep[0m  [231/339], [94mLoss[0m : 2.55202
[1mStep[0m  [264/339], [94mLoss[0m : 2.80864
[1mStep[0m  [297/339], [94mLoss[0m : 2.61197
[1mStep[0m  [330/339], [94mLoss[0m : 2.26471

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.323, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.68834
[1mStep[0m  [33/339], [94mLoss[0m : 2.70758
[1mStep[0m  [66/339], [94mLoss[0m : 2.68899
[1mStep[0m  [99/339], [94mLoss[0m : 2.54399
[1mStep[0m  [132/339], [94mLoss[0m : 2.17086
[1mStep[0m  [165/339], [94mLoss[0m : 2.62793
[1mStep[0m  [198/339], [94mLoss[0m : 2.32863
[1mStep[0m  [231/339], [94mLoss[0m : 2.46313
[1mStep[0m  [264/339], [94mLoss[0m : 2.08642
[1mStep[0m  [297/339], [94mLoss[0m : 2.47728
[1mStep[0m  [330/339], [94mLoss[0m : 2.59072

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.316, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.95608
[1mStep[0m  [33/339], [94mLoss[0m : 2.46189
[1mStep[0m  [66/339], [94mLoss[0m : 3.21245
[1mStep[0m  [99/339], [94mLoss[0m : 2.14566
[1mStep[0m  [132/339], [94mLoss[0m : 2.75368
[1mStep[0m  [165/339], [94mLoss[0m : 2.15222
[1mStep[0m  [198/339], [94mLoss[0m : 2.38417
[1mStep[0m  [231/339], [94mLoss[0m : 2.50800
[1mStep[0m  [264/339], [94mLoss[0m : 1.80671
[1mStep[0m  [297/339], [94mLoss[0m : 2.70457
[1mStep[0m  [330/339], [94mLoss[0m : 2.53396

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.319, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.26206
[1mStep[0m  [33/339], [94mLoss[0m : 1.87792
[1mStep[0m  [66/339], [94mLoss[0m : 2.39656
[1mStep[0m  [99/339], [94mLoss[0m : 2.97268
[1mStep[0m  [132/339], [94mLoss[0m : 1.96322
[1mStep[0m  [165/339], [94mLoss[0m : 2.76441
[1mStep[0m  [198/339], [94mLoss[0m : 1.90100
[1mStep[0m  [231/339], [94mLoss[0m : 2.91808
[1mStep[0m  [264/339], [94mLoss[0m : 2.20272
[1mStep[0m  [297/339], [94mLoss[0m : 2.59875
[1mStep[0m  [330/339], [94mLoss[0m : 2.22258

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.340, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.20258
[1mStep[0m  [33/339], [94mLoss[0m : 2.55757
[1mStep[0m  [66/339], [94mLoss[0m : 2.47740
[1mStep[0m  [99/339], [94mLoss[0m : 2.37252
[1mStep[0m  [132/339], [94mLoss[0m : 2.59702
[1mStep[0m  [165/339], [94mLoss[0m : 3.33719
[1mStep[0m  [198/339], [94mLoss[0m : 2.64765
[1mStep[0m  [231/339], [94mLoss[0m : 2.16706
[1mStep[0m  [264/339], [94mLoss[0m : 1.65687
[1mStep[0m  [297/339], [94mLoss[0m : 2.19107
[1mStep[0m  [330/339], [94mLoss[0m : 1.96686

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.336, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.56134
[1mStep[0m  [33/339], [94mLoss[0m : 2.15575
[1mStep[0m  [66/339], [94mLoss[0m : 2.29647
[1mStep[0m  [99/339], [94mLoss[0m : 2.47639
[1mStep[0m  [132/339], [94mLoss[0m : 2.40320
[1mStep[0m  [165/339], [94mLoss[0m : 2.83568
[1mStep[0m  [198/339], [94mLoss[0m : 2.34805
[1mStep[0m  [231/339], [94mLoss[0m : 2.27506
[1mStep[0m  [264/339], [94mLoss[0m : 2.48235
[1mStep[0m  [297/339], [94mLoss[0m : 2.09591
[1mStep[0m  [330/339], [94mLoss[0m : 2.29777

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.316, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.330
====================================

Phase 1 - Evaluation MAE:  2.3304284272995672
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/339], [94mLoss[0m : 2.11803
[1mStep[0m  [33/339], [94mLoss[0m : 2.48408
[1mStep[0m  [66/339], [94mLoss[0m : 2.48897
[1mStep[0m  [99/339], [94mLoss[0m : 2.75884
[1mStep[0m  [132/339], [94mLoss[0m : 2.21894
[1mStep[0m  [165/339], [94mLoss[0m : 2.70108
[1mStep[0m  [198/339], [94mLoss[0m : 2.93424
[1mStep[0m  [231/339], [94mLoss[0m : 2.28085
[1mStep[0m  [264/339], [94mLoss[0m : 2.24397
[1mStep[0m  [297/339], [94mLoss[0m : 2.19954
[1mStep[0m  [330/339], [94mLoss[0m : 2.01643

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.333, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.67866
[1mStep[0m  [33/339], [94mLoss[0m : 2.16448
[1mStep[0m  [66/339], [94mLoss[0m : 2.62079
[1mStep[0m  [99/339], [94mLoss[0m : 2.36292
[1mStep[0m  [132/339], [94mLoss[0m : 2.62576
[1mStep[0m  [165/339], [94mLoss[0m : 2.90583
[1mStep[0m  [198/339], [94mLoss[0m : 3.28939
[1mStep[0m  [231/339], [94mLoss[0m : 3.02606
[1mStep[0m  [264/339], [94mLoss[0m : 2.21421
[1mStep[0m  [297/339], [94mLoss[0m : 2.44418
[1mStep[0m  [330/339], [94mLoss[0m : 2.67172

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.374, [92mTest[0m: 2.412, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.40786
[1mStep[0m  [33/339], [94mLoss[0m : 2.50662
[1mStep[0m  [66/339], [94mLoss[0m : 1.86609
[1mStep[0m  [99/339], [94mLoss[0m : 1.66151
[1mStep[0m  [132/339], [94mLoss[0m : 2.62199
[1mStep[0m  [165/339], [94mLoss[0m : 2.77944
[1mStep[0m  [198/339], [94mLoss[0m : 2.67142
[1mStep[0m  [231/339], [94mLoss[0m : 2.51419
[1mStep[0m  [264/339], [94mLoss[0m : 1.95572
[1mStep[0m  [297/339], [94mLoss[0m : 2.48871
[1mStep[0m  [330/339], [94mLoss[0m : 1.81864

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.301, [92mTest[0m: 2.380, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.43706
[1mStep[0m  [33/339], [94mLoss[0m : 2.63411
[1mStep[0m  [66/339], [94mLoss[0m : 2.06343
[1mStep[0m  [99/339], [94mLoss[0m : 1.93528
[1mStep[0m  [132/339], [94mLoss[0m : 2.75114
[1mStep[0m  [165/339], [94mLoss[0m : 2.26331
[1mStep[0m  [198/339], [94mLoss[0m : 1.83636
[1mStep[0m  [231/339], [94mLoss[0m : 2.16677
[1mStep[0m  [264/339], [94mLoss[0m : 1.95942
[1mStep[0m  [297/339], [94mLoss[0m : 2.22348
[1mStep[0m  [330/339], [94mLoss[0m : 2.37716

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.254, [92mTest[0m: 2.378, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.60784
[1mStep[0m  [33/339], [94mLoss[0m : 1.62897
[1mStep[0m  [66/339], [94mLoss[0m : 1.94621
[1mStep[0m  [99/339], [94mLoss[0m : 2.56581
[1mStep[0m  [132/339], [94mLoss[0m : 2.69016
[1mStep[0m  [165/339], [94mLoss[0m : 2.14503
[1mStep[0m  [198/339], [94mLoss[0m : 1.69993
[1mStep[0m  [231/339], [94mLoss[0m : 1.98984
[1mStep[0m  [264/339], [94mLoss[0m : 2.68351
[1mStep[0m  [297/339], [94mLoss[0m : 2.39090
[1mStep[0m  [330/339], [94mLoss[0m : 2.41643

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.209, [92mTest[0m: 2.398, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.74765
[1mStep[0m  [33/339], [94mLoss[0m : 2.28974
[1mStep[0m  [66/339], [94mLoss[0m : 2.01924
[1mStep[0m  [99/339], [94mLoss[0m : 2.04317
[1mStep[0m  [132/339], [94mLoss[0m : 1.94967
[1mStep[0m  [165/339], [94mLoss[0m : 2.25503
[1mStep[0m  [198/339], [94mLoss[0m : 1.97212
[1mStep[0m  [231/339], [94mLoss[0m : 2.42831
[1mStep[0m  [264/339], [94mLoss[0m : 2.54724
[1mStep[0m  [297/339], [94mLoss[0m : 2.42215
[1mStep[0m  [330/339], [94mLoss[0m : 2.45580

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.158, [92mTest[0m: 2.416, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.10161
[1mStep[0m  [33/339], [94mLoss[0m : 1.97227
[1mStep[0m  [66/339], [94mLoss[0m : 2.26789
[1mStep[0m  [99/339], [94mLoss[0m : 2.14224
[1mStep[0m  [132/339], [94mLoss[0m : 1.87344
[1mStep[0m  [165/339], [94mLoss[0m : 1.73449
[1mStep[0m  [198/339], [94mLoss[0m : 2.01562
[1mStep[0m  [231/339], [94mLoss[0m : 2.04321
[1mStep[0m  [264/339], [94mLoss[0m : 1.42893
[1mStep[0m  [297/339], [94mLoss[0m : 2.16768
[1mStep[0m  [330/339], [94mLoss[0m : 1.84197

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.131, [92mTest[0m: 2.387, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.96652
[1mStep[0m  [33/339], [94mLoss[0m : 1.69209
[1mStep[0m  [66/339], [94mLoss[0m : 2.15220
[1mStep[0m  [99/339], [94mLoss[0m : 1.89586
[1mStep[0m  [132/339], [94mLoss[0m : 1.70106
[1mStep[0m  [165/339], [94mLoss[0m : 2.19175
[1mStep[0m  [198/339], [94mLoss[0m : 1.80832
[1mStep[0m  [231/339], [94mLoss[0m : 3.07043
[1mStep[0m  [264/339], [94mLoss[0m : 1.67523
[1mStep[0m  [297/339], [94mLoss[0m : 1.63561
[1mStep[0m  [330/339], [94mLoss[0m : 2.31857

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.080, [92mTest[0m: 2.422, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.87917
[1mStep[0m  [33/339], [94mLoss[0m : 2.11178
[1mStep[0m  [66/339], [94mLoss[0m : 2.15557
[1mStep[0m  [99/339], [94mLoss[0m : 2.02859
[1mStep[0m  [132/339], [94mLoss[0m : 1.78842
[1mStep[0m  [165/339], [94mLoss[0m : 1.82999
[1mStep[0m  [198/339], [94mLoss[0m : 1.99956
[1mStep[0m  [231/339], [94mLoss[0m : 1.80562
[1mStep[0m  [264/339], [94mLoss[0m : 2.15006
[1mStep[0m  [297/339], [94mLoss[0m : 1.95766
[1mStep[0m  [330/339], [94mLoss[0m : 2.12876

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.033, [92mTest[0m: 2.403, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.82479
[1mStep[0m  [33/339], [94mLoss[0m : 1.90249
[1mStep[0m  [66/339], [94mLoss[0m : 1.73063
[1mStep[0m  [99/339], [94mLoss[0m : 2.04592
[1mStep[0m  [132/339], [94mLoss[0m : 2.37082
[1mStep[0m  [165/339], [94mLoss[0m : 2.57318
[1mStep[0m  [198/339], [94mLoss[0m : 1.95514
[1mStep[0m  [231/339], [94mLoss[0m : 2.36848
[1mStep[0m  [264/339], [94mLoss[0m : 1.64045
[1mStep[0m  [297/339], [94mLoss[0m : 2.09709
[1mStep[0m  [330/339], [94mLoss[0m : 1.78077

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.994, [92mTest[0m: 2.451, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.56480
[1mStep[0m  [33/339], [94mLoss[0m : 1.40114
[1mStep[0m  [66/339], [94mLoss[0m : 1.25333
[1mStep[0m  [99/339], [94mLoss[0m : 2.20796
[1mStep[0m  [132/339], [94mLoss[0m : 1.80748
[1mStep[0m  [165/339], [94mLoss[0m : 2.09854
[1mStep[0m  [198/339], [94mLoss[0m : 1.91221
[1mStep[0m  [231/339], [94mLoss[0m : 1.74229
[1mStep[0m  [264/339], [94mLoss[0m : 1.86047
[1mStep[0m  [297/339], [94mLoss[0m : 1.86430
[1mStep[0m  [330/339], [94mLoss[0m : 2.19765

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.959, [92mTest[0m: 2.453, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.52623
[1mStep[0m  [33/339], [94mLoss[0m : 1.46131
[1mStep[0m  [66/339], [94mLoss[0m : 2.09015
[1mStep[0m  [99/339], [94mLoss[0m : 2.06014
[1mStep[0m  [132/339], [94mLoss[0m : 1.92004
[1mStep[0m  [165/339], [94mLoss[0m : 1.89742
[1mStep[0m  [198/339], [94mLoss[0m : 2.03287
[1mStep[0m  [231/339], [94mLoss[0m : 2.21683
[1mStep[0m  [264/339], [94mLoss[0m : 2.50355
[1mStep[0m  [297/339], [94mLoss[0m : 1.82754
[1mStep[0m  [330/339], [94mLoss[0m : 2.15471

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.932, [92mTest[0m: 2.414, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.67826
[1mStep[0m  [33/339], [94mLoss[0m : 1.60820
[1mStep[0m  [66/339], [94mLoss[0m : 1.39862
[1mStep[0m  [99/339], [94mLoss[0m : 1.92563
[1mStep[0m  [132/339], [94mLoss[0m : 1.74921
[1mStep[0m  [165/339], [94mLoss[0m : 1.46632
[1mStep[0m  [198/339], [94mLoss[0m : 1.87823
[1mStep[0m  [231/339], [94mLoss[0m : 2.31336
[1mStep[0m  [264/339], [94mLoss[0m : 1.90247
[1mStep[0m  [297/339], [94mLoss[0m : 1.84683
[1mStep[0m  [330/339], [94mLoss[0m : 2.15093

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.891, [92mTest[0m: 2.468, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.80033
[1mStep[0m  [33/339], [94mLoss[0m : 2.23102
[1mStep[0m  [66/339], [94mLoss[0m : 1.77947
[1mStep[0m  [99/339], [94mLoss[0m : 1.57168
[1mStep[0m  [132/339], [94mLoss[0m : 1.94578
[1mStep[0m  [165/339], [94mLoss[0m : 1.60803
[1mStep[0m  [198/339], [94mLoss[0m : 1.93095
[1mStep[0m  [231/339], [94mLoss[0m : 1.91566
[1mStep[0m  [264/339], [94mLoss[0m : 1.47447
[1mStep[0m  [297/339], [94mLoss[0m : 1.75761
[1mStep[0m  [330/339], [94mLoss[0m : 2.15369

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.858, [92mTest[0m: 2.452, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.50554
[1mStep[0m  [33/339], [94mLoss[0m : 1.65804
[1mStep[0m  [66/339], [94mLoss[0m : 1.55957
[1mStep[0m  [99/339], [94mLoss[0m : 1.90833
[1mStep[0m  [132/339], [94mLoss[0m : 1.46593
[1mStep[0m  [165/339], [94mLoss[0m : 1.99345
[1mStep[0m  [198/339], [94mLoss[0m : 1.97305
[1mStep[0m  [231/339], [94mLoss[0m : 2.44925
[1mStep[0m  [264/339], [94mLoss[0m : 1.46636
[1mStep[0m  [297/339], [94mLoss[0m : 2.12092
[1mStep[0m  [330/339], [94mLoss[0m : 1.77896

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.848, [92mTest[0m: 2.475, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.65891
[1mStep[0m  [33/339], [94mLoss[0m : 2.22530
[1mStep[0m  [66/339], [94mLoss[0m : 1.75181
[1mStep[0m  [99/339], [94mLoss[0m : 1.97603
[1mStep[0m  [132/339], [94mLoss[0m : 1.71096
[1mStep[0m  [165/339], [94mLoss[0m : 1.74130
[1mStep[0m  [198/339], [94mLoss[0m : 2.19210
[1mStep[0m  [231/339], [94mLoss[0m : 1.49723
[1mStep[0m  [264/339], [94mLoss[0m : 1.31552
[1mStep[0m  [297/339], [94mLoss[0m : 1.62400
[1mStep[0m  [330/339], [94mLoss[0m : 2.10368

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.829, [92mTest[0m: 2.424, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.43100
[1mStep[0m  [33/339], [94mLoss[0m : 1.76528
[1mStep[0m  [66/339], [94mLoss[0m : 2.28504
[1mStep[0m  [99/339], [94mLoss[0m : 2.11247
[1mStep[0m  [132/339], [94mLoss[0m : 1.49516
[1mStep[0m  [165/339], [94mLoss[0m : 2.15496
[1mStep[0m  [198/339], [94mLoss[0m : 1.26938
[1mStep[0m  [231/339], [94mLoss[0m : 1.33026
[1mStep[0m  [264/339], [94mLoss[0m : 1.79668
[1mStep[0m  [297/339], [94mLoss[0m : 1.86466
[1mStep[0m  [330/339], [94mLoss[0m : 1.83993

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.799, [92mTest[0m: 2.539, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.30116
[1mStep[0m  [33/339], [94mLoss[0m : 1.73924
[1mStep[0m  [66/339], [94mLoss[0m : 2.09050
[1mStep[0m  [99/339], [94mLoss[0m : 1.81504
[1mStep[0m  [132/339], [94mLoss[0m : 1.91753
[1mStep[0m  [165/339], [94mLoss[0m : 1.77498
[1mStep[0m  [198/339], [94mLoss[0m : 1.54890
[1mStep[0m  [231/339], [94mLoss[0m : 1.52121
[1mStep[0m  [264/339], [94mLoss[0m : 1.67632
[1mStep[0m  [297/339], [94mLoss[0m : 1.61569
[1mStep[0m  [330/339], [94mLoss[0m : 1.77992

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.756, [92mTest[0m: 2.474, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.56536
[1mStep[0m  [33/339], [94mLoss[0m : 1.71015
[1mStep[0m  [66/339], [94mLoss[0m : 2.00771
[1mStep[0m  [99/339], [94mLoss[0m : 1.61949
[1mStep[0m  [132/339], [94mLoss[0m : 1.57367
[1mStep[0m  [165/339], [94mLoss[0m : 1.55042
[1mStep[0m  [198/339], [94mLoss[0m : 1.91255
[1mStep[0m  [231/339], [94mLoss[0m : 1.69761
[1mStep[0m  [264/339], [94mLoss[0m : 1.92078
[1mStep[0m  [297/339], [94mLoss[0m : 1.82587
[1mStep[0m  [330/339], [94mLoss[0m : 1.62973

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.736, [92mTest[0m: 2.490, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.64777
[1mStep[0m  [33/339], [94mLoss[0m : 1.34155
[1mStep[0m  [66/339], [94mLoss[0m : 1.52385
[1mStep[0m  [99/339], [94mLoss[0m : 1.72095
[1mStep[0m  [132/339], [94mLoss[0m : 2.00590
[1mStep[0m  [165/339], [94mLoss[0m : 1.56536
[1mStep[0m  [198/339], [94mLoss[0m : 2.02199
[1mStep[0m  [231/339], [94mLoss[0m : 1.54955
[1mStep[0m  [264/339], [94mLoss[0m : 1.77468
[1mStep[0m  [297/339], [94mLoss[0m : 1.48340
[1mStep[0m  [330/339], [94mLoss[0m : 1.75882

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.707, [92mTest[0m: 2.510, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.44612
[1mStep[0m  [33/339], [94mLoss[0m : 1.84917
[1mStep[0m  [66/339], [94mLoss[0m : 1.96354
[1mStep[0m  [99/339], [94mLoss[0m : 1.39383
[1mStep[0m  [132/339], [94mLoss[0m : 1.83298
[1mStep[0m  [165/339], [94mLoss[0m : 1.75092
[1mStep[0m  [198/339], [94mLoss[0m : 1.73065
[1mStep[0m  [231/339], [94mLoss[0m : 1.78886
[1mStep[0m  [264/339], [94mLoss[0m : 2.27572
[1mStep[0m  [297/339], [94mLoss[0m : 1.89633
[1mStep[0m  [330/339], [94mLoss[0m : 2.18785

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.661, [92mTest[0m: 2.521, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.47680
[1mStep[0m  [33/339], [94mLoss[0m : 2.47847
[1mStep[0m  [66/339], [94mLoss[0m : 1.98070
[1mStep[0m  [99/339], [94mLoss[0m : 1.59073
[1mStep[0m  [132/339], [94mLoss[0m : 1.30601
[1mStep[0m  [165/339], [94mLoss[0m : 1.79840
[1mStep[0m  [198/339], [94mLoss[0m : 1.72473
[1mStep[0m  [231/339], [94mLoss[0m : 1.08091
[1mStep[0m  [264/339], [94mLoss[0m : 1.55265
[1mStep[0m  [297/339], [94mLoss[0m : 1.48984
[1mStep[0m  [330/339], [94mLoss[0m : 1.54476

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.647, [92mTest[0m: 2.529, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.07301
[1mStep[0m  [33/339], [94mLoss[0m : 1.56175
[1mStep[0m  [66/339], [94mLoss[0m : 1.57218
[1mStep[0m  [99/339], [94mLoss[0m : 1.67826
[1mStep[0m  [132/339], [94mLoss[0m : 1.75214
[1mStep[0m  [165/339], [94mLoss[0m : 1.26888
[1mStep[0m  [198/339], [94mLoss[0m : 1.49501
[1mStep[0m  [231/339], [94mLoss[0m : 1.48427
[1mStep[0m  [264/339], [94mLoss[0m : 1.42772
[1mStep[0m  [297/339], [94mLoss[0m : 1.54716
[1mStep[0m  [330/339], [94mLoss[0m : 1.87021

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.617, [92mTest[0m: 2.505, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.08390
[1mStep[0m  [33/339], [94mLoss[0m : 1.35351
[1mStep[0m  [66/339], [94mLoss[0m : 1.23135
[1mStep[0m  [99/339], [94mLoss[0m : 1.62452
[1mStep[0m  [132/339], [94mLoss[0m : 1.64961
[1mStep[0m  [165/339], [94mLoss[0m : 1.50075
[1mStep[0m  [198/339], [94mLoss[0m : 1.85744
[1mStep[0m  [231/339], [94mLoss[0m : 1.16596
[1mStep[0m  [264/339], [94mLoss[0m : 1.60861
[1mStep[0m  [297/339], [94mLoss[0m : 1.98882
[1mStep[0m  [330/339], [94mLoss[0m : 1.52895

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.591, [92mTest[0m: 2.538, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.97542
[1mStep[0m  [33/339], [94mLoss[0m : 1.62352
[1mStep[0m  [66/339], [94mLoss[0m : 1.61748
[1mStep[0m  [99/339], [94mLoss[0m : 1.59745
[1mStep[0m  [132/339], [94mLoss[0m : 1.15722
[1mStep[0m  [165/339], [94mLoss[0m : 1.67119
[1mStep[0m  [198/339], [94mLoss[0m : 1.76249
[1mStep[0m  [231/339], [94mLoss[0m : 1.26857
[1mStep[0m  [264/339], [94mLoss[0m : 0.95665
[1mStep[0m  [297/339], [94mLoss[0m : 1.66659
[1mStep[0m  [330/339], [94mLoss[0m : 1.73875

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.579, [92mTest[0m: 2.547, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.53810
[1mStep[0m  [33/339], [94mLoss[0m : 1.62808
[1mStep[0m  [66/339], [94mLoss[0m : 1.21665
[1mStep[0m  [99/339], [94mLoss[0m : 1.55016
[1mStep[0m  [132/339], [94mLoss[0m : 1.66103
[1mStep[0m  [165/339], [94mLoss[0m : 1.58688
[1mStep[0m  [198/339], [94mLoss[0m : 1.50588
[1mStep[0m  [231/339], [94mLoss[0m : 1.14724
[1mStep[0m  [264/339], [94mLoss[0m : 1.61843
[1mStep[0m  [297/339], [94mLoss[0m : 1.38583
[1mStep[0m  [330/339], [94mLoss[0m : 1.72668

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.581, [92mTest[0m: 2.516, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.84912
[1mStep[0m  [33/339], [94mLoss[0m : 1.52838
[1mStep[0m  [66/339], [94mLoss[0m : 1.32946
[1mStep[0m  [99/339], [94mLoss[0m : 1.38246
[1mStep[0m  [132/339], [94mLoss[0m : 1.69521
[1mStep[0m  [165/339], [94mLoss[0m : 1.50621
[1mStep[0m  [198/339], [94mLoss[0m : 1.57045
[1mStep[0m  [231/339], [94mLoss[0m : 1.25861
[1mStep[0m  [264/339], [94mLoss[0m : 1.70585
[1mStep[0m  [297/339], [94mLoss[0m : 1.76770
[1mStep[0m  [330/339], [94mLoss[0m : 2.14021

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.538, [92mTest[0m: 2.582, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.12731
[1mStep[0m  [33/339], [94mLoss[0m : 1.98398
[1mStep[0m  [66/339], [94mLoss[0m : 1.40285
[1mStep[0m  [99/339], [94mLoss[0m : 1.29024
[1mStep[0m  [132/339], [94mLoss[0m : 1.65245
[1mStep[0m  [165/339], [94mLoss[0m : 1.64639
[1mStep[0m  [198/339], [94mLoss[0m : 1.68685
[1mStep[0m  [231/339], [94mLoss[0m : 1.87234
[1mStep[0m  [264/339], [94mLoss[0m : 1.44819
[1mStep[0m  [297/339], [94mLoss[0m : 1.28200
[1mStep[0m  [330/339], [94mLoss[0m : 1.85126

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.537, [92mTest[0m: 2.528, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.76086
[1mStep[0m  [33/339], [94mLoss[0m : 1.49701
[1mStep[0m  [66/339], [94mLoss[0m : 1.36118
[1mStep[0m  [99/339], [94mLoss[0m : 1.29531
[1mStep[0m  [132/339], [94mLoss[0m : 1.72035
[1mStep[0m  [165/339], [94mLoss[0m : 1.60226
[1mStep[0m  [198/339], [94mLoss[0m : 1.19405
[1mStep[0m  [231/339], [94mLoss[0m : 2.68266
[1mStep[0m  [264/339], [94mLoss[0m : 1.15260
[1mStep[0m  [297/339], [94mLoss[0m : 1.62378
[1mStep[0m  [330/339], [94mLoss[0m : 1.43509

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.527, [92mTest[0m: 2.506, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.09574
[1mStep[0m  [33/339], [94mLoss[0m : 1.28656
[1mStep[0m  [66/339], [94mLoss[0m : 1.75118
[1mStep[0m  [99/339], [94mLoss[0m : 1.94992
[1mStep[0m  [132/339], [94mLoss[0m : 1.30611
[1mStep[0m  [165/339], [94mLoss[0m : 1.17462
[1mStep[0m  [198/339], [94mLoss[0m : 1.08934
[1mStep[0m  [231/339], [94mLoss[0m : 1.51191
[1mStep[0m  [264/339], [94mLoss[0m : 1.85723
[1mStep[0m  [297/339], [94mLoss[0m : 1.25734
[1mStep[0m  [330/339], [94mLoss[0m : 1.59724

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.498, [92mTest[0m: 2.586, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.576
====================================

Phase 2 - Evaluation MAE:  2.5755796812276923
MAE score P1       2.330428
MAE score P2        2.57558
loss               1.498088
learning_rate       0.00505
batch_size               32
hidden_sizes      [200, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.9
weight_decay         0.0001
Name: 9, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 10.33759
[1mStep[0m  [8/84], [94mLoss[0m : 7.12672
[1mStep[0m  [16/84], [94mLoss[0m : 4.46167
[1mStep[0m  [24/84], [94mLoss[0m : 3.61182
[1mStep[0m  [32/84], [94mLoss[0m : 3.17078
[1mStep[0m  [40/84], [94mLoss[0m : 2.85706
[1mStep[0m  [48/84], [94mLoss[0m : 2.32620
[1mStep[0m  [56/84], [94mLoss[0m : 2.68368
[1mStep[0m  [64/84], [94mLoss[0m : 2.88628
[1mStep[0m  [72/84], [94mLoss[0m : 2.30479
[1mStep[0m  [80/84], [94mLoss[0m : 2.55297

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.797, [92mTest[0m: 10.718, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49076
[1mStep[0m  [8/84], [94mLoss[0m : 2.82384
[1mStep[0m  [16/84], [94mLoss[0m : 2.60414
[1mStep[0m  [24/84], [94mLoss[0m : 2.94514
[1mStep[0m  [32/84], [94mLoss[0m : 2.59292
[1mStep[0m  [40/84], [94mLoss[0m : 2.84556
[1mStep[0m  [48/84], [94mLoss[0m : 2.55387
[1mStep[0m  [56/84], [94mLoss[0m : 2.70439
[1mStep[0m  [64/84], [94mLoss[0m : 2.67590
[1mStep[0m  [72/84], [94mLoss[0m : 2.69146
[1mStep[0m  [80/84], [94mLoss[0m : 2.40776

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.594, [92mTest[0m: 2.755, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44126
[1mStep[0m  [8/84], [94mLoss[0m : 2.53072
[1mStep[0m  [16/84], [94mLoss[0m : 2.46368
[1mStep[0m  [24/84], [94mLoss[0m : 2.73631
[1mStep[0m  [32/84], [94mLoss[0m : 2.59048
[1mStep[0m  [40/84], [94mLoss[0m : 2.36434
[1mStep[0m  [48/84], [94mLoss[0m : 2.37306
[1mStep[0m  [56/84], [94mLoss[0m : 2.29838
[1mStep[0m  [64/84], [94mLoss[0m : 2.40419
[1mStep[0m  [72/84], [94mLoss[0m : 2.56142
[1mStep[0m  [80/84], [94mLoss[0m : 2.42461

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.535, [92mTest[0m: 2.576, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.57468
[1mStep[0m  [8/84], [94mLoss[0m : 2.40955
[1mStep[0m  [16/84], [94mLoss[0m : 2.82388
[1mStep[0m  [24/84], [94mLoss[0m : 2.69921
[1mStep[0m  [32/84], [94mLoss[0m : 2.39671
[1mStep[0m  [40/84], [94mLoss[0m : 2.52959
[1mStep[0m  [48/84], [94mLoss[0m : 2.09820
[1mStep[0m  [56/84], [94mLoss[0m : 2.68699
[1mStep[0m  [64/84], [94mLoss[0m : 2.65992
[1mStep[0m  [72/84], [94mLoss[0m : 2.47363
[1mStep[0m  [80/84], [94mLoss[0m : 2.81612

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.591, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40386
[1mStep[0m  [8/84], [94mLoss[0m : 2.44406
[1mStep[0m  [16/84], [94mLoss[0m : 2.77900
[1mStep[0m  [24/84], [94mLoss[0m : 2.69639
[1mStep[0m  [32/84], [94mLoss[0m : 2.74040
[1mStep[0m  [40/84], [94mLoss[0m : 2.25375
[1mStep[0m  [48/84], [94mLoss[0m : 2.62847
[1mStep[0m  [56/84], [94mLoss[0m : 2.71949
[1mStep[0m  [64/84], [94mLoss[0m : 2.55122
[1mStep[0m  [72/84], [94mLoss[0m : 2.65108
[1mStep[0m  [80/84], [94mLoss[0m : 2.64831

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.514, [92mTest[0m: 2.537, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48814
[1mStep[0m  [8/84], [94mLoss[0m : 2.40070
[1mStep[0m  [16/84], [94mLoss[0m : 2.44731
[1mStep[0m  [24/84], [94mLoss[0m : 2.17651
[1mStep[0m  [32/84], [94mLoss[0m : 2.53101
[1mStep[0m  [40/84], [94mLoss[0m : 2.21959
[1mStep[0m  [48/84], [94mLoss[0m : 2.34797
[1mStep[0m  [56/84], [94mLoss[0m : 2.61876
[1mStep[0m  [64/84], [94mLoss[0m : 2.26848
[1mStep[0m  [72/84], [94mLoss[0m : 2.39038
[1mStep[0m  [80/84], [94mLoss[0m : 2.52040

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.505, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53153
[1mStep[0m  [8/84], [94mLoss[0m : 2.30325
[1mStep[0m  [16/84], [94mLoss[0m : 2.42698
[1mStep[0m  [24/84], [94mLoss[0m : 2.62921
[1mStep[0m  [32/84], [94mLoss[0m : 2.24188
[1mStep[0m  [40/84], [94mLoss[0m : 2.52987
[1mStep[0m  [48/84], [94mLoss[0m : 2.54536
[1mStep[0m  [56/84], [94mLoss[0m : 2.78098
[1mStep[0m  [64/84], [94mLoss[0m : 2.13312
[1mStep[0m  [72/84], [94mLoss[0m : 2.41693
[1mStep[0m  [80/84], [94mLoss[0m : 2.45939

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.507, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.62668
[1mStep[0m  [8/84], [94mLoss[0m : 2.69156
[1mStep[0m  [16/84], [94mLoss[0m : 2.65216
[1mStep[0m  [24/84], [94mLoss[0m : 2.26321
[1mStep[0m  [32/84], [94mLoss[0m : 2.48143
[1mStep[0m  [40/84], [94mLoss[0m : 2.35597
[1mStep[0m  [48/84], [94mLoss[0m : 2.36527
[1mStep[0m  [56/84], [94mLoss[0m : 2.27598
[1mStep[0m  [64/84], [94mLoss[0m : 2.67640
[1mStep[0m  [72/84], [94mLoss[0m : 2.32652
[1mStep[0m  [80/84], [94mLoss[0m : 2.63348

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.540, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.10932
[1mStep[0m  [8/84], [94mLoss[0m : 2.41203
[1mStep[0m  [16/84], [94mLoss[0m : 2.26207
[1mStep[0m  [24/84], [94mLoss[0m : 2.23201
[1mStep[0m  [32/84], [94mLoss[0m : 2.28049
[1mStep[0m  [40/84], [94mLoss[0m : 2.57074
[1mStep[0m  [48/84], [94mLoss[0m : 2.62454
[1mStep[0m  [56/84], [94mLoss[0m : 2.61321
[1mStep[0m  [64/84], [94mLoss[0m : 2.53870
[1mStep[0m  [72/84], [94mLoss[0m : 2.43757
[1mStep[0m  [80/84], [94mLoss[0m : 2.67125

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.501, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.78567
[1mStep[0m  [8/84], [94mLoss[0m : 2.38435
[1mStep[0m  [16/84], [94mLoss[0m : 2.32259
[1mStep[0m  [24/84], [94mLoss[0m : 2.37265
[1mStep[0m  [32/84], [94mLoss[0m : 2.31939
[1mStep[0m  [40/84], [94mLoss[0m : 2.31506
[1mStep[0m  [48/84], [94mLoss[0m : 2.23038
[1mStep[0m  [56/84], [94mLoss[0m : 2.25649
[1mStep[0m  [64/84], [94mLoss[0m : 2.55549
[1mStep[0m  [72/84], [94mLoss[0m : 2.09266
[1mStep[0m  [80/84], [94mLoss[0m : 2.47325

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.509, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.73700
[1mStep[0m  [8/84], [94mLoss[0m : 2.24172
[1mStep[0m  [16/84], [94mLoss[0m : 2.39424
[1mStep[0m  [24/84], [94mLoss[0m : 2.97767
[1mStep[0m  [32/84], [94mLoss[0m : 2.45759
[1mStep[0m  [40/84], [94mLoss[0m : 2.32990
[1mStep[0m  [48/84], [94mLoss[0m : 2.52981
[1mStep[0m  [56/84], [94mLoss[0m : 2.50921
[1mStep[0m  [64/84], [94mLoss[0m : 2.26347
[1mStep[0m  [72/84], [94mLoss[0m : 2.61962
[1mStep[0m  [80/84], [94mLoss[0m : 2.59065

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.496, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31070
[1mStep[0m  [8/84], [94mLoss[0m : 2.42154
[1mStep[0m  [16/84], [94mLoss[0m : 2.47060
[1mStep[0m  [24/84], [94mLoss[0m : 2.32554
[1mStep[0m  [32/84], [94mLoss[0m : 2.66560
[1mStep[0m  [40/84], [94mLoss[0m : 2.21299
[1mStep[0m  [48/84], [94mLoss[0m : 2.51023
[1mStep[0m  [56/84], [94mLoss[0m : 2.65487
[1mStep[0m  [64/84], [94mLoss[0m : 2.43289
[1mStep[0m  [72/84], [94mLoss[0m : 2.45514
[1mStep[0m  [80/84], [94mLoss[0m : 2.29366

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.443, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49459
[1mStep[0m  [8/84], [94mLoss[0m : 2.59470
[1mStep[0m  [16/84], [94mLoss[0m : 2.22567
[1mStep[0m  [24/84], [94mLoss[0m : 2.49186
[1mStep[0m  [32/84], [94mLoss[0m : 2.51833
[1mStep[0m  [40/84], [94mLoss[0m : 2.61863
[1mStep[0m  [48/84], [94mLoss[0m : 2.35563
[1mStep[0m  [56/84], [94mLoss[0m : 2.65296
[1mStep[0m  [64/84], [94mLoss[0m : 2.57943
[1mStep[0m  [72/84], [94mLoss[0m : 2.54453
[1mStep[0m  [80/84], [94mLoss[0m : 2.54215

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.488, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30670
[1mStep[0m  [8/84], [94mLoss[0m : 2.59891
[1mStep[0m  [16/84], [94mLoss[0m : 2.56445
[1mStep[0m  [24/84], [94mLoss[0m : 2.20538
[1mStep[0m  [32/84], [94mLoss[0m : 2.30493
[1mStep[0m  [40/84], [94mLoss[0m : 2.44493
[1mStep[0m  [48/84], [94mLoss[0m : 2.45208
[1mStep[0m  [56/84], [94mLoss[0m : 2.53197
[1mStep[0m  [64/84], [94mLoss[0m : 2.25664
[1mStep[0m  [72/84], [94mLoss[0m : 2.51463
[1mStep[0m  [80/84], [94mLoss[0m : 2.33069

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.457, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48729
[1mStep[0m  [8/84], [94mLoss[0m : 2.64350
[1mStep[0m  [16/84], [94mLoss[0m : 2.46805
[1mStep[0m  [24/84], [94mLoss[0m : 2.85819
[1mStep[0m  [32/84], [94mLoss[0m : 2.35552
[1mStep[0m  [40/84], [94mLoss[0m : 2.51054
[1mStep[0m  [48/84], [94mLoss[0m : 2.48127
[1mStep[0m  [56/84], [94mLoss[0m : 2.43796
[1mStep[0m  [64/84], [94mLoss[0m : 2.48965
[1mStep[0m  [72/84], [94mLoss[0m : 2.30784
[1mStep[0m  [80/84], [94mLoss[0m : 2.76121

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.474, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.18343
[1mStep[0m  [8/84], [94mLoss[0m : 2.23166
[1mStep[0m  [16/84], [94mLoss[0m : 2.18904
[1mStep[0m  [24/84], [94mLoss[0m : 2.45091
[1mStep[0m  [32/84], [94mLoss[0m : 2.22661
[1mStep[0m  [40/84], [94mLoss[0m : 2.67242
[1mStep[0m  [48/84], [94mLoss[0m : 2.15904
[1mStep[0m  [56/84], [94mLoss[0m : 2.40573
[1mStep[0m  [64/84], [94mLoss[0m : 2.62545
[1mStep[0m  [72/84], [94mLoss[0m : 2.43153
[1mStep[0m  [80/84], [94mLoss[0m : 2.31262

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.430, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.32910
[1mStep[0m  [8/84], [94mLoss[0m : 2.21528
[1mStep[0m  [16/84], [94mLoss[0m : 2.43877
[1mStep[0m  [24/84], [94mLoss[0m : 2.55831
[1mStep[0m  [32/84], [94mLoss[0m : 2.83326
[1mStep[0m  [40/84], [94mLoss[0m : 2.50933
[1mStep[0m  [48/84], [94mLoss[0m : 2.51422
[1mStep[0m  [56/84], [94mLoss[0m : 2.26809
[1mStep[0m  [64/84], [94mLoss[0m : 2.36426
[1mStep[0m  [72/84], [94mLoss[0m : 2.24437
[1mStep[0m  [80/84], [94mLoss[0m : 2.39639

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.415, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.57338
[1mStep[0m  [8/84], [94mLoss[0m : 2.50486
[1mStep[0m  [16/84], [94mLoss[0m : 2.26578
[1mStep[0m  [24/84], [94mLoss[0m : 2.28716
[1mStep[0m  [32/84], [94mLoss[0m : 2.24465
[1mStep[0m  [40/84], [94mLoss[0m : 2.43277
[1mStep[0m  [48/84], [94mLoss[0m : 2.48221
[1mStep[0m  [56/84], [94mLoss[0m : 2.48804
[1mStep[0m  [64/84], [94mLoss[0m : 2.05589
[1mStep[0m  [72/84], [94mLoss[0m : 2.42795
[1mStep[0m  [80/84], [94mLoss[0m : 2.24964

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.418, [92mTest[0m: 2.468, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45533
[1mStep[0m  [8/84], [94mLoss[0m : 2.45853
[1mStep[0m  [16/84], [94mLoss[0m : 2.64413
[1mStep[0m  [24/84], [94mLoss[0m : 2.31340
[1mStep[0m  [32/84], [94mLoss[0m : 2.41558
[1mStep[0m  [40/84], [94mLoss[0m : 2.43912
[1mStep[0m  [48/84], [94mLoss[0m : 2.53826
[1mStep[0m  [56/84], [94mLoss[0m : 2.31241
[1mStep[0m  [64/84], [94mLoss[0m : 2.45413
[1mStep[0m  [72/84], [94mLoss[0m : 2.32941
[1mStep[0m  [80/84], [94mLoss[0m : 2.25147

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.406, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41654
[1mStep[0m  [8/84], [94mLoss[0m : 2.26643
[1mStep[0m  [16/84], [94mLoss[0m : 2.27532
[1mStep[0m  [24/84], [94mLoss[0m : 2.46438
[1mStep[0m  [32/84], [94mLoss[0m : 2.48904
[1mStep[0m  [40/84], [94mLoss[0m : 2.37436
[1mStep[0m  [48/84], [94mLoss[0m : 2.63005
[1mStep[0m  [56/84], [94mLoss[0m : 2.65170
[1mStep[0m  [64/84], [94mLoss[0m : 2.43338
[1mStep[0m  [72/84], [94mLoss[0m : 2.33888
[1mStep[0m  [80/84], [94mLoss[0m : 2.27856

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.458, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.57128
[1mStep[0m  [8/84], [94mLoss[0m : 2.20172
[1mStep[0m  [16/84], [94mLoss[0m : 2.50868
[1mStep[0m  [24/84], [94mLoss[0m : 2.59115
[1mStep[0m  [32/84], [94mLoss[0m : 2.64324
[1mStep[0m  [40/84], [94mLoss[0m : 2.28778
[1mStep[0m  [48/84], [94mLoss[0m : 2.36384
[1mStep[0m  [56/84], [94mLoss[0m : 2.42818
[1mStep[0m  [64/84], [94mLoss[0m : 2.52059
[1mStep[0m  [72/84], [94mLoss[0m : 2.35686
[1mStep[0m  [80/84], [94mLoss[0m : 2.34837

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.392, [92mTest[0m: 2.409, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46195
[1mStep[0m  [8/84], [94mLoss[0m : 2.86072
[1mStep[0m  [16/84], [94mLoss[0m : 2.42650
[1mStep[0m  [24/84], [94mLoss[0m : 2.21678
[1mStep[0m  [32/84], [94mLoss[0m : 2.26005
[1mStep[0m  [40/84], [94mLoss[0m : 2.22360
[1mStep[0m  [48/84], [94mLoss[0m : 2.27298
[1mStep[0m  [56/84], [94mLoss[0m : 2.23598
[1mStep[0m  [64/84], [94mLoss[0m : 2.71887
[1mStep[0m  [72/84], [94mLoss[0m : 2.41939
[1mStep[0m  [80/84], [94mLoss[0m : 2.41237

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.433, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.51816
[1mStep[0m  [8/84], [94mLoss[0m : 2.31700
[1mStep[0m  [16/84], [94mLoss[0m : 2.98911
[1mStep[0m  [24/84], [94mLoss[0m : 2.26574
[1mStep[0m  [32/84], [94mLoss[0m : 2.28022
[1mStep[0m  [40/84], [94mLoss[0m : 2.51508
[1mStep[0m  [48/84], [94mLoss[0m : 2.36373
[1mStep[0m  [56/84], [94mLoss[0m : 2.26977
[1mStep[0m  [64/84], [94mLoss[0m : 2.27309
[1mStep[0m  [72/84], [94mLoss[0m : 2.55122
[1mStep[0m  [80/84], [94mLoss[0m : 2.35785

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.381, [92mTest[0m: 2.378, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.39269
[1mStep[0m  [8/84], [94mLoss[0m : 2.42208
[1mStep[0m  [16/84], [94mLoss[0m : 2.44727
[1mStep[0m  [24/84], [94mLoss[0m : 2.43629
[1mStep[0m  [32/84], [94mLoss[0m : 2.33810
[1mStep[0m  [40/84], [94mLoss[0m : 2.57333
[1mStep[0m  [48/84], [94mLoss[0m : 2.55095
[1mStep[0m  [56/84], [94mLoss[0m : 2.48075
[1mStep[0m  [64/84], [94mLoss[0m : 2.27628
[1mStep[0m  [72/84], [94mLoss[0m : 2.19512
[1mStep[0m  [80/84], [94mLoss[0m : 2.22362

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.381, [92mTest[0m: 2.418, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52415
[1mStep[0m  [8/84], [94mLoss[0m : 2.68630
[1mStep[0m  [16/84], [94mLoss[0m : 2.15251
[1mStep[0m  [24/84], [94mLoss[0m : 2.42070
[1mStep[0m  [32/84], [94mLoss[0m : 2.32670
[1mStep[0m  [40/84], [94mLoss[0m : 2.43599
[1mStep[0m  [48/84], [94mLoss[0m : 2.40813
[1mStep[0m  [56/84], [94mLoss[0m : 2.52380
[1mStep[0m  [64/84], [94mLoss[0m : 2.27027
[1mStep[0m  [72/84], [94mLoss[0m : 2.63570
[1mStep[0m  [80/84], [94mLoss[0m : 2.32919

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.372, [92mTest[0m: 2.381, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46054
[1mStep[0m  [8/84], [94mLoss[0m : 2.43184
[1mStep[0m  [16/84], [94mLoss[0m : 2.58741
[1mStep[0m  [24/84], [94mLoss[0m : 2.31106
[1mStep[0m  [32/84], [94mLoss[0m : 2.10899
[1mStep[0m  [40/84], [94mLoss[0m : 2.12477
[1mStep[0m  [48/84], [94mLoss[0m : 2.23890
[1mStep[0m  [56/84], [94mLoss[0m : 2.26307
[1mStep[0m  [64/84], [94mLoss[0m : 2.34860
[1mStep[0m  [72/84], [94mLoss[0m : 2.29152
[1mStep[0m  [80/84], [94mLoss[0m : 2.48939

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.374, [92mTest[0m: 2.386, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27432
[1mStep[0m  [8/84], [94mLoss[0m : 2.85319
[1mStep[0m  [16/84], [94mLoss[0m : 2.40113
[1mStep[0m  [24/84], [94mLoss[0m : 2.28978
[1mStep[0m  [32/84], [94mLoss[0m : 2.05956
[1mStep[0m  [40/84], [94mLoss[0m : 2.38223
[1mStep[0m  [48/84], [94mLoss[0m : 2.47128
[1mStep[0m  [56/84], [94mLoss[0m : 2.45174
[1mStep[0m  [64/84], [94mLoss[0m : 2.01145
[1mStep[0m  [72/84], [94mLoss[0m : 2.69250
[1mStep[0m  [80/84], [94mLoss[0m : 2.05503

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.384, [92mTest[0m: 2.404, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.18187
[1mStep[0m  [8/84], [94mLoss[0m : 2.30619
[1mStep[0m  [16/84], [94mLoss[0m : 2.30401
[1mStep[0m  [24/84], [94mLoss[0m : 2.55745
[1mStep[0m  [32/84], [94mLoss[0m : 2.49327
[1mStep[0m  [40/84], [94mLoss[0m : 2.35813
[1mStep[0m  [48/84], [94mLoss[0m : 2.66081
[1mStep[0m  [56/84], [94mLoss[0m : 2.33253
[1mStep[0m  [64/84], [94mLoss[0m : 2.70425
[1mStep[0m  [72/84], [94mLoss[0m : 2.20341
[1mStep[0m  [80/84], [94mLoss[0m : 2.51177

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.382, [92mTest[0m: 2.411, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.19243
[1mStep[0m  [8/84], [94mLoss[0m : 2.21329
[1mStep[0m  [16/84], [94mLoss[0m : 2.67173
[1mStep[0m  [24/84], [94mLoss[0m : 2.09886
[1mStep[0m  [32/84], [94mLoss[0m : 2.66043
[1mStep[0m  [40/84], [94mLoss[0m : 2.55971
[1mStep[0m  [48/84], [94mLoss[0m : 2.56051
[1mStep[0m  [56/84], [94mLoss[0m : 2.40181
[1mStep[0m  [64/84], [94mLoss[0m : 2.26229
[1mStep[0m  [72/84], [94mLoss[0m : 2.37860
[1mStep[0m  [80/84], [94mLoss[0m : 2.51577

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.382, [92mTest[0m: 2.376, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35885
[1mStep[0m  [8/84], [94mLoss[0m : 2.23117
[1mStep[0m  [16/84], [94mLoss[0m : 2.51197
[1mStep[0m  [24/84], [94mLoss[0m : 2.35017
[1mStep[0m  [32/84], [94mLoss[0m : 2.29264
[1mStep[0m  [40/84], [94mLoss[0m : 2.51639
[1mStep[0m  [48/84], [94mLoss[0m : 2.39180
[1mStep[0m  [56/84], [94mLoss[0m : 2.53562
[1mStep[0m  [64/84], [94mLoss[0m : 2.60106
[1mStep[0m  [72/84], [94mLoss[0m : 2.11971
[1mStep[0m  [80/84], [94mLoss[0m : 2.49659

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.388, [92mTest[0m: 2.362, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.407
====================================

Phase 1 - Evaluation MAE:  2.407324595110757
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 2.08433
[1mStep[0m  [8/84], [94mLoss[0m : 2.83197
[1mStep[0m  [16/84], [94mLoss[0m : 2.78339
[1mStep[0m  [24/84], [94mLoss[0m : 2.50852
[1mStep[0m  [32/84], [94mLoss[0m : 2.55327
[1mStep[0m  [40/84], [94mLoss[0m : 2.36480
[1mStep[0m  [48/84], [94mLoss[0m : 2.36134
[1mStep[0m  [56/84], [94mLoss[0m : 2.72806
[1mStep[0m  [64/84], [94mLoss[0m : 2.23497
[1mStep[0m  [72/84], [94mLoss[0m : 2.41964
[1mStep[0m  [80/84], [94mLoss[0m : 2.70765

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.403, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54544
[1mStep[0m  [8/84], [94mLoss[0m : 2.28601
[1mStep[0m  [16/84], [94mLoss[0m : 2.40202
[1mStep[0m  [24/84], [94mLoss[0m : 2.56424
[1mStep[0m  [32/84], [94mLoss[0m : 2.14776
[1mStep[0m  [40/84], [94mLoss[0m : 2.71324
[1mStep[0m  [48/84], [94mLoss[0m : 2.38348
[1mStep[0m  [56/84], [94mLoss[0m : 2.57220
[1mStep[0m  [64/84], [94mLoss[0m : 2.73463
[1mStep[0m  [72/84], [94mLoss[0m : 2.58709
[1mStep[0m  [80/84], [94mLoss[0m : 2.46080

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.391, [92mTest[0m: 2.460, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31626
[1mStep[0m  [8/84], [94mLoss[0m : 2.36845
[1mStep[0m  [16/84], [94mLoss[0m : 2.00875
[1mStep[0m  [24/84], [94mLoss[0m : 2.67457
[1mStep[0m  [32/84], [94mLoss[0m : 2.24079
[1mStep[0m  [40/84], [94mLoss[0m : 2.13944
[1mStep[0m  [48/84], [94mLoss[0m : 2.18908
[1mStep[0m  [56/84], [94mLoss[0m : 2.42261
[1mStep[0m  [64/84], [94mLoss[0m : 2.03517
[1mStep[0m  [72/84], [94mLoss[0m : 2.28562
[1mStep[0m  [80/84], [94mLoss[0m : 2.72330

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.296, [92mTest[0m: 2.409, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30187
[1mStep[0m  [8/84], [94mLoss[0m : 2.16113
[1mStep[0m  [16/84], [94mLoss[0m : 2.40052
[1mStep[0m  [24/84], [94mLoss[0m : 2.25178
[1mStep[0m  [32/84], [94mLoss[0m : 1.97387
[1mStep[0m  [40/84], [94mLoss[0m : 2.05468
[1mStep[0m  [48/84], [94mLoss[0m : 2.42195
[1mStep[0m  [56/84], [94mLoss[0m : 2.01280
[1mStep[0m  [64/84], [94mLoss[0m : 1.89173
[1mStep[0m  [72/84], [94mLoss[0m : 2.51242
[1mStep[0m  [80/84], [94mLoss[0m : 2.09945

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.258, [92mTest[0m: 2.379, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.11382
[1mStep[0m  [8/84], [94mLoss[0m : 2.20806
[1mStep[0m  [16/84], [94mLoss[0m : 2.25431
[1mStep[0m  [24/84], [94mLoss[0m : 2.02793
[1mStep[0m  [32/84], [94mLoss[0m : 2.30939
[1mStep[0m  [40/84], [94mLoss[0m : 2.02754
[1mStep[0m  [48/84], [94mLoss[0m : 2.16297
[1mStep[0m  [56/84], [94mLoss[0m : 2.03456
[1mStep[0m  [64/84], [94mLoss[0m : 2.22852
[1mStep[0m  [72/84], [94mLoss[0m : 2.27651
[1mStep[0m  [80/84], [94mLoss[0m : 2.26542

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.187, [92mTest[0m: 2.368, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.81246
[1mStep[0m  [8/84], [94mLoss[0m : 2.28208
[1mStep[0m  [16/84], [94mLoss[0m : 2.35017
[1mStep[0m  [24/84], [94mLoss[0m : 2.13659
[1mStep[0m  [32/84], [94mLoss[0m : 2.35734
[1mStep[0m  [40/84], [94mLoss[0m : 2.22582
[1mStep[0m  [48/84], [94mLoss[0m : 2.10657
[1mStep[0m  [56/84], [94mLoss[0m : 2.02350
[1mStep[0m  [64/84], [94mLoss[0m : 2.54361
[1mStep[0m  [72/84], [94mLoss[0m : 2.06353
[1mStep[0m  [80/84], [94mLoss[0m : 2.27446

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.129, [92mTest[0m: 2.441, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.00126
[1mStep[0m  [8/84], [94mLoss[0m : 1.91429
[1mStep[0m  [16/84], [94mLoss[0m : 2.26225
[1mStep[0m  [24/84], [94mLoss[0m : 2.17096
[1mStep[0m  [32/84], [94mLoss[0m : 1.73302
[1mStep[0m  [40/84], [94mLoss[0m : 1.91732
[1mStep[0m  [48/84], [94mLoss[0m : 1.98743
[1mStep[0m  [56/84], [94mLoss[0m : 1.87270
[1mStep[0m  [64/84], [94mLoss[0m : 2.02065
[1mStep[0m  [72/84], [94mLoss[0m : 2.10260
[1mStep[0m  [80/84], [94mLoss[0m : 2.38299

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.076, [92mTest[0m: 2.530, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.83743
[1mStep[0m  [8/84], [94mLoss[0m : 1.92047
[1mStep[0m  [16/84], [94mLoss[0m : 2.04765
[1mStep[0m  [24/84], [94mLoss[0m : 1.91942
[1mStep[0m  [32/84], [94mLoss[0m : 2.00397
[1mStep[0m  [40/84], [94mLoss[0m : 2.08327
[1mStep[0m  [48/84], [94mLoss[0m : 1.93553
[1mStep[0m  [56/84], [94mLoss[0m : 1.96731
[1mStep[0m  [64/84], [94mLoss[0m : 2.03365
[1mStep[0m  [72/84], [94mLoss[0m : 2.25198
[1mStep[0m  [80/84], [94mLoss[0m : 2.16466

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.030, [92mTest[0m: 2.523, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.93847
[1mStep[0m  [8/84], [94mLoss[0m : 1.80051
[1mStep[0m  [16/84], [94mLoss[0m : 1.59352
[1mStep[0m  [24/84], [94mLoss[0m : 2.24052
[1mStep[0m  [32/84], [94mLoss[0m : 2.09833
[1mStep[0m  [40/84], [94mLoss[0m : 2.14830
[1mStep[0m  [48/84], [94mLoss[0m : 2.06436
[1mStep[0m  [56/84], [94mLoss[0m : 1.84837
[1mStep[0m  [64/84], [94mLoss[0m : 1.97305
[1mStep[0m  [72/84], [94mLoss[0m : 2.01696
[1mStep[0m  [80/84], [94mLoss[0m : 2.04720

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.983, [92mTest[0m: 2.542, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.09755
[1mStep[0m  [8/84], [94mLoss[0m : 2.03429
[1mStep[0m  [16/84], [94mLoss[0m : 1.96028
[1mStep[0m  [24/84], [94mLoss[0m : 2.01362
[1mStep[0m  [32/84], [94mLoss[0m : 1.71694
[1mStep[0m  [40/84], [94mLoss[0m : 1.95391
[1mStep[0m  [48/84], [94mLoss[0m : 1.65998
[1mStep[0m  [56/84], [94mLoss[0m : 2.01665
[1mStep[0m  [64/84], [94mLoss[0m : 1.95265
[1mStep[0m  [72/84], [94mLoss[0m : 2.17238
[1mStep[0m  [80/84], [94mLoss[0m : 1.58910

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.953, [92mTest[0m: 2.526, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.76284
[1mStep[0m  [8/84], [94mLoss[0m : 1.79494
[1mStep[0m  [16/84], [94mLoss[0m : 1.56387
[1mStep[0m  [24/84], [94mLoss[0m : 1.85841
[1mStep[0m  [32/84], [94mLoss[0m : 1.88799
[1mStep[0m  [40/84], [94mLoss[0m : 1.86188
[1mStep[0m  [48/84], [94mLoss[0m : 1.93543
[1mStep[0m  [56/84], [94mLoss[0m : 1.75678
[1mStep[0m  [64/84], [94mLoss[0m : 2.10458
[1mStep[0m  [72/84], [94mLoss[0m : 1.68970
[1mStep[0m  [80/84], [94mLoss[0m : 1.98040

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.880, [92mTest[0m: 2.652, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.08285
[1mStep[0m  [8/84], [94mLoss[0m : 1.84382
[1mStep[0m  [16/84], [94mLoss[0m : 1.75009
[1mStep[0m  [24/84], [94mLoss[0m : 1.61114
[1mStep[0m  [32/84], [94mLoss[0m : 2.11934
[1mStep[0m  [40/84], [94mLoss[0m : 1.82709
[1mStep[0m  [48/84], [94mLoss[0m : 1.84957
[1mStep[0m  [56/84], [94mLoss[0m : 1.91259
[1mStep[0m  [64/84], [94mLoss[0m : 2.06161
[1mStep[0m  [72/84], [94mLoss[0m : 1.74958
[1mStep[0m  [80/84], [94mLoss[0m : 1.57584

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.858, [92mTest[0m: 2.634, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.06716
[1mStep[0m  [8/84], [94mLoss[0m : 1.91452
[1mStep[0m  [16/84], [94mLoss[0m : 1.89386
[1mStep[0m  [24/84], [94mLoss[0m : 1.98619
[1mStep[0m  [32/84], [94mLoss[0m : 1.75178
[1mStep[0m  [40/84], [94mLoss[0m : 1.75427
[1mStep[0m  [48/84], [94mLoss[0m : 1.97561
[1mStep[0m  [56/84], [94mLoss[0m : 1.57727
[1mStep[0m  [64/84], [94mLoss[0m : 1.90001
[1mStep[0m  [72/84], [94mLoss[0m : 1.98139
[1mStep[0m  [80/84], [94mLoss[0m : 1.58831

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.817, [92mTest[0m: 2.701, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.53103
[1mStep[0m  [8/84], [94mLoss[0m : 1.66916
[1mStep[0m  [16/84], [94mLoss[0m : 1.55997
[1mStep[0m  [24/84], [94mLoss[0m : 1.80013
[1mStep[0m  [32/84], [94mLoss[0m : 1.83447
[1mStep[0m  [40/84], [94mLoss[0m : 1.89645
[1mStep[0m  [48/84], [94mLoss[0m : 1.73752
[1mStep[0m  [56/84], [94mLoss[0m : 1.76342
[1mStep[0m  [64/84], [94mLoss[0m : 1.67302
[1mStep[0m  [72/84], [94mLoss[0m : 1.68657
[1mStep[0m  [80/84], [94mLoss[0m : 1.75212

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.767, [92mTest[0m: 2.519, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.58987
[1mStep[0m  [8/84], [94mLoss[0m : 1.36882
[1mStep[0m  [16/84], [94mLoss[0m : 1.62840
[1mStep[0m  [24/84], [94mLoss[0m : 1.83471
[1mStep[0m  [32/84], [94mLoss[0m : 1.83362
[1mStep[0m  [40/84], [94mLoss[0m : 1.87079
[1mStep[0m  [48/84], [94mLoss[0m : 1.74860
[1mStep[0m  [56/84], [94mLoss[0m : 1.73271
[1mStep[0m  [64/84], [94mLoss[0m : 1.97653
[1mStep[0m  [72/84], [94mLoss[0m : 1.62387
[1mStep[0m  [80/84], [94mLoss[0m : 1.81249

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.755, [92mTest[0m: 2.635, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.60571
[1mStep[0m  [8/84], [94mLoss[0m : 1.43884
[1mStep[0m  [16/84], [94mLoss[0m : 1.70034
[1mStep[0m  [24/84], [94mLoss[0m : 1.82564
[1mStep[0m  [32/84], [94mLoss[0m : 1.75025
[1mStep[0m  [40/84], [94mLoss[0m : 1.67223
[1mStep[0m  [48/84], [94mLoss[0m : 1.53052
[1mStep[0m  [56/84], [94mLoss[0m : 1.66190
[1mStep[0m  [64/84], [94mLoss[0m : 1.66090
[1mStep[0m  [72/84], [94mLoss[0m : 1.84630
[1mStep[0m  [80/84], [94mLoss[0m : 1.72796

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.682, [92mTest[0m: 2.574, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.50563
[1mStep[0m  [8/84], [94mLoss[0m : 1.68288
[1mStep[0m  [16/84], [94mLoss[0m : 1.55224
[1mStep[0m  [24/84], [94mLoss[0m : 1.62978
[1mStep[0m  [32/84], [94mLoss[0m : 1.74975
[1mStep[0m  [40/84], [94mLoss[0m : 1.72792
[1mStep[0m  [48/84], [94mLoss[0m : 1.55184
[1mStep[0m  [56/84], [94mLoss[0m : 1.55599
[1mStep[0m  [64/84], [94mLoss[0m : 1.85574
[1mStep[0m  [72/84], [94mLoss[0m : 1.51125
[1mStep[0m  [80/84], [94mLoss[0m : 1.78841

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.664, [92mTest[0m: 2.540, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.62475
[1mStep[0m  [8/84], [94mLoss[0m : 1.77888
[1mStep[0m  [16/84], [94mLoss[0m : 1.37346
[1mStep[0m  [24/84], [94mLoss[0m : 1.52606
[1mStep[0m  [32/84], [94mLoss[0m : 1.80661
[1mStep[0m  [40/84], [94mLoss[0m : 1.71409
[1mStep[0m  [48/84], [94mLoss[0m : 1.74519
[1mStep[0m  [56/84], [94mLoss[0m : 1.51576
[1mStep[0m  [64/84], [94mLoss[0m : 1.58169
[1mStep[0m  [72/84], [94mLoss[0m : 1.68803
[1mStep[0m  [80/84], [94mLoss[0m : 1.56228

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.639, [92mTest[0m: 2.692, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.64286
[1mStep[0m  [8/84], [94mLoss[0m : 1.49458
[1mStep[0m  [16/84], [94mLoss[0m : 1.53734
[1mStep[0m  [24/84], [94mLoss[0m : 1.56944
[1mStep[0m  [32/84], [94mLoss[0m : 1.58546
[1mStep[0m  [40/84], [94mLoss[0m : 1.72831
[1mStep[0m  [48/84], [94mLoss[0m : 1.58951
[1mStep[0m  [56/84], [94mLoss[0m : 1.74493
[1mStep[0m  [64/84], [94mLoss[0m : 1.65137
[1mStep[0m  [72/84], [94mLoss[0m : 1.76048
[1mStep[0m  [80/84], [94mLoss[0m : 1.62603

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.615, [92mTest[0m: 2.693, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.43005
[1mStep[0m  [8/84], [94mLoss[0m : 1.79418
[1mStep[0m  [16/84], [94mLoss[0m : 1.56282
[1mStep[0m  [24/84], [94mLoss[0m : 1.59682
[1mStep[0m  [32/84], [94mLoss[0m : 1.43433
[1mStep[0m  [40/84], [94mLoss[0m : 1.65044
[1mStep[0m  [48/84], [94mLoss[0m : 1.59000
[1mStep[0m  [56/84], [94mLoss[0m : 1.37530
[1mStep[0m  [64/84], [94mLoss[0m : 1.44426
[1mStep[0m  [72/84], [94mLoss[0m : 1.57455
[1mStep[0m  [80/84], [94mLoss[0m : 1.66402

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.589, [92mTest[0m: 2.557, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.49853
[1mStep[0m  [8/84], [94mLoss[0m : 1.34543
[1mStep[0m  [16/84], [94mLoss[0m : 1.46728
[1mStep[0m  [24/84], [94mLoss[0m : 1.44379
[1mStep[0m  [32/84], [94mLoss[0m : 1.52706
[1mStep[0m  [40/84], [94mLoss[0m : 1.52135
[1mStep[0m  [48/84], [94mLoss[0m : 1.73718
[1mStep[0m  [56/84], [94mLoss[0m : 1.30223
[1mStep[0m  [64/84], [94mLoss[0m : 1.49416
[1mStep[0m  [72/84], [94mLoss[0m : 1.64559
[1mStep[0m  [80/84], [94mLoss[0m : 1.56691

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.561, [92mTest[0m: 2.568, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.36254
[1mStep[0m  [8/84], [94mLoss[0m : 1.68009
[1mStep[0m  [16/84], [94mLoss[0m : 1.57358
[1mStep[0m  [24/84], [94mLoss[0m : 1.64572
[1mStep[0m  [32/84], [94mLoss[0m : 1.38211
[1mStep[0m  [40/84], [94mLoss[0m : 1.39782
[1mStep[0m  [48/84], [94mLoss[0m : 1.64355
[1mStep[0m  [56/84], [94mLoss[0m : 1.49242
[1mStep[0m  [64/84], [94mLoss[0m : 1.54364
[1mStep[0m  [72/84], [94mLoss[0m : 1.30802
[1mStep[0m  [80/84], [94mLoss[0m : 1.44440

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.518, [92mTest[0m: 2.592, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.40986
[1mStep[0m  [8/84], [94mLoss[0m : 1.60192
[1mStep[0m  [16/84], [94mLoss[0m : 1.57592
[1mStep[0m  [24/84], [94mLoss[0m : 1.46951
[1mStep[0m  [32/84], [94mLoss[0m : 1.54333
[1mStep[0m  [40/84], [94mLoss[0m : 1.74405
[1mStep[0m  [48/84], [94mLoss[0m : 1.56101
[1mStep[0m  [56/84], [94mLoss[0m : 1.68921
[1mStep[0m  [64/84], [94mLoss[0m : 1.77835
[1mStep[0m  [72/84], [94mLoss[0m : 1.77270
[1mStep[0m  [80/84], [94mLoss[0m : 1.55608

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.517, [92mTest[0m: 2.548, [96mlr[0m: 0.004545
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 22 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.558
====================================

Phase 2 - Evaluation MAE:  2.5582720381872996
MAE score P1      2.407325
MAE score P2      2.558272
loss              1.517109
learning_rate      0.00505
batch_size             128
hidden_sizes         [300]
epochs                  30
activation            relu
optimizer              sgd
early stopping        True
dropout                0.4
momentum               0.5
weight_decay         0.001
Name: 10, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 11.39473
[1mStep[0m  [8/84], [94mLoss[0m : 10.92031
[1mStep[0m  [16/84], [94mLoss[0m : 10.97088
[1mStep[0m  [24/84], [94mLoss[0m : 10.52288
[1mStep[0m  [32/84], [94mLoss[0m : 10.60659
[1mStep[0m  [40/84], [94mLoss[0m : 10.29638
[1mStep[0m  [48/84], [94mLoss[0m : 9.91010
[1mStep[0m  [56/84], [94mLoss[0m : 10.34956
[1mStep[0m  [64/84], [94mLoss[0m : 9.28278
[1mStep[0m  [72/84], [94mLoss[0m : 9.70494
[1mStep[0m  [80/84], [94mLoss[0m : 9.59878

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.250, [92mTest[0m: 11.025, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.17163
[1mStep[0m  [8/84], [94mLoss[0m : 9.18920
[1mStep[0m  [16/84], [94mLoss[0m : 9.03763
[1mStep[0m  [24/84], [94mLoss[0m : 8.85197
[1mStep[0m  [32/84], [94mLoss[0m : 8.21248
[1mStep[0m  [40/84], [94mLoss[0m : 8.49412
[1mStep[0m  [48/84], [94mLoss[0m : 8.02126
[1mStep[0m  [56/84], [94mLoss[0m : 8.36964
[1mStep[0m  [64/84], [94mLoss[0m : 7.80556
[1mStep[0m  [72/84], [94mLoss[0m : 7.86302
[1mStep[0m  [80/84], [94mLoss[0m : 7.99060

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 8.379, [92mTest[0m: 9.038, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.95081
[1mStep[0m  [8/84], [94mLoss[0m : 7.10791
[1mStep[0m  [16/84], [94mLoss[0m : 6.98504
[1mStep[0m  [24/84], [94mLoss[0m : 7.48805
[1mStep[0m  [32/84], [94mLoss[0m : 6.95261
[1mStep[0m  [40/84], [94mLoss[0m : 6.94580
[1mStep[0m  [48/84], [94mLoss[0m : 6.65292
[1mStep[0m  [56/84], [94mLoss[0m : 6.47844
[1mStep[0m  [64/84], [94mLoss[0m : 6.77390
[1mStep[0m  [72/84], [94mLoss[0m : 5.85968
[1mStep[0m  [80/84], [94mLoss[0m : 5.64358

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 6.693, [92mTest[0m: 6.686, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.77116
[1mStep[0m  [8/84], [94mLoss[0m : 5.29963
[1mStep[0m  [16/84], [94mLoss[0m : 5.52229
[1mStep[0m  [24/84], [94mLoss[0m : 5.48846
[1mStep[0m  [32/84], [94mLoss[0m : 5.11418
[1mStep[0m  [40/84], [94mLoss[0m : 5.43190
[1mStep[0m  [48/84], [94mLoss[0m : 4.53169
[1mStep[0m  [56/84], [94mLoss[0m : 4.82701
[1mStep[0m  [64/84], [94mLoss[0m : 4.91349
[1mStep[0m  [72/84], [94mLoss[0m : 4.28247
[1mStep[0m  [80/84], [94mLoss[0m : 4.57914

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 5.030, [92mTest[0m: 4.795, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.24944
[1mStep[0m  [8/84], [94mLoss[0m : 4.15882
[1mStep[0m  [16/84], [94mLoss[0m : 3.60578
[1mStep[0m  [24/84], [94mLoss[0m : 3.65437
[1mStep[0m  [32/84], [94mLoss[0m : 3.56739
[1mStep[0m  [40/84], [94mLoss[0m : 3.34577
[1mStep[0m  [48/84], [94mLoss[0m : 3.05067
[1mStep[0m  [56/84], [94mLoss[0m : 3.19962
[1mStep[0m  [64/84], [94mLoss[0m : 2.97459
[1mStep[0m  [72/84], [94mLoss[0m : 3.02920
[1mStep[0m  [80/84], [94mLoss[0m : 2.66448

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 3.367, [92mTest[0m: 3.017, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.12028
[1mStep[0m  [8/84], [94mLoss[0m : 2.82377
[1mStep[0m  [16/84], [94mLoss[0m : 3.00462
[1mStep[0m  [24/84], [94mLoss[0m : 2.80342
[1mStep[0m  [32/84], [94mLoss[0m : 2.37754
[1mStep[0m  [40/84], [94mLoss[0m : 2.79455
[1mStep[0m  [48/84], [94mLoss[0m : 2.68225
[1mStep[0m  [56/84], [94mLoss[0m : 2.67474
[1mStep[0m  [64/84], [94mLoss[0m : 2.64952
[1mStep[0m  [72/84], [94mLoss[0m : 3.07000
[1mStep[0m  [80/84], [94mLoss[0m : 2.83692

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.834, [92mTest[0m: 2.459, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.73252
[1mStep[0m  [8/84], [94mLoss[0m : 2.82827
[1mStep[0m  [16/84], [94mLoss[0m : 3.03942
[1mStep[0m  [24/84], [94mLoss[0m : 2.77648
[1mStep[0m  [32/84], [94mLoss[0m : 3.04302
[1mStep[0m  [40/84], [94mLoss[0m : 2.74347
[1mStep[0m  [48/84], [94mLoss[0m : 2.82771
[1mStep[0m  [56/84], [94mLoss[0m : 2.75037
[1mStep[0m  [64/84], [94mLoss[0m : 2.78916
[1mStep[0m  [72/84], [94mLoss[0m : 2.97881
[1mStep[0m  [80/84], [94mLoss[0m : 2.54448

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.757, [92mTest[0m: 2.410, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.93938
[1mStep[0m  [8/84], [94mLoss[0m : 2.71989
[1mStep[0m  [16/84], [94mLoss[0m : 3.12962
[1mStep[0m  [24/84], [94mLoss[0m : 2.84510
[1mStep[0m  [32/84], [94mLoss[0m : 2.56285
[1mStep[0m  [40/84], [94mLoss[0m : 2.81542
[1mStep[0m  [48/84], [94mLoss[0m : 2.45068
[1mStep[0m  [56/84], [94mLoss[0m : 2.62730
[1mStep[0m  [64/84], [94mLoss[0m : 2.98030
[1mStep[0m  [72/84], [94mLoss[0m : 2.91701
[1mStep[0m  [80/84], [94mLoss[0m : 2.88261

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.741, [92mTest[0m: 2.416, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67252
[1mStep[0m  [8/84], [94mLoss[0m : 2.78941
[1mStep[0m  [16/84], [94mLoss[0m : 2.23256
[1mStep[0m  [24/84], [94mLoss[0m : 2.87352
[1mStep[0m  [32/84], [94mLoss[0m : 2.61795
[1mStep[0m  [40/84], [94mLoss[0m : 2.65789
[1mStep[0m  [48/84], [94mLoss[0m : 2.41165
[1mStep[0m  [56/84], [94mLoss[0m : 2.50635
[1mStep[0m  [64/84], [94mLoss[0m : 2.86486
[1mStep[0m  [72/84], [94mLoss[0m : 2.98271
[1mStep[0m  [80/84], [94mLoss[0m : 2.63096

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.676, [92mTest[0m: 2.480, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.61862
[1mStep[0m  [8/84], [94mLoss[0m : 2.67822
[1mStep[0m  [16/84], [94mLoss[0m : 2.41109
[1mStep[0m  [24/84], [94mLoss[0m : 2.75633
[1mStep[0m  [32/84], [94mLoss[0m : 2.63748
[1mStep[0m  [40/84], [94mLoss[0m : 2.70128
[1mStep[0m  [48/84], [94mLoss[0m : 2.76559
[1mStep[0m  [56/84], [94mLoss[0m : 2.74824
[1mStep[0m  [64/84], [94mLoss[0m : 2.57505
[1mStep[0m  [72/84], [94mLoss[0m : 2.47406
[1mStep[0m  [80/84], [94mLoss[0m : 2.48824

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.653, [92mTest[0m: 2.390, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.64650
[1mStep[0m  [8/84], [94mLoss[0m : 2.40926
[1mStep[0m  [16/84], [94mLoss[0m : 2.41514
[1mStep[0m  [24/84], [94mLoss[0m : 2.70359
[1mStep[0m  [32/84], [94mLoss[0m : 2.73390
[1mStep[0m  [40/84], [94mLoss[0m : 2.80771
[1mStep[0m  [48/84], [94mLoss[0m : 2.63052
[1mStep[0m  [56/84], [94mLoss[0m : 3.00643
[1mStep[0m  [64/84], [94mLoss[0m : 2.73056
[1mStep[0m  [72/84], [94mLoss[0m : 2.73300
[1mStep[0m  [80/84], [94mLoss[0m : 2.49277

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.661, [92mTest[0m: 2.366, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46823
[1mStep[0m  [8/84], [94mLoss[0m : 2.06403
[1mStep[0m  [16/84], [94mLoss[0m : 2.90843
[1mStep[0m  [24/84], [94mLoss[0m : 2.60265
[1mStep[0m  [32/84], [94mLoss[0m : 2.75118
[1mStep[0m  [40/84], [94mLoss[0m : 3.07390
[1mStep[0m  [48/84], [94mLoss[0m : 2.60237
[1mStep[0m  [56/84], [94mLoss[0m : 2.84579
[1mStep[0m  [64/84], [94mLoss[0m : 2.32411
[1mStep[0m  [72/84], [94mLoss[0m : 2.89229
[1mStep[0m  [80/84], [94mLoss[0m : 2.51424

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.639, [92mTest[0m: 2.396, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.57071
[1mStep[0m  [8/84], [94mLoss[0m : 2.59878
[1mStep[0m  [16/84], [94mLoss[0m : 2.69170
[1mStep[0m  [24/84], [94mLoss[0m : 2.47308
[1mStep[0m  [32/84], [94mLoss[0m : 2.48017
[1mStep[0m  [40/84], [94mLoss[0m : 2.56524
[1mStep[0m  [48/84], [94mLoss[0m : 2.69146
[1mStep[0m  [56/84], [94mLoss[0m : 2.74711
[1mStep[0m  [64/84], [94mLoss[0m : 2.74384
[1mStep[0m  [72/84], [94mLoss[0m : 2.65805
[1mStep[0m  [80/84], [94mLoss[0m : 2.70909

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.614, [92mTest[0m: 2.345, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36606
[1mStep[0m  [8/84], [94mLoss[0m : 2.86278
[1mStep[0m  [16/84], [94mLoss[0m : 2.52831
[1mStep[0m  [24/84], [94mLoss[0m : 2.52567
[1mStep[0m  [32/84], [94mLoss[0m : 2.96656
[1mStep[0m  [40/84], [94mLoss[0m : 2.66019
[1mStep[0m  [48/84], [94mLoss[0m : 2.87114
[1mStep[0m  [56/84], [94mLoss[0m : 2.31446
[1mStep[0m  [64/84], [94mLoss[0m : 2.18532
[1mStep[0m  [72/84], [94mLoss[0m : 2.70773
[1mStep[0m  [80/84], [94mLoss[0m : 2.71932

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.618, [92mTest[0m: 2.407, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44266
[1mStep[0m  [8/84], [94mLoss[0m : 2.16818
[1mStep[0m  [16/84], [94mLoss[0m : 2.56950
[1mStep[0m  [24/84], [94mLoss[0m : 2.33400
[1mStep[0m  [32/84], [94mLoss[0m : 2.85339
[1mStep[0m  [40/84], [94mLoss[0m : 2.75406
[1mStep[0m  [48/84], [94mLoss[0m : 2.76625
[1mStep[0m  [56/84], [94mLoss[0m : 2.67035
[1mStep[0m  [64/84], [94mLoss[0m : 2.60387
[1mStep[0m  [72/84], [94mLoss[0m : 2.27827
[1mStep[0m  [80/84], [94mLoss[0m : 2.44322

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.593, [92mTest[0m: 2.376, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47495
[1mStep[0m  [8/84], [94mLoss[0m : 2.43183
[1mStep[0m  [16/84], [94mLoss[0m : 2.61453
[1mStep[0m  [24/84], [94mLoss[0m : 2.49982
[1mStep[0m  [32/84], [94mLoss[0m : 2.44056
[1mStep[0m  [40/84], [94mLoss[0m : 2.76363
[1mStep[0m  [48/84], [94mLoss[0m : 2.76781
[1mStep[0m  [56/84], [94mLoss[0m : 2.64990
[1mStep[0m  [64/84], [94mLoss[0m : 2.87243
[1mStep[0m  [72/84], [94mLoss[0m : 2.48961
[1mStep[0m  [80/84], [94mLoss[0m : 2.61987

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.597, [92mTest[0m: 2.348, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.71263
[1mStep[0m  [8/84], [94mLoss[0m : 2.48197
[1mStep[0m  [16/84], [94mLoss[0m : 2.57134
[1mStep[0m  [24/84], [94mLoss[0m : 2.90284
[1mStep[0m  [32/84], [94mLoss[0m : 2.54182
[1mStep[0m  [40/84], [94mLoss[0m : 2.39633
[1mStep[0m  [48/84], [94mLoss[0m : 2.50904
[1mStep[0m  [56/84], [94mLoss[0m : 2.65273
[1mStep[0m  [64/84], [94mLoss[0m : 2.57667
[1mStep[0m  [72/84], [94mLoss[0m : 2.70446
[1mStep[0m  [80/84], [94mLoss[0m : 2.24011

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.577, [92mTest[0m: 2.354, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.28763
[1mStep[0m  [8/84], [94mLoss[0m : 2.45592
[1mStep[0m  [16/84], [94mLoss[0m : 2.42891
[1mStep[0m  [24/84], [94mLoss[0m : 2.27469
[1mStep[0m  [32/84], [94mLoss[0m : 2.92338
[1mStep[0m  [40/84], [94mLoss[0m : 2.03530
[1mStep[0m  [48/84], [94mLoss[0m : 2.82246
[1mStep[0m  [56/84], [94mLoss[0m : 2.31184
[1mStep[0m  [64/84], [94mLoss[0m : 2.61988
[1mStep[0m  [72/84], [94mLoss[0m : 2.55541
[1mStep[0m  [80/84], [94mLoss[0m : 2.39458

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.551, [92mTest[0m: 2.369, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50498
[1mStep[0m  [8/84], [94mLoss[0m : 2.65007
[1mStep[0m  [16/84], [94mLoss[0m : 2.49986
[1mStep[0m  [24/84], [94mLoss[0m : 2.32332
[1mStep[0m  [32/84], [94mLoss[0m : 2.47634
[1mStep[0m  [40/84], [94mLoss[0m : 2.25348
[1mStep[0m  [48/84], [94mLoss[0m : 2.82789
[1mStep[0m  [56/84], [94mLoss[0m : 2.84469
[1mStep[0m  [64/84], [94mLoss[0m : 2.82112
[1mStep[0m  [72/84], [94mLoss[0m : 2.82155
[1mStep[0m  [80/84], [94mLoss[0m : 2.49006

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.564, [92mTest[0m: 2.352, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59050
[1mStep[0m  [8/84], [94mLoss[0m : 2.48470
[1mStep[0m  [16/84], [94mLoss[0m : 2.58777
[1mStep[0m  [24/84], [94mLoss[0m : 2.78824
[1mStep[0m  [32/84], [94mLoss[0m : 2.63647
[1mStep[0m  [40/84], [94mLoss[0m : 2.32695
[1mStep[0m  [48/84], [94mLoss[0m : 2.59063
[1mStep[0m  [56/84], [94mLoss[0m : 2.55174
[1mStep[0m  [64/84], [94mLoss[0m : 2.67096
[1mStep[0m  [72/84], [94mLoss[0m : 2.43355
[1mStep[0m  [80/84], [94mLoss[0m : 2.66496

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.546, [92mTest[0m: 2.358, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47171
[1mStep[0m  [8/84], [94mLoss[0m : 2.54905
[1mStep[0m  [16/84], [94mLoss[0m : 2.31666
[1mStep[0m  [24/84], [94mLoss[0m : 2.42360
[1mStep[0m  [32/84], [94mLoss[0m : 2.61827
[1mStep[0m  [40/84], [94mLoss[0m : 2.91611
[1mStep[0m  [48/84], [94mLoss[0m : 2.89782
[1mStep[0m  [56/84], [94mLoss[0m : 2.79890
[1mStep[0m  [64/84], [94mLoss[0m : 2.68504
[1mStep[0m  [72/84], [94mLoss[0m : 2.40969
[1mStep[0m  [80/84], [94mLoss[0m : 2.50617

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.557, [92mTest[0m: 2.350, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.39822
[1mStep[0m  [8/84], [94mLoss[0m : 2.38748
[1mStep[0m  [16/84], [94mLoss[0m : 2.53512
[1mStep[0m  [24/84], [94mLoss[0m : 2.50289
[1mStep[0m  [32/84], [94mLoss[0m : 2.47520
[1mStep[0m  [40/84], [94mLoss[0m : 2.38508
[1mStep[0m  [48/84], [94mLoss[0m : 2.41658
[1mStep[0m  [56/84], [94mLoss[0m : 2.25131
[1mStep[0m  [64/84], [94mLoss[0m : 2.64544
[1mStep[0m  [72/84], [94mLoss[0m : 2.68904
[1mStep[0m  [80/84], [94mLoss[0m : 2.50844

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.529, [92mTest[0m: 2.362, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.56508
[1mStep[0m  [8/84], [94mLoss[0m : 2.31403
[1mStep[0m  [16/84], [94mLoss[0m : 2.75089
[1mStep[0m  [24/84], [94mLoss[0m : 2.50048
[1mStep[0m  [32/84], [94mLoss[0m : 2.26257
[1mStep[0m  [40/84], [94mLoss[0m : 2.57076
[1mStep[0m  [48/84], [94mLoss[0m : 2.69622
[1mStep[0m  [56/84], [94mLoss[0m : 2.70935
[1mStep[0m  [64/84], [94mLoss[0m : 2.29355
[1mStep[0m  [72/84], [94mLoss[0m : 2.68772
[1mStep[0m  [80/84], [94mLoss[0m : 2.89103

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.351, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46417
[1mStep[0m  [8/84], [94mLoss[0m : 2.10632
[1mStep[0m  [16/84], [94mLoss[0m : 2.44266
[1mStep[0m  [24/84], [94mLoss[0m : 2.49058
[1mStep[0m  [32/84], [94mLoss[0m : 2.30773
[1mStep[0m  [40/84], [94mLoss[0m : 2.52490
[1mStep[0m  [48/84], [94mLoss[0m : 2.76596
[1mStep[0m  [56/84], [94mLoss[0m : 2.34109
[1mStep[0m  [64/84], [94mLoss[0m : 2.63449
[1mStep[0m  [72/84], [94mLoss[0m : 2.52611
[1mStep[0m  [80/84], [94mLoss[0m : 2.73714

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.526, [92mTest[0m: 2.356, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.73148
[1mStep[0m  [8/84], [94mLoss[0m : 2.54569
[1mStep[0m  [16/84], [94mLoss[0m : 2.44497
[1mStep[0m  [24/84], [94mLoss[0m : 2.55023
[1mStep[0m  [32/84], [94mLoss[0m : 2.34304
[1mStep[0m  [40/84], [94mLoss[0m : 2.41637
[1mStep[0m  [48/84], [94mLoss[0m : 2.53792
[1mStep[0m  [56/84], [94mLoss[0m : 2.31243
[1mStep[0m  [64/84], [94mLoss[0m : 2.51529
[1mStep[0m  [72/84], [94mLoss[0m : 2.26381
[1mStep[0m  [80/84], [94mLoss[0m : 2.43808

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.353, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49229
[1mStep[0m  [8/84], [94mLoss[0m : 2.24315
[1mStep[0m  [16/84], [94mLoss[0m : 2.60509
[1mStep[0m  [24/84], [94mLoss[0m : 2.48728
[1mStep[0m  [32/84], [94mLoss[0m : 2.07590
[1mStep[0m  [40/84], [94mLoss[0m : 2.45006
[1mStep[0m  [48/84], [94mLoss[0m : 2.16308
[1mStep[0m  [56/84], [94mLoss[0m : 2.19915
[1mStep[0m  [64/84], [94mLoss[0m : 2.54586
[1mStep[0m  [72/84], [94mLoss[0m : 2.53516
[1mStep[0m  [80/84], [94mLoss[0m : 2.62132

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.349, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.98028
[1mStep[0m  [8/84], [94mLoss[0m : 2.26773
[1mStep[0m  [16/84], [94mLoss[0m : 2.47185
[1mStep[0m  [24/84], [94mLoss[0m : 2.53535
[1mStep[0m  [32/84], [94mLoss[0m : 2.71041
[1mStep[0m  [40/84], [94mLoss[0m : 2.75602
[1mStep[0m  [48/84], [94mLoss[0m : 2.49908
[1mStep[0m  [56/84], [94mLoss[0m : 2.49355
[1mStep[0m  [64/84], [94mLoss[0m : 2.25090
[1mStep[0m  [72/84], [94mLoss[0m : 2.56414
[1mStep[0m  [80/84], [94mLoss[0m : 2.29044

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.344, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.75951
[1mStep[0m  [8/84], [94mLoss[0m : 2.63260
[1mStep[0m  [16/84], [94mLoss[0m : 2.21969
[1mStep[0m  [24/84], [94mLoss[0m : 2.41232
[1mStep[0m  [32/84], [94mLoss[0m : 2.76016
[1mStep[0m  [40/84], [94mLoss[0m : 2.14422
[1mStep[0m  [48/84], [94mLoss[0m : 2.67391
[1mStep[0m  [56/84], [94mLoss[0m : 2.63473
[1mStep[0m  [64/84], [94mLoss[0m : 2.51295
[1mStep[0m  [72/84], [94mLoss[0m : 2.26004
[1mStep[0m  [80/84], [94mLoss[0m : 2.44323

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.364, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.56867
[1mStep[0m  [8/84], [94mLoss[0m : 2.51871
[1mStep[0m  [16/84], [94mLoss[0m : 2.40184
[1mStep[0m  [24/84], [94mLoss[0m : 2.47027
[1mStep[0m  [32/84], [94mLoss[0m : 2.61694
[1mStep[0m  [40/84], [94mLoss[0m : 2.57864
[1mStep[0m  [48/84], [94mLoss[0m : 2.54683
[1mStep[0m  [56/84], [94mLoss[0m : 2.34089
[1mStep[0m  [64/84], [94mLoss[0m : 2.96558
[1mStep[0m  [72/84], [94mLoss[0m : 2.34688
[1mStep[0m  [80/84], [94mLoss[0m : 2.81046

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.357, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.02066
[1mStep[0m  [8/84], [94mLoss[0m : 2.32413
[1mStep[0m  [16/84], [94mLoss[0m : 2.88880
[1mStep[0m  [24/84], [94mLoss[0m : 2.66981
[1mStep[0m  [32/84], [94mLoss[0m : 2.51885
[1mStep[0m  [40/84], [94mLoss[0m : 2.56954
[1mStep[0m  [48/84], [94mLoss[0m : 2.40330
[1mStep[0m  [56/84], [94mLoss[0m : 2.38446
[1mStep[0m  [64/84], [94mLoss[0m : 2.56489
[1mStep[0m  [72/84], [94mLoss[0m : 2.33531
[1mStep[0m  [80/84], [94mLoss[0m : 2.76211

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.325, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.344
====================================

Phase 1 - Evaluation MAE:  2.344017335346767
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 2.26995
[1mStep[0m  [8/84], [94mLoss[0m : 2.62010
[1mStep[0m  [16/84], [94mLoss[0m : 2.52074
[1mStep[0m  [24/84], [94mLoss[0m : 2.31335
[1mStep[0m  [32/84], [94mLoss[0m : 2.60441
[1mStep[0m  [40/84], [94mLoss[0m : 2.47959
[1mStep[0m  [48/84], [94mLoss[0m : 2.86612
[1mStep[0m  [56/84], [94mLoss[0m : 2.40696
[1mStep[0m  [64/84], [94mLoss[0m : 2.57683
[1mStep[0m  [72/84], [94mLoss[0m : 2.64955
[1mStep[0m  [80/84], [94mLoss[0m : 2.66499

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.552, [92mTest[0m: 2.351, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60076
[1mStep[0m  [8/84], [94mLoss[0m : 2.25181
[1mStep[0m  [16/84], [94mLoss[0m : 2.42326
[1mStep[0m  [24/84], [94mLoss[0m : 2.65670
[1mStep[0m  [32/84], [94mLoss[0m : 2.61595
[1mStep[0m  [40/84], [94mLoss[0m : 2.70882
[1mStep[0m  [48/84], [94mLoss[0m : 2.46582
[1mStep[0m  [56/84], [94mLoss[0m : 2.43608
[1mStep[0m  [64/84], [94mLoss[0m : 2.51706
[1mStep[0m  [72/84], [94mLoss[0m : 2.23330
[1mStep[0m  [80/84], [94mLoss[0m : 2.63995

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.425, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53699
[1mStep[0m  [8/84], [94mLoss[0m : 2.51829
[1mStep[0m  [16/84], [94mLoss[0m : 2.26314
[1mStep[0m  [24/84], [94mLoss[0m : 2.73239
[1mStep[0m  [32/84], [94mLoss[0m : 2.46570
[1mStep[0m  [40/84], [94mLoss[0m : 2.34638
[1mStep[0m  [48/84], [94mLoss[0m : 2.09588
[1mStep[0m  [56/84], [94mLoss[0m : 2.27298
[1mStep[0m  [64/84], [94mLoss[0m : 2.70571
[1mStep[0m  [72/84], [94mLoss[0m : 2.38385
[1mStep[0m  [80/84], [94mLoss[0m : 2.47855

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.527, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.18396
[1mStep[0m  [8/84], [94mLoss[0m : 2.29052
[1mStep[0m  [16/84], [94mLoss[0m : 2.47733
[1mStep[0m  [24/84], [94mLoss[0m : 2.59921
[1mStep[0m  [32/84], [94mLoss[0m : 2.42075
[1mStep[0m  [40/84], [94mLoss[0m : 2.34690
[1mStep[0m  [48/84], [94mLoss[0m : 2.30057
[1mStep[0m  [56/84], [94mLoss[0m : 2.01976
[1mStep[0m  [64/84], [94mLoss[0m : 2.21281
[1mStep[0m  [72/84], [94mLoss[0m : 2.54188
[1mStep[0m  [80/84], [94mLoss[0m : 2.02112

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.376, [92mTest[0m: 2.561, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55940
[1mStep[0m  [8/84], [94mLoss[0m : 2.06596
[1mStep[0m  [16/84], [94mLoss[0m : 2.27371
[1mStep[0m  [24/84], [94mLoss[0m : 2.42307
[1mStep[0m  [32/84], [94mLoss[0m : 2.45100
[1mStep[0m  [40/84], [94mLoss[0m : 2.25905
[1mStep[0m  [48/84], [94mLoss[0m : 2.29720
[1mStep[0m  [56/84], [94mLoss[0m : 2.26467
[1mStep[0m  [64/84], [94mLoss[0m : 2.61051
[1mStep[0m  [72/84], [94mLoss[0m : 2.44688
[1mStep[0m  [80/84], [94mLoss[0m : 2.07497

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.324, [92mTest[0m: 2.469, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27350
[1mStep[0m  [8/84], [94mLoss[0m : 2.24620
[1mStep[0m  [16/84], [94mLoss[0m : 2.29199
[1mStep[0m  [24/84], [94mLoss[0m : 2.06434
[1mStep[0m  [32/84], [94mLoss[0m : 2.09437
[1mStep[0m  [40/84], [94mLoss[0m : 2.27538
[1mStep[0m  [48/84], [94mLoss[0m : 2.25230
[1mStep[0m  [56/84], [94mLoss[0m : 2.37518
[1mStep[0m  [64/84], [94mLoss[0m : 2.26967
[1mStep[0m  [72/84], [94mLoss[0m : 2.22087
[1mStep[0m  [80/84], [94mLoss[0m : 2.23514

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.274, [92mTest[0m: 2.472, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.93921
[1mStep[0m  [8/84], [94mLoss[0m : 2.11266
[1mStep[0m  [16/84], [94mLoss[0m : 2.34471
[1mStep[0m  [24/84], [94mLoss[0m : 2.12855
[1mStep[0m  [32/84], [94mLoss[0m : 2.20792
[1mStep[0m  [40/84], [94mLoss[0m : 2.17439
[1mStep[0m  [48/84], [94mLoss[0m : 2.17830
[1mStep[0m  [56/84], [94mLoss[0m : 2.28637
[1mStep[0m  [64/84], [94mLoss[0m : 2.16966
[1mStep[0m  [72/84], [94mLoss[0m : 2.03892
[1mStep[0m  [80/84], [94mLoss[0m : 2.12316

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.227, [92mTest[0m: 2.423, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35855
[1mStep[0m  [8/84], [94mLoss[0m : 2.00812
[1mStep[0m  [16/84], [94mLoss[0m : 2.26891
[1mStep[0m  [24/84], [94mLoss[0m : 1.84317
[1mStep[0m  [32/84], [94mLoss[0m : 2.19324
[1mStep[0m  [40/84], [94mLoss[0m : 2.22095
[1mStep[0m  [48/84], [94mLoss[0m : 2.39585
[1mStep[0m  [56/84], [94mLoss[0m : 2.28671
[1mStep[0m  [64/84], [94mLoss[0m : 2.14539
[1mStep[0m  [72/84], [94mLoss[0m : 1.93357
[1mStep[0m  [80/84], [94mLoss[0m : 1.95781

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.166, [92mTest[0m: 2.461, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54323
[1mStep[0m  [8/84], [94mLoss[0m : 2.02613
[1mStep[0m  [16/84], [94mLoss[0m : 2.17950
[1mStep[0m  [24/84], [94mLoss[0m : 1.93472
[1mStep[0m  [32/84], [94mLoss[0m : 2.05792
[1mStep[0m  [40/84], [94mLoss[0m : 2.34078
[1mStep[0m  [48/84], [94mLoss[0m : 2.24207
[1mStep[0m  [56/84], [94mLoss[0m : 1.82242
[1mStep[0m  [64/84], [94mLoss[0m : 1.98588
[1mStep[0m  [72/84], [94mLoss[0m : 2.04444
[1mStep[0m  [80/84], [94mLoss[0m : 2.35387

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.143, [92mTest[0m: 2.474, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.95235
[1mStep[0m  [8/84], [94mLoss[0m : 2.10355
[1mStep[0m  [16/84], [94mLoss[0m : 2.06857
[1mStep[0m  [24/84], [94mLoss[0m : 1.95601
[1mStep[0m  [32/84], [94mLoss[0m : 2.15924
[1mStep[0m  [40/84], [94mLoss[0m : 1.93975
[1mStep[0m  [48/84], [94mLoss[0m : 2.08803
[1mStep[0m  [56/84], [94mLoss[0m : 2.26957
[1mStep[0m  [64/84], [94mLoss[0m : 1.95065
[1mStep[0m  [72/84], [94mLoss[0m : 2.21449
[1mStep[0m  [80/84], [94mLoss[0m : 2.30937

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.068, [92mTest[0m: 2.510, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.96053
[1mStep[0m  [8/84], [94mLoss[0m : 1.95399
[1mStep[0m  [16/84], [94mLoss[0m : 2.18251
[1mStep[0m  [24/84], [94mLoss[0m : 1.82071
[1mStep[0m  [32/84], [94mLoss[0m : 1.96328
[1mStep[0m  [40/84], [94mLoss[0m : 2.28502
[1mStep[0m  [48/84], [94mLoss[0m : 1.79495
[1mStep[0m  [56/84], [94mLoss[0m : 1.89586
[1mStep[0m  [64/84], [94mLoss[0m : 2.11407
[1mStep[0m  [72/84], [94mLoss[0m : 2.29731
[1mStep[0m  [80/84], [94mLoss[0m : 2.12611

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.023, [92mTest[0m: 2.436, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.25829
[1mStep[0m  [8/84], [94mLoss[0m : 1.77017
[1mStep[0m  [16/84], [94mLoss[0m : 1.86768
[1mStep[0m  [24/84], [94mLoss[0m : 1.68407
[1mStep[0m  [32/84], [94mLoss[0m : 1.92353
[1mStep[0m  [40/84], [94mLoss[0m : 2.13236
[1mStep[0m  [48/84], [94mLoss[0m : 2.19414
[1mStep[0m  [56/84], [94mLoss[0m : 1.91318
[1mStep[0m  [64/84], [94mLoss[0m : 1.81486
[1mStep[0m  [72/84], [94mLoss[0m : 1.64082
[1mStep[0m  [80/84], [94mLoss[0m : 2.06852

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.990, [92mTest[0m: 2.511, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.73466
[1mStep[0m  [8/84], [94mLoss[0m : 1.77977
[1mStep[0m  [16/84], [94mLoss[0m : 1.87321
[1mStep[0m  [24/84], [94mLoss[0m : 1.71202
[1mStep[0m  [32/84], [94mLoss[0m : 2.07076
[1mStep[0m  [40/84], [94mLoss[0m : 1.82447
[1mStep[0m  [48/84], [94mLoss[0m : 2.04985
[1mStep[0m  [56/84], [94mLoss[0m : 2.07834
[1mStep[0m  [64/84], [94mLoss[0m : 2.03798
[1mStep[0m  [72/84], [94mLoss[0m : 2.16766
[1mStep[0m  [80/84], [94mLoss[0m : 1.80118

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.952, [92mTest[0m: 2.446, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.90251
[1mStep[0m  [8/84], [94mLoss[0m : 1.68110
[1mStep[0m  [16/84], [94mLoss[0m : 1.96023
[1mStep[0m  [24/84], [94mLoss[0m : 2.12455
[1mStep[0m  [32/84], [94mLoss[0m : 2.07070
[1mStep[0m  [40/84], [94mLoss[0m : 1.97716
[1mStep[0m  [48/84], [94mLoss[0m : 1.87076
[1mStep[0m  [56/84], [94mLoss[0m : 1.74922
[1mStep[0m  [64/84], [94mLoss[0m : 2.10417
[1mStep[0m  [72/84], [94mLoss[0m : 1.79107
[1mStep[0m  [80/84], [94mLoss[0m : 2.11933

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.924, [92mTest[0m: 2.443, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.77360
[1mStep[0m  [8/84], [94mLoss[0m : 1.63865
[1mStep[0m  [16/84], [94mLoss[0m : 1.79204
[1mStep[0m  [24/84], [94mLoss[0m : 1.80808
[1mStep[0m  [32/84], [94mLoss[0m : 1.83909
[1mStep[0m  [40/84], [94mLoss[0m : 2.07182
[1mStep[0m  [48/84], [94mLoss[0m : 1.94336
[1mStep[0m  [56/84], [94mLoss[0m : 1.82811
[1mStep[0m  [64/84], [94mLoss[0m : 1.88899
[1mStep[0m  [72/84], [94mLoss[0m : 1.95518
[1mStep[0m  [80/84], [94mLoss[0m : 2.02040

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.869, [92mTest[0m: 2.449, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.80072
[1mStep[0m  [8/84], [94mLoss[0m : 1.84195
[1mStep[0m  [16/84], [94mLoss[0m : 1.87863
[1mStep[0m  [24/84], [94mLoss[0m : 1.95868
[1mStep[0m  [32/84], [94mLoss[0m : 1.86999
[1mStep[0m  [40/84], [94mLoss[0m : 1.59922
[1mStep[0m  [48/84], [94mLoss[0m : 1.88445
[1mStep[0m  [56/84], [94mLoss[0m : 1.89220
[1mStep[0m  [64/84], [94mLoss[0m : 2.06784
[1mStep[0m  [72/84], [94mLoss[0m : 1.85063
[1mStep[0m  [80/84], [94mLoss[0m : 2.03547

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.835, [92mTest[0m: 2.443, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.67285
[1mStep[0m  [8/84], [94mLoss[0m : 1.46456
[1mStep[0m  [16/84], [94mLoss[0m : 1.93102
[1mStep[0m  [24/84], [94mLoss[0m : 1.60414
[1mStep[0m  [32/84], [94mLoss[0m : 1.74699
[1mStep[0m  [40/84], [94mLoss[0m : 1.78732
[1mStep[0m  [48/84], [94mLoss[0m : 1.91322
[1mStep[0m  [56/84], [94mLoss[0m : 1.83772
[1mStep[0m  [64/84], [94mLoss[0m : 1.69607
[1mStep[0m  [72/84], [94mLoss[0m : 2.02979
[1mStep[0m  [80/84], [94mLoss[0m : 1.68554

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.800, [92mTest[0m: 2.441, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.55409
[1mStep[0m  [8/84], [94mLoss[0m : 1.88716
[1mStep[0m  [16/84], [94mLoss[0m : 1.71689
[1mStep[0m  [24/84], [94mLoss[0m : 1.89804
[1mStep[0m  [32/84], [94mLoss[0m : 1.74379
[1mStep[0m  [40/84], [94mLoss[0m : 1.56660
[1mStep[0m  [48/84], [94mLoss[0m : 1.59596
[1mStep[0m  [56/84], [94mLoss[0m : 1.74050
[1mStep[0m  [64/84], [94mLoss[0m : 1.47851
[1mStep[0m  [72/84], [94mLoss[0m : 1.58423
[1mStep[0m  [80/84], [94mLoss[0m : 1.79129

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.763, [92mTest[0m: 2.565, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.51454
[1mStep[0m  [8/84], [94mLoss[0m : 1.79644
[1mStep[0m  [16/84], [94mLoss[0m : 1.95675
[1mStep[0m  [24/84], [94mLoss[0m : 1.82493
[1mStep[0m  [32/84], [94mLoss[0m : 1.72119
[1mStep[0m  [40/84], [94mLoss[0m : 1.70523
[1mStep[0m  [48/84], [94mLoss[0m : 1.63028
[1mStep[0m  [56/84], [94mLoss[0m : 1.79260
[1mStep[0m  [64/84], [94mLoss[0m : 1.62079
[1mStep[0m  [72/84], [94mLoss[0m : 1.89842
[1mStep[0m  [80/84], [94mLoss[0m : 1.78840

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.717, [92mTest[0m: 2.455, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.72710
[1mStep[0m  [8/84], [94mLoss[0m : 1.73216
[1mStep[0m  [16/84], [94mLoss[0m : 1.60942
[1mStep[0m  [24/84], [94mLoss[0m : 1.85775
[1mStep[0m  [32/84], [94mLoss[0m : 1.78883
[1mStep[0m  [40/84], [94mLoss[0m : 1.87196
[1mStep[0m  [48/84], [94mLoss[0m : 1.82949
[1mStep[0m  [56/84], [94mLoss[0m : 1.78388
[1mStep[0m  [64/84], [94mLoss[0m : 1.69682
[1mStep[0m  [72/84], [94mLoss[0m : 1.70920
[1mStep[0m  [80/84], [94mLoss[0m : 1.76360

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.716, [92mTest[0m: 2.499, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.67007
[1mStep[0m  [8/84], [94mLoss[0m : 1.50095
[1mStep[0m  [16/84], [94mLoss[0m : 1.73316
[1mStep[0m  [24/84], [94mLoss[0m : 1.58533
[1mStep[0m  [32/84], [94mLoss[0m : 1.68665
[1mStep[0m  [40/84], [94mLoss[0m : 1.52534
[1mStep[0m  [48/84], [94mLoss[0m : 1.81676
[1mStep[0m  [56/84], [94mLoss[0m : 1.72193
[1mStep[0m  [64/84], [94mLoss[0m : 1.47221
[1mStep[0m  [72/84], [94mLoss[0m : 2.07347
[1mStep[0m  [80/84], [94mLoss[0m : 1.62984

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.649, [92mTest[0m: 2.474, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.76937
[1mStep[0m  [8/84], [94mLoss[0m : 1.42408
[1mStep[0m  [16/84], [94mLoss[0m : 1.64935
[1mStep[0m  [24/84], [94mLoss[0m : 1.70007
[1mStep[0m  [32/84], [94mLoss[0m : 1.58806
[1mStep[0m  [40/84], [94mLoss[0m : 1.65527
[1mStep[0m  [48/84], [94mLoss[0m : 1.68588
[1mStep[0m  [56/84], [94mLoss[0m : 1.72203
[1mStep[0m  [64/84], [94mLoss[0m : 1.60886
[1mStep[0m  [72/84], [94mLoss[0m : 1.58121
[1mStep[0m  [80/84], [94mLoss[0m : 1.66767

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.613, [92mTest[0m: 2.459, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.69500
[1mStep[0m  [8/84], [94mLoss[0m : 1.37076
[1mStep[0m  [16/84], [94mLoss[0m : 1.79597
[1mStep[0m  [24/84], [94mLoss[0m : 1.49882
[1mStep[0m  [32/84], [94mLoss[0m : 1.64520
[1mStep[0m  [40/84], [94mLoss[0m : 1.51017
[1mStep[0m  [48/84], [94mLoss[0m : 1.61834
[1mStep[0m  [56/84], [94mLoss[0m : 1.62439
[1mStep[0m  [64/84], [94mLoss[0m : 1.54035
[1mStep[0m  [72/84], [94mLoss[0m : 1.90436
[1mStep[0m  [80/84], [94mLoss[0m : 1.84876

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.605, [92mTest[0m: 2.506, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.48192
[1mStep[0m  [8/84], [94mLoss[0m : 1.51556
[1mStep[0m  [16/84], [94mLoss[0m : 1.58884
[1mStep[0m  [24/84], [94mLoss[0m : 1.71971
[1mStep[0m  [32/84], [94mLoss[0m : 1.33853
[1mStep[0m  [40/84], [94mLoss[0m : 1.61386
[1mStep[0m  [48/84], [94mLoss[0m : 1.30920
[1mStep[0m  [56/84], [94mLoss[0m : 1.61678
[1mStep[0m  [64/84], [94mLoss[0m : 1.46139
[1mStep[0m  [72/84], [94mLoss[0m : 1.66811
[1mStep[0m  [80/84], [94mLoss[0m : 1.69160

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.584, [92mTest[0m: 2.588, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.59458
[1mStep[0m  [8/84], [94mLoss[0m : 1.64863
[1mStep[0m  [16/84], [94mLoss[0m : 1.74013
[1mStep[0m  [24/84], [94mLoss[0m : 1.61408
[1mStep[0m  [32/84], [94mLoss[0m : 1.48038
[1mStep[0m  [40/84], [94mLoss[0m : 1.49228
[1mStep[0m  [48/84], [94mLoss[0m : 1.54337
[1mStep[0m  [56/84], [94mLoss[0m : 1.52968
[1mStep[0m  [64/84], [94mLoss[0m : 1.48475
[1mStep[0m  [72/84], [94mLoss[0m : 1.55824
[1mStep[0m  [80/84], [94mLoss[0m : 1.65292

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.565, [92mTest[0m: 2.514, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.66116
[1mStep[0m  [8/84], [94mLoss[0m : 1.69510
[1mStep[0m  [16/84], [94mLoss[0m : 1.62746
[1mStep[0m  [24/84], [94mLoss[0m : 1.58728
[1mStep[0m  [32/84], [94mLoss[0m : 1.69938
[1mStep[0m  [40/84], [94mLoss[0m : 1.56228
[1mStep[0m  [48/84], [94mLoss[0m : 1.67426
[1mStep[0m  [56/84], [94mLoss[0m : 1.52611
[1mStep[0m  [64/84], [94mLoss[0m : 1.43485
[1mStep[0m  [72/84], [94mLoss[0m : 1.51152
[1mStep[0m  [80/84], [94mLoss[0m : 1.79138

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.541, [92mTest[0m: 2.513, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.65207
[1mStep[0m  [8/84], [94mLoss[0m : 1.45474
[1mStep[0m  [16/84], [94mLoss[0m : 1.39439
[1mStep[0m  [24/84], [94mLoss[0m : 1.55581
[1mStep[0m  [32/84], [94mLoss[0m : 1.36097
[1mStep[0m  [40/84], [94mLoss[0m : 1.55198
[1mStep[0m  [48/84], [94mLoss[0m : 1.52643
[1mStep[0m  [56/84], [94mLoss[0m : 1.21824
[1mStep[0m  [64/84], [94mLoss[0m : 1.56570
[1mStep[0m  [72/84], [94mLoss[0m : 1.40187
[1mStep[0m  [80/84], [94mLoss[0m : 1.47027

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.504, [92mTest[0m: 2.527, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.63624
[1mStep[0m  [8/84], [94mLoss[0m : 1.31833
[1mStep[0m  [16/84], [94mLoss[0m : 1.38871
[1mStep[0m  [24/84], [94mLoss[0m : 1.59153
[1mStep[0m  [32/84], [94mLoss[0m : 1.39025
[1mStep[0m  [40/84], [94mLoss[0m : 1.35242
[1mStep[0m  [48/84], [94mLoss[0m : 1.57229
[1mStep[0m  [56/84], [94mLoss[0m : 1.42318
[1mStep[0m  [64/84], [94mLoss[0m : 1.76535
[1mStep[0m  [72/84], [94mLoss[0m : 1.53197
[1mStep[0m  [80/84], [94mLoss[0m : 1.31307

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.495, [92mTest[0m: 2.520, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.37914
[1mStep[0m  [8/84], [94mLoss[0m : 1.40983
[1mStep[0m  [16/84], [94mLoss[0m : 1.37265
[1mStep[0m  [24/84], [94mLoss[0m : 1.43660
[1mStep[0m  [32/84], [94mLoss[0m : 1.52453
[1mStep[0m  [40/84], [94mLoss[0m : 1.46703
[1mStep[0m  [48/84], [94mLoss[0m : 1.36234
[1mStep[0m  [56/84], [94mLoss[0m : 1.39982
[1mStep[0m  [64/84], [94mLoss[0m : 1.35405
[1mStep[0m  [72/84], [94mLoss[0m : 1.37302
[1mStep[0m  [80/84], [94mLoss[0m : 1.68215

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.486, [92mTest[0m: 2.485, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.23905
[1mStep[0m  [8/84], [94mLoss[0m : 1.37098
[1mStep[0m  [16/84], [94mLoss[0m : 1.53710
[1mStep[0m  [24/84], [94mLoss[0m : 1.58881
[1mStep[0m  [32/84], [94mLoss[0m : 1.30289
[1mStep[0m  [40/84], [94mLoss[0m : 1.67429
[1mStep[0m  [48/84], [94mLoss[0m : 1.49226
[1mStep[0m  [56/84], [94mLoss[0m : 1.35075
[1mStep[0m  [64/84], [94mLoss[0m : 1.48263
[1mStep[0m  [72/84], [94mLoss[0m : 1.35368
[1mStep[0m  [80/84], [94mLoss[0m : 1.50258

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.451, [92mTest[0m: 2.451, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.540
====================================

Phase 2 - Evaluation MAE:  2.539607950619289
MAE score P1      2.344017
MAE score P2      2.539608
loss              1.451278
learning_rate      0.00505
batch_size             128
hidden_sizes         [300]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.4
momentum               0.5
weight_decay         0.001
Name: 11, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/169], [94mLoss[0m : 10.63763
[1mStep[0m  [16/169], [94mLoss[0m : 4.12514
[1mStep[0m  [32/169], [94mLoss[0m : 2.39375
[1mStep[0m  [48/169], [94mLoss[0m : 2.61470
[1mStep[0m  [64/169], [94mLoss[0m : 2.53914
[1mStep[0m  [80/169], [94mLoss[0m : 2.41823
[1mStep[0m  [96/169], [94mLoss[0m : 2.27905
[1mStep[0m  [112/169], [94mLoss[0m : 2.31832
[1mStep[0m  [128/169], [94mLoss[0m : 2.11230
[1mStep[0m  [144/169], [94mLoss[0m : 2.51673
[1mStep[0m  [160/169], [94mLoss[0m : 2.62599

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.901, [92mTest[0m: 10.777, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.26292
[1mStep[0m  [16/169], [94mLoss[0m : 2.60730
[1mStep[0m  [32/169], [94mLoss[0m : 2.54054
[1mStep[0m  [48/169], [94mLoss[0m : 2.57562
[1mStep[0m  [64/169], [94mLoss[0m : 2.41672
[1mStep[0m  [80/169], [94mLoss[0m : 2.30051
[1mStep[0m  [96/169], [94mLoss[0m : 2.30054
[1mStep[0m  [112/169], [94mLoss[0m : 2.51032
[1mStep[0m  [128/169], [94mLoss[0m : 2.74059
[1mStep[0m  [144/169], [94mLoss[0m : 2.84385
[1mStep[0m  [160/169], [94mLoss[0m : 2.62760

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.456, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.09734
[1mStep[0m  [16/169], [94mLoss[0m : 2.36043
[1mStep[0m  [32/169], [94mLoss[0m : 2.57373
[1mStep[0m  [48/169], [94mLoss[0m : 2.57401
[1mStep[0m  [64/169], [94mLoss[0m : 2.41929
[1mStep[0m  [80/169], [94mLoss[0m : 2.27049
[1mStep[0m  [96/169], [94mLoss[0m : 2.22711
[1mStep[0m  [112/169], [94mLoss[0m : 2.44680
[1mStep[0m  [128/169], [94mLoss[0m : 2.42717
[1mStep[0m  [144/169], [94mLoss[0m : 2.48805
[1mStep[0m  [160/169], [94mLoss[0m : 2.28813

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.348, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.59180
[1mStep[0m  [16/169], [94mLoss[0m : 2.03681
[1mStep[0m  [32/169], [94mLoss[0m : 2.22825
[1mStep[0m  [48/169], [94mLoss[0m : 1.99246
[1mStep[0m  [64/169], [94mLoss[0m : 2.58114
[1mStep[0m  [80/169], [94mLoss[0m : 2.79177
[1mStep[0m  [96/169], [94mLoss[0m : 2.56855
[1mStep[0m  [112/169], [94mLoss[0m : 2.67262
[1mStep[0m  [128/169], [94mLoss[0m : 2.84126
[1mStep[0m  [144/169], [94mLoss[0m : 2.16866
[1mStep[0m  [160/169], [94mLoss[0m : 2.75453

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.337, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.40161
[1mStep[0m  [16/169], [94mLoss[0m : 2.13436
[1mStep[0m  [32/169], [94mLoss[0m : 2.26873
[1mStep[0m  [48/169], [94mLoss[0m : 2.55136
[1mStep[0m  [64/169], [94mLoss[0m : 3.20308
[1mStep[0m  [80/169], [94mLoss[0m : 2.34195
[1mStep[0m  [96/169], [94mLoss[0m : 2.43002
[1mStep[0m  [112/169], [94mLoss[0m : 1.78755
[1mStep[0m  [128/169], [94mLoss[0m : 2.73279
[1mStep[0m  [144/169], [94mLoss[0m : 2.07627
[1mStep[0m  [160/169], [94mLoss[0m : 2.62824

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.369, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.64413
[1mStep[0m  [16/169], [94mLoss[0m : 2.13032
[1mStep[0m  [32/169], [94mLoss[0m : 2.51055
[1mStep[0m  [48/169], [94mLoss[0m : 2.22613
[1mStep[0m  [64/169], [94mLoss[0m : 2.65802
[1mStep[0m  [80/169], [94mLoss[0m : 2.79821
[1mStep[0m  [96/169], [94mLoss[0m : 2.11113
[1mStep[0m  [112/169], [94mLoss[0m : 2.54034
[1mStep[0m  [128/169], [94mLoss[0m : 2.09029
[1mStep[0m  [144/169], [94mLoss[0m : 2.16666
[1mStep[0m  [160/169], [94mLoss[0m : 2.32666

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.335, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.53393
[1mStep[0m  [16/169], [94mLoss[0m : 2.57063
[1mStep[0m  [32/169], [94mLoss[0m : 2.58384
[1mStep[0m  [48/169], [94mLoss[0m : 1.96434
[1mStep[0m  [64/169], [94mLoss[0m : 2.59912
[1mStep[0m  [80/169], [94mLoss[0m : 2.46271
[1mStep[0m  [96/169], [94mLoss[0m : 2.70640
[1mStep[0m  [112/169], [94mLoss[0m : 2.05865
[1mStep[0m  [128/169], [94mLoss[0m : 2.43858
[1mStep[0m  [144/169], [94mLoss[0m : 2.71580
[1mStep[0m  [160/169], [94mLoss[0m : 2.39224

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.345, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.37091
[1mStep[0m  [16/169], [94mLoss[0m : 2.36087
[1mStep[0m  [32/169], [94mLoss[0m : 2.73209
[1mStep[0m  [48/169], [94mLoss[0m : 2.18384
[1mStep[0m  [64/169], [94mLoss[0m : 2.33410
[1mStep[0m  [80/169], [94mLoss[0m : 2.49470
[1mStep[0m  [96/169], [94mLoss[0m : 2.46649
[1mStep[0m  [112/169], [94mLoss[0m : 2.92835
[1mStep[0m  [128/169], [94mLoss[0m : 2.33790
[1mStep[0m  [144/169], [94mLoss[0m : 2.35644
[1mStep[0m  [160/169], [94mLoss[0m : 2.55552

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.362, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.33276
[1mStep[0m  [16/169], [94mLoss[0m : 2.65362
[1mStep[0m  [32/169], [94mLoss[0m : 2.71338
[1mStep[0m  [48/169], [94mLoss[0m : 2.43772
[1mStep[0m  [64/169], [94mLoss[0m : 2.29106
[1mStep[0m  [80/169], [94mLoss[0m : 2.39345
[1mStep[0m  [96/169], [94mLoss[0m : 2.68361
[1mStep[0m  [112/169], [94mLoss[0m : 2.49186
[1mStep[0m  [128/169], [94mLoss[0m : 2.37649
[1mStep[0m  [144/169], [94mLoss[0m : 2.48055
[1mStep[0m  [160/169], [94mLoss[0m : 2.49135

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.373, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.40880
[1mStep[0m  [16/169], [94mLoss[0m : 2.09092
[1mStep[0m  [32/169], [94mLoss[0m : 2.58022
[1mStep[0m  [48/169], [94mLoss[0m : 2.54672
[1mStep[0m  [64/169], [94mLoss[0m : 2.67567
[1mStep[0m  [80/169], [94mLoss[0m : 2.14428
[1mStep[0m  [96/169], [94mLoss[0m : 2.81503
[1mStep[0m  [112/169], [94mLoss[0m : 2.64749
[1mStep[0m  [128/169], [94mLoss[0m : 2.16788
[1mStep[0m  [144/169], [94mLoss[0m : 2.01200
[1mStep[0m  [160/169], [94mLoss[0m : 2.67669

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.350, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.07488
[1mStep[0m  [16/169], [94mLoss[0m : 2.47736
[1mStep[0m  [32/169], [94mLoss[0m : 2.32875
[1mStep[0m  [48/169], [94mLoss[0m : 2.27489
[1mStep[0m  [64/169], [94mLoss[0m : 2.38996
[1mStep[0m  [80/169], [94mLoss[0m : 2.41400
[1mStep[0m  [96/169], [94mLoss[0m : 2.84601
[1mStep[0m  [112/169], [94mLoss[0m : 2.47248
[1mStep[0m  [128/169], [94mLoss[0m : 2.46378
[1mStep[0m  [144/169], [94mLoss[0m : 2.27404
[1mStep[0m  [160/169], [94mLoss[0m : 2.58977

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.347, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.00455
[1mStep[0m  [16/169], [94mLoss[0m : 3.06008
[1mStep[0m  [32/169], [94mLoss[0m : 2.26444
[1mStep[0m  [48/169], [94mLoss[0m : 2.24317
[1mStep[0m  [64/169], [94mLoss[0m : 2.12809
[1mStep[0m  [80/169], [94mLoss[0m : 2.39393
[1mStep[0m  [96/169], [94mLoss[0m : 2.15904
[1mStep[0m  [112/169], [94mLoss[0m : 2.45130
[1mStep[0m  [128/169], [94mLoss[0m : 2.65534
[1mStep[0m  [144/169], [94mLoss[0m : 2.26272
[1mStep[0m  [160/169], [94mLoss[0m : 2.68079

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.337, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.00614
[1mStep[0m  [16/169], [94mLoss[0m : 2.35745
[1mStep[0m  [32/169], [94mLoss[0m : 2.10237
[1mStep[0m  [48/169], [94mLoss[0m : 2.24373
[1mStep[0m  [64/169], [94mLoss[0m : 2.46499
[1mStep[0m  [80/169], [94mLoss[0m : 2.57559
[1mStep[0m  [96/169], [94mLoss[0m : 2.73396
[1mStep[0m  [112/169], [94mLoss[0m : 2.81839
[1mStep[0m  [128/169], [94mLoss[0m : 2.07496
[1mStep[0m  [144/169], [94mLoss[0m : 2.43360
[1mStep[0m  [160/169], [94mLoss[0m : 2.22541

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.354, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.30533
[1mStep[0m  [16/169], [94mLoss[0m : 2.60729
[1mStep[0m  [32/169], [94mLoss[0m : 3.01512
[1mStep[0m  [48/169], [94mLoss[0m : 2.57729
[1mStep[0m  [64/169], [94mLoss[0m : 2.19957
[1mStep[0m  [80/169], [94mLoss[0m : 2.30200
[1mStep[0m  [96/169], [94mLoss[0m : 2.67511
[1mStep[0m  [112/169], [94mLoss[0m : 2.58860
[1mStep[0m  [128/169], [94mLoss[0m : 2.12934
[1mStep[0m  [144/169], [94mLoss[0m : 2.41056
[1mStep[0m  [160/169], [94mLoss[0m : 2.06285

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.450, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.03815
[1mStep[0m  [16/169], [94mLoss[0m : 2.34566
[1mStep[0m  [32/169], [94mLoss[0m : 2.35377
[1mStep[0m  [48/169], [94mLoss[0m : 2.42048
[1mStep[0m  [64/169], [94mLoss[0m : 2.68411
[1mStep[0m  [80/169], [94mLoss[0m : 2.26872
[1mStep[0m  [96/169], [94mLoss[0m : 2.59153
[1mStep[0m  [112/169], [94mLoss[0m : 2.09110
[1mStep[0m  [128/169], [94mLoss[0m : 2.76163
[1mStep[0m  [144/169], [94mLoss[0m : 2.63236
[1mStep[0m  [160/169], [94mLoss[0m : 2.63051

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.352, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.39657
[1mStep[0m  [16/169], [94mLoss[0m : 2.33418
[1mStep[0m  [32/169], [94mLoss[0m : 2.83134
[1mStep[0m  [48/169], [94mLoss[0m : 2.26974
[1mStep[0m  [64/169], [94mLoss[0m : 2.27347
[1mStep[0m  [80/169], [94mLoss[0m : 2.37631
[1mStep[0m  [96/169], [94mLoss[0m : 3.09324
[1mStep[0m  [112/169], [94mLoss[0m : 2.60449
[1mStep[0m  [128/169], [94mLoss[0m : 2.13551
[1mStep[0m  [144/169], [94mLoss[0m : 2.30483
[1mStep[0m  [160/169], [94mLoss[0m : 2.23274

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.379, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.84091
[1mStep[0m  [16/169], [94mLoss[0m : 2.43385
[1mStep[0m  [32/169], [94mLoss[0m : 1.88908
[1mStep[0m  [48/169], [94mLoss[0m : 2.37914
[1mStep[0m  [64/169], [94mLoss[0m : 2.55090
[1mStep[0m  [80/169], [94mLoss[0m : 2.47640
[1mStep[0m  [96/169], [94mLoss[0m : 2.23252
[1mStep[0m  [112/169], [94mLoss[0m : 2.57184
[1mStep[0m  [128/169], [94mLoss[0m : 2.20888
[1mStep[0m  [144/169], [94mLoss[0m : 2.68059
[1mStep[0m  [160/169], [94mLoss[0m : 2.78533

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.379, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.53496
[1mStep[0m  [16/169], [94mLoss[0m : 2.30711
[1mStep[0m  [32/169], [94mLoss[0m : 2.81403
[1mStep[0m  [48/169], [94mLoss[0m : 2.55421
[1mStep[0m  [64/169], [94mLoss[0m : 2.39517
[1mStep[0m  [80/169], [94mLoss[0m : 3.42419
[1mStep[0m  [96/169], [94mLoss[0m : 2.27468
[1mStep[0m  [112/169], [94mLoss[0m : 2.52461
[1mStep[0m  [128/169], [94mLoss[0m : 2.91295
[1mStep[0m  [144/169], [94mLoss[0m : 2.29298
[1mStep[0m  [160/169], [94mLoss[0m : 1.96789

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.359, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.61146
[1mStep[0m  [16/169], [94mLoss[0m : 2.88871
[1mStep[0m  [32/169], [94mLoss[0m : 2.66198
[1mStep[0m  [48/169], [94mLoss[0m : 2.64737
[1mStep[0m  [64/169], [94mLoss[0m : 2.73185
[1mStep[0m  [80/169], [94mLoss[0m : 1.84814
[1mStep[0m  [96/169], [94mLoss[0m : 2.57649
[1mStep[0m  [112/169], [94mLoss[0m : 2.24814
[1mStep[0m  [128/169], [94mLoss[0m : 2.54372
[1mStep[0m  [144/169], [94mLoss[0m : 2.57434
[1mStep[0m  [160/169], [94mLoss[0m : 2.53607

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.384, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.73019
[1mStep[0m  [16/169], [94mLoss[0m : 2.27955
[1mStep[0m  [32/169], [94mLoss[0m : 2.25113
[1mStep[0m  [48/169], [94mLoss[0m : 2.30642
[1mStep[0m  [64/169], [94mLoss[0m : 2.45676
[1mStep[0m  [80/169], [94mLoss[0m : 2.72205
[1mStep[0m  [96/169], [94mLoss[0m : 3.12750
[1mStep[0m  [112/169], [94mLoss[0m : 2.26401
[1mStep[0m  [128/169], [94mLoss[0m : 2.31987
[1mStep[0m  [144/169], [94mLoss[0m : 2.31053
[1mStep[0m  [160/169], [94mLoss[0m : 2.20002

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.400, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.61025
[1mStep[0m  [16/169], [94mLoss[0m : 2.50632
[1mStep[0m  [32/169], [94mLoss[0m : 2.52886
[1mStep[0m  [48/169], [94mLoss[0m : 2.36541
[1mStep[0m  [64/169], [94mLoss[0m : 2.47428
[1mStep[0m  [80/169], [94mLoss[0m : 2.69221
[1mStep[0m  [96/169], [94mLoss[0m : 2.53171
[1mStep[0m  [112/169], [94mLoss[0m : 2.55352
[1mStep[0m  [128/169], [94mLoss[0m : 2.78542
[1mStep[0m  [144/169], [94mLoss[0m : 2.31121
[1mStep[0m  [160/169], [94mLoss[0m : 2.36531

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.408, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.12913
[1mStep[0m  [16/169], [94mLoss[0m : 2.40113
[1mStep[0m  [32/169], [94mLoss[0m : 2.38396
[1mStep[0m  [48/169], [94mLoss[0m : 1.77960
[1mStep[0m  [64/169], [94mLoss[0m : 2.39471
[1mStep[0m  [80/169], [94mLoss[0m : 2.55049
[1mStep[0m  [96/169], [94mLoss[0m : 2.24092
[1mStep[0m  [112/169], [94mLoss[0m : 2.40403
[1mStep[0m  [128/169], [94mLoss[0m : 2.29980
[1mStep[0m  [144/169], [94mLoss[0m : 2.22481
[1mStep[0m  [160/169], [94mLoss[0m : 2.64044

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.432, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.88754
[1mStep[0m  [16/169], [94mLoss[0m : 2.28739
[1mStep[0m  [32/169], [94mLoss[0m : 2.36418
[1mStep[0m  [48/169], [94mLoss[0m : 2.60607
[1mStep[0m  [64/169], [94mLoss[0m : 2.43908
[1mStep[0m  [80/169], [94mLoss[0m : 2.67489
[1mStep[0m  [96/169], [94mLoss[0m : 2.49163
[1mStep[0m  [112/169], [94mLoss[0m : 2.62071
[1mStep[0m  [128/169], [94mLoss[0m : 2.32224
[1mStep[0m  [144/169], [94mLoss[0m : 2.60207
[1mStep[0m  [160/169], [94mLoss[0m : 2.83187

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.380, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.89509
[1mStep[0m  [16/169], [94mLoss[0m : 2.37408
[1mStep[0m  [32/169], [94mLoss[0m : 2.61683
[1mStep[0m  [48/169], [94mLoss[0m : 2.50023
[1mStep[0m  [64/169], [94mLoss[0m : 2.43232
[1mStep[0m  [80/169], [94mLoss[0m : 2.40400
[1mStep[0m  [96/169], [94mLoss[0m : 2.86917
[1mStep[0m  [112/169], [94mLoss[0m : 2.67566
[1mStep[0m  [128/169], [94mLoss[0m : 2.07859
[1mStep[0m  [144/169], [94mLoss[0m : 2.32721
[1mStep[0m  [160/169], [94mLoss[0m : 2.72664

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.401, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.01382
[1mStep[0m  [16/169], [94mLoss[0m : 2.12873
[1mStep[0m  [32/169], [94mLoss[0m : 1.97903
[1mStep[0m  [48/169], [94mLoss[0m : 2.13068
[1mStep[0m  [64/169], [94mLoss[0m : 2.55359
[1mStep[0m  [80/169], [94mLoss[0m : 2.27037
[1mStep[0m  [96/169], [94mLoss[0m : 2.11327
[1mStep[0m  [112/169], [94mLoss[0m : 2.28911
[1mStep[0m  [128/169], [94mLoss[0m : 2.19457
[1mStep[0m  [144/169], [94mLoss[0m : 2.59296
[1mStep[0m  [160/169], [94mLoss[0m : 2.13538

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.382, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.87725
[1mStep[0m  [16/169], [94mLoss[0m : 2.31767
[1mStep[0m  [32/169], [94mLoss[0m : 2.72006
[1mStep[0m  [48/169], [94mLoss[0m : 2.37163
[1mStep[0m  [64/169], [94mLoss[0m : 2.09221
[1mStep[0m  [80/169], [94mLoss[0m : 2.09844
[1mStep[0m  [96/169], [94mLoss[0m : 2.44924
[1mStep[0m  [112/169], [94mLoss[0m : 2.27255
[1mStep[0m  [128/169], [94mLoss[0m : 2.68349
[1mStep[0m  [144/169], [94mLoss[0m : 2.59615
[1mStep[0m  [160/169], [94mLoss[0m : 2.47226

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.399, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.89129
[1mStep[0m  [16/169], [94mLoss[0m : 2.33462
[1mStep[0m  [32/169], [94mLoss[0m : 2.14264
[1mStep[0m  [48/169], [94mLoss[0m : 2.56331
[1mStep[0m  [64/169], [94mLoss[0m : 2.86277
[1mStep[0m  [80/169], [94mLoss[0m : 2.37669
[1mStep[0m  [96/169], [94mLoss[0m : 2.72323
[1mStep[0m  [112/169], [94mLoss[0m : 2.06402
[1mStep[0m  [128/169], [94mLoss[0m : 2.70664
[1mStep[0m  [144/169], [94mLoss[0m : 2.72291
[1mStep[0m  [160/169], [94mLoss[0m : 2.49693

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.369, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.71047
[1mStep[0m  [16/169], [94mLoss[0m : 2.89028
[1mStep[0m  [32/169], [94mLoss[0m : 2.36445
[1mStep[0m  [48/169], [94mLoss[0m : 2.22212
[1mStep[0m  [64/169], [94mLoss[0m : 2.54683
[1mStep[0m  [80/169], [94mLoss[0m : 2.33762
[1mStep[0m  [96/169], [94mLoss[0m : 2.88690
[1mStep[0m  [112/169], [94mLoss[0m : 2.27958
[1mStep[0m  [128/169], [94mLoss[0m : 2.42582
[1mStep[0m  [144/169], [94mLoss[0m : 2.88770
[1mStep[0m  [160/169], [94mLoss[0m : 2.12380

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.401, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.86033
[1mStep[0m  [16/169], [94mLoss[0m : 2.78622
[1mStep[0m  [32/169], [94mLoss[0m : 2.94354
[1mStep[0m  [48/169], [94mLoss[0m : 2.60011
[1mStep[0m  [64/169], [94mLoss[0m : 2.95562
[1mStep[0m  [80/169], [94mLoss[0m : 2.72426
[1mStep[0m  [96/169], [94mLoss[0m : 2.27894
[1mStep[0m  [112/169], [94mLoss[0m : 2.54208
[1mStep[0m  [128/169], [94mLoss[0m : 2.46123
[1mStep[0m  [144/169], [94mLoss[0m : 2.52268
[1mStep[0m  [160/169], [94mLoss[0m : 2.83808

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.452, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.31715
[1mStep[0m  [16/169], [94mLoss[0m : 2.60028
[1mStep[0m  [32/169], [94mLoss[0m : 2.62017
[1mStep[0m  [48/169], [94mLoss[0m : 2.07050
[1mStep[0m  [64/169], [94mLoss[0m : 2.55013
[1mStep[0m  [80/169], [94mLoss[0m : 2.61965
[1mStep[0m  [96/169], [94mLoss[0m : 2.86833
[1mStep[0m  [112/169], [94mLoss[0m : 2.80044
[1mStep[0m  [128/169], [94mLoss[0m : 2.50613
[1mStep[0m  [144/169], [94mLoss[0m : 2.79723
[1mStep[0m  [160/169], [94mLoss[0m : 2.67392

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.446, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.374
====================================

Phase 1 - Evaluation MAE:  2.3737454967839375
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/169], [94mLoss[0m : 2.53471
[1mStep[0m  [16/169], [94mLoss[0m : 2.81577
[1mStep[0m  [32/169], [94mLoss[0m : 2.77793
[1mStep[0m  [48/169], [94mLoss[0m : 2.19328
[1mStep[0m  [64/169], [94mLoss[0m : 2.38087
[1mStep[0m  [80/169], [94mLoss[0m : 2.43481
[1mStep[0m  [96/169], [94mLoss[0m : 2.44667
[1mStep[0m  [112/169], [94mLoss[0m : 2.25503
[1mStep[0m  [128/169], [94mLoss[0m : 2.59354
[1mStep[0m  [144/169], [94mLoss[0m : 2.19601
[1mStep[0m  [160/169], [94mLoss[0m : 2.43020

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.371, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.63541
[1mStep[0m  [16/169], [94mLoss[0m : 2.56830
[1mStep[0m  [32/169], [94mLoss[0m : 2.51319
[1mStep[0m  [48/169], [94mLoss[0m : 2.65225
[1mStep[0m  [64/169], [94mLoss[0m : 2.49876
[1mStep[0m  [80/169], [94mLoss[0m : 2.84506
[1mStep[0m  [96/169], [94mLoss[0m : 2.46448
[1mStep[0m  [112/169], [94mLoss[0m : 1.96424
[1mStep[0m  [128/169], [94mLoss[0m : 2.61387
[1mStep[0m  [144/169], [94mLoss[0m : 2.63992
[1mStep[0m  [160/169], [94mLoss[0m : 2.13156

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.396, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.56901
[1mStep[0m  [16/169], [94mLoss[0m : 2.05141
[1mStep[0m  [32/169], [94mLoss[0m : 2.41514
[1mStep[0m  [48/169], [94mLoss[0m : 2.57979
[1mStep[0m  [64/169], [94mLoss[0m : 2.86743
[1mStep[0m  [80/169], [94mLoss[0m : 2.20002
[1mStep[0m  [96/169], [94mLoss[0m : 2.67750
[1mStep[0m  [112/169], [94mLoss[0m : 2.20671
[1mStep[0m  [128/169], [94mLoss[0m : 2.53149
[1mStep[0m  [144/169], [94mLoss[0m : 2.27136
[1mStep[0m  [160/169], [94mLoss[0m : 3.03747

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.384, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.01921
[1mStep[0m  [16/169], [94mLoss[0m : 1.69885
[1mStep[0m  [32/169], [94mLoss[0m : 2.51941
[1mStep[0m  [48/169], [94mLoss[0m : 2.25205
[1mStep[0m  [64/169], [94mLoss[0m : 2.38106
[1mStep[0m  [80/169], [94mLoss[0m : 2.53238
[1mStep[0m  [96/169], [94mLoss[0m : 1.83729
[1mStep[0m  [112/169], [94mLoss[0m : 2.97421
[1mStep[0m  [128/169], [94mLoss[0m : 2.49496
[1mStep[0m  [144/169], [94mLoss[0m : 2.61169
[1mStep[0m  [160/169], [94mLoss[0m : 2.28304

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.330, [92mTest[0m: 2.385, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.53800
[1mStep[0m  [16/169], [94mLoss[0m : 2.26675
[1mStep[0m  [32/169], [94mLoss[0m : 2.46839
[1mStep[0m  [48/169], [94mLoss[0m : 2.45675
[1mStep[0m  [64/169], [94mLoss[0m : 2.10969
[1mStep[0m  [80/169], [94mLoss[0m : 2.25633
[1mStep[0m  [96/169], [94mLoss[0m : 2.15357
[1mStep[0m  [112/169], [94mLoss[0m : 2.12559
[1mStep[0m  [128/169], [94mLoss[0m : 2.36680
[1mStep[0m  [144/169], [94mLoss[0m : 2.29380
[1mStep[0m  [160/169], [94mLoss[0m : 2.33606

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.276, [92mTest[0m: 2.397, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.85983
[1mStep[0m  [16/169], [94mLoss[0m : 1.96380
[1mStep[0m  [32/169], [94mLoss[0m : 2.26688
[1mStep[0m  [48/169], [94mLoss[0m : 2.18313
[1mStep[0m  [64/169], [94mLoss[0m : 2.09299
[1mStep[0m  [80/169], [94mLoss[0m : 2.14794
[1mStep[0m  [96/169], [94mLoss[0m : 2.23522
[1mStep[0m  [112/169], [94mLoss[0m : 2.40209
[1mStep[0m  [128/169], [94mLoss[0m : 2.67342
[1mStep[0m  [144/169], [94mLoss[0m : 2.79355
[1mStep[0m  [160/169], [94mLoss[0m : 2.04545

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.268, [92mTest[0m: 2.406, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.47396
[1mStep[0m  [16/169], [94mLoss[0m : 1.93677
[1mStep[0m  [32/169], [94mLoss[0m : 2.18229
[1mStep[0m  [48/169], [94mLoss[0m : 2.27020
[1mStep[0m  [64/169], [94mLoss[0m : 2.35921
[1mStep[0m  [80/169], [94mLoss[0m : 2.15694
[1mStep[0m  [96/169], [94mLoss[0m : 2.50797
[1mStep[0m  [112/169], [94mLoss[0m : 2.25394
[1mStep[0m  [128/169], [94mLoss[0m : 1.94565
[1mStep[0m  [144/169], [94mLoss[0m : 2.39006
[1mStep[0m  [160/169], [94mLoss[0m : 2.65191

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.236, [92mTest[0m: 2.409, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.00228
[1mStep[0m  [16/169], [94mLoss[0m : 2.14248
[1mStep[0m  [32/169], [94mLoss[0m : 1.84883
[1mStep[0m  [48/169], [94mLoss[0m : 2.54316
[1mStep[0m  [64/169], [94mLoss[0m : 2.17028
[1mStep[0m  [80/169], [94mLoss[0m : 2.28710
[1mStep[0m  [96/169], [94mLoss[0m : 2.26129
[1mStep[0m  [112/169], [94mLoss[0m : 2.21845
[1mStep[0m  [128/169], [94mLoss[0m : 2.25227
[1mStep[0m  [144/169], [94mLoss[0m : 2.26711
[1mStep[0m  [160/169], [94mLoss[0m : 2.28217

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.222, [92mTest[0m: 2.435, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.06744
[1mStep[0m  [16/169], [94mLoss[0m : 1.97266
[1mStep[0m  [32/169], [94mLoss[0m : 2.00642
[1mStep[0m  [48/169], [94mLoss[0m : 2.18030
[1mStep[0m  [64/169], [94mLoss[0m : 2.10452
[1mStep[0m  [80/169], [94mLoss[0m : 2.16735
[1mStep[0m  [96/169], [94mLoss[0m : 2.79556
[1mStep[0m  [112/169], [94mLoss[0m : 2.01144
[1mStep[0m  [128/169], [94mLoss[0m : 1.96632
[1mStep[0m  [144/169], [94mLoss[0m : 2.06377
[1mStep[0m  [160/169], [94mLoss[0m : 2.59145

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.192, [92mTest[0m: 2.520, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.78172
[1mStep[0m  [16/169], [94mLoss[0m : 2.18730
[1mStep[0m  [32/169], [94mLoss[0m : 2.84293
[1mStep[0m  [48/169], [94mLoss[0m : 2.44570
[1mStep[0m  [64/169], [94mLoss[0m : 1.92943
[1mStep[0m  [80/169], [94mLoss[0m : 2.62259
[1mStep[0m  [96/169], [94mLoss[0m : 1.92244
[1mStep[0m  [112/169], [94mLoss[0m : 2.07904
[1mStep[0m  [128/169], [94mLoss[0m : 2.69577
[1mStep[0m  [144/169], [94mLoss[0m : 2.25903
[1mStep[0m  [160/169], [94mLoss[0m : 2.39236

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.180, [92mTest[0m: 2.409, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.90920
[1mStep[0m  [16/169], [94mLoss[0m : 2.38888
[1mStep[0m  [32/169], [94mLoss[0m : 2.15514
[1mStep[0m  [48/169], [94mLoss[0m : 2.07855
[1mStep[0m  [64/169], [94mLoss[0m : 2.09431
[1mStep[0m  [80/169], [94mLoss[0m : 1.72746
[1mStep[0m  [96/169], [94mLoss[0m : 2.07205
[1mStep[0m  [112/169], [94mLoss[0m : 1.95501
[1mStep[0m  [128/169], [94mLoss[0m : 2.24446
[1mStep[0m  [144/169], [94mLoss[0m : 1.93942
[1mStep[0m  [160/169], [94mLoss[0m : 1.94608

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.147, [92mTest[0m: 2.427, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.01407
[1mStep[0m  [16/169], [94mLoss[0m : 2.02073
[1mStep[0m  [32/169], [94mLoss[0m : 2.09211
[1mStep[0m  [48/169], [94mLoss[0m : 2.48153
[1mStep[0m  [64/169], [94mLoss[0m : 2.05306
[1mStep[0m  [80/169], [94mLoss[0m : 2.27128
[1mStep[0m  [96/169], [94mLoss[0m : 2.47234
[1mStep[0m  [112/169], [94mLoss[0m : 2.14462
[1mStep[0m  [128/169], [94mLoss[0m : 2.05755
[1mStep[0m  [144/169], [94mLoss[0m : 2.48864
[1mStep[0m  [160/169], [94mLoss[0m : 2.36975

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.160, [92mTest[0m: 2.428, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.89476
[1mStep[0m  [16/169], [94mLoss[0m : 2.28709
[1mStep[0m  [32/169], [94mLoss[0m : 2.06367
[1mStep[0m  [48/169], [94mLoss[0m : 2.14833
[1mStep[0m  [64/169], [94mLoss[0m : 2.12871
[1mStep[0m  [80/169], [94mLoss[0m : 2.41813
[1mStep[0m  [96/169], [94mLoss[0m : 2.01190
[1mStep[0m  [112/169], [94mLoss[0m : 2.12602
[1mStep[0m  [128/169], [94mLoss[0m : 1.94649
[1mStep[0m  [144/169], [94mLoss[0m : 2.08073
[1mStep[0m  [160/169], [94mLoss[0m : 1.91940

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.134, [92mTest[0m: 2.430, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.89018
[1mStep[0m  [16/169], [94mLoss[0m : 1.90904
[1mStep[0m  [32/169], [94mLoss[0m : 2.05008
[1mStep[0m  [48/169], [94mLoss[0m : 2.18477
[1mStep[0m  [64/169], [94mLoss[0m : 2.16296
[1mStep[0m  [80/169], [94mLoss[0m : 2.20040
[1mStep[0m  [96/169], [94mLoss[0m : 2.13845
[1mStep[0m  [112/169], [94mLoss[0m : 2.55544
[1mStep[0m  [128/169], [94mLoss[0m : 2.48527
[1mStep[0m  [144/169], [94mLoss[0m : 2.01316
[1mStep[0m  [160/169], [94mLoss[0m : 2.17318

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.134, [92mTest[0m: 2.425, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.06163
[1mStep[0m  [16/169], [94mLoss[0m : 2.02168
[1mStep[0m  [32/169], [94mLoss[0m : 2.50539
[1mStep[0m  [48/169], [94mLoss[0m : 1.75623
[1mStep[0m  [64/169], [94mLoss[0m : 2.48019
[1mStep[0m  [80/169], [94mLoss[0m : 2.19094
[1mStep[0m  [96/169], [94mLoss[0m : 1.95143
[1mStep[0m  [112/169], [94mLoss[0m : 2.40038
[1mStep[0m  [128/169], [94mLoss[0m : 1.78602
[1mStep[0m  [144/169], [94mLoss[0m : 2.34882
[1mStep[0m  [160/169], [94mLoss[0m : 2.01084

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.134, [92mTest[0m: 2.428, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.19947
[1mStep[0m  [16/169], [94mLoss[0m : 1.80310
[1mStep[0m  [32/169], [94mLoss[0m : 2.13990
[1mStep[0m  [48/169], [94mLoss[0m : 2.02503
[1mStep[0m  [64/169], [94mLoss[0m : 2.27816
[1mStep[0m  [80/169], [94mLoss[0m : 1.74581
[1mStep[0m  [96/169], [94mLoss[0m : 1.77378
[1mStep[0m  [112/169], [94mLoss[0m : 1.93846
[1mStep[0m  [128/169], [94mLoss[0m : 2.29893
[1mStep[0m  [144/169], [94mLoss[0m : 2.25154
[1mStep[0m  [160/169], [94mLoss[0m : 2.52184

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.113, [92mTest[0m: 2.442, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.74606
[1mStep[0m  [16/169], [94mLoss[0m : 2.01731
[1mStep[0m  [32/169], [94mLoss[0m : 2.08929
[1mStep[0m  [48/169], [94mLoss[0m : 2.01777
[1mStep[0m  [64/169], [94mLoss[0m : 1.56885
[1mStep[0m  [80/169], [94mLoss[0m : 2.06099
[1mStep[0m  [96/169], [94mLoss[0m : 1.97137
[1mStep[0m  [112/169], [94mLoss[0m : 2.38646
[1mStep[0m  [128/169], [94mLoss[0m : 2.37896
[1mStep[0m  [144/169], [94mLoss[0m : 2.23577
[1mStep[0m  [160/169], [94mLoss[0m : 1.78608

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.083, [92mTest[0m: 2.442, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.01937
[1mStep[0m  [16/169], [94mLoss[0m : 2.20650
[1mStep[0m  [32/169], [94mLoss[0m : 2.03676
[1mStep[0m  [48/169], [94mLoss[0m : 2.06549
[1mStep[0m  [64/169], [94mLoss[0m : 1.91902
[1mStep[0m  [80/169], [94mLoss[0m : 1.99576
[1mStep[0m  [96/169], [94mLoss[0m : 2.10955
[1mStep[0m  [112/169], [94mLoss[0m : 1.80836
[1mStep[0m  [128/169], [94mLoss[0m : 2.06760
[1mStep[0m  [144/169], [94mLoss[0m : 2.00096
[1mStep[0m  [160/169], [94mLoss[0m : 1.99860

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.072, [92mTest[0m: 2.488, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.84271
[1mStep[0m  [16/169], [94mLoss[0m : 1.91619
[1mStep[0m  [32/169], [94mLoss[0m : 2.06560
[1mStep[0m  [48/169], [94mLoss[0m : 2.24534
[1mStep[0m  [64/169], [94mLoss[0m : 2.11157
[1mStep[0m  [80/169], [94mLoss[0m : 1.94300
[1mStep[0m  [96/169], [94mLoss[0m : 1.53766
[1mStep[0m  [112/169], [94mLoss[0m : 2.06866
[1mStep[0m  [128/169], [94mLoss[0m : 2.31163
[1mStep[0m  [144/169], [94mLoss[0m : 2.52949
[1mStep[0m  [160/169], [94mLoss[0m : 1.99508

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.051, [92mTest[0m: 2.479, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.03698
[1mStep[0m  [16/169], [94mLoss[0m : 2.13690
[1mStep[0m  [32/169], [94mLoss[0m : 2.33390
[1mStep[0m  [48/169], [94mLoss[0m : 2.15300
[1mStep[0m  [64/169], [94mLoss[0m : 1.72818
[1mStep[0m  [80/169], [94mLoss[0m : 2.42630
[1mStep[0m  [96/169], [94mLoss[0m : 1.92304
[1mStep[0m  [112/169], [94mLoss[0m : 2.10831
[1mStep[0m  [128/169], [94mLoss[0m : 2.00913
[1mStep[0m  [144/169], [94mLoss[0m : 2.03471
[1mStep[0m  [160/169], [94mLoss[0m : 2.38835

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.082, [92mTest[0m: 2.433, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.01005
[1mStep[0m  [16/169], [94mLoss[0m : 2.20463
[1mStep[0m  [32/169], [94mLoss[0m : 1.83066
[1mStep[0m  [48/169], [94mLoss[0m : 2.22240
[1mStep[0m  [64/169], [94mLoss[0m : 2.23841
[1mStep[0m  [80/169], [94mLoss[0m : 2.08132
[1mStep[0m  [96/169], [94mLoss[0m : 2.20646
[1mStep[0m  [112/169], [94mLoss[0m : 2.06895
[1mStep[0m  [128/169], [94mLoss[0m : 2.14913
[1mStep[0m  [144/169], [94mLoss[0m : 1.80880
[1mStep[0m  [160/169], [94mLoss[0m : 2.19874

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.009, [92mTest[0m: 2.488, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.84751
[1mStep[0m  [16/169], [94mLoss[0m : 1.89797
[1mStep[0m  [32/169], [94mLoss[0m : 1.93233
[1mStep[0m  [48/169], [94mLoss[0m : 2.14367
[1mStep[0m  [64/169], [94mLoss[0m : 1.85939
[1mStep[0m  [80/169], [94mLoss[0m : 1.76105
[1mStep[0m  [96/169], [94mLoss[0m : 1.78491
[1mStep[0m  [112/169], [94mLoss[0m : 1.88515
[1mStep[0m  [128/169], [94mLoss[0m : 2.41092
[1mStep[0m  [144/169], [94mLoss[0m : 2.28462
[1mStep[0m  [160/169], [94mLoss[0m : 2.06021

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.994, [92mTest[0m: 2.444, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.79562
[1mStep[0m  [16/169], [94mLoss[0m : 2.51106
[1mStep[0m  [32/169], [94mLoss[0m : 1.96484
[1mStep[0m  [48/169], [94mLoss[0m : 2.06355
[1mStep[0m  [64/169], [94mLoss[0m : 1.86676
[1mStep[0m  [80/169], [94mLoss[0m : 2.41445
[1mStep[0m  [96/169], [94mLoss[0m : 2.31589
[1mStep[0m  [112/169], [94mLoss[0m : 2.00535
[1mStep[0m  [128/169], [94mLoss[0m : 1.55922
[1mStep[0m  [144/169], [94mLoss[0m : 2.00689
[1mStep[0m  [160/169], [94mLoss[0m : 2.19405

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.997, [92mTest[0m: 2.470, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.17030
[1mStep[0m  [16/169], [94mLoss[0m : 1.80828
[1mStep[0m  [32/169], [94mLoss[0m : 1.74769
[1mStep[0m  [48/169], [94mLoss[0m : 1.88480
[1mStep[0m  [64/169], [94mLoss[0m : 2.05389
[1mStep[0m  [80/169], [94mLoss[0m : 2.19622
[1mStep[0m  [96/169], [94mLoss[0m : 1.87510
[1mStep[0m  [112/169], [94mLoss[0m : 2.04288
[1mStep[0m  [128/169], [94mLoss[0m : 1.86909
[1mStep[0m  [144/169], [94mLoss[0m : 1.97137
[1mStep[0m  [160/169], [94mLoss[0m : 1.85620

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.989, [92mTest[0m: 2.422, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.94903
[1mStep[0m  [16/169], [94mLoss[0m : 1.66399
[1mStep[0m  [32/169], [94mLoss[0m : 1.80521
[1mStep[0m  [48/169], [94mLoss[0m : 1.80039
[1mStep[0m  [64/169], [94mLoss[0m : 1.82214
[1mStep[0m  [80/169], [94mLoss[0m : 2.20061
[1mStep[0m  [96/169], [94mLoss[0m : 2.01353
[1mStep[0m  [112/169], [94mLoss[0m : 2.29276
[1mStep[0m  [128/169], [94mLoss[0m : 2.14446
[1mStep[0m  [144/169], [94mLoss[0m : 2.08696
[1mStep[0m  [160/169], [94mLoss[0m : 1.70811

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.984, [92mTest[0m: 2.461, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.68039
[1mStep[0m  [16/169], [94mLoss[0m : 2.03333
[1mStep[0m  [32/169], [94mLoss[0m : 2.30229
[1mStep[0m  [48/169], [94mLoss[0m : 1.90643
[1mStep[0m  [64/169], [94mLoss[0m : 2.07240
[1mStep[0m  [80/169], [94mLoss[0m : 1.76486
[1mStep[0m  [96/169], [94mLoss[0m : 2.25198
[1mStep[0m  [112/169], [94mLoss[0m : 1.95260
[1mStep[0m  [128/169], [94mLoss[0m : 2.29476
[1mStep[0m  [144/169], [94mLoss[0m : 2.15289
[1mStep[0m  [160/169], [94mLoss[0m : 1.68745

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.952, [92mTest[0m: 2.508, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.90974
[1mStep[0m  [16/169], [94mLoss[0m : 2.19170
[1mStep[0m  [32/169], [94mLoss[0m : 2.37565
[1mStep[0m  [48/169], [94mLoss[0m : 1.92058
[1mStep[0m  [64/169], [94mLoss[0m : 2.22119
[1mStep[0m  [80/169], [94mLoss[0m : 1.83580
[1mStep[0m  [96/169], [94mLoss[0m : 1.90990
[1mStep[0m  [112/169], [94mLoss[0m : 2.18967
[1mStep[0m  [128/169], [94mLoss[0m : 2.20060
[1mStep[0m  [144/169], [94mLoss[0m : 1.81900
[1mStep[0m  [160/169], [94mLoss[0m : 2.08693

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.965, [92mTest[0m: 2.475, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.62396
[1mStep[0m  [16/169], [94mLoss[0m : 2.22932
[1mStep[0m  [32/169], [94mLoss[0m : 1.70852
[1mStep[0m  [48/169], [94mLoss[0m : 1.63053
[1mStep[0m  [64/169], [94mLoss[0m : 2.14374
[1mStep[0m  [80/169], [94mLoss[0m : 1.71649
[1mStep[0m  [96/169], [94mLoss[0m : 2.73433
[1mStep[0m  [112/169], [94mLoss[0m : 1.86712
[1mStep[0m  [128/169], [94mLoss[0m : 1.96613
[1mStep[0m  [144/169], [94mLoss[0m : 2.13814
[1mStep[0m  [160/169], [94mLoss[0m : 2.14990

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.964, [92mTest[0m: 2.460, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.99484
[1mStep[0m  [16/169], [94mLoss[0m : 2.00684
[1mStep[0m  [32/169], [94mLoss[0m : 1.58067
[1mStep[0m  [48/169], [94mLoss[0m : 2.00187
[1mStep[0m  [64/169], [94mLoss[0m : 1.77976
[1mStep[0m  [80/169], [94mLoss[0m : 2.62971
[1mStep[0m  [96/169], [94mLoss[0m : 2.08857
[1mStep[0m  [112/169], [94mLoss[0m : 1.91190
[1mStep[0m  [128/169], [94mLoss[0m : 1.91344
[1mStep[0m  [144/169], [94mLoss[0m : 1.92291
[1mStep[0m  [160/169], [94mLoss[0m : 1.89298

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.943, [92mTest[0m: 2.465, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.08515
[1mStep[0m  [16/169], [94mLoss[0m : 2.04211
[1mStep[0m  [32/169], [94mLoss[0m : 1.96041
[1mStep[0m  [48/169], [94mLoss[0m : 2.11210
[1mStep[0m  [64/169], [94mLoss[0m : 1.96374
[1mStep[0m  [80/169], [94mLoss[0m : 1.98950
[1mStep[0m  [96/169], [94mLoss[0m : 1.79194
[1mStep[0m  [112/169], [94mLoss[0m : 1.84034
[1mStep[0m  [128/169], [94mLoss[0m : 1.83620
[1mStep[0m  [144/169], [94mLoss[0m : 1.52685
[1mStep[0m  [160/169], [94mLoss[0m : 1.84481

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.964, [92mTest[0m: 2.509, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.463
====================================

Phase 2 - Evaluation MAE:  2.4632043625627245
MAE score P1      2.373745
MAE score P2      2.463204
loss              1.943235
learning_rate      0.00505
batch_size              64
hidden_sizes         [250]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.4
momentum               0.9
weight_decay          0.01
Name: 12, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/169], [94mLoss[0m : 11.15396
[1mStep[0m  [16/169], [94mLoss[0m : 9.93946
[1mStep[0m  [32/169], [94mLoss[0m : 9.10436
[1mStep[0m  [48/169], [94mLoss[0m : 7.32937
[1mStep[0m  [64/169], [94mLoss[0m : 5.45559
[1mStep[0m  [80/169], [94mLoss[0m : 5.16971
[1mStep[0m  [96/169], [94mLoss[0m : 3.56114
[1mStep[0m  [112/169], [94mLoss[0m : 2.57157
[1mStep[0m  [128/169], [94mLoss[0m : 2.64396
[1mStep[0m  [144/169], [94mLoss[0m : 2.81802
[1mStep[0m  [160/169], [94mLoss[0m : 2.62797

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 5.579, [92mTest[0m: 10.796, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.65870
[1mStep[0m  [16/169], [94mLoss[0m : 3.03725
[1mStep[0m  [32/169], [94mLoss[0m : 2.93635
[1mStep[0m  [48/169], [94mLoss[0m : 2.98429
[1mStep[0m  [64/169], [94mLoss[0m : 2.60170
[1mStep[0m  [80/169], [94mLoss[0m : 3.08957
[1mStep[0m  [96/169], [94mLoss[0m : 3.28266
[1mStep[0m  [112/169], [94mLoss[0m : 2.78492
[1mStep[0m  [128/169], [94mLoss[0m : 2.22080
[1mStep[0m  [144/169], [94mLoss[0m : 3.04538
[1mStep[0m  [160/169], [94mLoss[0m : 2.65250

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.672, [92mTest[0m: 2.591, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.43263
[1mStep[0m  [16/169], [94mLoss[0m : 2.68840
[1mStep[0m  [32/169], [94mLoss[0m : 2.59656
[1mStep[0m  [48/169], [94mLoss[0m : 2.73744
[1mStep[0m  [64/169], [94mLoss[0m : 2.94085
[1mStep[0m  [80/169], [94mLoss[0m : 1.87915
[1mStep[0m  [96/169], [94mLoss[0m : 2.92347
[1mStep[0m  [112/169], [94mLoss[0m : 2.19117
[1mStep[0m  [128/169], [94mLoss[0m : 2.71240
[1mStep[0m  [144/169], [94mLoss[0m : 2.18641
[1mStep[0m  [160/169], [94mLoss[0m : 2.93216

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.566, [92mTest[0m: 2.398, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.68749
[1mStep[0m  [16/169], [94mLoss[0m : 2.48016
[1mStep[0m  [32/169], [94mLoss[0m : 2.45748
[1mStep[0m  [48/169], [94mLoss[0m : 2.55134
[1mStep[0m  [64/169], [94mLoss[0m : 2.43970
[1mStep[0m  [80/169], [94mLoss[0m : 2.51417
[1mStep[0m  [96/169], [94mLoss[0m : 2.66991
[1mStep[0m  [112/169], [94mLoss[0m : 2.72352
[1mStep[0m  [128/169], [94mLoss[0m : 2.44981
[1mStep[0m  [144/169], [94mLoss[0m : 2.39640
[1mStep[0m  [160/169], [94mLoss[0m : 2.47471

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.381, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.74259
[1mStep[0m  [16/169], [94mLoss[0m : 2.99440
[1mStep[0m  [32/169], [94mLoss[0m : 1.80756
[1mStep[0m  [48/169], [94mLoss[0m : 2.33849
[1mStep[0m  [64/169], [94mLoss[0m : 2.42075
[1mStep[0m  [80/169], [94mLoss[0m : 2.51022
[1mStep[0m  [96/169], [94mLoss[0m : 1.96127
[1mStep[0m  [112/169], [94mLoss[0m : 2.57653
[1mStep[0m  [128/169], [94mLoss[0m : 2.64497
[1mStep[0m  [144/169], [94mLoss[0m : 2.76834
[1mStep[0m  [160/169], [94mLoss[0m : 2.76724

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.402, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.56685
[1mStep[0m  [16/169], [94mLoss[0m : 2.57228
[1mStep[0m  [32/169], [94mLoss[0m : 2.20289
[1mStep[0m  [48/169], [94mLoss[0m : 2.21739
[1mStep[0m  [64/169], [94mLoss[0m : 2.41083
[1mStep[0m  [80/169], [94mLoss[0m : 2.07989
[1mStep[0m  [96/169], [94mLoss[0m : 2.82788
[1mStep[0m  [112/169], [94mLoss[0m : 2.76375
[1mStep[0m  [128/169], [94mLoss[0m : 2.67846
[1mStep[0m  [144/169], [94mLoss[0m : 2.36942
[1mStep[0m  [160/169], [94mLoss[0m : 2.62072

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.396, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.43882
[1mStep[0m  [16/169], [94mLoss[0m : 2.02097
[1mStep[0m  [32/169], [94mLoss[0m : 2.67652
[1mStep[0m  [48/169], [94mLoss[0m : 2.56257
[1mStep[0m  [64/169], [94mLoss[0m : 2.77401
[1mStep[0m  [80/169], [94mLoss[0m : 2.35516
[1mStep[0m  [96/169], [94mLoss[0m : 2.42585
[1mStep[0m  [112/169], [94mLoss[0m : 2.94197
[1mStep[0m  [128/169], [94mLoss[0m : 2.57087
[1mStep[0m  [144/169], [94mLoss[0m : 2.92768
[1mStep[0m  [160/169], [94mLoss[0m : 2.67919

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.367, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.52751
[1mStep[0m  [16/169], [94mLoss[0m : 2.52834
[1mStep[0m  [32/169], [94mLoss[0m : 2.67507
[1mStep[0m  [48/169], [94mLoss[0m : 2.35756
[1mStep[0m  [64/169], [94mLoss[0m : 2.05441
[1mStep[0m  [80/169], [94mLoss[0m : 2.28941
[1mStep[0m  [96/169], [94mLoss[0m : 2.47056
[1mStep[0m  [112/169], [94mLoss[0m : 2.35825
[1mStep[0m  [128/169], [94mLoss[0m : 2.50764
[1mStep[0m  [144/169], [94mLoss[0m : 2.87196
[1mStep[0m  [160/169], [94mLoss[0m : 2.14863

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.394, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.41742
[1mStep[0m  [16/169], [94mLoss[0m : 2.45316
[1mStep[0m  [32/169], [94mLoss[0m : 2.60356
[1mStep[0m  [48/169], [94mLoss[0m : 2.53561
[1mStep[0m  [64/169], [94mLoss[0m : 2.60469
[1mStep[0m  [80/169], [94mLoss[0m : 2.61994
[1mStep[0m  [96/169], [94mLoss[0m : 2.34988
[1mStep[0m  [112/169], [94mLoss[0m : 2.60985
[1mStep[0m  [128/169], [94mLoss[0m : 2.35126
[1mStep[0m  [144/169], [94mLoss[0m : 2.19285
[1mStep[0m  [160/169], [94mLoss[0m : 2.46330

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.373, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.51053
[1mStep[0m  [16/169], [94mLoss[0m : 1.96332
[1mStep[0m  [32/169], [94mLoss[0m : 2.62138
[1mStep[0m  [48/169], [94mLoss[0m : 2.19696
[1mStep[0m  [64/169], [94mLoss[0m : 2.22166
[1mStep[0m  [80/169], [94mLoss[0m : 2.24310
[1mStep[0m  [96/169], [94mLoss[0m : 2.75856
[1mStep[0m  [112/169], [94mLoss[0m : 2.45941
[1mStep[0m  [128/169], [94mLoss[0m : 2.39279
[1mStep[0m  [144/169], [94mLoss[0m : 2.26108
[1mStep[0m  [160/169], [94mLoss[0m : 2.76420

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.363, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.03872
[1mStep[0m  [16/169], [94mLoss[0m : 2.67211
[1mStep[0m  [32/169], [94mLoss[0m : 2.18218
[1mStep[0m  [48/169], [94mLoss[0m : 2.17098
[1mStep[0m  [64/169], [94mLoss[0m : 2.38721
[1mStep[0m  [80/169], [94mLoss[0m : 2.51881
[1mStep[0m  [96/169], [94mLoss[0m : 2.41284
[1mStep[0m  [112/169], [94mLoss[0m : 2.59239
[1mStep[0m  [128/169], [94mLoss[0m : 2.47734
[1mStep[0m  [144/169], [94mLoss[0m : 2.13342
[1mStep[0m  [160/169], [94mLoss[0m : 2.51394

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.351, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.52891
[1mStep[0m  [16/169], [94mLoss[0m : 2.31934
[1mStep[0m  [32/169], [94mLoss[0m : 2.42887
[1mStep[0m  [48/169], [94mLoss[0m : 2.42901
[1mStep[0m  [64/169], [94mLoss[0m : 2.05760
[1mStep[0m  [80/169], [94mLoss[0m : 2.41460
[1mStep[0m  [96/169], [94mLoss[0m : 2.70269
[1mStep[0m  [112/169], [94mLoss[0m : 2.55349
[1mStep[0m  [128/169], [94mLoss[0m : 2.40747
[1mStep[0m  [144/169], [94mLoss[0m : 2.43084
[1mStep[0m  [160/169], [94mLoss[0m : 2.44862

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.352, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.23183
[1mStep[0m  [16/169], [94mLoss[0m : 2.76698
[1mStep[0m  [32/169], [94mLoss[0m : 1.85181
[1mStep[0m  [48/169], [94mLoss[0m : 2.34250
[1mStep[0m  [64/169], [94mLoss[0m : 2.60779
[1mStep[0m  [80/169], [94mLoss[0m : 2.73100
[1mStep[0m  [96/169], [94mLoss[0m : 2.86068
[1mStep[0m  [112/169], [94mLoss[0m : 2.08434
[1mStep[0m  [128/169], [94mLoss[0m : 2.61849
[1mStep[0m  [144/169], [94mLoss[0m : 2.72591
[1mStep[0m  [160/169], [94mLoss[0m : 2.84471

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.353, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.57161
[1mStep[0m  [16/169], [94mLoss[0m : 2.33261
[1mStep[0m  [32/169], [94mLoss[0m : 2.63156
[1mStep[0m  [48/169], [94mLoss[0m : 2.23648
[1mStep[0m  [64/169], [94mLoss[0m : 1.93526
[1mStep[0m  [80/169], [94mLoss[0m : 2.25261
[1mStep[0m  [96/169], [94mLoss[0m : 2.25579
[1mStep[0m  [112/169], [94mLoss[0m : 2.29429
[1mStep[0m  [128/169], [94mLoss[0m : 2.41370
[1mStep[0m  [144/169], [94mLoss[0m : 2.22753
[1mStep[0m  [160/169], [94mLoss[0m : 2.69503

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.364, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.86127
[1mStep[0m  [16/169], [94mLoss[0m : 2.20936
[1mStep[0m  [32/169], [94mLoss[0m : 2.38062
[1mStep[0m  [48/169], [94mLoss[0m : 2.65948
[1mStep[0m  [64/169], [94mLoss[0m : 2.90328
[1mStep[0m  [80/169], [94mLoss[0m : 2.19507
[1mStep[0m  [96/169], [94mLoss[0m : 2.11507
[1mStep[0m  [112/169], [94mLoss[0m : 2.20981
[1mStep[0m  [128/169], [94mLoss[0m : 2.49140
[1mStep[0m  [144/169], [94mLoss[0m : 2.78423
[1mStep[0m  [160/169], [94mLoss[0m : 2.90218

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.401, [92mTest[0m: 2.402, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.70325
[1mStep[0m  [16/169], [94mLoss[0m : 2.62311
[1mStep[0m  [32/169], [94mLoss[0m : 2.56688
[1mStep[0m  [48/169], [94mLoss[0m : 2.44143
[1mStep[0m  [64/169], [94mLoss[0m : 2.04096
[1mStep[0m  [80/169], [94mLoss[0m : 2.34683
[1mStep[0m  [96/169], [94mLoss[0m : 2.90914
[1mStep[0m  [112/169], [94mLoss[0m : 2.35089
[1mStep[0m  [128/169], [94mLoss[0m : 2.49593
[1mStep[0m  [144/169], [94mLoss[0m : 2.26386
[1mStep[0m  [160/169], [94mLoss[0m : 2.60081

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.333, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.27207
[1mStep[0m  [16/169], [94mLoss[0m : 2.30093
[1mStep[0m  [32/169], [94mLoss[0m : 2.30384
[1mStep[0m  [48/169], [94mLoss[0m : 2.68437
[1mStep[0m  [64/169], [94mLoss[0m : 2.00712
[1mStep[0m  [80/169], [94mLoss[0m : 3.00195
[1mStep[0m  [96/169], [94mLoss[0m : 2.15626
[1mStep[0m  [112/169], [94mLoss[0m : 2.73699
[1mStep[0m  [128/169], [94mLoss[0m : 2.81532
[1mStep[0m  [144/169], [94mLoss[0m : 2.44625
[1mStep[0m  [160/169], [94mLoss[0m : 2.21073

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.403, [92mTest[0m: 2.379, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.53711
[1mStep[0m  [16/169], [94mLoss[0m : 2.56982
[1mStep[0m  [32/169], [94mLoss[0m : 2.19869
[1mStep[0m  [48/169], [94mLoss[0m : 2.19026
[1mStep[0m  [64/169], [94mLoss[0m : 2.13385
[1mStep[0m  [80/169], [94mLoss[0m : 2.22781
[1mStep[0m  [96/169], [94mLoss[0m : 2.13641
[1mStep[0m  [112/169], [94mLoss[0m : 2.03031
[1mStep[0m  [128/169], [94mLoss[0m : 2.49446
[1mStep[0m  [144/169], [94mLoss[0m : 2.13737
[1mStep[0m  [160/169], [94mLoss[0m : 2.45805

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.350, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.26111
[1mStep[0m  [16/169], [94mLoss[0m : 2.22262
[1mStep[0m  [32/169], [94mLoss[0m : 2.11104
[1mStep[0m  [48/169], [94mLoss[0m : 2.49696
[1mStep[0m  [64/169], [94mLoss[0m : 2.37051
[1mStep[0m  [80/169], [94mLoss[0m : 2.39110
[1mStep[0m  [96/169], [94mLoss[0m : 2.61057
[1mStep[0m  [112/169], [94mLoss[0m : 2.69701
[1mStep[0m  [128/169], [94mLoss[0m : 2.80550
[1mStep[0m  [144/169], [94mLoss[0m : 2.54729
[1mStep[0m  [160/169], [94mLoss[0m : 1.87417

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.358, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.49818
[1mStep[0m  [16/169], [94mLoss[0m : 2.73552
[1mStep[0m  [32/169], [94mLoss[0m : 2.57553
[1mStep[0m  [48/169], [94mLoss[0m : 2.01760
[1mStep[0m  [64/169], [94mLoss[0m : 1.80444
[1mStep[0m  [80/169], [94mLoss[0m : 2.33582
[1mStep[0m  [96/169], [94mLoss[0m : 2.82216
[1mStep[0m  [112/169], [94mLoss[0m : 2.50154
[1mStep[0m  [128/169], [94mLoss[0m : 2.70002
[1mStep[0m  [144/169], [94mLoss[0m : 2.53124
[1mStep[0m  [160/169], [94mLoss[0m : 2.22797

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.368, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.87877
[1mStep[0m  [16/169], [94mLoss[0m : 2.90143
[1mStep[0m  [32/169], [94mLoss[0m : 2.12298
[1mStep[0m  [48/169], [94mLoss[0m : 2.98183
[1mStep[0m  [64/169], [94mLoss[0m : 2.56412
[1mStep[0m  [80/169], [94mLoss[0m : 2.54535
[1mStep[0m  [96/169], [94mLoss[0m : 2.12206
[1mStep[0m  [112/169], [94mLoss[0m : 2.32818
[1mStep[0m  [128/169], [94mLoss[0m : 2.34874
[1mStep[0m  [144/169], [94mLoss[0m : 2.56198
[1mStep[0m  [160/169], [94mLoss[0m : 2.38323

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.385, [92mTest[0m: 2.341, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.09892
[1mStep[0m  [16/169], [94mLoss[0m : 2.38374
[1mStep[0m  [32/169], [94mLoss[0m : 2.42236
[1mStep[0m  [48/169], [94mLoss[0m : 2.16491
[1mStep[0m  [64/169], [94mLoss[0m : 2.22094
[1mStep[0m  [80/169], [94mLoss[0m : 2.33480
[1mStep[0m  [96/169], [94mLoss[0m : 2.19164
[1mStep[0m  [112/169], [94mLoss[0m : 2.08088
[1mStep[0m  [128/169], [94mLoss[0m : 2.15818
[1mStep[0m  [144/169], [94mLoss[0m : 2.29200
[1mStep[0m  [160/169], [94mLoss[0m : 2.59344

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.392, [92mTest[0m: 2.349, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.01934
[1mStep[0m  [16/169], [94mLoss[0m : 2.19031
[1mStep[0m  [32/169], [94mLoss[0m : 2.18223
[1mStep[0m  [48/169], [94mLoss[0m : 2.23071
[1mStep[0m  [64/169], [94mLoss[0m : 2.58312
[1mStep[0m  [80/169], [94mLoss[0m : 2.57002
[1mStep[0m  [96/169], [94mLoss[0m : 2.45955
[1mStep[0m  [112/169], [94mLoss[0m : 2.10883
[1mStep[0m  [128/169], [94mLoss[0m : 2.26034
[1mStep[0m  [144/169], [94mLoss[0m : 2.39678
[1mStep[0m  [160/169], [94mLoss[0m : 2.46852

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.384, [92mTest[0m: 2.339, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.34889
[1mStep[0m  [16/169], [94mLoss[0m : 1.92926
[1mStep[0m  [32/169], [94mLoss[0m : 2.62259
[1mStep[0m  [48/169], [94mLoss[0m : 2.24624
[1mStep[0m  [64/169], [94mLoss[0m : 1.99726
[1mStep[0m  [80/169], [94mLoss[0m : 2.26011
[1mStep[0m  [96/169], [94mLoss[0m : 2.56058
[1mStep[0m  [112/169], [94mLoss[0m : 2.42272
[1mStep[0m  [128/169], [94mLoss[0m : 2.61202
[1mStep[0m  [144/169], [94mLoss[0m : 2.10459
[1mStep[0m  [160/169], [94mLoss[0m : 2.36481

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.382, [92mTest[0m: 2.361, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.15082
[1mStep[0m  [16/169], [94mLoss[0m : 2.18827
[1mStep[0m  [32/169], [94mLoss[0m : 2.29748
[1mStep[0m  [48/169], [94mLoss[0m : 2.26181
[1mStep[0m  [64/169], [94mLoss[0m : 2.65609
[1mStep[0m  [80/169], [94mLoss[0m : 2.54049
[1mStep[0m  [96/169], [94mLoss[0m : 2.49260
[1mStep[0m  [112/169], [94mLoss[0m : 2.73529
[1mStep[0m  [128/169], [94mLoss[0m : 2.23064
[1mStep[0m  [144/169], [94mLoss[0m : 2.19126
[1mStep[0m  [160/169], [94mLoss[0m : 2.97148

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.393, [92mTest[0m: 2.339, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.01941
[1mStep[0m  [16/169], [94mLoss[0m : 2.68466
[1mStep[0m  [32/169], [94mLoss[0m : 2.36226
[1mStep[0m  [48/169], [94mLoss[0m : 2.53215
[1mStep[0m  [64/169], [94mLoss[0m : 2.64409
[1mStep[0m  [80/169], [94mLoss[0m : 2.76227
[1mStep[0m  [96/169], [94mLoss[0m : 2.23823
[1mStep[0m  [112/169], [94mLoss[0m : 2.29070
[1mStep[0m  [128/169], [94mLoss[0m : 2.42679
[1mStep[0m  [144/169], [94mLoss[0m : 2.42610
[1mStep[0m  [160/169], [94mLoss[0m : 2.47817

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.379, [92mTest[0m: 2.379, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.60185
[1mStep[0m  [16/169], [94mLoss[0m : 2.39897
[1mStep[0m  [32/169], [94mLoss[0m : 2.24783
[1mStep[0m  [48/169], [94mLoss[0m : 2.14118
[1mStep[0m  [64/169], [94mLoss[0m : 2.40281
[1mStep[0m  [80/169], [94mLoss[0m : 2.35161
[1mStep[0m  [96/169], [94mLoss[0m : 2.44885
[1mStep[0m  [112/169], [94mLoss[0m : 2.14238
[1mStep[0m  [128/169], [94mLoss[0m : 2.06482
[1mStep[0m  [144/169], [94mLoss[0m : 2.38805
[1mStep[0m  [160/169], [94mLoss[0m : 2.75173

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.380, [92mTest[0m: 2.356, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.17998
[1mStep[0m  [16/169], [94mLoss[0m : 2.05698
[1mStep[0m  [32/169], [94mLoss[0m : 2.29572
[1mStep[0m  [48/169], [94mLoss[0m : 2.25284
[1mStep[0m  [64/169], [94mLoss[0m : 2.17408
[1mStep[0m  [80/169], [94mLoss[0m : 2.75440
[1mStep[0m  [96/169], [94mLoss[0m : 2.06015
[1mStep[0m  [112/169], [94mLoss[0m : 2.70603
[1mStep[0m  [128/169], [94mLoss[0m : 2.24655
[1mStep[0m  [144/169], [94mLoss[0m : 1.96803
[1mStep[0m  [160/169], [94mLoss[0m : 2.34330

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.364, [92mTest[0m: 2.340, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.44866
[1mStep[0m  [16/169], [94mLoss[0m : 2.48784
[1mStep[0m  [32/169], [94mLoss[0m : 2.09380
[1mStep[0m  [48/169], [94mLoss[0m : 1.90205
[1mStep[0m  [64/169], [94mLoss[0m : 2.12891
[1mStep[0m  [80/169], [94mLoss[0m : 2.30730
[1mStep[0m  [96/169], [94mLoss[0m : 1.99101
[1mStep[0m  [112/169], [94mLoss[0m : 2.28373
[1mStep[0m  [128/169], [94mLoss[0m : 2.27720
[1mStep[0m  [144/169], [94mLoss[0m : 2.02705
[1mStep[0m  [160/169], [94mLoss[0m : 2.22989

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.374, [92mTest[0m: 2.375, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.34397
[1mStep[0m  [16/169], [94mLoss[0m : 2.60812
[1mStep[0m  [32/169], [94mLoss[0m : 2.80851
[1mStep[0m  [48/169], [94mLoss[0m : 2.34861
[1mStep[0m  [64/169], [94mLoss[0m : 2.33128
[1mStep[0m  [80/169], [94mLoss[0m : 2.12900
[1mStep[0m  [96/169], [94mLoss[0m : 2.35977
[1mStep[0m  [112/169], [94mLoss[0m : 2.74263
[1mStep[0m  [128/169], [94mLoss[0m : 2.16818
[1mStep[0m  [144/169], [94mLoss[0m : 2.27732
[1mStep[0m  [160/169], [94mLoss[0m : 2.48633

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.354, [92mTest[0m: 2.326, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.373
====================================

Phase 1 - Evaluation MAE:  2.372706983770643
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/169], [94mLoss[0m : 2.28052
[1mStep[0m  [16/169], [94mLoss[0m : 3.05103
[1mStep[0m  [32/169], [94mLoss[0m : 2.96852
[1mStep[0m  [48/169], [94mLoss[0m : 2.82250
[1mStep[0m  [64/169], [94mLoss[0m : 2.29415
[1mStep[0m  [80/169], [94mLoss[0m : 2.50079
[1mStep[0m  [96/169], [94mLoss[0m : 2.48840
[1mStep[0m  [112/169], [94mLoss[0m : 2.34026
[1mStep[0m  [128/169], [94mLoss[0m : 2.64451
[1mStep[0m  [144/169], [94mLoss[0m : 2.76909
[1mStep[0m  [160/169], [94mLoss[0m : 2.57526

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.376, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.67566
[1mStep[0m  [16/169], [94mLoss[0m : 1.94915
[1mStep[0m  [32/169], [94mLoss[0m : 2.17894
[1mStep[0m  [48/169], [94mLoss[0m : 2.37577
[1mStep[0m  [64/169], [94mLoss[0m : 2.22451
[1mStep[0m  [80/169], [94mLoss[0m : 2.59976
[1mStep[0m  [96/169], [94mLoss[0m : 2.28357
[1mStep[0m  [112/169], [94mLoss[0m : 1.98898
[1mStep[0m  [128/169], [94mLoss[0m : 1.94997
[1mStep[0m  [144/169], [94mLoss[0m : 2.96428
[1mStep[0m  [160/169], [94mLoss[0m : 2.22261

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.344, [92mTest[0m: 2.356, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.42503
[1mStep[0m  [16/169], [94mLoss[0m : 2.54079
[1mStep[0m  [32/169], [94mLoss[0m : 2.19755
[1mStep[0m  [48/169], [94mLoss[0m : 2.21296
[1mStep[0m  [64/169], [94mLoss[0m : 2.43947
[1mStep[0m  [80/169], [94mLoss[0m : 2.84322
[1mStep[0m  [96/169], [94mLoss[0m : 2.38958
[1mStep[0m  [112/169], [94mLoss[0m : 1.92358
[1mStep[0m  [128/169], [94mLoss[0m : 2.41206
[1mStep[0m  [144/169], [94mLoss[0m : 2.18100
[1mStep[0m  [160/169], [94mLoss[0m : 2.10568

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.248, [92mTest[0m: 2.371, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.83207
[1mStep[0m  [16/169], [94mLoss[0m : 2.14917
[1mStep[0m  [32/169], [94mLoss[0m : 2.00790
[1mStep[0m  [48/169], [94mLoss[0m : 1.95210
[1mStep[0m  [64/169], [94mLoss[0m : 1.94447
[1mStep[0m  [80/169], [94mLoss[0m : 2.25662
[1mStep[0m  [96/169], [94mLoss[0m : 2.18886
[1mStep[0m  [112/169], [94mLoss[0m : 1.94189
[1mStep[0m  [128/169], [94mLoss[0m : 2.37134
[1mStep[0m  [144/169], [94mLoss[0m : 2.17023
[1mStep[0m  [160/169], [94mLoss[0m : 2.22651

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.169, [92mTest[0m: 2.383, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.17359
[1mStep[0m  [16/169], [94mLoss[0m : 2.10700
[1mStep[0m  [32/169], [94mLoss[0m : 1.97934
[1mStep[0m  [48/169], [94mLoss[0m : 2.37476
[1mStep[0m  [64/169], [94mLoss[0m : 2.66644
[1mStep[0m  [80/169], [94mLoss[0m : 2.22209
[1mStep[0m  [96/169], [94mLoss[0m : 2.19373
[1mStep[0m  [112/169], [94mLoss[0m : 1.84386
[1mStep[0m  [128/169], [94mLoss[0m : 2.21437
[1mStep[0m  [144/169], [94mLoss[0m : 2.55214
[1mStep[0m  [160/169], [94mLoss[0m : 1.92638

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.140, [92mTest[0m: 2.380, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.74011
[1mStep[0m  [16/169], [94mLoss[0m : 1.88166
[1mStep[0m  [32/169], [94mLoss[0m : 1.70314
[1mStep[0m  [48/169], [94mLoss[0m : 2.18614
[1mStep[0m  [64/169], [94mLoss[0m : 2.18176
[1mStep[0m  [80/169], [94mLoss[0m : 2.23634
[1mStep[0m  [96/169], [94mLoss[0m : 1.89482
[1mStep[0m  [112/169], [94mLoss[0m : 1.89052
[1mStep[0m  [128/169], [94mLoss[0m : 2.06746
[1mStep[0m  [144/169], [94mLoss[0m : 1.73088
[1mStep[0m  [160/169], [94mLoss[0m : 2.07351

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.060, [92mTest[0m: 2.406, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.70205
[1mStep[0m  [16/169], [94mLoss[0m : 2.02407
[1mStep[0m  [32/169], [94mLoss[0m : 1.67057
[1mStep[0m  [48/169], [94mLoss[0m : 2.32423
[1mStep[0m  [64/169], [94mLoss[0m : 1.74461
[1mStep[0m  [80/169], [94mLoss[0m : 1.41419
[1mStep[0m  [96/169], [94mLoss[0m : 2.40091
[1mStep[0m  [112/169], [94mLoss[0m : 2.19914
[1mStep[0m  [128/169], [94mLoss[0m : 2.16230
[1mStep[0m  [144/169], [94mLoss[0m : 2.12997
[1mStep[0m  [160/169], [94mLoss[0m : 2.07411

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.022, [92mTest[0m: 2.396, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.91113
[1mStep[0m  [16/169], [94mLoss[0m : 1.72694
[1mStep[0m  [32/169], [94mLoss[0m : 2.04262
[1mStep[0m  [48/169], [94mLoss[0m : 1.97204
[1mStep[0m  [64/169], [94mLoss[0m : 2.18274
[1mStep[0m  [80/169], [94mLoss[0m : 2.14999
[1mStep[0m  [96/169], [94mLoss[0m : 2.05909
[1mStep[0m  [112/169], [94mLoss[0m : 1.86969
[1mStep[0m  [128/169], [94mLoss[0m : 1.98419
[1mStep[0m  [144/169], [94mLoss[0m : 2.01228
[1mStep[0m  [160/169], [94mLoss[0m : 1.98042

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.973, [92mTest[0m: 2.440, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.81424
[1mStep[0m  [16/169], [94mLoss[0m : 1.78534
[1mStep[0m  [32/169], [94mLoss[0m : 1.84694
[1mStep[0m  [48/169], [94mLoss[0m : 2.01055
[1mStep[0m  [64/169], [94mLoss[0m : 1.64663
[1mStep[0m  [80/169], [94mLoss[0m : 1.69764
[1mStep[0m  [96/169], [94mLoss[0m : 2.07134
[1mStep[0m  [112/169], [94mLoss[0m : 2.19795
[1mStep[0m  [128/169], [94mLoss[0m : 1.67866
[1mStep[0m  [144/169], [94mLoss[0m : 2.31284
[1mStep[0m  [160/169], [94mLoss[0m : 1.63724

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.917, [92mTest[0m: 2.426, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.90892
[1mStep[0m  [16/169], [94mLoss[0m : 1.76722
[1mStep[0m  [32/169], [94mLoss[0m : 1.81401
[1mStep[0m  [48/169], [94mLoss[0m : 2.10346
[1mStep[0m  [64/169], [94mLoss[0m : 1.81164
[1mStep[0m  [80/169], [94mLoss[0m : 1.96957
[1mStep[0m  [96/169], [94mLoss[0m : 2.07463
[1mStep[0m  [112/169], [94mLoss[0m : 1.85772
[1mStep[0m  [128/169], [94mLoss[0m : 2.13254
[1mStep[0m  [144/169], [94mLoss[0m : 1.98206
[1mStep[0m  [160/169], [94mLoss[0m : 1.89170

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.859, [92mTest[0m: 2.422, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.88846
[1mStep[0m  [16/169], [94mLoss[0m : 1.59278
[1mStep[0m  [32/169], [94mLoss[0m : 1.71178
[1mStep[0m  [48/169], [94mLoss[0m : 1.71326
[1mStep[0m  [64/169], [94mLoss[0m : 1.93666
[1mStep[0m  [80/169], [94mLoss[0m : 1.96448
[1mStep[0m  [96/169], [94mLoss[0m : 1.99746
[1mStep[0m  [112/169], [94mLoss[0m : 1.88076
[1mStep[0m  [128/169], [94mLoss[0m : 1.88509
[1mStep[0m  [144/169], [94mLoss[0m : 1.89761
[1mStep[0m  [160/169], [94mLoss[0m : 2.17774

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.829, [92mTest[0m: 2.451, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.69442
[1mStep[0m  [16/169], [94mLoss[0m : 1.74072
[1mStep[0m  [32/169], [94mLoss[0m : 1.34176
[1mStep[0m  [48/169], [94mLoss[0m : 1.51020
[1mStep[0m  [64/169], [94mLoss[0m : 1.67835
[1mStep[0m  [80/169], [94mLoss[0m : 1.66214
[1mStep[0m  [96/169], [94mLoss[0m : 1.78372
[1mStep[0m  [112/169], [94mLoss[0m : 1.88801
[1mStep[0m  [128/169], [94mLoss[0m : 1.89939
[1mStep[0m  [144/169], [94mLoss[0m : 1.90868
[1mStep[0m  [160/169], [94mLoss[0m : 1.76117

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.763, [92mTest[0m: 2.470, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.55787
[1mStep[0m  [16/169], [94mLoss[0m : 1.50425
[1mStep[0m  [32/169], [94mLoss[0m : 1.75383
[1mStep[0m  [48/169], [94mLoss[0m : 1.39936
[1mStep[0m  [64/169], [94mLoss[0m : 1.54728
[1mStep[0m  [80/169], [94mLoss[0m : 1.86313
[1mStep[0m  [96/169], [94mLoss[0m : 1.71470
[1mStep[0m  [112/169], [94mLoss[0m : 1.87589
[1mStep[0m  [128/169], [94mLoss[0m : 1.99974
[1mStep[0m  [144/169], [94mLoss[0m : 1.42667
[1mStep[0m  [160/169], [94mLoss[0m : 1.84163

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.754, [92mTest[0m: 2.472, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.51165
[1mStep[0m  [16/169], [94mLoss[0m : 1.59934
[1mStep[0m  [32/169], [94mLoss[0m : 1.61558
[1mStep[0m  [48/169], [94mLoss[0m : 1.57961
[1mStep[0m  [64/169], [94mLoss[0m : 1.39026
[1mStep[0m  [80/169], [94mLoss[0m : 1.73196
[1mStep[0m  [96/169], [94mLoss[0m : 1.76021
[1mStep[0m  [112/169], [94mLoss[0m : 1.57948
[1mStep[0m  [128/169], [94mLoss[0m : 1.66264
[1mStep[0m  [144/169], [94mLoss[0m : 1.69550
[1mStep[0m  [160/169], [94mLoss[0m : 1.99742

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.708, [92mTest[0m: 2.475, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.30786
[1mStep[0m  [16/169], [94mLoss[0m : 1.62223
[1mStep[0m  [32/169], [94mLoss[0m : 1.87570
[1mStep[0m  [48/169], [94mLoss[0m : 1.72171
[1mStep[0m  [64/169], [94mLoss[0m : 1.65261
[1mStep[0m  [80/169], [94mLoss[0m : 1.93541
[1mStep[0m  [96/169], [94mLoss[0m : 2.01358
[1mStep[0m  [112/169], [94mLoss[0m : 1.61112
[1mStep[0m  [128/169], [94mLoss[0m : 1.33368
[1mStep[0m  [144/169], [94mLoss[0m : 1.84162
[1mStep[0m  [160/169], [94mLoss[0m : 1.70573

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.695, [92mTest[0m: 2.479, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.36644
[1mStep[0m  [16/169], [94mLoss[0m : 1.62244
[1mStep[0m  [32/169], [94mLoss[0m : 1.36976
[1mStep[0m  [48/169], [94mLoss[0m : 1.37682
[1mStep[0m  [64/169], [94mLoss[0m : 1.75470
[1mStep[0m  [80/169], [94mLoss[0m : 1.71852
[1mStep[0m  [96/169], [94mLoss[0m : 1.62189
[1mStep[0m  [112/169], [94mLoss[0m : 1.29791
[1mStep[0m  [128/169], [94mLoss[0m : 1.58582
[1mStep[0m  [144/169], [94mLoss[0m : 1.49914
[1mStep[0m  [160/169], [94mLoss[0m : 1.61424

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.638, [92mTest[0m: 2.467, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.64180
[1mStep[0m  [16/169], [94mLoss[0m : 1.74635
[1mStep[0m  [32/169], [94mLoss[0m : 1.65265
[1mStep[0m  [48/169], [94mLoss[0m : 1.64261
[1mStep[0m  [64/169], [94mLoss[0m : 1.79299
[1mStep[0m  [80/169], [94mLoss[0m : 1.18321
[1mStep[0m  [96/169], [94mLoss[0m : 1.41313
[1mStep[0m  [112/169], [94mLoss[0m : 1.93510
[1mStep[0m  [128/169], [94mLoss[0m : 1.95989
[1mStep[0m  [144/169], [94mLoss[0m : 1.80170
[1mStep[0m  [160/169], [94mLoss[0m : 1.34064

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.617, [92mTest[0m: 2.480, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.54565
[1mStep[0m  [16/169], [94mLoss[0m : 1.64915
[1mStep[0m  [32/169], [94mLoss[0m : 1.26305
[1mStep[0m  [48/169], [94mLoss[0m : 2.01631
[1mStep[0m  [64/169], [94mLoss[0m : 1.49642
[1mStep[0m  [80/169], [94mLoss[0m : 2.43874
[1mStep[0m  [96/169], [94mLoss[0m : 1.69306
[1mStep[0m  [112/169], [94mLoss[0m : 1.58792
[1mStep[0m  [128/169], [94mLoss[0m : 1.68411
[1mStep[0m  [144/169], [94mLoss[0m : 1.32853
[1mStep[0m  [160/169], [94mLoss[0m : 1.90626

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.579, [92mTest[0m: 2.507, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.48188
[1mStep[0m  [16/169], [94mLoss[0m : 1.66743
[1mStep[0m  [32/169], [94mLoss[0m : 1.28327
[1mStep[0m  [48/169], [94mLoss[0m : 1.80648
[1mStep[0m  [64/169], [94mLoss[0m : 1.48802
[1mStep[0m  [80/169], [94mLoss[0m : 1.55043
[1mStep[0m  [96/169], [94mLoss[0m : 1.62235
[1mStep[0m  [112/169], [94mLoss[0m : 1.57134
[1mStep[0m  [128/169], [94mLoss[0m : 1.37051
[1mStep[0m  [144/169], [94mLoss[0m : 1.69860
[1mStep[0m  [160/169], [94mLoss[0m : 1.49309

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.569, [92mTest[0m: 2.522, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.41752
[1mStep[0m  [16/169], [94mLoss[0m : 1.61320
[1mStep[0m  [32/169], [94mLoss[0m : 1.33199
[1mStep[0m  [48/169], [94mLoss[0m : 1.66696
[1mStep[0m  [64/169], [94mLoss[0m : 1.37713
[1mStep[0m  [80/169], [94mLoss[0m : 1.95185
[1mStep[0m  [96/169], [94mLoss[0m : 1.46567
[1mStep[0m  [112/169], [94mLoss[0m : 1.48744
[1mStep[0m  [128/169], [94mLoss[0m : 1.32120
[1mStep[0m  [144/169], [94mLoss[0m : 1.60245
[1mStep[0m  [160/169], [94mLoss[0m : 1.36639

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.535, [92mTest[0m: 2.515, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.39004
[1mStep[0m  [16/169], [94mLoss[0m : 1.66890
[1mStep[0m  [32/169], [94mLoss[0m : 1.69998
[1mStep[0m  [48/169], [94mLoss[0m : 1.63409
[1mStep[0m  [64/169], [94mLoss[0m : 1.34374
[1mStep[0m  [80/169], [94mLoss[0m : 1.50860
[1mStep[0m  [96/169], [94mLoss[0m : 1.28664
[1mStep[0m  [112/169], [94mLoss[0m : 1.38916
[1mStep[0m  [128/169], [94mLoss[0m : 1.47696
[1mStep[0m  [144/169], [94mLoss[0m : 1.43273
[1mStep[0m  [160/169], [94mLoss[0m : 1.69036

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.504, [92mTest[0m: 2.484, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.38668
[1mStep[0m  [16/169], [94mLoss[0m : 1.47317
[1mStep[0m  [32/169], [94mLoss[0m : 1.43543
[1mStep[0m  [48/169], [94mLoss[0m : 1.55523
[1mStep[0m  [64/169], [94mLoss[0m : 1.44817
[1mStep[0m  [80/169], [94mLoss[0m : 1.51877
[1mStep[0m  [96/169], [94mLoss[0m : 1.46344
[1mStep[0m  [112/169], [94mLoss[0m : 1.39254
[1mStep[0m  [128/169], [94mLoss[0m : 1.09690
[1mStep[0m  [144/169], [94mLoss[0m : 1.26612
[1mStep[0m  [160/169], [94mLoss[0m : 1.16540

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.461, [92mTest[0m: 2.503, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.42033
[1mStep[0m  [16/169], [94mLoss[0m : 1.65262
[1mStep[0m  [32/169], [94mLoss[0m : 1.12857
[1mStep[0m  [48/169], [94mLoss[0m : 1.39340
[1mStep[0m  [64/169], [94mLoss[0m : 1.22048
[1mStep[0m  [80/169], [94mLoss[0m : 1.28448
[1mStep[0m  [96/169], [94mLoss[0m : 1.60219
[1mStep[0m  [112/169], [94mLoss[0m : 1.35709
[1mStep[0m  [128/169], [94mLoss[0m : 1.31631
[1mStep[0m  [144/169], [94mLoss[0m : 1.69147
[1mStep[0m  [160/169], [94mLoss[0m : 1.45830

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.427, [92mTest[0m: 2.468, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.14089
[1mStep[0m  [16/169], [94mLoss[0m : 1.51137
[1mStep[0m  [32/169], [94mLoss[0m : 1.13765
[1mStep[0m  [48/169], [94mLoss[0m : 1.43536
[1mStep[0m  [64/169], [94mLoss[0m : 1.46333
[1mStep[0m  [80/169], [94mLoss[0m : 1.59733
[1mStep[0m  [96/169], [94mLoss[0m : 1.40809
[1mStep[0m  [112/169], [94mLoss[0m : 1.38814
[1mStep[0m  [128/169], [94mLoss[0m : 1.63823
[1mStep[0m  [144/169], [94mLoss[0m : 1.27997
[1mStep[0m  [160/169], [94mLoss[0m : 1.38667

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.413, [92mTest[0m: 2.527, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.10527
[1mStep[0m  [16/169], [94mLoss[0m : 1.41521
[1mStep[0m  [32/169], [94mLoss[0m : 1.44597
[1mStep[0m  [48/169], [94mLoss[0m : 1.37109
[1mStep[0m  [64/169], [94mLoss[0m : 1.55948
[1mStep[0m  [80/169], [94mLoss[0m : 1.58267
[1mStep[0m  [96/169], [94mLoss[0m : 1.57834
[1mStep[0m  [112/169], [94mLoss[0m : 1.62482
[1mStep[0m  [128/169], [94mLoss[0m : 1.16147
[1mStep[0m  [144/169], [94mLoss[0m : 1.70253
[1mStep[0m  [160/169], [94mLoss[0m : 1.37175

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.411, [92mTest[0m: 2.519, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.90547
[1mStep[0m  [16/169], [94mLoss[0m : 1.31036
[1mStep[0m  [32/169], [94mLoss[0m : 1.05431
[1mStep[0m  [48/169], [94mLoss[0m : 1.57747
[1mStep[0m  [64/169], [94mLoss[0m : 1.24402
[1mStep[0m  [80/169], [94mLoss[0m : 1.80798
[1mStep[0m  [96/169], [94mLoss[0m : 1.14087
[1mStep[0m  [112/169], [94mLoss[0m : 1.32814
[1mStep[0m  [128/169], [94mLoss[0m : 1.32247
[1mStep[0m  [144/169], [94mLoss[0m : 1.37580
[1mStep[0m  [160/169], [94mLoss[0m : 1.47721

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.401, [92mTest[0m: 2.544, [96mlr[0m: 0.004545
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 25 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.559
====================================

Phase 2 - Evaluation MAE:  2.559481835791043
MAE score P1      2.372707
MAE score P2      2.559482
loss              1.401324
learning_rate      0.00505
batch_size              64
hidden_sizes         [300]
epochs                  30
activation            tanh
optimizer              sgd
early stopping        True
dropout                0.3
momentum               0.9
weight_decay         0.001
Name: 13, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 11.49788
[1mStep[0m  [8/84], [94mLoss[0m : 2.67878
[1mStep[0m  [16/84], [94mLoss[0m : 4.48556
[1mStep[0m  [24/84], [94mLoss[0m : 3.25023
[1mStep[0m  [32/84], [94mLoss[0m : 2.49502
[1mStep[0m  [40/84], [94mLoss[0m : 2.50082
[1mStep[0m  [48/84], [94mLoss[0m : 2.57356
[1mStep[0m  [56/84], [94mLoss[0m : 2.45514
[1mStep[0m  [64/84], [94mLoss[0m : 2.73174
[1mStep[0m  [72/84], [94mLoss[0m : 2.48713
[1mStep[0m  [80/84], [94mLoss[0m : 2.53241

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.321, [92mTest[0m: 11.167, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60363
[1mStep[0m  [8/84], [94mLoss[0m : 2.44706
[1mStep[0m  [16/84], [94mLoss[0m : 2.71388
[1mStep[0m  [24/84], [94mLoss[0m : 2.33823
[1mStep[0m  [32/84], [94mLoss[0m : 2.65538
[1mStep[0m  [40/84], [94mLoss[0m : 2.37802
[1mStep[0m  [48/84], [94mLoss[0m : 2.30406
[1mStep[0m  [56/84], [94mLoss[0m : 2.42112
[1mStep[0m  [64/84], [94mLoss[0m : 2.49118
[1mStep[0m  [72/84], [94mLoss[0m : 2.70914
[1mStep[0m  [80/84], [94mLoss[0m : 2.40635

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.370, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42355
[1mStep[0m  [8/84], [94mLoss[0m : 2.60271
[1mStep[0m  [16/84], [94mLoss[0m : 2.48211
[1mStep[0m  [24/84], [94mLoss[0m : 2.61718
[1mStep[0m  [32/84], [94mLoss[0m : 2.47913
[1mStep[0m  [40/84], [94mLoss[0m : 2.48175
[1mStep[0m  [48/84], [94mLoss[0m : 2.22402
[1mStep[0m  [56/84], [94mLoss[0m : 2.21996
[1mStep[0m  [64/84], [94mLoss[0m : 2.75436
[1mStep[0m  [72/84], [94mLoss[0m : 2.58027
[1mStep[0m  [80/84], [94mLoss[0m : 2.23081

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.343, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.39416
[1mStep[0m  [8/84], [94mLoss[0m : 2.06775
[1mStep[0m  [16/84], [94mLoss[0m : 2.83094
[1mStep[0m  [24/84], [94mLoss[0m : 2.29337
[1mStep[0m  [32/84], [94mLoss[0m : 2.37573
[1mStep[0m  [40/84], [94mLoss[0m : 2.53804
[1mStep[0m  [48/84], [94mLoss[0m : 2.05521
[1mStep[0m  [56/84], [94mLoss[0m : 2.54825
[1mStep[0m  [64/84], [94mLoss[0m : 2.28664
[1mStep[0m  [72/84], [94mLoss[0m : 2.48430
[1mStep[0m  [80/84], [94mLoss[0m : 2.23219

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.336, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30787
[1mStep[0m  [8/84], [94mLoss[0m : 2.22709
[1mStep[0m  [16/84], [94mLoss[0m : 2.28302
[1mStep[0m  [24/84], [94mLoss[0m : 2.36711
[1mStep[0m  [32/84], [94mLoss[0m : 2.53383
[1mStep[0m  [40/84], [94mLoss[0m : 2.23990
[1mStep[0m  [48/84], [94mLoss[0m : 2.42976
[1mStep[0m  [56/84], [94mLoss[0m : 2.75042
[1mStep[0m  [64/84], [94mLoss[0m : 2.32686
[1mStep[0m  [72/84], [94mLoss[0m : 2.49838
[1mStep[0m  [80/84], [94mLoss[0m : 2.60474

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.332, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46573
[1mStep[0m  [8/84], [94mLoss[0m : 2.47308
[1mStep[0m  [16/84], [94mLoss[0m : 2.51175
[1mStep[0m  [24/84], [94mLoss[0m : 2.73779
[1mStep[0m  [32/84], [94mLoss[0m : 2.21932
[1mStep[0m  [40/84], [94mLoss[0m : 2.53758
[1mStep[0m  [48/84], [94mLoss[0m : 2.33829
[1mStep[0m  [56/84], [94mLoss[0m : 2.30948
[1mStep[0m  [64/84], [94mLoss[0m : 2.54973
[1mStep[0m  [72/84], [94mLoss[0m : 2.37277
[1mStep[0m  [80/84], [94mLoss[0m : 2.47405

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.333, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36112
[1mStep[0m  [8/84], [94mLoss[0m : 2.52053
[1mStep[0m  [16/84], [94mLoss[0m : 2.56854
[1mStep[0m  [24/84], [94mLoss[0m : 2.52745
[1mStep[0m  [32/84], [94mLoss[0m : 2.62086
[1mStep[0m  [40/84], [94mLoss[0m : 2.65692
[1mStep[0m  [48/84], [94mLoss[0m : 2.35032
[1mStep[0m  [56/84], [94mLoss[0m : 2.68108
[1mStep[0m  [64/84], [94mLoss[0m : 2.21434
[1mStep[0m  [72/84], [94mLoss[0m : 2.55800
[1mStep[0m  [80/84], [94mLoss[0m : 2.04219

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.321, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.39706
[1mStep[0m  [8/84], [94mLoss[0m : 2.31787
[1mStep[0m  [16/84], [94mLoss[0m : 2.07991
[1mStep[0m  [24/84], [94mLoss[0m : 2.06448
[1mStep[0m  [32/84], [94mLoss[0m : 2.24441
[1mStep[0m  [40/84], [94mLoss[0m : 2.16366
[1mStep[0m  [48/84], [94mLoss[0m : 2.91042
[1mStep[0m  [56/84], [94mLoss[0m : 2.58904
[1mStep[0m  [64/84], [94mLoss[0m : 2.48819
[1mStep[0m  [72/84], [94mLoss[0m : 2.36144
[1mStep[0m  [80/84], [94mLoss[0m : 2.49314

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.334, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31874
[1mStep[0m  [8/84], [94mLoss[0m : 2.39890
[1mStep[0m  [16/84], [94mLoss[0m : 2.26911
[1mStep[0m  [24/84], [94mLoss[0m : 2.41155
[1mStep[0m  [32/84], [94mLoss[0m : 2.42210
[1mStep[0m  [40/84], [94mLoss[0m : 2.35016
[1mStep[0m  [48/84], [94mLoss[0m : 2.51770
[1mStep[0m  [56/84], [94mLoss[0m : 2.31020
[1mStep[0m  [64/84], [94mLoss[0m : 2.51074
[1mStep[0m  [72/84], [94mLoss[0m : 2.33232
[1mStep[0m  [80/84], [94mLoss[0m : 2.35839

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.318, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27234
[1mStep[0m  [8/84], [94mLoss[0m : 2.45720
[1mStep[0m  [16/84], [94mLoss[0m : 2.48781
[1mStep[0m  [24/84], [94mLoss[0m : 2.47040
[1mStep[0m  [32/84], [94mLoss[0m : 2.33904
[1mStep[0m  [40/84], [94mLoss[0m : 2.25232
[1mStep[0m  [48/84], [94mLoss[0m : 2.62223
[1mStep[0m  [56/84], [94mLoss[0m : 2.10980
[1mStep[0m  [64/84], [94mLoss[0m : 2.56052
[1mStep[0m  [72/84], [94mLoss[0m : 2.61277
[1mStep[0m  [80/84], [94mLoss[0m : 2.45512

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.329, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52330
[1mStep[0m  [8/84], [94mLoss[0m : 2.37454
[1mStep[0m  [16/84], [94mLoss[0m : 2.27581
[1mStep[0m  [24/84], [94mLoss[0m : 2.17700
[1mStep[0m  [32/84], [94mLoss[0m : 2.46781
[1mStep[0m  [40/84], [94mLoss[0m : 2.50911
[1mStep[0m  [48/84], [94mLoss[0m : 2.47859
[1mStep[0m  [56/84], [94mLoss[0m : 2.34412
[1mStep[0m  [64/84], [94mLoss[0m : 2.34585
[1mStep[0m  [72/84], [94mLoss[0m : 2.99011
[1mStep[0m  [80/84], [94mLoss[0m : 2.52108

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.378, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45043
[1mStep[0m  [8/84], [94mLoss[0m : 2.66434
[1mStep[0m  [16/84], [94mLoss[0m : 2.33143
[1mStep[0m  [24/84], [94mLoss[0m : 2.04775
[1mStep[0m  [32/84], [94mLoss[0m : 2.35071
[1mStep[0m  [40/84], [94mLoss[0m : 2.56547
[1mStep[0m  [48/84], [94mLoss[0m : 2.55380
[1mStep[0m  [56/84], [94mLoss[0m : 2.37597
[1mStep[0m  [64/84], [94mLoss[0m : 2.31647
[1mStep[0m  [72/84], [94mLoss[0m : 2.03014
[1mStep[0m  [80/84], [94mLoss[0m : 2.27663

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.347, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41648
[1mStep[0m  [8/84], [94mLoss[0m : 2.41764
[1mStep[0m  [16/84], [94mLoss[0m : 2.14611
[1mStep[0m  [24/84], [94mLoss[0m : 2.15451
[1mStep[0m  [32/84], [94mLoss[0m : 2.21980
[1mStep[0m  [40/84], [94mLoss[0m : 2.54163
[1mStep[0m  [48/84], [94mLoss[0m : 2.56474
[1mStep[0m  [56/84], [94mLoss[0m : 2.39221
[1mStep[0m  [64/84], [94mLoss[0m : 2.32874
[1mStep[0m  [72/84], [94mLoss[0m : 2.20704
[1mStep[0m  [80/84], [94mLoss[0m : 2.38712

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.326, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43761
[1mStep[0m  [8/84], [94mLoss[0m : 2.26167
[1mStep[0m  [16/84], [94mLoss[0m : 2.55725
[1mStep[0m  [24/84], [94mLoss[0m : 2.49118
[1mStep[0m  [32/84], [94mLoss[0m : 2.35138
[1mStep[0m  [40/84], [94mLoss[0m : 2.68129
[1mStep[0m  [48/84], [94mLoss[0m : 2.25943
[1mStep[0m  [56/84], [94mLoss[0m : 2.44169
[1mStep[0m  [64/84], [94mLoss[0m : 2.40329
[1mStep[0m  [72/84], [94mLoss[0m : 2.62583
[1mStep[0m  [80/84], [94mLoss[0m : 2.41093

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.327, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.05806
[1mStep[0m  [8/84], [94mLoss[0m : 2.50153
[1mStep[0m  [16/84], [94mLoss[0m : 2.50055
[1mStep[0m  [24/84], [94mLoss[0m : 2.46344
[1mStep[0m  [32/84], [94mLoss[0m : 2.37266
[1mStep[0m  [40/84], [94mLoss[0m : 2.32711
[1mStep[0m  [48/84], [94mLoss[0m : 2.57774
[1mStep[0m  [56/84], [94mLoss[0m : 2.43746
[1mStep[0m  [64/84], [94mLoss[0m : 2.40287
[1mStep[0m  [72/84], [94mLoss[0m : 2.49993
[1mStep[0m  [80/84], [94mLoss[0m : 2.50964

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.329, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.39799
[1mStep[0m  [8/84], [94mLoss[0m : 2.19148
[1mStep[0m  [16/84], [94mLoss[0m : 2.12311
[1mStep[0m  [24/84], [94mLoss[0m : 2.38318
[1mStep[0m  [32/84], [94mLoss[0m : 2.46463
[1mStep[0m  [40/84], [94mLoss[0m : 2.31098
[1mStep[0m  [48/84], [94mLoss[0m : 2.41894
[1mStep[0m  [56/84], [94mLoss[0m : 2.52271
[1mStep[0m  [64/84], [94mLoss[0m : 2.36013
[1mStep[0m  [72/84], [94mLoss[0m : 2.28022
[1mStep[0m  [80/84], [94mLoss[0m : 2.83754

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.329, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.56137
[1mStep[0m  [8/84], [94mLoss[0m : 2.56385
[1mStep[0m  [16/84], [94mLoss[0m : 2.45922
[1mStep[0m  [24/84], [94mLoss[0m : 2.62084
[1mStep[0m  [32/84], [94mLoss[0m : 2.85118
[1mStep[0m  [40/84], [94mLoss[0m : 2.42697
[1mStep[0m  [48/84], [94mLoss[0m : 2.50875
[1mStep[0m  [56/84], [94mLoss[0m : 2.53720
[1mStep[0m  [64/84], [94mLoss[0m : 2.44096
[1mStep[0m  [72/84], [94mLoss[0m : 2.33762
[1mStep[0m  [80/84], [94mLoss[0m : 2.20274

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.387, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44521
[1mStep[0m  [8/84], [94mLoss[0m : 2.34269
[1mStep[0m  [16/84], [94mLoss[0m : 2.28079
[1mStep[0m  [24/84], [94mLoss[0m : 2.61956
[1mStep[0m  [32/84], [94mLoss[0m : 2.17584
[1mStep[0m  [40/84], [94mLoss[0m : 2.27239
[1mStep[0m  [48/84], [94mLoss[0m : 2.22687
[1mStep[0m  [56/84], [94mLoss[0m : 2.22267
[1mStep[0m  [64/84], [94mLoss[0m : 2.11714
[1mStep[0m  [72/84], [94mLoss[0m : 2.63187
[1mStep[0m  [80/84], [94mLoss[0m : 2.30853

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.325, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.15278
[1mStep[0m  [8/84], [94mLoss[0m : 2.53910
[1mStep[0m  [16/84], [94mLoss[0m : 2.53143
[1mStep[0m  [24/84], [94mLoss[0m : 2.69403
[1mStep[0m  [32/84], [94mLoss[0m : 2.63550
[1mStep[0m  [40/84], [94mLoss[0m : 2.82507
[1mStep[0m  [48/84], [94mLoss[0m : 2.42642
[1mStep[0m  [56/84], [94mLoss[0m : 2.50429
[1mStep[0m  [64/84], [94mLoss[0m : 2.38780
[1mStep[0m  [72/84], [94mLoss[0m : 2.33420
[1mStep[0m  [80/84], [94mLoss[0m : 2.51227

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.326, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.51463
[1mStep[0m  [8/84], [94mLoss[0m : 2.32211
[1mStep[0m  [16/84], [94mLoss[0m : 2.45221
[1mStep[0m  [24/84], [94mLoss[0m : 2.32237
[1mStep[0m  [32/84], [94mLoss[0m : 2.37990
[1mStep[0m  [40/84], [94mLoss[0m : 2.33924
[1mStep[0m  [48/84], [94mLoss[0m : 2.30108
[1mStep[0m  [56/84], [94mLoss[0m : 2.22025
[1mStep[0m  [64/84], [94mLoss[0m : 2.50971
[1mStep[0m  [72/84], [94mLoss[0m : 2.60656
[1mStep[0m  [80/84], [94mLoss[0m : 2.42028

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.325, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.34245
[1mStep[0m  [8/84], [94mLoss[0m : 2.52302
[1mStep[0m  [16/84], [94mLoss[0m : 2.58088
[1mStep[0m  [24/84], [94mLoss[0m : 2.24944
[1mStep[0m  [32/84], [94mLoss[0m : 2.21583
[1mStep[0m  [40/84], [94mLoss[0m : 2.53450
[1mStep[0m  [48/84], [94mLoss[0m : 2.29564
[1mStep[0m  [56/84], [94mLoss[0m : 2.48186
[1mStep[0m  [64/84], [94mLoss[0m : 2.23844
[1mStep[0m  [72/84], [94mLoss[0m : 2.49621
[1mStep[0m  [80/84], [94mLoss[0m : 2.50595

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.327, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55953
[1mStep[0m  [8/84], [94mLoss[0m : 2.28356
[1mStep[0m  [16/84], [94mLoss[0m : 2.54996
[1mStep[0m  [24/84], [94mLoss[0m : 2.54487
[1mStep[0m  [32/84], [94mLoss[0m : 2.22354
[1mStep[0m  [40/84], [94mLoss[0m : 2.73720
[1mStep[0m  [48/84], [94mLoss[0m : 2.51834
[1mStep[0m  [56/84], [94mLoss[0m : 2.26578
[1mStep[0m  [64/84], [94mLoss[0m : 2.18503
[1mStep[0m  [72/84], [94mLoss[0m : 2.32436
[1mStep[0m  [80/84], [94mLoss[0m : 2.63030

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.328, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60704
[1mStep[0m  [8/84], [94mLoss[0m : 2.08841
[1mStep[0m  [16/84], [94mLoss[0m : 2.19985
[1mStep[0m  [24/84], [94mLoss[0m : 2.39145
[1mStep[0m  [32/84], [94mLoss[0m : 2.42477
[1mStep[0m  [40/84], [94mLoss[0m : 2.54271
[1mStep[0m  [48/84], [94mLoss[0m : 2.38944
[1mStep[0m  [56/84], [94mLoss[0m : 2.38939
[1mStep[0m  [64/84], [94mLoss[0m : 2.38539
[1mStep[0m  [72/84], [94mLoss[0m : 2.55992
[1mStep[0m  [80/84], [94mLoss[0m : 2.36685

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.324, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.90042
[1mStep[0m  [8/84], [94mLoss[0m : 2.48361
[1mStep[0m  [16/84], [94mLoss[0m : 2.68819
[1mStep[0m  [24/84], [94mLoss[0m : 2.48824
[1mStep[0m  [32/84], [94mLoss[0m : 2.49522
[1mStep[0m  [40/84], [94mLoss[0m : 2.34914
[1mStep[0m  [48/84], [94mLoss[0m : 2.58889
[1mStep[0m  [56/84], [94mLoss[0m : 2.23013
[1mStep[0m  [64/84], [94mLoss[0m : 2.47852
[1mStep[0m  [72/84], [94mLoss[0m : 2.77860
[1mStep[0m  [80/84], [94mLoss[0m : 2.43464

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.333, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.51265
[1mStep[0m  [8/84], [94mLoss[0m : 2.22025
[1mStep[0m  [16/84], [94mLoss[0m : 2.49084
[1mStep[0m  [24/84], [94mLoss[0m : 2.06811
[1mStep[0m  [32/84], [94mLoss[0m : 2.63146
[1mStep[0m  [40/84], [94mLoss[0m : 2.38897
[1mStep[0m  [48/84], [94mLoss[0m : 2.48755
[1mStep[0m  [56/84], [94mLoss[0m : 2.50545
[1mStep[0m  [64/84], [94mLoss[0m : 2.34682
[1mStep[0m  [72/84], [94mLoss[0m : 2.16290
[1mStep[0m  [80/84], [94mLoss[0m : 2.55948

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.418, [92mTest[0m: 2.348, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33241
[1mStep[0m  [8/84], [94mLoss[0m : 2.26246
[1mStep[0m  [16/84], [94mLoss[0m : 2.25283
[1mStep[0m  [24/84], [94mLoss[0m : 2.37025
[1mStep[0m  [32/84], [94mLoss[0m : 2.60711
[1mStep[0m  [40/84], [94mLoss[0m : 2.22415
[1mStep[0m  [48/84], [94mLoss[0m : 2.59843
[1mStep[0m  [56/84], [94mLoss[0m : 2.40572
[1mStep[0m  [64/84], [94mLoss[0m : 2.29875
[1mStep[0m  [72/84], [94mLoss[0m : 2.15537
[1mStep[0m  [80/84], [94mLoss[0m : 2.31133

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.322, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.56703
[1mStep[0m  [8/84], [94mLoss[0m : 2.58581
[1mStep[0m  [16/84], [94mLoss[0m : 2.33688
[1mStep[0m  [24/84], [94mLoss[0m : 2.21948
[1mStep[0m  [32/84], [94mLoss[0m : 2.45989
[1mStep[0m  [40/84], [94mLoss[0m : 2.48727
[1mStep[0m  [48/84], [94mLoss[0m : 2.30587
[1mStep[0m  [56/84], [94mLoss[0m : 2.53084
[1mStep[0m  [64/84], [94mLoss[0m : 2.45698
[1mStep[0m  [72/84], [94mLoss[0m : 2.33688
[1mStep[0m  [80/84], [94mLoss[0m : 2.82845

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.332, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.04321
[1mStep[0m  [8/84], [94mLoss[0m : 2.71228
[1mStep[0m  [16/84], [94mLoss[0m : 2.66253
[1mStep[0m  [24/84], [94mLoss[0m : 2.32305
[1mStep[0m  [32/84], [94mLoss[0m : 2.30792
[1mStep[0m  [40/84], [94mLoss[0m : 2.51453
[1mStep[0m  [48/84], [94mLoss[0m : 2.32037
[1mStep[0m  [56/84], [94mLoss[0m : 2.36636
[1mStep[0m  [64/84], [94mLoss[0m : 2.42005
[1mStep[0m  [72/84], [94mLoss[0m : 2.40005
[1mStep[0m  [80/84], [94mLoss[0m : 2.46942

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.320, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42444
[1mStep[0m  [8/84], [94mLoss[0m : 2.59588
[1mStep[0m  [16/84], [94mLoss[0m : 2.89818
[1mStep[0m  [24/84], [94mLoss[0m : 2.44724
[1mStep[0m  [32/84], [94mLoss[0m : 2.12787
[1mStep[0m  [40/84], [94mLoss[0m : 2.17647
[1mStep[0m  [48/84], [94mLoss[0m : 2.41374
[1mStep[0m  [56/84], [94mLoss[0m : 2.57609
[1mStep[0m  [64/84], [94mLoss[0m : 2.42780
[1mStep[0m  [72/84], [94mLoss[0m : 2.00688
[1mStep[0m  [80/84], [94mLoss[0m : 2.21405

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.311, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.03682
[1mStep[0m  [8/84], [94mLoss[0m : 2.26870
[1mStep[0m  [16/84], [94mLoss[0m : 2.38028
[1mStep[0m  [24/84], [94mLoss[0m : 2.30142
[1mStep[0m  [32/84], [94mLoss[0m : 2.68192
[1mStep[0m  [40/84], [94mLoss[0m : 1.87914
[1mStep[0m  [48/84], [94mLoss[0m : 2.57135
[1mStep[0m  [56/84], [94mLoss[0m : 2.56107
[1mStep[0m  [64/84], [94mLoss[0m : 2.50448
[1mStep[0m  [72/84], [94mLoss[0m : 2.37261
[1mStep[0m  [80/84], [94mLoss[0m : 2.34929

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.322, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.347
====================================

Phase 1 - Evaluation MAE:  2.346524570669447
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 2.71320
[1mStep[0m  [8/84], [94mLoss[0m : 2.49399
[1mStep[0m  [16/84], [94mLoss[0m : 2.75007
[1mStep[0m  [24/84], [94mLoss[0m : 2.69914
[1mStep[0m  [32/84], [94mLoss[0m : 2.35946
[1mStep[0m  [40/84], [94mLoss[0m : 2.46916
[1mStep[0m  [48/84], [94mLoss[0m : 2.44867
[1mStep[0m  [56/84], [94mLoss[0m : 2.33015
[1mStep[0m  [64/84], [94mLoss[0m : 2.42533
[1mStep[0m  [72/84], [94mLoss[0m : 2.26717
[1mStep[0m  [80/84], [94mLoss[0m : 2.17840

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.343, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.04582
[1mStep[0m  [8/84], [94mLoss[0m : 2.36665
[1mStep[0m  [16/84], [94mLoss[0m : 2.33752
[1mStep[0m  [24/84], [94mLoss[0m : 2.53280
[1mStep[0m  [32/84], [94mLoss[0m : 2.07681
[1mStep[0m  [40/84], [94mLoss[0m : 2.32747
[1mStep[0m  [48/84], [94mLoss[0m : 2.22042
[1mStep[0m  [56/84], [94mLoss[0m : 2.52413
[1mStep[0m  [64/84], [94mLoss[0m : 2.31464
[1mStep[0m  [72/84], [94mLoss[0m : 2.34224
[1mStep[0m  [80/84], [94mLoss[0m : 2.47889

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.312, [92mTest[0m: 2.318, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.84237
[1mStep[0m  [8/84], [94mLoss[0m : 2.23374
[1mStep[0m  [16/84], [94mLoss[0m : 2.17798
[1mStep[0m  [24/84], [94mLoss[0m : 2.20660
[1mStep[0m  [32/84], [94mLoss[0m : 2.21200
[1mStep[0m  [40/84], [94mLoss[0m : 2.42202
[1mStep[0m  [48/84], [94mLoss[0m : 2.30437
[1mStep[0m  [56/84], [94mLoss[0m : 2.39633
[1mStep[0m  [64/84], [94mLoss[0m : 2.42277
[1mStep[0m  [72/84], [94mLoss[0m : 2.33159
[1mStep[0m  [80/84], [94mLoss[0m : 2.24426

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.220, [92mTest[0m: 2.344, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.06376
[1mStep[0m  [8/84], [94mLoss[0m : 2.06200
[1mStep[0m  [16/84], [94mLoss[0m : 2.17129
[1mStep[0m  [24/84], [94mLoss[0m : 2.11108
[1mStep[0m  [32/84], [94mLoss[0m : 2.45459
[1mStep[0m  [40/84], [94mLoss[0m : 2.13049
[1mStep[0m  [48/84], [94mLoss[0m : 2.34751
[1mStep[0m  [56/84], [94mLoss[0m : 2.19754
[1mStep[0m  [64/84], [94mLoss[0m : 1.93782
[1mStep[0m  [72/84], [94mLoss[0m : 2.29949
[1mStep[0m  [80/84], [94mLoss[0m : 1.91537

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.161, [92mTest[0m: 2.362, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.91485
[1mStep[0m  [8/84], [94mLoss[0m : 1.87387
[1mStep[0m  [16/84], [94mLoss[0m : 1.88965
[1mStep[0m  [24/84], [94mLoss[0m : 2.48427
[1mStep[0m  [32/84], [94mLoss[0m : 2.12564
[1mStep[0m  [40/84], [94mLoss[0m : 2.08734
[1mStep[0m  [48/84], [94mLoss[0m : 1.85209
[1mStep[0m  [56/84], [94mLoss[0m : 2.05446
[1mStep[0m  [64/84], [94mLoss[0m : 2.02263
[1mStep[0m  [72/84], [94mLoss[0m : 2.15592
[1mStep[0m  [80/84], [94mLoss[0m : 2.43173

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.082, [92mTest[0m: 2.442, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.93589
[1mStep[0m  [8/84], [94mLoss[0m : 1.92891
[1mStep[0m  [16/84], [94mLoss[0m : 1.75927
[1mStep[0m  [24/84], [94mLoss[0m : 1.97082
[1mStep[0m  [32/84], [94mLoss[0m : 2.04316
[1mStep[0m  [40/84], [94mLoss[0m : 2.29195
[1mStep[0m  [48/84], [94mLoss[0m : 2.02043
[1mStep[0m  [56/84], [94mLoss[0m : 2.13803
[1mStep[0m  [64/84], [94mLoss[0m : 2.04142
[1mStep[0m  [72/84], [94mLoss[0m : 2.06387
[1mStep[0m  [80/84], [94mLoss[0m : 2.02439

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.030, [92mTest[0m: 2.389, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.81305
[1mStep[0m  [8/84], [94mLoss[0m : 1.91865
[1mStep[0m  [16/84], [94mLoss[0m : 1.89394
[1mStep[0m  [24/84], [94mLoss[0m : 1.89790
[1mStep[0m  [32/84], [94mLoss[0m : 1.97526
[1mStep[0m  [40/84], [94mLoss[0m : 2.04453
[1mStep[0m  [48/84], [94mLoss[0m : 1.99166
[1mStep[0m  [56/84], [94mLoss[0m : 1.72190
[1mStep[0m  [64/84], [94mLoss[0m : 1.72579
[1mStep[0m  [72/84], [94mLoss[0m : 1.81021
[1mStep[0m  [80/84], [94mLoss[0m : 1.94323

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.949, [92mTest[0m: 2.463, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.89574
[1mStep[0m  [8/84], [94mLoss[0m : 2.02090
[1mStep[0m  [16/84], [94mLoss[0m : 1.97800
[1mStep[0m  [24/84], [94mLoss[0m : 1.81180
[1mStep[0m  [32/84], [94mLoss[0m : 1.77537
[1mStep[0m  [40/84], [94mLoss[0m : 1.76653
[1mStep[0m  [48/84], [94mLoss[0m : 1.76073
[1mStep[0m  [56/84], [94mLoss[0m : 1.91144
[1mStep[0m  [64/84], [94mLoss[0m : 2.24558
[1mStep[0m  [72/84], [94mLoss[0m : 1.94638
[1mStep[0m  [80/84], [94mLoss[0m : 1.59129

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.917, [92mTest[0m: 2.628, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.63105
[1mStep[0m  [8/84], [94mLoss[0m : 1.82187
[1mStep[0m  [16/84], [94mLoss[0m : 2.19800
[1mStep[0m  [24/84], [94mLoss[0m : 1.57551
[1mStep[0m  [32/84], [94mLoss[0m : 1.74633
[1mStep[0m  [40/84], [94mLoss[0m : 1.75443
[1mStep[0m  [48/84], [94mLoss[0m : 2.19026
[1mStep[0m  [56/84], [94mLoss[0m : 1.91726
[1mStep[0m  [64/84], [94mLoss[0m : 1.85114
[1mStep[0m  [72/84], [94mLoss[0m : 1.76289
[1mStep[0m  [80/84], [94mLoss[0m : 1.84736

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.846, [92mTest[0m: 2.421, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.80934
[1mStep[0m  [8/84], [94mLoss[0m : 1.84556
[1mStep[0m  [16/84], [94mLoss[0m : 1.79153
[1mStep[0m  [24/84], [94mLoss[0m : 1.75406
[1mStep[0m  [32/84], [94mLoss[0m : 1.87437
[1mStep[0m  [40/84], [94mLoss[0m : 2.03797
[1mStep[0m  [48/84], [94mLoss[0m : 1.88517
[1mStep[0m  [56/84], [94mLoss[0m : 1.74595
[1mStep[0m  [64/84], [94mLoss[0m : 1.99064
[1mStep[0m  [72/84], [94mLoss[0m : 1.86227
[1mStep[0m  [80/84], [94mLoss[0m : 1.98326

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.763, [92mTest[0m: 2.466, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.70819
[1mStep[0m  [8/84], [94mLoss[0m : 1.72199
[1mStep[0m  [16/84], [94mLoss[0m : 1.62749
[1mStep[0m  [24/84], [94mLoss[0m : 1.85692
[1mStep[0m  [32/84], [94mLoss[0m : 1.80652
[1mStep[0m  [40/84], [94mLoss[0m : 1.58978
[1mStep[0m  [48/84], [94mLoss[0m : 1.87080
[1mStep[0m  [56/84], [94mLoss[0m : 1.91109
[1mStep[0m  [64/84], [94mLoss[0m : 1.67046
[1mStep[0m  [72/84], [94mLoss[0m : 1.80270
[1mStep[0m  [80/84], [94mLoss[0m : 1.89572

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.737, [92mTest[0m: 2.484, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.67320
[1mStep[0m  [8/84], [94mLoss[0m : 1.59840
[1mStep[0m  [16/84], [94mLoss[0m : 1.70209
[1mStep[0m  [24/84], [94mLoss[0m : 1.75054
[1mStep[0m  [32/84], [94mLoss[0m : 1.66143
[1mStep[0m  [40/84], [94mLoss[0m : 1.53259
[1mStep[0m  [48/84], [94mLoss[0m : 1.64977
[1mStep[0m  [56/84], [94mLoss[0m : 1.70577
[1mStep[0m  [64/84], [94mLoss[0m : 1.65328
[1mStep[0m  [72/84], [94mLoss[0m : 1.71020
[1mStep[0m  [80/84], [94mLoss[0m : 1.73044

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.723, [92mTest[0m: 2.641, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.69321
[1mStep[0m  [8/84], [94mLoss[0m : 1.33812
[1mStep[0m  [16/84], [94mLoss[0m : 1.57989
[1mStep[0m  [24/84], [94mLoss[0m : 1.71678
[1mStep[0m  [32/84], [94mLoss[0m : 1.41561
[1mStep[0m  [40/84], [94mLoss[0m : 1.82994
[1mStep[0m  [48/84], [94mLoss[0m : 1.98817
[1mStep[0m  [56/84], [94mLoss[0m : 1.46930
[1mStep[0m  [64/84], [94mLoss[0m : 1.72682
[1mStep[0m  [72/84], [94mLoss[0m : 1.90291
[1mStep[0m  [80/84], [94mLoss[0m : 1.72093

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.673, [92mTest[0m: 2.507, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.80082
[1mStep[0m  [8/84], [94mLoss[0m : 1.75099
[1mStep[0m  [16/84], [94mLoss[0m : 1.35798
[1mStep[0m  [24/84], [94mLoss[0m : 1.56973
[1mStep[0m  [32/84], [94mLoss[0m : 1.83680
[1mStep[0m  [40/84], [94mLoss[0m : 1.42411
[1mStep[0m  [48/84], [94mLoss[0m : 1.64846
[1mStep[0m  [56/84], [94mLoss[0m : 1.62257
[1mStep[0m  [64/84], [94mLoss[0m : 1.43357
[1mStep[0m  [72/84], [94mLoss[0m : 1.55460
[1mStep[0m  [80/84], [94mLoss[0m : 1.61086

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.635, [92mTest[0m: 2.477, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.53104
[1mStep[0m  [8/84], [94mLoss[0m : 1.64438
[1mStep[0m  [16/84], [94mLoss[0m : 1.45482
[1mStep[0m  [24/84], [94mLoss[0m : 1.68521
[1mStep[0m  [32/84], [94mLoss[0m : 1.76301
[1mStep[0m  [40/84], [94mLoss[0m : 1.39021
[1mStep[0m  [48/84], [94mLoss[0m : 1.67157
[1mStep[0m  [56/84], [94mLoss[0m : 1.71784
[1mStep[0m  [64/84], [94mLoss[0m : 1.60647
[1mStep[0m  [72/84], [94mLoss[0m : 1.46731
[1mStep[0m  [80/84], [94mLoss[0m : 1.57654

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.574, [92mTest[0m: 2.552, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.49216
[1mStep[0m  [8/84], [94mLoss[0m : 1.68951
[1mStep[0m  [16/84], [94mLoss[0m : 1.48339
[1mStep[0m  [24/84], [94mLoss[0m : 1.52095
[1mStep[0m  [32/84], [94mLoss[0m : 1.56219
[1mStep[0m  [40/84], [94mLoss[0m : 1.72153
[1mStep[0m  [48/84], [94mLoss[0m : 1.42480
[1mStep[0m  [56/84], [94mLoss[0m : 1.86988
[1mStep[0m  [64/84], [94mLoss[0m : 1.73371
[1mStep[0m  [72/84], [94mLoss[0m : 1.50035
[1mStep[0m  [80/84], [94mLoss[0m : 1.72153

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.564, [92mTest[0m: 2.478, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.44495
[1mStep[0m  [8/84], [94mLoss[0m : 1.42819
[1mStep[0m  [16/84], [94mLoss[0m : 1.29329
[1mStep[0m  [24/84], [94mLoss[0m : 1.58173
[1mStep[0m  [32/84], [94mLoss[0m : 1.53985
[1mStep[0m  [40/84], [94mLoss[0m : 1.45301
[1mStep[0m  [48/84], [94mLoss[0m : 1.47921
[1mStep[0m  [56/84], [94mLoss[0m : 1.59306
[1mStep[0m  [64/84], [94mLoss[0m : 1.33324
[1mStep[0m  [72/84], [94mLoss[0m : 1.45528
[1mStep[0m  [80/84], [94mLoss[0m : 1.59490

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.514, [92mTest[0m: 2.487, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.51298
[1mStep[0m  [8/84], [94mLoss[0m : 1.54240
[1mStep[0m  [16/84], [94mLoss[0m : 1.31430
[1mStep[0m  [24/84], [94mLoss[0m : 1.63723
[1mStep[0m  [32/84], [94mLoss[0m : 1.50348
[1mStep[0m  [40/84], [94mLoss[0m : 1.45390
[1mStep[0m  [48/84], [94mLoss[0m : 1.37272
[1mStep[0m  [56/84], [94mLoss[0m : 1.36591
[1mStep[0m  [64/84], [94mLoss[0m : 1.33898
[1mStep[0m  [72/84], [94mLoss[0m : 1.61456
[1mStep[0m  [80/84], [94mLoss[0m : 1.39094

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.473, [92mTest[0m: 2.577, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.33994
[1mStep[0m  [8/84], [94mLoss[0m : 1.39195
[1mStep[0m  [16/84], [94mLoss[0m : 1.57541
[1mStep[0m  [24/84], [94mLoss[0m : 1.59431
[1mStep[0m  [32/84], [94mLoss[0m : 1.41405
[1mStep[0m  [40/84], [94mLoss[0m : 1.29690
[1mStep[0m  [48/84], [94mLoss[0m : 1.56350
[1mStep[0m  [56/84], [94mLoss[0m : 1.48613
[1mStep[0m  [64/84], [94mLoss[0m : 1.46378
[1mStep[0m  [72/84], [94mLoss[0m : 1.41089
[1mStep[0m  [80/84], [94mLoss[0m : 1.46406

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.465, [92mTest[0m: 2.528, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.41081
[1mStep[0m  [8/84], [94mLoss[0m : 1.37553
[1mStep[0m  [16/84], [94mLoss[0m : 1.33734
[1mStep[0m  [24/84], [94mLoss[0m : 1.43340
[1mStep[0m  [32/84], [94mLoss[0m : 1.20054
[1mStep[0m  [40/84], [94mLoss[0m : 1.43096
[1mStep[0m  [48/84], [94mLoss[0m : 1.54630
[1mStep[0m  [56/84], [94mLoss[0m : 1.39144
[1mStep[0m  [64/84], [94mLoss[0m : 1.57192
[1mStep[0m  [72/84], [94mLoss[0m : 1.28930
[1mStep[0m  [80/84], [94mLoss[0m : 1.55346

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.443, [92mTest[0m: 2.504, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.28362
[1mStep[0m  [8/84], [94mLoss[0m : 1.33923
[1mStep[0m  [16/84], [94mLoss[0m : 1.23513
[1mStep[0m  [24/84], [94mLoss[0m : 1.33607
[1mStep[0m  [32/84], [94mLoss[0m : 1.27051
[1mStep[0m  [40/84], [94mLoss[0m : 1.40841
[1mStep[0m  [48/84], [94mLoss[0m : 1.56666
[1mStep[0m  [56/84], [94mLoss[0m : 1.24678
[1mStep[0m  [64/84], [94mLoss[0m : 1.23471
[1mStep[0m  [72/84], [94mLoss[0m : 1.36023
[1mStep[0m  [80/84], [94mLoss[0m : 1.29017

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.397, [92mTest[0m: 2.514, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.36846
[1mStep[0m  [8/84], [94mLoss[0m : 1.32268
[1mStep[0m  [16/84], [94mLoss[0m : 1.61638
[1mStep[0m  [24/84], [94mLoss[0m : 1.39181
[1mStep[0m  [32/84], [94mLoss[0m : 1.41589
[1mStep[0m  [40/84], [94mLoss[0m : 1.45587
[1mStep[0m  [48/84], [94mLoss[0m : 1.41067
[1mStep[0m  [56/84], [94mLoss[0m : 1.52228
[1mStep[0m  [64/84], [94mLoss[0m : 1.26337
[1mStep[0m  [72/84], [94mLoss[0m : 1.42422
[1mStep[0m  [80/84], [94mLoss[0m : 1.36078

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.371, [92mTest[0m: 2.507, [96mlr[0m: 0.004545
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 21 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.621
====================================

Phase 2 - Evaluation MAE:  2.6214803201811656
MAE score P1      2.346525
MAE score P2       2.62148
loss              1.370933
learning_rate      0.00505
batch_size             128
hidden_sizes         [300]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping        True
dropout                0.4
momentum               0.9
weight_decay         0.001
Name: 14, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/169], [94mLoss[0m : 11.08588
[1mStep[0m  [16/169], [94mLoss[0m : 9.75264
[1mStep[0m  [32/169], [94mLoss[0m : 9.65079
[1mStep[0m  [48/169], [94mLoss[0m : 7.64080
[1mStep[0m  [64/169], [94mLoss[0m : 5.54040
[1mStep[0m  [80/169], [94mLoss[0m : 4.86317
[1mStep[0m  [96/169], [94mLoss[0m : 4.08052
[1mStep[0m  [112/169], [94mLoss[0m : 3.45805
[1mStep[0m  [128/169], [94mLoss[0m : 2.69205
[1mStep[0m  [144/169], [94mLoss[0m : 2.80655
[1mStep[0m  [160/169], [94mLoss[0m : 3.08350

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 5.563, [92mTest[0m: 11.073, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.94929
[1mStep[0m  [16/169], [94mLoss[0m : 2.44856
[1mStep[0m  [32/169], [94mLoss[0m : 2.80601
[1mStep[0m  [48/169], [94mLoss[0m : 2.25271
[1mStep[0m  [64/169], [94mLoss[0m : 2.81520
[1mStep[0m  [80/169], [94mLoss[0m : 2.50757
[1mStep[0m  [96/169], [94mLoss[0m : 2.62471
[1mStep[0m  [112/169], [94mLoss[0m : 2.24108
[1mStep[0m  [128/169], [94mLoss[0m : 2.84407
[1mStep[0m  [144/169], [94mLoss[0m : 2.65148
[1mStep[0m  [160/169], [94mLoss[0m : 2.41799

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.595, [92mTest[0m: 2.416, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.75716
[1mStep[0m  [16/169], [94mLoss[0m : 2.08203
[1mStep[0m  [32/169], [94mLoss[0m : 2.57681
[1mStep[0m  [48/169], [94mLoss[0m : 2.29478
[1mStep[0m  [64/169], [94mLoss[0m : 2.57002
[1mStep[0m  [80/169], [94mLoss[0m : 2.75309
[1mStep[0m  [96/169], [94mLoss[0m : 2.33829
[1mStep[0m  [112/169], [94mLoss[0m : 2.43075
[1mStep[0m  [128/169], [94mLoss[0m : 2.65933
[1mStep[0m  [144/169], [94mLoss[0m : 2.39959
[1mStep[0m  [160/169], [94mLoss[0m : 2.55210

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.408, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.08107
[1mStep[0m  [16/169], [94mLoss[0m : 2.42452
[1mStep[0m  [32/169], [94mLoss[0m : 2.24313
[1mStep[0m  [48/169], [94mLoss[0m : 2.19842
[1mStep[0m  [64/169], [94mLoss[0m : 2.76360
[1mStep[0m  [80/169], [94mLoss[0m : 2.30843
[1mStep[0m  [96/169], [94mLoss[0m : 2.85915
[1mStep[0m  [112/169], [94mLoss[0m : 2.24702
[1mStep[0m  [128/169], [94mLoss[0m : 3.04547
[1mStep[0m  [144/169], [94mLoss[0m : 2.30996
[1mStep[0m  [160/169], [94mLoss[0m : 2.93793

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.423, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.63987
[1mStep[0m  [16/169], [94mLoss[0m : 2.86932
[1mStep[0m  [32/169], [94mLoss[0m : 2.20175
[1mStep[0m  [48/169], [94mLoss[0m : 2.35605
[1mStep[0m  [64/169], [94mLoss[0m : 2.39803
[1mStep[0m  [80/169], [94mLoss[0m : 2.78684
[1mStep[0m  [96/169], [94mLoss[0m : 2.79104
[1mStep[0m  [112/169], [94mLoss[0m : 2.53126
[1mStep[0m  [128/169], [94mLoss[0m : 2.36473
[1mStep[0m  [144/169], [94mLoss[0m : 2.83353
[1mStep[0m  [160/169], [94mLoss[0m : 2.43463

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.335, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.51177
[1mStep[0m  [16/169], [94mLoss[0m : 2.24407
[1mStep[0m  [32/169], [94mLoss[0m : 2.35409
[1mStep[0m  [48/169], [94mLoss[0m : 2.52477
[1mStep[0m  [64/169], [94mLoss[0m : 2.72521
[1mStep[0m  [80/169], [94mLoss[0m : 2.13765
[1mStep[0m  [96/169], [94mLoss[0m : 2.36863
[1mStep[0m  [112/169], [94mLoss[0m : 2.55375
[1mStep[0m  [128/169], [94mLoss[0m : 2.06973
[1mStep[0m  [144/169], [94mLoss[0m : 2.68965
[1mStep[0m  [160/169], [94mLoss[0m : 2.38117

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.342, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.42895
[1mStep[0m  [16/169], [94mLoss[0m : 2.14022
[1mStep[0m  [32/169], [94mLoss[0m : 2.42152
[1mStep[0m  [48/169], [94mLoss[0m : 2.17638
[1mStep[0m  [64/169], [94mLoss[0m : 2.37328
[1mStep[0m  [80/169], [94mLoss[0m : 2.22658
[1mStep[0m  [96/169], [94mLoss[0m : 2.36448
[1mStep[0m  [112/169], [94mLoss[0m : 2.89509
[1mStep[0m  [128/169], [94mLoss[0m : 2.08374
[1mStep[0m  [144/169], [94mLoss[0m : 1.85877
[1mStep[0m  [160/169], [94mLoss[0m : 2.78384

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.347, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.11068
[1mStep[0m  [16/169], [94mLoss[0m : 2.39583
[1mStep[0m  [32/169], [94mLoss[0m : 2.08535
[1mStep[0m  [48/169], [94mLoss[0m : 2.23618
[1mStep[0m  [64/169], [94mLoss[0m : 2.37409
[1mStep[0m  [80/169], [94mLoss[0m : 2.36792
[1mStep[0m  [96/169], [94mLoss[0m : 2.30295
[1mStep[0m  [112/169], [94mLoss[0m : 2.44778
[1mStep[0m  [128/169], [94mLoss[0m : 2.31930
[1mStep[0m  [144/169], [94mLoss[0m : 2.71065
[1mStep[0m  [160/169], [94mLoss[0m : 2.45423

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.319, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.90474
[1mStep[0m  [16/169], [94mLoss[0m : 2.22971
[1mStep[0m  [32/169], [94mLoss[0m : 2.08446
[1mStep[0m  [48/169], [94mLoss[0m : 2.31092
[1mStep[0m  [64/169], [94mLoss[0m : 2.34701
[1mStep[0m  [80/169], [94mLoss[0m : 2.85905
[1mStep[0m  [96/169], [94mLoss[0m : 2.33631
[1mStep[0m  [112/169], [94mLoss[0m : 2.73752
[1mStep[0m  [128/169], [94mLoss[0m : 2.17501
[1mStep[0m  [144/169], [94mLoss[0m : 2.33451
[1mStep[0m  [160/169], [94mLoss[0m : 2.56835

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.345, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.43065
[1mStep[0m  [16/169], [94mLoss[0m : 2.55376
[1mStep[0m  [32/169], [94mLoss[0m : 2.68612
[1mStep[0m  [48/169], [94mLoss[0m : 2.71556
[1mStep[0m  [64/169], [94mLoss[0m : 2.70056
[1mStep[0m  [80/169], [94mLoss[0m : 2.41328
[1mStep[0m  [96/169], [94mLoss[0m : 2.64907
[1mStep[0m  [112/169], [94mLoss[0m : 2.51140
[1mStep[0m  [128/169], [94mLoss[0m : 2.53760
[1mStep[0m  [144/169], [94mLoss[0m : 2.31442
[1mStep[0m  [160/169], [94mLoss[0m : 2.76161

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.333, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.45409
[1mStep[0m  [16/169], [94mLoss[0m : 2.47212
[1mStep[0m  [32/169], [94mLoss[0m : 2.77644
[1mStep[0m  [48/169], [94mLoss[0m : 2.57653
[1mStep[0m  [64/169], [94mLoss[0m : 2.34388
[1mStep[0m  [80/169], [94mLoss[0m : 2.51451
[1mStep[0m  [96/169], [94mLoss[0m : 2.31411
[1mStep[0m  [112/169], [94mLoss[0m : 2.79141
[1mStep[0m  [128/169], [94mLoss[0m : 2.89249
[1mStep[0m  [144/169], [94mLoss[0m : 2.28805
[1mStep[0m  [160/169], [94mLoss[0m : 2.75043

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.363, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.88186
[1mStep[0m  [16/169], [94mLoss[0m : 2.07817
[1mStep[0m  [32/169], [94mLoss[0m : 2.15926
[1mStep[0m  [48/169], [94mLoss[0m : 2.39947
[1mStep[0m  [64/169], [94mLoss[0m : 2.37630
[1mStep[0m  [80/169], [94mLoss[0m : 2.37687
[1mStep[0m  [96/169], [94mLoss[0m : 2.40639
[1mStep[0m  [112/169], [94mLoss[0m : 2.24145
[1mStep[0m  [128/169], [94mLoss[0m : 3.06410
[1mStep[0m  [144/169], [94mLoss[0m : 2.19985
[1mStep[0m  [160/169], [94mLoss[0m : 1.96222

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.359, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.40411
[1mStep[0m  [16/169], [94mLoss[0m : 2.52780
[1mStep[0m  [32/169], [94mLoss[0m : 2.20481
[1mStep[0m  [48/169], [94mLoss[0m : 2.42073
[1mStep[0m  [64/169], [94mLoss[0m : 2.68133
[1mStep[0m  [80/169], [94mLoss[0m : 2.45907
[1mStep[0m  [96/169], [94mLoss[0m : 2.69725
[1mStep[0m  [112/169], [94mLoss[0m : 2.36253
[1mStep[0m  [128/169], [94mLoss[0m : 2.46050
[1mStep[0m  [144/169], [94mLoss[0m : 2.82050
[1mStep[0m  [160/169], [94mLoss[0m : 2.49421

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.342, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.08129
[1mStep[0m  [16/169], [94mLoss[0m : 2.28260
[1mStep[0m  [32/169], [94mLoss[0m : 2.69713
[1mStep[0m  [48/169], [94mLoss[0m : 2.41238
[1mStep[0m  [64/169], [94mLoss[0m : 2.16037
[1mStep[0m  [80/169], [94mLoss[0m : 2.53168
[1mStep[0m  [96/169], [94mLoss[0m : 2.20910
[1mStep[0m  [112/169], [94mLoss[0m : 2.75463
[1mStep[0m  [128/169], [94mLoss[0m : 2.76426
[1mStep[0m  [144/169], [94mLoss[0m : 2.62443
[1mStep[0m  [160/169], [94mLoss[0m : 2.48589

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.381, [92mTest[0m: 2.359, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.56514
[1mStep[0m  [16/169], [94mLoss[0m : 2.32686
[1mStep[0m  [32/169], [94mLoss[0m : 2.18253
[1mStep[0m  [48/169], [94mLoss[0m : 2.65275
[1mStep[0m  [64/169], [94mLoss[0m : 2.42601
[1mStep[0m  [80/169], [94mLoss[0m : 2.32231
[1mStep[0m  [96/169], [94mLoss[0m : 2.49259
[1mStep[0m  [112/169], [94mLoss[0m : 2.59990
[1mStep[0m  [128/169], [94mLoss[0m : 2.07967
[1mStep[0m  [144/169], [94mLoss[0m : 2.38426
[1mStep[0m  [160/169], [94mLoss[0m : 2.62972

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.369, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.21475
[1mStep[0m  [16/169], [94mLoss[0m : 2.52775
[1mStep[0m  [32/169], [94mLoss[0m : 2.72787
[1mStep[0m  [48/169], [94mLoss[0m : 2.60320
[1mStep[0m  [64/169], [94mLoss[0m : 2.06569
[1mStep[0m  [80/169], [94mLoss[0m : 1.99500
[1mStep[0m  [96/169], [94mLoss[0m : 2.71261
[1mStep[0m  [112/169], [94mLoss[0m : 2.35417
[1mStep[0m  [128/169], [94mLoss[0m : 2.36985
[1mStep[0m  [144/169], [94mLoss[0m : 2.69633
[1mStep[0m  [160/169], [94mLoss[0m : 2.34856

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.390, [92mTest[0m: 2.340, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.63855
[1mStep[0m  [16/169], [94mLoss[0m : 2.60539
[1mStep[0m  [32/169], [94mLoss[0m : 2.47935
[1mStep[0m  [48/169], [94mLoss[0m : 2.33877
[1mStep[0m  [64/169], [94mLoss[0m : 2.00391
[1mStep[0m  [80/169], [94mLoss[0m : 2.39073
[1mStep[0m  [96/169], [94mLoss[0m : 2.71458
[1mStep[0m  [112/169], [94mLoss[0m : 2.42392
[1mStep[0m  [128/169], [94mLoss[0m : 2.52440
[1mStep[0m  [144/169], [94mLoss[0m : 1.90072
[1mStep[0m  [160/169], [94mLoss[0m : 2.46215

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.370, [92mTest[0m: 2.343, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.46444
[1mStep[0m  [16/169], [94mLoss[0m : 2.13284
[1mStep[0m  [32/169], [94mLoss[0m : 2.31893
[1mStep[0m  [48/169], [94mLoss[0m : 2.10537
[1mStep[0m  [64/169], [94mLoss[0m : 2.21874
[1mStep[0m  [80/169], [94mLoss[0m : 2.78109
[1mStep[0m  [96/169], [94mLoss[0m : 2.31341
[1mStep[0m  [112/169], [94mLoss[0m : 2.62246
[1mStep[0m  [128/169], [94mLoss[0m : 2.65290
[1mStep[0m  [144/169], [94mLoss[0m : 2.60930
[1mStep[0m  [160/169], [94mLoss[0m : 2.29879

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.371, [92mTest[0m: 2.341, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.31585
[1mStep[0m  [16/169], [94mLoss[0m : 2.75653
[1mStep[0m  [32/169], [94mLoss[0m : 2.09811
[1mStep[0m  [48/169], [94mLoss[0m : 2.18851
[1mStep[0m  [64/169], [94mLoss[0m : 2.26439
[1mStep[0m  [80/169], [94mLoss[0m : 2.38501
[1mStep[0m  [96/169], [94mLoss[0m : 2.69077
[1mStep[0m  [112/169], [94mLoss[0m : 2.20331
[1mStep[0m  [128/169], [94mLoss[0m : 2.46277
[1mStep[0m  [144/169], [94mLoss[0m : 2.35799
[1mStep[0m  [160/169], [94mLoss[0m : 2.38117

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.383, [92mTest[0m: 2.343, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.15170
[1mStep[0m  [16/169], [94mLoss[0m : 2.60231
[1mStep[0m  [32/169], [94mLoss[0m : 2.05260
[1mStep[0m  [48/169], [94mLoss[0m : 2.23543
[1mStep[0m  [64/169], [94mLoss[0m : 2.43842
[1mStep[0m  [80/169], [94mLoss[0m : 2.15610
[1mStep[0m  [96/169], [94mLoss[0m : 2.12647
[1mStep[0m  [112/169], [94mLoss[0m : 2.24704
[1mStep[0m  [128/169], [94mLoss[0m : 2.41175
[1mStep[0m  [144/169], [94mLoss[0m : 2.70630
[1mStep[0m  [160/169], [94mLoss[0m : 2.20894

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.371, [92mTest[0m: 2.338, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.44285
[1mStep[0m  [16/169], [94mLoss[0m : 2.37263
[1mStep[0m  [32/169], [94mLoss[0m : 2.19551
[1mStep[0m  [48/169], [94mLoss[0m : 2.42427
[1mStep[0m  [64/169], [94mLoss[0m : 2.37514
[1mStep[0m  [80/169], [94mLoss[0m : 2.22964
[1mStep[0m  [96/169], [94mLoss[0m : 2.58271
[1mStep[0m  [112/169], [94mLoss[0m : 2.49809
[1mStep[0m  [128/169], [94mLoss[0m : 2.40454
[1mStep[0m  [144/169], [94mLoss[0m : 2.39708
[1mStep[0m  [160/169], [94mLoss[0m : 1.94696

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.368, [92mTest[0m: 2.333, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.92206
[1mStep[0m  [16/169], [94mLoss[0m : 2.54651
[1mStep[0m  [32/169], [94mLoss[0m : 2.10811
[1mStep[0m  [48/169], [94mLoss[0m : 1.87902
[1mStep[0m  [64/169], [94mLoss[0m : 2.26426
[1mStep[0m  [80/169], [94mLoss[0m : 2.74456
[1mStep[0m  [96/169], [94mLoss[0m : 2.48384
[1mStep[0m  [112/169], [94mLoss[0m : 2.70180
[1mStep[0m  [128/169], [94mLoss[0m : 1.81064
[1mStep[0m  [144/169], [94mLoss[0m : 2.10220
[1mStep[0m  [160/169], [94mLoss[0m : 2.05426

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.361, [92mTest[0m: 2.342, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.38952
[1mStep[0m  [16/169], [94mLoss[0m : 2.23479
[1mStep[0m  [32/169], [94mLoss[0m : 2.67718
[1mStep[0m  [48/169], [94mLoss[0m : 2.41118
[1mStep[0m  [64/169], [94mLoss[0m : 1.92791
[1mStep[0m  [80/169], [94mLoss[0m : 2.16721
[1mStep[0m  [96/169], [94mLoss[0m : 2.17782
[1mStep[0m  [112/169], [94mLoss[0m : 2.04016
[1mStep[0m  [128/169], [94mLoss[0m : 2.30967
[1mStep[0m  [144/169], [94mLoss[0m : 2.52684
[1mStep[0m  [160/169], [94mLoss[0m : 2.46495

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.360, [92mTest[0m: 2.323, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.15820
[1mStep[0m  [16/169], [94mLoss[0m : 2.43423
[1mStep[0m  [32/169], [94mLoss[0m : 2.36325
[1mStep[0m  [48/169], [94mLoss[0m : 2.33301
[1mStep[0m  [64/169], [94mLoss[0m : 2.41151
[1mStep[0m  [80/169], [94mLoss[0m : 2.39151
[1mStep[0m  [96/169], [94mLoss[0m : 2.36815
[1mStep[0m  [112/169], [94mLoss[0m : 3.04502
[1mStep[0m  [128/169], [94mLoss[0m : 2.70456
[1mStep[0m  [144/169], [94mLoss[0m : 2.52115
[1mStep[0m  [160/169], [94mLoss[0m : 1.94922

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.377, [92mTest[0m: 2.319, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.57099
[1mStep[0m  [16/169], [94mLoss[0m : 2.27546
[1mStep[0m  [32/169], [94mLoss[0m : 2.20640
[1mStep[0m  [48/169], [94mLoss[0m : 2.26747
[1mStep[0m  [64/169], [94mLoss[0m : 2.42932
[1mStep[0m  [80/169], [94mLoss[0m : 2.53815
[1mStep[0m  [96/169], [94mLoss[0m : 2.01883
[1mStep[0m  [112/169], [94mLoss[0m : 2.43015
[1mStep[0m  [128/169], [94mLoss[0m : 2.56766
[1mStep[0m  [144/169], [94mLoss[0m : 2.48109
[1mStep[0m  [160/169], [94mLoss[0m : 2.53916

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.360, [92mTest[0m: 2.341, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.70718
[1mStep[0m  [16/169], [94mLoss[0m : 2.56195
[1mStep[0m  [32/169], [94mLoss[0m : 2.37397
[1mStep[0m  [48/169], [94mLoss[0m : 2.42682
[1mStep[0m  [64/169], [94mLoss[0m : 2.43165
[1mStep[0m  [80/169], [94mLoss[0m : 2.27979
[1mStep[0m  [96/169], [94mLoss[0m : 2.85924
[1mStep[0m  [112/169], [94mLoss[0m : 2.23097
[1mStep[0m  [128/169], [94mLoss[0m : 2.15471
[1mStep[0m  [144/169], [94mLoss[0m : 2.85389
[1mStep[0m  [160/169], [94mLoss[0m : 2.11629

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.346, [92mTest[0m: 2.326, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.23137
[1mStep[0m  [16/169], [94mLoss[0m : 2.57649
[1mStep[0m  [32/169], [94mLoss[0m : 2.10508
[1mStep[0m  [48/169], [94mLoss[0m : 2.27877
[1mStep[0m  [64/169], [94mLoss[0m : 1.94431
[1mStep[0m  [80/169], [94mLoss[0m : 2.26358
[1mStep[0m  [96/169], [94mLoss[0m : 2.24441
[1mStep[0m  [112/169], [94mLoss[0m : 1.79802
[1mStep[0m  [128/169], [94mLoss[0m : 2.17072
[1mStep[0m  [144/169], [94mLoss[0m : 2.34594
[1mStep[0m  [160/169], [94mLoss[0m : 2.71686

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.353, [92mTest[0m: 2.323, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.41217
[1mStep[0m  [16/169], [94mLoss[0m : 2.19114
[1mStep[0m  [32/169], [94mLoss[0m : 2.38499
[1mStep[0m  [48/169], [94mLoss[0m : 2.52310
[1mStep[0m  [64/169], [94mLoss[0m : 2.15708
[1mStep[0m  [80/169], [94mLoss[0m : 2.08658
[1mStep[0m  [96/169], [94mLoss[0m : 2.29412
[1mStep[0m  [112/169], [94mLoss[0m : 2.50884
[1mStep[0m  [128/169], [94mLoss[0m : 2.39839
[1mStep[0m  [144/169], [94mLoss[0m : 1.77074
[1mStep[0m  [160/169], [94mLoss[0m : 2.22849

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.346, [92mTest[0m: 2.321, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.86172
[1mStep[0m  [16/169], [94mLoss[0m : 2.35108
[1mStep[0m  [32/169], [94mLoss[0m : 2.58556
[1mStep[0m  [48/169], [94mLoss[0m : 2.24869
[1mStep[0m  [64/169], [94mLoss[0m : 2.25347
[1mStep[0m  [80/169], [94mLoss[0m : 1.99138
[1mStep[0m  [96/169], [94mLoss[0m : 2.22510
[1mStep[0m  [112/169], [94mLoss[0m : 2.42440
[1mStep[0m  [128/169], [94mLoss[0m : 2.29439
[1mStep[0m  [144/169], [94mLoss[0m : 2.15016
[1mStep[0m  [160/169], [94mLoss[0m : 2.42878

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.356, [92mTest[0m: 2.361, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.00036
[1mStep[0m  [16/169], [94mLoss[0m : 2.45329
[1mStep[0m  [32/169], [94mLoss[0m : 2.30932
[1mStep[0m  [48/169], [94mLoss[0m : 1.99677
[1mStep[0m  [64/169], [94mLoss[0m : 2.09632
[1mStep[0m  [80/169], [94mLoss[0m : 1.92748
[1mStep[0m  [96/169], [94mLoss[0m : 2.49096
[1mStep[0m  [112/169], [94mLoss[0m : 2.58342
[1mStep[0m  [128/169], [94mLoss[0m : 2.37679
[1mStep[0m  [144/169], [94mLoss[0m : 2.25167
[1mStep[0m  [160/169], [94mLoss[0m : 2.38617

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.326, [92mTest[0m: 2.333, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.339
====================================

Phase 1 - Evaluation MAE:  2.3385884293488095
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/169], [94mLoss[0m : 2.07087
[1mStep[0m  [16/169], [94mLoss[0m : 2.64542
[1mStep[0m  [32/169], [94mLoss[0m : 2.68007
[1mStep[0m  [48/169], [94mLoss[0m : 2.43479
[1mStep[0m  [64/169], [94mLoss[0m : 2.43190
[1mStep[0m  [80/169], [94mLoss[0m : 2.62106
[1mStep[0m  [96/169], [94mLoss[0m : 2.31762
[1mStep[0m  [112/169], [94mLoss[0m : 2.67016
[1mStep[0m  [128/169], [94mLoss[0m : 2.32056
[1mStep[0m  [144/169], [94mLoss[0m : 2.40698
[1mStep[0m  [160/169], [94mLoss[0m : 2.17054

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.340, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.37593
[1mStep[0m  [16/169], [94mLoss[0m : 1.85514
[1mStep[0m  [32/169], [94mLoss[0m : 2.50921
[1mStep[0m  [48/169], [94mLoss[0m : 2.22125
[1mStep[0m  [64/169], [94mLoss[0m : 2.75942
[1mStep[0m  [80/169], [94mLoss[0m : 2.04927
[1mStep[0m  [96/169], [94mLoss[0m : 2.45856
[1mStep[0m  [112/169], [94mLoss[0m : 2.45495
[1mStep[0m  [128/169], [94mLoss[0m : 2.36810
[1mStep[0m  [144/169], [94mLoss[0m : 2.47701
[1mStep[0m  [160/169], [94mLoss[0m : 2.26205

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.299, [92mTest[0m: 2.371, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.95632
[1mStep[0m  [16/169], [94mLoss[0m : 2.67053
[1mStep[0m  [32/169], [94mLoss[0m : 1.98059
[1mStep[0m  [48/169], [94mLoss[0m : 2.47490
[1mStep[0m  [64/169], [94mLoss[0m : 2.28991
[1mStep[0m  [80/169], [94mLoss[0m : 2.26822
[1mStep[0m  [96/169], [94mLoss[0m : 2.36297
[1mStep[0m  [112/169], [94mLoss[0m : 2.01012
[1mStep[0m  [128/169], [94mLoss[0m : 2.41798
[1mStep[0m  [144/169], [94mLoss[0m : 2.23476
[1mStep[0m  [160/169], [94mLoss[0m : 2.26863

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.216, [92mTest[0m: 2.382, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.99365
[1mStep[0m  [16/169], [94mLoss[0m : 2.15384
[1mStep[0m  [32/169], [94mLoss[0m : 1.99161
[1mStep[0m  [48/169], [94mLoss[0m : 2.02690
[1mStep[0m  [64/169], [94mLoss[0m : 2.20310
[1mStep[0m  [80/169], [94mLoss[0m : 2.44772
[1mStep[0m  [96/169], [94mLoss[0m : 2.38370
[1mStep[0m  [112/169], [94mLoss[0m : 2.59790
[1mStep[0m  [128/169], [94mLoss[0m : 2.32858
[1mStep[0m  [144/169], [94mLoss[0m : 2.11664
[1mStep[0m  [160/169], [94mLoss[0m : 2.19087

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.158, [92mTest[0m: 2.381, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.25627
[1mStep[0m  [16/169], [94mLoss[0m : 2.04772
[1mStep[0m  [32/169], [94mLoss[0m : 2.22039
[1mStep[0m  [48/169], [94mLoss[0m : 2.17290
[1mStep[0m  [64/169], [94mLoss[0m : 1.87503
[1mStep[0m  [80/169], [94mLoss[0m : 2.32845
[1mStep[0m  [96/169], [94mLoss[0m : 1.88994
[1mStep[0m  [112/169], [94mLoss[0m : 2.02692
[1mStep[0m  [128/169], [94mLoss[0m : 2.20156
[1mStep[0m  [144/169], [94mLoss[0m : 1.73500
[1mStep[0m  [160/169], [94mLoss[0m : 1.82343

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.087, [92mTest[0m: 2.377, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.25704
[1mStep[0m  [16/169], [94mLoss[0m : 1.73582
[1mStep[0m  [32/169], [94mLoss[0m : 2.15033
[1mStep[0m  [48/169], [94mLoss[0m : 2.07912
[1mStep[0m  [64/169], [94mLoss[0m : 1.81857
[1mStep[0m  [80/169], [94mLoss[0m : 2.02080
[1mStep[0m  [96/169], [94mLoss[0m : 2.19713
[1mStep[0m  [112/169], [94mLoss[0m : 1.99801
[1mStep[0m  [128/169], [94mLoss[0m : 2.00807
[1mStep[0m  [144/169], [94mLoss[0m : 2.30602
[1mStep[0m  [160/169], [94mLoss[0m : 1.84039

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.020, [92mTest[0m: 2.375, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.07850
[1mStep[0m  [16/169], [94mLoss[0m : 1.71369
[1mStep[0m  [32/169], [94mLoss[0m : 2.09663
[1mStep[0m  [48/169], [94mLoss[0m : 1.97419
[1mStep[0m  [64/169], [94mLoss[0m : 1.77751
[1mStep[0m  [80/169], [94mLoss[0m : 2.17863
[1mStep[0m  [96/169], [94mLoss[0m : 2.11506
[1mStep[0m  [112/169], [94mLoss[0m : 1.73491
[1mStep[0m  [128/169], [94mLoss[0m : 1.71430
[1mStep[0m  [144/169], [94mLoss[0m : 2.17053
[1mStep[0m  [160/169], [94mLoss[0m : 1.96152

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.939, [92mTest[0m: 2.406, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.47148
[1mStep[0m  [16/169], [94mLoss[0m : 1.73272
[1mStep[0m  [32/169], [94mLoss[0m : 1.76025
[1mStep[0m  [48/169], [94mLoss[0m : 2.51723
[1mStep[0m  [64/169], [94mLoss[0m : 1.98514
[1mStep[0m  [80/169], [94mLoss[0m : 2.02782
[1mStep[0m  [96/169], [94mLoss[0m : 1.94662
[1mStep[0m  [112/169], [94mLoss[0m : 2.06848
[1mStep[0m  [128/169], [94mLoss[0m : 1.84332
[1mStep[0m  [144/169], [94mLoss[0m : 1.85411
[1mStep[0m  [160/169], [94mLoss[0m : 2.00133

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.903, [92mTest[0m: 2.435, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.81840
[1mStep[0m  [16/169], [94mLoss[0m : 1.61289
[1mStep[0m  [32/169], [94mLoss[0m : 1.50634
[1mStep[0m  [48/169], [94mLoss[0m : 1.87737
[1mStep[0m  [64/169], [94mLoss[0m : 1.50552
[1mStep[0m  [80/169], [94mLoss[0m : 1.86892
[1mStep[0m  [96/169], [94mLoss[0m : 1.86604
[1mStep[0m  [112/169], [94mLoss[0m : 1.79644
[1mStep[0m  [128/169], [94mLoss[0m : 2.04415
[1mStep[0m  [144/169], [94mLoss[0m : 1.73996
[1mStep[0m  [160/169], [94mLoss[0m : 1.61845

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.852, [92mTest[0m: 2.499, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.66413
[1mStep[0m  [16/169], [94mLoss[0m : 1.40380
[1mStep[0m  [32/169], [94mLoss[0m : 1.78832
[1mStep[0m  [48/169], [94mLoss[0m : 1.57922
[1mStep[0m  [64/169], [94mLoss[0m : 2.21770
[1mStep[0m  [80/169], [94mLoss[0m : 1.78874
[1mStep[0m  [96/169], [94mLoss[0m : 1.97082
[1mStep[0m  [112/169], [94mLoss[0m : 1.81507
[1mStep[0m  [128/169], [94mLoss[0m : 1.73391
[1mStep[0m  [144/169], [94mLoss[0m : 1.99855
[1mStep[0m  [160/169], [94mLoss[0m : 1.71145

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.801, [92mTest[0m: 2.436, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.54934
[1mStep[0m  [16/169], [94mLoss[0m : 1.52987
[1mStep[0m  [32/169], [94mLoss[0m : 1.58060
[1mStep[0m  [48/169], [94mLoss[0m : 2.01508
[1mStep[0m  [64/169], [94mLoss[0m : 1.75236
[1mStep[0m  [80/169], [94mLoss[0m : 1.54495
[1mStep[0m  [96/169], [94mLoss[0m : 1.86928
[1mStep[0m  [112/169], [94mLoss[0m : 2.15179
[1mStep[0m  [128/169], [94mLoss[0m : 1.78767
[1mStep[0m  [144/169], [94mLoss[0m : 1.69506
[1mStep[0m  [160/169], [94mLoss[0m : 2.02745

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.754, [92mTest[0m: 2.491, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.91042
[1mStep[0m  [16/169], [94mLoss[0m : 1.73962
[1mStep[0m  [32/169], [94mLoss[0m : 1.86753
[1mStep[0m  [48/169], [94mLoss[0m : 1.50322
[1mStep[0m  [64/169], [94mLoss[0m : 1.65832
[1mStep[0m  [80/169], [94mLoss[0m : 1.57354
[1mStep[0m  [96/169], [94mLoss[0m : 1.52050
[1mStep[0m  [112/169], [94mLoss[0m : 1.50543
[1mStep[0m  [128/169], [94mLoss[0m : 1.95981
[1mStep[0m  [144/169], [94mLoss[0m : 1.60982
[1mStep[0m  [160/169], [94mLoss[0m : 1.58390

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.725, [92mTest[0m: 2.494, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.26587
[1mStep[0m  [16/169], [94mLoss[0m : 1.56314
[1mStep[0m  [32/169], [94mLoss[0m : 1.97532
[1mStep[0m  [48/169], [94mLoss[0m : 1.45746
[1mStep[0m  [64/169], [94mLoss[0m : 1.82823
[1mStep[0m  [80/169], [94mLoss[0m : 1.67643
[1mStep[0m  [96/169], [94mLoss[0m : 1.59396
[1mStep[0m  [112/169], [94mLoss[0m : 1.77072
[1mStep[0m  [128/169], [94mLoss[0m : 1.26732
[1mStep[0m  [144/169], [94mLoss[0m : 1.58010
[1mStep[0m  [160/169], [94mLoss[0m : 1.56148

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.677, [92mTest[0m: 2.512, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.62373
[1mStep[0m  [16/169], [94mLoss[0m : 1.61102
[1mStep[0m  [32/169], [94mLoss[0m : 2.09324
[1mStep[0m  [48/169], [94mLoss[0m : 1.70006
[1mStep[0m  [64/169], [94mLoss[0m : 1.64445
[1mStep[0m  [80/169], [94mLoss[0m : 1.59771
[1mStep[0m  [96/169], [94mLoss[0m : 1.58992
[1mStep[0m  [112/169], [94mLoss[0m : 1.62863
[1mStep[0m  [128/169], [94mLoss[0m : 1.39603
[1mStep[0m  [144/169], [94mLoss[0m : 1.69379
[1mStep[0m  [160/169], [94mLoss[0m : 1.70734

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.657, [92mTest[0m: 2.455, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.50562
[1mStep[0m  [16/169], [94mLoss[0m : 1.52460
[1mStep[0m  [32/169], [94mLoss[0m : 1.49165
[1mStep[0m  [48/169], [94mLoss[0m : 1.85033
[1mStep[0m  [64/169], [94mLoss[0m : 1.83168
[1mStep[0m  [80/169], [94mLoss[0m : 1.64360
[1mStep[0m  [96/169], [94mLoss[0m : 1.36527
[1mStep[0m  [112/169], [94mLoss[0m : 1.36905
[1mStep[0m  [128/169], [94mLoss[0m : 1.80832
[1mStep[0m  [144/169], [94mLoss[0m : 1.47597
[1mStep[0m  [160/169], [94mLoss[0m : 1.78024

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.588, [92mTest[0m: 2.483, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.59272
[1mStep[0m  [16/169], [94mLoss[0m : 1.67184
[1mStep[0m  [32/169], [94mLoss[0m : 1.52868
[1mStep[0m  [48/169], [94mLoss[0m : 1.45573
[1mStep[0m  [64/169], [94mLoss[0m : 1.63042
[1mStep[0m  [80/169], [94mLoss[0m : 1.36323
[1mStep[0m  [96/169], [94mLoss[0m : 1.63516
[1mStep[0m  [112/169], [94mLoss[0m : 1.70439
[1mStep[0m  [128/169], [94mLoss[0m : 1.87556
[1mStep[0m  [144/169], [94mLoss[0m : 1.51069
[1mStep[0m  [160/169], [94mLoss[0m : 1.60180

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.586, [92mTest[0m: 2.537, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.48576
[1mStep[0m  [16/169], [94mLoss[0m : 1.64421
[1mStep[0m  [32/169], [94mLoss[0m : 1.24713
[1mStep[0m  [48/169], [94mLoss[0m : 1.46039
[1mStep[0m  [64/169], [94mLoss[0m : 1.56003
[1mStep[0m  [80/169], [94mLoss[0m : 1.59351
[1mStep[0m  [96/169], [94mLoss[0m : 1.40923
[1mStep[0m  [112/169], [94mLoss[0m : 1.48973
[1mStep[0m  [128/169], [94mLoss[0m : 1.92359
[1mStep[0m  [144/169], [94mLoss[0m : 1.67566
[1mStep[0m  [160/169], [94mLoss[0m : 1.46639

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.528, [92mTest[0m: 2.477, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.58738
[1mStep[0m  [16/169], [94mLoss[0m : 1.53055
[1mStep[0m  [32/169], [94mLoss[0m : 1.48363
[1mStep[0m  [48/169], [94mLoss[0m : 1.40918
[1mStep[0m  [64/169], [94mLoss[0m : 1.12530
[1mStep[0m  [80/169], [94mLoss[0m : 1.57499
[1mStep[0m  [96/169], [94mLoss[0m : 1.30865
[1mStep[0m  [112/169], [94mLoss[0m : 1.56819
[1mStep[0m  [128/169], [94mLoss[0m : 1.65582
[1mStep[0m  [144/169], [94mLoss[0m : 1.47596
[1mStep[0m  [160/169], [94mLoss[0m : 1.80402

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.504, [92mTest[0m: 2.500, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.33819
[1mStep[0m  [16/169], [94mLoss[0m : 1.17662
[1mStep[0m  [32/169], [94mLoss[0m : 1.54170
[1mStep[0m  [48/169], [94mLoss[0m : 1.38607
[1mStep[0m  [64/169], [94mLoss[0m : 1.42221
[1mStep[0m  [80/169], [94mLoss[0m : 1.47071
[1mStep[0m  [96/169], [94mLoss[0m : 1.76389
[1mStep[0m  [112/169], [94mLoss[0m : 1.73009
[1mStep[0m  [128/169], [94mLoss[0m : 1.51461
[1mStep[0m  [144/169], [94mLoss[0m : 1.35544
[1mStep[0m  [160/169], [94mLoss[0m : 1.58393

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.472, [92mTest[0m: 2.478, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.60093
[1mStep[0m  [16/169], [94mLoss[0m : 1.23349
[1mStep[0m  [32/169], [94mLoss[0m : 1.66972
[1mStep[0m  [48/169], [94mLoss[0m : 1.70306
[1mStep[0m  [64/169], [94mLoss[0m : 1.51081
[1mStep[0m  [80/169], [94mLoss[0m : 1.52112
[1mStep[0m  [96/169], [94mLoss[0m : 1.58371
[1mStep[0m  [112/169], [94mLoss[0m : 1.63188
[1mStep[0m  [128/169], [94mLoss[0m : 1.49462
[1mStep[0m  [144/169], [94mLoss[0m : 1.44804
[1mStep[0m  [160/169], [94mLoss[0m : 1.59756

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.459, [92mTest[0m: 2.492, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.52721
[1mStep[0m  [16/169], [94mLoss[0m : 1.17531
[1mStep[0m  [32/169], [94mLoss[0m : 1.45786
[1mStep[0m  [48/169], [94mLoss[0m : 1.14618
[1mStep[0m  [64/169], [94mLoss[0m : 1.16274
[1mStep[0m  [80/169], [94mLoss[0m : 1.34280
[1mStep[0m  [96/169], [94mLoss[0m : 1.43933
[1mStep[0m  [112/169], [94mLoss[0m : 1.68180
[1mStep[0m  [128/169], [94mLoss[0m : 1.60046
[1mStep[0m  [144/169], [94mLoss[0m : 1.24703
[1mStep[0m  [160/169], [94mLoss[0m : 1.46377

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.405, [92mTest[0m: 2.486, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.30468
[1mStep[0m  [16/169], [94mLoss[0m : 1.35257
[1mStep[0m  [32/169], [94mLoss[0m : 1.49937
[1mStep[0m  [48/169], [94mLoss[0m : 1.46198
[1mStep[0m  [64/169], [94mLoss[0m : 1.58000
[1mStep[0m  [80/169], [94mLoss[0m : 1.63249
[1mStep[0m  [96/169], [94mLoss[0m : 1.54989
[1mStep[0m  [112/169], [94mLoss[0m : 1.28801
[1mStep[0m  [128/169], [94mLoss[0m : 1.38950
[1mStep[0m  [144/169], [94mLoss[0m : 1.23278
[1mStep[0m  [160/169], [94mLoss[0m : 1.39588

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.374, [92mTest[0m: 2.523, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.03514
[1mStep[0m  [16/169], [94mLoss[0m : 1.66677
[1mStep[0m  [32/169], [94mLoss[0m : 1.37746
[1mStep[0m  [48/169], [94mLoss[0m : 1.26531
[1mStep[0m  [64/169], [94mLoss[0m : 1.25932
[1mStep[0m  [80/169], [94mLoss[0m : 1.43007
[1mStep[0m  [96/169], [94mLoss[0m : 1.36657
[1mStep[0m  [112/169], [94mLoss[0m : 1.47661
[1mStep[0m  [128/169], [94mLoss[0m : 1.13863
[1mStep[0m  [144/169], [94mLoss[0m : 1.49002
[1mStep[0m  [160/169], [94mLoss[0m : 1.61331

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.355, [92mTest[0m: 2.451, [96mlr[0m: 0.004545
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 22 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.526
====================================

Phase 2 - Evaluation MAE:  2.5257068957601274
MAE score P1      2.338588
MAE score P2      2.525707
loss               1.35471
learning_rate      0.00505
batch_size              64
hidden_sizes         [100]
epochs                  30
activation            tanh
optimizer              sgd
early stopping        True
dropout                0.2
momentum               0.9
weight_decay        0.0001
Name: 15, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
