no change     /home/modelrep/sadiya/miniconda/condabin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda-env
no change     /home/modelrep/sadiya/miniconda/bin/activate
no change     /home/modelrep/sadiya/miniconda/bin/deactivate
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.sh
no change     /home/modelrep/sadiya/miniconda/etc/fish/conf.d/conda.fish
no change     /home/modelrep/sadiya/miniconda/shell/condabin/Conda.psm1
no change     /home/modelrep/sadiya/miniconda/shell/condabin/conda-hook.ps1
no change     /home/modelrep/sadiya/miniconda/lib/python3.10/site-packages/xontrib/conda.xsh
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.csh
no change     /home/modelrep/sadiya/.bashrc
No action taken.
-----------------
Process ID:  5
Number of Nodes:  1
Number of Array Tasks:  16
-----------------
Using device: cuda

AMD Instinct MI210
Memory Usage:
Allocated: 0.0 GB
Cached:    0.0 GB
theta
whole_spec
delta
alpha
beta
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 11.09549
[1mStep[0m  [4/42], [94mLoss[0m : 10.27882
[1mStep[0m  [8/42], [94mLoss[0m : 9.17595
[1mStep[0m  [12/42], [94mLoss[0m : 8.65239
[1mStep[0m  [16/42], [94mLoss[0m : 7.12941
[1mStep[0m  [20/42], [94mLoss[0m : 7.02326
[1mStep[0m  [24/42], [94mLoss[0m : 5.52746
[1mStep[0m  [28/42], [94mLoss[0m : 5.11140
[1mStep[0m  [32/42], [94mLoss[0m : 4.71706
[1mStep[0m  [36/42], [94mLoss[0m : 4.01731
[1mStep[0m  [40/42], [94mLoss[0m : 3.79819

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.818, [92mTest[0m: 11.037, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.58091
[1mStep[0m  [4/42], [94mLoss[0m : 3.15506
[1mStep[0m  [8/42], [94mLoss[0m : 2.96377
[1mStep[0m  [12/42], [94mLoss[0m : 3.06302
[1mStep[0m  [16/42], [94mLoss[0m : 3.08670
[1mStep[0m  [20/42], [94mLoss[0m : 3.04098
[1mStep[0m  [24/42], [94mLoss[0m : 2.97825
[1mStep[0m  [28/42], [94mLoss[0m : 2.69790
[1mStep[0m  [32/42], [94mLoss[0m : 2.96646
[1mStep[0m  [36/42], [94mLoss[0m : 2.85404
[1mStep[0m  [40/42], [94mLoss[0m : 2.65633

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.972, [92mTest[0m: 3.425, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.88861
[1mStep[0m  [4/42], [94mLoss[0m : 2.51497
[1mStep[0m  [8/42], [94mLoss[0m : 2.67461
[1mStep[0m  [12/42], [94mLoss[0m : 2.79422
[1mStep[0m  [16/42], [94mLoss[0m : 2.82735
[1mStep[0m  [20/42], [94mLoss[0m : 2.69465
[1mStep[0m  [24/42], [94mLoss[0m : 2.76698
[1mStep[0m  [28/42], [94mLoss[0m : 2.79620
[1mStep[0m  [32/42], [94mLoss[0m : 2.67707
[1mStep[0m  [36/42], [94mLoss[0m : 2.76767
[1mStep[0m  [40/42], [94mLoss[0m : 2.74615

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.708, [92mTest[0m: 2.724, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62204
[1mStep[0m  [4/42], [94mLoss[0m : 2.57619
[1mStep[0m  [8/42], [94mLoss[0m : 2.50191
[1mStep[0m  [12/42], [94mLoss[0m : 2.52262
[1mStep[0m  [16/42], [94mLoss[0m : 2.45061
[1mStep[0m  [20/42], [94mLoss[0m : 2.77646
[1mStep[0m  [24/42], [94mLoss[0m : 2.45507
[1mStep[0m  [28/42], [94mLoss[0m : 2.72507
[1mStep[0m  [32/42], [94mLoss[0m : 2.76558
[1mStep[0m  [36/42], [94mLoss[0m : 2.57906
[1mStep[0m  [40/42], [94mLoss[0m : 2.63958

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.635, [92mTest[0m: 2.592, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24940
[1mStep[0m  [4/42], [94mLoss[0m : 2.63722
[1mStep[0m  [8/42], [94mLoss[0m : 2.50487
[1mStep[0m  [12/42], [94mLoss[0m : 2.83015
[1mStep[0m  [16/42], [94mLoss[0m : 2.47565
[1mStep[0m  [20/42], [94mLoss[0m : 2.70062
[1mStep[0m  [24/42], [94mLoss[0m : 2.53977
[1mStep[0m  [28/42], [94mLoss[0m : 2.38690
[1mStep[0m  [32/42], [94mLoss[0m : 2.66075
[1mStep[0m  [36/42], [94mLoss[0m : 2.61430
[1mStep[0m  [40/42], [94mLoss[0m : 2.80037

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.580, [92mTest[0m: 2.540, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.79473
[1mStep[0m  [4/42], [94mLoss[0m : 2.58653
[1mStep[0m  [8/42], [94mLoss[0m : 2.44494
[1mStep[0m  [12/42], [94mLoss[0m : 2.73452
[1mStep[0m  [16/42], [94mLoss[0m : 2.62585
[1mStep[0m  [20/42], [94mLoss[0m : 2.51405
[1mStep[0m  [24/42], [94mLoss[0m : 2.60882
[1mStep[0m  [28/42], [94mLoss[0m : 2.50920
[1mStep[0m  [32/42], [94mLoss[0m : 2.54005
[1mStep[0m  [36/42], [94mLoss[0m : 2.50013
[1mStep[0m  [40/42], [94mLoss[0m : 2.55005

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.565, [92mTest[0m: 2.507, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44216
[1mStep[0m  [4/42], [94mLoss[0m : 2.64800
[1mStep[0m  [8/42], [94mLoss[0m : 2.60009
[1mStep[0m  [12/42], [94mLoss[0m : 2.53395
[1mStep[0m  [16/42], [94mLoss[0m : 2.53530
[1mStep[0m  [20/42], [94mLoss[0m : 2.24692
[1mStep[0m  [24/42], [94mLoss[0m : 2.52378
[1mStep[0m  [28/42], [94mLoss[0m : 2.63714
[1mStep[0m  [32/42], [94mLoss[0m : 2.30431
[1mStep[0m  [36/42], [94mLoss[0m : 2.67334
[1mStep[0m  [40/42], [94mLoss[0m : 2.52592

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.537, [92mTest[0m: 2.478, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.63452
[1mStep[0m  [4/42], [94mLoss[0m : 2.72985
[1mStep[0m  [8/42], [94mLoss[0m : 2.51406
[1mStep[0m  [12/42], [94mLoss[0m : 2.68679
[1mStep[0m  [16/42], [94mLoss[0m : 2.39410
[1mStep[0m  [20/42], [94mLoss[0m : 2.60588
[1mStep[0m  [24/42], [94mLoss[0m : 2.57864
[1mStep[0m  [28/42], [94mLoss[0m : 2.32411
[1mStep[0m  [32/42], [94mLoss[0m : 2.60635
[1mStep[0m  [36/42], [94mLoss[0m : 2.64640
[1mStep[0m  [40/42], [94mLoss[0m : 2.59736

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.544, [92mTest[0m: 2.467, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59579
[1mStep[0m  [4/42], [94mLoss[0m : 2.57507
[1mStep[0m  [8/42], [94mLoss[0m : 2.50426
[1mStep[0m  [12/42], [94mLoss[0m : 2.37717
[1mStep[0m  [16/42], [94mLoss[0m : 2.67096
[1mStep[0m  [20/42], [94mLoss[0m : 2.48257
[1mStep[0m  [24/42], [94mLoss[0m : 2.48416
[1mStep[0m  [28/42], [94mLoss[0m : 2.44441
[1mStep[0m  [32/42], [94mLoss[0m : 2.64410
[1mStep[0m  [36/42], [94mLoss[0m : 2.28851
[1mStep[0m  [40/42], [94mLoss[0m : 2.61785

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.528, [92mTest[0m: 2.445, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64041
[1mStep[0m  [4/42], [94mLoss[0m : 2.25394
[1mStep[0m  [8/42], [94mLoss[0m : 2.55606
[1mStep[0m  [12/42], [94mLoss[0m : 2.52831
[1mStep[0m  [16/42], [94mLoss[0m : 2.40036
[1mStep[0m  [20/42], [94mLoss[0m : 2.49356
[1mStep[0m  [24/42], [94mLoss[0m : 2.46431
[1mStep[0m  [28/42], [94mLoss[0m : 2.46983
[1mStep[0m  [32/42], [94mLoss[0m : 2.44072
[1mStep[0m  [36/42], [94mLoss[0m : 2.37375
[1mStep[0m  [40/42], [94mLoss[0m : 2.50398

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.442, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48355
[1mStep[0m  [4/42], [94mLoss[0m : 2.63872
[1mStep[0m  [8/42], [94mLoss[0m : 2.44137
[1mStep[0m  [12/42], [94mLoss[0m : 2.52571
[1mStep[0m  [16/42], [94mLoss[0m : 2.62308
[1mStep[0m  [20/42], [94mLoss[0m : 2.72235
[1mStep[0m  [24/42], [94mLoss[0m : 2.67424
[1mStep[0m  [28/42], [94mLoss[0m : 2.68107
[1mStep[0m  [32/42], [94mLoss[0m : 2.43320
[1mStep[0m  [36/42], [94mLoss[0m : 2.49857
[1mStep[0m  [40/42], [94mLoss[0m : 2.51484

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.422, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53255
[1mStep[0m  [4/42], [94mLoss[0m : 2.41335
[1mStep[0m  [8/42], [94mLoss[0m : 2.45912
[1mStep[0m  [12/42], [94mLoss[0m : 2.38720
[1mStep[0m  [16/42], [94mLoss[0m : 2.55081
[1mStep[0m  [20/42], [94mLoss[0m : 2.49757
[1mStep[0m  [24/42], [94mLoss[0m : 2.40475
[1mStep[0m  [28/42], [94mLoss[0m : 2.37948
[1mStep[0m  [32/42], [94mLoss[0m : 2.56744
[1mStep[0m  [36/42], [94mLoss[0m : 2.66653
[1mStep[0m  [40/42], [94mLoss[0m : 2.55926

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.428, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57534
[1mStep[0m  [4/42], [94mLoss[0m : 2.58476
[1mStep[0m  [8/42], [94mLoss[0m : 2.51375
[1mStep[0m  [12/42], [94mLoss[0m : 2.50077
[1mStep[0m  [16/42], [94mLoss[0m : 2.57597
[1mStep[0m  [20/42], [94mLoss[0m : 2.40712
[1mStep[0m  [24/42], [94mLoss[0m : 2.44319
[1mStep[0m  [28/42], [94mLoss[0m : 2.40641
[1mStep[0m  [32/42], [94mLoss[0m : 2.51578
[1mStep[0m  [36/42], [94mLoss[0m : 2.76769
[1mStep[0m  [40/42], [94mLoss[0m : 2.44609

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.428, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46447
[1mStep[0m  [4/42], [94mLoss[0m : 2.75991
[1mStep[0m  [8/42], [94mLoss[0m : 2.49986
[1mStep[0m  [12/42], [94mLoss[0m : 2.35963
[1mStep[0m  [16/42], [94mLoss[0m : 2.51252
[1mStep[0m  [20/42], [94mLoss[0m : 2.54799
[1mStep[0m  [24/42], [94mLoss[0m : 2.35294
[1mStep[0m  [28/42], [94mLoss[0m : 2.33792
[1mStep[0m  [32/42], [94mLoss[0m : 2.58527
[1mStep[0m  [36/42], [94mLoss[0m : 2.55845
[1mStep[0m  [40/42], [94mLoss[0m : 2.68760

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.413, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.65579
[1mStep[0m  [4/42], [94mLoss[0m : 2.36670
[1mStep[0m  [8/42], [94mLoss[0m : 2.35050
[1mStep[0m  [12/42], [94mLoss[0m : 2.31148
[1mStep[0m  [16/42], [94mLoss[0m : 2.47409
[1mStep[0m  [20/42], [94mLoss[0m : 2.64980
[1mStep[0m  [24/42], [94mLoss[0m : 2.37347
[1mStep[0m  [28/42], [94mLoss[0m : 2.75177
[1mStep[0m  [32/42], [94mLoss[0m : 2.67344
[1mStep[0m  [36/42], [94mLoss[0m : 2.53081
[1mStep[0m  [40/42], [94mLoss[0m : 2.37325

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.405, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52222
[1mStep[0m  [4/42], [94mLoss[0m : 2.50651
[1mStep[0m  [8/42], [94mLoss[0m : 2.46268
[1mStep[0m  [12/42], [94mLoss[0m : 2.53005
[1mStep[0m  [16/42], [94mLoss[0m : 2.41261
[1mStep[0m  [20/42], [94mLoss[0m : 2.68462
[1mStep[0m  [24/42], [94mLoss[0m : 2.36972
[1mStep[0m  [28/42], [94mLoss[0m : 2.60704
[1mStep[0m  [32/42], [94mLoss[0m : 2.81822
[1mStep[0m  [36/42], [94mLoss[0m : 2.33075
[1mStep[0m  [40/42], [94mLoss[0m : 2.39883

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.394, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42096
[1mStep[0m  [4/42], [94mLoss[0m : 2.48847
[1mStep[0m  [8/42], [94mLoss[0m : 2.26561
[1mStep[0m  [12/42], [94mLoss[0m : 2.32797
[1mStep[0m  [16/42], [94mLoss[0m : 2.49769
[1mStep[0m  [20/42], [94mLoss[0m : 2.47771
[1mStep[0m  [24/42], [94mLoss[0m : 2.47030
[1mStep[0m  [28/42], [94mLoss[0m : 2.53928
[1mStep[0m  [32/42], [94mLoss[0m : 2.45188
[1mStep[0m  [36/42], [94mLoss[0m : 2.50084
[1mStep[0m  [40/42], [94mLoss[0m : 2.37363

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.399, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42877
[1mStep[0m  [4/42], [94mLoss[0m : 2.55264
[1mStep[0m  [8/42], [94mLoss[0m : 2.76151
[1mStep[0m  [12/42], [94mLoss[0m : 2.54597
[1mStep[0m  [16/42], [94mLoss[0m : 2.45667
[1mStep[0m  [20/42], [94mLoss[0m : 2.51466
[1mStep[0m  [24/42], [94mLoss[0m : 2.47669
[1mStep[0m  [28/42], [94mLoss[0m : 2.38760
[1mStep[0m  [32/42], [94mLoss[0m : 2.57851
[1mStep[0m  [36/42], [94mLoss[0m : 2.58782
[1mStep[0m  [40/42], [94mLoss[0m : 2.47426

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.395, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52108
[1mStep[0m  [4/42], [94mLoss[0m : 2.45917
[1mStep[0m  [8/42], [94mLoss[0m : 2.37677
[1mStep[0m  [12/42], [94mLoss[0m : 2.61832
[1mStep[0m  [16/42], [94mLoss[0m : 2.54130
[1mStep[0m  [20/42], [94mLoss[0m : 2.50246
[1mStep[0m  [24/42], [94mLoss[0m : 2.53270
[1mStep[0m  [28/42], [94mLoss[0m : 2.28312
[1mStep[0m  [32/42], [94mLoss[0m : 2.56471
[1mStep[0m  [36/42], [94mLoss[0m : 2.44219
[1mStep[0m  [40/42], [94mLoss[0m : 2.39028

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.387, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62700
[1mStep[0m  [4/42], [94mLoss[0m : 2.38866
[1mStep[0m  [8/42], [94mLoss[0m : 2.59883
[1mStep[0m  [12/42], [94mLoss[0m : 2.45344
[1mStep[0m  [16/42], [94mLoss[0m : 2.31688
[1mStep[0m  [20/42], [94mLoss[0m : 2.44767
[1mStep[0m  [24/42], [94mLoss[0m : 2.49019
[1mStep[0m  [28/42], [94mLoss[0m : 2.31278
[1mStep[0m  [32/42], [94mLoss[0m : 2.57140
[1mStep[0m  [36/42], [94mLoss[0m : 2.40102
[1mStep[0m  [40/42], [94mLoss[0m : 2.37042

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.392, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58611
[1mStep[0m  [4/42], [94mLoss[0m : 2.20473
[1mStep[0m  [8/42], [94mLoss[0m : 2.37767
[1mStep[0m  [12/42], [94mLoss[0m : 2.59529
[1mStep[0m  [16/42], [94mLoss[0m : 2.38096
[1mStep[0m  [20/42], [94mLoss[0m : 2.44908
[1mStep[0m  [24/42], [94mLoss[0m : 2.45133
[1mStep[0m  [28/42], [94mLoss[0m : 2.59081
[1mStep[0m  [32/42], [94mLoss[0m : 2.62366
[1mStep[0m  [36/42], [94mLoss[0m : 2.41226
[1mStep[0m  [40/42], [94mLoss[0m : 2.61887

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.380, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37798
[1mStep[0m  [4/42], [94mLoss[0m : 2.40859
[1mStep[0m  [8/42], [94mLoss[0m : 2.30872
[1mStep[0m  [12/42], [94mLoss[0m : 2.31694
[1mStep[0m  [16/42], [94mLoss[0m : 2.58783
[1mStep[0m  [20/42], [94mLoss[0m : 2.40354
[1mStep[0m  [24/42], [94mLoss[0m : 2.63467
[1mStep[0m  [28/42], [94mLoss[0m : 2.50425
[1mStep[0m  [32/42], [94mLoss[0m : 2.41578
[1mStep[0m  [36/42], [94mLoss[0m : 2.50571
[1mStep[0m  [40/42], [94mLoss[0m : 2.54016

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.387, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46775
[1mStep[0m  [4/42], [94mLoss[0m : 2.36228
[1mStep[0m  [8/42], [94mLoss[0m : 2.57297
[1mStep[0m  [12/42], [94mLoss[0m : 2.51022
[1mStep[0m  [16/42], [94mLoss[0m : 2.35995
[1mStep[0m  [20/42], [94mLoss[0m : 2.36703
[1mStep[0m  [24/42], [94mLoss[0m : 2.59790
[1mStep[0m  [28/42], [94mLoss[0m : 2.62478
[1mStep[0m  [32/42], [94mLoss[0m : 2.41814
[1mStep[0m  [36/42], [94mLoss[0m : 2.38258
[1mStep[0m  [40/42], [94mLoss[0m : 2.69224

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.382, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41180
[1mStep[0m  [4/42], [94mLoss[0m : 2.62979
[1mStep[0m  [8/42], [94mLoss[0m : 2.27911
[1mStep[0m  [12/42], [94mLoss[0m : 2.39914
[1mStep[0m  [16/42], [94mLoss[0m : 2.49506
[1mStep[0m  [20/42], [94mLoss[0m : 2.26413
[1mStep[0m  [24/42], [94mLoss[0m : 2.56177
[1mStep[0m  [28/42], [94mLoss[0m : 2.49158
[1mStep[0m  [32/42], [94mLoss[0m : 2.18102
[1mStep[0m  [36/42], [94mLoss[0m : 2.58196
[1mStep[0m  [40/42], [94mLoss[0m : 2.64386

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.383, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36248
[1mStep[0m  [4/42], [94mLoss[0m : 2.43685
[1mStep[0m  [8/42], [94mLoss[0m : 2.34236
[1mStep[0m  [12/42], [94mLoss[0m : 2.52938
[1mStep[0m  [16/42], [94mLoss[0m : 2.51911
[1mStep[0m  [20/42], [94mLoss[0m : 2.71296
[1mStep[0m  [24/42], [94mLoss[0m : 2.31923
[1mStep[0m  [28/42], [94mLoss[0m : 2.74772
[1mStep[0m  [32/42], [94mLoss[0m : 2.46115
[1mStep[0m  [36/42], [94mLoss[0m : 2.45292
[1mStep[0m  [40/42], [94mLoss[0m : 2.57434

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.371, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42177
[1mStep[0m  [4/42], [94mLoss[0m : 2.33681
[1mStep[0m  [8/42], [94mLoss[0m : 2.37402
[1mStep[0m  [12/42], [94mLoss[0m : 2.62182
[1mStep[0m  [16/42], [94mLoss[0m : 2.64172
[1mStep[0m  [20/42], [94mLoss[0m : 2.46672
[1mStep[0m  [24/42], [94mLoss[0m : 2.66692
[1mStep[0m  [28/42], [94mLoss[0m : 2.53247
[1mStep[0m  [32/42], [94mLoss[0m : 2.57676
[1mStep[0m  [36/42], [94mLoss[0m : 2.40861
[1mStep[0m  [40/42], [94mLoss[0m : 2.16980

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.373, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52105
[1mStep[0m  [4/42], [94mLoss[0m : 2.26654
[1mStep[0m  [8/42], [94mLoss[0m : 2.41097
[1mStep[0m  [12/42], [94mLoss[0m : 2.66794
[1mStep[0m  [16/42], [94mLoss[0m : 2.55695
[1mStep[0m  [20/42], [94mLoss[0m : 2.70865
[1mStep[0m  [24/42], [94mLoss[0m : 2.36258
[1mStep[0m  [28/42], [94mLoss[0m : 2.52239
[1mStep[0m  [32/42], [94mLoss[0m : 2.62676
[1mStep[0m  [36/42], [94mLoss[0m : 2.32969
[1mStep[0m  [40/42], [94mLoss[0m : 2.42275

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.371, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48591
[1mStep[0m  [4/42], [94mLoss[0m : 2.47542
[1mStep[0m  [8/42], [94mLoss[0m : 2.51932
[1mStep[0m  [12/42], [94mLoss[0m : 2.30453
[1mStep[0m  [16/42], [94mLoss[0m : 2.48725
[1mStep[0m  [20/42], [94mLoss[0m : 2.59230
[1mStep[0m  [24/42], [94mLoss[0m : 2.35824
[1mStep[0m  [28/42], [94mLoss[0m : 2.38648
[1mStep[0m  [32/42], [94mLoss[0m : 2.61685
[1mStep[0m  [36/42], [94mLoss[0m : 2.72279
[1mStep[0m  [40/42], [94mLoss[0m : 2.37822

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.368, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59162
[1mStep[0m  [4/42], [94mLoss[0m : 2.27607
[1mStep[0m  [8/42], [94mLoss[0m : 2.34448
[1mStep[0m  [12/42], [94mLoss[0m : 2.24453
[1mStep[0m  [16/42], [94mLoss[0m : 2.47471
[1mStep[0m  [20/42], [94mLoss[0m : 2.24800
[1mStep[0m  [24/42], [94mLoss[0m : 2.38708
[1mStep[0m  [28/42], [94mLoss[0m : 2.42064
[1mStep[0m  [32/42], [94mLoss[0m : 2.51745
[1mStep[0m  [36/42], [94mLoss[0m : 2.52316
[1mStep[0m  [40/42], [94mLoss[0m : 2.41727

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.363, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55594
[1mStep[0m  [4/42], [94mLoss[0m : 2.51281
[1mStep[0m  [8/42], [94mLoss[0m : 2.37653
[1mStep[0m  [12/42], [94mLoss[0m : 2.49735
[1mStep[0m  [16/42], [94mLoss[0m : 2.58068
[1mStep[0m  [20/42], [94mLoss[0m : 2.33097
[1mStep[0m  [24/42], [94mLoss[0m : 2.60231
[1mStep[0m  [28/42], [94mLoss[0m : 2.58982
[1mStep[0m  [32/42], [94mLoss[0m : 2.52604
[1mStep[0m  [36/42], [94mLoss[0m : 2.51594
[1mStep[0m  [40/42], [94mLoss[0m : 2.30445

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.367, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.368
====================================

Phase 1 - Evaluation MAE:  2.368193711553301
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 2.73817
[1mStep[0m  [4/42], [94mLoss[0m : 2.50285
[1mStep[0m  [8/42], [94mLoss[0m : 2.34520
[1mStep[0m  [12/42], [94mLoss[0m : 2.35367
[1mStep[0m  [16/42], [94mLoss[0m : 2.21945
[1mStep[0m  [20/42], [94mLoss[0m : 2.39623
[1mStep[0m  [24/42], [94mLoss[0m : 2.34793
[1mStep[0m  [28/42], [94mLoss[0m : 2.66151
[1mStep[0m  [32/42], [94mLoss[0m : 2.65566
[1mStep[0m  [36/42], [94mLoss[0m : 2.45492
[1mStep[0m  [40/42], [94mLoss[0m : 2.51083

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.362, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57991
[1mStep[0m  [4/42], [94mLoss[0m : 2.49102
[1mStep[0m  [8/42], [94mLoss[0m : 2.27792
[1mStep[0m  [12/42], [94mLoss[0m : 2.37355
[1mStep[0m  [16/42], [94mLoss[0m : 2.58978
[1mStep[0m  [20/42], [94mLoss[0m : 2.41262
[1mStep[0m  [24/42], [94mLoss[0m : 2.37177
[1mStep[0m  [28/42], [94mLoss[0m : 2.66505
[1mStep[0m  [32/42], [94mLoss[0m : 2.33138
[1mStep[0m  [36/42], [94mLoss[0m : 2.68063
[1mStep[0m  [40/42], [94mLoss[0m : 2.60524

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.353, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47540
[1mStep[0m  [4/42], [94mLoss[0m : 2.38365
[1mStep[0m  [8/42], [94mLoss[0m : 2.42389
[1mStep[0m  [12/42], [94mLoss[0m : 2.32260
[1mStep[0m  [16/42], [94mLoss[0m : 2.35931
[1mStep[0m  [20/42], [94mLoss[0m : 2.62900
[1mStep[0m  [24/42], [94mLoss[0m : 2.46893
[1mStep[0m  [28/42], [94mLoss[0m : 2.47102
[1mStep[0m  [32/42], [94mLoss[0m : 2.49298
[1mStep[0m  [36/42], [94mLoss[0m : 2.46270
[1mStep[0m  [40/42], [94mLoss[0m : 2.39739

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.384, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43811
[1mStep[0m  [4/42], [94mLoss[0m : 2.63956
[1mStep[0m  [8/42], [94mLoss[0m : 2.50854
[1mStep[0m  [12/42], [94mLoss[0m : 2.38240
[1mStep[0m  [16/42], [94mLoss[0m : 2.40069
[1mStep[0m  [20/42], [94mLoss[0m : 2.44205
[1mStep[0m  [24/42], [94mLoss[0m : 2.48698
[1mStep[0m  [28/42], [94mLoss[0m : 2.40132
[1mStep[0m  [32/42], [94mLoss[0m : 2.24634
[1mStep[0m  [36/42], [94mLoss[0m : 2.37277
[1mStep[0m  [40/42], [94mLoss[0m : 2.23292

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.383, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49031
[1mStep[0m  [4/42], [94mLoss[0m : 2.21035
[1mStep[0m  [8/42], [94mLoss[0m : 2.48783
[1mStep[0m  [12/42], [94mLoss[0m : 2.45345
[1mStep[0m  [16/42], [94mLoss[0m : 2.66105
[1mStep[0m  [20/42], [94mLoss[0m : 2.39801
[1mStep[0m  [24/42], [94mLoss[0m : 2.46441
[1mStep[0m  [28/42], [94mLoss[0m : 2.41603
[1mStep[0m  [32/42], [94mLoss[0m : 2.27615
[1mStep[0m  [36/42], [94mLoss[0m : 2.56501
[1mStep[0m  [40/42], [94mLoss[0m : 2.53430

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.369, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51695
[1mStep[0m  [4/42], [94mLoss[0m : 2.57298
[1mStep[0m  [8/42], [94mLoss[0m : 2.42969
[1mStep[0m  [12/42], [94mLoss[0m : 2.52889
[1mStep[0m  [16/42], [94mLoss[0m : 2.58469
[1mStep[0m  [20/42], [94mLoss[0m : 2.32870
[1mStep[0m  [24/42], [94mLoss[0m : 2.40651
[1mStep[0m  [28/42], [94mLoss[0m : 2.60607
[1mStep[0m  [32/42], [94mLoss[0m : 2.23690
[1mStep[0m  [36/42], [94mLoss[0m : 2.25434
[1mStep[0m  [40/42], [94mLoss[0m : 2.25681

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.383, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51104
[1mStep[0m  [4/42], [94mLoss[0m : 2.41373
[1mStep[0m  [8/42], [94mLoss[0m : 2.49175
[1mStep[0m  [12/42], [94mLoss[0m : 2.40590
[1mStep[0m  [16/42], [94mLoss[0m : 2.42272
[1mStep[0m  [20/42], [94mLoss[0m : 2.48172
[1mStep[0m  [24/42], [94mLoss[0m : 2.30887
[1mStep[0m  [28/42], [94mLoss[0m : 2.37412
[1mStep[0m  [32/42], [94mLoss[0m : 2.71544
[1mStep[0m  [36/42], [94mLoss[0m : 2.49821
[1mStep[0m  [40/42], [94mLoss[0m : 2.36976

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.408, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43253
[1mStep[0m  [4/42], [94mLoss[0m : 2.24994
[1mStep[0m  [8/42], [94mLoss[0m : 2.42866
[1mStep[0m  [12/42], [94mLoss[0m : 2.27859
[1mStep[0m  [16/42], [94mLoss[0m : 2.31538
[1mStep[0m  [20/42], [94mLoss[0m : 2.42236
[1mStep[0m  [24/42], [94mLoss[0m : 2.51347
[1mStep[0m  [28/42], [94mLoss[0m : 2.37959
[1mStep[0m  [32/42], [94mLoss[0m : 2.47845
[1mStep[0m  [36/42], [94mLoss[0m : 2.34085
[1mStep[0m  [40/42], [94mLoss[0m : 2.66384

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.383, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41526
[1mStep[0m  [4/42], [94mLoss[0m : 2.64059
[1mStep[0m  [8/42], [94mLoss[0m : 2.51565
[1mStep[0m  [12/42], [94mLoss[0m : 2.32817
[1mStep[0m  [16/42], [94mLoss[0m : 2.23941
[1mStep[0m  [20/42], [94mLoss[0m : 2.30787
[1mStep[0m  [24/42], [94mLoss[0m : 2.26480
[1mStep[0m  [28/42], [94mLoss[0m : 2.23926
[1mStep[0m  [32/42], [94mLoss[0m : 2.41596
[1mStep[0m  [36/42], [94mLoss[0m : 2.45297
[1mStep[0m  [40/42], [94mLoss[0m : 2.36796

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.365, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35264
[1mStep[0m  [4/42], [94mLoss[0m : 2.31839
[1mStep[0m  [8/42], [94mLoss[0m : 2.42360
[1mStep[0m  [12/42], [94mLoss[0m : 2.38165
[1mStep[0m  [16/42], [94mLoss[0m : 2.40735
[1mStep[0m  [20/42], [94mLoss[0m : 2.45048
[1mStep[0m  [24/42], [94mLoss[0m : 2.12706
[1mStep[0m  [28/42], [94mLoss[0m : 2.36824
[1mStep[0m  [32/42], [94mLoss[0m : 2.37528
[1mStep[0m  [36/42], [94mLoss[0m : 2.44339
[1mStep[0m  [40/42], [94mLoss[0m : 2.37132

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.403, [92mTest[0m: 2.386, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26221
[1mStep[0m  [4/42], [94mLoss[0m : 2.31144
[1mStep[0m  [8/42], [94mLoss[0m : 2.45164
[1mStep[0m  [12/42], [94mLoss[0m : 2.25194
[1mStep[0m  [16/42], [94mLoss[0m : 2.24529
[1mStep[0m  [20/42], [94mLoss[0m : 2.49457
[1mStep[0m  [24/42], [94mLoss[0m : 2.37520
[1mStep[0m  [28/42], [94mLoss[0m : 2.32176
[1mStep[0m  [32/42], [94mLoss[0m : 2.45217
[1mStep[0m  [36/42], [94mLoss[0m : 2.25300
[1mStep[0m  [40/42], [94mLoss[0m : 2.32895

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.390, [92mTest[0m: 2.407, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55819
[1mStep[0m  [4/42], [94mLoss[0m : 2.27888
[1mStep[0m  [8/42], [94mLoss[0m : 2.32084
[1mStep[0m  [12/42], [94mLoss[0m : 2.48400
[1mStep[0m  [16/42], [94mLoss[0m : 2.31054
[1mStep[0m  [20/42], [94mLoss[0m : 2.61640
[1mStep[0m  [24/42], [94mLoss[0m : 2.59447
[1mStep[0m  [28/42], [94mLoss[0m : 2.34587
[1mStep[0m  [32/42], [94mLoss[0m : 2.39366
[1mStep[0m  [36/42], [94mLoss[0m : 2.44943
[1mStep[0m  [40/42], [94mLoss[0m : 2.43818

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.401, [92mTest[0m: 2.431, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56620
[1mStep[0m  [4/42], [94mLoss[0m : 2.25757
[1mStep[0m  [8/42], [94mLoss[0m : 2.32840
[1mStep[0m  [12/42], [94mLoss[0m : 2.34889
[1mStep[0m  [16/42], [94mLoss[0m : 2.35584
[1mStep[0m  [20/42], [94mLoss[0m : 2.34537
[1mStep[0m  [24/42], [94mLoss[0m : 2.36158
[1mStep[0m  [28/42], [94mLoss[0m : 2.34080
[1mStep[0m  [32/42], [94mLoss[0m : 2.42336
[1mStep[0m  [36/42], [94mLoss[0m : 2.42493
[1mStep[0m  [40/42], [94mLoss[0m : 2.34157

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.387, [92mTest[0m: 2.401, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28313
[1mStep[0m  [4/42], [94mLoss[0m : 2.26165
[1mStep[0m  [8/42], [94mLoss[0m : 2.58510
[1mStep[0m  [12/42], [94mLoss[0m : 2.18660
[1mStep[0m  [16/42], [94mLoss[0m : 2.18539
[1mStep[0m  [20/42], [94mLoss[0m : 2.17792
[1mStep[0m  [24/42], [94mLoss[0m : 2.14277
[1mStep[0m  [28/42], [94mLoss[0m : 2.40823
[1mStep[0m  [32/42], [94mLoss[0m : 2.39255
[1mStep[0m  [36/42], [94mLoss[0m : 2.35516
[1mStep[0m  [40/42], [94mLoss[0m : 2.30752

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.365, [92mTest[0m: 2.371, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51867
[1mStep[0m  [4/42], [94mLoss[0m : 2.23643
[1mStep[0m  [8/42], [94mLoss[0m : 2.39037
[1mStep[0m  [12/42], [94mLoss[0m : 2.45078
[1mStep[0m  [16/42], [94mLoss[0m : 2.42079
[1mStep[0m  [20/42], [94mLoss[0m : 2.35292
[1mStep[0m  [24/42], [94mLoss[0m : 2.31654
[1mStep[0m  [28/42], [94mLoss[0m : 2.26747
[1mStep[0m  [32/42], [94mLoss[0m : 2.37338
[1mStep[0m  [36/42], [94mLoss[0m : 2.16509
[1mStep[0m  [40/42], [94mLoss[0m : 2.24695

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.373, [92mTest[0m: 2.426, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24172
[1mStep[0m  [4/42], [94mLoss[0m : 2.42607
[1mStep[0m  [8/42], [94mLoss[0m : 2.27409
[1mStep[0m  [12/42], [94mLoss[0m : 2.30012
[1mStep[0m  [16/42], [94mLoss[0m : 2.34155
[1mStep[0m  [20/42], [94mLoss[0m : 2.36009
[1mStep[0m  [24/42], [94mLoss[0m : 2.33457
[1mStep[0m  [28/42], [94mLoss[0m : 2.39268
[1mStep[0m  [32/42], [94mLoss[0m : 2.44828
[1mStep[0m  [36/42], [94mLoss[0m : 2.44078
[1mStep[0m  [40/42], [94mLoss[0m : 2.44380

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.375, [92mTest[0m: 2.416, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31304
[1mStep[0m  [4/42], [94mLoss[0m : 2.35790
[1mStep[0m  [8/42], [94mLoss[0m : 2.16674
[1mStep[0m  [12/42], [94mLoss[0m : 2.44445
[1mStep[0m  [16/42], [94mLoss[0m : 2.23772
[1mStep[0m  [20/42], [94mLoss[0m : 2.13368
[1mStep[0m  [24/42], [94mLoss[0m : 2.35852
[1mStep[0m  [28/42], [94mLoss[0m : 2.43289
[1mStep[0m  [32/42], [94mLoss[0m : 2.34247
[1mStep[0m  [36/42], [94mLoss[0m : 2.36536
[1mStep[0m  [40/42], [94mLoss[0m : 2.25755

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.364, [92mTest[0m: 2.389, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41665
[1mStep[0m  [4/42], [94mLoss[0m : 2.29146
[1mStep[0m  [8/42], [94mLoss[0m : 2.62503
[1mStep[0m  [12/42], [94mLoss[0m : 2.37951
[1mStep[0m  [16/42], [94mLoss[0m : 2.38108
[1mStep[0m  [20/42], [94mLoss[0m : 2.32128
[1mStep[0m  [24/42], [94mLoss[0m : 2.43122
[1mStep[0m  [28/42], [94mLoss[0m : 2.54559
[1mStep[0m  [32/42], [94mLoss[0m : 2.28310
[1mStep[0m  [36/42], [94mLoss[0m : 2.39708
[1mStep[0m  [40/42], [94mLoss[0m : 2.31974

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.357, [92mTest[0m: 2.434, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.16926
[1mStep[0m  [4/42], [94mLoss[0m : 2.50835
[1mStep[0m  [8/42], [94mLoss[0m : 2.43320
[1mStep[0m  [12/42], [94mLoss[0m : 2.33133
[1mStep[0m  [16/42], [94mLoss[0m : 2.32706
[1mStep[0m  [20/42], [94mLoss[0m : 2.42471
[1mStep[0m  [24/42], [94mLoss[0m : 2.33821
[1mStep[0m  [28/42], [94mLoss[0m : 2.26072
[1mStep[0m  [32/42], [94mLoss[0m : 2.27890
[1mStep[0m  [36/42], [94mLoss[0m : 2.36824
[1mStep[0m  [40/42], [94mLoss[0m : 2.14517

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.355, [92mTest[0m: 2.421, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45320
[1mStep[0m  [4/42], [94mLoss[0m : 2.37642
[1mStep[0m  [8/42], [94mLoss[0m : 2.26200
[1mStep[0m  [12/42], [94mLoss[0m : 2.20909
[1mStep[0m  [16/42], [94mLoss[0m : 2.45682
[1mStep[0m  [20/42], [94mLoss[0m : 2.47050
[1mStep[0m  [24/42], [94mLoss[0m : 2.32648
[1mStep[0m  [28/42], [94mLoss[0m : 2.27415
[1mStep[0m  [32/42], [94mLoss[0m : 2.32634
[1mStep[0m  [36/42], [94mLoss[0m : 2.45881
[1mStep[0m  [40/42], [94mLoss[0m : 2.22061

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.340, [92mTest[0m: 2.387, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42490
[1mStep[0m  [4/42], [94mLoss[0m : 2.28239
[1mStep[0m  [8/42], [94mLoss[0m : 2.31001
[1mStep[0m  [12/42], [94mLoss[0m : 2.31238
[1mStep[0m  [16/42], [94mLoss[0m : 2.39545
[1mStep[0m  [20/42], [94mLoss[0m : 2.21721
[1mStep[0m  [24/42], [94mLoss[0m : 2.30596
[1mStep[0m  [28/42], [94mLoss[0m : 2.31695
[1mStep[0m  [32/42], [94mLoss[0m : 2.53868
[1mStep[0m  [36/42], [94mLoss[0m : 2.40564
[1mStep[0m  [40/42], [94mLoss[0m : 2.22068

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.316, [92mTest[0m: 2.380, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42221
[1mStep[0m  [4/42], [94mLoss[0m : 2.15893
[1mStep[0m  [8/42], [94mLoss[0m : 2.39052
[1mStep[0m  [12/42], [94mLoss[0m : 2.42880
[1mStep[0m  [16/42], [94mLoss[0m : 2.22663
[1mStep[0m  [20/42], [94mLoss[0m : 2.15735
[1mStep[0m  [24/42], [94mLoss[0m : 2.41466
[1mStep[0m  [28/42], [94mLoss[0m : 2.56875
[1mStep[0m  [32/42], [94mLoss[0m : 2.27489
[1mStep[0m  [36/42], [94mLoss[0m : 2.42258
[1mStep[0m  [40/42], [94mLoss[0m : 2.42937

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.317, [92mTest[0m: 2.421, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35271
[1mStep[0m  [4/42], [94mLoss[0m : 2.24408
[1mStep[0m  [8/42], [94mLoss[0m : 2.33344
[1mStep[0m  [12/42], [94mLoss[0m : 2.36024
[1mStep[0m  [16/42], [94mLoss[0m : 2.35841
[1mStep[0m  [20/42], [94mLoss[0m : 2.34210
[1mStep[0m  [24/42], [94mLoss[0m : 2.46132
[1mStep[0m  [28/42], [94mLoss[0m : 2.28195
[1mStep[0m  [32/42], [94mLoss[0m : 2.21043
[1mStep[0m  [36/42], [94mLoss[0m : 2.26446
[1mStep[0m  [40/42], [94mLoss[0m : 2.42627

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.313, [92mTest[0m: 2.419, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27359
[1mStep[0m  [4/42], [94mLoss[0m : 2.05954
[1mStep[0m  [8/42], [94mLoss[0m : 2.20507
[1mStep[0m  [12/42], [94mLoss[0m : 2.45472
[1mStep[0m  [16/42], [94mLoss[0m : 2.60009
[1mStep[0m  [20/42], [94mLoss[0m : 2.27370
[1mStep[0m  [24/42], [94mLoss[0m : 2.29399
[1mStep[0m  [28/42], [94mLoss[0m : 2.03883
[1mStep[0m  [32/42], [94mLoss[0m : 2.33152
[1mStep[0m  [36/42], [94mLoss[0m : 2.22538
[1mStep[0m  [40/42], [94mLoss[0m : 2.44211

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.318, [92mTest[0m: 2.460, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28528
[1mStep[0m  [4/42], [94mLoss[0m : 2.26344
[1mStep[0m  [8/42], [94mLoss[0m : 2.26908
[1mStep[0m  [12/42], [94mLoss[0m : 2.10278
[1mStep[0m  [16/42], [94mLoss[0m : 2.15505
[1mStep[0m  [20/42], [94mLoss[0m : 2.26287
[1mStep[0m  [24/42], [94mLoss[0m : 2.41661
[1mStep[0m  [28/42], [94mLoss[0m : 2.59731
[1mStep[0m  [32/42], [94mLoss[0m : 2.28611
[1mStep[0m  [36/42], [94mLoss[0m : 2.37922
[1mStep[0m  [40/42], [94mLoss[0m : 2.27836

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.292, [92mTest[0m: 2.484, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.30784
[1mStep[0m  [4/42], [94mLoss[0m : 2.25518
[1mStep[0m  [8/42], [94mLoss[0m : 2.33449
[1mStep[0m  [12/42], [94mLoss[0m : 2.21071
[1mStep[0m  [16/42], [94mLoss[0m : 2.22338
[1mStep[0m  [20/42], [94mLoss[0m : 2.37154
[1mStep[0m  [24/42], [94mLoss[0m : 2.50511
[1mStep[0m  [28/42], [94mLoss[0m : 2.38308
[1mStep[0m  [32/42], [94mLoss[0m : 2.33104
[1mStep[0m  [36/42], [94mLoss[0m : 2.03037
[1mStep[0m  [40/42], [94mLoss[0m : 2.30272

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.299, [92mTest[0m: 2.435, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26795
[1mStep[0m  [4/42], [94mLoss[0m : 2.37569
[1mStep[0m  [8/42], [94mLoss[0m : 2.67026
[1mStep[0m  [12/42], [94mLoss[0m : 2.13578
[1mStep[0m  [16/42], [94mLoss[0m : 2.43172
[1mStep[0m  [20/42], [94mLoss[0m : 2.16190
[1mStep[0m  [24/42], [94mLoss[0m : 2.14089
[1mStep[0m  [28/42], [94mLoss[0m : 2.40793
[1mStep[0m  [32/42], [94mLoss[0m : 2.41723
[1mStep[0m  [36/42], [94mLoss[0m : 2.39414
[1mStep[0m  [40/42], [94mLoss[0m : 2.46838

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.280, [92mTest[0m: 2.379, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.99248
[1mStep[0m  [4/42], [94mLoss[0m : 2.19706
[1mStep[0m  [8/42], [94mLoss[0m : 2.44039
[1mStep[0m  [12/42], [94mLoss[0m : 2.23087
[1mStep[0m  [16/42], [94mLoss[0m : 2.22637
[1mStep[0m  [20/42], [94mLoss[0m : 2.52164
[1mStep[0m  [24/42], [94mLoss[0m : 2.04779
[1mStep[0m  [28/42], [94mLoss[0m : 2.36963
[1mStep[0m  [32/42], [94mLoss[0m : 2.10519
[1mStep[0m  [36/42], [94mLoss[0m : 2.30336
[1mStep[0m  [40/42], [94mLoss[0m : 2.28960

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.283, [92mTest[0m: 2.473, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.11046
[1mStep[0m  [4/42], [94mLoss[0m : 2.22223
[1mStep[0m  [8/42], [94mLoss[0m : 2.40657
[1mStep[0m  [12/42], [94mLoss[0m : 2.37020
[1mStep[0m  [16/42], [94mLoss[0m : 2.04428
[1mStep[0m  [20/42], [94mLoss[0m : 2.01092
[1mStep[0m  [24/42], [94mLoss[0m : 2.28763
[1mStep[0m  [28/42], [94mLoss[0m : 2.23913
[1mStep[0m  [32/42], [94mLoss[0m : 2.49374
[1mStep[0m  [36/42], [94mLoss[0m : 2.30647
[1mStep[0m  [40/42], [94mLoss[0m : 2.30488

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.268, [92mTest[0m: 2.421, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.23645
[1mStep[0m  [4/42], [94mLoss[0m : 2.32196
[1mStep[0m  [8/42], [94mLoss[0m : 2.36287
[1mStep[0m  [12/42], [94mLoss[0m : 2.47714
[1mStep[0m  [16/42], [94mLoss[0m : 2.29040
[1mStep[0m  [20/42], [94mLoss[0m : 2.31413
[1mStep[0m  [24/42], [94mLoss[0m : 2.43932
[1mStep[0m  [28/42], [94mLoss[0m : 2.33171
[1mStep[0m  [32/42], [94mLoss[0m : 2.57126
[1mStep[0m  [36/42], [94mLoss[0m : 2.55742
[1mStep[0m  [40/42], [94mLoss[0m : 2.28673

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.262, [92mTest[0m: 2.432, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.397
====================================

Phase 2 - Evaluation MAE:  2.3974788870130266
MAE score P1      2.368194
MAE score P2      2.397479
loss              2.262086
learning_rate     0.002575
batch_size             256
hidden_sizes         [300]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.4
momentum               0.1
weight_decay        0.0001
Name: 0, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 10.50564
[1mStep[0m  [4/42], [94mLoss[0m : 10.07259
[1mStep[0m  [8/42], [94mLoss[0m : 9.60701
[1mStep[0m  [12/42], [94mLoss[0m : 8.25445
[1mStep[0m  [16/42], [94mLoss[0m : 8.39481
[1mStep[0m  [20/42], [94mLoss[0m : 7.61201
[1mStep[0m  [24/42], [94mLoss[0m : 7.32286
[1mStep[0m  [28/42], [94mLoss[0m : 6.90955
[1mStep[0m  [32/42], [94mLoss[0m : 6.48936
[1mStep[0m  [36/42], [94mLoss[0m : 5.17173
[1mStep[0m  [40/42], [94mLoss[0m : 5.13743

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 7.709, [92mTest[0m: 10.241, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.88285
[1mStep[0m  [4/42], [94mLoss[0m : 4.30109
[1mStep[0m  [8/42], [94mLoss[0m : 4.15056
[1mStep[0m  [12/42], [94mLoss[0m : 4.21111
[1mStep[0m  [16/42], [94mLoss[0m : 3.67923
[1mStep[0m  [20/42], [94mLoss[0m : 3.59254
[1mStep[0m  [24/42], [94mLoss[0m : 3.17672
[1mStep[0m  [28/42], [94mLoss[0m : 3.12292
[1mStep[0m  [32/42], [94mLoss[0m : 3.19597
[1mStep[0m  [36/42], [94mLoss[0m : 3.05078
[1mStep[0m  [40/42], [94mLoss[0m : 3.25986

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.605, [92mTest[0m: 4.897, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54142
[1mStep[0m  [4/42], [94mLoss[0m : 2.87407
[1mStep[0m  [8/42], [94mLoss[0m : 3.01033
[1mStep[0m  [12/42], [94mLoss[0m : 2.66653
[1mStep[0m  [16/42], [94mLoss[0m : 2.96115
[1mStep[0m  [20/42], [94mLoss[0m : 2.56680
[1mStep[0m  [24/42], [94mLoss[0m : 2.62861
[1mStep[0m  [28/42], [94mLoss[0m : 2.72454
[1mStep[0m  [32/42], [94mLoss[0m : 2.61464
[1mStep[0m  [36/42], [94mLoss[0m : 2.51911
[1mStep[0m  [40/42], [94mLoss[0m : 2.67484

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.748, [92mTest[0m: 2.833, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27392
[1mStep[0m  [4/42], [94mLoss[0m : 2.57933
[1mStep[0m  [8/42], [94mLoss[0m : 2.80572
[1mStep[0m  [12/42], [94mLoss[0m : 2.75006
[1mStep[0m  [16/42], [94mLoss[0m : 2.45992
[1mStep[0m  [20/42], [94mLoss[0m : 2.50252
[1mStep[0m  [24/42], [94mLoss[0m : 2.59072
[1mStep[0m  [28/42], [94mLoss[0m : 2.49038
[1mStep[0m  [32/42], [94mLoss[0m : 2.79335
[1mStep[0m  [36/42], [94mLoss[0m : 2.48192
[1mStep[0m  [40/42], [94mLoss[0m : 2.50195

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.582, [92mTest[0m: 2.488, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54330
[1mStep[0m  [4/42], [94mLoss[0m : 2.42734
[1mStep[0m  [8/42], [94mLoss[0m : 2.60683
[1mStep[0m  [12/42], [94mLoss[0m : 2.57974
[1mStep[0m  [16/42], [94mLoss[0m : 2.39322
[1mStep[0m  [20/42], [94mLoss[0m : 2.54153
[1mStep[0m  [24/42], [94mLoss[0m : 2.43494
[1mStep[0m  [28/42], [94mLoss[0m : 2.53488
[1mStep[0m  [32/42], [94mLoss[0m : 2.82227
[1mStep[0m  [36/42], [94mLoss[0m : 2.36403
[1mStep[0m  [40/42], [94mLoss[0m : 2.58434

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.441, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51228
[1mStep[0m  [4/42], [94mLoss[0m : 2.66280
[1mStep[0m  [8/42], [94mLoss[0m : 2.49104
[1mStep[0m  [12/42], [94mLoss[0m : 2.65562
[1mStep[0m  [16/42], [94mLoss[0m : 2.39735
[1mStep[0m  [20/42], [94mLoss[0m : 2.48136
[1mStep[0m  [24/42], [94mLoss[0m : 2.43434
[1mStep[0m  [28/42], [94mLoss[0m : 2.68788
[1mStep[0m  [32/42], [94mLoss[0m : 2.52976
[1mStep[0m  [36/42], [94mLoss[0m : 2.28510
[1mStep[0m  [40/42], [94mLoss[0m : 2.63638

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.522, [92mTest[0m: 2.428, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32755
[1mStep[0m  [4/42], [94mLoss[0m : 2.61850
[1mStep[0m  [8/42], [94mLoss[0m : 2.60648
[1mStep[0m  [12/42], [94mLoss[0m : 2.56570
[1mStep[0m  [16/42], [94mLoss[0m : 2.61099
[1mStep[0m  [20/42], [94mLoss[0m : 2.76920
[1mStep[0m  [24/42], [94mLoss[0m : 2.52324
[1mStep[0m  [28/42], [94mLoss[0m : 2.52806
[1mStep[0m  [32/42], [94mLoss[0m : 2.47743
[1mStep[0m  [36/42], [94mLoss[0m : 2.56178
[1mStep[0m  [40/42], [94mLoss[0m : 2.67067

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.519, [92mTest[0m: 2.418, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64216
[1mStep[0m  [4/42], [94mLoss[0m : 2.41308
[1mStep[0m  [8/42], [94mLoss[0m : 2.50392
[1mStep[0m  [12/42], [94mLoss[0m : 2.44694
[1mStep[0m  [16/42], [94mLoss[0m : 2.40740
[1mStep[0m  [20/42], [94mLoss[0m : 2.44815
[1mStep[0m  [24/42], [94mLoss[0m : 2.42710
[1mStep[0m  [28/42], [94mLoss[0m : 2.46096
[1mStep[0m  [32/42], [94mLoss[0m : 2.41153
[1mStep[0m  [36/42], [94mLoss[0m : 2.62740
[1mStep[0m  [40/42], [94mLoss[0m : 2.47960

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.409, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37662
[1mStep[0m  [4/42], [94mLoss[0m : 2.60032
[1mStep[0m  [8/42], [94mLoss[0m : 2.45977
[1mStep[0m  [12/42], [94mLoss[0m : 2.41634
[1mStep[0m  [16/42], [94mLoss[0m : 2.55383
[1mStep[0m  [20/42], [94mLoss[0m : 2.27668
[1mStep[0m  [24/42], [94mLoss[0m : 2.51890
[1mStep[0m  [28/42], [94mLoss[0m : 2.66118
[1mStep[0m  [32/42], [94mLoss[0m : 2.82813
[1mStep[0m  [36/42], [94mLoss[0m : 2.45988
[1mStep[0m  [40/42], [94mLoss[0m : 2.40977

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.396, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55169
[1mStep[0m  [4/42], [94mLoss[0m : 2.52312
[1mStep[0m  [8/42], [94mLoss[0m : 2.31508
[1mStep[0m  [12/42], [94mLoss[0m : 2.64899
[1mStep[0m  [16/42], [94mLoss[0m : 2.45860
[1mStep[0m  [20/42], [94mLoss[0m : 2.34043
[1mStep[0m  [24/42], [94mLoss[0m : 2.30018
[1mStep[0m  [28/42], [94mLoss[0m : 2.66063
[1mStep[0m  [32/42], [94mLoss[0m : 2.47089
[1mStep[0m  [36/42], [94mLoss[0m : 2.31479
[1mStep[0m  [40/42], [94mLoss[0m : 2.52033

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.385, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61496
[1mStep[0m  [4/42], [94mLoss[0m : 2.39149
[1mStep[0m  [8/42], [94mLoss[0m : 2.31776
[1mStep[0m  [12/42], [94mLoss[0m : 2.37409
[1mStep[0m  [16/42], [94mLoss[0m : 2.42369
[1mStep[0m  [20/42], [94mLoss[0m : 2.29960
[1mStep[0m  [24/42], [94mLoss[0m : 2.71391
[1mStep[0m  [28/42], [94mLoss[0m : 2.55795
[1mStep[0m  [32/42], [94mLoss[0m : 2.60060
[1mStep[0m  [36/42], [94mLoss[0m : 2.42051
[1mStep[0m  [40/42], [94mLoss[0m : 2.82170

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.381, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45719
[1mStep[0m  [4/42], [94mLoss[0m : 2.39308
[1mStep[0m  [8/42], [94mLoss[0m : 2.64075
[1mStep[0m  [12/42], [94mLoss[0m : 2.39814
[1mStep[0m  [16/42], [94mLoss[0m : 2.57692
[1mStep[0m  [20/42], [94mLoss[0m : 2.59237
[1mStep[0m  [24/42], [94mLoss[0m : 2.45171
[1mStep[0m  [28/42], [94mLoss[0m : 2.54640
[1mStep[0m  [32/42], [94mLoss[0m : 2.51565
[1mStep[0m  [36/42], [94mLoss[0m : 2.39797
[1mStep[0m  [40/42], [94mLoss[0m : 2.44639

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.375, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56305
[1mStep[0m  [4/42], [94mLoss[0m : 2.41996
[1mStep[0m  [8/42], [94mLoss[0m : 2.18226
[1mStep[0m  [12/42], [94mLoss[0m : 2.38528
[1mStep[0m  [16/42], [94mLoss[0m : 2.58861
[1mStep[0m  [20/42], [94mLoss[0m : 2.52049
[1mStep[0m  [24/42], [94mLoss[0m : 2.28838
[1mStep[0m  [28/42], [94mLoss[0m : 2.47088
[1mStep[0m  [32/42], [94mLoss[0m : 2.48462
[1mStep[0m  [36/42], [94mLoss[0m : 2.54447
[1mStep[0m  [40/42], [94mLoss[0m : 2.37284

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.370, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37305
[1mStep[0m  [4/42], [94mLoss[0m : 2.38825
[1mStep[0m  [8/42], [94mLoss[0m : 2.54194
[1mStep[0m  [12/42], [94mLoss[0m : 2.53039
[1mStep[0m  [16/42], [94mLoss[0m : 2.41987
[1mStep[0m  [20/42], [94mLoss[0m : 2.32559
[1mStep[0m  [24/42], [94mLoss[0m : 2.39218
[1mStep[0m  [28/42], [94mLoss[0m : 2.34350
[1mStep[0m  [32/42], [94mLoss[0m : 2.53229
[1mStep[0m  [36/42], [94mLoss[0m : 2.71946
[1mStep[0m  [40/42], [94mLoss[0m : 2.65167

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.361, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50414
[1mStep[0m  [4/42], [94mLoss[0m : 2.63590
[1mStep[0m  [8/42], [94mLoss[0m : 2.52114
[1mStep[0m  [12/42], [94mLoss[0m : 2.32654
[1mStep[0m  [16/42], [94mLoss[0m : 2.27316
[1mStep[0m  [20/42], [94mLoss[0m : 2.35106
[1mStep[0m  [24/42], [94mLoss[0m : 2.48293
[1mStep[0m  [28/42], [94mLoss[0m : 2.56094
[1mStep[0m  [32/42], [94mLoss[0m : 2.39918
[1mStep[0m  [36/42], [94mLoss[0m : 2.59527
[1mStep[0m  [40/42], [94mLoss[0m : 2.40295

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.374, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56001
[1mStep[0m  [4/42], [94mLoss[0m : 2.42267
[1mStep[0m  [8/42], [94mLoss[0m : 2.47225
[1mStep[0m  [12/42], [94mLoss[0m : 2.59923
[1mStep[0m  [16/42], [94mLoss[0m : 2.35603
[1mStep[0m  [20/42], [94mLoss[0m : 2.51024
[1mStep[0m  [24/42], [94mLoss[0m : 2.36835
[1mStep[0m  [28/42], [94mLoss[0m : 2.38331
[1mStep[0m  [32/42], [94mLoss[0m : 2.57044
[1mStep[0m  [36/42], [94mLoss[0m : 2.36806
[1mStep[0m  [40/42], [94mLoss[0m : 2.25370

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.357, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58524
[1mStep[0m  [4/42], [94mLoss[0m : 2.42850
[1mStep[0m  [8/42], [94mLoss[0m : 2.39568
[1mStep[0m  [12/42], [94mLoss[0m : 2.52012
[1mStep[0m  [16/42], [94mLoss[0m : 2.43181
[1mStep[0m  [20/42], [94mLoss[0m : 2.45004
[1mStep[0m  [24/42], [94mLoss[0m : 2.44446
[1mStep[0m  [28/42], [94mLoss[0m : 2.26595
[1mStep[0m  [32/42], [94mLoss[0m : 2.71252
[1mStep[0m  [36/42], [94mLoss[0m : 2.31353
[1mStep[0m  [40/42], [94mLoss[0m : 2.77182

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.360, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41935
[1mStep[0m  [4/42], [94mLoss[0m : 2.51592
[1mStep[0m  [8/42], [94mLoss[0m : 2.38655
[1mStep[0m  [12/42], [94mLoss[0m : 2.55975
[1mStep[0m  [16/42], [94mLoss[0m : 2.26701
[1mStep[0m  [20/42], [94mLoss[0m : 2.34033
[1mStep[0m  [24/42], [94mLoss[0m : 2.51435
[1mStep[0m  [28/42], [94mLoss[0m : 2.52244
[1mStep[0m  [32/42], [94mLoss[0m : 2.35670
[1mStep[0m  [36/42], [94mLoss[0m : 2.28910
[1mStep[0m  [40/42], [94mLoss[0m : 2.57615

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.356, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40318
[1mStep[0m  [4/42], [94mLoss[0m : 2.26258
[1mStep[0m  [8/42], [94mLoss[0m : 2.55127
[1mStep[0m  [12/42], [94mLoss[0m : 2.35358
[1mStep[0m  [16/42], [94mLoss[0m : 2.53178
[1mStep[0m  [20/42], [94mLoss[0m : 2.35994
[1mStep[0m  [24/42], [94mLoss[0m : 2.38729
[1mStep[0m  [28/42], [94mLoss[0m : 2.51870
[1mStep[0m  [32/42], [94mLoss[0m : 2.51692
[1mStep[0m  [36/42], [94mLoss[0m : 2.30678
[1mStep[0m  [40/42], [94mLoss[0m : 2.53219

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.349, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45075
[1mStep[0m  [4/42], [94mLoss[0m : 2.42222
[1mStep[0m  [8/42], [94mLoss[0m : 2.34375
[1mStep[0m  [12/42], [94mLoss[0m : 2.26082
[1mStep[0m  [16/42], [94mLoss[0m : 2.47302
[1mStep[0m  [20/42], [94mLoss[0m : 2.27999
[1mStep[0m  [24/42], [94mLoss[0m : 2.50894
[1mStep[0m  [28/42], [94mLoss[0m : 2.40992
[1mStep[0m  [32/42], [94mLoss[0m : 2.60282
[1mStep[0m  [36/42], [94mLoss[0m : 2.70525
[1mStep[0m  [40/42], [94mLoss[0m : 2.36690

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.351, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46476
[1mStep[0m  [4/42], [94mLoss[0m : 2.51787
[1mStep[0m  [8/42], [94mLoss[0m : 2.25308
[1mStep[0m  [12/42], [94mLoss[0m : 2.43439
[1mStep[0m  [16/42], [94mLoss[0m : 2.42568
[1mStep[0m  [20/42], [94mLoss[0m : 2.43099
[1mStep[0m  [24/42], [94mLoss[0m : 2.62474
[1mStep[0m  [28/42], [94mLoss[0m : 2.51222
[1mStep[0m  [32/42], [94mLoss[0m : 2.60006
[1mStep[0m  [36/42], [94mLoss[0m : 2.52661
[1mStep[0m  [40/42], [94mLoss[0m : 2.51981

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.351, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33593
[1mStep[0m  [4/42], [94mLoss[0m : 2.29091
[1mStep[0m  [8/42], [94mLoss[0m : 2.53222
[1mStep[0m  [12/42], [94mLoss[0m : 2.44753
[1mStep[0m  [16/42], [94mLoss[0m : 2.53718
[1mStep[0m  [20/42], [94mLoss[0m : 2.35007
[1mStep[0m  [24/42], [94mLoss[0m : 2.53684
[1mStep[0m  [28/42], [94mLoss[0m : 2.61102
[1mStep[0m  [32/42], [94mLoss[0m : 2.73953
[1mStep[0m  [36/42], [94mLoss[0m : 2.34741
[1mStep[0m  [40/42], [94mLoss[0m : 2.43912

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.350, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.25792
[1mStep[0m  [4/42], [94mLoss[0m : 2.42576
[1mStep[0m  [8/42], [94mLoss[0m : 2.48195
[1mStep[0m  [12/42], [94mLoss[0m : 2.47270
[1mStep[0m  [16/42], [94mLoss[0m : 2.43955
[1mStep[0m  [20/42], [94mLoss[0m : 2.45566
[1mStep[0m  [24/42], [94mLoss[0m : 2.43130
[1mStep[0m  [28/42], [94mLoss[0m : 2.30257
[1mStep[0m  [32/42], [94mLoss[0m : 2.35642
[1mStep[0m  [36/42], [94mLoss[0m : 2.25705
[1mStep[0m  [40/42], [94mLoss[0m : 2.35327

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.352, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41081
[1mStep[0m  [4/42], [94mLoss[0m : 2.32482
[1mStep[0m  [8/42], [94mLoss[0m : 2.53804
[1mStep[0m  [12/42], [94mLoss[0m : 2.34097
[1mStep[0m  [16/42], [94mLoss[0m : 2.45977
[1mStep[0m  [20/42], [94mLoss[0m : 2.21173
[1mStep[0m  [24/42], [94mLoss[0m : 2.15247
[1mStep[0m  [28/42], [94mLoss[0m : 2.21887
[1mStep[0m  [32/42], [94mLoss[0m : 2.39857
[1mStep[0m  [36/42], [94mLoss[0m : 2.41939
[1mStep[0m  [40/42], [94mLoss[0m : 2.40355

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.348, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50146
[1mStep[0m  [4/42], [94mLoss[0m : 2.37958
[1mStep[0m  [8/42], [94mLoss[0m : 2.18108
[1mStep[0m  [12/42], [94mLoss[0m : 2.32655
[1mStep[0m  [16/42], [94mLoss[0m : 2.49381
[1mStep[0m  [20/42], [94mLoss[0m : 2.50422
[1mStep[0m  [24/42], [94mLoss[0m : 2.58601
[1mStep[0m  [28/42], [94mLoss[0m : 2.50372
[1mStep[0m  [32/42], [94mLoss[0m : 2.58063
[1mStep[0m  [36/42], [94mLoss[0m : 2.32585
[1mStep[0m  [40/42], [94mLoss[0m : 2.55655

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.344, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33299
[1mStep[0m  [4/42], [94mLoss[0m : 2.27160
[1mStep[0m  [8/42], [94mLoss[0m : 2.14077
[1mStep[0m  [12/42], [94mLoss[0m : 2.53037
[1mStep[0m  [16/42], [94mLoss[0m : 2.59961
[1mStep[0m  [20/42], [94mLoss[0m : 2.48226
[1mStep[0m  [24/42], [94mLoss[0m : 2.58656
[1mStep[0m  [28/42], [94mLoss[0m : 2.47792
[1mStep[0m  [32/42], [94mLoss[0m : 2.51124
[1mStep[0m  [36/42], [94mLoss[0m : 2.40057
[1mStep[0m  [40/42], [94mLoss[0m : 2.63010

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.345, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59164
[1mStep[0m  [4/42], [94mLoss[0m : 2.34681
[1mStep[0m  [8/42], [94mLoss[0m : 2.32480
[1mStep[0m  [12/42], [94mLoss[0m : 2.40544
[1mStep[0m  [16/42], [94mLoss[0m : 2.40051
[1mStep[0m  [20/42], [94mLoss[0m : 2.45467
[1mStep[0m  [24/42], [94mLoss[0m : 2.44374
[1mStep[0m  [28/42], [94mLoss[0m : 2.54801
[1mStep[0m  [32/42], [94mLoss[0m : 2.50968
[1mStep[0m  [36/42], [94mLoss[0m : 2.48514
[1mStep[0m  [40/42], [94mLoss[0m : 2.60887

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.346, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.22069
[1mStep[0m  [4/42], [94mLoss[0m : 2.41628
[1mStep[0m  [8/42], [94mLoss[0m : 2.35393
[1mStep[0m  [12/42], [94mLoss[0m : 2.47133
[1mStep[0m  [16/42], [94mLoss[0m : 2.47300
[1mStep[0m  [20/42], [94mLoss[0m : 2.33332
[1mStep[0m  [24/42], [94mLoss[0m : 2.46631
[1mStep[0m  [28/42], [94mLoss[0m : 2.47881
[1mStep[0m  [32/42], [94mLoss[0m : 2.65351
[1mStep[0m  [36/42], [94mLoss[0m : 2.39086
[1mStep[0m  [40/42], [94mLoss[0m : 2.52883

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.348, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49996
[1mStep[0m  [4/42], [94mLoss[0m : 2.45814
[1mStep[0m  [8/42], [94mLoss[0m : 2.18313
[1mStep[0m  [12/42], [94mLoss[0m : 2.42277
[1mStep[0m  [16/42], [94mLoss[0m : 2.42791
[1mStep[0m  [20/42], [94mLoss[0m : 2.50143
[1mStep[0m  [24/42], [94mLoss[0m : 2.51074
[1mStep[0m  [28/42], [94mLoss[0m : 2.61253
[1mStep[0m  [32/42], [94mLoss[0m : 2.62808
[1mStep[0m  [36/42], [94mLoss[0m : 2.65598
[1mStep[0m  [40/42], [94mLoss[0m : 2.59432

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.344, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35609
[1mStep[0m  [4/42], [94mLoss[0m : 2.64472
[1mStep[0m  [8/42], [94mLoss[0m : 2.42364
[1mStep[0m  [12/42], [94mLoss[0m : 2.58435
[1mStep[0m  [16/42], [94mLoss[0m : 2.53111
[1mStep[0m  [20/42], [94mLoss[0m : 2.27217
[1mStep[0m  [24/42], [94mLoss[0m : 2.38162
[1mStep[0m  [28/42], [94mLoss[0m : 2.44886
[1mStep[0m  [32/42], [94mLoss[0m : 2.35052
[1mStep[0m  [36/42], [94mLoss[0m : 2.37741
[1mStep[0m  [40/42], [94mLoss[0m : 2.47646

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.333, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.338
====================================

Phase 1 - Evaluation MAE:  2.338475772312709
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 2.34048
[1mStep[0m  [4/42], [94mLoss[0m : 2.30114
[1mStep[0m  [8/42], [94mLoss[0m : 2.38530
[1mStep[0m  [12/42], [94mLoss[0m : 2.40836
[1mStep[0m  [16/42], [94mLoss[0m : 2.58282
[1mStep[0m  [20/42], [94mLoss[0m : 2.63190
[1mStep[0m  [24/42], [94mLoss[0m : 2.55589
[1mStep[0m  [28/42], [94mLoss[0m : 2.56947
[1mStep[0m  [32/42], [94mLoss[0m : 2.45878
[1mStep[0m  [36/42], [94mLoss[0m : 2.50424
[1mStep[0m  [40/42], [94mLoss[0m : 2.40149

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.339, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.66563
[1mStep[0m  [4/42], [94mLoss[0m : 2.48488
[1mStep[0m  [8/42], [94mLoss[0m : 2.28723
[1mStep[0m  [12/42], [94mLoss[0m : 2.36399
[1mStep[0m  [16/42], [94mLoss[0m : 2.53695
[1mStep[0m  [20/42], [94mLoss[0m : 2.44003
[1mStep[0m  [24/42], [94mLoss[0m : 2.74560
[1mStep[0m  [28/42], [94mLoss[0m : 2.58040
[1mStep[0m  [32/42], [94mLoss[0m : 2.56189
[1mStep[0m  [36/42], [94mLoss[0m : 2.47366
[1mStep[0m  [40/42], [94mLoss[0m : 2.51220

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.334, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37762
[1mStep[0m  [4/42], [94mLoss[0m : 2.56438
[1mStep[0m  [8/42], [94mLoss[0m : 2.53589
[1mStep[0m  [12/42], [94mLoss[0m : 2.31530
[1mStep[0m  [16/42], [94mLoss[0m : 2.25591
[1mStep[0m  [20/42], [94mLoss[0m : 2.56075
[1mStep[0m  [24/42], [94mLoss[0m : 2.62502
[1mStep[0m  [28/42], [94mLoss[0m : 2.40504
[1mStep[0m  [32/42], [94mLoss[0m : 2.62712
[1mStep[0m  [36/42], [94mLoss[0m : 2.34417
[1mStep[0m  [40/42], [94mLoss[0m : 2.37355

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.382, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32217
[1mStep[0m  [4/42], [94mLoss[0m : 2.61287
[1mStep[0m  [8/42], [94mLoss[0m : 2.36463
[1mStep[0m  [12/42], [94mLoss[0m : 2.44795
[1mStep[0m  [16/42], [94mLoss[0m : 2.39576
[1mStep[0m  [20/42], [94mLoss[0m : 2.48627
[1mStep[0m  [24/42], [94mLoss[0m : 2.59970
[1mStep[0m  [28/42], [94mLoss[0m : 2.48898
[1mStep[0m  [32/42], [94mLoss[0m : 2.49138
[1mStep[0m  [36/42], [94mLoss[0m : 2.46943
[1mStep[0m  [40/42], [94mLoss[0m : 2.59667

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.403, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35679
[1mStep[0m  [4/42], [94mLoss[0m : 2.65283
[1mStep[0m  [8/42], [94mLoss[0m : 2.04211
[1mStep[0m  [12/42], [94mLoss[0m : 2.31630
[1mStep[0m  [16/42], [94mLoss[0m : 2.44254
[1mStep[0m  [20/42], [94mLoss[0m : 2.49918
[1mStep[0m  [24/42], [94mLoss[0m : 2.32220
[1mStep[0m  [28/42], [94mLoss[0m : 2.48704
[1mStep[0m  [32/42], [94mLoss[0m : 2.40122
[1mStep[0m  [36/42], [94mLoss[0m : 2.39712
[1mStep[0m  [40/42], [94mLoss[0m : 2.33279

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.441, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52105
[1mStep[0m  [4/42], [94mLoss[0m : 2.34161
[1mStep[0m  [8/42], [94mLoss[0m : 2.42227
[1mStep[0m  [12/42], [94mLoss[0m : 2.33542
[1mStep[0m  [16/42], [94mLoss[0m : 2.22873
[1mStep[0m  [20/42], [94mLoss[0m : 2.56338
[1mStep[0m  [24/42], [94mLoss[0m : 2.47859
[1mStep[0m  [28/42], [94mLoss[0m : 2.38203
[1mStep[0m  [32/42], [94mLoss[0m : 2.29812
[1mStep[0m  [36/42], [94mLoss[0m : 2.39683
[1mStep[0m  [40/42], [94mLoss[0m : 2.37043

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.468, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43571
[1mStep[0m  [4/42], [94mLoss[0m : 2.52766
[1mStep[0m  [8/42], [94mLoss[0m : 2.37238
[1mStep[0m  [12/42], [94mLoss[0m : 2.26369
[1mStep[0m  [16/42], [94mLoss[0m : 2.39103
[1mStep[0m  [20/42], [94mLoss[0m : 2.59493
[1mStep[0m  [24/42], [94mLoss[0m : 2.35905
[1mStep[0m  [28/42], [94mLoss[0m : 2.22437
[1mStep[0m  [32/42], [94mLoss[0m : 2.22396
[1mStep[0m  [36/42], [94mLoss[0m : 2.35551
[1mStep[0m  [40/42], [94mLoss[0m : 2.47402

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.378, [92mTest[0m: 2.499, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48513
[1mStep[0m  [4/42], [94mLoss[0m : 2.47081
[1mStep[0m  [8/42], [94mLoss[0m : 2.34994
[1mStep[0m  [12/42], [94mLoss[0m : 2.09593
[1mStep[0m  [16/42], [94mLoss[0m : 2.26206
[1mStep[0m  [20/42], [94mLoss[0m : 2.18350
[1mStep[0m  [24/42], [94mLoss[0m : 2.49890
[1mStep[0m  [28/42], [94mLoss[0m : 2.14353
[1mStep[0m  [32/42], [94mLoss[0m : 2.30682
[1mStep[0m  [36/42], [94mLoss[0m : 2.36438
[1mStep[0m  [40/42], [94mLoss[0m : 2.43172

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.364, [92mTest[0m: 2.492, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29756
[1mStep[0m  [4/42], [94mLoss[0m : 2.36223
[1mStep[0m  [8/42], [94mLoss[0m : 2.31777
[1mStep[0m  [12/42], [94mLoss[0m : 2.47978
[1mStep[0m  [16/42], [94mLoss[0m : 2.58454
[1mStep[0m  [20/42], [94mLoss[0m : 2.40015
[1mStep[0m  [24/42], [94mLoss[0m : 2.48129
[1mStep[0m  [28/42], [94mLoss[0m : 2.49000
[1mStep[0m  [32/42], [94mLoss[0m : 2.47108
[1mStep[0m  [36/42], [94mLoss[0m : 2.26756
[1mStep[0m  [40/42], [94mLoss[0m : 2.45606

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.367, [92mTest[0m: 2.568, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36987
[1mStep[0m  [4/42], [94mLoss[0m : 2.28743
[1mStep[0m  [8/42], [94mLoss[0m : 2.25502
[1mStep[0m  [12/42], [94mLoss[0m : 2.42360
[1mStep[0m  [16/42], [94mLoss[0m : 2.45192
[1mStep[0m  [20/42], [94mLoss[0m : 2.30602
[1mStep[0m  [24/42], [94mLoss[0m : 2.21474
[1mStep[0m  [28/42], [94mLoss[0m : 2.44744
[1mStep[0m  [32/42], [94mLoss[0m : 2.40109
[1mStep[0m  [36/42], [94mLoss[0m : 2.23183
[1mStep[0m  [40/42], [94mLoss[0m : 2.49650

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.338, [92mTest[0m: 2.540, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31476
[1mStep[0m  [4/42], [94mLoss[0m : 2.25444
[1mStep[0m  [8/42], [94mLoss[0m : 2.37183
[1mStep[0m  [12/42], [94mLoss[0m : 2.39125
[1mStep[0m  [16/42], [94mLoss[0m : 2.29694
[1mStep[0m  [20/42], [94mLoss[0m : 2.15718
[1mStep[0m  [24/42], [94mLoss[0m : 2.26236
[1mStep[0m  [28/42], [94mLoss[0m : 2.34655
[1mStep[0m  [32/42], [94mLoss[0m : 2.40376
[1mStep[0m  [36/42], [94mLoss[0m : 2.38182
[1mStep[0m  [40/42], [94mLoss[0m : 2.24499

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.334, [92mTest[0m: 2.577, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34466
[1mStep[0m  [4/42], [94mLoss[0m : 2.49576
[1mStep[0m  [8/42], [94mLoss[0m : 2.27529
[1mStep[0m  [12/42], [94mLoss[0m : 2.21810
[1mStep[0m  [16/42], [94mLoss[0m : 2.14321
[1mStep[0m  [20/42], [94mLoss[0m : 2.54092
[1mStep[0m  [24/42], [94mLoss[0m : 2.48004
[1mStep[0m  [28/42], [94mLoss[0m : 2.16464
[1mStep[0m  [32/42], [94mLoss[0m : 2.32251
[1mStep[0m  [36/42], [94mLoss[0m : 2.23258
[1mStep[0m  [40/42], [94mLoss[0m : 2.45704

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.319, [92mTest[0m: 2.558, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39249
[1mStep[0m  [4/42], [94mLoss[0m : 2.27689
[1mStep[0m  [8/42], [94mLoss[0m : 2.42809
[1mStep[0m  [12/42], [94mLoss[0m : 2.27285
[1mStep[0m  [16/42], [94mLoss[0m : 2.27208
[1mStep[0m  [20/42], [94mLoss[0m : 2.32604
[1mStep[0m  [24/42], [94mLoss[0m : 2.36689
[1mStep[0m  [28/42], [94mLoss[0m : 2.30879
[1mStep[0m  [32/42], [94mLoss[0m : 2.21708
[1mStep[0m  [36/42], [94mLoss[0m : 2.17700
[1mStep[0m  [40/42], [94mLoss[0m : 2.32785

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.297, [92mTest[0m: 2.582, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24591
[1mStep[0m  [4/42], [94mLoss[0m : 2.35001
[1mStep[0m  [8/42], [94mLoss[0m : 2.28359
[1mStep[0m  [12/42], [94mLoss[0m : 2.38708
[1mStep[0m  [16/42], [94mLoss[0m : 2.43084
[1mStep[0m  [20/42], [94mLoss[0m : 2.04474
[1mStep[0m  [24/42], [94mLoss[0m : 2.25996
[1mStep[0m  [28/42], [94mLoss[0m : 1.96988
[1mStep[0m  [32/42], [94mLoss[0m : 2.14069
[1mStep[0m  [36/42], [94mLoss[0m : 2.24401
[1mStep[0m  [40/42], [94mLoss[0m : 2.37906

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.277, [92mTest[0m: 2.646, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31729
[1mStep[0m  [4/42], [94mLoss[0m : 2.22258
[1mStep[0m  [8/42], [94mLoss[0m : 2.42326
[1mStep[0m  [12/42], [94mLoss[0m : 2.40527
[1mStep[0m  [16/42], [94mLoss[0m : 2.16391
[1mStep[0m  [20/42], [94mLoss[0m : 2.09069
[1mStep[0m  [24/42], [94mLoss[0m : 2.18766
[1mStep[0m  [28/42], [94mLoss[0m : 2.42831
[1mStep[0m  [32/42], [94mLoss[0m : 2.15049
[1mStep[0m  [36/42], [94mLoss[0m : 1.97916
[1mStep[0m  [40/42], [94mLoss[0m : 2.41223

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.256, [92mTest[0m: 2.572, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26447
[1mStep[0m  [4/42], [94mLoss[0m : 2.19559
[1mStep[0m  [8/42], [94mLoss[0m : 2.07043
[1mStep[0m  [12/42], [94mLoss[0m : 2.31567
[1mStep[0m  [16/42], [94mLoss[0m : 2.17517
[1mStep[0m  [20/42], [94mLoss[0m : 2.46412
[1mStep[0m  [24/42], [94mLoss[0m : 2.26511
[1mStep[0m  [28/42], [94mLoss[0m : 2.19091
[1mStep[0m  [32/42], [94mLoss[0m : 2.27855
[1mStep[0m  [36/42], [94mLoss[0m : 2.22919
[1mStep[0m  [40/42], [94mLoss[0m : 2.04297

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.247, [92mTest[0m: 2.634, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32344
[1mStep[0m  [4/42], [94mLoss[0m : 2.20347
[1mStep[0m  [8/42], [94mLoss[0m : 2.30553
[1mStep[0m  [12/42], [94mLoss[0m : 2.11718
[1mStep[0m  [16/42], [94mLoss[0m : 1.97751
[1mStep[0m  [20/42], [94mLoss[0m : 2.19365
[1mStep[0m  [24/42], [94mLoss[0m : 2.32979
[1mStep[0m  [28/42], [94mLoss[0m : 2.18217
[1mStep[0m  [32/42], [94mLoss[0m : 2.27996
[1mStep[0m  [36/42], [94mLoss[0m : 2.16448
[1mStep[0m  [40/42], [94mLoss[0m : 2.02243

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.225, [92mTest[0m: 2.666, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.18708
[1mStep[0m  [4/42], [94mLoss[0m : 2.03291
[1mStep[0m  [8/42], [94mLoss[0m : 2.25470
[1mStep[0m  [12/42], [94mLoss[0m : 2.08434
[1mStep[0m  [16/42], [94mLoss[0m : 2.11420
[1mStep[0m  [20/42], [94mLoss[0m : 2.40161
[1mStep[0m  [24/42], [94mLoss[0m : 2.34010
[1mStep[0m  [28/42], [94mLoss[0m : 1.86926
[1mStep[0m  [32/42], [94mLoss[0m : 2.38200
[1mStep[0m  [36/42], [94mLoss[0m : 2.35589
[1mStep[0m  [40/42], [94mLoss[0m : 2.26845

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.201, [92mTest[0m: 2.646, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.00009
[1mStep[0m  [4/42], [94mLoss[0m : 2.43235
[1mStep[0m  [8/42], [94mLoss[0m : 1.96013
[1mStep[0m  [12/42], [94mLoss[0m : 2.20916
[1mStep[0m  [16/42], [94mLoss[0m : 2.09532
[1mStep[0m  [20/42], [94mLoss[0m : 2.20876
[1mStep[0m  [24/42], [94mLoss[0m : 2.31868
[1mStep[0m  [28/42], [94mLoss[0m : 2.09121
[1mStep[0m  [32/42], [94mLoss[0m : 2.26046
[1mStep[0m  [36/42], [94mLoss[0m : 2.06929
[1mStep[0m  [40/42], [94mLoss[0m : 2.18486

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.189, [92mTest[0m: 2.600, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.17647
[1mStep[0m  [4/42], [94mLoss[0m : 2.12924
[1mStep[0m  [8/42], [94mLoss[0m : 2.19165
[1mStep[0m  [12/42], [94mLoss[0m : 2.09976
[1mStep[0m  [16/42], [94mLoss[0m : 2.20844
[1mStep[0m  [20/42], [94mLoss[0m : 2.25292
[1mStep[0m  [24/42], [94mLoss[0m : 2.02922
[1mStep[0m  [28/42], [94mLoss[0m : 2.32669
[1mStep[0m  [32/42], [94mLoss[0m : 2.16138
[1mStep[0m  [36/42], [94mLoss[0m : 2.12066
[1mStep[0m  [40/42], [94mLoss[0m : 2.07156

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.159, [92mTest[0m: 2.623, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.98932
[1mStep[0m  [4/42], [94mLoss[0m : 2.48136
[1mStep[0m  [8/42], [94mLoss[0m : 1.88010
[1mStep[0m  [12/42], [94mLoss[0m : 2.19448
[1mStep[0m  [16/42], [94mLoss[0m : 2.23025
[1mStep[0m  [20/42], [94mLoss[0m : 2.16795
[1mStep[0m  [24/42], [94mLoss[0m : 2.18806
[1mStep[0m  [28/42], [94mLoss[0m : 2.07855
[1mStep[0m  [32/42], [94mLoss[0m : 2.11989
[1mStep[0m  [36/42], [94mLoss[0m : 2.07066
[1mStep[0m  [40/42], [94mLoss[0m : 2.16480

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.147, [92mTest[0m: 2.631, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.89645
[1mStep[0m  [4/42], [94mLoss[0m : 2.02218
[1mStep[0m  [8/42], [94mLoss[0m : 2.14658
[1mStep[0m  [12/42], [94mLoss[0m : 2.20184
[1mStep[0m  [16/42], [94mLoss[0m : 2.05995
[1mStep[0m  [20/42], [94mLoss[0m : 2.03505
[1mStep[0m  [24/42], [94mLoss[0m : 2.06072
[1mStep[0m  [28/42], [94mLoss[0m : 2.09757
[1mStep[0m  [32/42], [94mLoss[0m : 2.07849
[1mStep[0m  [36/42], [94mLoss[0m : 2.27565
[1mStep[0m  [40/42], [94mLoss[0m : 2.08228

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.122, [92mTest[0m: 2.614, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.13797
[1mStep[0m  [4/42], [94mLoss[0m : 2.07469
[1mStep[0m  [8/42], [94mLoss[0m : 2.03238
[1mStep[0m  [12/42], [94mLoss[0m : 2.09417
[1mStep[0m  [16/42], [94mLoss[0m : 2.10624
[1mStep[0m  [20/42], [94mLoss[0m : 2.18173
[1mStep[0m  [24/42], [94mLoss[0m : 2.13659
[1mStep[0m  [28/42], [94mLoss[0m : 2.01602
[1mStep[0m  [32/42], [94mLoss[0m : 2.02061
[1mStep[0m  [36/42], [94mLoss[0m : 2.10008
[1mStep[0m  [40/42], [94mLoss[0m : 2.10301

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.102, [92mTest[0m: 2.600, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.02236
[1mStep[0m  [4/42], [94mLoss[0m : 2.23289
[1mStep[0m  [8/42], [94mLoss[0m : 2.17143
[1mStep[0m  [12/42], [94mLoss[0m : 2.06775
[1mStep[0m  [16/42], [94mLoss[0m : 2.04091
[1mStep[0m  [20/42], [94mLoss[0m : 1.98981
[1mStep[0m  [24/42], [94mLoss[0m : 2.09909
[1mStep[0m  [28/42], [94mLoss[0m : 2.07989
[1mStep[0m  [32/42], [94mLoss[0m : 1.95964
[1mStep[0m  [36/42], [94mLoss[0m : 1.91453
[1mStep[0m  [40/42], [94mLoss[0m : 2.12106

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.082, [92mTest[0m: 2.646, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.00239
[1mStep[0m  [4/42], [94mLoss[0m : 2.01293
[1mStep[0m  [8/42], [94mLoss[0m : 1.99069
[1mStep[0m  [12/42], [94mLoss[0m : 2.07532
[1mStep[0m  [16/42], [94mLoss[0m : 1.99465
[1mStep[0m  [20/42], [94mLoss[0m : 2.00540
[1mStep[0m  [24/42], [94mLoss[0m : 2.08176
[1mStep[0m  [28/42], [94mLoss[0m : 2.08300
[1mStep[0m  [32/42], [94mLoss[0m : 2.09463
[1mStep[0m  [36/42], [94mLoss[0m : 2.17396
[1mStep[0m  [40/42], [94mLoss[0m : 2.19450

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.065, [92mTest[0m: 2.724, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.00278
[1mStep[0m  [4/42], [94mLoss[0m : 1.91385
[1mStep[0m  [8/42], [94mLoss[0m : 2.22396
[1mStep[0m  [12/42], [94mLoss[0m : 2.19996
[1mStep[0m  [16/42], [94mLoss[0m : 2.27663
[1mStep[0m  [20/42], [94mLoss[0m : 2.13008
[1mStep[0m  [24/42], [94mLoss[0m : 2.03416
[1mStep[0m  [28/42], [94mLoss[0m : 1.95545
[1mStep[0m  [32/42], [94mLoss[0m : 2.21101
[1mStep[0m  [36/42], [94mLoss[0m : 2.02241
[1mStep[0m  [40/42], [94mLoss[0m : 1.93450

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.037, [92mTest[0m: 2.654, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.88906
[1mStep[0m  [4/42], [94mLoss[0m : 2.05219
[1mStep[0m  [8/42], [94mLoss[0m : 1.84766
[1mStep[0m  [12/42], [94mLoss[0m : 1.98405
[1mStep[0m  [16/42], [94mLoss[0m : 1.99730
[1mStep[0m  [20/42], [94mLoss[0m : 1.99564
[1mStep[0m  [24/42], [94mLoss[0m : 2.13050
[1mStep[0m  [28/42], [94mLoss[0m : 2.14278
[1mStep[0m  [32/42], [94mLoss[0m : 2.08081
[1mStep[0m  [36/42], [94mLoss[0m : 2.20110
[1mStep[0m  [40/42], [94mLoss[0m : 2.05776

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.014, [92mTest[0m: 2.611, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.96659
[1mStep[0m  [4/42], [94mLoss[0m : 2.12001
[1mStep[0m  [8/42], [94mLoss[0m : 1.98570
[1mStep[0m  [12/42], [94mLoss[0m : 1.98695
[1mStep[0m  [16/42], [94mLoss[0m : 1.88840
[1mStep[0m  [20/42], [94mLoss[0m : 2.12826
[1mStep[0m  [24/42], [94mLoss[0m : 1.77791
[1mStep[0m  [28/42], [94mLoss[0m : 1.96968
[1mStep[0m  [32/42], [94mLoss[0m : 2.05269
[1mStep[0m  [36/42], [94mLoss[0m : 1.82025
[1mStep[0m  [40/42], [94mLoss[0m : 1.98261

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.988, [92mTest[0m: 2.702, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.98033
[1mStep[0m  [4/42], [94mLoss[0m : 1.94101
[1mStep[0m  [8/42], [94mLoss[0m : 1.93200
[1mStep[0m  [12/42], [94mLoss[0m : 2.18806
[1mStep[0m  [16/42], [94mLoss[0m : 2.08590
[1mStep[0m  [20/42], [94mLoss[0m : 1.95321
[1mStep[0m  [24/42], [94mLoss[0m : 1.95894
[1mStep[0m  [28/42], [94mLoss[0m : 2.06173
[1mStep[0m  [32/42], [94mLoss[0m : 1.76830
[1mStep[0m  [36/42], [94mLoss[0m : 2.02567
[1mStep[0m  [40/42], [94mLoss[0m : 1.90005

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.947, [92mTest[0m: 2.584, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.89531
[1mStep[0m  [4/42], [94mLoss[0m : 2.17807
[1mStep[0m  [8/42], [94mLoss[0m : 2.02560
[1mStep[0m  [12/42], [94mLoss[0m : 1.93945
[1mStep[0m  [16/42], [94mLoss[0m : 1.96678
[1mStep[0m  [20/42], [94mLoss[0m : 2.06921
[1mStep[0m  [24/42], [94mLoss[0m : 2.08601
[1mStep[0m  [28/42], [94mLoss[0m : 1.83321
[1mStep[0m  [32/42], [94mLoss[0m : 1.76823
[1mStep[0m  [36/42], [94mLoss[0m : 2.00459
[1mStep[0m  [40/42], [94mLoss[0m : 1.82385

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.940, [92mTest[0m: 2.614, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.661
====================================

Phase 2 - Evaluation MAE:  2.660521490233285
MAE score P1        2.338476
MAE score P2        2.660521
loss                1.940107
learning_rate       0.002575
batch_size               256
hidden_sizes      [300, 100]
epochs                    30
activation           sigmoid
optimizer                sgd
early stopping         False
dropout                  0.2
momentum                 0.5
weight_decay          0.0001
Name: 1, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 10.59168
[1mStep[0m  [4/42], [94mLoss[0m : 9.56887
[1mStep[0m  [8/42], [94mLoss[0m : 8.42067
[1mStep[0m  [12/42], [94mLoss[0m : 7.19966
[1mStep[0m  [16/42], [94mLoss[0m : 5.84954
[1mStep[0m  [20/42], [94mLoss[0m : 4.49807
[1mStep[0m  [24/42], [94mLoss[0m : 3.99695
[1mStep[0m  [28/42], [94mLoss[0m : 3.23872
[1mStep[0m  [32/42], [94mLoss[0m : 3.12203
[1mStep[0m  [36/42], [94mLoss[0m : 2.71479
[1mStep[0m  [40/42], [94mLoss[0m : 2.83624

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 5.544, [92mTest[0m: 10.696, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.85824
[1mStep[0m  [4/42], [94mLoss[0m : 2.76661
[1mStep[0m  [8/42], [94mLoss[0m : 2.89992
[1mStep[0m  [12/42], [94mLoss[0m : 2.49409
[1mStep[0m  [16/42], [94mLoss[0m : 2.67110
[1mStep[0m  [20/42], [94mLoss[0m : 2.65828
[1mStep[0m  [24/42], [94mLoss[0m : 2.78943
[1mStep[0m  [28/42], [94mLoss[0m : 2.75283
[1mStep[0m  [32/42], [94mLoss[0m : 2.76818
[1mStep[0m  [36/42], [94mLoss[0m : 2.68680
[1mStep[0m  [40/42], [94mLoss[0m : 2.59348

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.705, [92mTest[0m: 2.854, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58434
[1mStep[0m  [4/42], [94mLoss[0m : 2.46594
[1mStep[0m  [8/42], [94mLoss[0m : 2.63319
[1mStep[0m  [12/42], [94mLoss[0m : 2.63834
[1mStep[0m  [16/42], [94mLoss[0m : 2.53516
[1mStep[0m  [20/42], [94mLoss[0m : 2.80398
[1mStep[0m  [24/42], [94mLoss[0m : 2.69237
[1mStep[0m  [28/42], [94mLoss[0m : 2.67962
[1mStep[0m  [32/42], [94mLoss[0m : 2.34620
[1mStep[0m  [36/42], [94mLoss[0m : 2.57092
[1mStep[0m  [40/42], [94mLoss[0m : 2.42419

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.577, [92mTest[0m: 2.544, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42623
[1mStep[0m  [4/42], [94mLoss[0m : 2.61399
[1mStep[0m  [8/42], [94mLoss[0m : 2.58721
[1mStep[0m  [12/42], [94mLoss[0m : 2.39309
[1mStep[0m  [16/42], [94mLoss[0m : 2.39077
[1mStep[0m  [20/42], [94mLoss[0m : 2.47702
[1mStep[0m  [24/42], [94mLoss[0m : 2.55456
[1mStep[0m  [28/42], [94mLoss[0m : 2.51944
[1mStep[0m  [32/42], [94mLoss[0m : 2.37460
[1mStep[0m  [36/42], [94mLoss[0m : 2.51668
[1mStep[0m  [40/42], [94mLoss[0m : 2.61973

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.483, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.65252
[1mStep[0m  [4/42], [94mLoss[0m : 2.42908
[1mStep[0m  [8/42], [94mLoss[0m : 2.53684
[1mStep[0m  [12/42], [94mLoss[0m : 2.54461
[1mStep[0m  [16/42], [94mLoss[0m : 2.52789
[1mStep[0m  [20/42], [94mLoss[0m : 2.50451
[1mStep[0m  [24/42], [94mLoss[0m : 2.49802
[1mStep[0m  [28/42], [94mLoss[0m : 2.60486
[1mStep[0m  [32/42], [94mLoss[0m : 2.47463
[1mStep[0m  [36/42], [94mLoss[0m : 2.58518
[1mStep[0m  [40/42], [94mLoss[0m : 2.62095

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.517, [92mTest[0m: 2.451, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56634
[1mStep[0m  [4/42], [94mLoss[0m : 2.48083
[1mStep[0m  [8/42], [94mLoss[0m : 2.53161
[1mStep[0m  [12/42], [94mLoss[0m : 2.40693
[1mStep[0m  [16/42], [94mLoss[0m : 2.76070
[1mStep[0m  [20/42], [94mLoss[0m : 2.40091
[1mStep[0m  [24/42], [94mLoss[0m : 2.65594
[1mStep[0m  [28/42], [94mLoss[0m : 2.45953
[1mStep[0m  [32/42], [94mLoss[0m : 2.49582
[1mStep[0m  [36/42], [94mLoss[0m : 2.53103
[1mStep[0m  [40/42], [94mLoss[0m : 2.54953

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.435, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31348
[1mStep[0m  [4/42], [94mLoss[0m : 2.42405
[1mStep[0m  [8/42], [94mLoss[0m : 2.56380
[1mStep[0m  [12/42], [94mLoss[0m : 2.36378
[1mStep[0m  [16/42], [94mLoss[0m : 2.33122
[1mStep[0m  [20/42], [94mLoss[0m : 2.56958
[1mStep[0m  [24/42], [94mLoss[0m : 2.60209
[1mStep[0m  [28/42], [94mLoss[0m : 2.58518
[1mStep[0m  [32/42], [94mLoss[0m : 2.47324
[1mStep[0m  [36/42], [94mLoss[0m : 2.44026
[1mStep[0m  [40/42], [94mLoss[0m : 2.74255

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.423, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42222
[1mStep[0m  [4/42], [94mLoss[0m : 2.53858
[1mStep[0m  [8/42], [94mLoss[0m : 2.41859
[1mStep[0m  [12/42], [94mLoss[0m : 2.48650
[1mStep[0m  [16/42], [94mLoss[0m : 2.38585
[1mStep[0m  [20/42], [94mLoss[0m : 2.60501
[1mStep[0m  [24/42], [94mLoss[0m : 2.71434
[1mStep[0m  [28/42], [94mLoss[0m : 2.51099
[1mStep[0m  [32/42], [94mLoss[0m : 2.53872
[1mStep[0m  [36/42], [94mLoss[0m : 2.24081
[1mStep[0m  [40/42], [94mLoss[0m : 2.72769

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.421, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45769
[1mStep[0m  [4/42], [94mLoss[0m : 2.56436
[1mStep[0m  [8/42], [94mLoss[0m : 2.49865
[1mStep[0m  [12/42], [94mLoss[0m : 2.57309
[1mStep[0m  [16/42], [94mLoss[0m : 2.48970
[1mStep[0m  [20/42], [94mLoss[0m : 2.44797
[1mStep[0m  [24/42], [94mLoss[0m : 2.62387
[1mStep[0m  [28/42], [94mLoss[0m : 2.47372
[1mStep[0m  [32/42], [94mLoss[0m : 2.54123
[1mStep[0m  [36/42], [94mLoss[0m : 2.38570
[1mStep[0m  [40/42], [94mLoss[0m : 2.49980

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.398, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.66140
[1mStep[0m  [4/42], [94mLoss[0m : 2.37212
[1mStep[0m  [8/42], [94mLoss[0m : 2.55972
[1mStep[0m  [12/42], [94mLoss[0m : 2.64570
[1mStep[0m  [16/42], [94mLoss[0m : 2.57339
[1mStep[0m  [20/42], [94mLoss[0m : 2.53409
[1mStep[0m  [24/42], [94mLoss[0m : 2.42697
[1mStep[0m  [28/42], [94mLoss[0m : 2.43763
[1mStep[0m  [32/42], [94mLoss[0m : 2.39503
[1mStep[0m  [36/42], [94mLoss[0m : 2.36653
[1mStep[0m  [40/42], [94mLoss[0m : 2.24438

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.394, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58183
[1mStep[0m  [4/42], [94mLoss[0m : 2.34292
[1mStep[0m  [8/42], [94mLoss[0m : 2.50898
[1mStep[0m  [12/42], [94mLoss[0m : 2.66520
[1mStep[0m  [16/42], [94mLoss[0m : 2.72306
[1mStep[0m  [20/42], [94mLoss[0m : 2.48188
[1mStep[0m  [24/42], [94mLoss[0m : 2.64285
[1mStep[0m  [28/42], [94mLoss[0m : 2.42456
[1mStep[0m  [32/42], [94mLoss[0m : 2.33984
[1mStep[0m  [36/42], [94mLoss[0m : 2.51906
[1mStep[0m  [40/42], [94mLoss[0m : 2.63149

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.388, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44226
[1mStep[0m  [4/42], [94mLoss[0m : 2.65205
[1mStep[0m  [8/42], [94mLoss[0m : 2.40877
[1mStep[0m  [12/42], [94mLoss[0m : 2.56100
[1mStep[0m  [16/42], [94mLoss[0m : 2.51471
[1mStep[0m  [20/42], [94mLoss[0m : 2.48300
[1mStep[0m  [24/42], [94mLoss[0m : 2.17078
[1mStep[0m  [28/42], [94mLoss[0m : 2.53303
[1mStep[0m  [32/42], [94mLoss[0m : 2.47618
[1mStep[0m  [36/42], [94mLoss[0m : 2.54104
[1mStep[0m  [40/42], [94mLoss[0m : 2.37888

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.383, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44352
[1mStep[0m  [4/42], [94mLoss[0m : 2.46927
[1mStep[0m  [8/42], [94mLoss[0m : 2.66615
[1mStep[0m  [12/42], [94mLoss[0m : 2.37344
[1mStep[0m  [16/42], [94mLoss[0m : 2.55693
[1mStep[0m  [20/42], [94mLoss[0m : 2.67888
[1mStep[0m  [24/42], [94mLoss[0m : 2.38894
[1mStep[0m  [28/42], [94mLoss[0m : 2.51620
[1mStep[0m  [32/42], [94mLoss[0m : 2.42289
[1mStep[0m  [36/42], [94mLoss[0m : 2.24547
[1mStep[0m  [40/42], [94mLoss[0m : 2.49171

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.374, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50774
[1mStep[0m  [4/42], [94mLoss[0m : 2.33320
[1mStep[0m  [8/42], [94mLoss[0m : 2.27645
[1mStep[0m  [12/42], [94mLoss[0m : 2.45695
[1mStep[0m  [16/42], [94mLoss[0m : 2.52371
[1mStep[0m  [20/42], [94mLoss[0m : 2.35333
[1mStep[0m  [24/42], [94mLoss[0m : 2.73948
[1mStep[0m  [28/42], [94mLoss[0m : 2.69496
[1mStep[0m  [32/42], [94mLoss[0m : 2.47443
[1mStep[0m  [36/42], [94mLoss[0m : 2.42396
[1mStep[0m  [40/42], [94mLoss[0m : 2.31372

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.371, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39696
[1mStep[0m  [4/42], [94mLoss[0m : 2.41224
[1mStep[0m  [8/42], [94mLoss[0m : 2.54376
[1mStep[0m  [12/42], [94mLoss[0m : 2.50962
[1mStep[0m  [16/42], [94mLoss[0m : 2.26422
[1mStep[0m  [20/42], [94mLoss[0m : 2.50024
[1mStep[0m  [24/42], [94mLoss[0m : 2.28537
[1mStep[0m  [28/42], [94mLoss[0m : 2.62965
[1mStep[0m  [32/42], [94mLoss[0m : 2.56889
[1mStep[0m  [36/42], [94mLoss[0m : 2.41328
[1mStep[0m  [40/42], [94mLoss[0m : 2.38259

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.373, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41683
[1mStep[0m  [4/42], [94mLoss[0m : 2.50890
[1mStep[0m  [8/42], [94mLoss[0m : 2.52010
[1mStep[0m  [12/42], [94mLoss[0m : 2.52095
[1mStep[0m  [16/42], [94mLoss[0m : 2.65101
[1mStep[0m  [20/42], [94mLoss[0m : 2.37548
[1mStep[0m  [24/42], [94mLoss[0m : 2.71285
[1mStep[0m  [28/42], [94mLoss[0m : 2.27651
[1mStep[0m  [32/42], [94mLoss[0m : 2.27036
[1mStep[0m  [36/42], [94mLoss[0m : 2.48055
[1mStep[0m  [40/42], [94mLoss[0m : 2.19534

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.373, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50237
[1mStep[0m  [4/42], [94mLoss[0m : 2.60247
[1mStep[0m  [8/42], [94mLoss[0m : 2.30480
[1mStep[0m  [12/42], [94mLoss[0m : 2.44690
[1mStep[0m  [16/42], [94mLoss[0m : 2.45079
[1mStep[0m  [20/42], [94mLoss[0m : 2.46412
[1mStep[0m  [24/42], [94mLoss[0m : 2.64656
[1mStep[0m  [28/42], [94mLoss[0m : 2.29511
[1mStep[0m  [32/42], [94mLoss[0m : 2.40152
[1mStep[0m  [36/42], [94mLoss[0m : 2.61545
[1mStep[0m  [40/42], [94mLoss[0m : 2.30692

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.365, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41383
[1mStep[0m  [4/42], [94mLoss[0m : 2.31397
[1mStep[0m  [8/42], [94mLoss[0m : 2.38291
[1mStep[0m  [12/42], [94mLoss[0m : 2.33540
[1mStep[0m  [16/42], [94mLoss[0m : 2.42999
[1mStep[0m  [20/42], [94mLoss[0m : 2.51442
[1mStep[0m  [24/42], [94mLoss[0m : 2.63314
[1mStep[0m  [28/42], [94mLoss[0m : 2.42764
[1mStep[0m  [32/42], [94mLoss[0m : 2.46251
[1mStep[0m  [36/42], [94mLoss[0m : 2.46838
[1mStep[0m  [40/42], [94mLoss[0m : 2.23383

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.368, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64459
[1mStep[0m  [4/42], [94mLoss[0m : 2.33351
[1mStep[0m  [8/42], [94mLoss[0m : 2.52887
[1mStep[0m  [12/42], [94mLoss[0m : 2.24584
[1mStep[0m  [16/42], [94mLoss[0m : 2.37067
[1mStep[0m  [20/42], [94mLoss[0m : 2.53046
[1mStep[0m  [24/42], [94mLoss[0m : 2.62818
[1mStep[0m  [28/42], [94mLoss[0m : 2.54778
[1mStep[0m  [32/42], [94mLoss[0m : 2.42357
[1mStep[0m  [36/42], [94mLoss[0m : 2.65080
[1mStep[0m  [40/42], [94mLoss[0m : 2.64385

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.361, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38410
[1mStep[0m  [4/42], [94mLoss[0m : 2.36008
[1mStep[0m  [8/42], [94mLoss[0m : 2.58783
[1mStep[0m  [12/42], [94mLoss[0m : 2.57074
[1mStep[0m  [16/42], [94mLoss[0m : 2.56023
[1mStep[0m  [20/42], [94mLoss[0m : 2.32152
[1mStep[0m  [24/42], [94mLoss[0m : 2.49658
[1mStep[0m  [28/42], [94mLoss[0m : 2.35694
[1mStep[0m  [32/42], [94mLoss[0m : 2.65566
[1mStep[0m  [36/42], [94mLoss[0m : 2.52865
[1mStep[0m  [40/42], [94mLoss[0m : 2.84723

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.357, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41928
[1mStep[0m  [4/42], [94mLoss[0m : 2.35921
[1mStep[0m  [8/42], [94mLoss[0m : 2.46619
[1mStep[0m  [12/42], [94mLoss[0m : 2.55882
[1mStep[0m  [16/42], [94mLoss[0m : 2.40459
[1mStep[0m  [20/42], [94mLoss[0m : 2.41450
[1mStep[0m  [24/42], [94mLoss[0m : 2.53279
[1mStep[0m  [28/42], [94mLoss[0m : 2.46020
[1mStep[0m  [32/42], [94mLoss[0m : 2.34015
[1mStep[0m  [36/42], [94mLoss[0m : 2.27327
[1mStep[0m  [40/42], [94mLoss[0m : 2.48122

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.348, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31616
[1mStep[0m  [4/42], [94mLoss[0m : 2.53210
[1mStep[0m  [8/42], [94mLoss[0m : 2.35129
[1mStep[0m  [12/42], [94mLoss[0m : 2.57467
[1mStep[0m  [16/42], [94mLoss[0m : 2.39296
[1mStep[0m  [20/42], [94mLoss[0m : 2.51615
[1mStep[0m  [24/42], [94mLoss[0m : 2.41265
[1mStep[0m  [28/42], [94mLoss[0m : 2.49906
[1mStep[0m  [32/42], [94mLoss[0m : 2.56808
[1mStep[0m  [36/42], [94mLoss[0m : 2.37804
[1mStep[0m  [40/42], [94mLoss[0m : 2.27869

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.345, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37565
[1mStep[0m  [4/42], [94mLoss[0m : 2.52801
[1mStep[0m  [8/42], [94mLoss[0m : 2.49974
[1mStep[0m  [12/42], [94mLoss[0m : 2.52168
[1mStep[0m  [16/42], [94mLoss[0m : 2.35333
[1mStep[0m  [20/42], [94mLoss[0m : 2.54051
[1mStep[0m  [24/42], [94mLoss[0m : 2.49095
[1mStep[0m  [28/42], [94mLoss[0m : 2.27045
[1mStep[0m  [32/42], [94mLoss[0m : 2.42472
[1mStep[0m  [36/42], [94mLoss[0m : 2.58458
[1mStep[0m  [40/42], [94mLoss[0m : 2.35379

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.357, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53116
[1mStep[0m  [4/42], [94mLoss[0m : 2.42664
[1mStep[0m  [8/42], [94mLoss[0m : 2.50104
[1mStep[0m  [12/42], [94mLoss[0m : 2.37890
[1mStep[0m  [16/42], [94mLoss[0m : 2.37440
[1mStep[0m  [20/42], [94mLoss[0m : 2.32039
[1mStep[0m  [24/42], [94mLoss[0m : 2.40011
[1mStep[0m  [28/42], [94mLoss[0m : 2.31183
[1mStep[0m  [32/42], [94mLoss[0m : 2.41963
[1mStep[0m  [36/42], [94mLoss[0m : 2.72392
[1mStep[0m  [40/42], [94mLoss[0m : 2.48550

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.352, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64060
[1mStep[0m  [4/42], [94mLoss[0m : 2.72821
[1mStep[0m  [8/42], [94mLoss[0m : 2.37865
[1mStep[0m  [12/42], [94mLoss[0m : 2.29620
[1mStep[0m  [16/42], [94mLoss[0m : 2.36237
[1mStep[0m  [20/42], [94mLoss[0m : 2.52842
[1mStep[0m  [24/42], [94mLoss[0m : 2.42661
[1mStep[0m  [28/42], [94mLoss[0m : 2.33946
[1mStep[0m  [32/42], [94mLoss[0m : 2.42837
[1mStep[0m  [36/42], [94mLoss[0m : 2.43657
[1mStep[0m  [40/42], [94mLoss[0m : 2.49249

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.348, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.30204
[1mStep[0m  [4/42], [94mLoss[0m : 2.30534
[1mStep[0m  [8/42], [94mLoss[0m : 2.50553
[1mStep[0m  [12/42], [94mLoss[0m : 2.49644
[1mStep[0m  [16/42], [94mLoss[0m : 2.20380
[1mStep[0m  [20/42], [94mLoss[0m : 2.71847
[1mStep[0m  [24/42], [94mLoss[0m : 2.30011
[1mStep[0m  [28/42], [94mLoss[0m : 2.46887
[1mStep[0m  [32/42], [94mLoss[0m : 2.36829
[1mStep[0m  [36/42], [94mLoss[0m : 2.61173
[1mStep[0m  [40/42], [94mLoss[0m : 2.49094

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.348, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40086
[1mStep[0m  [4/42], [94mLoss[0m : 2.62578
[1mStep[0m  [8/42], [94mLoss[0m : 2.51488
[1mStep[0m  [12/42], [94mLoss[0m : 2.48142
[1mStep[0m  [16/42], [94mLoss[0m : 2.33864
[1mStep[0m  [20/42], [94mLoss[0m : 2.47462
[1mStep[0m  [24/42], [94mLoss[0m : 2.29694
[1mStep[0m  [28/42], [94mLoss[0m : 2.60409
[1mStep[0m  [32/42], [94mLoss[0m : 2.24806
[1mStep[0m  [36/42], [94mLoss[0m : 2.11281
[1mStep[0m  [40/42], [94mLoss[0m : 2.41485

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.347, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36257
[1mStep[0m  [4/42], [94mLoss[0m : 2.24363
[1mStep[0m  [8/42], [94mLoss[0m : 2.53445
[1mStep[0m  [12/42], [94mLoss[0m : 2.44723
[1mStep[0m  [16/42], [94mLoss[0m : 2.28925
[1mStep[0m  [20/42], [94mLoss[0m : 2.41923
[1mStep[0m  [24/42], [94mLoss[0m : 2.52206
[1mStep[0m  [28/42], [94mLoss[0m : 2.36540
[1mStep[0m  [32/42], [94mLoss[0m : 2.37336
[1mStep[0m  [36/42], [94mLoss[0m : 2.22284
[1mStep[0m  [40/42], [94mLoss[0m : 2.56371

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.341, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36831
[1mStep[0m  [4/42], [94mLoss[0m : 2.30415
[1mStep[0m  [8/42], [94mLoss[0m : 2.45282
[1mStep[0m  [12/42], [94mLoss[0m : 2.42079
[1mStep[0m  [16/42], [94mLoss[0m : 2.54066
[1mStep[0m  [20/42], [94mLoss[0m : 2.42383
[1mStep[0m  [24/42], [94mLoss[0m : 2.35358
[1mStep[0m  [28/42], [94mLoss[0m : 2.53327
[1mStep[0m  [32/42], [94mLoss[0m : 2.49977
[1mStep[0m  [36/42], [94mLoss[0m : 2.58484
[1mStep[0m  [40/42], [94mLoss[0m : 2.29224

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.349, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34372
[1mStep[0m  [4/42], [94mLoss[0m : 2.42306
[1mStep[0m  [8/42], [94mLoss[0m : 2.47910
[1mStep[0m  [12/42], [94mLoss[0m : 2.33760
[1mStep[0m  [16/42], [94mLoss[0m : 2.50099
[1mStep[0m  [20/42], [94mLoss[0m : 2.51339
[1mStep[0m  [24/42], [94mLoss[0m : 2.41620
[1mStep[0m  [28/42], [94mLoss[0m : 2.43543
[1mStep[0m  [32/42], [94mLoss[0m : 2.69235
[1mStep[0m  [36/42], [94mLoss[0m : 2.42090
[1mStep[0m  [40/42], [94mLoss[0m : 2.46025

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.342, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.345
====================================

Phase 1 - Evaluation MAE:  2.3450064148221696
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 2.55974
[1mStep[0m  [4/42], [94mLoss[0m : 2.53573
[1mStep[0m  [8/42], [94mLoss[0m : 2.29674
[1mStep[0m  [12/42], [94mLoss[0m : 2.50498
[1mStep[0m  [16/42], [94mLoss[0m : 2.42682
[1mStep[0m  [20/42], [94mLoss[0m : 2.32907
[1mStep[0m  [24/42], [94mLoss[0m : 2.60198
[1mStep[0m  [28/42], [94mLoss[0m : 2.53344
[1mStep[0m  [32/42], [94mLoss[0m : 2.35969
[1mStep[0m  [36/42], [94mLoss[0m : 2.41889
[1mStep[0m  [40/42], [94mLoss[0m : 2.44205

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.345, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40277
[1mStep[0m  [4/42], [94mLoss[0m : 2.52591
[1mStep[0m  [8/42], [94mLoss[0m : 2.57142
[1mStep[0m  [12/42], [94mLoss[0m : 2.61413
[1mStep[0m  [16/42], [94mLoss[0m : 2.34104
[1mStep[0m  [20/42], [94mLoss[0m : 2.36095
[1mStep[0m  [24/42], [94mLoss[0m : 2.20256
[1mStep[0m  [28/42], [94mLoss[0m : 2.43603
[1mStep[0m  [32/42], [94mLoss[0m : 2.47010
[1mStep[0m  [36/42], [94mLoss[0m : 2.52428
[1mStep[0m  [40/42], [94mLoss[0m : 2.62501

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.352, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34809
[1mStep[0m  [4/42], [94mLoss[0m : 2.39369
[1mStep[0m  [8/42], [94mLoss[0m : 2.37204
[1mStep[0m  [12/42], [94mLoss[0m : 2.35988
[1mStep[0m  [16/42], [94mLoss[0m : 2.36399
[1mStep[0m  [20/42], [94mLoss[0m : 2.36257
[1mStep[0m  [24/42], [94mLoss[0m : 2.37797
[1mStep[0m  [28/42], [94mLoss[0m : 2.41444
[1mStep[0m  [32/42], [94mLoss[0m : 2.25706
[1mStep[0m  [36/42], [94mLoss[0m : 2.46389
[1mStep[0m  [40/42], [94mLoss[0m : 2.68142

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.334, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46253
[1mStep[0m  [4/42], [94mLoss[0m : 2.31885
[1mStep[0m  [8/42], [94mLoss[0m : 2.38762
[1mStep[0m  [12/42], [94mLoss[0m : 2.43282
[1mStep[0m  [16/42], [94mLoss[0m : 2.48319
[1mStep[0m  [20/42], [94mLoss[0m : 2.40373
[1mStep[0m  [24/42], [94mLoss[0m : 2.37636
[1mStep[0m  [28/42], [94mLoss[0m : 2.44989
[1mStep[0m  [32/42], [94mLoss[0m : 2.38211
[1mStep[0m  [36/42], [94mLoss[0m : 2.46298
[1mStep[0m  [40/42], [94mLoss[0m : 2.43913

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.329, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43276
[1mStep[0m  [4/42], [94mLoss[0m : 2.27918
[1mStep[0m  [8/42], [94mLoss[0m : 2.47377
[1mStep[0m  [12/42], [94mLoss[0m : 2.17638
[1mStep[0m  [16/42], [94mLoss[0m : 2.13601
[1mStep[0m  [20/42], [94mLoss[0m : 2.26169
[1mStep[0m  [24/42], [94mLoss[0m : 2.42016
[1mStep[0m  [28/42], [94mLoss[0m : 2.49446
[1mStep[0m  [32/42], [94mLoss[0m : 2.38843
[1mStep[0m  [36/42], [94mLoss[0m : 2.45552
[1mStep[0m  [40/42], [94mLoss[0m : 2.32814

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.383, [92mTest[0m: 2.329, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.13597
[1mStep[0m  [4/42], [94mLoss[0m : 2.57709
[1mStep[0m  [8/42], [94mLoss[0m : 2.29874
[1mStep[0m  [12/42], [94mLoss[0m : 2.32845
[1mStep[0m  [16/42], [94mLoss[0m : 2.10840
[1mStep[0m  [20/42], [94mLoss[0m : 2.61538
[1mStep[0m  [24/42], [94mLoss[0m : 2.34728
[1mStep[0m  [28/42], [94mLoss[0m : 2.27882
[1mStep[0m  [32/42], [94mLoss[0m : 2.48488
[1mStep[0m  [36/42], [94mLoss[0m : 2.28661
[1mStep[0m  [40/42], [94mLoss[0m : 2.41905

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.367, [92mTest[0m: 2.314, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24666
[1mStep[0m  [4/42], [94mLoss[0m : 2.63634
[1mStep[0m  [8/42], [94mLoss[0m : 2.35126
[1mStep[0m  [12/42], [94mLoss[0m : 2.20246
[1mStep[0m  [16/42], [94mLoss[0m : 2.34132
[1mStep[0m  [20/42], [94mLoss[0m : 2.56702
[1mStep[0m  [24/42], [94mLoss[0m : 2.55876
[1mStep[0m  [28/42], [94mLoss[0m : 2.34248
[1mStep[0m  [32/42], [94mLoss[0m : 2.47779
[1mStep[0m  [36/42], [94mLoss[0m : 2.26323
[1mStep[0m  [40/42], [94mLoss[0m : 2.45056

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.370, [92mTest[0m: 2.324, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33721
[1mStep[0m  [4/42], [94mLoss[0m : 2.35244
[1mStep[0m  [8/42], [94mLoss[0m : 2.24798
[1mStep[0m  [12/42], [94mLoss[0m : 2.53622
[1mStep[0m  [16/42], [94mLoss[0m : 2.42017
[1mStep[0m  [20/42], [94mLoss[0m : 2.22137
[1mStep[0m  [24/42], [94mLoss[0m : 2.38649
[1mStep[0m  [28/42], [94mLoss[0m : 2.32876
[1mStep[0m  [32/42], [94mLoss[0m : 2.30138
[1mStep[0m  [36/42], [94mLoss[0m : 2.41481
[1mStep[0m  [40/42], [94mLoss[0m : 2.36922

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.344, [92mTest[0m: 2.366, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26278
[1mStep[0m  [4/42], [94mLoss[0m : 2.03093
[1mStep[0m  [8/42], [94mLoss[0m : 2.51430
[1mStep[0m  [12/42], [94mLoss[0m : 2.39897
[1mStep[0m  [16/42], [94mLoss[0m : 2.35097
[1mStep[0m  [20/42], [94mLoss[0m : 2.17937
[1mStep[0m  [24/42], [94mLoss[0m : 2.17134
[1mStep[0m  [28/42], [94mLoss[0m : 2.29291
[1mStep[0m  [32/42], [94mLoss[0m : 2.31366
[1mStep[0m  [36/42], [94mLoss[0m : 2.45774
[1mStep[0m  [40/42], [94mLoss[0m : 2.59805

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.332, [92mTest[0m: 2.359, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49179
[1mStep[0m  [4/42], [94mLoss[0m : 2.40279
[1mStep[0m  [8/42], [94mLoss[0m : 2.39491
[1mStep[0m  [12/42], [94mLoss[0m : 2.19950
[1mStep[0m  [16/42], [94mLoss[0m : 2.50040
[1mStep[0m  [20/42], [94mLoss[0m : 2.25510
[1mStep[0m  [24/42], [94mLoss[0m : 2.41693
[1mStep[0m  [28/42], [94mLoss[0m : 2.32882
[1mStep[0m  [32/42], [94mLoss[0m : 2.16110
[1mStep[0m  [36/42], [94mLoss[0m : 2.35005
[1mStep[0m  [40/42], [94mLoss[0m : 2.28838

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.315, [92mTest[0m: 2.398, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31881
[1mStep[0m  [4/42], [94mLoss[0m : 2.38828
[1mStep[0m  [8/42], [94mLoss[0m : 2.38347
[1mStep[0m  [12/42], [94mLoss[0m : 2.32527
[1mStep[0m  [16/42], [94mLoss[0m : 2.57125
[1mStep[0m  [20/42], [94mLoss[0m : 2.27021
[1mStep[0m  [24/42], [94mLoss[0m : 2.15906
[1mStep[0m  [28/42], [94mLoss[0m : 2.51160
[1mStep[0m  [32/42], [94mLoss[0m : 2.40334
[1mStep[0m  [36/42], [94mLoss[0m : 2.15276
[1mStep[0m  [40/42], [94mLoss[0m : 2.20174

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.295, [92mTest[0m: 2.392, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.17952
[1mStep[0m  [4/42], [94mLoss[0m : 2.20990
[1mStep[0m  [8/42], [94mLoss[0m : 2.56407
[1mStep[0m  [12/42], [94mLoss[0m : 2.09541
[1mStep[0m  [16/42], [94mLoss[0m : 2.44794
[1mStep[0m  [20/42], [94mLoss[0m : 2.07256
[1mStep[0m  [24/42], [94mLoss[0m : 2.21684
[1mStep[0m  [28/42], [94mLoss[0m : 2.22207
[1mStep[0m  [32/42], [94mLoss[0m : 2.32159
[1mStep[0m  [36/42], [94mLoss[0m : 2.24813
[1mStep[0m  [40/42], [94mLoss[0m : 2.24251

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.290, [92mTest[0m: 2.354, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42181
[1mStep[0m  [4/42], [94mLoss[0m : 2.18061
[1mStep[0m  [8/42], [94mLoss[0m : 2.10329
[1mStep[0m  [12/42], [94mLoss[0m : 2.34105
[1mStep[0m  [16/42], [94mLoss[0m : 2.41209
[1mStep[0m  [20/42], [94mLoss[0m : 2.27713
[1mStep[0m  [24/42], [94mLoss[0m : 2.39977
[1mStep[0m  [28/42], [94mLoss[0m : 2.24618
[1mStep[0m  [32/42], [94mLoss[0m : 2.34806
[1mStep[0m  [36/42], [94mLoss[0m : 2.66811
[1mStep[0m  [40/42], [94mLoss[0m : 2.31962

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.270, [92mTest[0m: 2.367, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.18920
[1mStep[0m  [4/42], [94mLoss[0m : 1.99981
[1mStep[0m  [8/42], [94mLoss[0m : 2.37411
[1mStep[0m  [12/42], [94mLoss[0m : 2.32063
[1mStep[0m  [16/42], [94mLoss[0m : 2.16188
[1mStep[0m  [20/42], [94mLoss[0m : 2.20288
[1mStep[0m  [24/42], [94mLoss[0m : 2.30820
[1mStep[0m  [28/42], [94mLoss[0m : 2.30464
[1mStep[0m  [32/42], [94mLoss[0m : 2.24061
[1mStep[0m  [36/42], [94mLoss[0m : 2.33616
[1mStep[0m  [40/42], [94mLoss[0m : 2.03905

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.259, [92mTest[0m: 2.364, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40353
[1mStep[0m  [4/42], [94mLoss[0m : 2.15411
[1mStep[0m  [8/42], [94mLoss[0m : 2.47866
[1mStep[0m  [12/42], [94mLoss[0m : 2.30278
[1mStep[0m  [16/42], [94mLoss[0m : 2.19875
[1mStep[0m  [20/42], [94mLoss[0m : 2.18543
[1mStep[0m  [24/42], [94mLoss[0m : 2.28743
[1mStep[0m  [28/42], [94mLoss[0m : 2.39624
[1mStep[0m  [32/42], [94mLoss[0m : 2.08692
[1mStep[0m  [36/42], [94mLoss[0m : 2.28532
[1mStep[0m  [40/42], [94mLoss[0m : 2.25763

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.249, [92mTest[0m: 2.379, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.16962
[1mStep[0m  [4/42], [94mLoss[0m : 2.23794
[1mStep[0m  [8/42], [94mLoss[0m : 2.00549
[1mStep[0m  [12/42], [94mLoss[0m : 2.11375
[1mStep[0m  [16/42], [94mLoss[0m : 2.20172
[1mStep[0m  [20/42], [94mLoss[0m : 2.30224
[1mStep[0m  [24/42], [94mLoss[0m : 2.17331
[1mStep[0m  [28/42], [94mLoss[0m : 2.37140
[1mStep[0m  [32/42], [94mLoss[0m : 2.39743
[1mStep[0m  [36/42], [94mLoss[0m : 2.09792
[1mStep[0m  [40/42], [94mLoss[0m : 2.52812

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.224, [92mTest[0m: 2.385, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.11108
[1mStep[0m  [4/42], [94mLoss[0m : 2.25167
[1mStep[0m  [8/42], [94mLoss[0m : 2.32287
[1mStep[0m  [12/42], [94mLoss[0m : 2.10470
[1mStep[0m  [16/42], [94mLoss[0m : 2.29090
[1mStep[0m  [20/42], [94mLoss[0m : 2.38633
[1mStep[0m  [24/42], [94mLoss[0m : 2.35326
[1mStep[0m  [28/42], [94mLoss[0m : 2.25498
[1mStep[0m  [32/42], [94mLoss[0m : 2.04976
[1mStep[0m  [36/42], [94mLoss[0m : 2.17332
[1mStep[0m  [40/42], [94mLoss[0m : 2.25985

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.208, [92mTest[0m: 2.391, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.00048
[1mStep[0m  [4/42], [94mLoss[0m : 2.28024
[1mStep[0m  [8/42], [94mLoss[0m : 2.04264
[1mStep[0m  [12/42], [94mLoss[0m : 2.14599
[1mStep[0m  [16/42], [94mLoss[0m : 2.20430
[1mStep[0m  [20/42], [94mLoss[0m : 2.29084
[1mStep[0m  [24/42], [94mLoss[0m : 2.18210
[1mStep[0m  [28/42], [94mLoss[0m : 2.27627
[1mStep[0m  [32/42], [94mLoss[0m : 2.05308
[1mStep[0m  [36/42], [94mLoss[0m : 2.30018
[1mStep[0m  [40/42], [94mLoss[0m : 2.32247

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.179, [92mTest[0m: 2.383, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.22788
[1mStep[0m  [4/42], [94mLoss[0m : 2.13985
[1mStep[0m  [8/42], [94mLoss[0m : 2.11302
[1mStep[0m  [12/42], [94mLoss[0m : 2.04248
[1mStep[0m  [16/42], [94mLoss[0m : 2.06552
[1mStep[0m  [20/42], [94mLoss[0m : 2.04802
[1mStep[0m  [24/42], [94mLoss[0m : 2.00018
[1mStep[0m  [28/42], [94mLoss[0m : 1.99740
[1mStep[0m  [32/42], [94mLoss[0m : 2.06001
[1mStep[0m  [36/42], [94mLoss[0m : 2.18554
[1mStep[0m  [40/42], [94mLoss[0m : 2.40211

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.157, [92mTest[0m: 2.390, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.17040
[1mStep[0m  [4/42], [94mLoss[0m : 2.12665
[1mStep[0m  [8/42], [94mLoss[0m : 1.79866
[1mStep[0m  [12/42], [94mLoss[0m : 2.34324
[1mStep[0m  [16/42], [94mLoss[0m : 2.21846
[1mStep[0m  [20/42], [94mLoss[0m : 2.19177
[1mStep[0m  [24/42], [94mLoss[0m : 2.28204
[1mStep[0m  [28/42], [94mLoss[0m : 2.28067
[1mStep[0m  [32/42], [94mLoss[0m : 2.19678
[1mStep[0m  [36/42], [94mLoss[0m : 2.14757
[1mStep[0m  [40/42], [94mLoss[0m : 2.25639

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.142, [92mTest[0m: 2.409, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.15949
[1mStep[0m  [4/42], [94mLoss[0m : 2.13305
[1mStep[0m  [8/42], [94mLoss[0m : 2.14958
[1mStep[0m  [12/42], [94mLoss[0m : 2.18303
[1mStep[0m  [16/42], [94mLoss[0m : 2.35456
[1mStep[0m  [20/42], [94mLoss[0m : 2.07874
[1mStep[0m  [24/42], [94mLoss[0m : 2.10481
[1mStep[0m  [28/42], [94mLoss[0m : 2.27932
[1mStep[0m  [32/42], [94mLoss[0m : 2.01644
[1mStep[0m  [36/42], [94mLoss[0m : 2.14066
[1mStep[0m  [40/42], [94mLoss[0m : 2.09457

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.134, [92mTest[0m: 2.385, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.16672
[1mStep[0m  [4/42], [94mLoss[0m : 2.00079
[1mStep[0m  [8/42], [94mLoss[0m : 1.95085
[1mStep[0m  [12/42], [94mLoss[0m : 2.26851
[1mStep[0m  [16/42], [94mLoss[0m : 1.89279
[1mStep[0m  [20/42], [94mLoss[0m : 2.19336
[1mStep[0m  [24/42], [94mLoss[0m : 2.15806
[1mStep[0m  [28/42], [94mLoss[0m : 1.95665
[1mStep[0m  [32/42], [94mLoss[0m : 2.09538
[1mStep[0m  [36/42], [94mLoss[0m : 2.00160
[1mStep[0m  [40/42], [94mLoss[0m : 2.27795

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.088, [92mTest[0m: 2.426, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.15656
[1mStep[0m  [4/42], [94mLoss[0m : 2.11173
[1mStep[0m  [8/42], [94mLoss[0m : 2.08660
[1mStep[0m  [12/42], [94mLoss[0m : 1.99645
[1mStep[0m  [16/42], [94mLoss[0m : 2.05973
[1mStep[0m  [20/42], [94mLoss[0m : 2.07696
[1mStep[0m  [24/42], [94mLoss[0m : 1.98127
[1mStep[0m  [28/42], [94mLoss[0m : 2.01270
[1mStep[0m  [32/42], [94mLoss[0m : 2.19836
[1mStep[0m  [36/42], [94mLoss[0m : 2.11568
[1mStep[0m  [40/42], [94mLoss[0m : 2.18656

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.089, [92mTest[0m: 2.400, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.01811
[1mStep[0m  [4/42], [94mLoss[0m : 2.20555
[1mStep[0m  [8/42], [94mLoss[0m : 2.03291
[1mStep[0m  [12/42], [94mLoss[0m : 1.87469
[1mStep[0m  [16/42], [94mLoss[0m : 2.31739
[1mStep[0m  [20/42], [94mLoss[0m : 2.14672
[1mStep[0m  [24/42], [94mLoss[0m : 1.95418
[1mStep[0m  [28/42], [94mLoss[0m : 2.13712
[1mStep[0m  [32/42], [94mLoss[0m : 1.93669
[1mStep[0m  [36/42], [94mLoss[0m : 1.81058
[1mStep[0m  [40/42], [94mLoss[0m : 2.05637

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.064, [92mTest[0m: 2.394, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.03579
[1mStep[0m  [4/42], [94mLoss[0m : 1.93434
[1mStep[0m  [8/42], [94mLoss[0m : 1.94416
[1mStep[0m  [12/42], [94mLoss[0m : 2.07464
[1mStep[0m  [16/42], [94mLoss[0m : 1.98104
[1mStep[0m  [20/42], [94mLoss[0m : 2.02120
[1mStep[0m  [24/42], [94mLoss[0m : 2.11773
[1mStep[0m  [28/42], [94mLoss[0m : 1.96612
[1mStep[0m  [32/42], [94mLoss[0m : 2.07064
[1mStep[0m  [36/42], [94mLoss[0m : 2.00408
[1mStep[0m  [40/42], [94mLoss[0m : 2.06452

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.029, [92mTest[0m: 2.382, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.06543
[1mStep[0m  [4/42], [94mLoss[0m : 1.90246
[1mStep[0m  [8/42], [94mLoss[0m : 2.04568
[1mStep[0m  [12/42], [94mLoss[0m : 2.05067
[1mStep[0m  [16/42], [94mLoss[0m : 2.10822
[1mStep[0m  [20/42], [94mLoss[0m : 1.99407
[1mStep[0m  [24/42], [94mLoss[0m : 2.01882
[1mStep[0m  [28/42], [94mLoss[0m : 1.99341
[1mStep[0m  [32/42], [94mLoss[0m : 1.86496
[1mStep[0m  [36/42], [94mLoss[0m : 1.85096
[1mStep[0m  [40/42], [94mLoss[0m : 2.08255

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.996, [92mTest[0m: 2.381, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.03325
[1mStep[0m  [4/42], [94mLoss[0m : 1.92932
[1mStep[0m  [8/42], [94mLoss[0m : 1.94372
[1mStep[0m  [12/42], [94mLoss[0m : 1.99068
[1mStep[0m  [16/42], [94mLoss[0m : 2.11981
[1mStep[0m  [20/42], [94mLoss[0m : 1.93402
[1mStep[0m  [24/42], [94mLoss[0m : 2.10847
[1mStep[0m  [28/42], [94mLoss[0m : 2.10968
[1mStep[0m  [32/42], [94mLoss[0m : 2.17621
[1mStep[0m  [36/42], [94mLoss[0m : 1.84991
[1mStep[0m  [40/42], [94mLoss[0m : 1.94341

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.991, [92mTest[0m: 2.423, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.94373
[1mStep[0m  [4/42], [94mLoss[0m : 2.10701
[1mStep[0m  [8/42], [94mLoss[0m : 2.13422
[1mStep[0m  [12/42], [94mLoss[0m : 2.00594
[1mStep[0m  [16/42], [94mLoss[0m : 1.87134
[1mStep[0m  [20/42], [94mLoss[0m : 1.99697
[1mStep[0m  [24/42], [94mLoss[0m : 2.05240
[1mStep[0m  [28/42], [94mLoss[0m : 2.03550
[1mStep[0m  [32/42], [94mLoss[0m : 1.81252
[1mStep[0m  [36/42], [94mLoss[0m : 1.90716
[1mStep[0m  [40/42], [94mLoss[0m : 1.97016

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.969, [92mTest[0m: 2.413, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.93835
[1mStep[0m  [4/42], [94mLoss[0m : 2.00824
[1mStep[0m  [8/42], [94mLoss[0m : 1.91732
[1mStep[0m  [12/42], [94mLoss[0m : 1.93173
[1mStep[0m  [16/42], [94mLoss[0m : 1.90741
[1mStep[0m  [20/42], [94mLoss[0m : 1.84002
[1mStep[0m  [24/42], [94mLoss[0m : 1.89358
[1mStep[0m  [28/42], [94mLoss[0m : 1.97498
[1mStep[0m  [32/42], [94mLoss[0m : 1.71890
[1mStep[0m  [36/42], [94mLoss[0m : 1.92729
[1mStep[0m  [40/42], [94mLoss[0m : 1.99716

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.953, [92mTest[0m: 2.429, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.75816
[1mStep[0m  [4/42], [94mLoss[0m : 1.98624
[1mStep[0m  [8/42], [94mLoss[0m : 2.00305
[1mStep[0m  [12/42], [94mLoss[0m : 2.03778
[1mStep[0m  [16/42], [94mLoss[0m : 1.74057
[1mStep[0m  [20/42], [94mLoss[0m : 2.01846
[1mStep[0m  [24/42], [94mLoss[0m : 1.97273
[1mStep[0m  [28/42], [94mLoss[0m : 1.92755
[1mStep[0m  [32/42], [94mLoss[0m : 1.90435
[1mStep[0m  [36/42], [94mLoss[0m : 1.93866
[1mStep[0m  [40/42], [94mLoss[0m : 1.95033

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.922, [92mTest[0m: 2.394, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.390
====================================

Phase 2 - Evaluation MAE:  2.390326142311096
MAE score P1      2.345006
MAE score P2      2.390326
loss              1.921797
learning_rate     0.002575
batch_size             256
hidden_sizes         [250]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.5
weight_decay          0.01
Name: 2, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 10.65937
[1mStep[0m  [4/42], [94mLoss[0m : 10.97523
[1mStep[0m  [8/42], [94mLoss[0m : 10.72761
[1mStep[0m  [12/42], [94mLoss[0m : 10.97655
[1mStep[0m  [16/42], [94mLoss[0m : 11.04254
[1mStep[0m  [20/42], [94mLoss[0m : 10.74115
[1mStep[0m  [24/42], [94mLoss[0m : 10.77935
[1mStep[0m  [28/42], [94mLoss[0m : 10.93168
[1mStep[0m  [32/42], [94mLoss[0m : 10.83646
[1mStep[0m  [36/42], [94mLoss[0m : 10.54047
[1mStep[0m  [40/42], [94mLoss[0m : 11.39068

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.834, [92mTest[0m: 11.004, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.51196
[1mStep[0m  [4/42], [94mLoss[0m : 10.71943
[1mStep[0m  [8/42], [94mLoss[0m : 11.08361
[1mStep[0m  [12/42], [94mLoss[0m : 10.49036
[1mStep[0m  [16/42], [94mLoss[0m : 10.85986
[1mStep[0m  [20/42], [94mLoss[0m : 10.75714
[1mStep[0m  [24/42], [94mLoss[0m : 10.83644
[1mStep[0m  [28/42], [94mLoss[0m : 10.42459
[1mStep[0m  [32/42], [94mLoss[0m : 10.54409
[1mStep[0m  [36/42], [94mLoss[0m : 10.80506
[1mStep[0m  [40/42], [94mLoss[0m : 10.72261

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.680, [92mTest[0m: 10.727, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.52643
[1mStep[0m  [4/42], [94mLoss[0m : 10.76624
[1mStep[0m  [8/42], [94mLoss[0m : 10.62220
[1mStep[0m  [12/42], [94mLoss[0m : 10.61829
[1mStep[0m  [16/42], [94mLoss[0m : 10.08253
[1mStep[0m  [20/42], [94mLoss[0m : 10.52405
[1mStep[0m  [24/42], [94mLoss[0m : 10.68198
[1mStep[0m  [28/42], [94mLoss[0m : 10.42691
[1mStep[0m  [32/42], [94mLoss[0m : 10.26902
[1mStep[0m  [36/42], [94mLoss[0m : 10.18038
[1mStep[0m  [40/42], [94mLoss[0m : 10.59139

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.537, [92mTest[0m: 10.539, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.46832
[1mStep[0m  [4/42], [94mLoss[0m : 10.14031
[1mStep[0m  [8/42], [94mLoss[0m : 10.28800
[1mStep[0m  [12/42], [94mLoss[0m : 10.70937
[1mStep[0m  [16/42], [94mLoss[0m : 10.52025
[1mStep[0m  [20/42], [94mLoss[0m : 10.58592
[1mStep[0m  [24/42], [94mLoss[0m : 10.47082
[1mStep[0m  [28/42], [94mLoss[0m : 10.34895
[1mStep[0m  [32/42], [94mLoss[0m : 9.92769
[1mStep[0m  [36/42], [94mLoss[0m : 10.39618
[1mStep[0m  [40/42], [94mLoss[0m : 9.74117

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.406, [92mTest[0m: 10.380, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.05472
[1mStep[0m  [4/42], [94mLoss[0m : 10.08691
[1mStep[0m  [8/42], [94mLoss[0m : 10.71785
[1mStep[0m  [12/42], [94mLoss[0m : 10.25096
[1mStep[0m  [16/42], [94mLoss[0m : 10.18417
[1mStep[0m  [20/42], [94mLoss[0m : 10.37682
[1mStep[0m  [24/42], [94mLoss[0m : 10.35868
[1mStep[0m  [28/42], [94mLoss[0m : 10.43723
[1mStep[0m  [32/42], [94mLoss[0m : 10.43050
[1mStep[0m  [36/42], [94mLoss[0m : 10.21947
[1mStep[0m  [40/42], [94mLoss[0m : 10.33984

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.249, [92mTest[0m: 10.195, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.44043
[1mStep[0m  [4/42], [94mLoss[0m : 10.12652
[1mStep[0m  [8/42], [94mLoss[0m : 10.16142
[1mStep[0m  [12/42], [94mLoss[0m : 10.17869
[1mStep[0m  [16/42], [94mLoss[0m : 10.12895
[1mStep[0m  [20/42], [94mLoss[0m : 10.24020
[1mStep[0m  [24/42], [94mLoss[0m : 9.90666
[1mStep[0m  [28/42], [94mLoss[0m : 10.70942
[1mStep[0m  [32/42], [94mLoss[0m : 9.83674
[1mStep[0m  [36/42], [94mLoss[0m : 9.90849
[1mStep[0m  [40/42], [94mLoss[0m : 9.93250

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.097, [92mTest[0m: 10.033, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.30087
[1mStep[0m  [4/42], [94mLoss[0m : 9.95630
[1mStep[0m  [8/42], [94mLoss[0m : 9.76754
[1mStep[0m  [12/42], [94mLoss[0m : 9.73410
[1mStep[0m  [16/42], [94mLoss[0m : 9.80151
[1mStep[0m  [20/42], [94mLoss[0m : 9.69136
[1mStep[0m  [24/42], [94mLoss[0m : 9.79219
[1mStep[0m  [28/42], [94mLoss[0m : 10.05851
[1mStep[0m  [32/42], [94mLoss[0m : 10.14773
[1mStep[0m  [36/42], [94mLoss[0m : 9.85078
[1mStep[0m  [40/42], [94mLoss[0m : 9.97549

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 9.950, [92mTest[0m: 9.824, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.70831
[1mStep[0m  [4/42], [94mLoss[0m : 9.87413
[1mStep[0m  [8/42], [94mLoss[0m : 9.85963
[1mStep[0m  [12/42], [94mLoss[0m : 9.82441
[1mStep[0m  [16/42], [94mLoss[0m : 9.66911
[1mStep[0m  [20/42], [94mLoss[0m : 10.16197
[1mStep[0m  [24/42], [94mLoss[0m : 9.89200
[1mStep[0m  [28/42], [94mLoss[0m : 10.24429
[1mStep[0m  [32/42], [94mLoss[0m : 9.86098
[1mStep[0m  [36/42], [94mLoss[0m : 9.87816
[1mStep[0m  [40/42], [94mLoss[0m : 9.56490

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 9.790, [92mTest[0m: 9.629, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.79712
[1mStep[0m  [4/42], [94mLoss[0m : 9.82374
[1mStep[0m  [8/42], [94mLoss[0m : 9.59345
[1mStep[0m  [12/42], [94mLoss[0m : 9.79140
[1mStep[0m  [16/42], [94mLoss[0m : 9.81336
[1mStep[0m  [20/42], [94mLoss[0m : 9.53016
[1mStep[0m  [24/42], [94mLoss[0m : 9.57063
[1mStep[0m  [28/42], [94mLoss[0m : 9.75639
[1mStep[0m  [32/42], [94mLoss[0m : 9.48900
[1mStep[0m  [36/42], [94mLoss[0m : 9.99698
[1mStep[0m  [40/42], [94mLoss[0m : 9.67615

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 9.628, [92mTest[0m: 9.421, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.65887
[1mStep[0m  [4/42], [94mLoss[0m : 9.93486
[1mStep[0m  [8/42], [94mLoss[0m : 9.37959
[1mStep[0m  [12/42], [94mLoss[0m : 9.75911
[1mStep[0m  [16/42], [94mLoss[0m : 9.82308
[1mStep[0m  [20/42], [94mLoss[0m : 9.77432
[1mStep[0m  [24/42], [94mLoss[0m : 9.57304
[1mStep[0m  [28/42], [94mLoss[0m : 9.37619
[1mStep[0m  [32/42], [94mLoss[0m : 9.66998
[1mStep[0m  [36/42], [94mLoss[0m : 9.30630
[1mStep[0m  [40/42], [94mLoss[0m : 9.26418

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 9.452, [92mTest[0m: 9.203, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.11264
[1mStep[0m  [4/42], [94mLoss[0m : 9.45582
[1mStep[0m  [8/42], [94mLoss[0m : 9.24404
[1mStep[0m  [12/42], [94mLoss[0m : 9.38661
[1mStep[0m  [16/42], [94mLoss[0m : 9.04132
[1mStep[0m  [20/42], [94mLoss[0m : 9.38215
[1mStep[0m  [24/42], [94mLoss[0m : 8.83300
[1mStep[0m  [28/42], [94mLoss[0m : 9.26153
[1mStep[0m  [32/42], [94mLoss[0m : 9.46114
[1mStep[0m  [36/42], [94mLoss[0m : 8.85732
[1mStep[0m  [40/42], [94mLoss[0m : 9.79452

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 9.265, [92mTest[0m: 8.983, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.26509
[1mStep[0m  [4/42], [94mLoss[0m : 8.98559
[1mStep[0m  [8/42], [94mLoss[0m : 9.55796
[1mStep[0m  [12/42], [94mLoss[0m : 9.33241
[1mStep[0m  [16/42], [94mLoss[0m : 9.14725
[1mStep[0m  [20/42], [94mLoss[0m : 8.89640
[1mStep[0m  [24/42], [94mLoss[0m : 8.90183
[1mStep[0m  [28/42], [94mLoss[0m : 8.98266
[1mStep[0m  [32/42], [94mLoss[0m : 8.74254
[1mStep[0m  [36/42], [94mLoss[0m : 8.97368
[1mStep[0m  [40/42], [94mLoss[0m : 9.22514

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 9.067, [92mTest[0m: 8.758, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.29183
[1mStep[0m  [4/42], [94mLoss[0m : 9.04724
[1mStep[0m  [8/42], [94mLoss[0m : 8.92762
[1mStep[0m  [12/42], [94mLoss[0m : 9.17189
[1mStep[0m  [16/42], [94mLoss[0m : 9.31259
[1mStep[0m  [20/42], [94mLoss[0m : 8.65750
[1mStep[0m  [24/42], [94mLoss[0m : 8.65557
[1mStep[0m  [28/42], [94mLoss[0m : 8.77411
[1mStep[0m  [32/42], [94mLoss[0m : 8.93360
[1mStep[0m  [36/42], [94mLoss[0m : 8.63170
[1mStep[0m  [40/42], [94mLoss[0m : 8.37086

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 8.857, [92mTest[0m: 8.528, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.52675
[1mStep[0m  [4/42], [94mLoss[0m : 8.70902
[1mStep[0m  [8/42], [94mLoss[0m : 8.63169
[1mStep[0m  [12/42], [94mLoss[0m : 8.84909
[1mStep[0m  [16/42], [94mLoss[0m : 8.62011
[1mStep[0m  [20/42], [94mLoss[0m : 8.97832
[1mStep[0m  [24/42], [94mLoss[0m : 8.61166
[1mStep[0m  [28/42], [94mLoss[0m : 8.71773
[1mStep[0m  [32/42], [94mLoss[0m : 8.50164
[1mStep[0m  [36/42], [94mLoss[0m : 8.14274
[1mStep[0m  [40/42], [94mLoss[0m : 8.24683

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 8.621, [92mTest[0m: 8.267, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.11965
[1mStep[0m  [4/42], [94mLoss[0m : 8.32331
[1mStep[0m  [8/42], [94mLoss[0m : 8.67817
[1mStep[0m  [12/42], [94mLoss[0m : 8.54750
[1mStep[0m  [16/42], [94mLoss[0m : 8.59122
[1mStep[0m  [20/42], [94mLoss[0m : 8.23613
[1mStep[0m  [24/42], [94mLoss[0m : 8.66264
[1mStep[0m  [28/42], [94mLoss[0m : 8.60590
[1mStep[0m  [32/42], [94mLoss[0m : 8.36975
[1mStep[0m  [36/42], [94mLoss[0m : 8.40231
[1mStep[0m  [40/42], [94mLoss[0m : 8.22245

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 8.383, [92mTest[0m: 7.903, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.63565
[1mStep[0m  [4/42], [94mLoss[0m : 8.32578
[1mStep[0m  [8/42], [94mLoss[0m : 8.04858
[1mStep[0m  [12/42], [94mLoss[0m : 8.46810
[1mStep[0m  [16/42], [94mLoss[0m : 8.39350
[1mStep[0m  [20/42], [94mLoss[0m : 8.29411
[1mStep[0m  [24/42], [94mLoss[0m : 7.94018
[1mStep[0m  [28/42], [94mLoss[0m : 7.95708
[1mStep[0m  [32/42], [94mLoss[0m : 8.08108
[1mStep[0m  [36/42], [94mLoss[0m : 8.12923
[1mStep[0m  [40/42], [94mLoss[0m : 8.14948

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 8.121, [92mTest[0m: 7.606, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.36993
[1mStep[0m  [4/42], [94mLoss[0m : 8.31164
[1mStep[0m  [8/42], [94mLoss[0m : 7.98522
[1mStep[0m  [12/42], [94mLoss[0m : 7.91566
[1mStep[0m  [16/42], [94mLoss[0m : 8.22364
[1mStep[0m  [20/42], [94mLoss[0m : 7.73297
[1mStep[0m  [24/42], [94mLoss[0m : 8.09443
[1mStep[0m  [28/42], [94mLoss[0m : 7.58462
[1mStep[0m  [32/42], [94mLoss[0m : 7.63714
[1mStep[0m  [36/42], [94mLoss[0m : 7.73543
[1mStep[0m  [40/42], [94mLoss[0m : 7.54781

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 7.812, [92mTest[0m: 7.325, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.61745
[1mStep[0m  [4/42], [94mLoss[0m : 7.35597
[1mStep[0m  [8/42], [94mLoss[0m : 7.62364
[1mStep[0m  [12/42], [94mLoss[0m : 7.85646
[1mStep[0m  [16/42], [94mLoss[0m : 7.57807
[1mStep[0m  [20/42], [94mLoss[0m : 7.83217
[1mStep[0m  [24/42], [94mLoss[0m : 7.18641
[1mStep[0m  [28/42], [94mLoss[0m : 7.35179
[1mStep[0m  [32/42], [94mLoss[0m : 7.35540
[1mStep[0m  [36/42], [94mLoss[0m : 7.88536
[1mStep[0m  [40/42], [94mLoss[0m : 7.38229

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 7.514, [92mTest[0m: 6.871, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.27644
[1mStep[0m  [4/42], [94mLoss[0m : 7.43137
[1mStep[0m  [8/42], [94mLoss[0m : 7.12347
[1mStep[0m  [12/42], [94mLoss[0m : 7.43871
[1mStep[0m  [16/42], [94mLoss[0m : 7.29313
[1mStep[0m  [20/42], [94mLoss[0m : 7.50063
[1mStep[0m  [24/42], [94mLoss[0m : 7.30856
[1mStep[0m  [28/42], [94mLoss[0m : 7.40110
[1mStep[0m  [32/42], [94mLoss[0m : 6.96757
[1mStep[0m  [36/42], [94mLoss[0m : 6.95009
[1mStep[0m  [40/42], [94mLoss[0m : 7.44094

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 7.207, [92mTest[0m: 6.574, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.41061
[1mStep[0m  [4/42], [94mLoss[0m : 6.90525
[1mStep[0m  [8/42], [94mLoss[0m : 6.93999
[1mStep[0m  [12/42], [94mLoss[0m : 6.94203
[1mStep[0m  [16/42], [94mLoss[0m : 6.73055
[1mStep[0m  [20/42], [94mLoss[0m : 6.63323
[1mStep[0m  [24/42], [94mLoss[0m : 6.79279
[1mStep[0m  [28/42], [94mLoss[0m : 6.93841
[1mStep[0m  [32/42], [94mLoss[0m : 6.79181
[1mStep[0m  [36/42], [94mLoss[0m : 6.66037
[1mStep[0m  [40/42], [94mLoss[0m : 7.28248

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 6.907, [92mTest[0m: 6.289, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.87695
[1mStep[0m  [4/42], [94mLoss[0m : 6.58980
[1mStep[0m  [8/42], [94mLoss[0m : 6.59647
[1mStep[0m  [12/42], [94mLoss[0m : 6.65960
[1mStep[0m  [16/42], [94mLoss[0m : 7.03748
[1mStep[0m  [20/42], [94mLoss[0m : 6.45916
[1mStep[0m  [24/42], [94mLoss[0m : 6.59190
[1mStep[0m  [28/42], [94mLoss[0m : 6.22768
[1mStep[0m  [32/42], [94mLoss[0m : 6.71012
[1mStep[0m  [36/42], [94mLoss[0m : 6.68469
[1mStep[0m  [40/42], [94mLoss[0m : 6.78659

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 6.624, [92mTest[0m: 5.901, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.35092
[1mStep[0m  [4/42], [94mLoss[0m : 6.95496
[1mStep[0m  [8/42], [94mLoss[0m : 6.25691
[1mStep[0m  [12/42], [94mLoss[0m : 6.56930
[1mStep[0m  [16/42], [94mLoss[0m : 6.26127
[1mStep[0m  [20/42], [94mLoss[0m : 6.31219
[1mStep[0m  [24/42], [94mLoss[0m : 6.22163
[1mStep[0m  [28/42], [94mLoss[0m : 6.28663
[1mStep[0m  [32/42], [94mLoss[0m : 6.17329
[1mStep[0m  [36/42], [94mLoss[0m : 6.25639
[1mStep[0m  [40/42], [94mLoss[0m : 5.95848

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 6.380, [92mTest[0m: 5.691, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.41271
[1mStep[0m  [4/42], [94mLoss[0m : 6.30232
[1mStep[0m  [8/42], [94mLoss[0m : 6.32880
[1mStep[0m  [12/42], [94mLoss[0m : 6.13064
[1mStep[0m  [16/42], [94mLoss[0m : 6.19579
[1mStep[0m  [20/42], [94mLoss[0m : 6.08606
[1mStep[0m  [24/42], [94mLoss[0m : 6.03778
[1mStep[0m  [28/42], [94mLoss[0m : 6.11163
[1mStep[0m  [32/42], [94mLoss[0m : 5.87884
[1mStep[0m  [36/42], [94mLoss[0m : 6.07286
[1mStep[0m  [40/42], [94mLoss[0m : 6.23467

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 6.127, [92mTest[0m: 5.430, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.00800
[1mStep[0m  [4/42], [94mLoss[0m : 5.99466
[1mStep[0m  [8/42], [94mLoss[0m : 5.99733
[1mStep[0m  [12/42], [94mLoss[0m : 5.71847
[1mStep[0m  [16/42], [94mLoss[0m : 5.88568
[1mStep[0m  [20/42], [94mLoss[0m : 5.82950
[1mStep[0m  [24/42], [94mLoss[0m : 5.91137
[1mStep[0m  [28/42], [94mLoss[0m : 6.21073
[1mStep[0m  [32/42], [94mLoss[0m : 5.74193
[1mStep[0m  [36/42], [94mLoss[0m : 5.73437
[1mStep[0m  [40/42], [94mLoss[0m : 5.92787

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 5.865, [92mTest[0m: 5.164, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.76195
[1mStep[0m  [4/42], [94mLoss[0m : 5.70163
[1mStep[0m  [8/42], [94mLoss[0m : 5.69082
[1mStep[0m  [12/42], [94mLoss[0m : 5.54917
[1mStep[0m  [16/42], [94mLoss[0m : 5.79282
[1mStep[0m  [20/42], [94mLoss[0m : 5.59565
[1mStep[0m  [24/42], [94mLoss[0m : 5.94457
[1mStep[0m  [28/42], [94mLoss[0m : 5.34456
[1mStep[0m  [32/42], [94mLoss[0m : 5.24710
[1mStep[0m  [36/42], [94mLoss[0m : 5.94486
[1mStep[0m  [40/42], [94mLoss[0m : 5.55020

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 5.611, [92mTest[0m: 4.948, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.43220
[1mStep[0m  [4/42], [94mLoss[0m : 5.52614
[1mStep[0m  [8/42], [94mLoss[0m : 5.50411
[1mStep[0m  [12/42], [94mLoss[0m : 5.28976
[1mStep[0m  [16/42], [94mLoss[0m : 5.35001
[1mStep[0m  [20/42], [94mLoss[0m : 5.18792
[1mStep[0m  [24/42], [94mLoss[0m : 5.59774
[1mStep[0m  [28/42], [94mLoss[0m : 5.21333
[1mStep[0m  [32/42], [94mLoss[0m : 5.35303
[1mStep[0m  [36/42], [94mLoss[0m : 5.02967
[1mStep[0m  [40/42], [94mLoss[0m : 5.25190

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 5.357, [92mTest[0m: 4.662, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.10849
[1mStep[0m  [4/42], [94mLoss[0m : 5.51978
[1mStep[0m  [8/42], [94mLoss[0m : 4.74130
[1mStep[0m  [12/42], [94mLoss[0m : 5.10691
[1mStep[0m  [16/42], [94mLoss[0m : 5.04924
[1mStep[0m  [20/42], [94mLoss[0m : 5.40952
[1mStep[0m  [24/42], [94mLoss[0m : 5.37046
[1mStep[0m  [28/42], [94mLoss[0m : 5.29047
[1mStep[0m  [32/42], [94mLoss[0m : 5.17532
[1mStep[0m  [36/42], [94mLoss[0m : 4.99921
[1mStep[0m  [40/42], [94mLoss[0m : 5.35968

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 5.111, [92mTest[0m: 4.385, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.21441
[1mStep[0m  [4/42], [94mLoss[0m : 4.98739
[1mStep[0m  [8/42], [94mLoss[0m : 5.10335
[1mStep[0m  [12/42], [94mLoss[0m : 4.91301
[1mStep[0m  [16/42], [94mLoss[0m : 4.53959
[1mStep[0m  [20/42], [94mLoss[0m : 4.83540
[1mStep[0m  [24/42], [94mLoss[0m : 5.03312
[1mStep[0m  [28/42], [94mLoss[0m : 4.44142
[1mStep[0m  [32/42], [94mLoss[0m : 4.78749
[1mStep[0m  [36/42], [94mLoss[0m : 5.04259
[1mStep[0m  [40/42], [94mLoss[0m : 4.72344

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 4.854, [92mTest[0m: 4.210, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.50797
[1mStep[0m  [4/42], [94mLoss[0m : 4.59530
[1mStep[0m  [8/42], [94mLoss[0m : 4.72558
[1mStep[0m  [12/42], [94mLoss[0m : 4.91951
[1mStep[0m  [16/42], [94mLoss[0m : 4.72675
[1mStep[0m  [20/42], [94mLoss[0m : 4.36053
[1mStep[0m  [24/42], [94mLoss[0m : 4.54881
[1mStep[0m  [28/42], [94mLoss[0m : 4.35928
[1mStep[0m  [32/42], [94mLoss[0m : 4.49667
[1mStep[0m  [36/42], [94mLoss[0m : 4.70438
[1mStep[0m  [40/42], [94mLoss[0m : 4.69234

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 4.588, [92mTest[0m: 3.975, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.68203
[1mStep[0m  [4/42], [94mLoss[0m : 4.17253
[1mStep[0m  [8/42], [94mLoss[0m : 4.17784
[1mStep[0m  [12/42], [94mLoss[0m : 4.45995
[1mStep[0m  [16/42], [94mLoss[0m : 4.21650
[1mStep[0m  [20/42], [94mLoss[0m : 4.26778
[1mStep[0m  [24/42], [94mLoss[0m : 3.95936
[1mStep[0m  [28/42], [94mLoss[0m : 4.11241
[1mStep[0m  [32/42], [94mLoss[0m : 4.34710
[1mStep[0m  [36/42], [94mLoss[0m : 4.25921
[1mStep[0m  [40/42], [94mLoss[0m : 4.49948

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 4.320, [92mTest[0m: 3.732, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 3.519
====================================

Phase 1 - Evaluation MAE:  3.5190359694617137
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 4.37278
[1mStep[0m  [4/42], [94mLoss[0m : 4.31470
[1mStep[0m  [8/42], [94mLoss[0m : 4.16201
[1mStep[0m  [12/42], [94mLoss[0m : 4.09709
[1mStep[0m  [16/42], [94mLoss[0m : 4.38337
[1mStep[0m  [20/42], [94mLoss[0m : 3.71835
[1mStep[0m  [24/42], [94mLoss[0m : 4.27933
[1mStep[0m  [28/42], [94mLoss[0m : 3.62584
[1mStep[0m  [32/42], [94mLoss[0m : 3.85006
[1mStep[0m  [36/42], [94mLoss[0m : 4.01854
[1mStep[0m  [40/42], [94mLoss[0m : 3.65006

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.086, [92mTest[0m: 3.514, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.86544
[1mStep[0m  [4/42], [94mLoss[0m : 3.64700
[1mStep[0m  [8/42], [94mLoss[0m : 4.01501
[1mStep[0m  [12/42], [94mLoss[0m : 3.73208
[1mStep[0m  [16/42], [94mLoss[0m : 4.02135
[1mStep[0m  [20/42], [94mLoss[0m : 3.91279
[1mStep[0m  [24/42], [94mLoss[0m : 3.65561
[1mStep[0m  [28/42], [94mLoss[0m : 3.55102
[1mStep[0m  [32/42], [94mLoss[0m : 3.66854
[1mStep[0m  [36/42], [94mLoss[0m : 3.74402
[1mStep[0m  [40/42], [94mLoss[0m : 3.80326

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.810, [92mTest[0m: 3.340, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.47165
[1mStep[0m  [4/42], [94mLoss[0m : 3.45029
[1mStep[0m  [8/42], [94mLoss[0m : 3.63576
[1mStep[0m  [12/42], [94mLoss[0m : 3.35923
[1mStep[0m  [16/42], [94mLoss[0m : 3.39914
[1mStep[0m  [20/42], [94mLoss[0m : 3.63079
[1mStep[0m  [24/42], [94mLoss[0m : 3.28532
[1mStep[0m  [28/42], [94mLoss[0m : 3.66910
[1mStep[0m  [32/42], [94mLoss[0m : 3.28042
[1mStep[0m  [36/42], [94mLoss[0m : 3.32872
[1mStep[0m  [40/42], [94mLoss[0m : 3.72041

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.540, [92mTest[0m: 3.132, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.32219
[1mStep[0m  [4/42], [94mLoss[0m : 3.59082
[1mStep[0m  [8/42], [94mLoss[0m : 3.29882
[1mStep[0m  [12/42], [94mLoss[0m : 3.26516
[1mStep[0m  [16/42], [94mLoss[0m : 3.10832
[1mStep[0m  [20/42], [94mLoss[0m : 2.95253
[1mStep[0m  [24/42], [94mLoss[0m : 3.14398
[1mStep[0m  [28/42], [94mLoss[0m : 3.33189
[1mStep[0m  [32/42], [94mLoss[0m : 3.04864
[1mStep[0m  [36/42], [94mLoss[0m : 3.28477
[1mStep[0m  [40/42], [94mLoss[0m : 3.04927

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 3.248, [92mTest[0m: 2.798, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.13085
[1mStep[0m  [4/42], [94mLoss[0m : 3.33440
[1mStep[0m  [8/42], [94mLoss[0m : 3.24947
[1mStep[0m  [12/42], [94mLoss[0m : 3.09149
[1mStep[0m  [16/42], [94mLoss[0m : 2.98217
[1mStep[0m  [20/42], [94mLoss[0m : 2.95083
[1mStep[0m  [24/42], [94mLoss[0m : 3.11200
[1mStep[0m  [28/42], [94mLoss[0m : 2.97799
[1mStep[0m  [32/42], [94mLoss[0m : 3.48988
[1mStep[0m  [36/42], [94mLoss[0m : 3.08144
[1mStep[0m  [40/42], [94mLoss[0m : 3.10239

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 3.104, [92mTest[0m: 2.652, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.67908
[1mStep[0m  [4/42], [94mLoss[0m : 3.11225
[1mStep[0m  [8/42], [94mLoss[0m : 2.88047
[1mStep[0m  [12/42], [94mLoss[0m : 2.98702
[1mStep[0m  [16/42], [94mLoss[0m : 3.09843
[1mStep[0m  [20/42], [94mLoss[0m : 2.64880
[1mStep[0m  [24/42], [94mLoss[0m : 3.06664
[1mStep[0m  [28/42], [94mLoss[0m : 3.02088
[1mStep[0m  [32/42], [94mLoss[0m : 2.76367
[1mStep[0m  [36/42], [94mLoss[0m : 2.78602
[1mStep[0m  [40/42], [94mLoss[0m : 2.89944

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.984, [92mTest[0m: 2.562, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.66741
[1mStep[0m  [4/42], [94mLoss[0m : 3.01260
[1mStep[0m  [8/42], [94mLoss[0m : 3.07265
[1mStep[0m  [12/42], [94mLoss[0m : 3.09702
[1mStep[0m  [16/42], [94mLoss[0m : 2.81145
[1mStep[0m  [20/42], [94mLoss[0m : 2.89019
[1mStep[0m  [24/42], [94mLoss[0m : 2.85915
[1mStep[0m  [28/42], [94mLoss[0m : 2.76619
[1mStep[0m  [32/42], [94mLoss[0m : 2.81761
[1mStep[0m  [36/42], [94mLoss[0m : 2.77529
[1mStep[0m  [40/42], [94mLoss[0m : 2.64997

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.853, [92mTest[0m: 2.485, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.77738
[1mStep[0m  [4/42], [94mLoss[0m : 2.96281
[1mStep[0m  [8/42], [94mLoss[0m : 2.78223
[1mStep[0m  [12/42], [94mLoss[0m : 2.63020
[1mStep[0m  [16/42], [94mLoss[0m : 2.73436
[1mStep[0m  [20/42], [94mLoss[0m : 2.69189
[1mStep[0m  [24/42], [94mLoss[0m : 2.66715
[1mStep[0m  [28/42], [94mLoss[0m : 2.97252
[1mStep[0m  [32/42], [94mLoss[0m : 2.86245
[1mStep[0m  [36/42], [94mLoss[0m : 2.75448
[1mStep[0m  [40/42], [94mLoss[0m : 2.75073

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.800, [92mTest[0m: 2.425, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.87754
[1mStep[0m  [4/42], [94mLoss[0m : 2.96831
[1mStep[0m  [8/42], [94mLoss[0m : 2.77002
[1mStep[0m  [12/42], [94mLoss[0m : 2.71674
[1mStep[0m  [16/42], [94mLoss[0m : 2.65796
[1mStep[0m  [20/42], [94mLoss[0m : 2.85896
[1mStep[0m  [24/42], [94mLoss[0m : 2.81347
[1mStep[0m  [28/42], [94mLoss[0m : 3.02779
[1mStep[0m  [32/42], [94mLoss[0m : 2.53229
[1mStep[0m  [36/42], [94mLoss[0m : 2.80236
[1mStep[0m  [40/42], [94mLoss[0m : 2.89769

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.748, [92mTest[0m: 2.449, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.78050
[1mStep[0m  [4/42], [94mLoss[0m : 2.86024
[1mStep[0m  [8/42], [94mLoss[0m : 2.68485
[1mStep[0m  [12/42], [94mLoss[0m : 2.50861
[1mStep[0m  [16/42], [94mLoss[0m : 2.83838
[1mStep[0m  [20/42], [94mLoss[0m : 2.92983
[1mStep[0m  [24/42], [94mLoss[0m : 2.87042
[1mStep[0m  [28/42], [94mLoss[0m : 2.66967
[1mStep[0m  [32/42], [94mLoss[0m : 2.70035
[1mStep[0m  [36/42], [94mLoss[0m : 2.70938
[1mStep[0m  [40/42], [94mLoss[0m : 2.49594

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.699, [92mTest[0m: 2.469, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60439
[1mStep[0m  [4/42], [94mLoss[0m : 2.33647
[1mStep[0m  [8/42], [94mLoss[0m : 2.53416
[1mStep[0m  [12/42], [94mLoss[0m : 2.76663
[1mStep[0m  [16/42], [94mLoss[0m : 2.93469
[1mStep[0m  [20/42], [94mLoss[0m : 2.64152
[1mStep[0m  [24/42], [94mLoss[0m : 2.56671
[1mStep[0m  [28/42], [94mLoss[0m : 2.75842
[1mStep[0m  [32/42], [94mLoss[0m : 2.42865
[1mStep[0m  [36/42], [94mLoss[0m : 2.79073
[1mStep[0m  [40/42], [94mLoss[0m : 2.81454

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.668, [92mTest[0m: 2.486, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46821
[1mStep[0m  [4/42], [94mLoss[0m : 2.78118
[1mStep[0m  [8/42], [94mLoss[0m : 2.42278
[1mStep[0m  [12/42], [94mLoss[0m : 2.49506
[1mStep[0m  [16/42], [94mLoss[0m : 2.65492
[1mStep[0m  [20/42], [94mLoss[0m : 2.66283
[1mStep[0m  [24/42], [94mLoss[0m : 2.74238
[1mStep[0m  [28/42], [94mLoss[0m : 2.89305
[1mStep[0m  [32/42], [94mLoss[0m : 2.68266
[1mStep[0m  [36/42], [94mLoss[0m : 2.55989
[1mStep[0m  [40/42], [94mLoss[0m : 2.68152

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.639, [92mTest[0m: 2.441, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.73223
[1mStep[0m  [4/42], [94mLoss[0m : 2.40163
[1mStep[0m  [8/42], [94mLoss[0m : 2.73863
[1mStep[0m  [12/42], [94mLoss[0m : 2.70277
[1mStep[0m  [16/42], [94mLoss[0m : 2.54601
[1mStep[0m  [20/42], [94mLoss[0m : 2.47591
[1mStep[0m  [24/42], [94mLoss[0m : 2.86850
[1mStep[0m  [28/42], [94mLoss[0m : 2.44034
[1mStep[0m  [32/42], [94mLoss[0m : 2.50860
[1mStep[0m  [36/42], [94mLoss[0m : 2.68438
[1mStep[0m  [40/42], [94mLoss[0m : 2.63615

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.630, [92mTest[0m: 2.485, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.03662
[1mStep[0m  [4/42], [94mLoss[0m : 2.61279
[1mStep[0m  [8/42], [94mLoss[0m : 2.63825
[1mStep[0m  [12/42], [94mLoss[0m : 2.48363
[1mStep[0m  [16/42], [94mLoss[0m : 2.62929
[1mStep[0m  [20/42], [94mLoss[0m : 2.54065
[1mStep[0m  [24/42], [94mLoss[0m : 2.69105
[1mStep[0m  [28/42], [94mLoss[0m : 2.55078
[1mStep[0m  [32/42], [94mLoss[0m : 2.55288
[1mStep[0m  [36/42], [94mLoss[0m : 2.37604
[1mStep[0m  [40/42], [94mLoss[0m : 2.61019

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.631, [92mTest[0m: 2.465, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61917
[1mStep[0m  [4/42], [94mLoss[0m : 2.64132
[1mStep[0m  [8/42], [94mLoss[0m : 2.51528
[1mStep[0m  [12/42], [94mLoss[0m : 2.55807
[1mStep[0m  [16/42], [94mLoss[0m : 2.73106
[1mStep[0m  [20/42], [94mLoss[0m : 2.74924
[1mStep[0m  [24/42], [94mLoss[0m : 2.56454
[1mStep[0m  [28/42], [94mLoss[0m : 2.43845
[1mStep[0m  [32/42], [94mLoss[0m : 2.53234
[1mStep[0m  [36/42], [94mLoss[0m : 2.63849
[1mStep[0m  [40/42], [94mLoss[0m : 2.57995

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.590, [92mTest[0m: 2.452, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.71483
[1mStep[0m  [4/42], [94mLoss[0m : 2.65189
[1mStep[0m  [8/42], [94mLoss[0m : 2.64876
[1mStep[0m  [12/42], [94mLoss[0m : 2.52798
[1mStep[0m  [16/42], [94mLoss[0m : 2.48127
[1mStep[0m  [20/42], [94mLoss[0m : 2.57564
[1mStep[0m  [24/42], [94mLoss[0m : 2.59880
[1mStep[0m  [28/42], [94mLoss[0m : 2.59588
[1mStep[0m  [32/42], [94mLoss[0m : 2.68167
[1mStep[0m  [36/42], [94mLoss[0m : 2.68164
[1mStep[0m  [40/42], [94mLoss[0m : 2.70248

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.579, [92mTest[0m: 2.510, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.70037
[1mStep[0m  [4/42], [94mLoss[0m : 2.43110
[1mStep[0m  [8/42], [94mLoss[0m : 2.59047
[1mStep[0m  [12/42], [94mLoss[0m : 2.60216
[1mStep[0m  [16/42], [94mLoss[0m : 2.68336
[1mStep[0m  [20/42], [94mLoss[0m : 2.48221
[1mStep[0m  [24/42], [94mLoss[0m : 2.54836
[1mStep[0m  [28/42], [94mLoss[0m : 2.41627
[1mStep[0m  [32/42], [94mLoss[0m : 2.40435
[1mStep[0m  [36/42], [94mLoss[0m : 2.57509
[1mStep[0m  [40/42], [94mLoss[0m : 2.55648

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.569, [92mTest[0m: 2.491, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.71801
[1mStep[0m  [4/42], [94mLoss[0m : 2.70193
[1mStep[0m  [8/42], [94mLoss[0m : 2.47590
[1mStep[0m  [12/42], [94mLoss[0m : 2.73438
[1mStep[0m  [16/42], [94mLoss[0m : 2.53862
[1mStep[0m  [20/42], [94mLoss[0m : 2.53620
[1mStep[0m  [24/42], [94mLoss[0m : 2.71294
[1mStep[0m  [28/42], [94mLoss[0m : 2.44847
[1mStep[0m  [32/42], [94mLoss[0m : 2.51189
[1mStep[0m  [36/42], [94mLoss[0m : 2.38379
[1mStep[0m  [40/42], [94mLoss[0m : 2.25160

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.558, [92mTest[0m: 2.512, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46575
[1mStep[0m  [4/42], [94mLoss[0m : 2.57080
[1mStep[0m  [8/42], [94mLoss[0m : 2.33637
[1mStep[0m  [12/42], [94mLoss[0m : 2.69030
[1mStep[0m  [16/42], [94mLoss[0m : 2.47793
[1mStep[0m  [20/42], [94mLoss[0m : 2.29027
[1mStep[0m  [24/42], [94mLoss[0m : 2.55886
[1mStep[0m  [28/42], [94mLoss[0m : 2.50380
[1mStep[0m  [32/42], [94mLoss[0m : 2.52695
[1mStep[0m  [36/42], [94mLoss[0m : 2.76586
[1mStep[0m  [40/42], [94mLoss[0m : 2.52324

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.542, [92mTest[0m: 2.479, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49991
[1mStep[0m  [4/42], [94mLoss[0m : 2.61511
[1mStep[0m  [8/42], [94mLoss[0m : 2.56712
[1mStep[0m  [12/42], [94mLoss[0m : 2.56086
[1mStep[0m  [16/42], [94mLoss[0m : 2.42634
[1mStep[0m  [20/42], [94mLoss[0m : 2.33190
[1mStep[0m  [24/42], [94mLoss[0m : 2.57882
[1mStep[0m  [28/42], [94mLoss[0m : 2.60117
[1mStep[0m  [32/42], [94mLoss[0m : 2.53572
[1mStep[0m  [36/42], [94mLoss[0m : 2.38277
[1mStep[0m  [40/42], [94mLoss[0m : 2.42061

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.470, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.77362
[1mStep[0m  [4/42], [94mLoss[0m : 2.40184
[1mStep[0m  [8/42], [94mLoss[0m : 2.61629
[1mStep[0m  [12/42], [94mLoss[0m : 2.45296
[1mStep[0m  [16/42], [94mLoss[0m : 2.46623
[1mStep[0m  [20/42], [94mLoss[0m : 2.42623
[1mStep[0m  [24/42], [94mLoss[0m : 2.65470
[1mStep[0m  [28/42], [94mLoss[0m : 2.39732
[1mStep[0m  [32/42], [94mLoss[0m : 2.40392
[1mStep[0m  [36/42], [94mLoss[0m : 2.54307
[1mStep[0m  [40/42], [94mLoss[0m : 2.54566

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.522, [92mTest[0m: 2.479, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59719
[1mStep[0m  [4/42], [94mLoss[0m : 2.49950
[1mStep[0m  [8/42], [94mLoss[0m : 2.49720
[1mStep[0m  [12/42], [94mLoss[0m : 2.48877
[1mStep[0m  [16/42], [94mLoss[0m : 2.54362
[1mStep[0m  [20/42], [94mLoss[0m : 2.34775
[1mStep[0m  [24/42], [94mLoss[0m : 2.49934
[1mStep[0m  [28/42], [94mLoss[0m : 2.69243
[1mStep[0m  [32/42], [94mLoss[0m : 2.50285
[1mStep[0m  [36/42], [94mLoss[0m : 2.43624
[1mStep[0m  [40/42], [94mLoss[0m : 2.69529

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.520, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44315
[1mStep[0m  [4/42], [94mLoss[0m : 2.51381
[1mStep[0m  [8/42], [94mLoss[0m : 2.59988
[1mStep[0m  [12/42], [94mLoss[0m : 2.26774
[1mStep[0m  [16/42], [94mLoss[0m : 2.53322
[1mStep[0m  [20/42], [94mLoss[0m : 2.59609
[1mStep[0m  [24/42], [94mLoss[0m : 2.47617
[1mStep[0m  [28/42], [94mLoss[0m : 2.53775
[1mStep[0m  [32/42], [94mLoss[0m : 2.34420
[1mStep[0m  [36/42], [94mLoss[0m : 2.54118
[1mStep[0m  [40/42], [94mLoss[0m : 2.65518

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.490, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42472
[1mStep[0m  [4/42], [94mLoss[0m : 2.55589
[1mStep[0m  [8/42], [94mLoss[0m : 2.30579
[1mStep[0m  [12/42], [94mLoss[0m : 2.52524
[1mStep[0m  [16/42], [94mLoss[0m : 2.33106
[1mStep[0m  [20/42], [94mLoss[0m : 2.42064
[1mStep[0m  [24/42], [94mLoss[0m : 2.36336
[1mStep[0m  [28/42], [94mLoss[0m : 2.44780
[1mStep[0m  [32/42], [94mLoss[0m : 2.57469
[1mStep[0m  [36/42], [94mLoss[0m : 2.60031
[1mStep[0m  [40/42], [94mLoss[0m : 2.79533

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.499, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.63085
[1mStep[0m  [4/42], [94mLoss[0m : 2.58302
[1mStep[0m  [8/42], [94mLoss[0m : 2.46436
[1mStep[0m  [12/42], [94mLoss[0m : 2.37426
[1mStep[0m  [16/42], [94mLoss[0m : 2.46944
[1mStep[0m  [20/42], [94mLoss[0m : 2.61601
[1mStep[0m  [24/42], [94mLoss[0m : 2.54404
[1mStep[0m  [28/42], [94mLoss[0m : 2.52306
[1mStep[0m  [32/42], [94mLoss[0m : 2.53852
[1mStep[0m  [36/42], [94mLoss[0m : 2.33048
[1mStep[0m  [40/42], [94mLoss[0m : 2.40810

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.512, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53902
[1mStep[0m  [4/42], [94mLoss[0m : 2.55522
[1mStep[0m  [8/42], [94mLoss[0m : 2.33950
[1mStep[0m  [12/42], [94mLoss[0m : 2.48268
[1mStep[0m  [16/42], [94mLoss[0m : 2.38616
[1mStep[0m  [20/42], [94mLoss[0m : 2.72872
[1mStep[0m  [24/42], [94mLoss[0m : 2.68681
[1mStep[0m  [28/42], [94mLoss[0m : 2.51211
[1mStep[0m  [32/42], [94mLoss[0m : 2.46579
[1mStep[0m  [36/42], [94mLoss[0m : 2.56283
[1mStep[0m  [40/42], [94mLoss[0m : 2.70649

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.472, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40420
[1mStep[0m  [4/42], [94mLoss[0m : 2.50898
[1mStep[0m  [8/42], [94mLoss[0m : 2.35098
[1mStep[0m  [12/42], [94mLoss[0m : 2.52910
[1mStep[0m  [16/42], [94mLoss[0m : 2.30397
[1mStep[0m  [20/42], [94mLoss[0m : 2.43163
[1mStep[0m  [24/42], [94mLoss[0m : 2.52040
[1mStep[0m  [28/42], [94mLoss[0m : 2.63179
[1mStep[0m  [32/42], [94mLoss[0m : 2.41499
[1mStep[0m  [36/42], [94mLoss[0m : 2.51471
[1mStep[0m  [40/42], [94mLoss[0m : 2.36412

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.472, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37569
[1mStep[0m  [4/42], [94mLoss[0m : 2.23660
[1mStep[0m  [8/42], [94mLoss[0m : 2.65056
[1mStep[0m  [12/42], [94mLoss[0m : 2.34730
[1mStep[0m  [16/42], [94mLoss[0m : 2.48900
[1mStep[0m  [20/42], [94mLoss[0m : 2.43007
[1mStep[0m  [24/42], [94mLoss[0m : 2.41931
[1mStep[0m  [28/42], [94mLoss[0m : 2.60492
[1mStep[0m  [32/42], [94mLoss[0m : 2.32441
[1mStep[0m  [36/42], [94mLoss[0m : 2.34756
[1mStep[0m  [40/42], [94mLoss[0m : 2.52079

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.477, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27872
[1mStep[0m  [4/42], [94mLoss[0m : 2.49535
[1mStep[0m  [8/42], [94mLoss[0m : 2.58519
[1mStep[0m  [12/42], [94mLoss[0m : 2.22336
[1mStep[0m  [16/42], [94mLoss[0m : 2.46631
[1mStep[0m  [20/42], [94mLoss[0m : 2.46807
[1mStep[0m  [24/42], [94mLoss[0m : 2.54262
[1mStep[0m  [28/42], [94mLoss[0m : 2.35840
[1mStep[0m  [32/42], [94mLoss[0m : 2.29217
[1mStep[0m  [36/42], [94mLoss[0m : 2.29403
[1mStep[0m  [40/42], [94mLoss[0m : 2.30696

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.477, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.16159
[1mStep[0m  [4/42], [94mLoss[0m : 2.33816
[1mStep[0m  [8/42], [94mLoss[0m : 2.56418
[1mStep[0m  [12/42], [94mLoss[0m : 2.38705
[1mStep[0m  [16/42], [94mLoss[0m : 2.34236
[1mStep[0m  [20/42], [94mLoss[0m : 2.48308
[1mStep[0m  [24/42], [94mLoss[0m : 2.25887
[1mStep[0m  [28/42], [94mLoss[0m : 2.39792
[1mStep[0m  [32/42], [94mLoss[0m : 2.46849
[1mStep[0m  [36/42], [94mLoss[0m : 2.39837
[1mStep[0m  [40/42], [94mLoss[0m : 2.46398

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.462, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.484
====================================

Phase 2 - Evaluation MAE:  2.48367817061288
MAE score P1        3.519036
MAE score P2        2.483678
loss                2.397383
learning_rate       0.002575
batch_size               256
hidden_sizes      [300, 100]
epochs                    30
activation              tanh
optimizer                sgd
early stopping         False
dropout                  0.3
momentum                 0.1
weight_decay           0.001
Name: 3, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/21], [94mLoss[0m : 10.94733
[1mStep[0m  [2/21], [94mLoss[0m : 10.69063
[1mStep[0m  [4/21], [94mLoss[0m : 10.90336
[1mStep[0m  [6/21], [94mLoss[0m : 10.70629
[1mStep[0m  [8/21], [94mLoss[0m : 10.75315
[1mStep[0m  [10/21], [94mLoss[0m : 10.88470
[1mStep[0m  [12/21], [94mLoss[0m : 11.00520
[1mStep[0m  [14/21], [94mLoss[0m : 11.03263
[1mStep[0m  [16/21], [94mLoss[0m : 10.75970
[1mStep[0m  [18/21], [94mLoss[0m : 11.16105
[1mStep[0m  [20/21], [94mLoss[0m : 10.96204

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.831, [92mTest[0m: 10.836, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.63097
[1mStep[0m  [2/21], [94mLoss[0m : 10.57804
[1mStep[0m  [4/21], [94mLoss[0m : 10.53866
[1mStep[0m  [6/21], [94mLoss[0m : 10.49250
[1mStep[0m  [8/21], [94mLoss[0m : 10.64785
[1mStep[0m  [10/21], [94mLoss[0m : 10.83013
[1mStep[0m  [12/21], [94mLoss[0m : 10.79384
[1mStep[0m  [14/21], [94mLoss[0m : 10.83433
[1mStep[0m  [16/21], [94mLoss[0m : 10.97972
[1mStep[0m  [18/21], [94mLoss[0m : 10.79523
[1mStep[0m  [20/21], [94mLoss[0m : 10.47111

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.730, [92mTest[0m: 10.787, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.62085
[1mStep[0m  [2/21], [94mLoss[0m : 10.70940
[1mStep[0m  [4/21], [94mLoss[0m : 10.62416
[1mStep[0m  [6/21], [94mLoss[0m : 10.76278
[1mStep[0m  [8/21], [94mLoss[0m : 10.62990
[1mStep[0m  [10/21], [94mLoss[0m : 10.76759
[1mStep[0m  [12/21], [94mLoss[0m : 10.48001
[1mStep[0m  [14/21], [94mLoss[0m : 10.81780
[1mStep[0m  [16/21], [94mLoss[0m : 10.48368
[1mStep[0m  [18/21], [94mLoss[0m : 10.44253
[1mStep[0m  [20/21], [94mLoss[0m : 10.62099

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.619, [92mTest[0m: 10.660, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.40792
[1mStep[0m  [2/21], [94mLoss[0m : 10.60962
[1mStep[0m  [4/21], [94mLoss[0m : 10.54563
[1mStep[0m  [6/21], [94mLoss[0m : 10.78637
[1mStep[0m  [8/21], [94mLoss[0m : 10.57952
[1mStep[0m  [10/21], [94mLoss[0m : 10.56296
[1mStep[0m  [12/21], [94mLoss[0m : 10.36992
[1mStep[0m  [14/21], [94mLoss[0m : 10.40029
[1mStep[0m  [16/21], [94mLoss[0m : 10.59883
[1mStep[0m  [18/21], [94mLoss[0m : 10.33444
[1mStep[0m  [20/21], [94mLoss[0m : 10.50424

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.520, [92mTest[0m: 10.504, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.65962
[1mStep[0m  [2/21], [94mLoss[0m : 10.63063
[1mStep[0m  [4/21], [94mLoss[0m : 10.39226
[1mStep[0m  [6/21], [94mLoss[0m : 10.47598
[1mStep[0m  [8/21], [94mLoss[0m : 10.29351
[1mStep[0m  [10/21], [94mLoss[0m : 10.47931
[1mStep[0m  [12/21], [94mLoss[0m : 10.41299
[1mStep[0m  [14/21], [94mLoss[0m : 10.24102
[1mStep[0m  [16/21], [94mLoss[0m : 10.27842
[1mStep[0m  [18/21], [94mLoss[0m : 10.39108
[1mStep[0m  [20/21], [94mLoss[0m : 10.50716

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.428, [92mTest[0m: 10.390, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.27141
[1mStep[0m  [2/21], [94mLoss[0m : 10.44719
[1mStep[0m  [4/21], [94mLoss[0m : 10.23671
[1mStep[0m  [6/21], [94mLoss[0m : 10.17930
[1mStep[0m  [8/21], [94mLoss[0m : 10.25998
[1mStep[0m  [10/21], [94mLoss[0m : 10.27029
[1mStep[0m  [12/21], [94mLoss[0m : 10.20193
[1mStep[0m  [14/21], [94mLoss[0m : 10.11598
[1mStep[0m  [16/21], [94mLoss[0m : 10.36921
[1mStep[0m  [18/21], [94mLoss[0m : 10.41185
[1mStep[0m  [20/21], [94mLoss[0m : 10.27166

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.319, [92mTest[0m: 10.251, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.89652
[1mStep[0m  [2/21], [94mLoss[0m : 10.20611
[1mStep[0m  [4/21], [94mLoss[0m : 10.28683
[1mStep[0m  [6/21], [94mLoss[0m : 10.44404
[1mStep[0m  [8/21], [94mLoss[0m : 10.39304
[1mStep[0m  [10/21], [94mLoss[0m : 10.32927
[1mStep[0m  [12/21], [94mLoss[0m : 10.05006
[1mStep[0m  [14/21], [94mLoss[0m : 10.27007
[1mStep[0m  [16/21], [94mLoss[0m : 10.34249
[1mStep[0m  [18/21], [94mLoss[0m : 10.38717
[1mStep[0m  [20/21], [94mLoss[0m : 10.09937

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.211, [92mTest[0m: 10.096, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.10760
[1mStep[0m  [2/21], [94mLoss[0m : 10.14320
[1mStep[0m  [4/21], [94mLoss[0m : 10.35037
[1mStep[0m  [6/21], [94mLoss[0m : 10.16799
[1mStep[0m  [8/21], [94mLoss[0m : 10.02666
[1mStep[0m  [10/21], [94mLoss[0m : 10.19399
[1mStep[0m  [12/21], [94mLoss[0m : 10.05830
[1mStep[0m  [14/21], [94mLoss[0m : 9.98009
[1mStep[0m  [16/21], [94mLoss[0m : 10.22237
[1mStep[0m  [18/21], [94mLoss[0m : 9.80518
[1mStep[0m  [20/21], [94mLoss[0m : 10.09983

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.109, [92mTest[0m: 9.954, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.96986
[1mStep[0m  [2/21], [94mLoss[0m : 9.99948
[1mStep[0m  [4/21], [94mLoss[0m : 10.22626
[1mStep[0m  [6/21], [94mLoss[0m : 10.00309
[1mStep[0m  [8/21], [94mLoss[0m : 10.11800
[1mStep[0m  [10/21], [94mLoss[0m : 10.06430
[1mStep[0m  [12/21], [94mLoss[0m : 9.88337
[1mStep[0m  [14/21], [94mLoss[0m : 10.07063
[1mStep[0m  [16/21], [94mLoss[0m : 10.07151
[1mStep[0m  [18/21], [94mLoss[0m : 9.92779
[1mStep[0m  [20/21], [94mLoss[0m : 10.05845

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 9.994, [92mTest[0m: 9.794, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.84040
[1mStep[0m  [2/21], [94mLoss[0m : 10.13797
[1mStep[0m  [4/21], [94mLoss[0m : 9.53984
[1mStep[0m  [6/21], [94mLoss[0m : 10.02822
[1mStep[0m  [8/21], [94mLoss[0m : 9.98067
[1mStep[0m  [10/21], [94mLoss[0m : 10.03482
[1mStep[0m  [12/21], [94mLoss[0m : 9.83275
[1mStep[0m  [14/21], [94mLoss[0m : 9.85889
[1mStep[0m  [16/21], [94mLoss[0m : 9.83070
[1mStep[0m  [18/21], [94mLoss[0m : 9.52309
[1mStep[0m  [20/21], [94mLoss[0m : 9.89364

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 9.878, [92mTest[0m: 9.641, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.90984
[1mStep[0m  [2/21], [94mLoss[0m : 9.69893
[1mStep[0m  [4/21], [94mLoss[0m : 9.50961
[1mStep[0m  [6/21], [94mLoss[0m : 9.62313
[1mStep[0m  [8/21], [94mLoss[0m : 9.73233
[1mStep[0m  [10/21], [94mLoss[0m : 9.68302
[1mStep[0m  [12/21], [94mLoss[0m : 9.90272
[1mStep[0m  [14/21], [94mLoss[0m : 9.76159
[1mStep[0m  [16/21], [94mLoss[0m : 9.83685
[1mStep[0m  [18/21], [94mLoss[0m : 9.79658
[1mStep[0m  [20/21], [94mLoss[0m : 9.54507

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 9.756, [92mTest[0m: 9.471, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.44899
[1mStep[0m  [2/21], [94mLoss[0m : 9.61293
[1mStep[0m  [4/21], [94mLoss[0m : 9.81702
[1mStep[0m  [6/21], [94mLoss[0m : 9.86754
[1mStep[0m  [8/21], [94mLoss[0m : 9.49753
[1mStep[0m  [10/21], [94mLoss[0m : 9.74838
[1mStep[0m  [12/21], [94mLoss[0m : 9.79680
[1mStep[0m  [14/21], [94mLoss[0m : 9.78876
[1mStep[0m  [16/21], [94mLoss[0m : 9.56896
[1mStep[0m  [18/21], [94mLoss[0m : 9.51606
[1mStep[0m  [20/21], [94mLoss[0m : 9.65546

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 9.630, [92mTest[0m: 9.347, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.70603
[1mStep[0m  [2/21], [94mLoss[0m : 9.63680
[1mStep[0m  [4/21], [94mLoss[0m : 9.41653
[1mStep[0m  [6/21], [94mLoss[0m : 9.43746
[1mStep[0m  [8/21], [94mLoss[0m : 9.51530
[1mStep[0m  [10/21], [94mLoss[0m : 9.51431
[1mStep[0m  [12/21], [94mLoss[0m : 9.55455
[1mStep[0m  [14/21], [94mLoss[0m : 9.46346
[1mStep[0m  [16/21], [94mLoss[0m : 9.44982
[1mStep[0m  [18/21], [94mLoss[0m : 9.50781
[1mStep[0m  [20/21], [94mLoss[0m : 9.49211

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 9.506, [92mTest[0m: 9.153, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.70669
[1mStep[0m  [2/21], [94mLoss[0m : 9.38070
[1mStep[0m  [4/21], [94mLoss[0m : 9.50804
[1mStep[0m  [6/21], [94mLoss[0m : 9.56698
[1mStep[0m  [8/21], [94mLoss[0m : 9.36034
[1mStep[0m  [10/21], [94mLoss[0m : 9.35257
[1mStep[0m  [12/21], [94mLoss[0m : 9.33429
[1mStep[0m  [14/21], [94mLoss[0m : 9.24894
[1mStep[0m  [16/21], [94mLoss[0m : 9.29332
[1mStep[0m  [18/21], [94mLoss[0m : 9.45810
[1mStep[0m  [20/21], [94mLoss[0m : 9.40749

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 9.360, [92mTest[0m: 8.968, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.93172
[1mStep[0m  [2/21], [94mLoss[0m : 9.39334
[1mStep[0m  [4/21], [94mLoss[0m : 9.02019
[1mStep[0m  [6/21], [94mLoss[0m : 9.16021
[1mStep[0m  [8/21], [94mLoss[0m : 9.38240
[1mStep[0m  [10/21], [94mLoss[0m : 9.18730
[1mStep[0m  [12/21], [94mLoss[0m : 9.37554
[1mStep[0m  [14/21], [94mLoss[0m : 9.38030
[1mStep[0m  [16/21], [94mLoss[0m : 9.24012
[1mStep[0m  [18/21], [94mLoss[0m : 9.36956
[1mStep[0m  [20/21], [94mLoss[0m : 9.16108

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 9.229, [92mTest[0m: 8.794, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.17076
[1mStep[0m  [2/21], [94mLoss[0m : 9.16060
[1mStep[0m  [4/21], [94mLoss[0m : 9.21515
[1mStep[0m  [6/21], [94mLoss[0m : 9.23111
[1mStep[0m  [8/21], [94mLoss[0m : 9.24900
[1mStep[0m  [10/21], [94mLoss[0m : 8.88428
[1mStep[0m  [12/21], [94mLoss[0m : 9.20661
[1mStep[0m  [14/21], [94mLoss[0m : 8.92890
[1mStep[0m  [16/21], [94mLoss[0m : 9.11710
[1mStep[0m  [18/21], [94mLoss[0m : 8.85274
[1mStep[0m  [20/21], [94mLoss[0m : 9.06639

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 9.083, [92mTest[0m: 8.654, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.14814
[1mStep[0m  [2/21], [94mLoss[0m : 8.87503
[1mStep[0m  [4/21], [94mLoss[0m : 9.18862
[1mStep[0m  [6/21], [94mLoss[0m : 8.88833
[1mStep[0m  [8/21], [94mLoss[0m : 8.97572
[1mStep[0m  [10/21], [94mLoss[0m : 8.94148
[1mStep[0m  [12/21], [94mLoss[0m : 9.35806
[1mStep[0m  [14/21], [94mLoss[0m : 9.06412
[1mStep[0m  [16/21], [94mLoss[0m : 8.94784
[1mStep[0m  [18/21], [94mLoss[0m : 8.83256
[1mStep[0m  [20/21], [94mLoss[0m : 8.67625

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 8.936, [92mTest[0m: 8.483, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.78084
[1mStep[0m  [2/21], [94mLoss[0m : 8.92283
[1mStep[0m  [4/21], [94mLoss[0m : 8.68110
[1mStep[0m  [6/21], [94mLoss[0m : 8.82043
[1mStep[0m  [8/21], [94mLoss[0m : 8.82149
[1mStep[0m  [10/21], [94mLoss[0m : 8.70999
[1mStep[0m  [12/21], [94mLoss[0m : 8.89526
[1mStep[0m  [14/21], [94mLoss[0m : 8.67678
[1mStep[0m  [16/21], [94mLoss[0m : 8.70675
[1mStep[0m  [18/21], [94mLoss[0m : 8.91558
[1mStep[0m  [20/21], [94mLoss[0m : 8.81560

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 8.786, [92mTest[0m: 8.242, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.61932
[1mStep[0m  [2/21], [94mLoss[0m : 8.55384
[1mStep[0m  [4/21], [94mLoss[0m : 8.70266
[1mStep[0m  [6/21], [94mLoss[0m : 8.75112
[1mStep[0m  [8/21], [94mLoss[0m : 9.07699
[1mStep[0m  [10/21], [94mLoss[0m : 8.68037
[1mStep[0m  [12/21], [94mLoss[0m : 8.63267
[1mStep[0m  [14/21], [94mLoss[0m : 8.30397
[1mStep[0m  [16/21], [94mLoss[0m : 8.57972
[1mStep[0m  [18/21], [94mLoss[0m : 8.52594
[1mStep[0m  [20/21], [94mLoss[0m : 8.74613

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 8.633, [92mTest[0m: 8.089, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.62814
[1mStep[0m  [2/21], [94mLoss[0m : 8.47110
[1mStep[0m  [4/21], [94mLoss[0m : 8.41394
[1mStep[0m  [6/21], [94mLoss[0m : 8.60361
[1mStep[0m  [8/21], [94mLoss[0m : 8.49568
[1mStep[0m  [10/21], [94mLoss[0m : 8.56416
[1mStep[0m  [12/21], [94mLoss[0m : 8.39461
[1mStep[0m  [14/21], [94mLoss[0m : 8.56385
[1mStep[0m  [16/21], [94mLoss[0m : 8.68495
[1mStep[0m  [18/21], [94mLoss[0m : 8.24584
[1mStep[0m  [20/21], [94mLoss[0m : 8.58587

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 8.489, [92mTest[0m: 7.862, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.29288
[1mStep[0m  [2/21], [94mLoss[0m : 8.32529
[1mStep[0m  [4/21], [94mLoss[0m : 8.26898
[1mStep[0m  [6/21], [94mLoss[0m : 8.34225
[1mStep[0m  [8/21], [94mLoss[0m : 8.50591
[1mStep[0m  [10/21], [94mLoss[0m : 8.44706
[1mStep[0m  [12/21], [94mLoss[0m : 8.32481
[1mStep[0m  [14/21], [94mLoss[0m : 8.72339
[1mStep[0m  [16/21], [94mLoss[0m : 8.16833
[1mStep[0m  [18/21], [94mLoss[0m : 8.19622
[1mStep[0m  [20/21], [94mLoss[0m : 8.45490

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 8.347, [92mTest[0m: 7.665, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.29123
[1mStep[0m  [2/21], [94mLoss[0m : 8.54446
[1mStep[0m  [4/21], [94mLoss[0m : 8.31092
[1mStep[0m  [6/21], [94mLoss[0m : 8.26460
[1mStep[0m  [8/21], [94mLoss[0m : 8.31411
[1mStep[0m  [10/21], [94mLoss[0m : 8.26871
[1mStep[0m  [12/21], [94mLoss[0m : 8.16819
[1mStep[0m  [14/21], [94mLoss[0m : 8.09580
[1mStep[0m  [16/21], [94mLoss[0m : 8.03524
[1mStep[0m  [18/21], [94mLoss[0m : 8.20161
[1mStep[0m  [20/21], [94mLoss[0m : 8.08220

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 8.219, [92mTest[0m: 7.589, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.07797
[1mStep[0m  [2/21], [94mLoss[0m : 8.18999
[1mStep[0m  [4/21], [94mLoss[0m : 8.07520
[1mStep[0m  [6/21], [94mLoss[0m : 7.96874
[1mStep[0m  [8/21], [94mLoss[0m : 8.10328
[1mStep[0m  [10/21], [94mLoss[0m : 8.33310
[1mStep[0m  [12/21], [94mLoss[0m : 7.76103
[1mStep[0m  [14/21], [94mLoss[0m : 8.16747
[1mStep[0m  [16/21], [94mLoss[0m : 7.91476
[1mStep[0m  [18/21], [94mLoss[0m : 7.91694
[1mStep[0m  [20/21], [94mLoss[0m : 7.85044

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 8.076, [92mTest[0m: 7.366, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.98937
[1mStep[0m  [2/21], [94mLoss[0m : 8.11789
[1mStep[0m  [4/21], [94mLoss[0m : 7.94671
[1mStep[0m  [6/21], [94mLoss[0m : 7.87671
[1mStep[0m  [8/21], [94mLoss[0m : 8.05602
[1mStep[0m  [10/21], [94mLoss[0m : 7.90524
[1mStep[0m  [12/21], [94mLoss[0m : 8.15236
[1mStep[0m  [14/21], [94mLoss[0m : 7.91348
[1mStep[0m  [16/21], [94mLoss[0m : 7.76342
[1mStep[0m  [18/21], [94mLoss[0m : 7.92147
[1mStep[0m  [20/21], [94mLoss[0m : 7.80312

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 7.965, [92mTest[0m: 7.283, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.76504
[1mStep[0m  [2/21], [94mLoss[0m : 7.78816
[1mStep[0m  [4/21], [94mLoss[0m : 7.87100
[1mStep[0m  [6/21], [94mLoss[0m : 8.07205
[1mStep[0m  [8/21], [94mLoss[0m : 7.83381
[1mStep[0m  [10/21], [94mLoss[0m : 7.97110
[1mStep[0m  [12/21], [94mLoss[0m : 7.67912
[1mStep[0m  [14/21], [94mLoss[0m : 7.74300
[1mStep[0m  [16/21], [94mLoss[0m : 7.92361
[1mStep[0m  [18/21], [94mLoss[0m : 7.62225
[1mStep[0m  [20/21], [94mLoss[0m : 7.92397

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 7.820, [92mTest[0m: 7.155, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.66666
[1mStep[0m  [2/21], [94mLoss[0m : 7.67979
[1mStep[0m  [4/21], [94mLoss[0m : 7.73015
[1mStep[0m  [6/21], [94mLoss[0m : 7.89914
[1mStep[0m  [8/21], [94mLoss[0m : 7.64144
[1mStep[0m  [10/21], [94mLoss[0m : 7.60960
[1mStep[0m  [12/21], [94mLoss[0m : 7.77703
[1mStep[0m  [14/21], [94mLoss[0m : 7.63736
[1mStep[0m  [16/21], [94mLoss[0m : 7.60451
[1mStep[0m  [18/21], [94mLoss[0m : 7.73000
[1mStep[0m  [20/21], [94mLoss[0m : 7.53254

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 7.679, [92mTest[0m: 6.911, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.77244
[1mStep[0m  [2/21], [94mLoss[0m : 7.57563
[1mStep[0m  [4/21], [94mLoss[0m : 7.58041
[1mStep[0m  [6/21], [94mLoss[0m : 7.50757
[1mStep[0m  [8/21], [94mLoss[0m : 7.58660
[1mStep[0m  [10/21], [94mLoss[0m : 7.85048
[1mStep[0m  [12/21], [94mLoss[0m : 8.15667
[1mStep[0m  [14/21], [94mLoss[0m : 7.44057
[1mStep[0m  [16/21], [94mLoss[0m : 7.71744
[1mStep[0m  [18/21], [94mLoss[0m : 7.41436
[1mStep[0m  [20/21], [94mLoss[0m : 7.52277

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 7.573, [92mTest[0m: 6.843, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.59368
[1mStep[0m  [2/21], [94mLoss[0m : 7.61286
[1mStep[0m  [4/21], [94mLoss[0m : 7.29630
[1mStep[0m  [6/21], [94mLoss[0m : 7.48901
[1mStep[0m  [8/21], [94mLoss[0m : 7.38279
[1mStep[0m  [10/21], [94mLoss[0m : 7.38128
[1mStep[0m  [12/21], [94mLoss[0m : 7.27247
[1mStep[0m  [14/21], [94mLoss[0m : 7.43628
[1mStep[0m  [16/21], [94mLoss[0m : 7.46220
[1mStep[0m  [18/21], [94mLoss[0m : 7.47599
[1mStep[0m  [20/21], [94mLoss[0m : 7.44827

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 7.456, [92mTest[0m: 6.715, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.43377
[1mStep[0m  [2/21], [94mLoss[0m : 7.54665
[1mStep[0m  [4/21], [94mLoss[0m : 7.67339
[1mStep[0m  [6/21], [94mLoss[0m : 7.19568
[1mStep[0m  [8/21], [94mLoss[0m : 7.31542
[1mStep[0m  [10/21], [94mLoss[0m : 7.43700
[1mStep[0m  [12/21], [94mLoss[0m : 7.09577
[1mStep[0m  [14/21], [94mLoss[0m : 7.12296
[1mStep[0m  [16/21], [94mLoss[0m : 7.04695
[1mStep[0m  [18/21], [94mLoss[0m : 7.51409
[1mStep[0m  [20/21], [94mLoss[0m : 7.21373

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 7.334, [92mTest[0m: 6.604, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.05061
[1mStep[0m  [2/21], [94mLoss[0m : 7.23130
[1mStep[0m  [4/21], [94mLoss[0m : 7.37584
[1mStep[0m  [6/21], [94mLoss[0m : 7.35740
[1mStep[0m  [8/21], [94mLoss[0m : 7.39808
[1mStep[0m  [10/21], [94mLoss[0m : 7.49562
[1mStep[0m  [12/21], [94mLoss[0m : 7.18803
[1mStep[0m  [14/21], [94mLoss[0m : 7.22118
[1mStep[0m  [16/21], [94mLoss[0m : 7.19696
[1mStep[0m  [18/21], [94mLoss[0m : 7.29284
[1mStep[0m  [20/21], [94mLoss[0m : 7.11998

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 7.236, [92mTest[0m: 6.454, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 6.403
====================================

Phase 1 - Evaluation MAE:  6.403254985809326
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/21], [94mLoss[0m : 7.25765
[1mStep[0m  [2/21], [94mLoss[0m : 7.20373
[1mStep[0m  [4/21], [94mLoss[0m : 7.10282
[1mStep[0m  [6/21], [94mLoss[0m : 7.09764
[1mStep[0m  [8/21], [94mLoss[0m : 6.96464
[1mStep[0m  [10/21], [94mLoss[0m : 7.06445
[1mStep[0m  [12/21], [94mLoss[0m : 6.70663
[1mStep[0m  [14/21], [94mLoss[0m : 7.12991
[1mStep[0m  [16/21], [94mLoss[0m : 6.98989
[1mStep[0m  [18/21], [94mLoss[0m : 7.07246
[1mStep[0m  [20/21], [94mLoss[0m : 6.89082

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 7.011, [92mTest[0m: 6.419, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.74936
[1mStep[0m  [2/21], [94mLoss[0m : 7.09093
[1mStep[0m  [4/21], [94mLoss[0m : 6.93440
[1mStep[0m  [6/21], [94mLoss[0m : 7.16864
[1mStep[0m  [8/21], [94mLoss[0m : 7.00179
[1mStep[0m  [10/21], [94mLoss[0m : 6.64260
[1mStep[0m  [12/21], [94mLoss[0m : 6.95461
[1mStep[0m  [14/21], [94mLoss[0m : 6.51156
[1mStep[0m  [16/21], [94mLoss[0m : 6.59264
[1mStep[0m  [18/21], [94mLoss[0m : 6.64739
[1mStep[0m  [20/21], [94mLoss[0m : 6.57110

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 6.807, [92mTest[0m: 5.855, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.84435
[1mStep[0m  [2/21], [94mLoss[0m : 6.84886
[1mStep[0m  [4/21], [94mLoss[0m : 6.73222
[1mStep[0m  [6/21], [94mLoss[0m : 6.87901
[1mStep[0m  [8/21], [94mLoss[0m : 6.66064
[1mStep[0m  [10/21], [94mLoss[0m : 6.36860
[1mStep[0m  [12/21], [94mLoss[0m : 6.53969
[1mStep[0m  [14/21], [94mLoss[0m : 6.51809
[1mStep[0m  [16/21], [94mLoss[0m : 6.62312
[1mStep[0m  [18/21], [94mLoss[0m : 6.56095
[1mStep[0m  [20/21], [94mLoss[0m : 6.38762

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 6.645, [92mTest[0m: 6.129, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.81782
[1mStep[0m  [2/21], [94mLoss[0m : 6.64660
[1mStep[0m  [4/21], [94mLoss[0m : 6.68551
[1mStep[0m  [6/21], [94mLoss[0m : 6.67800
[1mStep[0m  [8/21], [94mLoss[0m : 6.51713
[1mStep[0m  [10/21], [94mLoss[0m : 6.36396
[1mStep[0m  [12/21], [94mLoss[0m : 6.50254
[1mStep[0m  [14/21], [94mLoss[0m : 6.43023
[1mStep[0m  [16/21], [94mLoss[0m : 6.35611
[1mStep[0m  [18/21], [94mLoss[0m : 6.31744
[1mStep[0m  [20/21], [94mLoss[0m : 6.24866

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 6.488, [92mTest[0m: 8.312, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.39761
[1mStep[0m  [2/21], [94mLoss[0m : 6.52369
[1mStep[0m  [4/21], [94mLoss[0m : 6.38569
[1mStep[0m  [6/21], [94mLoss[0m : 6.38271
[1mStep[0m  [8/21], [94mLoss[0m : 6.52257
[1mStep[0m  [10/21], [94mLoss[0m : 6.10392
[1mStep[0m  [12/21], [94mLoss[0m : 6.21110
[1mStep[0m  [14/21], [94mLoss[0m : 6.29909
[1mStep[0m  [16/21], [94mLoss[0m : 6.31559
[1mStep[0m  [18/21], [94mLoss[0m : 6.23627
[1mStep[0m  [20/21], [94mLoss[0m : 6.33016

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 6.321, [92mTest[0m: 6.191, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.13792
[1mStep[0m  [2/21], [94mLoss[0m : 6.41798
[1mStep[0m  [4/21], [94mLoss[0m : 6.20491
[1mStep[0m  [6/21], [94mLoss[0m : 6.10683
[1mStep[0m  [8/21], [94mLoss[0m : 6.34177
[1mStep[0m  [10/21], [94mLoss[0m : 6.01029
[1mStep[0m  [12/21], [94mLoss[0m : 6.15546
[1mStep[0m  [14/21], [94mLoss[0m : 6.14724
[1mStep[0m  [16/21], [94mLoss[0m : 5.90668
[1mStep[0m  [18/21], [94mLoss[0m : 5.81183
[1mStep[0m  [20/21], [94mLoss[0m : 6.27561

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 6.174, [92mTest[0m: 6.052, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.86802
[1mStep[0m  [2/21], [94mLoss[0m : 5.87318
[1mStep[0m  [4/21], [94mLoss[0m : 6.09015
[1mStep[0m  [6/21], [94mLoss[0m : 6.28717
[1mStep[0m  [8/21], [94mLoss[0m : 6.09300
[1mStep[0m  [10/21], [94mLoss[0m : 6.12954
[1mStep[0m  [12/21], [94mLoss[0m : 6.19798
[1mStep[0m  [14/21], [94mLoss[0m : 6.01410
[1mStep[0m  [16/21], [94mLoss[0m : 6.18623
[1mStep[0m  [18/21], [94mLoss[0m : 5.80899
[1mStep[0m  [20/21], [94mLoss[0m : 5.87437

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 6.027, [92mTest[0m: 6.654, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.74961
[1mStep[0m  [2/21], [94mLoss[0m : 5.81046
[1mStep[0m  [4/21], [94mLoss[0m : 6.01612
[1mStep[0m  [6/21], [94mLoss[0m : 5.89317
[1mStep[0m  [8/21], [94mLoss[0m : 5.72671
[1mStep[0m  [10/21], [94mLoss[0m : 5.79006
[1mStep[0m  [12/21], [94mLoss[0m : 5.93412
[1mStep[0m  [14/21], [94mLoss[0m : 5.88330
[1mStep[0m  [16/21], [94mLoss[0m : 5.65236
[1mStep[0m  [18/21], [94mLoss[0m : 6.09067
[1mStep[0m  [20/21], [94mLoss[0m : 5.50870

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 5.851, [92mTest[0m: 5.335, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.80363
[1mStep[0m  [2/21], [94mLoss[0m : 5.84364
[1mStep[0m  [4/21], [94mLoss[0m : 5.65404
[1mStep[0m  [6/21], [94mLoss[0m : 5.77197
[1mStep[0m  [8/21], [94mLoss[0m : 5.72566
[1mStep[0m  [10/21], [94mLoss[0m : 5.56392
[1mStep[0m  [12/21], [94mLoss[0m : 5.65406
[1mStep[0m  [14/21], [94mLoss[0m : 5.55772
[1mStep[0m  [16/21], [94mLoss[0m : 5.38313
[1mStep[0m  [18/21], [94mLoss[0m : 5.62572
[1mStep[0m  [20/21], [94mLoss[0m : 5.73350

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 5.683, [92mTest[0m: 5.195, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.71332
[1mStep[0m  [2/21], [94mLoss[0m : 5.01383
[1mStep[0m  [4/21], [94mLoss[0m : 5.53612
[1mStep[0m  [6/21], [94mLoss[0m : 5.55078
[1mStep[0m  [8/21], [94mLoss[0m : 5.51572
[1mStep[0m  [10/21], [94mLoss[0m : 5.80481
[1mStep[0m  [12/21], [94mLoss[0m : 5.59505
[1mStep[0m  [14/21], [94mLoss[0m : 5.51350
[1mStep[0m  [16/21], [94mLoss[0m : 5.54519
[1mStep[0m  [18/21], [94mLoss[0m : 5.57449
[1mStep[0m  [20/21], [94mLoss[0m : 5.48637

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 5.519, [92mTest[0m: 5.466, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.62102
[1mStep[0m  [2/21], [94mLoss[0m : 5.45336
[1mStep[0m  [4/21], [94mLoss[0m : 5.54699
[1mStep[0m  [6/21], [94mLoss[0m : 5.38437
[1mStep[0m  [8/21], [94mLoss[0m : 5.19370
[1mStep[0m  [10/21], [94mLoss[0m : 5.41968
[1mStep[0m  [12/21], [94mLoss[0m : 5.56050
[1mStep[0m  [14/21], [94mLoss[0m : 5.24716
[1mStep[0m  [16/21], [94mLoss[0m : 5.35094
[1mStep[0m  [18/21], [94mLoss[0m : 5.37447
[1mStep[0m  [20/21], [94mLoss[0m : 5.34002

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 5.328, [92mTest[0m: 5.607, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.27098
[1mStep[0m  [2/21], [94mLoss[0m : 5.30057
[1mStep[0m  [4/21], [94mLoss[0m : 5.55144
[1mStep[0m  [6/21], [94mLoss[0m : 5.22642
[1mStep[0m  [8/21], [94mLoss[0m : 4.89642
[1mStep[0m  [10/21], [94mLoss[0m : 4.98738
[1mStep[0m  [12/21], [94mLoss[0m : 4.98718
[1mStep[0m  [14/21], [94mLoss[0m : 5.35668
[1mStep[0m  [16/21], [94mLoss[0m : 5.20632
[1mStep[0m  [18/21], [94mLoss[0m : 5.32697
[1mStep[0m  [20/21], [94mLoss[0m : 5.16703

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 5.160, [92mTest[0m: 5.780, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.05935
[1mStep[0m  [2/21], [94mLoss[0m : 4.92265
[1mStep[0m  [4/21], [94mLoss[0m : 5.18039
[1mStep[0m  [6/21], [94mLoss[0m : 4.93579
[1mStep[0m  [8/21], [94mLoss[0m : 4.81141
[1mStep[0m  [10/21], [94mLoss[0m : 5.04244
[1mStep[0m  [12/21], [94mLoss[0m : 5.22391
[1mStep[0m  [14/21], [94mLoss[0m : 5.06401
[1mStep[0m  [16/21], [94mLoss[0m : 4.84357
[1mStep[0m  [18/21], [94mLoss[0m : 4.69736
[1mStep[0m  [20/21], [94mLoss[0m : 5.30466

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 4.989, [92mTest[0m: 5.646, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.16271
[1mStep[0m  [2/21], [94mLoss[0m : 4.98183
[1mStep[0m  [4/21], [94mLoss[0m : 4.63168
[1mStep[0m  [6/21], [94mLoss[0m : 4.64496
[1mStep[0m  [8/21], [94mLoss[0m : 4.76025
[1mStep[0m  [10/21], [94mLoss[0m : 4.65922
[1mStep[0m  [12/21], [94mLoss[0m : 4.90180
[1mStep[0m  [14/21], [94mLoss[0m : 4.60232
[1mStep[0m  [16/21], [94mLoss[0m : 4.54953
[1mStep[0m  [18/21], [94mLoss[0m : 4.67242
[1mStep[0m  [20/21], [94mLoss[0m : 4.90142

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 4.800, [92mTest[0m: 6.149, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.75928
[1mStep[0m  [2/21], [94mLoss[0m : 4.63821
[1mStep[0m  [4/21], [94mLoss[0m : 4.50355
[1mStep[0m  [6/21], [94mLoss[0m : 4.94134
[1mStep[0m  [8/21], [94mLoss[0m : 4.64979
[1mStep[0m  [10/21], [94mLoss[0m : 4.68771
[1mStep[0m  [12/21], [94mLoss[0m : 4.67905
[1mStep[0m  [14/21], [94mLoss[0m : 4.63366
[1mStep[0m  [16/21], [94mLoss[0m : 4.85225
[1mStep[0m  [18/21], [94mLoss[0m : 4.77237
[1mStep[0m  [20/21], [94mLoss[0m : 4.88172

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 4.662, [92mTest[0m: 5.179, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.38297
[1mStep[0m  [2/21], [94mLoss[0m : 4.68910
[1mStep[0m  [4/21], [94mLoss[0m : 4.67950
[1mStep[0m  [6/21], [94mLoss[0m : 4.55460
[1mStep[0m  [8/21], [94mLoss[0m : 4.59896
[1mStep[0m  [10/21], [94mLoss[0m : 4.58350
[1mStep[0m  [12/21], [94mLoss[0m : 4.34399
[1mStep[0m  [14/21], [94mLoss[0m : 4.33383
[1mStep[0m  [16/21], [94mLoss[0m : 4.17170
[1mStep[0m  [18/21], [94mLoss[0m : 4.52756
[1mStep[0m  [20/21], [94mLoss[0m : 4.77345

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 4.500, [92mTest[0m: 4.439, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.13791
[1mStep[0m  [2/21], [94mLoss[0m : 4.25235
[1mStep[0m  [4/21], [94mLoss[0m : 4.38759
[1mStep[0m  [6/21], [94mLoss[0m : 4.47279
[1mStep[0m  [8/21], [94mLoss[0m : 4.56774
[1mStep[0m  [10/21], [94mLoss[0m : 4.21906
[1mStep[0m  [12/21], [94mLoss[0m : 4.62811
[1mStep[0m  [14/21], [94mLoss[0m : 4.27232
[1mStep[0m  [16/21], [94mLoss[0m : 4.29555
[1mStep[0m  [18/21], [94mLoss[0m : 4.32393
[1mStep[0m  [20/21], [94mLoss[0m : 4.10613

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 4.317, [92mTest[0m: 4.468, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.17346
[1mStep[0m  [2/21], [94mLoss[0m : 4.41484
[1mStep[0m  [4/21], [94mLoss[0m : 4.21026
[1mStep[0m  [6/21], [94mLoss[0m : 4.31814
[1mStep[0m  [8/21], [94mLoss[0m : 4.39071
[1mStep[0m  [10/21], [94mLoss[0m : 4.08053
[1mStep[0m  [12/21], [94mLoss[0m : 3.88156
[1mStep[0m  [14/21], [94mLoss[0m : 3.90026
[1mStep[0m  [16/21], [94mLoss[0m : 3.83335
[1mStep[0m  [18/21], [94mLoss[0m : 4.02561
[1mStep[0m  [20/21], [94mLoss[0m : 4.04229

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 4.150, [92mTest[0m: 3.971, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.93938
[1mStep[0m  [2/21], [94mLoss[0m : 3.85856
[1mStep[0m  [4/21], [94mLoss[0m : 3.93191
[1mStep[0m  [6/21], [94mLoss[0m : 3.80610
[1mStep[0m  [8/21], [94mLoss[0m : 3.81395
[1mStep[0m  [10/21], [94mLoss[0m : 4.01408
[1mStep[0m  [12/21], [94mLoss[0m : 4.11236
[1mStep[0m  [14/21], [94mLoss[0m : 3.98310
[1mStep[0m  [16/21], [94mLoss[0m : 4.09658
[1mStep[0m  [18/21], [94mLoss[0m : 4.23755
[1mStep[0m  [20/21], [94mLoss[0m : 4.15716

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 4.036, [92mTest[0m: 3.886, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.19691
[1mStep[0m  [2/21], [94mLoss[0m : 3.74928
[1mStep[0m  [4/21], [94mLoss[0m : 4.18563
[1mStep[0m  [6/21], [94mLoss[0m : 4.06669
[1mStep[0m  [8/21], [94mLoss[0m : 3.81993
[1mStep[0m  [10/21], [94mLoss[0m : 3.94261
[1mStep[0m  [12/21], [94mLoss[0m : 3.70258
[1mStep[0m  [14/21], [94mLoss[0m : 3.83394
[1mStep[0m  [16/21], [94mLoss[0m : 3.83297
[1mStep[0m  [18/21], [94mLoss[0m : 3.93745
[1mStep[0m  [20/21], [94mLoss[0m : 3.77618

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 3.890, [92mTest[0m: 6.936, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.55455
[1mStep[0m  [2/21], [94mLoss[0m : 3.79978
[1mStep[0m  [4/21], [94mLoss[0m : 3.82430
[1mStep[0m  [6/21], [94mLoss[0m : 3.85414
[1mStep[0m  [8/21], [94mLoss[0m : 3.64466
[1mStep[0m  [10/21], [94mLoss[0m : 3.67735
[1mStep[0m  [12/21], [94mLoss[0m : 3.81513
[1mStep[0m  [14/21], [94mLoss[0m : 3.76825
[1mStep[0m  [16/21], [94mLoss[0m : 3.63815
[1mStep[0m  [18/21], [94mLoss[0m : 3.83777
[1mStep[0m  [20/21], [94mLoss[0m : 3.72785

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 3.745, [92mTest[0m: 3.561, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.57070
[1mStep[0m  [2/21], [94mLoss[0m : 3.79385
[1mStep[0m  [4/21], [94mLoss[0m : 3.47189
[1mStep[0m  [6/21], [94mLoss[0m : 3.48450
[1mStep[0m  [8/21], [94mLoss[0m : 3.72004
[1mStep[0m  [10/21], [94mLoss[0m : 3.89804
[1mStep[0m  [12/21], [94mLoss[0m : 3.51907
[1mStep[0m  [14/21], [94mLoss[0m : 3.39188
[1mStep[0m  [16/21], [94mLoss[0m : 3.55153
[1mStep[0m  [18/21], [94mLoss[0m : 3.59239
[1mStep[0m  [20/21], [94mLoss[0m : 3.72420

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 3.627, [92mTest[0m: 3.326, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.51655
[1mStep[0m  [2/21], [94mLoss[0m : 3.73069
[1mStep[0m  [4/21], [94mLoss[0m : 3.53972
[1mStep[0m  [6/21], [94mLoss[0m : 3.42271
[1mStep[0m  [8/21], [94mLoss[0m : 3.39636
[1mStep[0m  [10/21], [94mLoss[0m : 3.39740
[1mStep[0m  [12/21], [94mLoss[0m : 3.31214
[1mStep[0m  [14/21], [94mLoss[0m : 3.44966
[1mStep[0m  [16/21], [94mLoss[0m : 3.43588
[1mStep[0m  [18/21], [94mLoss[0m : 3.40541
[1mStep[0m  [20/21], [94mLoss[0m : 3.40849

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 3.489, [92mTest[0m: 3.912, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.47975
[1mStep[0m  [2/21], [94mLoss[0m : 3.41447
[1mStep[0m  [4/21], [94mLoss[0m : 3.30390
[1mStep[0m  [6/21], [94mLoss[0m : 3.31327
[1mStep[0m  [8/21], [94mLoss[0m : 3.35979
[1mStep[0m  [10/21], [94mLoss[0m : 3.28073
[1mStep[0m  [12/21], [94mLoss[0m : 3.38488
[1mStep[0m  [14/21], [94mLoss[0m : 3.54439
[1mStep[0m  [16/21], [94mLoss[0m : 3.27307
[1mStep[0m  [18/21], [94mLoss[0m : 3.18586
[1mStep[0m  [20/21], [94mLoss[0m : 3.26290

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 3.352, [92mTest[0m: 3.169, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.29041
[1mStep[0m  [2/21], [94mLoss[0m : 3.41440
[1mStep[0m  [4/21], [94mLoss[0m : 3.22979
[1mStep[0m  [6/21], [94mLoss[0m : 3.23792
[1mStep[0m  [8/21], [94mLoss[0m : 3.36332
[1mStep[0m  [10/21], [94mLoss[0m : 3.36804
[1mStep[0m  [12/21], [94mLoss[0m : 3.32680
[1mStep[0m  [14/21], [94mLoss[0m : 3.25737
[1mStep[0m  [16/21], [94mLoss[0m : 3.19930
[1mStep[0m  [18/21], [94mLoss[0m : 3.17677
[1mStep[0m  [20/21], [94mLoss[0m : 3.19751

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 3.252, [92mTest[0m: 3.086, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.47136
[1mStep[0m  [2/21], [94mLoss[0m : 3.13120
[1mStep[0m  [4/21], [94mLoss[0m : 3.19432
[1mStep[0m  [6/21], [94mLoss[0m : 3.08998
[1mStep[0m  [8/21], [94mLoss[0m : 3.22340
[1mStep[0m  [10/21], [94mLoss[0m : 2.92602
[1mStep[0m  [12/21], [94mLoss[0m : 3.26883
[1mStep[0m  [14/21], [94mLoss[0m : 3.11666
[1mStep[0m  [16/21], [94mLoss[0m : 3.17655
[1mStep[0m  [18/21], [94mLoss[0m : 3.24594
[1mStep[0m  [20/21], [94mLoss[0m : 3.09689

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 3.182, [92mTest[0m: 3.167, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.13942
[1mStep[0m  [2/21], [94mLoss[0m : 3.36378
[1mStep[0m  [4/21], [94mLoss[0m : 3.00140
[1mStep[0m  [6/21], [94mLoss[0m : 3.24623
[1mStep[0m  [8/21], [94mLoss[0m : 2.90963
[1mStep[0m  [10/21], [94mLoss[0m : 2.90671
[1mStep[0m  [12/21], [94mLoss[0m : 3.11863
[1mStep[0m  [14/21], [94mLoss[0m : 2.99624
[1mStep[0m  [16/21], [94mLoss[0m : 3.16521
[1mStep[0m  [18/21], [94mLoss[0m : 3.04933
[1mStep[0m  [20/21], [94mLoss[0m : 2.97577

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 3.071, [92mTest[0m: 2.889, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.01037
[1mStep[0m  [2/21], [94mLoss[0m : 3.18032
[1mStep[0m  [4/21], [94mLoss[0m : 3.02017
[1mStep[0m  [6/21], [94mLoss[0m : 3.08639
[1mStep[0m  [8/21], [94mLoss[0m : 2.89050
[1mStep[0m  [10/21], [94mLoss[0m : 2.97936
[1mStep[0m  [12/21], [94mLoss[0m : 2.97264
[1mStep[0m  [14/21], [94mLoss[0m : 2.96099
[1mStep[0m  [16/21], [94mLoss[0m : 2.90902
[1mStep[0m  [18/21], [94mLoss[0m : 3.03859
[1mStep[0m  [20/21], [94mLoss[0m : 2.89538

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.972, [92mTest[0m: 2.908, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.93671
[1mStep[0m  [2/21], [94mLoss[0m : 2.99953
[1mStep[0m  [4/21], [94mLoss[0m : 3.01631
[1mStep[0m  [6/21], [94mLoss[0m : 3.00944
[1mStep[0m  [8/21], [94mLoss[0m : 3.15746
[1mStep[0m  [10/21], [94mLoss[0m : 2.64806
[1mStep[0m  [12/21], [94mLoss[0m : 2.83727
[1mStep[0m  [14/21], [94mLoss[0m : 2.97021
[1mStep[0m  [16/21], [94mLoss[0m : 2.75370
[1mStep[0m  [18/21], [94mLoss[0m : 2.86684
[1mStep[0m  [20/21], [94mLoss[0m : 2.95830

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.920, [92mTest[0m: 2.915, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.80370
[1mStep[0m  [2/21], [94mLoss[0m : 2.71999
[1mStep[0m  [4/21], [94mLoss[0m : 2.86644
[1mStep[0m  [6/21], [94mLoss[0m : 2.86779
[1mStep[0m  [8/21], [94mLoss[0m : 2.60944
[1mStep[0m  [10/21], [94mLoss[0m : 2.94182
[1mStep[0m  [12/21], [94mLoss[0m : 2.74094
[1mStep[0m  [14/21], [94mLoss[0m : 2.51992
[1mStep[0m  [16/21], [94mLoss[0m : 2.87196
[1mStep[0m  [18/21], [94mLoss[0m : 2.91427
[1mStep[0m  [20/21], [94mLoss[0m : 2.79349

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.810, [92mTest[0m: 2.758, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.736
====================================

Phase 2 - Evaluation MAE:  2.7355960437229703
MAE score P1      6.403255
MAE score P2      2.735596
loss              2.809649
learning_rate     0.002575
batch_size             512
hidden_sizes         [250]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.1
weight_decay          0.01
Name: 4, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 10.79828
[1mStep[0m  [4/42], [94mLoss[0m : 10.82669
[1mStep[0m  [8/42], [94mLoss[0m : 10.65201
[1mStep[0m  [12/42], [94mLoss[0m : 10.63698
[1mStep[0m  [16/42], [94mLoss[0m : 10.48603
[1mStep[0m  [20/42], [94mLoss[0m : 10.60725
[1mStep[0m  [24/42], [94mLoss[0m : 10.56083
[1mStep[0m  [28/42], [94mLoss[0m : 10.07644
[1mStep[0m  [32/42], [94mLoss[0m : 9.88436
[1mStep[0m  [36/42], [94mLoss[0m : 9.78757
[1mStep[0m  [40/42], [94mLoss[0m : 9.70008

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.351, [92mTest[0m: 10.898, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.52321
[1mStep[0m  [4/42], [94mLoss[0m : 9.16945
[1mStep[0m  [8/42], [94mLoss[0m : 9.04961
[1mStep[0m  [12/42], [94mLoss[0m : 8.85545
[1mStep[0m  [16/42], [94mLoss[0m : 8.59349
[1mStep[0m  [20/42], [94mLoss[0m : 8.24297
[1mStep[0m  [24/42], [94mLoss[0m : 8.17878
[1mStep[0m  [28/42], [94mLoss[0m : 7.78317
[1mStep[0m  [32/42], [94mLoss[0m : 7.80562
[1mStep[0m  [36/42], [94mLoss[0m : 7.42177
[1mStep[0m  [40/42], [94mLoss[0m : 7.00200

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 8.299, [92mTest[0m: 9.318, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.42655
[1mStep[0m  [4/42], [94mLoss[0m : 6.98723
[1mStep[0m  [8/42], [94mLoss[0m : 7.05516
[1mStep[0m  [12/42], [94mLoss[0m : 6.55137
[1mStep[0m  [16/42], [94mLoss[0m : 6.73744
[1mStep[0m  [20/42], [94mLoss[0m : 6.16084
[1mStep[0m  [24/42], [94mLoss[0m : 6.37974
[1mStep[0m  [28/42], [94mLoss[0m : 6.03392
[1mStep[0m  [32/42], [94mLoss[0m : 6.20742
[1mStep[0m  [36/42], [94mLoss[0m : 5.76124
[1mStep[0m  [40/42], [94mLoss[0m : 5.76061

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 6.344, [92mTest[0m: 7.055, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.51263
[1mStep[0m  [4/42], [94mLoss[0m : 5.34293
[1mStep[0m  [8/42], [94mLoss[0m : 5.21265
[1mStep[0m  [12/42], [94mLoss[0m : 4.94462
[1mStep[0m  [16/42], [94mLoss[0m : 4.54790
[1mStep[0m  [20/42], [94mLoss[0m : 3.88653
[1mStep[0m  [24/42], [94mLoss[0m : 4.43563
[1mStep[0m  [28/42], [94mLoss[0m : 3.87390
[1mStep[0m  [32/42], [94mLoss[0m : 3.78472
[1mStep[0m  [36/42], [94mLoss[0m : 3.54577
[1mStep[0m  [40/42], [94mLoss[0m : 3.64062

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 4.417, [92mTest[0m: 4.273, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.26539
[1mStep[0m  [4/42], [94mLoss[0m : 2.79383
[1mStep[0m  [8/42], [94mLoss[0m : 2.98157
[1mStep[0m  [12/42], [94mLoss[0m : 2.92626
[1mStep[0m  [16/42], [94mLoss[0m : 2.85166
[1mStep[0m  [20/42], [94mLoss[0m : 2.64746
[1mStep[0m  [24/42], [94mLoss[0m : 2.52582
[1mStep[0m  [28/42], [94mLoss[0m : 2.63891
[1mStep[0m  [32/42], [94mLoss[0m : 2.41746
[1mStep[0m  [36/42], [94mLoss[0m : 2.55360
[1mStep[0m  [40/42], [94mLoss[0m : 2.86861

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.789, [92mTest[0m: 2.733, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.87539
[1mStep[0m  [4/42], [94mLoss[0m : 2.54700
[1mStep[0m  [8/42], [94mLoss[0m : 2.56592
[1mStep[0m  [12/42], [94mLoss[0m : 2.85133
[1mStep[0m  [16/42], [94mLoss[0m : 2.67049
[1mStep[0m  [20/42], [94mLoss[0m : 2.61721
[1mStep[0m  [24/42], [94mLoss[0m : 2.67524
[1mStep[0m  [28/42], [94mLoss[0m : 2.43156
[1mStep[0m  [32/42], [94mLoss[0m : 2.45482
[1mStep[0m  [36/42], [94mLoss[0m : 2.64717
[1mStep[0m  [40/42], [94mLoss[0m : 2.59329

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.597, [92mTest[0m: 2.561, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43353
[1mStep[0m  [4/42], [94mLoss[0m : 2.57487
[1mStep[0m  [8/42], [94mLoss[0m : 2.56674
[1mStep[0m  [12/42], [94mLoss[0m : 2.66736
[1mStep[0m  [16/42], [94mLoss[0m : 2.58799
[1mStep[0m  [20/42], [94mLoss[0m : 2.84664
[1mStep[0m  [24/42], [94mLoss[0m : 2.53279
[1mStep[0m  [28/42], [94mLoss[0m : 2.56781
[1mStep[0m  [32/42], [94mLoss[0m : 2.39816
[1mStep[0m  [36/42], [94mLoss[0m : 2.45246
[1mStep[0m  [40/42], [94mLoss[0m : 2.55772

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.581, [92mTest[0m: 2.471, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64838
[1mStep[0m  [4/42], [94mLoss[0m : 2.54721
[1mStep[0m  [8/42], [94mLoss[0m : 2.53300
[1mStep[0m  [12/42], [94mLoss[0m : 2.37248
[1mStep[0m  [16/42], [94mLoss[0m : 2.69475
[1mStep[0m  [20/42], [94mLoss[0m : 2.63748
[1mStep[0m  [24/42], [94mLoss[0m : 2.69712
[1mStep[0m  [28/42], [94mLoss[0m : 2.67402
[1mStep[0m  [32/42], [94mLoss[0m : 2.58912
[1mStep[0m  [36/42], [94mLoss[0m : 2.33228
[1mStep[0m  [40/42], [94mLoss[0m : 2.25348

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.542, [92mTest[0m: 2.451, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45705
[1mStep[0m  [4/42], [94mLoss[0m : 2.55054
[1mStep[0m  [8/42], [94mLoss[0m : 2.48779
[1mStep[0m  [12/42], [94mLoss[0m : 2.42422
[1mStep[0m  [16/42], [94mLoss[0m : 2.49217
[1mStep[0m  [20/42], [94mLoss[0m : 2.72030
[1mStep[0m  [24/42], [94mLoss[0m : 2.42641
[1mStep[0m  [28/42], [94mLoss[0m : 2.43326
[1mStep[0m  [32/42], [94mLoss[0m : 2.56798
[1mStep[0m  [36/42], [94mLoss[0m : 2.57712
[1mStep[0m  [40/42], [94mLoss[0m : 2.66239

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.431, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55804
[1mStep[0m  [4/42], [94mLoss[0m : 2.51730
[1mStep[0m  [8/42], [94mLoss[0m : 2.48347
[1mStep[0m  [12/42], [94mLoss[0m : 2.31120
[1mStep[0m  [16/42], [94mLoss[0m : 2.34040
[1mStep[0m  [20/42], [94mLoss[0m : 2.43721
[1mStep[0m  [24/42], [94mLoss[0m : 2.75168
[1mStep[0m  [28/42], [94mLoss[0m : 2.56074
[1mStep[0m  [32/42], [94mLoss[0m : 2.60714
[1mStep[0m  [36/42], [94mLoss[0m : 2.33502
[1mStep[0m  [40/42], [94mLoss[0m : 2.66673

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.351, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56300
[1mStep[0m  [4/42], [94mLoss[0m : 2.30708
[1mStep[0m  [8/42], [94mLoss[0m : 2.60164
[1mStep[0m  [12/42], [94mLoss[0m : 2.48675
[1mStep[0m  [16/42], [94mLoss[0m : 2.56554
[1mStep[0m  [20/42], [94mLoss[0m : 2.27749
[1mStep[0m  [24/42], [94mLoss[0m : 2.45478
[1mStep[0m  [28/42], [94mLoss[0m : 2.53074
[1mStep[0m  [32/42], [94mLoss[0m : 2.63729
[1mStep[0m  [36/42], [94mLoss[0m : 2.59624
[1mStep[0m  [40/42], [94mLoss[0m : 2.88246

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.386, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.19980
[1mStep[0m  [4/42], [94mLoss[0m : 2.54906
[1mStep[0m  [8/42], [94mLoss[0m : 2.43414
[1mStep[0m  [12/42], [94mLoss[0m : 2.40450
[1mStep[0m  [16/42], [94mLoss[0m : 2.38232
[1mStep[0m  [20/42], [94mLoss[0m : 2.49345
[1mStep[0m  [24/42], [94mLoss[0m : 2.42181
[1mStep[0m  [28/42], [94mLoss[0m : 2.53004
[1mStep[0m  [32/42], [94mLoss[0m : 2.41872
[1mStep[0m  [36/42], [94mLoss[0m : 2.50086
[1mStep[0m  [40/42], [94mLoss[0m : 2.61177

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.366, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50447
[1mStep[0m  [4/42], [94mLoss[0m : 2.31708
[1mStep[0m  [8/42], [94mLoss[0m : 2.39490
[1mStep[0m  [12/42], [94mLoss[0m : 2.40754
[1mStep[0m  [16/42], [94mLoss[0m : 2.39826
[1mStep[0m  [20/42], [94mLoss[0m : 2.63415
[1mStep[0m  [24/42], [94mLoss[0m : 2.20836
[1mStep[0m  [28/42], [94mLoss[0m : 2.49743
[1mStep[0m  [32/42], [94mLoss[0m : 2.37073
[1mStep[0m  [36/42], [94mLoss[0m : 2.14365
[1mStep[0m  [40/42], [94mLoss[0m : 2.41489

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.380, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38130
[1mStep[0m  [4/42], [94mLoss[0m : 2.62475
[1mStep[0m  [8/42], [94mLoss[0m : 2.33034
[1mStep[0m  [12/42], [94mLoss[0m : 2.34034
[1mStep[0m  [16/42], [94mLoss[0m : 2.50437
[1mStep[0m  [20/42], [94mLoss[0m : 2.12827
[1mStep[0m  [24/42], [94mLoss[0m : 2.48099
[1mStep[0m  [28/42], [94mLoss[0m : 2.75733
[1mStep[0m  [32/42], [94mLoss[0m : 2.33674
[1mStep[0m  [36/42], [94mLoss[0m : 2.33997
[1mStep[0m  [40/42], [94mLoss[0m : 2.51629

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.400, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42981
[1mStep[0m  [4/42], [94mLoss[0m : 2.50726
[1mStep[0m  [8/42], [94mLoss[0m : 2.33313
[1mStep[0m  [12/42], [94mLoss[0m : 2.39321
[1mStep[0m  [16/42], [94mLoss[0m : 2.52802
[1mStep[0m  [20/42], [94mLoss[0m : 2.39680
[1mStep[0m  [24/42], [94mLoss[0m : 2.55466
[1mStep[0m  [28/42], [94mLoss[0m : 2.40184
[1mStep[0m  [32/42], [94mLoss[0m : 2.60869
[1mStep[0m  [36/42], [94mLoss[0m : 2.50967
[1mStep[0m  [40/42], [94mLoss[0m : 2.26785

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.381, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38261
[1mStep[0m  [4/42], [94mLoss[0m : 2.53480
[1mStep[0m  [8/42], [94mLoss[0m : 2.49603
[1mStep[0m  [12/42], [94mLoss[0m : 2.39764
[1mStep[0m  [16/42], [94mLoss[0m : 2.54159
[1mStep[0m  [20/42], [94mLoss[0m : 2.27107
[1mStep[0m  [24/42], [94mLoss[0m : 2.56608
[1mStep[0m  [28/42], [94mLoss[0m : 2.32393
[1mStep[0m  [32/42], [94mLoss[0m : 2.34917
[1mStep[0m  [36/42], [94mLoss[0m : 2.43116
[1mStep[0m  [40/42], [94mLoss[0m : 2.59633

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.358, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44722
[1mStep[0m  [4/42], [94mLoss[0m : 2.19459
[1mStep[0m  [8/42], [94mLoss[0m : 2.48112
[1mStep[0m  [12/42], [94mLoss[0m : 2.19979
[1mStep[0m  [16/42], [94mLoss[0m : 2.61328
[1mStep[0m  [20/42], [94mLoss[0m : 2.36139
[1mStep[0m  [24/42], [94mLoss[0m : 2.61241
[1mStep[0m  [28/42], [94mLoss[0m : 2.28824
[1mStep[0m  [32/42], [94mLoss[0m : 2.43570
[1mStep[0m  [36/42], [94mLoss[0m : 2.31841
[1mStep[0m  [40/42], [94mLoss[0m : 2.45652

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.387, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46343
[1mStep[0m  [4/42], [94mLoss[0m : 2.41550
[1mStep[0m  [8/42], [94mLoss[0m : 2.39018
[1mStep[0m  [12/42], [94mLoss[0m : 2.41176
[1mStep[0m  [16/42], [94mLoss[0m : 2.43394
[1mStep[0m  [20/42], [94mLoss[0m : 2.49084
[1mStep[0m  [24/42], [94mLoss[0m : 2.23199
[1mStep[0m  [28/42], [94mLoss[0m : 2.61925
[1mStep[0m  [32/42], [94mLoss[0m : 2.31772
[1mStep[0m  [36/42], [94mLoss[0m : 2.61481
[1mStep[0m  [40/42], [94mLoss[0m : 2.52378

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.406, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44266
[1mStep[0m  [4/42], [94mLoss[0m : 2.36500
[1mStep[0m  [8/42], [94mLoss[0m : 2.68248
[1mStep[0m  [12/42], [94mLoss[0m : 2.33464
[1mStep[0m  [16/42], [94mLoss[0m : 2.44799
[1mStep[0m  [20/42], [94mLoss[0m : 2.32013
[1mStep[0m  [24/42], [94mLoss[0m : 2.49003
[1mStep[0m  [28/42], [94mLoss[0m : 2.34862
[1mStep[0m  [32/42], [94mLoss[0m : 2.25459
[1mStep[0m  [36/42], [94mLoss[0m : 2.20778
[1mStep[0m  [40/42], [94mLoss[0m : 2.48599

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.396, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42150
[1mStep[0m  [4/42], [94mLoss[0m : 2.34947
[1mStep[0m  [8/42], [94mLoss[0m : 2.45477
[1mStep[0m  [12/42], [94mLoss[0m : 2.71108
[1mStep[0m  [16/42], [94mLoss[0m : 2.49740
[1mStep[0m  [20/42], [94mLoss[0m : 2.16626
[1mStep[0m  [24/42], [94mLoss[0m : 2.40186
[1mStep[0m  [28/42], [94mLoss[0m : 2.39971
[1mStep[0m  [32/42], [94mLoss[0m : 2.27509
[1mStep[0m  [36/42], [94mLoss[0m : 2.41768
[1mStep[0m  [40/42], [94mLoss[0m : 2.42683

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.359, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37208
[1mStep[0m  [4/42], [94mLoss[0m : 2.40803
[1mStep[0m  [8/42], [94mLoss[0m : 2.40724
[1mStep[0m  [12/42], [94mLoss[0m : 2.50007
[1mStep[0m  [16/42], [94mLoss[0m : 2.46071
[1mStep[0m  [20/42], [94mLoss[0m : 2.25202
[1mStep[0m  [24/42], [94mLoss[0m : 2.59620
[1mStep[0m  [28/42], [94mLoss[0m : 2.36546
[1mStep[0m  [32/42], [94mLoss[0m : 2.45609
[1mStep[0m  [36/42], [94mLoss[0m : 2.52481
[1mStep[0m  [40/42], [94mLoss[0m : 2.60969

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.395, [92mTest[0m: 2.347, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38581
[1mStep[0m  [4/42], [94mLoss[0m : 2.36366
[1mStep[0m  [8/42], [94mLoss[0m : 2.45500
[1mStep[0m  [12/42], [94mLoss[0m : 2.24376
[1mStep[0m  [16/42], [94mLoss[0m : 2.31196
[1mStep[0m  [20/42], [94mLoss[0m : 2.05217
[1mStep[0m  [24/42], [94mLoss[0m : 2.17557
[1mStep[0m  [28/42], [94mLoss[0m : 2.13936
[1mStep[0m  [32/42], [94mLoss[0m : 2.35750
[1mStep[0m  [36/42], [94mLoss[0m : 2.58816
[1mStep[0m  [40/42], [94mLoss[0m : 2.40456

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.387, [92mTest[0m: 2.383, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54533
[1mStep[0m  [4/42], [94mLoss[0m : 2.29500
[1mStep[0m  [8/42], [94mLoss[0m : 2.25184
[1mStep[0m  [12/42], [94mLoss[0m : 2.45419
[1mStep[0m  [16/42], [94mLoss[0m : 2.56776
[1mStep[0m  [20/42], [94mLoss[0m : 2.32835
[1mStep[0m  [24/42], [94mLoss[0m : 2.41669
[1mStep[0m  [28/42], [94mLoss[0m : 2.37706
[1mStep[0m  [32/42], [94mLoss[0m : 2.55892
[1mStep[0m  [36/42], [94mLoss[0m : 2.39572
[1mStep[0m  [40/42], [94mLoss[0m : 2.68327

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.356, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48778
[1mStep[0m  [4/42], [94mLoss[0m : 2.38190
[1mStep[0m  [8/42], [94mLoss[0m : 2.45909
[1mStep[0m  [12/42], [94mLoss[0m : 2.32954
[1mStep[0m  [16/42], [94mLoss[0m : 2.33158
[1mStep[0m  [20/42], [94mLoss[0m : 2.54123
[1mStep[0m  [24/42], [94mLoss[0m : 2.11055
[1mStep[0m  [28/42], [94mLoss[0m : 2.50163
[1mStep[0m  [32/42], [94mLoss[0m : 2.21979
[1mStep[0m  [36/42], [94mLoss[0m : 2.35652
[1mStep[0m  [40/42], [94mLoss[0m : 2.56171

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.374, [92mTest[0m: 2.366, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21134
[1mStep[0m  [4/42], [94mLoss[0m : 2.37031
[1mStep[0m  [8/42], [94mLoss[0m : 2.37192
[1mStep[0m  [12/42], [94mLoss[0m : 2.37513
[1mStep[0m  [16/42], [94mLoss[0m : 2.45982
[1mStep[0m  [20/42], [94mLoss[0m : 2.18789
[1mStep[0m  [24/42], [94mLoss[0m : 2.34600
[1mStep[0m  [28/42], [94mLoss[0m : 2.20442
[1mStep[0m  [32/42], [94mLoss[0m : 2.55526
[1mStep[0m  [36/42], [94mLoss[0m : 2.29783
[1mStep[0m  [40/42], [94mLoss[0m : 2.44526

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.387, [92mTest[0m: 2.344, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27129
[1mStep[0m  [4/42], [94mLoss[0m : 2.29721
[1mStep[0m  [8/42], [94mLoss[0m : 2.49122
[1mStep[0m  [12/42], [94mLoss[0m : 2.41428
[1mStep[0m  [16/42], [94mLoss[0m : 2.41701
[1mStep[0m  [20/42], [94mLoss[0m : 2.19202
[1mStep[0m  [24/42], [94mLoss[0m : 2.33480
[1mStep[0m  [28/42], [94mLoss[0m : 2.65728
[1mStep[0m  [32/42], [94mLoss[0m : 2.65923
[1mStep[0m  [36/42], [94mLoss[0m : 2.46941
[1mStep[0m  [40/42], [94mLoss[0m : 2.32971

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.388, [92mTest[0m: 2.376, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57196
[1mStep[0m  [4/42], [94mLoss[0m : 2.34627
[1mStep[0m  [8/42], [94mLoss[0m : 2.46279
[1mStep[0m  [12/42], [94mLoss[0m : 2.49525
[1mStep[0m  [16/42], [94mLoss[0m : 2.46917
[1mStep[0m  [20/42], [94mLoss[0m : 2.21943
[1mStep[0m  [24/42], [94mLoss[0m : 2.23918
[1mStep[0m  [28/42], [94mLoss[0m : 2.45058
[1mStep[0m  [32/42], [94mLoss[0m : 2.46848
[1mStep[0m  [36/42], [94mLoss[0m : 2.42618
[1mStep[0m  [40/42], [94mLoss[0m : 2.42518

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.387, [92mTest[0m: 2.342, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44721
[1mStep[0m  [4/42], [94mLoss[0m : 2.43404
[1mStep[0m  [8/42], [94mLoss[0m : 2.49008
[1mStep[0m  [12/42], [94mLoss[0m : 2.43082
[1mStep[0m  [16/42], [94mLoss[0m : 2.24109
[1mStep[0m  [20/42], [94mLoss[0m : 2.54663
[1mStep[0m  [24/42], [94mLoss[0m : 2.37215
[1mStep[0m  [28/42], [94mLoss[0m : 2.47947
[1mStep[0m  [32/42], [94mLoss[0m : 2.31494
[1mStep[0m  [36/42], [94mLoss[0m : 2.44178
[1mStep[0m  [40/42], [94mLoss[0m : 2.34939

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.377, [92mTest[0m: 2.360, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.14490
[1mStep[0m  [4/42], [94mLoss[0m : 2.60662
[1mStep[0m  [8/42], [94mLoss[0m : 2.36631
[1mStep[0m  [12/42], [94mLoss[0m : 2.56792
[1mStep[0m  [16/42], [94mLoss[0m : 2.30564
[1mStep[0m  [20/42], [94mLoss[0m : 2.67356
[1mStep[0m  [24/42], [94mLoss[0m : 2.17499
[1mStep[0m  [28/42], [94mLoss[0m : 2.58061
[1mStep[0m  [32/42], [94mLoss[0m : 2.11774
[1mStep[0m  [36/42], [94mLoss[0m : 2.50564
[1mStep[0m  [40/42], [94mLoss[0m : 2.52758

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.367, [92mTest[0m: 2.350, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32108
[1mStep[0m  [4/42], [94mLoss[0m : 2.35979
[1mStep[0m  [8/42], [94mLoss[0m : 2.36695
[1mStep[0m  [12/42], [94mLoss[0m : 2.26664
[1mStep[0m  [16/42], [94mLoss[0m : 2.26199
[1mStep[0m  [20/42], [94mLoss[0m : 2.36035
[1mStep[0m  [24/42], [94mLoss[0m : 2.33541
[1mStep[0m  [28/42], [94mLoss[0m : 2.29534
[1mStep[0m  [32/42], [94mLoss[0m : 2.55152
[1mStep[0m  [36/42], [94mLoss[0m : 2.37529
[1mStep[0m  [40/42], [94mLoss[0m : 2.29475

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.374, [92mTest[0m: 2.356, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.342
====================================

Phase 1 - Evaluation MAE:  2.3417297261101857
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 2.38504
[1mStep[0m  [4/42], [94mLoss[0m : 2.32410
[1mStep[0m  [8/42], [94mLoss[0m : 2.31052
[1mStep[0m  [12/42], [94mLoss[0m : 2.60886
[1mStep[0m  [16/42], [94mLoss[0m : 2.58142
[1mStep[0m  [20/42], [94mLoss[0m : 2.49561
[1mStep[0m  [24/42], [94mLoss[0m : 2.45839
[1mStep[0m  [28/42], [94mLoss[0m : 2.18140
[1mStep[0m  [32/42], [94mLoss[0m : 2.45517
[1mStep[0m  [36/42], [94mLoss[0m : 2.47518
[1mStep[0m  [40/42], [94mLoss[0m : 2.56742

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.346, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44129
[1mStep[0m  [4/42], [94mLoss[0m : 2.25755
[1mStep[0m  [8/42], [94mLoss[0m : 2.52406
[1mStep[0m  [12/42], [94mLoss[0m : 2.45549
[1mStep[0m  [16/42], [94mLoss[0m : 2.47488
[1mStep[0m  [20/42], [94mLoss[0m : 2.38855
[1mStep[0m  [24/42], [94mLoss[0m : 2.28204
[1mStep[0m  [28/42], [94mLoss[0m : 2.22433
[1mStep[0m  [32/42], [94mLoss[0m : 2.25337
[1mStep[0m  [36/42], [94mLoss[0m : 2.32672
[1mStep[0m  [40/42], [94mLoss[0m : 2.30810

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.356, [92mTest[0m: 2.435, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.11993
[1mStep[0m  [4/42], [94mLoss[0m : 2.54200
[1mStep[0m  [8/42], [94mLoss[0m : 2.25404
[1mStep[0m  [12/42], [94mLoss[0m : 2.22076
[1mStep[0m  [16/42], [94mLoss[0m : 2.36779
[1mStep[0m  [20/42], [94mLoss[0m : 2.48558
[1mStep[0m  [24/42], [94mLoss[0m : 2.26139
[1mStep[0m  [28/42], [94mLoss[0m : 2.41299
[1mStep[0m  [32/42], [94mLoss[0m : 2.27031
[1mStep[0m  [36/42], [94mLoss[0m : 2.30035
[1mStep[0m  [40/42], [94mLoss[0m : 2.34336

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.298, [92mTest[0m: 2.463, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.14797
[1mStep[0m  [4/42], [94mLoss[0m : 2.32570
[1mStep[0m  [8/42], [94mLoss[0m : 2.18706
[1mStep[0m  [12/42], [94mLoss[0m : 2.39791
[1mStep[0m  [16/42], [94mLoss[0m : 2.18141
[1mStep[0m  [20/42], [94mLoss[0m : 2.06832
[1mStep[0m  [24/42], [94mLoss[0m : 2.44473
[1mStep[0m  [28/42], [94mLoss[0m : 2.28486
[1mStep[0m  [32/42], [94mLoss[0m : 2.28562
[1mStep[0m  [36/42], [94mLoss[0m : 2.20503
[1mStep[0m  [40/42], [94mLoss[0m : 2.21526

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.236, [92mTest[0m: 2.474, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.22949
[1mStep[0m  [4/42], [94mLoss[0m : 2.12286
[1mStep[0m  [8/42], [94mLoss[0m : 1.99699
[1mStep[0m  [12/42], [94mLoss[0m : 2.16704
[1mStep[0m  [16/42], [94mLoss[0m : 2.35766
[1mStep[0m  [20/42], [94mLoss[0m : 2.04265
[1mStep[0m  [24/42], [94mLoss[0m : 2.38405
[1mStep[0m  [28/42], [94mLoss[0m : 2.10465
[1mStep[0m  [32/42], [94mLoss[0m : 2.29117
[1mStep[0m  [36/42], [94mLoss[0m : 2.21381
[1mStep[0m  [40/42], [94mLoss[0m : 1.93123

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.171, [92mTest[0m: 2.491, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.18815
[1mStep[0m  [4/42], [94mLoss[0m : 2.05993
[1mStep[0m  [8/42], [94mLoss[0m : 2.02152
[1mStep[0m  [12/42], [94mLoss[0m : 2.14371
[1mStep[0m  [16/42], [94mLoss[0m : 2.14382
[1mStep[0m  [20/42], [94mLoss[0m : 2.11934
[1mStep[0m  [24/42], [94mLoss[0m : 1.91803
[1mStep[0m  [28/42], [94mLoss[0m : 2.10576
[1mStep[0m  [32/42], [94mLoss[0m : 2.38715
[1mStep[0m  [36/42], [94mLoss[0m : 2.26080
[1mStep[0m  [40/42], [94mLoss[0m : 2.26392

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.122, [92mTest[0m: 2.474, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.91416
[1mStep[0m  [4/42], [94mLoss[0m : 1.94812
[1mStep[0m  [8/42], [94mLoss[0m : 1.92496
[1mStep[0m  [12/42], [94mLoss[0m : 2.05131
[1mStep[0m  [16/42], [94mLoss[0m : 2.04565
[1mStep[0m  [20/42], [94mLoss[0m : 2.12007
[1mStep[0m  [24/42], [94mLoss[0m : 1.93766
[1mStep[0m  [28/42], [94mLoss[0m : 1.94491
[1mStep[0m  [32/42], [94mLoss[0m : 2.06739
[1mStep[0m  [36/42], [94mLoss[0m : 1.95738
[1mStep[0m  [40/42], [94mLoss[0m : 2.05231

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.038, [92mTest[0m: 2.836, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.15948
[1mStep[0m  [4/42], [94mLoss[0m : 1.82767
[1mStep[0m  [8/42], [94mLoss[0m : 2.21179
[1mStep[0m  [12/42], [94mLoss[0m : 2.00082
[1mStep[0m  [16/42], [94mLoss[0m : 1.72823
[1mStep[0m  [20/42], [94mLoss[0m : 1.90905
[1mStep[0m  [24/42], [94mLoss[0m : 1.93620
[1mStep[0m  [28/42], [94mLoss[0m : 2.05048
[1mStep[0m  [32/42], [94mLoss[0m : 2.20675
[1mStep[0m  [36/42], [94mLoss[0m : 1.97124
[1mStep[0m  [40/42], [94mLoss[0m : 1.90745

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.981, [92mTest[0m: 2.506, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.71722
[1mStep[0m  [4/42], [94mLoss[0m : 1.83425
[1mStep[0m  [8/42], [94mLoss[0m : 1.66435
[1mStep[0m  [12/42], [94mLoss[0m : 1.85464
[1mStep[0m  [16/42], [94mLoss[0m : 2.01753
[1mStep[0m  [20/42], [94mLoss[0m : 1.87587
[1mStep[0m  [24/42], [94mLoss[0m : 1.98664
[1mStep[0m  [28/42], [94mLoss[0m : 1.98699
[1mStep[0m  [32/42], [94mLoss[0m : 2.17630
[1mStep[0m  [36/42], [94mLoss[0m : 2.09987
[1mStep[0m  [40/42], [94mLoss[0m : 1.91116

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.929, [92mTest[0m: 2.467, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.95771
[1mStep[0m  [4/42], [94mLoss[0m : 1.81826
[1mStep[0m  [8/42], [94mLoss[0m : 1.79459
[1mStep[0m  [12/42], [94mLoss[0m : 2.03568
[1mStep[0m  [16/42], [94mLoss[0m : 1.96936
[1mStep[0m  [20/42], [94mLoss[0m : 1.85935
[1mStep[0m  [24/42], [94mLoss[0m : 1.64612
[1mStep[0m  [28/42], [94mLoss[0m : 1.69681
[1mStep[0m  [32/42], [94mLoss[0m : 1.88343
[1mStep[0m  [36/42], [94mLoss[0m : 1.78612
[1mStep[0m  [40/42], [94mLoss[0m : 2.02904

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.876, [92mTest[0m: 2.557, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.67894
[1mStep[0m  [4/42], [94mLoss[0m : 1.75751
[1mStep[0m  [8/42], [94mLoss[0m : 1.82725
[1mStep[0m  [12/42], [94mLoss[0m : 1.88245
[1mStep[0m  [16/42], [94mLoss[0m : 1.79446
[1mStep[0m  [20/42], [94mLoss[0m : 1.91578
[1mStep[0m  [24/42], [94mLoss[0m : 1.76677
[1mStep[0m  [28/42], [94mLoss[0m : 1.84996
[1mStep[0m  [32/42], [94mLoss[0m : 1.77070
[1mStep[0m  [36/42], [94mLoss[0m : 1.75938
[1mStep[0m  [40/42], [94mLoss[0m : 1.68256

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.827, [92mTest[0m: 2.474, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.74007
[1mStep[0m  [4/42], [94mLoss[0m : 1.73860
[1mStep[0m  [8/42], [94mLoss[0m : 1.70021
[1mStep[0m  [12/42], [94mLoss[0m : 1.65613
[1mStep[0m  [16/42], [94mLoss[0m : 1.65913
[1mStep[0m  [20/42], [94mLoss[0m : 1.88836
[1mStep[0m  [24/42], [94mLoss[0m : 1.90749
[1mStep[0m  [28/42], [94mLoss[0m : 1.81373
[1mStep[0m  [32/42], [94mLoss[0m : 1.85948
[1mStep[0m  [36/42], [94mLoss[0m : 1.65584
[1mStep[0m  [40/42], [94mLoss[0m : 1.67777

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.795, [92mTest[0m: 2.497, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.65952
[1mStep[0m  [4/42], [94mLoss[0m : 1.70147
[1mStep[0m  [8/42], [94mLoss[0m : 1.79135
[1mStep[0m  [12/42], [94mLoss[0m : 1.61204
[1mStep[0m  [16/42], [94mLoss[0m : 1.69255
[1mStep[0m  [20/42], [94mLoss[0m : 1.73648
[1mStep[0m  [24/42], [94mLoss[0m : 1.91765
[1mStep[0m  [28/42], [94mLoss[0m : 1.77206
[1mStep[0m  [32/42], [94mLoss[0m : 1.94062
[1mStep[0m  [36/42], [94mLoss[0m : 1.69740
[1mStep[0m  [40/42], [94mLoss[0m : 1.79501

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.740, [92mTest[0m: 2.479, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.67081
[1mStep[0m  [4/42], [94mLoss[0m : 1.45669
[1mStep[0m  [8/42], [94mLoss[0m : 1.56043
[1mStep[0m  [12/42], [94mLoss[0m : 1.63502
[1mStep[0m  [16/42], [94mLoss[0m : 1.64409
[1mStep[0m  [20/42], [94mLoss[0m : 1.74074
[1mStep[0m  [24/42], [94mLoss[0m : 1.55917
[1mStep[0m  [28/42], [94mLoss[0m : 1.62047
[1mStep[0m  [32/42], [94mLoss[0m : 1.72155
[1mStep[0m  [36/42], [94mLoss[0m : 1.67136
[1mStep[0m  [40/42], [94mLoss[0m : 1.60880

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.684, [92mTest[0m: 2.528, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.50928
[1mStep[0m  [4/42], [94mLoss[0m : 1.66030
[1mStep[0m  [8/42], [94mLoss[0m : 1.54974
[1mStep[0m  [12/42], [94mLoss[0m : 1.57419
[1mStep[0m  [16/42], [94mLoss[0m : 1.63051
[1mStep[0m  [20/42], [94mLoss[0m : 1.60865
[1mStep[0m  [24/42], [94mLoss[0m : 1.61944
[1mStep[0m  [28/42], [94mLoss[0m : 1.51178
[1mStep[0m  [32/42], [94mLoss[0m : 1.64023
[1mStep[0m  [36/42], [94mLoss[0m : 1.69663
[1mStep[0m  [40/42], [94mLoss[0m : 1.62154

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.650, [92mTest[0m: 2.486, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.52019
[1mStep[0m  [4/42], [94mLoss[0m : 1.72993
[1mStep[0m  [8/42], [94mLoss[0m : 1.45554
[1mStep[0m  [12/42], [94mLoss[0m : 1.42683
[1mStep[0m  [16/42], [94mLoss[0m : 1.47430
[1mStep[0m  [20/42], [94mLoss[0m : 1.53255
[1mStep[0m  [24/42], [94mLoss[0m : 1.64539
[1mStep[0m  [28/42], [94mLoss[0m : 1.58827
[1mStep[0m  [32/42], [94mLoss[0m : 1.60162
[1mStep[0m  [36/42], [94mLoss[0m : 1.42319
[1mStep[0m  [40/42], [94mLoss[0m : 1.96426

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.611, [92mTest[0m: 2.469, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.41206
[1mStep[0m  [4/42], [94mLoss[0m : 1.61768
[1mStep[0m  [8/42], [94mLoss[0m : 1.73827
[1mStep[0m  [12/42], [94mLoss[0m : 1.54852
[1mStep[0m  [16/42], [94mLoss[0m : 1.52184
[1mStep[0m  [20/42], [94mLoss[0m : 1.65123
[1mStep[0m  [24/42], [94mLoss[0m : 1.49531
[1mStep[0m  [28/42], [94mLoss[0m : 1.60408
[1mStep[0m  [32/42], [94mLoss[0m : 1.50538
[1mStep[0m  [36/42], [94mLoss[0m : 1.51927
[1mStep[0m  [40/42], [94mLoss[0m : 1.62287

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.575, [92mTest[0m: 2.451, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.54588
[1mStep[0m  [4/42], [94mLoss[0m : 1.47361
[1mStep[0m  [8/42], [94mLoss[0m : 1.52001
[1mStep[0m  [12/42], [94mLoss[0m : 1.53159
[1mStep[0m  [16/42], [94mLoss[0m : 1.62247
[1mStep[0m  [20/42], [94mLoss[0m : 1.70631
[1mStep[0m  [24/42], [94mLoss[0m : 1.56640
[1mStep[0m  [28/42], [94mLoss[0m : 1.55083
[1mStep[0m  [32/42], [94mLoss[0m : 1.54086
[1mStep[0m  [36/42], [94mLoss[0m : 1.57506
[1mStep[0m  [40/42], [94mLoss[0m : 1.46991

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.544, [92mTest[0m: 2.577, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.45347
[1mStep[0m  [4/42], [94mLoss[0m : 1.52632
[1mStep[0m  [8/42], [94mLoss[0m : 1.71699
[1mStep[0m  [12/42], [94mLoss[0m : 1.44450
[1mStep[0m  [16/42], [94mLoss[0m : 1.58169
[1mStep[0m  [20/42], [94mLoss[0m : 1.45630
[1mStep[0m  [24/42], [94mLoss[0m : 1.47504
[1mStep[0m  [28/42], [94mLoss[0m : 1.57680
[1mStep[0m  [32/42], [94mLoss[0m : 1.40390
[1mStep[0m  [36/42], [94mLoss[0m : 1.41209
[1mStep[0m  [40/42], [94mLoss[0m : 1.60344

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.530, [92mTest[0m: 2.480, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.42143
[1mStep[0m  [4/42], [94mLoss[0m : 1.59844
[1mStep[0m  [8/42], [94mLoss[0m : 1.55183
[1mStep[0m  [12/42], [94mLoss[0m : 1.50304
[1mStep[0m  [16/42], [94mLoss[0m : 1.59334
[1mStep[0m  [20/42], [94mLoss[0m : 1.40900
[1mStep[0m  [24/42], [94mLoss[0m : 1.46514
[1mStep[0m  [28/42], [94mLoss[0m : 1.53537
[1mStep[0m  [32/42], [94mLoss[0m : 1.41793
[1mStep[0m  [36/42], [94mLoss[0m : 1.45750
[1mStep[0m  [40/42], [94mLoss[0m : 1.38445

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.497, [92mTest[0m: 2.545, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.46027
[1mStep[0m  [4/42], [94mLoss[0m : 1.50436
[1mStep[0m  [8/42], [94mLoss[0m : 1.45590
[1mStep[0m  [12/42], [94mLoss[0m : 1.50502
[1mStep[0m  [16/42], [94mLoss[0m : 1.43388
[1mStep[0m  [20/42], [94mLoss[0m : 1.57626
[1mStep[0m  [24/42], [94mLoss[0m : 1.36843
[1mStep[0m  [28/42], [94mLoss[0m : 1.48002
[1mStep[0m  [32/42], [94mLoss[0m : 1.32590
[1mStep[0m  [36/42], [94mLoss[0m : 1.35472
[1mStep[0m  [40/42], [94mLoss[0m : 1.40647

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.459, [92mTest[0m: 2.527, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.40001
[1mStep[0m  [4/42], [94mLoss[0m : 1.31772
[1mStep[0m  [8/42], [94mLoss[0m : 1.32187
[1mStep[0m  [12/42], [94mLoss[0m : 1.36387
[1mStep[0m  [16/42], [94mLoss[0m : 1.41373
[1mStep[0m  [20/42], [94mLoss[0m : 1.35553
[1mStep[0m  [24/42], [94mLoss[0m : 1.40194
[1mStep[0m  [28/42], [94mLoss[0m : 1.48917
[1mStep[0m  [32/42], [94mLoss[0m : 1.48944
[1mStep[0m  [36/42], [94mLoss[0m : 1.29642
[1mStep[0m  [40/42], [94mLoss[0m : 1.37748

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.404, [92mTest[0m: 2.535, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.40885
[1mStep[0m  [4/42], [94mLoss[0m : 1.25862
[1mStep[0m  [8/42], [94mLoss[0m : 1.23936
[1mStep[0m  [12/42], [94mLoss[0m : 1.24013
[1mStep[0m  [16/42], [94mLoss[0m : 1.35817
[1mStep[0m  [20/42], [94mLoss[0m : 1.45228
[1mStep[0m  [24/42], [94mLoss[0m : 1.24705
[1mStep[0m  [28/42], [94mLoss[0m : 1.39365
[1mStep[0m  [32/42], [94mLoss[0m : 1.35142
[1mStep[0m  [36/42], [94mLoss[0m : 1.32008
[1mStep[0m  [40/42], [94mLoss[0m : 1.31129

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.369, [92mTest[0m: 2.516, [96mlr[0m: 0.0023175
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 22 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.557
====================================

Phase 2 - Evaluation MAE:  2.5569201026644026
MAE score P1       2.34173
MAE score P2       2.55692
loss              1.368948
learning_rate     0.002575
batch_size             256
hidden_sizes         [300]
epochs                  30
activation            tanh
optimizer              sgd
early stopping        True
dropout                0.2
momentum               0.9
weight_decay        0.0001
Name: 5, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/21], [94mLoss[0m : 10.83197
[1mStep[0m  [2/21], [94mLoss[0m : 10.84155
[1mStep[0m  [4/21], [94mLoss[0m : 10.83105
[1mStep[0m  [6/21], [94mLoss[0m : 10.87404
[1mStep[0m  [8/21], [94mLoss[0m : 10.75605
[1mStep[0m  [10/21], [94mLoss[0m : 10.78595
[1mStep[0m  [12/21], [94mLoss[0m : 10.65080
[1mStep[0m  [14/21], [94mLoss[0m : 10.47638
[1mStep[0m  [16/21], [94mLoss[0m : 10.44578
[1mStep[0m  [18/21], [94mLoss[0m : 10.34651
[1mStep[0m  [20/21], [94mLoss[0m : 10.60758

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.719, [92mTest[0m: 10.821, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.20473
[1mStep[0m  [2/21], [94mLoss[0m : 10.09780
[1mStep[0m  [4/21], [94mLoss[0m : 10.36815
[1mStep[0m  [6/21], [94mLoss[0m : 10.17703
[1mStep[0m  [8/21], [94mLoss[0m : 10.15967
[1mStep[0m  [10/21], [94mLoss[0m : 9.93548
[1mStep[0m  [12/21], [94mLoss[0m : 9.77068
[1mStep[0m  [14/21], [94mLoss[0m : 9.63492
[1mStep[0m  [16/21], [94mLoss[0m : 9.57825
[1mStep[0m  [18/21], [94mLoss[0m : 9.73249
[1mStep[0m  [20/21], [94mLoss[0m : 9.60631

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.935, [92mTest[0m: 10.297, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.25613
[1mStep[0m  [2/21], [94mLoss[0m : 9.24763
[1mStep[0m  [4/21], [94mLoss[0m : 8.82639
[1mStep[0m  [6/21], [94mLoss[0m : 8.85291
[1mStep[0m  [8/21], [94mLoss[0m : 8.78204
[1mStep[0m  [10/21], [94mLoss[0m : 8.99990
[1mStep[0m  [12/21], [94mLoss[0m : 8.72564
[1mStep[0m  [14/21], [94mLoss[0m : 8.48279
[1mStep[0m  [16/21], [94mLoss[0m : 8.79325
[1mStep[0m  [18/21], [94mLoss[0m : 8.35873
[1mStep[0m  [20/21], [94mLoss[0m : 8.32238

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 8.852, [92mTest[0m: 9.216, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.04523
[1mStep[0m  [2/21], [94mLoss[0m : 8.37191
[1mStep[0m  [4/21], [94mLoss[0m : 7.97909
[1mStep[0m  [6/21], [94mLoss[0m : 7.75168
[1mStep[0m  [8/21], [94mLoss[0m : 7.87892
[1mStep[0m  [10/21], [94mLoss[0m : 7.75491
[1mStep[0m  [12/21], [94mLoss[0m : 7.71194
[1mStep[0m  [14/21], [94mLoss[0m : 7.69667
[1mStep[0m  [16/21], [94mLoss[0m : 7.42097
[1mStep[0m  [18/21], [94mLoss[0m : 7.31683
[1mStep[0m  [20/21], [94mLoss[0m : 7.20564

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 7.725, [92mTest[0m: 8.155, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.08355
[1mStep[0m  [2/21], [94mLoss[0m : 7.22762
[1mStep[0m  [4/21], [94mLoss[0m : 7.13747
[1mStep[0m  [6/21], [94mLoss[0m : 6.92494
[1mStep[0m  [8/21], [94mLoss[0m : 6.83170
[1mStep[0m  [10/21], [94mLoss[0m : 6.55005
[1mStep[0m  [12/21], [94mLoss[0m : 6.88192
[1mStep[0m  [14/21], [94mLoss[0m : 6.44852
[1mStep[0m  [16/21], [94mLoss[0m : 6.37276
[1mStep[0m  [18/21], [94mLoss[0m : 6.62129
[1mStep[0m  [20/21], [94mLoss[0m : 6.25157

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 6.736, [92mTest[0m: 7.088, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.17732
[1mStep[0m  [2/21], [94mLoss[0m : 6.28005
[1mStep[0m  [4/21], [94mLoss[0m : 6.03114
[1mStep[0m  [6/21], [94mLoss[0m : 5.74616
[1mStep[0m  [8/21], [94mLoss[0m : 6.03292
[1mStep[0m  [10/21], [94mLoss[0m : 5.87672
[1mStep[0m  [12/21], [94mLoss[0m : 5.79786
[1mStep[0m  [14/21], [94mLoss[0m : 5.55797
[1mStep[0m  [16/21], [94mLoss[0m : 5.60128
[1mStep[0m  [18/21], [94mLoss[0m : 5.56106
[1mStep[0m  [20/21], [94mLoss[0m : 5.47932

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 5.816, [92mTest[0m: 5.485, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.37151
[1mStep[0m  [2/21], [94mLoss[0m : 5.11008
[1mStep[0m  [4/21], [94mLoss[0m : 5.20813
[1mStep[0m  [6/21], [94mLoss[0m : 4.89402
[1mStep[0m  [8/21], [94mLoss[0m : 4.95410
[1mStep[0m  [10/21], [94mLoss[0m : 4.88062
[1mStep[0m  [12/21], [94mLoss[0m : 4.88029
[1mStep[0m  [14/21], [94mLoss[0m : 4.80795
[1mStep[0m  [16/21], [94mLoss[0m : 4.19147
[1mStep[0m  [18/21], [94mLoss[0m : 4.40097
[1mStep[0m  [20/21], [94mLoss[0m : 4.23064

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 4.787, [92mTest[0m: 4.122, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.09001
[1mStep[0m  [2/21], [94mLoss[0m : 4.04833
[1mStep[0m  [4/21], [94mLoss[0m : 4.30078
[1mStep[0m  [6/21], [94mLoss[0m : 3.88372
[1mStep[0m  [8/21], [94mLoss[0m : 3.93307
[1mStep[0m  [10/21], [94mLoss[0m : 3.74797
[1mStep[0m  [12/21], [94mLoss[0m : 3.61924
[1mStep[0m  [14/21], [94mLoss[0m : 3.49666
[1mStep[0m  [16/21], [94mLoss[0m : 3.63363
[1mStep[0m  [18/21], [94mLoss[0m : 3.49699
[1mStep[0m  [20/21], [94mLoss[0m : 3.30636

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 3.737, [92mTest[0m: 3.227, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.26205
[1mStep[0m  [2/21], [94mLoss[0m : 3.09938
[1mStep[0m  [4/21], [94mLoss[0m : 3.02636
[1mStep[0m  [6/21], [94mLoss[0m : 3.02129
[1mStep[0m  [8/21], [94mLoss[0m : 2.99426
[1mStep[0m  [10/21], [94mLoss[0m : 2.88035
[1mStep[0m  [12/21], [94mLoss[0m : 2.59428
[1mStep[0m  [14/21], [94mLoss[0m : 2.75499
[1mStep[0m  [16/21], [94mLoss[0m : 2.81507
[1mStep[0m  [18/21], [94mLoss[0m : 2.64569
[1mStep[0m  [20/21], [94mLoss[0m : 2.71567

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.914, [92mTest[0m: 2.621, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.60502
[1mStep[0m  [2/21], [94mLoss[0m : 2.66566
[1mStep[0m  [4/21], [94mLoss[0m : 2.60864
[1mStep[0m  [6/21], [94mLoss[0m : 2.72036
[1mStep[0m  [8/21], [94mLoss[0m : 2.60260
[1mStep[0m  [10/21], [94mLoss[0m : 2.75855
[1mStep[0m  [12/21], [94mLoss[0m : 2.56485
[1mStep[0m  [14/21], [94mLoss[0m : 2.62580
[1mStep[0m  [16/21], [94mLoss[0m : 2.60829
[1mStep[0m  [18/21], [94mLoss[0m : 2.69523
[1mStep[0m  [20/21], [94mLoss[0m : 2.70070

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.652, [92mTest[0m: 2.422, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.59786
[1mStep[0m  [2/21], [94mLoss[0m : 2.62224
[1mStep[0m  [4/21], [94mLoss[0m : 2.61222
[1mStep[0m  [6/21], [94mLoss[0m : 2.61825
[1mStep[0m  [8/21], [94mLoss[0m : 2.58864
[1mStep[0m  [10/21], [94mLoss[0m : 2.48617
[1mStep[0m  [12/21], [94mLoss[0m : 2.65637
[1mStep[0m  [14/21], [94mLoss[0m : 2.58010
[1mStep[0m  [16/21], [94mLoss[0m : 2.55619
[1mStep[0m  [18/21], [94mLoss[0m : 2.51372
[1mStep[0m  [20/21], [94mLoss[0m : 2.70827

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.604, [92mTest[0m: 2.643, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.58142
[1mStep[0m  [2/21], [94mLoss[0m : 2.60399
[1mStep[0m  [4/21], [94mLoss[0m : 2.57543
[1mStep[0m  [6/21], [94mLoss[0m : 2.42679
[1mStep[0m  [8/21], [94mLoss[0m : 2.49923
[1mStep[0m  [10/21], [94mLoss[0m : 2.63862
[1mStep[0m  [12/21], [94mLoss[0m : 2.63329
[1mStep[0m  [14/21], [94mLoss[0m : 2.73394
[1mStep[0m  [16/21], [94mLoss[0m : 2.69410
[1mStep[0m  [18/21], [94mLoss[0m : 2.53734
[1mStep[0m  [20/21], [94mLoss[0m : 2.61169

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.596, [92mTest[0m: 2.431, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44004
[1mStep[0m  [2/21], [94mLoss[0m : 2.75853
[1mStep[0m  [4/21], [94mLoss[0m : 2.52760
[1mStep[0m  [6/21], [94mLoss[0m : 2.67093
[1mStep[0m  [8/21], [94mLoss[0m : 2.49022
[1mStep[0m  [10/21], [94mLoss[0m : 2.65651
[1mStep[0m  [12/21], [94mLoss[0m : 2.51650
[1mStep[0m  [14/21], [94mLoss[0m : 2.53993
[1mStep[0m  [16/21], [94mLoss[0m : 2.58194
[1mStep[0m  [18/21], [94mLoss[0m : 2.54525
[1mStep[0m  [20/21], [94mLoss[0m : 2.55214

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.593, [92mTest[0m: 2.399, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.58348
[1mStep[0m  [2/21], [94mLoss[0m : 2.68973
[1mStep[0m  [4/21], [94mLoss[0m : 2.60239
[1mStep[0m  [6/21], [94mLoss[0m : 2.70762
[1mStep[0m  [8/21], [94mLoss[0m : 2.58363
[1mStep[0m  [10/21], [94mLoss[0m : 2.46709
[1mStep[0m  [12/21], [94mLoss[0m : 2.57113
[1mStep[0m  [14/21], [94mLoss[0m : 2.52177
[1mStep[0m  [16/21], [94mLoss[0m : 2.50005
[1mStep[0m  [18/21], [94mLoss[0m : 2.55180
[1mStep[0m  [20/21], [94mLoss[0m : 2.63926

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.566, [92mTest[0m: 2.388, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.57446
[1mStep[0m  [2/21], [94mLoss[0m : 2.71806
[1mStep[0m  [4/21], [94mLoss[0m : 2.56233
[1mStep[0m  [6/21], [94mLoss[0m : 2.51226
[1mStep[0m  [8/21], [94mLoss[0m : 2.60659
[1mStep[0m  [10/21], [94mLoss[0m : 2.56622
[1mStep[0m  [12/21], [94mLoss[0m : 2.47025
[1mStep[0m  [14/21], [94mLoss[0m : 2.60443
[1mStep[0m  [16/21], [94mLoss[0m : 2.61334
[1mStep[0m  [18/21], [94mLoss[0m : 2.57743
[1mStep[0m  [20/21], [94mLoss[0m : 2.67227

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.557, [92mTest[0m: 2.398, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51885
[1mStep[0m  [2/21], [94mLoss[0m : 2.59830
[1mStep[0m  [4/21], [94mLoss[0m : 2.54229
[1mStep[0m  [6/21], [94mLoss[0m : 2.54306
[1mStep[0m  [8/21], [94mLoss[0m : 2.49254
[1mStep[0m  [10/21], [94mLoss[0m : 2.41361
[1mStep[0m  [12/21], [94mLoss[0m : 2.51425
[1mStep[0m  [14/21], [94mLoss[0m : 2.50822
[1mStep[0m  [16/21], [94mLoss[0m : 2.54027
[1mStep[0m  [18/21], [94mLoss[0m : 2.64326
[1mStep[0m  [20/21], [94mLoss[0m : 2.50365

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.544, [92mTest[0m: 2.408, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.67601
[1mStep[0m  [2/21], [94mLoss[0m : 2.53478
[1mStep[0m  [4/21], [94mLoss[0m : 2.49092
[1mStep[0m  [6/21], [94mLoss[0m : 2.52197
[1mStep[0m  [8/21], [94mLoss[0m : 2.65361
[1mStep[0m  [10/21], [94mLoss[0m : 2.57882
[1mStep[0m  [12/21], [94mLoss[0m : 2.80569
[1mStep[0m  [14/21], [94mLoss[0m : 2.53506
[1mStep[0m  [16/21], [94mLoss[0m : 2.64497
[1mStep[0m  [18/21], [94mLoss[0m : 2.49930
[1mStep[0m  [20/21], [94mLoss[0m : 2.52247

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.562, [92mTest[0m: 2.400, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48050
[1mStep[0m  [2/21], [94mLoss[0m : 2.69192
[1mStep[0m  [4/21], [94mLoss[0m : 2.37991
[1mStep[0m  [6/21], [94mLoss[0m : 2.65283
[1mStep[0m  [8/21], [94mLoss[0m : 2.55965
[1mStep[0m  [10/21], [94mLoss[0m : 2.42741
[1mStep[0m  [12/21], [94mLoss[0m : 2.65538
[1mStep[0m  [14/21], [94mLoss[0m : 2.49821
[1mStep[0m  [16/21], [94mLoss[0m : 2.47497
[1mStep[0m  [18/21], [94mLoss[0m : 2.55155
[1mStep[0m  [20/21], [94mLoss[0m : 2.43858

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.529, [92mTest[0m: 2.366, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.59132
[1mStep[0m  [2/21], [94mLoss[0m : 2.64680
[1mStep[0m  [4/21], [94mLoss[0m : 2.52197
[1mStep[0m  [6/21], [94mLoss[0m : 2.53704
[1mStep[0m  [8/21], [94mLoss[0m : 2.57532
[1mStep[0m  [10/21], [94mLoss[0m : 2.55385
[1mStep[0m  [12/21], [94mLoss[0m : 2.45077
[1mStep[0m  [14/21], [94mLoss[0m : 2.60832
[1mStep[0m  [16/21], [94mLoss[0m : 2.45258
[1mStep[0m  [18/21], [94mLoss[0m : 2.47882
[1mStep[0m  [20/21], [94mLoss[0m : 2.51234

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.363, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52549
[1mStep[0m  [2/21], [94mLoss[0m : 2.43625
[1mStep[0m  [4/21], [94mLoss[0m : 2.38715
[1mStep[0m  [6/21], [94mLoss[0m : 2.52587
[1mStep[0m  [8/21], [94mLoss[0m : 2.42826
[1mStep[0m  [10/21], [94mLoss[0m : 2.48126
[1mStep[0m  [12/21], [94mLoss[0m : 2.58456
[1mStep[0m  [14/21], [94mLoss[0m : 2.53456
[1mStep[0m  [16/21], [94mLoss[0m : 2.56839
[1mStep[0m  [18/21], [94mLoss[0m : 2.63662
[1mStep[0m  [20/21], [94mLoss[0m : 2.60489

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.406, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56272
[1mStep[0m  [2/21], [94mLoss[0m : 2.63976
[1mStep[0m  [4/21], [94mLoss[0m : 2.56401
[1mStep[0m  [6/21], [94mLoss[0m : 2.49724
[1mStep[0m  [8/21], [94mLoss[0m : 2.48804
[1mStep[0m  [10/21], [94mLoss[0m : 2.50954
[1mStep[0m  [12/21], [94mLoss[0m : 2.65401
[1mStep[0m  [14/21], [94mLoss[0m : 2.53018
[1mStep[0m  [16/21], [94mLoss[0m : 2.38944
[1mStep[0m  [18/21], [94mLoss[0m : 2.42717
[1mStep[0m  [20/21], [94mLoss[0m : 2.50479

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.529, [92mTest[0m: 2.370, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.45801
[1mStep[0m  [2/21], [94mLoss[0m : 2.53999
[1mStep[0m  [4/21], [94mLoss[0m : 2.59778
[1mStep[0m  [6/21], [94mLoss[0m : 2.60574
[1mStep[0m  [8/21], [94mLoss[0m : 2.42486
[1mStep[0m  [10/21], [94mLoss[0m : 2.58323
[1mStep[0m  [12/21], [94mLoss[0m : 2.55555
[1mStep[0m  [14/21], [94mLoss[0m : 2.53500
[1mStep[0m  [16/21], [94mLoss[0m : 2.51356
[1mStep[0m  [18/21], [94mLoss[0m : 2.59415
[1mStep[0m  [20/21], [94mLoss[0m : 2.42341

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.515, [92mTest[0m: 2.382, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.59248
[1mStep[0m  [2/21], [94mLoss[0m : 2.56083
[1mStep[0m  [4/21], [94mLoss[0m : 2.55276
[1mStep[0m  [6/21], [94mLoss[0m : 2.47399
[1mStep[0m  [8/21], [94mLoss[0m : 2.59781
[1mStep[0m  [10/21], [94mLoss[0m : 2.26956
[1mStep[0m  [12/21], [94mLoss[0m : 2.57804
[1mStep[0m  [14/21], [94mLoss[0m : 2.45428
[1mStep[0m  [16/21], [94mLoss[0m : 2.61453
[1mStep[0m  [18/21], [94mLoss[0m : 2.52585
[1mStep[0m  [20/21], [94mLoss[0m : 2.42353

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.366, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.41442
[1mStep[0m  [2/21], [94mLoss[0m : 2.52852
[1mStep[0m  [4/21], [94mLoss[0m : 2.51763
[1mStep[0m  [6/21], [94mLoss[0m : 2.54668
[1mStep[0m  [8/21], [94mLoss[0m : 2.47800
[1mStep[0m  [10/21], [94mLoss[0m : 2.45994
[1mStep[0m  [12/21], [94mLoss[0m : 2.61535
[1mStep[0m  [14/21], [94mLoss[0m : 2.41897
[1mStep[0m  [16/21], [94mLoss[0m : 2.48998
[1mStep[0m  [18/21], [94mLoss[0m : 2.64115
[1mStep[0m  [20/21], [94mLoss[0m : 2.33977

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.361, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.55752
[1mStep[0m  [2/21], [94mLoss[0m : 2.45885
[1mStep[0m  [4/21], [94mLoss[0m : 2.41090
[1mStep[0m  [6/21], [94mLoss[0m : 2.29198
[1mStep[0m  [8/21], [94mLoss[0m : 2.57386
[1mStep[0m  [10/21], [94mLoss[0m : 2.53111
[1mStep[0m  [12/21], [94mLoss[0m : 2.60714
[1mStep[0m  [14/21], [94mLoss[0m : 2.52356
[1mStep[0m  [16/21], [94mLoss[0m : 2.35232
[1mStep[0m  [18/21], [94mLoss[0m : 2.45152
[1mStep[0m  [20/21], [94mLoss[0m : 2.55614

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.328, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.60058
[1mStep[0m  [2/21], [94mLoss[0m : 2.37653
[1mStep[0m  [4/21], [94mLoss[0m : 2.36964
[1mStep[0m  [6/21], [94mLoss[0m : 2.40734
[1mStep[0m  [8/21], [94mLoss[0m : 2.49231
[1mStep[0m  [10/21], [94mLoss[0m : 2.57232
[1mStep[0m  [12/21], [94mLoss[0m : 2.43570
[1mStep[0m  [14/21], [94mLoss[0m : 2.56221
[1mStep[0m  [16/21], [94mLoss[0m : 2.34391
[1mStep[0m  [18/21], [94mLoss[0m : 2.40883
[1mStep[0m  [20/21], [94mLoss[0m : 2.54880

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.347, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56770
[1mStep[0m  [2/21], [94mLoss[0m : 2.44914
[1mStep[0m  [4/21], [94mLoss[0m : 2.44446
[1mStep[0m  [6/21], [94mLoss[0m : 2.53974
[1mStep[0m  [8/21], [94mLoss[0m : 2.50365
[1mStep[0m  [10/21], [94mLoss[0m : 2.42386
[1mStep[0m  [12/21], [94mLoss[0m : 2.43484
[1mStep[0m  [14/21], [94mLoss[0m : 2.41741
[1mStep[0m  [16/21], [94mLoss[0m : 2.61140
[1mStep[0m  [18/21], [94mLoss[0m : 2.49841
[1mStep[0m  [20/21], [94mLoss[0m : 2.45584

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.353, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42766
[1mStep[0m  [2/21], [94mLoss[0m : 2.42144
[1mStep[0m  [4/21], [94mLoss[0m : 2.41666
[1mStep[0m  [6/21], [94mLoss[0m : 2.44837
[1mStep[0m  [8/21], [94mLoss[0m : 2.55476
[1mStep[0m  [10/21], [94mLoss[0m : 2.46858
[1mStep[0m  [12/21], [94mLoss[0m : 2.51903
[1mStep[0m  [14/21], [94mLoss[0m : 2.45219
[1mStep[0m  [16/21], [94mLoss[0m : 2.37763
[1mStep[0m  [18/21], [94mLoss[0m : 2.49210
[1mStep[0m  [20/21], [94mLoss[0m : 2.38222

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.330, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.39389
[1mStep[0m  [2/21], [94mLoss[0m : 2.46459
[1mStep[0m  [4/21], [94mLoss[0m : 2.53962
[1mStep[0m  [6/21], [94mLoss[0m : 2.50883
[1mStep[0m  [8/21], [94mLoss[0m : 2.41377
[1mStep[0m  [10/21], [94mLoss[0m : 2.47777
[1mStep[0m  [12/21], [94mLoss[0m : 2.41093
[1mStep[0m  [14/21], [94mLoss[0m : 2.47778
[1mStep[0m  [16/21], [94mLoss[0m : 2.44588
[1mStep[0m  [18/21], [94mLoss[0m : 2.45570
[1mStep[0m  [20/21], [94mLoss[0m : 2.60450

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.351, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53683
[1mStep[0m  [2/21], [94mLoss[0m : 2.63309
[1mStep[0m  [4/21], [94mLoss[0m : 2.59130
[1mStep[0m  [6/21], [94mLoss[0m : 2.62778
[1mStep[0m  [8/21], [94mLoss[0m : 2.48041
[1mStep[0m  [10/21], [94mLoss[0m : 2.42751
[1mStep[0m  [12/21], [94mLoss[0m : 2.28662
[1mStep[0m  [14/21], [94mLoss[0m : 2.52971
[1mStep[0m  [16/21], [94mLoss[0m : 2.48189
[1mStep[0m  [18/21], [94mLoss[0m : 2.21818
[1mStep[0m  [20/21], [94mLoss[0m : 2.51327

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.350, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.348
====================================

Phase 1 - Evaluation MAE:  2.348203590938023
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/21], [94mLoss[0m : 2.39110
[1mStep[0m  [2/21], [94mLoss[0m : 2.43112
[1mStep[0m  [4/21], [94mLoss[0m : 2.55378
[1mStep[0m  [6/21], [94mLoss[0m : 2.61840
[1mStep[0m  [8/21], [94mLoss[0m : 2.43575
[1mStep[0m  [10/21], [94mLoss[0m : 2.48546
[1mStep[0m  [12/21], [94mLoss[0m : 2.50964
[1mStep[0m  [14/21], [94mLoss[0m : 2.66109
[1mStep[0m  [16/21], [94mLoss[0m : 2.55631
[1mStep[0m  [18/21], [94mLoss[0m : 2.49217
[1mStep[0m  [20/21], [94mLoss[0m : 2.49968

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.341, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.45490
[1mStep[0m  [2/21], [94mLoss[0m : 2.42080
[1mStep[0m  [4/21], [94mLoss[0m : 2.39354
[1mStep[0m  [6/21], [94mLoss[0m : 2.47917
[1mStep[0m  [8/21], [94mLoss[0m : 2.42051
[1mStep[0m  [10/21], [94mLoss[0m : 2.63285
[1mStep[0m  [12/21], [94mLoss[0m : 2.39326
[1mStep[0m  [14/21], [94mLoss[0m : 2.45260
[1mStep[0m  [16/21], [94mLoss[0m : 2.40308
[1mStep[0m  [18/21], [94mLoss[0m : 2.55923
[1mStep[0m  [20/21], [94mLoss[0m : 2.37927

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.849, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43907
[1mStep[0m  [2/21], [94mLoss[0m : 2.40810
[1mStep[0m  [4/21], [94mLoss[0m : 2.35605
[1mStep[0m  [6/21], [94mLoss[0m : 2.36517
[1mStep[0m  [8/21], [94mLoss[0m : 2.48888
[1mStep[0m  [10/21], [94mLoss[0m : 2.43395
[1mStep[0m  [12/21], [94mLoss[0m : 2.40225
[1mStep[0m  [14/21], [94mLoss[0m : 2.42904
[1mStep[0m  [16/21], [94mLoss[0m : 2.37827
[1mStep[0m  [18/21], [94mLoss[0m : 2.51768
[1mStep[0m  [20/21], [94mLoss[0m : 2.35785

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.504, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.36198
[1mStep[0m  [2/21], [94mLoss[0m : 2.31777
[1mStep[0m  [4/21], [94mLoss[0m : 2.39526
[1mStep[0m  [6/21], [94mLoss[0m : 2.39241
[1mStep[0m  [8/21], [94mLoss[0m : 2.41079
[1mStep[0m  [10/21], [94mLoss[0m : 2.41856
[1mStep[0m  [12/21], [94mLoss[0m : 2.50789
[1mStep[0m  [14/21], [94mLoss[0m : 2.39470
[1mStep[0m  [16/21], [94mLoss[0m : 2.38249
[1mStep[0m  [18/21], [94mLoss[0m : 2.43486
[1mStep[0m  [20/21], [94mLoss[0m : 2.43551

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.737, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.30517
[1mStep[0m  [2/21], [94mLoss[0m : 2.35087
[1mStep[0m  [4/21], [94mLoss[0m : 2.37881
[1mStep[0m  [6/21], [94mLoss[0m : 2.40567
[1mStep[0m  [8/21], [94mLoss[0m : 2.49178
[1mStep[0m  [10/21], [94mLoss[0m : 2.38021
[1mStep[0m  [12/21], [94mLoss[0m : 2.34470
[1mStep[0m  [14/21], [94mLoss[0m : 2.21153
[1mStep[0m  [16/21], [94mLoss[0m : 2.40491
[1mStep[0m  [18/21], [94mLoss[0m : 2.31958
[1mStep[0m  [20/21], [94mLoss[0m : 2.24988

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.336, [92mTest[0m: 3.092, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.20279
[1mStep[0m  [2/21], [94mLoss[0m : 2.22580
[1mStep[0m  [4/21], [94mLoss[0m : 2.37551
[1mStep[0m  [6/21], [94mLoss[0m : 2.22638
[1mStep[0m  [8/21], [94mLoss[0m : 2.37032
[1mStep[0m  [10/21], [94mLoss[0m : 2.54410
[1mStep[0m  [12/21], [94mLoss[0m : 2.33557
[1mStep[0m  [14/21], [94mLoss[0m : 2.26566
[1mStep[0m  [16/21], [94mLoss[0m : 2.29448
[1mStep[0m  [18/21], [94mLoss[0m : 2.40849
[1mStep[0m  [20/21], [94mLoss[0m : 2.30336

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.317, [92mTest[0m: 2.765, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.33401
[1mStep[0m  [2/21], [94mLoss[0m : 2.38707
[1mStep[0m  [4/21], [94mLoss[0m : 2.20501
[1mStep[0m  [6/21], [94mLoss[0m : 2.24325
[1mStep[0m  [8/21], [94mLoss[0m : 2.31795
[1mStep[0m  [10/21], [94mLoss[0m : 2.29245
[1mStep[0m  [12/21], [94mLoss[0m : 2.25023
[1mStep[0m  [14/21], [94mLoss[0m : 2.21631
[1mStep[0m  [16/21], [94mLoss[0m : 2.12416
[1mStep[0m  [18/21], [94mLoss[0m : 2.15434
[1mStep[0m  [20/21], [94mLoss[0m : 2.25609

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.261, [92mTest[0m: 2.616, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.13690
[1mStep[0m  [2/21], [94mLoss[0m : 2.20220
[1mStep[0m  [4/21], [94mLoss[0m : 2.14149
[1mStep[0m  [6/21], [94mLoss[0m : 2.12929
[1mStep[0m  [8/21], [94mLoss[0m : 2.39439
[1mStep[0m  [10/21], [94mLoss[0m : 2.32451
[1mStep[0m  [12/21], [94mLoss[0m : 2.25251
[1mStep[0m  [14/21], [94mLoss[0m : 2.17917
[1mStep[0m  [16/21], [94mLoss[0m : 2.16601
[1mStep[0m  [18/21], [94mLoss[0m : 2.15199
[1mStep[0m  [20/21], [94mLoss[0m : 2.26876

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.201, [92mTest[0m: 2.659, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.15942
[1mStep[0m  [2/21], [94mLoss[0m : 2.23275
[1mStep[0m  [4/21], [94mLoss[0m : 2.09611
[1mStep[0m  [6/21], [94mLoss[0m : 2.17874
[1mStep[0m  [8/21], [94mLoss[0m : 2.22006
[1mStep[0m  [10/21], [94mLoss[0m : 2.12064
[1mStep[0m  [12/21], [94mLoss[0m : 2.10197
[1mStep[0m  [14/21], [94mLoss[0m : 2.15672
[1mStep[0m  [16/21], [94mLoss[0m : 2.03671
[1mStep[0m  [18/21], [94mLoss[0m : 2.17545
[1mStep[0m  [20/21], [94mLoss[0m : 2.24795

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.154, [92mTest[0m: 2.693, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.14897
[1mStep[0m  [2/21], [94mLoss[0m : 2.22116
[1mStep[0m  [4/21], [94mLoss[0m : 2.14569
[1mStep[0m  [6/21], [94mLoss[0m : 1.96803
[1mStep[0m  [8/21], [94mLoss[0m : 2.04430
[1mStep[0m  [10/21], [94mLoss[0m : 2.10544
[1mStep[0m  [12/21], [94mLoss[0m : 2.13497
[1mStep[0m  [14/21], [94mLoss[0m : 2.12198
[1mStep[0m  [16/21], [94mLoss[0m : 2.01086
[1mStep[0m  [18/21], [94mLoss[0m : 2.22470
[1mStep[0m  [20/21], [94mLoss[0m : 2.13506

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.116, [92mTest[0m: 2.629, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.08089
[1mStep[0m  [2/21], [94mLoss[0m : 1.94971
[1mStep[0m  [4/21], [94mLoss[0m : 2.09191
[1mStep[0m  [6/21], [94mLoss[0m : 2.09749
[1mStep[0m  [8/21], [94mLoss[0m : 1.90580
[1mStep[0m  [10/21], [94mLoss[0m : 1.96977
[1mStep[0m  [12/21], [94mLoss[0m : 2.03781
[1mStep[0m  [14/21], [94mLoss[0m : 2.05318
[1mStep[0m  [16/21], [94mLoss[0m : 2.00867
[1mStep[0m  [18/21], [94mLoss[0m : 2.09402
[1mStep[0m  [20/21], [94mLoss[0m : 2.15080

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.036, [92mTest[0m: 2.708, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.97109
[1mStep[0m  [2/21], [94mLoss[0m : 2.02930
[1mStep[0m  [4/21], [94mLoss[0m : 2.01583
[1mStep[0m  [6/21], [94mLoss[0m : 2.00586
[1mStep[0m  [8/21], [94mLoss[0m : 2.04828
[1mStep[0m  [10/21], [94mLoss[0m : 1.97056
[1mStep[0m  [12/21], [94mLoss[0m : 2.04326
[1mStep[0m  [14/21], [94mLoss[0m : 1.83056
[1mStep[0m  [16/21], [94mLoss[0m : 2.06369
[1mStep[0m  [18/21], [94mLoss[0m : 2.00376
[1mStep[0m  [20/21], [94mLoss[0m : 2.16386

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.016, [92mTest[0m: 2.612, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.99582
[1mStep[0m  [2/21], [94mLoss[0m : 2.04735
[1mStep[0m  [4/21], [94mLoss[0m : 1.86885
[1mStep[0m  [6/21], [94mLoss[0m : 1.99597
[1mStep[0m  [8/21], [94mLoss[0m : 1.92250
[1mStep[0m  [10/21], [94mLoss[0m : 1.96585
[1mStep[0m  [12/21], [94mLoss[0m : 2.11761
[1mStep[0m  [14/21], [94mLoss[0m : 1.90003
[1mStep[0m  [16/21], [94mLoss[0m : 2.06804
[1mStep[0m  [18/21], [94mLoss[0m : 1.92631
[1mStep[0m  [20/21], [94mLoss[0m : 1.96324

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.952, [92mTest[0m: 2.558, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.99640
[1mStep[0m  [2/21], [94mLoss[0m : 1.94229
[1mStep[0m  [4/21], [94mLoss[0m : 1.76676
[1mStep[0m  [6/21], [94mLoss[0m : 1.83126
[1mStep[0m  [8/21], [94mLoss[0m : 1.98534
[1mStep[0m  [10/21], [94mLoss[0m : 1.86668
[1mStep[0m  [12/21], [94mLoss[0m : 2.06351
[1mStep[0m  [14/21], [94mLoss[0m : 1.94031
[1mStep[0m  [16/21], [94mLoss[0m : 2.04654
[1mStep[0m  [18/21], [94mLoss[0m : 1.84319
[1mStep[0m  [20/21], [94mLoss[0m : 1.90450

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.925, [92mTest[0m: 2.595, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.84702
[1mStep[0m  [2/21], [94mLoss[0m : 1.89229
[1mStep[0m  [4/21], [94mLoss[0m : 1.88134
[1mStep[0m  [6/21], [94mLoss[0m : 1.85644
[1mStep[0m  [8/21], [94mLoss[0m : 1.97131
[1mStep[0m  [10/21], [94mLoss[0m : 1.86186
[1mStep[0m  [12/21], [94mLoss[0m : 1.90559
[1mStep[0m  [14/21], [94mLoss[0m : 1.91848
[1mStep[0m  [16/21], [94mLoss[0m : 1.90964
[1mStep[0m  [18/21], [94mLoss[0m : 1.82023
[1mStep[0m  [20/21], [94mLoss[0m : 1.96976

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.919, [92mTest[0m: 2.545, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.88351
[1mStep[0m  [2/21], [94mLoss[0m : 1.95615
[1mStep[0m  [4/21], [94mLoss[0m : 1.86178
[1mStep[0m  [6/21], [94mLoss[0m : 1.82319
[1mStep[0m  [8/21], [94mLoss[0m : 1.89229
[1mStep[0m  [10/21], [94mLoss[0m : 1.79812
[1mStep[0m  [12/21], [94mLoss[0m : 1.87500
[1mStep[0m  [14/21], [94mLoss[0m : 1.69222
[1mStep[0m  [16/21], [94mLoss[0m : 1.79143
[1mStep[0m  [18/21], [94mLoss[0m : 1.91924
[1mStep[0m  [20/21], [94mLoss[0m : 1.81436

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.860, [92mTest[0m: 2.619, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.84429
[1mStep[0m  [2/21], [94mLoss[0m : 1.73541
[1mStep[0m  [4/21], [94mLoss[0m : 1.74135
[1mStep[0m  [6/21], [94mLoss[0m : 1.82599
[1mStep[0m  [8/21], [94mLoss[0m : 1.86666
[1mStep[0m  [10/21], [94mLoss[0m : 1.87242
[1mStep[0m  [12/21], [94mLoss[0m : 1.67963
[1mStep[0m  [14/21], [94mLoss[0m : 1.87029
[1mStep[0m  [16/21], [94mLoss[0m : 1.79662
[1mStep[0m  [18/21], [94mLoss[0m : 1.93280
[1mStep[0m  [20/21], [94mLoss[0m : 1.92808

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.814, [92mTest[0m: 2.560, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.74496
[1mStep[0m  [2/21], [94mLoss[0m : 1.91476
[1mStep[0m  [4/21], [94mLoss[0m : 1.74150
[1mStep[0m  [6/21], [94mLoss[0m : 1.75684
[1mStep[0m  [8/21], [94mLoss[0m : 1.79656
[1mStep[0m  [10/21], [94mLoss[0m : 1.82945
[1mStep[0m  [12/21], [94mLoss[0m : 1.86475
[1mStep[0m  [14/21], [94mLoss[0m : 1.64212
[1mStep[0m  [16/21], [94mLoss[0m : 1.70808
[1mStep[0m  [18/21], [94mLoss[0m : 1.79386
[1mStep[0m  [20/21], [94mLoss[0m : 1.78406

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.792, [92mTest[0m: 2.493, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.73061
[1mStep[0m  [2/21], [94mLoss[0m : 1.80228
[1mStep[0m  [4/21], [94mLoss[0m : 1.72747
[1mStep[0m  [6/21], [94mLoss[0m : 1.69323
[1mStep[0m  [8/21], [94mLoss[0m : 1.75531
[1mStep[0m  [10/21], [94mLoss[0m : 1.71286
[1mStep[0m  [12/21], [94mLoss[0m : 1.79777
[1mStep[0m  [14/21], [94mLoss[0m : 1.81543
[1mStep[0m  [16/21], [94mLoss[0m : 1.67254
[1mStep[0m  [18/21], [94mLoss[0m : 1.72023
[1mStep[0m  [20/21], [94mLoss[0m : 1.86765

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.761, [92mTest[0m: 2.503, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.71137
[1mStep[0m  [2/21], [94mLoss[0m : 1.65985
[1mStep[0m  [4/21], [94mLoss[0m : 1.69923
[1mStep[0m  [6/21], [94mLoss[0m : 1.75440
[1mStep[0m  [8/21], [94mLoss[0m : 1.76630
[1mStep[0m  [10/21], [94mLoss[0m : 1.88432
[1mStep[0m  [12/21], [94mLoss[0m : 1.71786
[1mStep[0m  [14/21], [94mLoss[0m : 1.73038
[1mStep[0m  [16/21], [94mLoss[0m : 1.77746
[1mStep[0m  [18/21], [94mLoss[0m : 1.71836
[1mStep[0m  [20/21], [94mLoss[0m : 1.68999

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.728, [92mTest[0m: 2.498, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.67079
[1mStep[0m  [2/21], [94mLoss[0m : 1.52262
[1mStep[0m  [4/21], [94mLoss[0m : 1.70297
[1mStep[0m  [6/21], [94mLoss[0m : 1.69702
[1mStep[0m  [8/21], [94mLoss[0m : 1.75049
[1mStep[0m  [10/21], [94mLoss[0m : 1.71902
[1mStep[0m  [12/21], [94mLoss[0m : 1.72731
[1mStep[0m  [14/21], [94mLoss[0m : 1.68589
[1mStep[0m  [16/21], [94mLoss[0m : 1.64736
[1mStep[0m  [18/21], [94mLoss[0m : 1.67513
[1mStep[0m  [20/21], [94mLoss[0m : 1.69245

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.678, [92mTest[0m: 2.471, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.57146
[1mStep[0m  [2/21], [94mLoss[0m : 1.62020
[1mStep[0m  [4/21], [94mLoss[0m : 1.79818
[1mStep[0m  [6/21], [94mLoss[0m : 1.65761
[1mStep[0m  [8/21], [94mLoss[0m : 1.64757
[1mStep[0m  [10/21], [94mLoss[0m : 1.66440
[1mStep[0m  [12/21], [94mLoss[0m : 1.70695
[1mStep[0m  [14/21], [94mLoss[0m : 1.58555
[1mStep[0m  [16/21], [94mLoss[0m : 1.61234
[1mStep[0m  [18/21], [94mLoss[0m : 1.60598
[1mStep[0m  [20/21], [94mLoss[0m : 1.67650

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.669, [92mTest[0m: 2.577, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.61603
[1mStep[0m  [2/21], [94mLoss[0m : 1.64816
[1mStep[0m  [4/21], [94mLoss[0m : 1.62899
[1mStep[0m  [6/21], [94mLoss[0m : 1.61474
[1mStep[0m  [8/21], [94mLoss[0m : 1.69282
[1mStep[0m  [10/21], [94mLoss[0m : 1.61939
[1mStep[0m  [12/21], [94mLoss[0m : 1.51249
[1mStep[0m  [14/21], [94mLoss[0m : 1.60226
[1mStep[0m  [16/21], [94mLoss[0m : 1.58226
[1mStep[0m  [18/21], [94mLoss[0m : 1.64537
[1mStep[0m  [20/21], [94mLoss[0m : 1.57164

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.630, [92mTest[0m: 2.507, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.71413
[1mStep[0m  [2/21], [94mLoss[0m : 1.55071
[1mStep[0m  [4/21], [94mLoss[0m : 1.52594
[1mStep[0m  [6/21], [94mLoss[0m : 1.65510
[1mStep[0m  [8/21], [94mLoss[0m : 1.55628
[1mStep[0m  [10/21], [94mLoss[0m : 1.65339
[1mStep[0m  [12/21], [94mLoss[0m : 1.61398
[1mStep[0m  [14/21], [94mLoss[0m : 1.56674
[1mStep[0m  [16/21], [94mLoss[0m : 1.68222
[1mStep[0m  [18/21], [94mLoss[0m : 1.76373
[1mStep[0m  [20/21], [94mLoss[0m : 1.57490

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.604, [92mTest[0m: 2.755, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.59034
[1mStep[0m  [2/21], [94mLoss[0m : 1.64032
[1mStep[0m  [4/21], [94mLoss[0m : 1.54106
[1mStep[0m  [6/21], [94mLoss[0m : 1.56780
[1mStep[0m  [8/21], [94mLoss[0m : 1.47084
[1mStep[0m  [10/21], [94mLoss[0m : 1.57448
[1mStep[0m  [12/21], [94mLoss[0m : 1.41759
[1mStep[0m  [14/21], [94mLoss[0m : 1.53014
[1mStep[0m  [16/21], [94mLoss[0m : 1.46670
[1mStep[0m  [18/21], [94mLoss[0m : 1.54957
[1mStep[0m  [20/21], [94mLoss[0m : 1.66556

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.567, [92mTest[0m: 2.521, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.53102
[1mStep[0m  [2/21], [94mLoss[0m : 1.59841
[1mStep[0m  [4/21], [94mLoss[0m : 1.40779
[1mStep[0m  [6/21], [94mLoss[0m : 1.54477
[1mStep[0m  [8/21], [94mLoss[0m : 1.47464
[1mStep[0m  [10/21], [94mLoss[0m : 1.55246
[1mStep[0m  [12/21], [94mLoss[0m : 1.63254
[1mStep[0m  [14/21], [94mLoss[0m : 1.55429
[1mStep[0m  [16/21], [94mLoss[0m : 1.57039
[1mStep[0m  [18/21], [94mLoss[0m : 1.54968
[1mStep[0m  [20/21], [94mLoss[0m : 1.47864

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.545, [92mTest[0m: 2.503, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.46276
[1mStep[0m  [2/21], [94mLoss[0m : 1.47618
[1mStep[0m  [4/21], [94mLoss[0m : 1.50770
[1mStep[0m  [6/21], [94mLoss[0m : 1.50638
[1mStep[0m  [8/21], [94mLoss[0m : 1.45275
[1mStep[0m  [10/21], [94mLoss[0m : 1.51057
[1mStep[0m  [12/21], [94mLoss[0m : 1.62220
[1mStep[0m  [14/21], [94mLoss[0m : 1.55125
[1mStep[0m  [16/21], [94mLoss[0m : 1.55197
[1mStep[0m  [18/21], [94mLoss[0m : 1.55993
[1mStep[0m  [20/21], [94mLoss[0m : 1.60816

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.531, [92mTest[0m: 2.544, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.46561
[1mStep[0m  [2/21], [94mLoss[0m : 1.45055
[1mStep[0m  [4/21], [94mLoss[0m : 1.49139
[1mStep[0m  [6/21], [94mLoss[0m : 1.53471
[1mStep[0m  [8/21], [94mLoss[0m : 1.42697
[1mStep[0m  [10/21], [94mLoss[0m : 1.47974
[1mStep[0m  [12/21], [94mLoss[0m : 1.50039
[1mStep[0m  [14/21], [94mLoss[0m : 1.34913
[1mStep[0m  [16/21], [94mLoss[0m : 1.64736
[1mStep[0m  [18/21], [94mLoss[0m : 1.48475
[1mStep[0m  [20/21], [94mLoss[0m : 1.56291

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.491, [92mTest[0m: 2.562, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.55871
[1mStep[0m  [2/21], [94mLoss[0m : 1.51387
[1mStep[0m  [4/21], [94mLoss[0m : 1.54885
[1mStep[0m  [6/21], [94mLoss[0m : 1.56396
[1mStep[0m  [8/21], [94mLoss[0m : 1.45194
[1mStep[0m  [10/21], [94mLoss[0m : 1.42378
[1mStep[0m  [12/21], [94mLoss[0m : 1.54523
[1mStep[0m  [14/21], [94mLoss[0m : 1.47531
[1mStep[0m  [16/21], [94mLoss[0m : 1.42109
[1mStep[0m  [18/21], [94mLoss[0m : 1.51816
[1mStep[0m  [20/21], [94mLoss[0m : 1.46905

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.501, [92mTest[0m: 2.518, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.61463
[1mStep[0m  [2/21], [94mLoss[0m : 1.47191
[1mStep[0m  [4/21], [94mLoss[0m : 1.45866
[1mStep[0m  [6/21], [94mLoss[0m : 1.50193
[1mStep[0m  [8/21], [94mLoss[0m : 1.52087
[1mStep[0m  [10/21], [94mLoss[0m : 1.45859
[1mStep[0m  [12/21], [94mLoss[0m : 1.45882
[1mStep[0m  [14/21], [94mLoss[0m : 1.31737
[1mStep[0m  [16/21], [94mLoss[0m : 1.51282
[1mStep[0m  [18/21], [94mLoss[0m : 1.50118
[1mStep[0m  [20/21], [94mLoss[0m : 1.46234

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.473, [92mTest[0m: 2.539, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.547
====================================

Phase 2 - Evaluation MAE:  2.546966927392142
MAE score P1      2.348204
MAE score P2      2.546967
loss              1.472674
learning_rate     0.002575
batch_size             512
hidden_sizes         [300]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.9
weight_decay         0.001
Name: 6, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 10.77903
[1mStep[0m  [4/42], [94mLoss[0m : 10.88415
[1mStep[0m  [8/42], [94mLoss[0m : 11.05861
[1mStep[0m  [12/42], [94mLoss[0m : 10.25653
[1mStep[0m  [16/42], [94mLoss[0m : 10.58731
[1mStep[0m  [20/42], [94mLoss[0m : 10.03889
[1mStep[0m  [24/42], [94mLoss[0m : 10.06914
[1mStep[0m  [28/42], [94mLoss[0m : 10.33127
[1mStep[0m  [32/42], [94mLoss[0m : 10.24322
[1mStep[0m  [36/42], [94mLoss[0m : 9.54595
[1mStep[0m  [40/42], [94mLoss[0m : 9.80961

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.282, [92mTest[0m: 10.837, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.72405
[1mStep[0m  [4/42], [94mLoss[0m : 9.30469
[1mStep[0m  [8/42], [94mLoss[0m : 9.63940
[1mStep[0m  [12/42], [94mLoss[0m : 9.45139
[1mStep[0m  [16/42], [94mLoss[0m : 8.93754
[1mStep[0m  [20/42], [94mLoss[0m : 9.21262
[1mStep[0m  [24/42], [94mLoss[0m : 9.54405
[1mStep[0m  [28/42], [94mLoss[0m : 8.75547
[1mStep[0m  [32/42], [94mLoss[0m : 8.79513
[1mStep[0m  [36/42], [94mLoss[0m : 8.72053
[1mStep[0m  [40/42], [94mLoss[0m : 8.35840

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.251, [92mTest[0m: 10.202, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.05495
[1mStep[0m  [4/42], [94mLoss[0m : 8.48066
[1mStep[0m  [8/42], [94mLoss[0m : 8.39819
[1mStep[0m  [12/42], [94mLoss[0m : 8.46478
[1mStep[0m  [16/42], [94mLoss[0m : 8.27093
[1mStep[0m  [20/42], [94mLoss[0m : 8.13443
[1mStep[0m  [24/42], [94mLoss[0m : 7.75206
[1mStep[0m  [28/42], [94mLoss[0m : 7.90256
[1mStep[0m  [32/42], [94mLoss[0m : 8.10212
[1mStep[0m  [36/42], [94mLoss[0m : 7.97999
[1mStep[0m  [40/42], [94mLoss[0m : 7.61383

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 8.176, [92mTest[0m: 9.508, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.24925
[1mStep[0m  [4/42], [94mLoss[0m : 7.16715
[1mStep[0m  [8/42], [94mLoss[0m : 7.38928
[1mStep[0m  [12/42], [94mLoss[0m : 7.42018
[1mStep[0m  [16/42], [94mLoss[0m : 7.34911
[1mStep[0m  [20/42], [94mLoss[0m : 7.24812
[1mStep[0m  [24/42], [94mLoss[0m : 7.00353
[1mStep[0m  [28/42], [94mLoss[0m : 6.79343
[1mStep[0m  [32/42], [94mLoss[0m : 6.77863
[1mStep[0m  [36/42], [94mLoss[0m : 6.71965
[1mStep[0m  [40/42], [94mLoss[0m : 6.78517

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 7.099, [92mTest[0m: 8.793, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.83242
[1mStep[0m  [4/42], [94mLoss[0m : 6.49026
[1mStep[0m  [8/42], [94mLoss[0m : 6.71693
[1mStep[0m  [12/42], [94mLoss[0m : 6.19447
[1mStep[0m  [16/42], [94mLoss[0m : 6.06421
[1mStep[0m  [20/42], [94mLoss[0m : 6.56630
[1mStep[0m  [24/42], [94mLoss[0m : 5.98494
[1mStep[0m  [28/42], [94mLoss[0m : 5.92306
[1mStep[0m  [32/42], [94mLoss[0m : 5.43126
[1mStep[0m  [36/42], [94mLoss[0m : 5.56948
[1mStep[0m  [40/42], [94mLoss[0m : 5.78293

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 6.038, [92mTest[0m: 8.017, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.60474
[1mStep[0m  [4/42], [94mLoss[0m : 5.29011
[1mStep[0m  [8/42], [94mLoss[0m : 5.15178
[1mStep[0m  [12/42], [94mLoss[0m : 5.10312
[1mStep[0m  [16/42], [94mLoss[0m : 4.89605
[1mStep[0m  [20/42], [94mLoss[0m : 5.12207
[1mStep[0m  [24/42], [94mLoss[0m : 4.89605
[1mStep[0m  [28/42], [94mLoss[0m : 4.95733
[1mStep[0m  [32/42], [94mLoss[0m : 4.79124
[1mStep[0m  [36/42], [94mLoss[0m : 4.91444
[1mStep[0m  [40/42], [94mLoss[0m : 4.61162

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 5.026, [92mTest[0m: 6.999, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.51357
[1mStep[0m  [4/42], [94mLoss[0m : 4.95851
[1mStep[0m  [8/42], [94mLoss[0m : 4.35600
[1mStep[0m  [12/42], [94mLoss[0m : 4.24854
[1mStep[0m  [16/42], [94mLoss[0m : 4.13412
[1mStep[0m  [20/42], [94mLoss[0m : 4.02800
[1mStep[0m  [24/42], [94mLoss[0m : 3.98081
[1mStep[0m  [28/42], [94mLoss[0m : 4.13302
[1mStep[0m  [32/42], [94mLoss[0m : 4.07463
[1mStep[0m  [36/42], [94mLoss[0m : 4.02637
[1mStep[0m  [40/42], [94mLoss[0m : 4.26528

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 4.222, [92mTest[0m: 5.891, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.00254
[1mStep[0m  [4/42], [94mLoss[0m : 3.50470
[1mStep[0m  [8/42], [94mLoss[0m : 3.39822
[1mStep[0m  [12/42], [94mLoss[0m : 3.67179
[1mStep[0m  [16/42], [94mLoss[0m : 3.56808
[1mStep[0m  [20/42], [94mLoss[0m : 3.72339
[1mStep[0m  [24/42], [94mLoss[0m : 3.66287
[1mStep[0m  [28/42], [94mLoss[0m : 3.68758
[1mStep[0m  [32/42], [94mLoss[0m : 3.40159
[1mStep[0m  [36/42], [94mLoss[0m : 3.54824
[1mStep[0m  [40/42], [94mLoss[0m : 3.40714

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 3.656, [92mTest[0m: 5.053, [96mlr[0m: 0.002575
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 7 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 4.400
====================================

Phase 1 - Evaluation MAE:  4.399741172790527
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 3.46939
[1mStep[0m  [4/42], [94mLoss[0m : 3.49475
[1mStep[0m  [8/42], [94mLoss[0m : 3.78920
[1mStep[0m  [12/42], [94mLoss[0m : 3.15427
[1mStep[0m  [16/42], [94mLoss[0m : 3.26872
[1mStep[0m  [20/42], [94mLoss[0m : 3.60865
[1mStep[0m  [24/42], [94mLoss[0m : 3.33388
[1mStep[0m  [28/42], [94mLoss[0m : 3.50045
[1mStep[0m  [32/42], [94mLoss[0m : 3.31758
[1mStep[0m  [36/42], [94mLoss[0m : 3.71188
[1mStep[0m  [40/42], [94mLoss[0m : 3.41410

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.403, [92mTest[0m: 4.395, [96mlr[0m: 0.002575
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 0 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 3.257
====================================

Phase 2 - Evaluation MAE:  3.256973913737706
MAE score P1       4.399741
MAE score P2       3.256974
loss               3.403304
learning_rate      0.002575
batch_size              256
hidden_sizes      [250, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping         True
dropout                 0.4
momentum                0.1
weight_decay           0.01
Name: 7, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 10.77465
[1mStep[0m  [8/84], [94mLoss[0m : 9.72440
[1mStep[0m  [16/84], [94mLoss[0m : 8.39225
[1mStep[0m  [24/84], [94mLoss[0m : 5.21088
[1mStep[0m  [32/84], [94mLoss[0m : 3.28015
[1mStep[0m  [40/84], [94mLoss[0m : 2.84742
[1mStep[0m  [48/84], [94mLoss[0m : 2.54026
[1mStep[0m  [56/84], [94mLoss[0m : 2.58454
[1mStep[0m  [64/84], [94mLoss[0m : 2.48016
[1mStep[0m  [72/84], [94mLoss[0m : 2.52127
[1mStep[0m  [80/84], [94mLoss[0m : 2.58169

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.571, [92mTest[0m: 10.735, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44392
[1mStep[0m  [8/84], [94mLoss[0m : 2.39457
[1mStep[0m  [16/84], [94mLoss[0m : 2.46100
[1mStep[0m  [24/84], [94mLoss[0m : 2.45724
[1mStep[0m  [32/84], [94mLoss[0m : 2.64047
[1mStep[0m  [40/84], [94mLoss[0m : 2.64787
[1mStep[0m  [48/84], [94mLoss[0m : 2.47405
[1mStep[0m  [56/84], [94mLoss[0m : 2.52672
[1mStep[0m  [64/84], [94mLoss[0m : 2.02904
[1mStep[0m  [72/84], [94mLoss[0m : 2.52171
[1mStep[0m  [80/84], [94mLoss[0m : 2.76874

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.401, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.29070
[1mStep[0m  [8/84], [94mLoss[0m : 2.51461
[1mStep[0m  [16/84], [94mLoss[0m : 2.63637
[1mStep[0m  [24/84], [94mLoss[0m : 2.48783
[1mStep[0m  [32/84], [94mLoss[0m : 2.77027
[1mStep[0m  [40/84], [94mLoss[0m : 2.74545
[1mStep[0m  [48/84], [94mLoss[0m : 2.56204
[1mStep[0m  [56/84], [94mLoss[0m : 2.12987
[1mStep[0m  [64/84], [94mLoss[0m : 2.33247
[1mStep[0m  [72/84], [94mLoss[0m : 2.39573
[1mStep[0m  [80/84], [94mLoss[0m : 2.70678

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.352, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47629
[1mStep[0m  [8/84], [94mLoss[0m : 2.59179
[1mStep[0m  [16/84], [94mLoss[0m : 2.25004
[1mStep[0m  [24/84], [94mLoss[0m : 2.46480
[1mStep[0m  [32/84], [94mLoss[0m : 2.26369
[1mStep[0m  [40/84], [94mLoss[0m : 2.51020
[1mStep[0m  [48/84], [94mLoss[0m : 2.21932
[1mStep[0m  [56/84], [94mLoss[0m : 2.44017
[1mStep[0m  [64/84], [94mLoss[0m : 2.68012
[1mStep[0m  [72/84], [94mLoss[0m : 2.29739
[1mStep[0m  [80/84], [94mLoss[0m : 2.67833

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.335, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.05149
[1mStep[0m  [8/84], [94mLoss[0m : 2.59757
[1mStep[0m  [16/84], [94mLoss[0m : 2.31462
[1mStep[0m  [24/84], [94mLoss[0m : 2.15763
[1mStep[0m  [32/84], [94mLoss[0m : 2.20235
[1mStep[0m  [40/84], [94mLoss[0m : 2.32108
[1mStep[0m  [48/84], [94mLoss[0m : 2.70054
[1mStep[0m  [56/84], [94mLoss[0m : 2.57347
[1mStep[0m  [64/84], [94mLoss[0m : 2.41248
[1mStep[0m  [72/84], [94mLoss[0m : 2.37148
[1mStep[0m  [80/84], [94mLoss[0m : 2.58694

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.329, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.64448
[1mStep[0m  [8/84], [94mLoss[0m : 2.50562
[1mStep[0m  [16/84], [94mLoss[0m : 2.28928
[1mStep[0m  [24/84], [94mLoss[0m : 2.25976
[1mStep[0m  [32/84], [94mLoss[0m : 2.85739
[1mStep[0m  [40/84], [94mLoss[0m : 2.37979
[1mStep[0m  [48/84], [94mLoss[0m : 2.54549
[1mStep[0m  [56/84], [94mLoss[0m : 2.29906
[1mStep[0m  [64/84], [94mLoss[0m : 2.18034
[1mStep[0m  [72/84], [94mLoss[0m : 2.50211
[1mStep[0m  [80/84], [94mLoss[0m : 2.76057

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.339, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44843
[1mStep[0m  [8/84], [94mLoss[0m : 2.67392
[1mStep[0m  [16/84], [94mLoss[0m : 2.63898
[1mStep[0m  [24/84], [94mLoss[0m : 2.42604
[1mStep[0m  [32/84], [94mLoss[0m : 2.49460
[1mStep[0m  [40/84], [94mLoss[0m : 2.40701
[1mStep[0m  [48/84], [94mLoss[0m : 2.63594
[1mStep[0m  [56/84], [94mLoss[0m : 2.61950
[1mStep[0m  [64/84], [94mLoss[0m : 2.47076
[1mStep[0m  [72/84], [94mLoss[0m : 2.41157
[1mStep[0m  [80/84], [94mLoss[0m : 2.57908

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.328, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.51058
[1mStep[0m  [8/84], [94mLoss[0m : 2.32376
[1mStep[0m  [16/84], [94mLoss[0m : 2.29134
[1mStep[0m  [24/84], [94mLoss[0m : 2.33859
[1mStep[0m  [32/84], [94mLoss[0m : 2.73930
[1mStep[0m  [40/84], [94mLoss[0m : 2.59525
[1mStep[0m  [48/84], [94mLoss[0m : 2.46113
[1mStep[0m  [56/84], [94mLoss[0m : 2.48859
[1mStep[0m  [64/84], [94mLoss[0m : 2.48029
[1mStep[0m  [72/84], [94mLoss[0m : 2.42475
[1mStep[0m  [80/84], [94mLoss[0m : 2.34636

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.334, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50818
[1mStep[0m  [8/84], [94mLoss[0m : 2.68841
[1mStep[0m  [16/84], [94mLoss[0m : 2.42111
[1mStep[0m  [24/84], [94mLoss[0m : 2.15727
[1mStep[0m  [32/84], [94mLoss[0m : 2.45922
[1mStep[0m  [40/84], [94mLoss[0m : 2.58734
[1mStep[0m  [48/84], [94mLoss[0m : 2.51161
[1mStep[0m  [56/84], [94mLoss[0m : 2.44458
[1mStep[0m  [64/84], [94mLoss[0m : 2.42955
[1mStep[0m  [72/84], [94mLoss[0m : 2.46475
[1mStep[0m  [80/84], [94mLoss[0m : 2.43701

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.331, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52090
[1mStep[0m  [8/84], [94mLoss[0m : 2.21369
[1mStep[0m  [16/84], [94mLoss[0m : 2.33799
[1mStep[0m  [24/84], [94mLoss[0m : 2.26955
[1mStep[0m  [32/84], [94mLoss[0m : 2.29702
[1mStep[0m  [40/84], [94mLoss[0m : 2.53139
[1mStep[0m  [48/84], [94mLoss[0m : 2.39208
[1mStep[0m  [56/84], [94mLoss[0m : 2.27996
[1mStep[0m  [64/84], [94mLoss[0m : 2.82915
[1mStep[0m  [72/84], [94mLoss[0m : 2.47517
[1mStep[0m  [80/84], [94mLoss[0m : 2.30654

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.337, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.69999
[1mStep[0m  [8/84], [94mLoss[0m : 2.52101
[1mStep[0m  [16/84], [94mLoss[0m : 2.53656
[1mStep[0m  [24/84], [94mLoss[0m : 2.58103
[1mStep[0m  [32/84], [94mLoss[0m : 2.36375
[1mStep[0m  [40/84], [94mLoss[0m : 2.58138
[1mStep[0m  [48/84], [94mLoss[0m : 2.59143
[1mStep[0m  [56/84], [94mLoss[0m : 2.68231
[1mStep[0m  [64/84], [94mLoss[0m : 2.62171
[1mStep[0m  [72/84], [94mLoss[0m : 2.21798
[1mStep[0m  [80/84], [94mLoss[0m : 2.53493

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.335, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46005
[1mStep[0m  [8/84], [94mLoss[0m : 2.46016
[1mStep[0m  [16/84], [94mLoss[0m : 2.57334
[1mStep[0m  [24/84], [94mLoss[0m : 2.44681
[1mStep[0m  [32/84], [94mLoss[0m : 2.57997
[1mStep[0m  [40/84], [94mLoss[0m : 2.54209
[1mStep[0m  [48/84], [94mLoss[0m : 2.58641
[1mStep[0m  [56/84], [94mLoss[0m : 2.45633
[1mStep[0m  [64/84], [94mLoss[0m : 2.44804
[1mStep[0m  [72/84], [94mLoss[0m : 2.50937
[1mStep[0m  [80/84], [94mLoss[0m : 2.50921

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.330, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.34524
[1mStep[0m  [8/84], [94mLoss[0m : 2.41133
[1mStep[0m  [16/84], [94mLoss[0m : 2.54562
[1mStep[0m  [24/84], [94mLoss[0m : 2.50345
[1mStep[0m  [32/84], [94mLoss[0m : 2.71833
[1mStep[0m  [40/84], [94mLoss[0m : 2.42261
[1mStep[0m  [48/84], [94mLoss[0m : 2.37919
[1mStep[0m  [56/84], [94mLoss[0m : 2.36931
[1mStep[0m  [64/84], [94mLoss[0m : 2.54792
[1mStep[0m  [72/84], [94mLoss[0m : 2.35646
[1mStep[0m  [80/84], [94mLoss[0m : 2.31021

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.326, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.71386
[1mStep[0m  [8/84], [94mLoss[0m : 2.43785
[1mStep[0m  [16/84], [94mLoss[0m : 2.52801
[1mStep[0m  [24/84], [94mLoss[0m : 2.15322
[1mStep[0m  [32/84], [94mLoss[0m : 2.42534
[1mStep[0m  [40/84], [94mLoss[0m : 2.36595
[1mStep[0m  [48/84], [94mLoss[0m : 2.19039
[1mStep[0m  [56/84], [94mLoss[0m : 2.35416
[1mStep[0m  [64/84], [94mLoss[0m : 2.31728
[1mStep[0m  [72/84], [94mLoss[0m : 2.77719
[1mStep[0m  [80/84], [94mLoss[0m : 2.37803

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.324, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.70355
[1mStep[0m  [8/84], [94mLoss[0m : 2.20190
[1mStep[0m  [16/84], [94mLoss[0m : 2.52989
[1mStep[0m  [24/84], [94mLoss[0m : 2.57314
[1mStep[0m  [32/84], [94mLoss[0m : 2.59101
[1mStep[0m  [40/84], [94mLoss[0m : 2.37532
[1mStep[0m  [48/84], [94mLoss[0m : 2.46259
[1mStep[0m  [56/84], [94mLoss[0m : 2.50675
[1mStep[0m  [64/84], [94mLoss[0m : 2.47346
[1mStep[0m  [72/84], [94mLoss[0m : 2.21608
[1mStep[0m  [80/84], [94mLoss[0m : 2.50589

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.332, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44386
[1mStep[0m  [8/84], [94mLoss[0m : 2.24486
[1mStep[0m  [16/84], [94mLoss[0m : 2.43516
[1mStep[0m  [24/84], [94mLoss[0m : 2.46925
[1mStep[0m  [32/84], [94mLoss[0m : 2.61551
[1mStep[0m  [40/84], [94mLoss[0m : 2.50375
[1mStep[0m  [48/84], [94mLoss[0m : 2.50233
[1mStep[0m  [56/84], [94mLoss[0m : 2.51725
[1mStep[0m  [64/84], [94mLoss[0m : 2.35783
[1mStep[0m  [72/84], [94mLoss[0m : 2.20051
[1mStep[0m  [80/84], [94mLoss[0m : 2.20141

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.331, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40656
[1mStep[0m  [8/84], [94mLoss[0m : 2.38747
[1mStep[0m  [16/84], [94mLoss[0m : 2.24720
[1mStep[0m  [24/84], [94mLoss[0m : 2.37358
[1mStep[0m  [32/84], [94mLoss[0m : 2.44828
[1mStep[0m  [40/84], [94mLoss[0m : 2.38779
[1mStep[0m  [48/84], [94mLoss[0m : 2.16022
[1mStep[0m  [56/84], [94mLoss[0m : 2.46318
[1mStep[0m  [64/84], [94mLoss[0m : 2.33843
[1mStep[0m  [72/84], [94mLoss[0m : 2.29470
[1mStep[0m  [80/84], [94mLoss[0m : 2.48750

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.332, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.07081
[1mStep[0m  [8/84], [94mLoss[0m : 2.41951
[1mStep[0m  [16/84], [94mLoss[0m : 2.38944
[1mStep[0m  [24/84], [94mLoss[0m : 2.88177
[1mStep[0m  [32/84], [94mLoss[0m : 2.36815
[1mStep[0m  [40/84], [94mLoss[0m : 2.15800
[1mStep[0m  [48/84], [94mLoss[0m : 2.65809
[1mStep[0m  [56/84], [94mLoss[0m : 2.41227
[1mStep[0m  [64/84], [94mLoss[0m : 2.33893
[1mStep[0m  [72/84], [94mLoss[0m : 2.59034
[1mStep[0m  [80/84], [94mLoss[0m : 2.21259

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.326, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.63256
[1mStep[0m  [8/84], [94mLoss[0m : 2.78965
[1mStep[0m  [16/84], [94mLoss[0m : 2.40474
[1mStep[0m  [24/84], [94mLoss[0m : 2.48943
[1mStep[0m  [32/84], [94mLoss[0m : 2.47533
[1mStep[0m  [40/84], [94mLoss[0m : 2.54286
[1mStep[0m  [48/84], [94mLoss[0m : 2.48395
[1mStep[0m  [56/84], [94mLoss[0m : 2.54563
[1mStep[0m  [64/84], [94mLoss[0m : 2.43278
[1mStep[0m  [72/84], [94mLoss[0m : 2.39684
[1mStep[0m  [80/84], [94mLoss[0m : 2.49391

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.324, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43008
[1mStep[0m  [8/84], [94mLoss[0m : 2.59541
[1mStep[0m  [16/84], [94mLoss[0m : 2.46313
[1mStep[0m  [24/84], [94mLoss[0m : 2.39407
[1mStep[0m  [32/84], [94mLoss[0m : 2.71935
[1mStep[0m  [40/84], [94mLoss[0m : 2.55297
[1mStep[0m  [48/84], [94mLoss[0m : 2.43359
[1mStep[0m  [56/84], [94mLoss[0m : 2.48490
[1mStep[0m  [64/84], [94mLoss[0m : 2.28722
[1mStep[0m  [72/84], [94mLoss[0m : 2.47532
[1mStep[0m  [80/84], [94mLoss[0m : 2.39126

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.331, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26681
[1mStep[0m  [8/84], [94mLoss[0m : 2.44186
[1mStep[0m  [16/84], [94mLoss[0m : 2.19222
[1mStep[0m  [24/84], [94mLoss[0m : 2.58954
[1mStep[0m  [32/84], [94mLoss[0m : 2.63119
[1mStep[0m  [40/84], [94mLoss[0m : 2.61729
[1mStep[0m  [48/84], [94mLoss[0m : 2.44793
[1mStep[0m  [56/84], [94mLoss[0m : 2.48205
[1mStep[0m  [64/84], [94mLoss[0m : 2.78729
[1mStep[0m  [72/84], [94mLoss[0m : 2.36377
[1mStep[0m  [80/84], [94mLoss[0m : 2.12922

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.329, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.57784
[1mStep[0m  [8/84], [94mLoss[0m : 2.34741
[1mStep[0m  [16/84], [94mLoss[0m : 2.36140
[1mStep[0m  [24/84], [94mLoss[0m : 2.61489
[1mStep[0m  [32/84], [94mLoss[0m : 2.67230
[1mStep[0m  [40/84], [94mLoss[0m : 2.41467
[1mStep[0m  [48/84], [94mLoss[0m : 2.70984
[1mStep[0m  [56/84], [94mLoss[0m : 2.58345
[1mStep[0m  [64/84], [94mLoss[0m : 2.34469
[1mStep[0m  [72/84], [94mLoss[0m : 2.57001
[1mStep[0m  [80/84], [94mLoss[0m : 2.59826

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.332, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.51248
[1mStep[0m  [8/84], [94mLoss[0m : 2.70967
[1mStep[0m  [16/84], [94mLoss[0m : 2.61188
[1mStep[0m  [24/84], [94mLoss[0m : 2.12331
[1mStep[0m  [32/84], [94mLoss[0m : 2.23571
[1mStep[0m  [40/84], [94mLoss[0m : 2.55341
[1mStep[0m  [48/84], [94mLoss[0m : 2.48403
[1mStep[0m  [56/84], [94mLoss[0m : 2.61220
[1mStep[0m  [64/84], [94mLoss[0m : 2.43062
[1mStep[0m  [72/84], [94mLoss[0m : 2.32470
[1mStep[0m  [80/84], [94mLoss[0m : 2.34382

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.330, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.32431
[1mStep[0m  [8/84], [94mLoss[0m : 2.35583
[1mStep[0m  [16/84], [94mLoss[0m : 2.41267
[1mStep[0m  [24/84], [94mLoss[0m : 2.41401
[1mStep[0m  [32/84], [94mLoss[0m : 2.35335
[1mStep[0m  [40/84], [94mLoss[0m : 2.54078
[1mStep[0m  [48/84], [94mLoss[0m : 2.41153
[1mStep[0m  [56/84], [94mLoss[0m : 2.56474
[1mStep[0m  [64/84], [94mLoss[0m : 2.64712
[1mStep[0m  [72/84], [94mLoss[0m : 2.52815
[1mStep[0m  [80/84], [94mLoss[0m : 2.32631

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.327, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50494
[1mStep[0m  [8/84], [94mLoss[0m : 2.39800
[1mStep[0m  [16/84], [94mLoss[0m : 2.38692
[1mStep[0m  [24/84], [94mLoss[0m : 2.52046
[1mStep[0m  [32/84], [94mLoss[0m : 2.27833
[1mStep[0m  [40/84], [94mLoss[0m : 2.51127
[1mStep[0m  [48/84], [94mLoss[0m : 2.60737
[1mStep[0m  [56/84], [94mLoss[0m : 2.46096
[1mStep[0m  [64/84], [94mLoss[0m : 2.33088
[1mStep[0m  [72/84], [94mLoss[0m : 2.61962
[1mStep[0m  [80/84], [94mLoss[0m : 2.53240

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.331, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33818
[1mStep[0m  [8/84], [94mLoss[0m : 2.56088
[1mStep[0m  [16/84], [94mLoss[0m : 2.51356
[1mStep[0m  [24/84], [94mLoss[0m : 2.11405
[1mStep[0m  [32/84], [94mLoss[0m : 2.37047
[1mStep[0m  [40/84], [94mLoss[0m : 2.24013
[1mStep[0m  [48/84], [94mLoss[0m : 2.48230
[1mStep[0m  [56/84], [94mLoss[0m : 2.28089
[1mStep[0m  [64/84], [94mLoss[0m : 2.70126
[1mStep[0m  [72/84], [94mLoss[0m : 2.30535
[1mStep[0m  [80/84], [94mLoss[0m : 2.60956

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.312, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.23696
[1mStep[0m  [8/84], [94mLoss[0m : 2.68729
[1mStep[0m  [16/84], [94mLoss[0m : 2.27109
[1mStep[0m  [24/84], [94mLoss[0m : 2.34783
[1mStep[0m  [32/84], [94mLoss[0m : 2.28614
[1mStep[0m  [40/84], [94mLoss[0m : 2.60988
[1mStep[0m  [48/84], [94mLoss[0m : 2.44763
[1mStep[0m  [56/84], [94mLoss[0m : 2.67545
[1mStep[0m  [64/84], [94mLoss[0m : 2.27924
[1mStep[0m  [72/84], [94mLoss[0m : 2.37303
[1mStep[0m  [80/84], [94mLoss[0m : 2.51990

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.326, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55138
[1mStep[0m  [8/84], [94mLoss[0m : 2.46600
[1mStep[0m  [16/84], [94mLoss[0m : 2.42896
[1mStep[0m  [24/84], [94mLoss[0m : 2.10894
[1mStep[0m  [32/84], [94mLoss[0m : 2.42195
[1mStep[0m  [40/84], [94mLoss[0m : 2.39711
[1mStep[0m  [48/84], [94mLoss[0m : 2.48181
[1mStep[0m  [56/84], [94mLoss[0m : 2.47696
[1mStep[0m  [64/84], [94mLoss[0m : 2.22190
[1mStep[0m  [72/84], [94mLoss[0m : 2.53268
[1mStep[0m  [80/84], [94mLoss[0m : 2.44960

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.332, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41274
[1mStep[0m  [8/84], [94mLoss[0m : 2.20761
[1mStep[0m  [16/84], [94mLoss[0m : 2.79935
[1mStep[0m  [24/84], [94mLoss[0m : 2.52411
[1mStep[0m  [32/84], [94mLoss[0m : 2.37499
[1mStep[0m  [40/84], [94mLoss[0m : 2.53847
[1mStep[0m  [48/84], [94mLoss[0m : 2.38645
[1mStep[0m  [56/84], [94mLoss[0m : 2.57766
[1mStep[0m  [64/84], [94mLoss[0m : 2.06464
[1mStep[0m  [72/84], [94mLoss[0m : 2.43538
[1mStep[0m  [80/84], [94mLoss[0m : 2.56395

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.330, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.73305
[1mStep[0m  [8/84], [94mLoss[0m : 2.46403
[1mStep[0m  [16/84], [94mLoss[0m : 2.52661
[1mStep[0m  [24/84], [94mLoss[0m : 2.36273
[1mStep[0m  [32/84], [94mLoss[0m : 2.39436
[1mStep[0m  [40/84], [94mLoss[0m : 2.35730
[1mStep[0m  [48/84], [94mLoss[0m : 2.32626
[1mStep[0m  [56/84], [94mLoss[0m : 2.34877
[1mStep[0m  [64/84], [94mLoss[0m : 2.75782
[1mStep[0m  [72/84], [94mLoss[0m : 2.36533
[1mStep[0m  [80/84], [94mLoss[0m : 2.29264

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.337, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.326
====================================

Phase 1 - Evaluation MAE:  2.3264488662992204
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 2.42992
[1mStep[0m  [8/84], [94mLoss[0m : 2.45436
[1mStep[0m  [16/84], [94mLoss[0m : 2.34908
[1mStep[0m  [24/84], [94mLoss[0m : 2.42392
[1mStep[0m  [32/84], [94mLoss[0m : 2.62164
[1mStep[0m  [40/84], [94mLoss[0m : 2.66889
[1mStep[0m  [48/84], [94mLoss[0m : 2.53303
[1mStep[0m  [56/84], [94mLoss[0m : 2.44103
[1mStep[0m  [64/84], [94mLoss[0m : 2.48629
[1mStep[0m  [72/84], [94mLoss[0m : 2.56571
[1mStep[0m  [80/84], [94mLoss[0m : 2.56922

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.322, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.10112
[1mStep[0m  [8/84], [94mLoss[0m : 2.36021
[1mStep[0m  [16/84], [94mLoss[0m : 2.26941
[1mStep[0m  [24/84], [94mLoss[0m : 2.23222
[1mStep[0m  [32/84], [94mLoss[0m : 2.44263
[1mStep[0m  [40/84], [94mLoss[0m : 2.60836
[1mStep[0m  [48/84], [94mLoss[0m : 2.34889
[1mStep[0m  [56/84], [94mLoss[0m : 2.30004
[1mStep[0m  [64/84], [94mLoss[0m : 2.24169
[1mStep[0m  [72/84], [94mLoss[0m : 2.25517
[1mStep[0m  [80/84], [94mLoss[0m : 2.68356

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.394, [92mTest[0m: 2.330, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.21573
[1mStep[0m  [8/84], [94mLoss[0m : 2.62013
[1mStep[0m  [16/84], [94mLoss[0m : 2.51170
[1mStep[0m  [24/84], [94mLoss[0m : 2.44832
[1mStep[0m  [32/84], [94mLoss[0m : 2.37743
[1mStep[0m  [40/84], [94mLoss[0m : 2.37584
[1mStep[0m  [48/84], [94mLoss[0m : 2.27499
[1mStep[0m  [56/84], [94mLoss[0m : 2.15133
[1mStep[0m  [64/84], [94mLoss[0m : 2.28652
[1mStep[0m  [72/84], [94mLoss[0m : 2.34780
[1mStep[0m  [80/84], [94mLoss[0m : 2.54232

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.324, [92mTest[0m: 2.338, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.02012
[1mStep[0m  [8/84], [94mLoss[0m : 1.93753
[1mStep[0m  [16/84], [94mLoss[0m : 2.52284
[1mStep[0m  [24/84], [94mLoss[0m : 2.40408
[1mStep[0m  [32/84], [94mLoss[0m : 2.34211
[1mStep[0m  [40/84], [94mLoss[0m : 2.07072
[1mStep[0m  [48/84], [94mLoss[0m : 2.29978
[1mStep[0m  [56/84], [94mLoss[0m : 2.20974
[1mStep[0m  [64/84], [94mLoss[0m : 2.35797
[1mStep[0m  [72/84], [94mLoss[0m : 2.07973
[1mStep[0m  [80/84], [94mLoss[0m : 2.36217

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.267, [92mTest[0m: 2.331, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.16258
[1mStep[0m  [8/84], [94mLoss[0m : 2.28874
[1mStep[0m  [16/84], [94mLoss[0m : 1.95739
[1mStep[0m  [24/84], [94mLoss[0m : 2.34302
[1mStep[0m  [32/84], [94mLoss[0m : 2.33103
[1mStep[0m  [40/84], [94mLoss[0m : 2.20843
[1mStep[0m  [48/84], [94mLoss[0m : 2.26745
[1mStep[0m  [56/84], [94mLoss[0m : 2.02820
[1mStep[0m  [64/84], [94mLoss[0m : 2.21535
[1mStep[0m  [72/84], [94mLoss[0m : 2.07576
[1mStep[0m  [80/84], [94mLoss[0m : 2.12445

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.194, [92mTest[0m: 2.347, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.09596
[1mStep[0m  [8/84], [94mLoss[0m : 1.84825
[1mStep[0m  [16/84], [94mLoss[0m : 1.91266
[1mStep[0m  [24/84], [94mLoss[0m : 2.15011
[1mStep[0m  [32/84], [94mLoss[0m : 2.19553
[1mStep[0m  [40/84], [94mLoss[0m : 2.27269
[1mStep[0m  [48/84], [94mLoss[0m : 2.14965
[1mStep[0m  [56/84], [94mLoss[0m : 2.42511
[1mStep[0m  [64/84], [94mLoss[0m : 2.19272
[1mStep[0m  [72/84], [94mLoss[0m : 2.34744
[1mStep[0m  [80/84], [94mLoss[0m : 2.33172

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.140, [92mTest[0m: 2.365, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.97682
[1mStep[0m  [8/84], [94mLoss[0m : 2.14608
[1mStep[0m  [16/84], [94mLoss[0m : 2.18579
[1mStep[0m  [24/84], [94mLoss[0m : 1.92097
[1mStep[0m  [32/84], [94mLoss[0m : 2.07761
[1mStep[0m  [40/84], [94mLoss[0m : 2.34786
[1mStep[0m  [48/84], [94mLoss[0m : 2.06334
[1mStep[0m  [56/84], [94mLoss[0m : 2.04607
[1mStep[0m  [64/84], [94mLoss[0m : 2.54621
[1mStep[0m  [72/84], [94mLoss[0m : 2.01767
[1mStep[0m  [80/84], [94mLoss[0m : 2.09224

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.096, [92mTest[0m: 2.372, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.82037
[1mStep[0m  [8/84], [94mLoss[0m : 1.89935
[1mStep[0m  [16/84], [94mLoss[0m : 2.05712
[1mStep[0m  [24/84], [94mLoss[0m : 1.93662
[1mStep[0m  [32/84], [94mLoss[0m : 2.31979
[1mStep[0m  [40/84], [94mLoss[0m : 2.23215
[1mStep[0m  [48/84], [94mLoss[0m : 2.02756
[1mStep[0m  [56/84], [94mLoss[0m : 1.98602
[1mStep[0m  [64/84], [94mLoss[0m : 2.13401
[1mStep[0m  [72/84], [94mLoss[0m : 1.98525
[1mStep[0m  [80/84], [94mLoss[0m : 2.07746

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.044, [92mTest[0m: 2.391, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.97111
[1mStep[0m  [8/84], [94mLoss[0m : 1.87624
[1mStep[0m  [16/84], [94mLoss[0m : 1.98324
[1mStep[0m  [24/84], [94mLoss[0m : 1.80881
[1mStep[0m  [32/84], [94mLoss[0m : 1.91280
[1mStep[0m  [40/84], [94mLoss[0m : 2.24029
[1mStep[0m  [48/84], [94mLoss[0m : 2.07750
[1mStep[0m  [56/84], [94mLoss[0m : 2.09728
[1mStep[0m  [64/84], [94mLoss[0m : 1.95653
[1mStep[0m  [72/84], [94mLoss[0m : 2.22192
[1mStep[0m  [80/84], [94mLoss[0m : 2.21450

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.008, [92mTest[0m: 2.462, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.00158
[1mStep[0m  [8/84], [94mLoss[0m : 2.03512
[1mStep[0m  [16/84], [94mLoss[0m : 2.03326
[1mStep[0m  [24/84], [94mLoss[0m : 2.04243
[1mStep[0m  [32/84], [94mLoss[0m : 1.70566
[1mStep[0m  [40/84], [94mLoss[0m : 2.08740
[1mStep[0m  [48/84], [94mLoss[0m : 1.85388
[1mStep[0m  [56/84], [94mLoss[0m : 2.27984
[1mStep[0m  [64/84], [94mLoss[0m : 1.90120
[1mStep[0m  [72/84], [94mLoss[0m : 2.02853
[1mStep[0m  [80/84], [94mLoss[0m : 2.05204

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.972, [92mTest[0m: 2.525, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.81638
[1mStep[0m  [8/84], [94mLoss[0m : 1.94517
[1mStep[0m  [16/84], [94mLoss[0m : 1.76127
[1mStep[0m  [24/84], [94mLoss[0m : 1.83803
[1mStep[0m  [32/84], [94mLoss[0m : 1.71514
[1mStep[0m  [40/84], [94mLoss[0m : 1.93735
[1mStep[0m  [48/84], [94mLoss[0m : 1.65007
[1mStep[0m  [56/84], [94mLoss[0m : 2.13597
[1mStep[0m  [64/84], [94mLoss[0m : 2.09645
[1mStep[0m  [72/84], [94mLoss[0m : 1.87963
[1mStep[0m  [80/84], [94mLoss[0m : 1.92391

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.936, [92mTest[0m: 2.436, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.91548
[1mStep[0m  [8/84], [94mLoss[0m : 1.75885
[1mStep[0m  [16/84], [94mLoss[0m : 1.78996
[1mStep[0m  [24/84], [94mLoss[0m : 1.91750
[1mStep[0m  [32/84], [94mLoss[0m : 1.91427
[1mStep[0m  [40/84], [94mLoss[0m : 1.89483
[1mStep[0m  [48/84], [94mLoss[0m : 1.82083
[1mStep[0m  [56/84], [94mLoss[0m : 1.83771
[1mStep[0m  [64/84], [94mLoss[0m : 1.86069
[1mStep[0m  [72/84], [94mLoss[0m : 1.99321
[1mStep[0m  [80/84], [94mLoss[0m : 1.81920

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.886, [92mTest[0m: 2.393, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.81002
[1mStep[0m  [8/84], [94mLoss[0m : 1.65921
[1mStep[0m  [16/84], [94mLoss[0m : 1.84424
[1mStep[0m  [24/84], [94mLoss[0m : 1.59012
[1mStep[0m  [32/84], [94mLoss[0m : 1.64469
[1mStep[0m  [40/84], [94mLoss[0m : 1.95886
[1mStep[0m  [48/84], [94mLoss[0m : 1.94008
[1mStep[0m  [56/84], [94mLoss[0m : 1.95377
[1mStep[0m  [64/84], [94mLoss[0m : 1.89879
[1mStep[0m  [72/84], [94mLoss[0m : 1.89656
[1mStep[0m  [80/84], [94mLoss[0m : 1.81162

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.882, [92mTest[0m: 2.491, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.70945
[1mStep[0m  [8/84], [94mLoss[0m : 1.81741
[1mStep[0m  [16/84], [94mLoss[0m : 1.72310
[1mStep[0m  [24/84], [94mLoss[0m : 1.88272
[1mStep[0m  [32/84], [94mLoss[0m : 2.15452
[1mStep[0m  [40/84], [94mLoss[0m : 1.94222
[1mStep[0m  [48/84], [94mLoss[0m : 1.87400
[1mStep[0m  [56/84], [94mLoss[0m : 1.98248
[1mStep[0m  [64/84], [94mLoss[0m : 2.03592
[1mStep[0m  [72/84], [94mLoss[0m : 1.80967
[1mStep[0m  [80/84], [94mLoss[0m : 1.91327

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.841, [92mTest[0m: 2.468, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.64353
[1mStep[0m  [8/84], [94mLoss[0m : 1.55958
[1mStep[0m  [16/84], [94mLoss[0m : 1.68141
[1mStep[0m  [24/84], [94mLoss[0m : 1.69738
[1mStep[0m  [32/84], [94mLoss[0m : 1.60283
[1mStep[0m  [40/84], [94mLoss[0m : 2.01489
[1mStep[0m  [48/84], [94mLoss[0m : 1.64697
[1mStep[0m  [56/84], [94mLoss[0m : 1.86059
[1mStep[0m  [64/84], [94mLoss[0m : 1.90235
[1mStep[0m  [72/84], [94mLoss[0m : 1.87796
[1mStep[0m  [80/84], [94mLoss[0m : 1.48404

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.810, [92mTest[0m: 2.425, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.74130
[1mStep[0m  [8/84], [94mLoss[0m : 1.93868
[1mStep[0m  [16/84], [94mLoss[0m : 1.87277
[1mStep[0m  [24/84], [94mLoss[0m : 1.67541
[1mStep[0m  [32/84], [94mLoss[0m : 1.83151
[1mStep[0m  [40/84], [94mLoss[0m : 1.69804
[1mStep[0m  [48/84], [94mLoss[0m : 1.74896
[1mStep[0m  [56/84], [94mLoss[0m : 1.78159
[1mStep[0m  [64/84], [94mLoss[0m : 1.91687
[1mStep[0m  [72/84], [94mLoss[0m : 1.75180
[1mStep[0m  [80/84], [94mLoss[0m : 2.03724

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.785, [92mTest[0m: 2.476, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.92845
[1mStep[0m  [8/84], [94mLoss[0m : 1.75628
[1mStep[0m  [16/84], [94mLoss[0m : 1.92229
[1mStep[0m  [24/84], [94mLoss[0m : 1.89746
[1mStep[0m  [32/84], [94mLoss[0m : 1.80221
[1mStep[0m  [40/84], [94mLoss[0m : 1.36496
[1mStep[0m  [48/84], [94mLoss[0m : 1.88555
[1mStep[0m  [56/84], [94mLoss[0m : 1.58653
[1mStep[0m  [64/84], [94mLoss[0m : 1.70650
[1mStep[0m  [72/84], [94mLoss[0m : 1.98252
[1mStep[0m  [80/84], [94mLoss[0m : 2.00457

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.784, [92mTest[0m: 2.459, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.79448
[1mStep[0m  [8/84], [94mLoss[0m : 1.62690
[1mStep[0m  [16/84], [94mLoss[0m : 1.66397
[1mStep[0m  [24/84], [94mLoss[0m : 1.84588
[1mStep[0m  [32/84], [94mLoss[0m : 1.60230
[1mStep[0m  [40/84], [94mLoss[0m : 1.43297
[1mStep[0m  [48/84], [94mLoss[0m : 1.75603
[1mStep[0m  [56/84], [94mLoss[0m : 1.67697
[1mStep[0m  [64/84], [94mLoss[0m : 1.66834
[1mStep[0m  [72/84], [94mLoss[0m : 1.95721
[1mStep[0m  [80/84], [94mLoss[0m : 1.79945

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.730, [92mTest[0m: 2.502, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.66595
[1mStep[0m  [8/84], [94mLoss[0m : 1.53473
[1mStep[0m  [16/84], [94mLoss[0m : 1.69186
[1mStep[0m  [24/84], [94mLoss[0m : 1.63229
[1mStep[0m  [32/84], [94mLoss[0m : 1.85711
[1mStep[0m  [40/84], [94mLoss[0m : 1.59507
[1mStep[0m  [48/84], [94mLoss[0m : 1.73307
[1mStep[0m  [56/84], [94mLoss[0m : 1.65214
[1mStep[0m  [64/84], [94mLoss[0m : 1.70935
[1mStep[0m  [72/84], [94mLoss[0m : 1.66007
[1mStep[0m  [80/84], [94mLoss[0m : 1.85881

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.727, [92mTest[0m: 2.416, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.74437
[1mStep[0m  [8/84], [94mLoss[0m : 1.66692
[1mStep[0m  [16/84], [94mLoss[0m : 1.56241
[1mStep[0m  [24/84], [94mLoss[0m : 1.89205
[1mStep[0m  [32/84], [94mLoss[0m : 1.88014
[1mStep[0m  [40/84], [94mLoss[0m : 1.70298
[1mStep[0m  [48/84], [94mLoss[0m : 1.60846
[1mStep[0m  [56/84], [94mLoss[0m : 1.74428
[1mStep[0m  [64/84], [94mLoss[0m : 1.69211
[1mStep[0m  [72/84], [94mLoss[0m : 1.89447
[1mStep[0m  [80/84], [94mLoss[0m : 1.76156

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.708, [92mTest[0m: 2.448, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.66727
[1mStep[0m  [8/84], [94mLoss[0m : 1.70326
[1mStep[0m  [16/84], [94mLoss[0m : 1.77443
[1mStep[0m  [24/84], [94mLoss[0m : 1.60151
[1mStep[0m  [32/84], [94mLoss[0m : 1.85272
[1mStep[0m  [40/84], [94mLoss[0m : 1.75417
[1mStep[0m  [48/84], [94mLoss[0m : 1.70974
[1mStep[0m  [56/84], [94mLoss[0m : 1.89555
[1mStep[0m  [64/84], [94mLoss[0m : 1.72434
[1mStep[0m  [72/84], [94mLoss[0m : 1.81692
[1mStep[0m  [80/84], [94mLoss[0m : 1.51349

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.691, [92mTest[0m: 2.463, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.60365
[1mStep[0m  [8/84], [94mLoss[0m : 1.64300
[1mStep[0m  [16/84], [94mLoss[0m : 1.55431
[1mStep[0m  [24/84], [94mLoss[0m : 1.65953
[1mStep[0m  [32/84], [94mLoss[0m : 1.59160
[1mStep[0m  [40/84], [94mLoss[0m : 1.66777
[1mStep[0m  [48/84], [94mLoss[0m : 1.54493
[1mStep[0m  [56/84], [94mLoss[0m : 1.50722
[1mStep[0m  [64/84], [94mLoss[0m : 1.48475
[1mStep[0m  [72/84], [94mLoss[0m : 1.57696
[1mStep[0m  [80/84], [94mLoss[0m : 1.75419

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.643, [92mTest[0m: 2.521, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.57812
[1mStep[0m  [8/84], [94mLoss[0m : 1.43856
[1mStep[0m  [16/84], [94mLoss[0m : 1.41667
[1mStep[0m  [24/84], [94mLoss[0m : 1.60763
[1mStep[0m  [32/84], [94mLoss[0m : 1.53452
[1mStep[0m  [40/84], [94mLoss[0m : 1.70923
[1mStep[0m  [48/84], [94mLoss[0m : 1.77206
[1mStep[0m  [56/84], [94mLoss[0m : 1.62481
[1mStep[0m  [64/84], [94mLoss[0m : 1.68823
[1mStep[0m  [72/84], [94mLoss[0m : 1.56664
[1mStep[0m  [80/84], [94mLoss[0m : 1.84835

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.640, [92mTest[0m: 2.473, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.93537
[1mStep[0m  [8/84], [94mLoss[0m : 1.66053
[1mStep[0m  [16/84], [94mLoss[0m : 1.64283
[1mStep[0m  [24/84], [94mLoss[0m : 1.60986
[1mStep[0m  [32/84], [94mLoss[0m : 1.90310
[1mStep[0m  [40/84], [94mLoss[0m : 1.33644
[1mStep[0m  [48/84], [94mLoss[0m : 1.90711
[1mStep[0m  [56/84], [94mLoss[0m : 1.56635
[1mStep[0m  [64/84], [94mLoss[0m : 1.59362
[1mStep[0m  [72/84], [94mLoss[0m : 1.70344
[1mStep[0m  [80/84], [94mLoss[0m : 1.63152

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.622, [92mTest[0m: 2.559, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.82467
[1mStep[0m  [8/84], [94mLoss[0m : 1.56516
[1mStep[0m  [16/84], [94mLoss[0m : 1.50630
[1mStep[0m  [24/84], [94mLoss[0m : 1.80602
[1mStep[0m  [32/84], [94mLoss[0m : 1.72960
[1mStep[0m  [40/84], [94mLoss[0m : 1.59269
[1mStep[0m  [48/84], [94mLoss[0m : 1.46819
[1mStep[0m  [56/84], [94mLoss[0m : 1.31373
[1mStep[0m  [64/84], [94mLoss[0m : 1.55343
[1mStep[0m  [72/84], [94mLoss[0m : 1.74027
[1mStep[0m  [80/84], [94mLoss[0m : 1.67386

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.619, [92mTest[0m: 2.472, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.69563
[1mStep[0m  [8/84], [94mLoss[0m : 1.60050
[1mStep[0m  [16/84], [94mLoss[0m : 1.63124
[1mStep[0m  [24/84], [94mLoss[0m : 1.74917
[1mStep[0m  [32/84], [94mLoss[0m : 1.65018
[1mStep[0m  [40/84], [94mLoss[0m : 1.58430
[1mStep[0m  [48/84], [94mLoss[0m : 1.49935
[1mStep[0m  [56/84], [94mLoss[0m : 1.69656
[1mStep[0m  [64/84], [94mLoss[0m : 1.35036
[1mStep[0m  [72/84], [94mLoss[0m : 1.81883
[1mStep[0m  [80/84], [94mLoss[0m : 1.59603

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.599, [92mTest[0m: 2.471, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.42946
[1mStep[0m  [8/84], [94mLoss[0m : 1.44361
[1mStep[0m  [16/84], [94mLoss[0m : 1.54697
[1mStep[0m  [24/84], [94mLoss[0m : 1.69674
[1mStep[0m  [32/84], [94mLoss[0m : 1.47126
[1mStep[0m  [40/84], [94mLoss[0m : 1.48637
[1mStep[0m  [48/84], [94mLoss[0m : 1.59293
[1mStep[0m  [56/84], [94mLoss[0m : 1.59754
[1mStep[0m  [64/84], [94mLoss[0m : 1.25610
[1mStep[0m  [72/84], [94mLoss[0m : 1.75106
[1mStep[0m  [80/84], [94mLoss[0m : 1.63792

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.613, [92mTest[0m: 2.497, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.67425
[1mStep[0m  [8/84], [94mLoss[0m : 1.46661
[1mStep[0m  [16/84], [94mLoss[0m : 1.52763
[1mStep[0m  [24/84], [94mLoss[0m : 1.75798
[1mStep[0m  [32/84], [94mLoss[0m : 1.67915
[1mStep[0m  [40/84], [94mLoss[0m : 1.56431
[1mStep[0m  [48/84], [94mLoss[0m : 1.67403
[1mStep[0m  [56/84], [94mLoss[0m : 1.65422
[1mStep[0m  [64/84], [94mLoss[0m : 1.53583
[1mStep[0m  [72/84], [94mLoss[0m : 1.71736
[1mStep[0m  [80/84], [94mLoss[0m : 1.59876

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.601, [92mTest[0m: 2.492, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.67518
[1mStep[0m  [8/84], [94mLoss[0m : 1.63025
[1mStep[0m  [16/84], [94mLoss[0m : 1.48456
[1mStep[0m  [24/84], [94mLoss[0m : 1.53233
[1mStep[0m  [32/84], [94mLoss[0m : 1.47878
[1mStep[0m  [40/84], [94mLoss[0m : 1.71990
[1mStep[0m  [48/84], [94mLoss[0m : 1.50659
[1mStep[0m  [56/84], [94mLoss[0m : 1.68590
[1mStep[0m  [64/84], [94mLoss[0m : 1.60361
[1mStep[0m  [72/84], [94mLoss[0m : 1.71514
[1mStep[0m  [80/84], [94mLoss[0m : 1.44610

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.572, [92mTest[0m: 2.510, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.68387
[1mStep[0m  [8/84], [94mLoss[0m : 1.53498
[1mStep[0m  [16/84], [94mLoss[0m : 1.78774
[1mStep[0m  [24/84], [94mLoss[0m : 1.43161
[1mStep[0m  [32/84], [94mLoss[0m : 1.48735
[1mStep[0m  [40/84], [94mLoss[0m : 1.57462
[1mStep[0m  [48/84], [94mLoss[0m : 1.37962
[1mStep[0m  [56/84], [94mLoss[0m : 1.60438
[1mStep[0m  [64/84], [94mLoss[0m : 1.58981
[1mStep[0m  [72/84], [94mLoss[0m : 1.87537
[1mStep[0m  [80/84], [94mLoss[0m : 1.51252

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.581, [92mTest[0m: 2.517, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.509
====================================

Phase 2 - Evaluation MAE:  2.5087721858705794
MAE score P1       2.326449
MAE score P2       2.508772
loss               1.572434
learning_rate      0.002575
batch_size              128
hidden_sizes      [200, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.9
weight_decay           0.01
Name: 8, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 10.21458
[1mStep[0m  [8/84], [94mLoss[0m : 10.28796
[1mStep[0m  [16/84], [94mLoss[0m : 10.17651
[1mStep[0m  [24/84], [94mLoss[0m : 10.19776
[1mStep[0m  [32/84], [94mLoss[0m : 9.69985
[1mStep[0m  [40/84], [94mLoss[0m : 9.15438
[1mStep[0m  [48/84], [94mLoss[0m : 8.83741
[1mStep[0m  [56/84], [94mLoss[0m : 8.14277
[1mStep[0m  [64/84], [94mLoss[0m : 8.22328
[1mStep[0m  [72/84], [94mLoss[0m : 7.73204
[1mStep[0m  [80/84], [94mLoss[0m : 7.77794

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.198, [92mTest[0m: 11.031, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.85483
[1mStep[0m  [8/84], [94mLoss[0m : 6.73308
[1mStep[0m  [16/84], [94mLoss[0m : 6.07778
[1mStep[0m  [24/84], [94mLoss[0m : 6.41478
[1mStep[0m  [32/84], [94mLoss[0m : 5.88779
[1mStep[0m  [40/84], [94mLoss[0m : 4.90121
[1mStep[0m  [48/84], [94mLoss[0m : 5.01613
[1mStep[0m  [56/84], [94mLoss[0m : 5.01836
[1mStep[0m  [64/84], [94mLoss[0m : 4.26143
[1mStep[0m  [72/84], [94mLoss[0m : 3.92337
[1mStep[0m  [80/84], [94mLoss[0m : 3.38506

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 5.267, [92mTest[0m: 8.229, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.02637
[1mStep[0m  [8/84], [94mLoss[0m : 3.62355
[1mStep[0m  [16/84], [94mLoss[0m : 3.63779
[1mStep[0m  [24/84], [94mLoss[0m : 3.36996
[1mStep[0m  [32/84], [94mLoss[0m : 2.95830
[1mStep[0m  [40/84], [94mLoss[0m : 2.89705
[1mStep[0m  [48/84], [94mLoss[0m : 2.76018
[1mStep[0m  [56/84], [94mLoss[0m : 2.68977
[1mStep[0m  [64/84], [94mLoss[0m : 2.55963
[1mStep[0m  [72/84], [94mLoss[0m : 2.71645
[1mStep[0m  [80/84], [94mLoss[0m : 2.99077

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.051, [92mTest[0m: 4.559, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53886
[1mStep[0m  [8/84], [94mLoss[0m : 2.77072
[1mStep[0m  [16/84], [94mLoss[0m : 2.79752
[1mStep[0m  [24/84], [94mLoss[0m : 2.71474
[1mStep[0m  [32/84], [94mLoss[0m : 2.61389
[1mStep[0m  [40/84], [94mLoss[0m : 2.84093
[1mStep[0m  [48/84], [94mLoss[0m : 2.67706
[1mStep[0m  [56/84], [94mLoss[0m : 2.55179
[1mStep[0m  [64/84], [94mLoss[0m : 2.88273
[1mStep[0m  [72/84], [94mLoss[0m : 2.84650
[1mStep[0m  [80/84], [94mLoss[0m : 2.31143

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.712, [92mTest[0m: 2.821, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.79969
[1mStep[0m  [8/84], [94mLoss[0m : 2.34315
[1mStep[0m  [16/84], [94mLoss[0m : 2.74512
[1mStep[0m  [24/84], [94mLoss[0m : 2.77994
[1mStep[0m  [32/84], [94mLoss[0m : 2.96059
[1mStep[0m  [40/84], [94mLoss[0m : 2.45118
[1mStep[0m  [48/84], [94mLoss[0m : 2.67794
[1mStep[0m  [56/84], [94mLoss[0m : 2.52300
[1mStep[0m  [64/84], [94mLoss[0m : 3.14974
[1mStep[0m  [72/84], [94mLoss[0m : 2.68027
[1mStep[0m  [80/84], [94mLoss[0m : 3.12546

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.683, [92mTest[0m: 2.540, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.68515
[1mStep[0m  [8/84], [94mLoss[0m : 2.46632
[1mStep[0m  [16/84], [94mLoss[0m : 2.74689
[1mStep[0m  [24/84], [94mLoss[0m : 2.84632
[1mStep[0m  [32/84], [94mLoss[0m : 3.01869
[1mStep[0m  [40/84], [94mLoss[0m : 2.50119
[1mStep[0m  [48/84], [94mLoss[0m : 2.36940
[1mStep[0m  [56/84], [94mLoss[0m : 2.60019
[1mStep[0m  [64/84], [94mLoss[0m : 2.27078
[1mStep[0m  [72/84], [94mLoss[0m : 3.05168
[1mStep[0m  [80/84], [94mLoss[0m : 2.49574

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.649, [92mTest[0m: 2.449, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.70777
[1mStep[0m  [8/84], [94mLoss[0m : 2.54172
[1mStep[0m  [16/84], [94mLoss[0m : 2.87990
[1mStep[0m  [24/84], [94mLoss[0m : 2.31572
[1mStep[0m  [32/84], [94mLoss[0m : 3.09517
[1mStep[0m  [40/84], [94mLoss[0m : 2.18748
[1mStep[0m  [48/84], [94mLoss[0m : 2.62967
[1mStep[0m  [56/84], [94mLoss[0m : 2.55091
[1mStep[0m  [64/84], [94mLoss[0m : 2.57983
[1mStep[0m  [72/84], [94mLoss[0m : 2.70144
[1mStep[0m  [80/84], [94mLoss[0m : 2.06519

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.603, [92mTest[0m: 2.432, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31921
[1mStep[0m  [8/84], [94mLoss[0m : 2.44148
[1mStep[0m  [16/84], [94mLoss[0m : 2.98202
[1mStep[0m  [24/84], [94mLoss[0m : 2.68212
[1mStep[0m  [32/84], [94mLoss[0m : 2.58456
[1mStep[0m  [40/84], [94mLoss[0m : 2.61012
[1mStep[0m  [48/84], [94mLoss[0m : 2.65634
[1mStep[0m  [56/84], [94mLoss[0m : 2.56469
[1mStep[0m  [64/84], [94mLoss[0m : 2.82898
[1mStep[0m  [72/84], [94mLoss[0m : 3.03477
[1mStep[0m  [80/84], [94mLoss[0m : 2.42917

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.623, [92mTest[0m: 2.362, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.82919
[1mStep[0m  [8/84], [94mLoss[0m : 2.95395
[1mStep[0m  [16/84], [94mLoss[0m : 2.59021
[1mStep[0m  [24/84], [94mLoss[0m : 2.71759
[1mStep[0m  [32/84], [94mLoss[0m : 2.30084
[1mStep[0m  [40/84], [94mLoss[0m : 2.61995
[1mStep[0m  [48/84], [94mLoss[0m : 2.48332
[1mStep[0m  [56/84], [94mLoss[0m : 2.83651
[1mStep[0m  [64/84], [94mLoss[0m : 2.50896
[1mStep[0m  [72/84], [94mLoss[0m : 2.62384
[1mStep[0m  [80/84], [94mLoss[0m : 2.76718

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.593, [92mTest[0m: 2.366, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.70555
[1mStep[0m  [8/84], [94mLoss[0m : 2.51471
[1mStep[0m  [16/84], [94mLoss[0m : 2.41449
[1mStep[0m  [24/84], [94mLoss[0m : 2.53707
[1mStep[0m  [32/84], [94mLoss[0m : 2.56693
[1mStep[0m  [40/84], [94mLoss[0m : 2.61949
[1mStep[0m  [48/84], [94mLoss[0m : 2.20375
[1mStep[0m  [56/84], [94mLoss[0m : 2.28899
[1mStep[0m  [64/84], [94mLoss[0m : 2.40296
[1mStep[0m  [72/84], [94mLoss[0m : 2.35925
[1mStep[0m  [80/84], [94mLoss[0m : 2.51119

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.557, [92mTest[0m: 2.370, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44182
[1mStep[0m  [8/84], [94mLoss[0m : 2.72604
[1mStep[0m  [16/84], [94mLoss[0m : 2.41768
[1mStep[0m  [24/84], [94mLoss[0m : 2.83231
[1mStep[0m  [32/84], [94mLoss[0m : 2.48445
[1mStep[0m  [40/84], [94mLoss[0m : 2.58745
[1mStep[0m  [48/84], [94mLoss[0m : 2.69470
[1mStep[0m  [56/84], [94mLoss[0m : 2.37516
[1mStep[0m  [64/84], [94mLoss[0m : 2.18868
[1mStep[0m  [72/84], [94mLoss[0m : 2.56491
[1mStep[0m  [80/84], [94mLoss[0m : 2.67499

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.559, [92mTest[0m: 2.366, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40433
[1mStep[0m  [8/84], [94mLoss[0m : 2.54441
[1mStep[0m  [16/84], [94mLoss[0m : 2.73099
[1mStep[0m  [24/84], [94mLoss[0m : 2.46887
[1mStep[0m  [32/84], [94mLoss[0m : 2.72888
[1mStep[0m  [40/84], [94mLoss[0m : 2.66558
[1mStep[0m  [48/84], [94mLoss[0m : 2.71667
[1mStep[0m  [56/84], [94mLoss[0m : 2.83420
[1mStep[0m  [64/84], [94mLoss[0m : 2.40977
[1mStep[0m  [72/84], [94mLoss[0m : 2.77130
[1mStep[0m  [80/84], [94mLoss[0m : 2.62534

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.352, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46966
[1mStep[0m  [8/84], [94mLoss[0m : 2.61972
[1mStep[0m  [16/84], [94mLoss[0m : 2.52205
[1mStep[0m  [24/84], [94mLoss[0m : 2.84983
[1mStep[0m  [32/84], [94mLoss[0m : 2.44472
[1mStep[0m  [40/84], [94mLoss[0m : 2.64792
[1mStep[0m  [48/84], [94mLoss[0m : 2.48873
[1mStep[0m  [56/84], [94mLoss[0m : 2.35195
[1mStep[0m  [64/84], [94mLoss[0m : 2.56136
[1mStep[0m  [72/84], [94mLoss[0m : 2.04996
[1mStep[0m  [80/84], [94mLoss[0m : 2.54201

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.366, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.25555
[1mStep[0m  [8/84], [94mLoss[0m : 2.49267
[1mStep[0m  [16/84], [94mLoss[0m : 2.49607
[1mStep[0m  [24/84], [94mLoss[0m : 2.66456
[1mStep[0m  [32/84], [94mLoss[0m : 2.65537
[1mStep[0m  [40/84], [94mLoss[0m : 2.57764
[1mStep[0m  [48/84], [94mLoss[0m : 2.55506
[1mStep[0m  [56/84], [94mLoss[0m : 2.58727
[1mStep[0m  [64/84], [94mLoss[0m : 2.40326
[1mStep[0m  [72/84], [94mLoss[0m : 2.80774
[1mStep[0m  [80/84], [94mLoss[0m : 2.43801

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.559, [92mTest[0m: 2.352, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.05725
[1mStep[0m  [8/84], [94mLoss[0m : 2.50336
[1mStep[0m  [16/84], [94mLoss[0m : 2.58666
[1mStep[0m  [24/84], [94mLoss[0m : 2.51304
[1mStep[0m  [32/84], [94mLoss[0m : 2.72835
[1mStep[0m  [40/84], [94mLoss[0m : 2.16234
[1mStep[0m  [48/84], [94mLoss[0m : 2.61312
[1mStep[0m  [56/84], [94mLoss[0m : 2.56658
[1mStep[0m  [64/84], [94mLoss[0m : 2.38274
[1mStep[0m  [72/84], [94mLoss[0m : 2.43748
[1mStep[0m  [80/84], [94mLoss[0m : 2.61940

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.346, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.19643
[1mStep[0m  [8/84], [94mLoss[0m : 2.64560
[1mStep[0m  [16/84], [94mLoss[0m : 2.38019
[1mStep[0m  [24/84], [94mLoss[0m : 2.67269
[1mStep[0m  [32/84], [94mLoss[0m : 2.35929
[1mStep[0m  [40/84], [94mLoss[0m : 2.56370
[1mStep[0m  [48/84], [94mLoss[0m : 2.61506
[1mStep[0m  [56/84], [94mLoss[0m : 2.78933
[1mStep[0m  [64/84], [94mLoss[0m : 2.50850
[1mStep[0m  [72/84], [94mLoss[0m : 2.49984
[1mStep[0m  [80/84], [94mLoss[0m : 2.64542

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.345, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37095
[1mStep[0m  [8/84], [94mLoss[0m : 2.50741
[1mStep[0m  [16/84], [94mLoss[0m : 2.42823
[1mStep[0m  [24/84], [94mLoss[0m : 2.27111
[1mStep[0m  [32/84], [94mLoss[0m : 2.58756
[1mStep[0m  [40/84], [94mLoss[0m : 2.62409
[1mStep[0m  [48/84], [94mLoss[0m : 2.68658
[1mStep[0m  [56/84], [94mLoss[0m : 2.31654
[1mStep[0m  [64/84], [94mLoss[0m : 2.27860
[1mStep[0m  [72/84], [94mLoss[0m : 2.37616
[1mStep[0m  [80/84], [94mLoss[0m : 2.50852

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.347, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.51982
[1mStep[0m  [8/84], [94mLoss[0m : 2.68293
[1mStep[0m  [16/84], [94mLoss[0m : 2.72555
[1mStep[0m  [24/84], [94mLoss[0m : 2.56225
[1mStep[0m  [32/84], [94mLoss[0m : 2.27170
[1mStep[0m  [40/84], [94mLoss[0m : 2.11444
[1mStep[0m  [48/84], [94mLoss[0m : 2.40421
[1mStep[0m  [56/84], [94mLoss[0m : 2.67436
[1mStep[0m  [64/84], [94mLoss[0m : 2.58824
[1mStep[0m  [72/84], [94mLoss[0m : 2.58764
[1mStep[0m  [80/84], [94mLoss[0m : 2.29953

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.351, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.80717
[1mStep[0m  [8/84], [94mLoss[0m : 2.47213
[1mStep[0m  [16/84], [94mLoss[0m : 2.54235
[1mStep[0m  [24/84], [94mLoss[0m : 2.22179
[1mStep[0m  [32/84], [94mLoss[0m : 2.69474
[1mStep[0m  [40/84], [94mLoss[0m : 2.32375
[1mStep[0m  [48/84], [94mLoss[0m : 2.36047
[1mStep[0m  [56/84], [94mLoss[0m : 2.46352
[1mStep[0m  [64/84], [94mLoss[0m : 2.51042
[1mStep[0m  [72/84], [94mLoss[0m : 2.65191
[1mStep[0m  [80/84], [94mLoss[0m : 2.34055

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.337, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.38301
[1mStep[0m  [8/84], [94mLoss[0m : 2.20651
[1mStep[0m  [16/84], [94mLoss[0m : 2.36349
[1mStep[0m  [24/84], [94mLoss[0m : 2.49188
[1mStep[0m  [32/84], [94mLoss[0m : 2.60948
[1mStep[0m  [40/84], [94mLoss[0m : 2.50071
[1mStep[0m  [48/84], [94mLoss[0m : 2.30873
[1mStep[0m  [56/84], [94mLoss[0m : 2.27546
[1mStep[0m  [64/84], [94mLoss[0m : 2.15173
[1mStep[0m  [72/84], [94mLoss[0m : 2.73421
[1mStep[0m  [80/84], [94mLoss[0m : 2.51958

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.329, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.63474
[1mStep[0m  [8/84], [94mLoss[0m : 2.41220
[1mStep[0m  [16/84], [94mLoss[0m : 2.43495
[1mStep[0m  [24/84], [94mLoss[0m : 2.32591
[1mStep[0m  [32/84], [94mLoss[0m : 2.50729
[1mStep[0m  [40/84], [94mLoss[0m : 2.08292
[1mStep[0m  [48/84], [94mLoss[0m : 2.42680
[1mStep[0m  [56/84], [94mLoss[0m : 2.26702
[1mStep[0m  [64/84], [94mLoss[0m : 2.42157
[1mStep[0m  [72/84], [94mLoss[0m : 2.42222
[1mStep[0m  [80/84], [94mLoss[0m : 2.44359

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.322, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37015
[1mStep[0m  [8/84], [94mLoss[0m : 2.12907
[1mStep[0m  [16/84], [94mLoss[0m : 2.30571
[1mStep[0m  [24/84], [94mLoss[0m : 2.30664
[1mStep[0m  [32/84], [94mLoss[0m : 2.37592
[1mStep[0m  [40/84], [94mLoss[0m : 2.65415
[1mStep[0m  [48/84], [94mLoss[0m : 3.30364
[1mStep[0m  [56/84], [94mLoss[0m : 2.31578
[1mStep[0m  [64/84], [94mLoss[0m : 2.45369
[1mStep[0m  [72/84], [94mLoss[0m : 2.47716
[1mStep[0m  [80/84], [94mLoss[0m : 2.03138

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.321, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.89168
[1mStep[0m  [8/84], [94mLoss[0m : 2.51742
[1mStep[0m  [16/84], [94mLoss[0m : 2.32237
[1mStep[0m  [24/84], [94mLoss[0m : 2.54524
[1mStep[0m  [32/84], [94mLoss[0m : 2.44923
[1mStep[0m  [40/84], [94mLoss[0m : 2.43008
[1mStep[0m  [48/84], [94mLoss[0m : 2.24703
[1mStep[0m  [56/84], [94mLoss[0m : 2.35719
[1mStep[0m  [64/84], [94mLoss[0m : 2.55255
[1mStep[0m  [72/84], [94mLoss[0m : 2.44406
[1mStep[0m  [80/84], [94mLoss[0m : 2.75388

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.324, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36468
[1mStep[0m  [8/84], [94mLoss[0m : 2.53939
[1mStep[0m  [16/84], [94mLoss[0m : 2.37994
[1mStep[0m  [24/84], [94mLoss[0m : 2.25861
[1mStep[0m  [32/84], [94mLoss[0m : 2.33042
[1mStep[0m  [40/84], [94mLoss[0m : 2.53502
[1mStep[0m  [48/84], [94mLoss[0m : 2.73848
[1mStep[0m  [56/84], [94mLoss[0m : 2.63702
[1mStep[0m  [64/84], [94mLoss[0m : 2.53506
[1mStep[0m  [72/84], [94mLoss[0m : 2.72647
[1mStep[0m  [80/84], [94mLoss[0m : 2.67909

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.313, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46490
[1mStep[0m  [8/84], [94mLoss[0m : 2.17814
[1mStep[0m  [16/84], [94mLoss[0m : 2.26004
[1mStep[0m  [24/84], [94mLoss[0m : 2.59189
[1mStep[0m  [32/84], [94mLoss[0m : 2.44202
[1mStep[0m  [40/84], [94mLoss[0m : 2.67458
[1mStep[0m  [48/84], [94mLoss[0m : 2.49789
[1mStep[0m  [56/84], [94mLoss[0m : 2.11491
[1mStep[0m  [64/84], [94mLoss[0m : 2.59952
[1mStep[0m  [72/84], [94mLoss[0m : 2.51575
[1mStep[0m  [80/84], [94mLoss[0m : 2.50291

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.312, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.38697
[1mStep[0m  [8/84], [94mLoss[0m : 2.45645
[1mStep[0m  [16/84], [94mLoss[0m : 2.35467
[1mStep[0m  [24/84], [94mLoss[0m : 2.53492
[1mStep[0m  [32/84], [94mLoss[0m : 2.25991
[1mStep[0m  [40/84], [94mLoss[0m : 2.60900
[1mStep[0m  [48/84], [94mLoss[0m : 2.62820
[1mStep[0m  [56/84], [94mLoss[0m : 2.50266
[1mStep[0m  [64/84], [94mLoss[0m : 2.45471
[1mStep[0m  [72/84], [94mLoss[0m : 2.35290
[1mStep[0m  [80/84], [94mLoss[0m : 2.32933

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.313, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33811
[1mStep[0m  [8/84], [94mLoss[0m : 2.59925
[1mStep[0m  [16/84], [94mLoss[0m : 2.44937
[1mStep[0m  [24/84], [94mLoss[0m : 2.54077
[1mStep[0m  [32/84], [94mLoss[0m : 2.61060
[1mStep[0m  [40/84], [94mLoss[0m : 2.63957
[1mStep[0m  [48/84], [94mLoss[0m : 2.42339
[1mStep[0m  [56/84], [94mLoss[0m : 2.65936
[1mStep[0m  [64/84], [94mLoss[0m : 2.31282
[1mStep[0m  [72/84], [94mLoss[0m : 2.19930
[1mStep[0m  [80/84], [94mLoss[0m : 2.19151

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.316, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26279
[1mStep[0m  [8/84], [94mLoss[0m : 2.56839
[1mStep[0m  [16/84], [94mLoss[0m : 2.50053
[1mStep[0m  [24/84], [94mLoss[0m : 2.36615
[1mStep[0m  [32/84], [94mLoss[0m : 2.70802
[1mStep[0m  [40/84], [94mLoss[0m : 2.40110
[1mStep[0m  [48/84], [94mLoss[0m : 2.64392
[1mStep[0m  [56/84], [94mLoss[0m : 2.56891
[1mStep[0m  [64/84], [94mLoss[0m : 2.33555
[1mStep[0m  [72/84], [94mLoss[0m : 2.39028
[1mStep[0m  [80/84], [94mLoss[0m : 2.38324

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.327, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37038
[1mStep[0m  [8/84], [94mLoss[0m : 2.35080
[1mStep[0m  [16/84], [94mLoss[0m : 2.80552
[1mStep[0m  [24/84], [94mLoss[0m : 2.38098
[1mStep[0m  [32/84], [94mLoss[0m : 2.56867
[1mStep[0m  [40/84], [94mLoss[0m : 2.28485
[1mStep[0m  [48/84], [94mLoss[0m : 2.67010
[1mStep[0m  [56/84], [94mLoss[0m : 2.69163
[1mStep[0m  [64/84], [94mLoss[0m : 2.85916
[1mStep[0m  [72/84], [94mLoss[0m : 2.40304
[1mStep[0m  [80/84], [94mLoss[0m : 2.50841

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.322, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41658
[1mStep[0m  [8/84], [94mLoss[0m : 2.20523
[1mStep[0m  [16/84], [94mLoss[0m : 2.33396
[1mStep[0m  [24/84], [94mLoss[0m : 2.41496
[1mStep[0m  [32/84], [94mLoss[0m : 2.35726
[1mStep[0m  [40/84], [94mLoss[0m : 2.45329
[1mStep[0m  [48/84], [94mLoss[0m : 2.37574
[1mStep[0m  [56/84], [94mLoss[0m : 2.23979
[1mStep[0m  [64/84], [94mLoss[0m : 2.42860
[1mStep[0m  [72/84], [94mLoss[0m : 2.58219
[1mStep[0m  [80/84], [94mLoss[0m : 2.57656

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.318, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.311
====================================

Phase 1 - Evaluation MAE:  2.3113764439310347
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 2.30432
[1mStep[0m  [8/84], [94mLoss[0m : 2.84636
[1mStep[0m  [16/84], [94mLoss[0m : 2.39921
[1mStep[0m  [24/84], [94mLoss[0m : 2.96017
[1mStep[0m  [32/84], [94mLoss[0m : 2.36473
[1mStep[0m  [40/84], [94mLoss[0m : 2.57842
[1mStep[0m  [48/84], [94mLoss[0m : 2.65103
[1mStep[0m  [56/84], [94mLoss[0m : 2.59234
[1mStep[0m  [64/84], [94mLoss[0m : 2.49566
[1mStep[0m  [72/84], [94mLoss[0m : 2.16411
[1mStep[0m  [80/84], [94mLoss[0m : 2.26037

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.531, [92mTest[0m: 2.314, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.69888
[1mStep[0m  [8/84], [94mLoss[0m : 2.60127
[1mStep[0m  [16/84], [94mLoss[0m : 2.48861
[1mStep[0m  [24/84], [94mLoss[0m : 2.61871
[1mStep[0m  [32/84], [94mLoss[0m : 2.31556
[1mStep[0m  [40/84], [94mLoss[0m : 2.42815
[1mStep[0m  [48/84], [94mLoss[0m : 2.43047
[1mStep[0m  [56/84], [94mLoss[0m : 2.65399
[1mStep[0m  [64/84], [94mLoss[0m : 2.60552
[1mStep[0m  [72/84], [94mLoss[0m : 2.41571
[1mStep[0m  [80/84], [94mLoss[0m : 2.41227

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.688, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52043
[1mStep[0m  [8/84], [94mLoss[0m : 2.30624
[1mStep[0m  [16/84], [94mLoss[0m : 2.49907
[1mStep[0m  [24/84], [94mLoss[0m : 2.26566
[1mStep[0m  [32/84], [94mLoss[0m : 2.33140
[1mStep[0m  [40/84], [94mLoss[0m : 2.51898
[1mStep[0m  [48/84], [94mLoss[0m : 2.31013
[1mStep[0m  [56/84], [94mLoss[0m : 2.39696
[1mStep[0m  [64/84], [94mLoss[0m : 2.61621
[1mStep[0m  [72/84], [94mLoss[0m : 2.28048
[1mStep[0m  [80/84], [94mLoss[0m : 2.34485

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.559, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26774
[1mStep[0m  [8/84], [94mLoss[0m : 2.28940
[1mStep[0m  [16/84], [94mLoss[0m : 2.39075
[1mStep[0m  [24/84], [94mLoss[0m : 2.27779
[1mStep[0m  [32/84], [94mLoss[0m : 2.44858
[1mStep[0m  [40/84], [94mLoss[0m : 2.21146
[1mStep[0m  [48/84], [94mLoss[0m : 2.30874
[1mStep[0m  [56/84], [94mLoss[0m : 2.33728
[1mStep[0m  [64/84], [94mLoss[0m : 2.39930
[1mStep[0m  [72/84], [94mLoss[0m : 2.44876
[1mStep[0m  [80/84], [94mLoss[0m : 2.39189

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.550, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.61365
[1mStep[0m  [8/84], [94mLoss[0m : 2.40633
[1mStep[0m  [16/84], [94mLoss[0m : 2.33805
[1mStep[0m  [24/84], [94mLoss[0m : 2.53174
[1mStep[0m  [32/84], [94mLoss[0m : 2.30577
[1mStep[0m  [40/84], [94mLoss[0m : 2.23502
[1mStep[0m  [48/84], [94mLoss[0m : 2.46195
[1mStep[0m  [56/84], [94mLoss[0m : 2.33406
[1mStep[0m  [64/84], [94mLoss[0m : 2.38312
[1mStep[0m  [72/84], [94mLoss[0m : 2.28776
[1mStep[0m  [80/84], [94mLoss[0m : 2.47972

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.634, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.24496
[1mStep[0m  [8/84], [94mLoss[0m : 2.22758
[1mStep[0m  [16/84], [94mLoss[0m : 2.76577
[1mStep[0m  [24/84], [94mLoss[0m : 2.46479
[1mStep[0m  [32/84], [94mLoss[0m : 2.23129
[1mStep[0m  [40/84], [94mLoss[0m : 2.56375
[1mStep[0m  [48/84], [94mLoss[0m : 2.18916
[1mStep[0m  [56/84], [94mLoss[0m : 2.45642
[1mStep[0m  [64/84], [94mLoss[0m : 2.53164
[1mStep[0m  [72/84], [94mLoss[0m : 2.58067
[1mStep[0m  [80/84], [94mLoss[0m : 2.36191

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.393, [92mTest[0m: 2.561, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.11235
[1mStep[0m  [8/84], [94mLoss[0m : 2.49419
[1mStep[0m  [16/84], [94mLoss[0m : 2.24410
[1mStep[0m  [24/84], [94mLoss[0m : 2.44667
[1mStep[0m  [32/84], [94mLoss[0m : 2.72935
[1mStep[0m  [40/84], [94mLoss[0m : 2.42323
[1mStep[0m  [48/84], [94mLoss[0m : 2.62500
[1mStep[0m  [56/84], [94mLoss[0m : 2.56897
[1mStep[0m  [64/84], [94mLoss[0m : 2.32400
[1mStep[0m  [72/84], [94mLoss[0m : 2.66377
[1mStep[0m  [80/84], [94mLoss[0m : 2.25754

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.365, [92mTest[0m: 2.543, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45887
[1mStep[0m  [8/84], [94mLoss[0m : 2.49027
[1mStep[0m  [16/84], [94mLoss[0m : 2.33297
[1mStep[0m  [24/84], [94mLoss[0m : 2.38877
[1mStep[0m  [32/84], [94mLoss[0m : 2.28357
[1mStep[0m  [40/84], [94mLoss[0m : 2.54673
[1mStep[0m  [48/84], [94mLoss[0m : 2.36088
[1mStep[0m  [56/84], [94mLoss[0m : 2.32868
[1mStep[0m  [64/84], [94mLoss[0m : 2.31160
[1mStep[0m  [72/84], [94mLoss[0m : 2.23095
[1mStep[0m  [80/84], [94mLoss[0m : 2.75088

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.317, [92mTest[0m: 2.523, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36130
[1mStep[0m  [8/84], [94mLoss[0m : 2.36995
[1mStep[0m  [16/84], [94mLoss[0m : 2.10807
[1mStep[0m  [24/84], [94mLoss[0m : 2.36162
[1mStep[0m  [32/84], [94mLoss[0m : 2.00229
[1mStep[0m  [40/84], [94mLoss[0m : 2.36255
[1mStep[0m  [48/84], [94mLoss[0m : 2.44716
[1mStep[0m  [56/84], [94mLoss[0m : 2.49872
[1mStep[0m  [64/84], [94mLoss[0m : 2.17382
[1mStep[0m  [72/84], [94mLoss[0m : 2.23690
[1mStep[0m  [80/84], [94mLoss[0m : 2.28129

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.297, [92mTest[0m: 2.548, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35051
[1mStep[0m  [8/84], [94mLoss[0m : 2.32251
[1mStep[0m  [16/84], [94mLoss[0m : 2.11981
[1mStep[0m  [24/84], [94mLoss[0m : 2.30652
[1mStep[0m  [32/84], [94mLoss[0m : 2.49098
[1mStep[0m  [40/84], [94mLoss[0m : 2.47252
[1mStep[0m  [48/84], [94mLoss[0m : 2.19349
[1mStep[0m  [56/84], [94mLoss[0m : 1.86645
[1mStep[0m  [64/84], [94mLoss[0m : 2.13753
[1mStep[0m  [72/84], [94mLoss[0m : 2.36706
[1mStep[0m  [80/84], [94mLoss[0m : 2.41271

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.281, [92mTest[0m: 2.495, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31940
[1mStep[0m  [8/84], [94mLoss[0m : 2.34693
[1mStep[0m  [16/84], [94mLoss[0m : 2.10023
[1mStep[0m  [24/84], [94mLoss[0m : 2.29043
[1mStep[0m  [32/84], [94mLoss[0m : 2.31972
[1mStep[0m  [40/84], [94mLoss[0m : 2.28344
[1mStep[0m  [48/84], [94mLoss[0m : 2.50306
[1mStep[0m  [56/84], [94mLoss[0m : 2.31215
[1mStep[0m  [64/84], [94mLoss[0m : 2.08178
[1mStep[0m  [72/84], [94mLoss[0m : 2.30645
[1mStep[0m  [80/84], [94mLoss[0m : 2.25032

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.233, [92mTest[0m: 2.445, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.21657
[1mStep[0m  [8/84], [94mLoss[0m : 2.31734
[1mStep[0m  [16/84], [94mLoss[0m : 2.16414
[1mStep[0m  [24/84], [94mLoss[0m : 2.28208
[1mStep[0m  [32/84], [94mLoss[0m : 2.34604
[1mStep[0m  [40/84], [94mLoss[0m : 2.34687
[1mStep[0m  [48/84], [94mLoss[0m : 2.53545
[1mStep[0m  [56/84], [94mLoss[0m : 2.29884
[1mStep[0m  [64/84], [94mLoss[0m : 2.21968
[1mStep[0m  [72/84], [94mLoss[0m : 2.23444
[1mStep[0m  [80/84], [94mLoss[0m : 2.11770

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.196, [92mTest[0m: 2.471, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.20824
[1mStep[0m  [8/84], [94mLoss[0m : 2.05633
[1mStep[0m  [16/84], [94mLoss[0m : 2.16758
[1mStep[0m  [24/84], [94mLoss[0m : 2.02338
[1mStep[0m  [32/84], [94mLoss[0m : 2.14342
[1mStep[0m  [40/84], [94mLoss[0m : 1.96720
[1mStep[0m  [48/84], [94mLoss[0m : 2.33248
[1mStep[0m  [56/84], [94mLoss[0m : 2.24408
[1mStep[0m  [64/84], [94mLoss[0m : 2.45015
[1mStep[0m  [72/84], [94mLoss[0m : 2.33340
[1mStep[0m  [80/84], [94mLoss[0m : 1.95299

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.176, [92mTest[0m: 2.583, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.05645
[1mStep[0m  [8/84], [94mLoss[0m : 2.47935
[1mStep[0m  [16/84], [94mLoss[0m : 2.11281
[1mStep[0m  [24/84], [94mLoss[0m : 1.94528
[1mStep[0m  [32/84], [94mLoss[0m : 2.19473
[1mStep[0m  [40/84], [94mLoss[0m : 2.09568
[1mStep[0m  [48/84], [94mLoss[0m : 2.12646
[1mStep[0m  [56/84], [94mLoss[0m : 2.11270
[1mStep[0m  [64/84], [94mLoss[0m : 1.96343
[1mStep[0m  [72/84], [94mLoss[0m : 2.09006
[1mStep[0m  [80/84], [94mLoss[0m : 2.12286

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.136, [92mTest[0m: 2.444, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.07723
[1mStep[0m  [8/84], [94mLoss[0m : 2.01645
[1mStep[0m  [16/84], [94mLoss[0m : 2.15402
[1mStep[0m  [24/84], [94mLoss[0m : 1.98551
[1mStep[0m  [32/84], [94mLoss[0m : 2.40593
[1mStep[0m  [40/84], [94mLoss[0m : 2.49440
[1mStep[0m  [48/84], [94mLoss[0m : 1.68704
[1mStep[0m  [56/84], [94mLoss[0m : 2.13697
[1mStep[0m  [64/84], [94mLoss[0m : 1.96177
[1mStep[0m  [72/84], [94mLoss[0m : 2.04180
[1mStep[0m  [80/84], [94mLoss[0m : 2.18502

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.117, [92mTest[0m: 2.538, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.93378
[1mStep[0m  [8/84], [94mLoss[0m : 2.15756
[1mStep[0m  [16/84], [94mLoss[0m : 2.22776
[1mStep[0m  [24/84], [94mLoss[0m : 1.99428
[1mStep[0m  [32/84], [94mLoss[0m : 2.10392
[1mStep[0m  [40/84], [94mLoss[0m : 1.98194
[1mStep[0m  [48/84], [94mLoss[0m : 2.04434
[1mStep[0m  [56/84], [94mLoss[0m : 2.03586
[1mStep[0m  [64/84], [94mLoss[0m : 2.03916
[1mStep[0m  [72/84], [94mLoss[0m : 2.17645
[1mStep[0m  [80/84], [94mLoss[0m : 1.92911

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.093, [92mTest[0m: 2.426, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.99085
[1mStep[0m  [8/84], [94mLoss[0m : 2.02869
[1mStep[0m  [16/84], [94mLoss[0m : 2.14370
[1mStep[0m  [24/84], [94mLoss[0m : 2.03546
[1mStep[0m  [32/84], [94mLoss[0m : 2.22604
[1mStep[0m  [40/84], [94mLoss[0m : 2.19127
[1mStep[0m  [48/84], [94mLoss[0m : 1.94543
[1mStep[0m  [56/84], [94mLoss[0m : 2.06439
[1mStep[0m  [64/84], [94mLoss[0m : 2.13158
[1mStep[0m  [72/84], [94mLoss[0m : 2.06237
[1mStep[0m  [80/84], [94mLoss[0m : 2.19989

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.080, [92mTest[0m: 2.445, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.23517
[1mStep[0m  [8/84], [94mLoss[0m : 2.19583
[1mStep[0m  [16/84], [94mLoss[0m : 1.77276
[1mStep[0m  [24/84], [94mLoss[0m : 1.76832
[1mStep[0m  [32/84], [94mLoss[0m : 2.05232
[1mStep[0m  [40/84], [94mLoss[0m : 2.01314
[1mStep[0m  [48/84], [94mLoss[0m : 2.07157
[1mStep[0m  [56/84], [94mLoss[0m : 1.88499
[1mStep[0m  [64/84], [94mLoss[0m : 2.24942
[1mStep[0m  [72/84], [94mLoss[0m : 2.28433
[1mStep[0m  [80/84], [94mLoss[0m : 1.75721

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.004, [92mTest[0m: 2.428, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.81998
[1mStep[0m  [8/84], [94mLoss[0m : 2.03835
[1mStep[0m  [16/84], [94mLoss[0m : 1.94874
[1mStep[0m  [24/84], [94mLoss[0m : 2.01494
[1mStep[0m  [32/84], [94mLoss[0m : 1.90678
[1mStep[0m  [40/84], [94mLoss[0m : 1.91834
[1mStep[0m  [48/84], [94mLoss[0m : 1.77631
[1mStep[0m  [56/84], [94mLoss[0m : 1.98998
[1mStep[0m  [64/84], [94mLoss[0m : 1.90608
[1mStep[0m  [72/84], [94mLoss[0m : 2.02615
[1mStep[0m  [80/84], [94mLoss[0m : 1.78920

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.000, [92mTest[0m: 2.461, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.89487
[1mStep[0m  [8/84], [94mLoss[0m : 1.75117
[1mStep[0m  [16/84], [94mLoss[0m : 1.74829
[1mStep[0m  [24/84], [94mLoss[0m : 2.20328
[1mStep[0m  [32/84], [94mLoss[0m : 1.91733
[1mStep[0m  [40/84], [94mLoss[0m : 2.05821
[1mStep[0m  [48/84], [94mLoss[0m : 2.02446
[1mStep[0m  [56/84], [94mLoss[0m : 1.92989
[1mStep[0m  [64/84], [94mLoss[0m : 1.97160
[1mStep[0m  [72/84], [94mLoss[0m : 2.09926
[1mStep[0m  [80/84], [94mLoss[0m : 2.03219

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.968, [92mTest[0m: 2.481, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.88816
[1mStep[0m  [8/84], [94mLoss[0m : 1.83499
[1mStep[0m  [16/84], [94mLoss[0m : 1.81155
[1mStep[0m  [24/84], [94mLoss[0m : 1.78876
[1mStep[0m  [32/84], [94mLoss[0m : 1.88102
[1mStep[0m  [40/84], [94mLoss[0m : 2.19215
[1mStep[0m  [48/84], [94mLoss[0m : 1.83311
[1mStep[0m  [56/84], [94mLoss[0m : 2.01376
[1mStep[0m  [64/84], [94mLoss[0m : 1.82286
[1mStep[0m  [72/84], [94mLoss[0m : 1.83057
[1mStep[0m  [80/84], [94mLoss[0m : 1.74009

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.935, [92mTest[0m: 2.436, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.91564
[1mStep[0m  [8/84], [94mLoss[0m : 1.98236
[1mStep[0m  [16/84], [94mLoss[0m : 1.70941
[1mStep[0m  [24/84], [94mLoss[0m : 2.15483
[1mStep[0m  [32/84], [94mLoss[0m : 2.02818
[1mStep[0m  [40/84], [94mLoss[0m : 1.76555
[1mStep[0m  [48/84], [94mLoss[0m : 1.62711
[1mStep[0m  [56/84], [94mLoss[0m : 2.04902
[1mStep[0m  [64/84], [94mLoss[0m : 1.72226
[1mStep[0m  [72/84], [94mLoss[0m : 1.89997
[1mStep[0m  [80/84], [94mLoss[0m : 2.12891

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.900, [92mTest[0m: 2.435, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.77826
[1mStep[0m  [8/84], [94mLoss[0m : 1.94843
[1mStep[0m  [16/84], [94mLoss[0m : 1.87348
[1mStep[0m  [24/84], [94mLoss[0m : 1.78506
[1mStep[0m  [32/84], [94mLoss[0m : 1.73569
[1mStep[0m  [40/84], [94mLoss[0m : 1.95600
[1mStep[0m  [48/84], [94mLoss[0m : 1.52580
[1mStep[0m  [56/84], [94mLoss[0m : 1.84860
[1mStep[0m  [64/84], [94mLoss[0m : 2.07302
[1mStep[0m  [72/84], [94mLoss[0m : 1.93492
[1mStep[0m  [80/84], [94mLoss[0m : 1.99310

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.863, [92mTest[0m: 2.483, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.04049
[1mStep[0m  [8/84], [94mLoss[0m : 1.77297
[1mStep[0m  [16/84], [94mLoss[0m : 1.76627
[1mStep[0m  [24/84], [94mLoss[0m : 1.87975
[1mStep[0m  [32/84], [94mLoss[0m : 2.04007
[1mStep[0m  [40/84], [94mLoss[0m : 1.71064
[1mStep[0m  [48/84], [94mLoss[0m : 1.64103
[1mStep[0m  [56/84], [94mLoss[0m : 1.60182
[1mStep[0m  [64/84], [94mLoss[0m : 1.78039
[1mStep[0m  [72/84], [94mLoss[0m : 2.02090
[1mStep[0m  [80/84], [94mLoss[0m : 1.83321

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.824, [92mTest[0m: 2.462, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.61984
[1mStep[0m  [8/84], [94mLoss[0m : 1.75852
[1mStep[0m  [16/84], [94mLoss[0m : 1.75926
[1mStep[0m  [24/84], [94mLoss[0m : 1.89714
[1mStep[0m  [32/84], [94mLoss[0m : 1.91878
[1mStep[0m  [40/84], [94mLoss[0m : 1.82919
[1mStep[0m  [48/84], [94mLoss[0m : 1.81272
[1mStep[0m  [56/84], [94mLoss[0m : 1.88989
[1mStep[0m  [64/84], [94mLoss[0m : 1.86777
[1mStep[0m  [72/84], [94mLoss[0m : 1.72854
[1mStep[0m  [80/84], [94mLoss[0m : 1.71657

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.821, [92mTest[0m: 2.457, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.82456
[1mStep[0m  [8/84], [94mLoss[0m : 1.72046
[1mStep[0m  [16/84], [94mLoss[0m : 1.88719
[1mStep[0m  [24/84], [94mLoss[0m : 1.75561
[1mStep[0m  [32/84], [94mLoss[0m : 1.91002
[1mStep[0m  [40/84], [94mLoss[0m : 1.83028
[1mStep[0m  [48/84], [94mLoss[0m : 1.87115
[1mStep[0m  [56/84], [94mLoss[0m : 1.76592
[1mStep[0m  [64/84], [94mLoss[0m : 1.67060
[1mStep[0m  [72/84], [94mLoss[0m : 1.86561
[1mStep[0m  [80/84], [94mLoss[0m : 1.75293

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.824, [92mTest[0m: 2.426, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.96222
[1mStep[0m  [8/84], [94mLoss[0m : 1.81227
[1mStep[0m  [16/84], [94mLoss[0m : 1.89094
[1mStep[0m  [24/84], [94mLoss[0m : 1.67910
[1mStep[0m  [32/84], [94mLoss[0m : 1.85100
[1mStep[0m  [40/84], [94mLoss[0m : 1.68294
[1mStep[0m  [48/84], [94mLoss[0m : 1.71371
[1mStep[0m  [56/84], [94mLoss[0m : 1.69783
[1mStep[0m  [64/84], [94mLoss[0m : 1.71789
[1mStep[0m  [72/84], [94mLoss[0m : 1.78423
[1mStep[0m  [80/84], [94mLoss[0m : 1.86686

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.805, [92mTest[0m: 2.446, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.63240
[1mStep[0m  [8/84], [94mLoss[0m : 1.64422
[1mStep[0m  [16/84], [94mLoss[0m : 1.88536
[1mStep[0m  [24/84], [94mLoss[0m : 1.84765
[1mStep[0m  [32/84], [94mLoss[0m : 1.80098
[1mStep[0m  [40/84], [94mLoss[0m : 1.53808
[1mStep[0m  [48/84], [94mLoss[0m : 1.78723
[1mStep[0m  [56/84], [94mLoss[0m : 1.82451
[1mStep[0m  [64/84], [94mLoss[0m : 1.77313
[1mStep[0m  [72/84], [94mLoss[0m : 2.05370
[1mStep[0m  [80/84], [94mLoss[0m : 1.70185

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.773, [92mTest[0m: 2.498, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.72245
[1mStep[0m  [8/84], [94mLoss[0m : 1.72243
[1mStep[0m  [16/84], [94mLoss[0m : 1.73929
[1mStep[0m  [24/84], [94mLoss[0m : 1.77747
[1mStep[0m  [32/84], [94mLoss[0m : 1.73445
[1mStep[0m  [40/84], [94mLoss[0m : 1.80045
[1mStep[0m  [48/84], [94mLoss[0m : 1.67406
[1mStep[0m  [56/84], [94mLoss[0m : 1.64625
[1mStep[0m  [64/84], [94mLoss[0m : 1.77988
[1mStep[0m  [72/84], [94mLoss[0m : 1.72050
[1mStep[0m  [80/84], [94mLoss[0m : 1.58032

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.737, [92mTest[0m: 2.453, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.51548
[1mStep[0m  [8/84], [94mLoss[0m : 1.80610
[1mStep[0m  [16/84], [94mLoss[0m : 1.75119
[1mStep[0m  [24/84], [94mLoss[0m : 1.67624
[1mStep[0m  [32/84], [94mLoss[0m : 1.79697
[1mStep[0m  [40/84], [94mLoss[0m : 1.85425
[1mStep[0m  [48/84], [94mLoss[0m : 1.55046
[1mStep[0m  [56/84], [94mLoss[0m : 2.01722
[1mStep[0m  [64/84], [94mLoss[0m : 1.90599
[1mStep[0m  [72/84], [94mLoss[0m : 1.60454
[1mStep[0m  [80/84], [94mLoss[0m : 1.68087

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.744, [92mTest[0m: 2.559, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.516
====================================

Phase 2 - Evaluation MAE:  2.5155982971191406
MAE score P1       2.311376
MAE score P2       2.515598
loss                1.73709
learning_rate      0.002575
batch_size              128
hidden_sizes      [200, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.5
weight_decay          0.001
Name: 9, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/21], [94mLoss[0m : 11.11208
[1mStep[0m  [2/21], [94mLoss[0m : 10.60493
[1mStep[0m  [4/21], [94mLoss[0m : 10.19710
[1mStep[0m  [6/21], [94mLoss[0m : 10.05342
[1mStep[0m  [8/21], [94mLoss[0m : 9.58314
[1mStep[0m  [10/21], [94mLoss[0m : 9.34285
[1mStep[0m  [12/21], [94mLoss[0m : 8.68619
[1mStep[0m  [14/21], [94mLoss[0m : 8.24335
[1mStep[0m  [16/21], [94mLoss[0m : 7.80200
[1mStep[0m  [18/21], [94mLoss[0m : 7.80239
[1mStep[0m  [20/21], [94mLoss[0m : 7.40919

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.165, [92mTest[0m: 10.876, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.85656
[1mStep[0m  [2/21], [94mLoss[0m : 6.56695
[1mStep[0m  [4/21], [94mLoss[0m : 6.07386
[1mStep[0m  [6/21], [94mLoss[0m : 6.13552
[1mStep[0m  [8/21], [94mLoss[0m : 5.67074
[1mStep[0m  [10/21], [94mLoss[0m : 5.60644
[1mStep[0m  [12/21], [94mLoss[0m : 5.37018
[1mStep[0m  [14/21], [94mLoss[0m : 5.29984
[1mStep[0m  [16/21], [94mLoss[0m : 4.82590
[1mStep[0m  [18/21], [94mLoss[0m : 4.65943
[1mStep[0m  [20/21], [94mLoss[0m : 4.18406

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 5.568, [92mTest[0m: 8.666, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.33786
[1mStep[0m  [2/21], [94mLoss[0m : 4.05440
[1mStep[0m  [4/21], [94mLoss[0m : 3.75405
[1mStep[0m  [6/21], [94mLoss[0m : 3.63990
[1mStep[0m  [8/21], [94mLoss[0m : 3.78853
[1mStep[0m  [10/21], [94mLoss[0m : 3.47959
[1mStep[0m  [12/21], [94mLoss[0m : 3.30842
[1mStep[0m  [14/21], [94mLoss[0m : 3.24663
[1mStep[0m  [16/21], [94mLoss[0m : 2.96472
[1mStep[0m  [18/21], [94mLoss[0m : 2.98035
[1mStep[0m  [20/21], [94mLoss[0m : 2.80309

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.469, [92mTest[0m: 5.382, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.97839
[1mStep[0m  [2/21], [94mLoss[0m : 2.70222
[1mStep[0m  [4/21], [94mLoss[0m : 2.99473
[1mStep[0m  [6/21], [94mLoss[0m : 3.09449
[1mStep[0m  [8/21], [94mLoss[0m : 2.81456
[1mStep[0m  [10/21], [94mLoss[0m : 2.70384
[1mStep[0m  [12/21], [94mLoss[0m : 2.89736
[1mStep[0m  [14/21], [94mLoss[0m : 2.64648
[1mStep[0m  [16/21], [94mLoss[0m : 2.88944
[1mStep[0m  [18/21], [94mLoss[0m : 2.75922
[1mStep[0m  [20/21], [94mLoss[0m : 2.80354

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.831, [92mTest[0m: 3.415, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.55230
[1mStep[0m  [2/21], [94mLoss[0m : 2.71799
[1mStep[0m  [4/21], [94mLoss[0m : 2.58660
[1mStep[0m  [6/21], [94mLoss[0m : 2.77141
[1mStep[0m  [8/21], [94mLoss[0m : 2.73997
[1mStep[0m  [10/21], [94mLoss[0m : 2.59944
[1mStep[0m  [12/21], [94mLoss[0m : 2.63435
[1mStep[0m  [14/21], [94mLoss[0m : 2.65099
[1mStep[0m  [16/21], [94mLoss[0m : 2.73526
[1mStep[0m  [18/21], [94mLoss[0m : 2.69668
[1mStep[0m  [20/21], [94mLoss[0m : 2.79710

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.703, [92mTest[0m: 2.953, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.61312
[1mStep[0m  [2/21], [94mLoss[0m : 2.69344
[1mStep[0m  [4/21], [94mLoss[0m : 2.64006
[1mStep[0m  [6/21], [94mLoss[0m : 2.83531
[1mStep[0m  [8/21], [94mLoss[0m : 2.77095
[1mStep[0m  [10/21], [94mLoss[0m : 2.44469
[1mStep[0m  [12/21], [94mLoss[0m : 2.54632
[1mStep[0m  [14/21], [94mLoss[0m : 2.92325
[1mStep[0m  [16/21], [94mLoss[0m : 2.50360
[1mStep[0m  [18/21], [94mLoss[0m : 2.64237
[1mStep[0m  [20/21], [94mLoss[0m : 2.67171

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.649, [92mTest[0m: 2.808, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.76088
[1mStep[0m  [2/21], [94mLoss[0m : 2.65034
[1mStep[0m  [4/21], [94mLoss[0m : 2.63081
[1mStep[0m  [6/21], [94mLoss[0m : 2.57385
[1mStep[0m  [8/21], [94mLoss[0m : 2.53804
[1mStep[0m  [10/21], [94mLoss[0m : 2.73918
[1mStep[0m  [12/21], [94mLoss[0m : 2.49569
[1mStep[0m  [14/21], [94mLoss[0m : 2.72020
[1mStep[0m  [16/21], [94mLoss[0m : 2.72987
[1mStep[0m  [18/21], [94mLoss[0m : 2.63652
[1mStep[0m  [20/21], [94mLoss[0m : 2.70378

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.641, [92mTest[0m: 2.713, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52087
[1mStep[0m  [2/21], [94mLoss[0m : 2.71920
[1mStep[0m  [4/21], [94mLoss[0m : 2.55960
[1mStep[0m  [6/21], [94mLoss[0m : 2.71749
[1mStep[0m  [8/21], [94mLoss[0m : 2.63252
[1mStep[0m  [10/21], [94mLoss[0m : 2.56033
[1mStep[0m  [12/21], [94mLoss[0m : 2.57225
[1mStep[0m  [14/21], [94mLoss[0m : 2.43499
[1mStep[0m  [16/21], [94mLoss[0m : 2.68015
[1mStep[0m  [18/21], [94mLoss[0m : 2.64154
[1mStep[0m  [20/21], [94mLoss[0m : 2.72863

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.606, [92mTest[0m: 2.689, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56979
[1mStep[0m  [2/21], [94mLoss[0m : 2.67817
[1mStep[0m  [4/21], [94mLoss[0m : 2.60110
[1mStep[0m  [6/21], [94mLoss[0m : 2.60044
[1mStep[0m  [8/21], [94mLoss[0m : 2.60522
[1mStep[0m  [10/21], [94mLoss[0m : 2.49935
[1mStep[0m  [12/21], [94mLoss[0m : 2.69237
[1mStep[0m  [14/21], [94mLoss[0m : 2.53809
[1mStep[0m  [16/21], [94mLoss[0m : 2.40349
[1mStep[0m  [18/21], [94mLoss[0m : 2.53312
[1mStep[0m  [20/21], [94mLoss[0m : 2.54585

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.605, [92mTest[0m: 2.653, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.74611
[1mStep[0m  [2/21], [94mLoss[0m : 2.49611
[1mStep[0m  [4/21], [94mLoss[0m : 2.61794
[1mStep[0m  [6/21], [94mLoss[0m : 2.67125
[1mStep[0m  [8/21], [94mLoss[0m : 2.45638
[1mStep[0m  [10/21], [94mLoss[0m : 2.50458
[1mStep[0m  [12/21], [94mLoss[0m : 2.52936
[1mStep[0m  [14/21], [94mLoss[0m : 2.59541
[1mStep[0m  [16/21], [94mLoss[0m : 2.64856
[1mStep[0m  [18/21], [94mLoss[0m : 2.72494
[1mStep[0m  [20/21], [94mLoss[0m : 2.57833

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.598, [92mTest[0m: 2.642, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52929
[1mStep[0m  [2/21], [94mLoss[0m : 2.66593
[1mStep[0m  [4/21], [94mLoss[0m : 2.78127
[1mStep[0m  [6/21], [94mLoss[0m : 2.58731
[1mStep[0m  [8/21], [94mLoss[0m : 2.69533
[1mStep[0m  [10/21], [94mLoss[0m : 2.61321
[1mStep[0m  [12/21], [94mLoss[0m : 2.69926
[1mStep[0m  [14/21], [94mLoss[0m : 2.56087
[1mStep[0m  [16/21], [94mLoss[0m : 2.56231
[1mStep[0m  [18/21], [94mLoss[0m : 2.50501
[1mStep[0m  [20/21], [94mLoss[0m : 2.48897

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.577, [92mTest[0m: 2.650, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42844
[1mStep[0m  [2/21], [94mLoss[0m : 2.46143
[1mStep[0m  [4/21], [94mLoss[0m : 2.55796
[1mStep[0m  [6/21], [94mLoss[0m : 2.58467
[1mStep[0m  [8/21], [94mLoss[0m : 2.67730
[1mStep[0m  [10/21], [94mLoss[0m : 2.39796
[1mStep[0m  [12/21], [94mLoss[0m : 2.65501
[1mStep[0m  [14/21], [94mLoss[0m : 2.51672
[1mStep[0m  [16/21], [94mLoss[0m : 2.42577
[1mStep[0m  [18/21], [94mLoss[0m : 2.57901
[1mStep[0m  [20/21], [94mLoss[0m : 2.50916

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.544, [92mTest[0m: 2.617, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56318
[1mStep[0m  [2/21], [94mLoss[0m : 2.70808
[1mStep[0m  [4/21], [94mLoss[0m : 2.56442
[1mStep[0m  [6/21], [94mLoss[0m : 2.53230
[1mStep[0m  [8/21], [94mLoss[0m : 2.44379
[1mStep[0m  [10/21], [94mLoss[0m : 2.49803
[1mStep[0m  [12/21], [94mLoss[0m : 2.57779
[1mStep[0m  [14/21], [94mLoss[0m : 2.60106
[1mStep[0m  [16/21], [94mLoss[0m : 2.33099
[1mStep[0m  [18/21], [94mLoss[0m : 2.53895
[1mStep[0m  [20/21], [94mLoss[0m : 2.57304

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.561, [92mTest[0m: 2.620, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.57133
[1mStep[0m  [2/21], [94mLoss[0m : 2.56524
[1mStep[0m  [4/21], [94mLoss[0m : 2.46939
[1mStep[0m  [6/21], [94mLoss[0m : 2.49772
[1mStep[0m  [8/21], [94mLoss[0m : 2.35046
[1mStep[0m  [10/21], [94mLoss[0m : 2.55088
[1mStep[0m  [12/21], [94mLoss[0m : 2.26992
[1mStep[0m  [14/21], [94mLoss[0m : 2.63342
[1mStep[0m  [16/21], [94mLoss[0m : 2.59430
[1mStep[0m  [18/21], [94mLoss[0m : 2.69352
[1mStep[0m  [20/21], [94mLoss[0m : 2.51079

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.534, [92mTest[0m: 2.617, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48162
[1mStep[0m  [2/21], [94mLoss[0m : 2.35973
[1mStep[0m  [4/21], [94mLoss[0m : 2.60687
[1mStep[0m  [6/21], [94mLoss[0m : 2.50721
[1mStep[0m  [8/21], [94mLoss[0m : 2.46789
[1mStep[0m  [10/21], [94mLoss[0m : 2.62172
[1mStep[0m  [12/21], [94mLoss[0m : 2.51150
[1mStep[0m  [14/21], [94mLoss[0m : 2.48142
[1mStep[0m  [16/21], [94mLoss[0m : 2.58100
[1mStep[0m  [18/21], [94mLoss[0m : 2.57372
[1mStep[0m  [20/21], [94mLoss[0m : 2.47311

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.531, [92mTest[0m: 2.582, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56774
[1mStep[0m  [2/21], [94mLoss[0m : 2.62736
[1mStep[0m  [4/21], [94mLoss[0m : 2.51416
[1mStep[0m  [6/21], [94mLoss[0m : 2.46118
[1mStep[0m  [8/21], [94mLoss[0m : 2.52662
[1mStep[0m  [10/21], [94mLoss[0m : 2.42201
[1mStep[0m  [12/21], [94mLoss[0m : 2.59296
[1mStep[0m  [14/21], [94mLoss[0m : 2.50223
[1mStep[0m  [16/21], [94mLoss[0m : 2.49416
[1mStep[0m  [18/21], [94mLoss[0m : 2.49628
[1mStep[0m  [20/21], [94mLoss[0m : 2.47508

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.538, [92mTest[0m: 2.596, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56070
[1mStep[0m  [2/21], [94mLoss[0m : 2.52990
[1mStep[0m  [4/21], [94mLoss[0m : 2.51398
[1mStep[0m  [6/21], [94mLoss[0m : 2.53657
[1mStep[0m  [8/21], [94mLoss[0m : 2.70320
[1mStep[0m  [10/21], [94mLoss[0m : 2.50014
[1mStep[0m  [12/21], [94mLoss[0m : 2.59808
[1mStep[0m  [14/21], [94mLoss[0m : 2.54968
[1mStep[0m  [16/21], [94mLoss[0m : 2.56831
[1mStep[0m  [18/21], [94mLoss[0m : 2.51031
[1mStep[0m  [20/21], [94mLoss[0m : 2.61162

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.550, [92mTest[0m: 2.610, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.62200
[1mStep[0m  [2/21], [94mLoss[0m : 2.43485
[1mStep[0m  [4/21], [94mLoss[0m : 2.54178
[1mStep[0m  [6/21], [94mLoss[0m : 2.45835
[1mStep[0m  [8/21], [94mLoss[0m : 2.54624
[1mStep[0m  [10/21], [94mLoss[0m : 2.57413
[1mStep[0m  [12/21], [94mLoss[0m : 2.52597
[1mStep[0m  [14/21], [94mLoss[0m : 2.53776
[1mStep[0m  [16/21], [94mLoss[0m : 2.52241
[1mStep[0m  [18/21], [94mLoss[0m : 2.40029
[1mStep[0m  [20/21], [94mLoss[0m : 2.45388

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.620, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.37468
[1mStep[0m  [2/21], [94mLoss[0m : 2.57524
[1mStep[0m  [4/21], [94mLoss[0m : 2.50948
[1mStep[0m  [6/21], [94mLoss[0m : 2.51047
[1mStep[0m  [8/21], [94mLoss[0m : 2.48054
[1mStep[0m  [10/21], [94mLoss[0m : 2.55709
[1mStep[0m  [12/21], [94mLoss[0m : 2.48046
[1mStep[0m  [14/21], [94mLoss[0m : 2.61933
[1mStep[0m  [16/21], [94mLoss[0m : 2.45946
[1mStep[0m  [18/21], [94mLoss[0m : 2.57733
[1mStep[0m  [20/21], [94mLoss[0m : 2.47849

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.575, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56766
[1mStep[0m  [2/21], [94mLoss[0m : 2.41611
[1mStep[0m  [4/21], [94mLoss[0m : 2.60754
[1mStep[0m  [6/21], [94mLoss[0m : 2.65118
[1mStep[0m  [8/21], [94mLoss[0m : 2.55707
[1mStep[0m  [10/21], [94mLoss[0m : 2.43528
[1mStep[0m  [12/21], [94mLoss[0m : 2.47612
[1mStep[0m  [14/21], [94mLoss[0m : 2.44648
[1mStep[0m  [16/21], [94mLoss[0m : 2.47775
[1mStep[0m  [18/21], [94mLoss[0m : 2.51465
[1mStep[0m  [20/21], [94mLoss[0m : 2.64291

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.526, [92mTest[0m: 2.590, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.45780
[1mStep[0m  [2/21], [94mLoss[0m : 2.61843
[1mStep[0m  [4/21], [94mLoss[0m : 2.50195
[1mStep[0m  [6/21], [94mLoss[0m : 2.62129
[1mStep[0m  [8/21], [94mLoss[0m : 2.36598
[1mStep[0m  [10/21], [94mLoss[0m : 2.47819
[1mStep[0m  [12/21], [94mLoss[0m : 2.37330
[1mStep[0m  [14/21], [94mLoss[0m : 2.36671
[1mStep[0m  [16/21], [94mLoss[0m : 2.50889
[1mStep[0m  [18/21], [94mLoss[0m : 2.65425
[1mStep[0m  [20/21], [94mLoss[0m : 2.76951

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.529, [92mTest[0m: 2.559, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43205
[1mStep[0m  [2/21], [94mLoss[0m : 2.34402
[1mStep[0m  [4/21], [94mLoss[0m : 2.58415
[1mStep[0m  [6/21], [94mLoss[0m : 2.54166
[1mStep[0m  [8/21], [94mLoss[0m : 2.58574
[1mStep[0m  [10/21], [94mLoss[0m : 2.58115
[1mStep[0m  [12/21], [94mLoss[0m : 2.52709
[1mStep[0m  [14/21], [94mLoss[0m : 2.57047
[1mStep[0m  [16/21], [94mLoss[0m : 2.51858
[1mStep[0m  [18/21], [94mLoss[0m : 2.60937
[1mStep[0m  [20/21], [94mLoss[0m : 2.39113

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.575, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.49213
[1mStep[0m  [2/21], [94mLoss[0m : 2.39323
[1mStep[0m  [4/21], [94mLoss[0m : 2.38137
[1mStep[0m  [6/21], [94mLoss[0m : 2.35794
[1mStep[0m  [8/21], [94mLoss[0m : 2.43803
[1mStep[0m  [10/21], [94mLoss[0m : 2.49731
[1mStep[0m  [12/21], [94mLoss[0m : 2.52857
[1mStep[0m  [14/21], [94mLoss[0m : 2.48612
[1mStep[0m  [16/21], [94mLoss[0m : 2.50084
[1mStep[0m  [18/21], [94mLoss[0m : 2.39783
[1mStep[0m  [20/21], [94mLoss[0m : 2.57706

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.590, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.59684
[1mStep[0m  [2/21], [94mLoss[0m : 2.53930
[1mStep[0m  [4/21], [94mLoss[0m : 2.53522
[1mStep[0m  [6/21], [94mLoss[0m : 2.35756
[1mStep[0m  [8/21], [94mLoss[0m : 2.52966
[1mStep[0m  [10/21], [94mLoss[0m : 2.46695
[1mStep[0m  [12/21], [94mLoss[0m : 2.37823
[1mStep[0m  [14/21], [94mLoss[0m : 2.44933
[1mStep[0m  [16/21], [94mLoss[0m : 2.54036
[1mStep[0m  [18/21], [94mLoss[0m : 2.42293
[1mStep[0m  [20/21], [94mLoss[0m : 2.66689

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.570, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53122
[1mStep[0m  [2/21], [94mLoss[0m : 2.46335
[1mStep[0m  [4/21], [94mLoss[0m : 2.31765
[1mStep[0m  [6/21], [94mLoss[0m : 2.68976
[1mStep[0m  [8/21], [94mLoss[0m : 2.49581
[1mStep[0m  [10/21], [94mLoss[0m : 2.51368
[1mStep[0m  [12/21], [94mLoss[0m : 2.65619
[1mStep[0m  [14/21], [94mLoss[0m : 2.54018
[1mStep[0m  [16/21], [94mLoss[0m : 2.49076
[1mStep[0m  [18/21], [94mLoss[0m : 2.44089
[1mStep[0m  [20/21], [94mLoss[0m : 2.38071

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.573, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51578
[1mStep[0m  [2/21], [94mLoss[0m : 2.51982
[1mStep[0m  [4/21], [94mLoss[0m : 2.45352
[1mStep[0m  [6/21], [94mLoss[0m : 2.49595
[1mStep[0m  [8/21], [94mLoss[0m : 2.59347
[1mStep[0m  [10/21], [94mLoss[0m : 2.59887
[1mStep[0m  [12/21], [94mLoss[0m : 2.45902
[1mStep[0m  [14/21], [94mLoss[0m : 2.61416
[1mStep[0m  [16/21], [94mLoss[0m : 2.44861
[1mStep[0m  [18/21], [94mLoss[0m : 2.57343
[1mStep[0m  [20/21], [94mLoss[0m : 2.55952

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.510, [92mTest[0m: 2.568, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54355
[1mStep[0m  [2/21], [94mLoss[0m : 2.54564
[1mStep[0m  [4/21], [94mLoss[0m : 2.54836
[1mStep[0m  [6/21], [94mLoss[0m : 2.42688
[1mStep[0m  [8/21], [94mLoss[0m : 2.38791
[1mStep[0m  [10/21], [94mLoss[0m : 2.54160
[1mStep[0m  [12/21], [94mLoss[0m : 2.54069
[1mStep[0m  [14/21], [94mLoss[0m : 2.38786
[1mStep[0m  [16/21], [94mLoss[0m : 2.74025
[1mStep[0m  [18/21], [94mLoss[0m : 2.54418
[1mStep[0m  [20/21], [94mLoss[0m : 2.46813

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.561, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44190
[1mStep[0m  [2/21], [94mLoss[0m : 2.37743
[1mStep[0m  [4/21], [94mLoss[0m : 2.45617
[1mStep[0m  [6/21], [94mLoss[0m : 2.50337
[1mStep[0m  [8/21], [94mLoss[0m : 2.47735
[1mStep[0m  [10/21], [94mLoss[0m : 2.50523
[1mStep[0m  [12/21], [94mLoss[0m : 2.46891
[1mStep[0m  [14/21], [94mLoss[0m : 2.53597
[1mStep[0m  [16/21], [94mLoss[0m : 2.65359
[1mStep[0m  [18/21], [94mLoss[0m : 2.68353
[1mStep[0m  [20/21], [94mLoss[0m : 2.53900

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.554, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.34973
[1mStep[0m  [2/21], [94mLoss[0m : 2.62242
[1mStep[0m  [4/21], [94mLoss[0m : 2.52314
[1mStep[0m  [6/21], [94mLoss[0m : 2.47762
[1mStep[0m  [8/21], [94mLoss[0m : 2.52412
[1mStep[0m  [10/21], [94mLoss[0m : 2.57613
[1mStep[0m  [12/21], [94mLoss[0m : 2.50356
[1mStep[0m  [14/21], [94mLoss[0m : 2.52403
[1mStep[0m  [16/21], [94mLoss[0m : 2.71030
[1mStep[0m  [18/21], [94mLoss[0m : 2.45859
[1mStep[0m  [20/21], [94mLoss[0m : 2.34735

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.561, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.59514
[1mStep[0m  [2/21], [94mLoss[0m : 2.41197
[1mStep[0m  [4/21], [94mLoss[0m : 2.48002
[1mStep[0m  [6/21], [94mLoss[0m : 2.43863
[1mStep[0m  [8/21], [94mLoss[0m : 2.46174
[1mStep[0m  [10/21], [94mLoss[0m : 2.34678
[1mStep[0m  [12/21], [94mLoss[0m : 2.56204
[1mStep[0m  [14/21], [94mLoss[0m : 2.43292
[1mStep[0m  [16/21], [94mLoss[0m : 2.41704
[1mStep[0m  [18/21], [94mLoss[0m : 2.54480
[1mStep[0m  [20/21], [94mLoss[0m : 2.54344

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.544, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.565
====================================

Phase 1 - Evaluation MAE:  2.565485511507307
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/21], [94mLoss[0m : 2.54505
[1mStep[0m  [2/21], [94mLoss[0m : 2.43774
[1mStep[0m  [4/21], [94mLoss[0m : 2.55854
[1mStep[0m  [6/21], [94mLoss[0m : 2.47550
[1mStep[0m  [8/21], [94mLoss[0m : 2.54841
[1mStep[0m  [10/21], [94mLoss[0m : 2.50513
[1mStep[0m  [12/21], [94mLoss[0m : 2.61013
[1mStep[0m  [14/21], [94mLoss[0m : 2.28916
[1mStep[0m  [16/21], [94mLoss[0m : 2.44763
[1mStep[0m  [18/21], [94mLoss[0m : 2.51019
[1mStep[0m  [20/21], [94mLoss[0m : 2.50650

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.566, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.39941
[1mStep[0m  [2/21], [94mLoss[0m : 2.59117
[1mStep[0m  [4/21], [94mLoss[0m : 2.36796
[1mStep[0m  [6/21], [94mLoss[0m : 2.66999
[1mStep[0m  [8/21], [94mLoss[0m : 2.51176
[1mStep[0m  [10/21], [94mLoss[0m : 2.50083
[1mStep[0m  [12/21], [94mLoss[0m : 2.57987
[1mStep[0m  [14/21], [94mLoss[0m : 2.49704
[1mStep[0m  [16/21], [94mLoss[0m : 2.53644
[1mStep[0m  [18/21], [94mLoss[0m : 2.51607
[1mStep[0m  [20/21], [94mLoss[0m : 2.29822

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.522, [92mTest[0m: 3.234, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46170
[1mStep[0m  [2/21], [94mLoss[0m : 2.38218
[1mStep[0m  [4/21], [94mLoss[0m : 2.61085
[1mStep[0m  [6/21], [94mLoss[0m : 2.50433
[1mStep[0m  [8/21], [94mLoss[0m : 2.63831
[1mStep[0m  [10/21], [94mLoss[0m : 2.55786
[1mStep[0m  [12/21], [94mLoss[0m : 2.40813
[1mStep[0m  [14/21], [94mLoss[0m : 2.25978
[1mStep[0m  [16/21], [94mLoss[0m : 2.54219
[1mStep[0m  [18/21], [94mLoss[0m : 2.36224
[1mStep[0m  [20/21], [94mLoss[0m : 2.50950

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.651, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.76519
[1mStep[0m  [2/21], [94mLoss[0m : 2.60038
[1mStep[0m  [4/21], [94mLoss[0m : 2.49472
[1mStep[0m  [6/21], [94mLoss[0m : 2.51343
[1mStep[0m  [8/21], [94mLoss[0m : 2.38346
[1mStep[0m  [10/21], [94mLoss[0m : 2.46956
[1mStep[0m  [12/21], [94mLoss[0m : 2.54206
[1mStep[0m  [14/21], [94mLoss[0m : 2.37074
[1mStep[0m  [16/21], [94mLoss[0m : 2.49624
[1mStep[0m  [18/21], [94mLoss[0m : 2.53088
[1mStep[0m  [20/21], [94mLoss[0m : 2.50078

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.530, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.36047
[1mStep[0m  [2/21], [94mLoss[0m : 2.31295
[1mStep[0m  [4/21], [94mLoss[0m : 2.46439
[1mStep[0m  [6/21], [94mLoss[0m : 2.41781
[1mStep[0m  [8/21], [94mLoss[0m : 2.46284
[1mStep[0m  [10/21], [94mLoss[0m : 2.55887
[1mStep[0m  [12/21], [94mLoss[0m : 2.42714
[1mStep[0m  [14/21], [94mLoss[0m : 2.50171
[1mStep[0m  [16/21], [94mLoss[0m : 2.41390
[1mStep[0m  [18/21], [94mLoss[0m : 2.51068
[1mStep[0m  [20/21], [94mLoss[0m : 2.45872

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.474, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42498
[1mStep[0m  [2/21], [94mLoss[0m : 2.47809
[1mStep[0m  [4/21], [94mLoss[0m : 2.46499
[1mStep[0m  [6/21], [94mLoss[0m : 2.31672
[1mStep[0m  [8/21], [94mLoss[0m : 2.48597
[1mStep[0m  [10/21], [94mLoss[0m : 2.27836
[1mStep[0m  [12/21], [94mLoss[0m : 2.41796
[1mStep[0m  [14/21], [94mLoss[0m : 2.45423
[1mStep[0m  [16/21], [94mLoss[0m : 2.28468
[1mStep[0m  [18/21], [94mLoss[0m : 2.51232
[1mStep[0m  [20/21], [94mLoss[0m : 2.31722

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.431, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51267
[1mStep[0m  [2/21], [94mLoss[0m : 2.40683
[1mStep[0m  [4/21], [94mLoss[0m : 2.40084
[1mStep[0m  [6/21], [94mLoss[0m : 2.48030
[1mStep[0m  [8/21], [94mLoss[0m : 2.58973
[1mStep[0m  [10/21], [94mLoss[0m : 2.57617
[1mStep[0m  [12/21], [94mLoss[0m : 2.40991
[1mStep[0m  [14/21], [94mLoss[0m : 2.41327
[1mStep[0m  [16/21], [94mLoss[0m : 2.34290
[1mStep[0m  [18/21], [94mLoss[0m : 2.33136
[1mStep[0m  [20/21], [94mLoss[0m : 2.39523

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.417, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48673
[1mStep[0m  [2/21], [94mLoss[0m : 2.53737
[1mStep[0m  [4/21], [94mLoss[0m : 2.42599
[1mStep[0m  [6/21], [94mLoss[0m : 2.45680
[1mStep[0m  [8/21], [94mLoss[0m : 2.37581
[1mStep[0m  [10/21], [94mLoss[0m : 2.28600
[1mStep[0m  [12/21], [94mLoss[0m : 2.46583
[1mStep[0m  [14/21], [94mLoss[0m : 2.40450
[1mStep[0m  [16/21], [94mLoss[0m : 2.41489
[1mStep[0m  [18/21], [94mLoss[0m : 2.43084
[1mStep[0m  [20/21], [94mLoss[0m : 2.50302

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.428, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52841
[1mStep[0m  [2/21], [94mLoss[0m : 2.49283
[1mStep[0m  [4/21], [94mLoss[0m : 2.48479
[1mStep[0m  [6/21], [94mLoss[0m : 2.47509
[1mStep[0m  [8/21], [94mLoss[0m : 2.54540
[1mStep[0m  [10/21], [94mLoss[0m : 2.34187
[1mStep[0m  [12/21], [94mLoss[0m : 2.35866
[1mStep[0m  [14/21], [94mLoss[0m : 2.48917
[1mStep[0m  [16/21], [94mLoss[0m : 2.44487
[1mStep[0m  [18/21], [94mLoss[0m : 2.54747
[1mStep[0m  [20/21], [94mLoss[0m : 2.38816

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.416, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.45327
[1mStep[0m  [2/21], [94mLoss[0m : 2.42770
[1mStep[0m  [4/21], [94mLoss[0m : 2.49287
[1mStep[0m  [6/21], [94mLoss[0m : 2.31212
[1mStep[0m  [8/21], [94mLoss[0m : 2.45553
[1mStep[0m  [10/21], [94mLoss[0m : 2.26925
[1mStep[0m  [12/21], [94mLoss[0m : 2.43778
[1mStep[0m  [14/21], [94mLoss[0m : 2.40559
[1mStep[0m  [16/21], [94mLoss[0m : 2.43029
[1mStep[0m  [18/21], [94mLoss[0m : 2.41400
[1mStep[0m  [20/21], [94mLoss[0m : 2.39239

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.394, [92mTest[0m: 2.435, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44813
[1mStep[0m  [2/21], [94mLoss[0m : 2.17866
[1mStep[0m  [4/21], [94mLoss[0m : 2.40353
[1mStep[0m  [6/21], [94mLoss[0m : 2.45456
[1mStep[0m  [8/21], [94mLoss[0m : 2.37553
[1mStep[0m  [10/21], [94mLoss[0m : 2.46033
[1mStep[0m  [12/21], [94mLoss[0m : 2.25107
[1mStep[0m  [14/21], [94mLoss[0m : 2.31885
[1mStep[0m  [16/21], [94mLoss[0m : 2.32869
[1mStep[0m  [18/21], [94mLoss[0m : 2.36338
[1mStep[0m  [20/21], [94mLoss[0m : 2.31461

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.369, [92mTest[0m: 2.440, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.22237
[1mStep[0m  [2/21], [94mLoss[0m : 2.40359
[1mStep[0m  [4/21], [94mLoss[0m : 2.27872
[1mStep[0m  [6/21], [94mLoss[0m : 2.38174
[1mStep[0m  [8/21], [94mLoss[0m : 2.33496
[1mStep[0m  [10/21], [94mLoss[0m : 2.36240
[1mStep[0m  [12/21], [94mLoss[0m : 2.36200
[1mStep[0m  [14/21], [94mLoss[0m : 2.33779
[1mStep[0m  [16/21], [94mLoss[0m : 2.41179
[1mStep[0m  [18/21], [94mLoss[0m : 2.38598
[1mStep[0m  [20/21], [94mLoss[0m : 2.38420

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.368, [92mTest[0m: 2.456, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.34595
[1mStep[0m  [2/21], [94mLoss[0m : 2.41792
[1mStep[0m  [4/21], [94mLoss[0m : 2.52398
[1mStep[0m  [6/21], [94mLoss[0m : 2.19358
[1mStep[0m  [8/21], [94mLoss[0m : 2.30630
[1mStep[0m  [10/21], [94mLoss[0m : 2.39265
[1mStep[0m  [12/21], [94mLoss[0m : 2.41243
[1mStep[0m  [14/21], [94mLoss[0m : 2.23689
[1mStep[0m  [16/21], [94mLoss[0m : 2.26707
[1mStep[0m  [18/21], [94mLoss[0m : 2.29596
[1mStep[0m  [20/21], [94mLoss[0m : 2.49040

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.362, [92mTest[0m: 2.502, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42378
[1mStep[0m  [2/21], [94mLoss[0m : 2.25340
[1mStep[0m  [4/21], [94mLoss[0m : 2.21454
[1mStep[0m  [6/21], [94mLoss[0m : 2.24484
[1mStep[0m  [8/21], [94mLoss[0m : 2.25200
[1mStep[0m  [10/21], [94mLoss[0m : 2.30286
[1mStep[0m  [12/21], [94mLoss[0m : 2.50569
[1mStep[0m  [14/21], [94mLoss[0m : 2.33967
[1mStep[0m  [16/21], [94mLoss[0m : 2.45358
[1mStep[0m  [18/21], [94mLoss[0m : 2.28418
[1mStep[0m  [20/21], [94mLoss[0m : 2.44535

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.328, [92mTest[0m: 2.467, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.29323
[1mStep[0m  [2/21], [94mLoss[0m : 2.33081
[1mStep[0m  [4/21], [94mLoss[0m : 2.14829
[1mStep[0m  [6/21], [94mLoss[0m : 2.47596
[1mStep[0m  [8/21], [94mLoss[0m : 2.28196
[1mStep[0m  [10/21], [94mLoss[0m : 2.39754
[1mStep[0m  [12/21], [94mLoss[0m : 2.26631
[1mStep[0m  [14/21], [94mLoss[0m : 2.33369
[1mStep[0m  [16/21], [94mLoss[0m : 2.35234
[1mStep[0m  [18/21], [94mLoss[0m : 2.43652
[1mStep[0m  [20/21], [94mLoss[0m : 2.32110

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.323, [92mTest[0m: 2.466, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.12230
[1mStep[0m  [2/21], [94mLoss[0m : 2.30127
[1mStep[0m  [4/21], [94mLoss[0m : 2.29745
[1mStep[0m  [6/21], [94mLoss[0m : 2.32003
[1mStep[0m  [8/21], [94mLoss[0m : 2.27069
[1mStep[0m  [10/21], [94mLoss[0m : 2.40029
[1mStep[0m  [12/21], [94mLoss[0m : 2.22414
[1mStep[0m  [14/21], [94mLoss[0m : 2.38673
[1mStep[0m  [16/21], [94mLoss[0m : 2.23576
[1mStep[0m  [18/21], [94mLoss[0m : 2.26985
[1mStep[0m  [20/21], [94mLoss[0m : 2.35818

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.291, [92mTest[0m: 2.474, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.31745
[1mStep[0m  [2/21], [94mLoss[0m : 2.29326
[1mStep[0m  [4/21], [94mLoss[0m : 2.27843
[1mStep[0m  [6/21], [94mLoss[0m : 2.29364
[1mStep[0m  [8/21], [94mLoss[0m : 2.31459
[1mStep[0m  [10/21], [94mLoss[0m : 2.29845
[1mStep[0m  [12/21], [94mLoss[0m : 2.12451
[1mStep[0m  [14/21], [94mLoss[0m : 2.32984
[1mStep[0m  [16/21], [94mLoss[0m : 2.22389
[1mStep[0m  [18/21], [94mLoss[0m : 2.10512
[1mStep[0m  [20/21], [94mLoss[0m : 2.33297

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.283, [92mTest[0m: 2.420, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.29456
[1mStep[0m  [2/21], [94mLoss[0m : 2.09493
[1mStep[0m  [4/21], [94mLoss[0m : 2.23442
[1mStep[0m  [6/21], [94mLoss[0m : 2.33385
[1mStep[0m  [8/21], [94mLoss[0m : 2.25091
[1mStep[0m  [10/21], [94mLoss[0m : 2.24319
[1mStep[0m  [12/21], [94mLoss[0m : 2.27923
[1mStep[0m  [14/21], [94mLoss[0m : 2.36654
[1mStep[0m  [16/21], [94mLoss[0m : 2.19115
[1mStep[0m  [18/21], [94mLoss[0m : 2.24761
[1mStep[0m  [20/21], [94mLoss[0m : 2.35482

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.274, [92mTest[0m: 2.411, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.19844
[1mStep[0m  [2/21], [94mLoss[0m : 2.13734
[1mStep[0m  [4/21], [94mLoss[0m : 2.13785
[1mStep[0m  [6/21], [94mLoss[0m : 2.29877
[1mStep[0m  [8/21], [94mLoss[0m : 2.26364
[1mStep[0m  [10/21], [94mLoss[0m : 2.26320
[1mStep[0m  [12/21], [94mLoss[0m : 2.24088
[1mStep[0m  [14/21], [94mLoss[0m : 2.39206
[1mStep[0m  [16/21], [94mLoss[0m : 2.38972
[1mStep[0m  [18/21], [94mLoss[0m : 2.32758
[1mStep[0m  [20/21], [94mLoss[0m : 2.16631

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.249, [92mTest[0m: 2.402, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.33290
[1mStep[0m  [2/21], [94mLoss[0m : 2.33795
[1mStep[0m  [4/21], [94mLoss[0m : 2.34531
[1mStep[0m  [6/21], [94mLoss[0m : 2.25363
[1mStep[0m  [8/21], [94mLoss[0m : 2.07773
[1mStep[0m  [10/21], [94mLoss[0m : 2.12339
[1mStep[0m  [12/21], [94mLoss[0m : 2.27021
[1mStep[0m  [14/21], [94mLoss[0m : 2.11874
[1mStep[0m  [16/21], [94mLoss[0m : 2.44352
[1mStep[0m  [18/21], [94mLoss[0m : 2.38917
[1mStep[0m  [20/21], [94mLoss[0m : 2.20324

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.258, [92mTest[0m: 2.401, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.28408
[1mStep[0m  [2/21], [94mLoss[0m : 2.26567
[1mStep[0m  [4/21], [94mLoss[0m : 2.22423
[1mStep[0m  [6/21], [94mLoss[0m : 2.27868
[1mStep[0m  [8/21], [94mLoss[0m : 2.24597
[1mStep[0m  [10/21], [94mLoss[0m : 2.22771
[1mStep[0m  [12/21], [94mLoss[0m : 2.22977
[1mStep[0m  [14/21], [94mLoss[0m : 2.19900
[1mStep[0m  [16/21], [94mLoss[0m : 2.22667
[1mStep[0m  [18/21], [94mLoss[0m : 2.33040
[1mStep[0m  [20/21], [94mLoss[0m : 2.23612

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.242, [92mTest[0m: 2.411, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.17085
[1mStep[0m  [2/21], [94mLoss[0m : 2.19614
[1mStep[0m  [4/21], [94mLoss[0m : 2.21077
[1mStep[0m  [6/21], [94mLoss[0m : 2.37986
[1mStep[0m  [8/21], [94mLoss[0m : 2.05719
[1mStep[0m  [10/21], [94mLoss[0m : 2.14549
[1mStep[0m  [12/21], [94mLoss[0m : 2.18127
[1mStep[0m  [14/21], [94mLoss[0m : 2.33083
[1mStep[0m  [16/21], [94mLoss[0m : 2.31346
[1mStep[0m  [18/21], [94mLoss[0m : 2.28637
[1mStep[0m  [20/21], [94mLoss[0m : 2.29259

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.216, [92mTest[0m: 2.421, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.25681
[1mStep[0m  [2/21], [94mLoss[0m : 2.08184
[1mStep[0m  [4/21], [94mLoss[0m : 2.19903
[1mStep[0m  [6/21], [94mLoss[0m : 2.07575
[1mStep[0m  [8/21], [94mLoss[0m : 2.20715
[1mStep[0m  [10/21], [94mLoss[0m : 2.14046
[1mStep[0m  [12/21], [94mLoss[0m : 2.15237
[1mStep[0m  [14/21], [94mLoss[0m : 2.13509
[1mStep[0m  [16/21], [94mLoss[0m : 2.32231
[1mStep[0m  [18/21], [94mLoss[0m : 2.32233
[1mStep[0m  [20/21], [94mLoss[0m : 2.26704

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.190, [92mTest[0m: 2.411, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.19172
[1mStep[0m  [2/21], [94mLoss[0m : 2.25858
[1mStep[0m  [4/21], [94mLoss[0m : 2.13368
[1mStep[0m  [6/21], [94mLoss[0m : 2.19037
[1mStep[0m  [8/21], [94mLoss[0m : 2.05385
[1mStep[0m  [10/21], [94mLoss[0m : 2.13314
[1mStep[0m  [12/21], [94mLoss[0m : 2.26902
[1mStep[0m  [14/21], [94mLoss[0m : 2.13761
[1mStep[0m  [16/21], [94mLoss[0m : 2.22039
[1mStep[0m  [18/21], [94mLoss[0m : 2.20629
[1mStep[0m  [20/21], [94mLoss[0m : 2.14122

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.179, [92mTest[0m: 2.426, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.10575
[1mStep[0m  [2/21], [94mLoss[0m : 2.20545
[1mStep[0m  [4/21], [94mLoss[0m : 2.09984
[1mStep[0m  [6/21], [94mLoss[0m : 2.18961
[1mStep[0m  [8/21], [94mLoss[0m : 2.00054
[1mStep[0m  [10/21], [94mLoss[0m : 2.06401
[1mStep[0m  [12/21], [94mLoss[0m : 2.04619
[1mStep[0m  [14/21], [94mLoss[0m : 2.20745
[1mStep[0m  [16/21], [94mLoss[0m : 2.26805
[1mStep[0m  [18/21], [94mLoss[0m : 2.16181
[1mStep[0m  [20/21], [94mLoss[0m : 2.05747

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.164, [92mTest[0m: 2.403, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.29349
[1mStep[0m  [2/21], [94mLoss[0m : 2.03869
[1mStep[0m  [4/21], [94mLoss[0m : 2.02750
[1mStep[0m  [6/21], [94mLoss[0m : 2.03151
[1mStep[0m  [8/21], [94mLoss[0m : 2.19232
[1mStep[0m  [10/21], [94mLoss[0m : 2.11263
[1mStep[0m  [12/21], [94mLoss[0m : 2.16625
[1mStep[0m  [14/21], [94mLoss[0m : 2.01348
[1mStep[0m  [16/21], [94mLoss[0m : 2.15554
[1mStep[0m  [18/21], [94mLoss[0m : 2.28866
[1mStep[0m  [20/21], [94mLoss[0m : 2.19020

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.163, [92mTest[0m: 2.417, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.11263
[1mStep[0m  [2/21], [94mLoss[0m : 2.24641
[1mStep[0m  [4/21], [94mLoss[0m : 2.11159
[1mStep[0m  [6/21], [94mLoss[0m : 2.20413
[1mStep[0m  [8/21], [94mLoss[0m : 2.19498
[1mStep[0m  [10/21], [94mLoss[0m : 2.20808
[1mStep[0m  [12/21], [94mLoss[0m : 2.07785
[1mStep[0m  [14/21], [94mLoss[0m : 2.18871
[1mStep[0m  [16/21], [94mLoss[0m : 2.07782
[1mStep[0m  [18/21], [94mLoss[0m : 2.07948
[1mStep[0m  [20/21], [94mLoss[0m : 2.06955

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.148, [92mTest[0m: 2.417, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.14695
[1mStep[0m  [2/21], [94mLoss[0m : 2.15622
[1mStep[0m  [4/21], [94mLoss[0m : 2.18353
[1mStep[0m  [6/21], [94mLoss[0m : 2.10677
[1mStep[0m  [8/21], [94mLoss[0m : 2.10085
[1mStep[0m  [10/21], [94mLoss[0m : 2.19058
[1mStep[0m  [12/21], [94mLoss[0m : 2.07967
[1mStep[0m  [14/21], [94mLoss[0m : 2.10126
[1mStep[0m  [16/21], [94mLoss[0m : 2.10848
[1mStep[0m  [18/21], [94mLoss[0m : 2.14474
[1mStep[0m  [20/21], [94mLoss[0m : 2.14590

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.135, [92mTest[0m: 2.418, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.12017
[1mStep[0m  [2/21], [94mLoss[0m : 2.01196
[1mStep[0m  [4/21], [94mLoss[0m : 2.14031
[1mStep[0m  [6/21], [94mLoss[0m : 2.19948
[1mStep[0m  [8/21], [94mLoss[0m : 2.03003
[1mStep[0m  [10/21], [94mLoss[0m : 2.13451
[1mStep[0m  [12/21], [94mLoss[0m : 2.10485
[1mStep[0m  [14/21], [94mLoss[0m : 2.05671
[1mStep[0m  [16/21], [94mLoss[0m : 2.08956
[1mStep[0m  [18/21], [94mLoss[0m : 2.14103
[1mStep[0m  [20/21], [94mLoss[0m : 2.01450

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.103, [92mTest[0m: 2.433, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.17262
[1mStep[0m  [2/21], [94mLoss[0m : 2.02577
[1mStep[0m  [4/21], [94mLoss[0m : 2.13272
[1mStep[0m  [6/21], [94mLoss[0m : 2.06326
[1mStep[0m  [8/21], [94mLoss[0m : 2.06427
[1mStep[0m  [10/21], [94mLoss[0m : 2.08622
[1mStep[0m  [12/21], [94mLoss[0m : 2.04329
[1mStep[0m  [14/21], [94mLoss[0m : 2.09410
[1mStep[0m  [16/21], [94mLoss[0m : 2.16934
[1mStep[0m  [18/21], [94mLoss[0m : 2.18768
[1mStep[0m  [20/21], [94mLoss[0m : 2.07737

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.100, [92mTest[0m: 2.412, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.429
====================================

Phase 2 - Evaluation MAE:  2.4292622974940707
MAE score P1      2.565486
MAE score P2      2.429262
loss               2.09992
learning_rate     0.002575
batch_size             512
hidden_sizes         [250]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.4
momentum               0.5
weight_decay          0.01
Name: 10, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 11.15201
[1mStep[0m  [2/21], [94mLoss[0m : 10.45656
[1mStep[0m  [4/21], [94mLoss[0m : 9.65147
[1mStep[0m  [6/21], [94mLoss[0m : 8.89382
[1mStep[0m  [8/21], [94mLoss[0m : 7.30263
[1mStep[0m  [10/21], [94mLoss[0m : 6.00142
[1mStep[0m  [12/21], [94mLoss[0m : 5.06005
[1mStep[0m  [14/21], [94mLoss[0m : 4.18322
[1mStep[0m  [16/21], [94mLoss[0m : 3.66308
[1mStep[0m  [18/21], [94mLoss[0m : 3.01896
[1mStep[0m  [20/21], [94mLoss[0m : 2.94184

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.589, [92mTest[0m: 10.987, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.78451
[1mStep[0m  [2/21], [94mLoss[0m : 3.18543
[1mStep[0m  [4/21], [94mLoss[0m : 3.28618
[1mStep[0m  [6/21], [94mLoss[0m : 3.36665
[1mStep[0m  [8/21], [94mLoss[0m : 3.07263
[1mStep[0m  [10/21], [94mLoss[0m : 2.96404
[1mStep[0m  [12/21], [94mLoss[0m : 2.77680
[1mStep[0m  [14/21], [94mLoss[0m : 2.73703
[1mStep[0m  [16/21], [94mLoss[0m : 2.57834
[1mStep[0m  [18/21], [94mLoss[0m : 2.78139
[1mStep[0m  [20/21], [94mLoss[0m : 2.60400

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.928, [92mTest[0m: 4.085, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56613
[1mStep[0m  [2/21], [94mLoss[0m : 2.74417
[1mStep[0m  [4/21], [94mLoss[0m : 2.59398
[1mStep[0m  [6/21], [94mLoss[0m : 2.51726
[1mStep[0m  [8/21], [94mLoss[0m : 2.46564
[1mStep[0m  [10/21], [94mLoss[0m : 2.54134
[1mStep[0m  [12/21], [94mLoss[0m : 2.45202
[1mStep[0m  [14/21], [94mLoss[0m : 2.57199
[1mStep[0m  [16/21], [94mLoss[0m : 2.55267
[1mStep[0m  [18/21], [94mLoss[0m : 2.67122
[1mStep[0m  [20/21], [94mLoss[0m : 2.34303

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.553, [92mTest[0m: 3.488, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.50909
[1mStep[0m  [2/21], [94mLoss[0m : 2.55971
[1mStep[0m  [4/21], [94mLoss[0m : 2.62310
[1mStep[0m  [6/21], [94mLoss[0m : 2.66080
[1mStep[0m  [8/21], [94mLoss[0m : 2.56395
[1mStep[0m  [10/21], [94mLoss[0m : 2.42585
[1mStep[0m  [12/21], [94mLoss[0m : 2.44332
[1mStep[0m  [14/21], [94mLoss[0m : 2.55685
[1mStep[0m  [16/21], [94mLoss[0m : 2.51895
[1mStep[0m  [18/21], [94mLoss[0m : 2.51284
[1mStep[0m  [20/21], [94mLoss[0m : 2.44692

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.529, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54379
[1mStep[0m  [2/21], [94mLoss[0m : 2.54506
[1mStep[0m  [4/21], [94mLoss[0m : 2.47728
[1mStep[0m  [6/21], [94mLoss[0m : 2.40521
[1mStep[0m  [8/21], [94mLoss[0m : 2.68482
[1mStep[0m  [10/21], [94mLoss[0m : 2.64817
[1mStep[0m  [12/21], [94mLoss[0m : 2.45596
[1mStep[0m  [14/21], [94mLoss[0m : 2.47319
[1mStep[0m  [16/21], [94mLoss[0m : 2.45083
[1mStep[0m  [18/21], [94mLoss[0m : 2.42110
[1mStep[0m  [20/21], [94mLoss[0m : 2.45657

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.636, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.57118
[1mStep[0m  [2/21], [94mLoss[0m : 2.38448
[1mStep[0m  [4/21], [94mLoss[0m : 2.44668
[1mStep[0m  [6/21], [94mLoss[0m : 2.49042
[1mStep[0m  [8/21], [94mLoss[0m : 2.42276
[1mStep[0m  [10/21], [94mLoss[0m : 2.55758
[1mStep[0m  [12/21], [94mLoss[0m : 2.42695
[1mStep[0m  [14/21], [94mLoss[0m : 2.41904
[1mStep[0m  [16/21], [94mLoss[0m : 2.45842
[1mStep[0m  [18/21], [94mLoss[0m : 2.29815
[1mStep[0m  [20/21], [94mLoss[0m : 2.41689

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.531, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56900
[1mStep[0m  [2/21], [94mLoss[0m : 2.41263
[1mStep[0m  [4/21], [94mLoss[0m : 2.32929
[1mStep[0m  [6/21], [94mLoss[0m : 2.42248
[1mStep[0m  [8/21], [94mLoss[0m : 2.44541
[1mStep[0m  [10/21], [94mLoss[0m : 2.39841
[1mStep[0m  [12/21], [94mLoss[0m : 2.46893
[1mStep[0m  [14/21], [94mLoss[0m : 2.31962
[1mStep[0m  [16/21], [94mLoss[0m : 2.42628
[1mStep[0m  [18/21], [94mLoss[0m : 2.48685
[1mStep[0m  [20/21], [94mLoss[0m : 2.35447

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.469, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.47879
[1mStep[0m  [2/21], [94mLoss[0m : 2.35646
[1mStep[0m  [4/21], [94mLoss[0m : 2.35938
[1mStep[0m  [6/21], [94mLoss[0m : 2.42653
[1mStep[0m  [8/21], [94mLoss[0m : 2.50993
[1mStep[0m  [10/21], [94mLoss[0m : 2.42212
[1mStep[0m  [12/21], [94mLoss[0m : 2.49806
[1mStep[0m  [14/21], [94mLoss[0m : 2.24271
[1mStep[0m  [16/21], [94mLoss[0m : 2.44795
[1mStep[0m  [18/21], [94mLoss[0m : 2.45176
[1mStep[0m  [20/21], [94mLoss[0m : 2.39219

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.598, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.38799
[1mStep[0m  [2/21], [94mLoss[0m : 2.36841
[1mStep[0m  [4/21], [94mLoss[0m : 2.43894
[1mStep[0m  [6/21], [94mLoss[0m : 2.43630
[1mStep[0m  [8/21], [94mLoss[0m : 2.34734
[1mStep[0m  [10/21], [94mLoss[0m : 2.46759
[1mStep[0m  [12/21], [94mLoss[0m : 2.40760
[1mStep[0m  [14/21], [94mLoss[0m : 2.38913
[1mStep[0m  [16/21], [94mLoss[0m : 2.46020
[1mStep[0m  [18/21], [94mLoss[0m : 2.35536
[1mStep[0m  [20/21], [94mLoss[0m : 2.32089

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.518, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42165
[1mStep[0m  [2/21], [94mLoss[0m : 2.44665
[1mStep[0m  [4/21], [94mLoss[0m : 2.40941
[1mStep[0m  [6/21], [94mLoss[0m : 2.42225
[1mStep[0m  [8/21], [94mLoss[0m : 2.33690
[1mStep[0m  [10/21], [94mLoss[0m : 2.29884
[1mStep[0m  [12/21], [94mLoss[0m : 2.48144
[1mStep[0m  [14/21], [94mLoss[0m : 2.51677
[1mStep[0m  [16/21], [94mLoss[0m : 2.49859
[1mStep[0m  [18/21], [94mLoss[0m : 2.40338
[1mStep[0m  [20/21], [94mLoss[0m : 2.42940

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.496, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.33327
[1mStep[0m  [2/21], [94mLoss[0m : 2.31196
[1mStep[0m  [4/21], [94mLoss[0m : 2.42835
[1mStep[0m  [6/21], [94mLoss[0m : 2.35980
[1mStep[0m  [8/21], [94mLoss[0m : 2.51104
[1mStep[0m  [10/21], [94mLoss[0m : 2.44442
[1mStep[0m  [12/21], [94mLoss[0m : 2.39373
[1mStep[0m  [14/21], [94mLoss[0m : 2.40186
[1mStep[0m  [16/21], [94mLoss[0m : 2.42754
[1mStep[0m  [18/21], [94mLoss[0m : 2.51685
[1mStep[0m  [20/21], [94mLoss[0m : 2.34317

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.390, [92mTest[0m: 2.472, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52841
[1mStep[0m  [2/21], [94mLoss[0m : 2.38569
[1mStep[0m  [4/21], [94mLoss[0m : 2.51050
[1mStep[0m  [6/21], [94mLoss[0m : 2.37494
[1mStep[0m  [8/21], [94mLoss[0m : 2.48408
[1mStep[0m  [10/21], [94mLoss[0m : 2.51976
[1mStep[0m  [12/21], [94mLoss[0m : 2.44707
[1mStep[0m  [14/21], [94mLoss[0m : 2.33188
[1mStep[0m  [16/21], [94mLoss[0m : 2.37292
[1mStep[0m  [18/21], [94mLoss[0m : 2.45044
[1mStep[0m  [20/21], [94mLoss[0m : 2.35980

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.511, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.35212
[1mStep[0m  [2/21], [94mLoss[0m : 2.34305
[1mStep[0m  [4/21], [94mLoss[0m : 2.31541
[1mStep[0m  [6/21], [94mLoss[0m : 2.42726
[1mStep[0m  [8/21], [94mLoss[0m : 2.39459
[1mStep[0m  [10/21], [94mLoss[0m : 2.41349
[1mStep[0m  [12/21], [94mLoss[0m : 2.45091
[1mStep[0m  [14/21], [94mLoss[0m : 2.45003
[1mStep[0m  [16/21], [94mLoss[0m : 2.19962
[1mStep[0m  [18/21], [94mLoss[0m : 2.47566
[1mStep[0m  [20/21], [94mLoss[0m : 2.47089

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.392, [92mTest[0m: 2.539, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.36315
[1mStep[0m  [2/21], [94mLoss[0m : 2.37069
[1mStep[0m  [4/21], [94mLoss[0m : 2.37473
[1mStep[0m  [6/21], [94mLoss[0m : 2.42786
[1mStep[0m  [8/21], [94mLoss[0m : 2.36103
[1mStep[0m  [10/21], [94mLoss[0m : 2.32193
[1mStep[0m  [12/21], [94mLoss[0m : 2.41333
[1mStep[0m  [14/21], [94mLoss[0m : 2.22227
[1mStep[0m  [16/21], [94mLoss[0m : 2.43619
[1mStep[0m  [18/21], [94mLoss[0m : 2.31307
[1mStep[0m  [20/21], [94mLoss[0m : 2.32382

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.373, [92mTest[0m: 2.513, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.32458
[1mStep[0m  [2/21], [94mLoss[0m : 2.33924
[1mStep[0m  [4/21], [94mLoss[0m : 2.33598
[1mStep[0m  [6/21], [94mLoss[0m : 2.46542
[1mStep[0m  [8/21], [94mLoss[0m : 2.39377
[1mStep[0m  [10/21], [94mLoss[0m : 2.46829
[1mStep[0m  [12/21], [94mLoss[0m : 2.25628
[1mStep[0m  [14/21], [94mLoss[0m : 2.42095
[1mStep[0m  [16/21], [94mLoss[0m : 2.39982
[1mStep[0m  [18/21], [94mLoss[0m : 2.46523
[1mStep[0m  [20/21], [94mLoss[0m : 2.35072

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.378, [92mTest[0m: 2.458, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.19386
[1mStep[0m  [2/21], [94mLoss[0m : 2.40688
[1mStep[0m  [4/21], [94mLoss[0m : 2.45657
[1mStep[0m  [6/21], [94mLoss[0m : 2.35920
[1mStep[0m  [8/21], [94mLoss[0m : 2.36411
[1mStep[0m  [10/21], [94mLoss[0m : 2.33430
[1mStep[0m  [12/21], [94mLoss[0m : 2.24064
[1mStep[0m  [14/21], [94mLoss[0m : 2.39099
[1mStep[0m  [16/21], [94mLoss[0m : 2.30266
[1mStep[0m  [18/21], [94mLoss[0m : 2.52512
[1mStep[0m  [20/21], [94mLoss[0m : 2.37346

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.365, [92mTest[0m: 2.454, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43178
[1mStep[0m  [2/21], [94mLoss[0m : 2.30416
[1mStep[0m  [4/21], [94mLoss[0m : 2.27427
[1mStep[0m  [6/21], [94mLoss[0m : 2.48592
[1mStep[0m  [8/21], [94mLoss[0m : 2.26725
[1mStep[0m  [10/21], [94mLoss[0m : 2.44007
[1mStep[0m  [12/21], [94mLoss[0m : 2.35547
[1mStep[0m  [14/21], [94mLoss[0m : 2.30879
[1mStep[0m  [16/21], [94mLoss[0m : 2.37031
[1mStep[0m  [18/21], [94mLoss[0m : 2.30199
[1mStep[0m  [20/21], [94mLoss[0m : 2.46480

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.368, [92mTest[0m: 2.466, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.27633
[1mStep[0m  [2/21], [94mLoss[0m : 2.41772
[1mStep[0m  [4/21], [94mLoss[0m : 2.27301
[1mStep[0m  [6/21], [94mLoss[0m : 2.36374
[1mStep[0m  [8/21], [94mLoss[0m : 2.34326
[1mStep[0m  [10/21], [94mLoss[0m : 2.21334
[1mStep[0m  [12/21], [94mLoss[0m : 2.39432
[1mStep[0m  [14/21], [94mLoss[0m : 2.42840
[1mStep[0m  [16/21], [94mLoss[0m : 2.30276
[1mStep[0m  [18/21], [94mLoss[0m : 2.46423
[1mStep[0m  [20/21], [94mLoss[0m : 2.42256

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.375, [92mTest[0m: 2.468, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.23107
[1mStep[0m  [2/21], [94mLoss[0m : 2.42851
[1mStep[0m  [4/21], [94mLoss[0m : 2.39935
[1mStep[0m  [6/21], [94mLoss[0m : 2.38033
[1mStep[0m  [8/21], [94mLoss[0m : 2.30534
[1mStep[0m  [10/21], [94mLoss[0m : 2.37192
[1mStep[0m  [12/21], [94mLoss[0m : 2.30709
[1mStep[0m  [14/21], [94mLoss[0m : 2.24873
[1mStep[0m  [16/21], [94mLoss[0m : 2.36713
[1mStep[0m  [18/21], [94mLoss[0m : 2.30004
[1mStep[0m  [20/21], [94mLoss[0m : 2.43911

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.369, [92mTest[0m: 2.445, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43259
[1mStep[0m  [2/21], [94mLoss[0m : 2.44014
[1mStep[0m  [4/21], [94mLoss[0m : 2.30888
[1mStep[0m  [6/21], [94mLoss[0m : 2.43637
[1mStep[0m  [8/21], [94mLoss[0m : 2.20367
[1mStep[0m  [10/21], [94mLoss[0m : 2.27762
[1mStep[0m  [12/21], [94mLoss[0m : 2.30392
[1mStep[0m  [14/21], [94mLoss[0m : 2.46373
[1mStep[0m  [16/21], [94mLoss[0m : 2.21878
[1mStep[0m  [18/21], [94mLoss[0m : 2.50604
[1mStep[0m  [20/21], [94mLoss[0m : 2.48028

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.348, [92mTest[0m: 2.448, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.29386
[1mStep[0m  [2/21], [94mLoss[0m : 2.49351
[1mStep[0m  [4/21], [94mLoss[0m : 2.33835
[1mStep[0m  [6/21], [94mLoss[0m : 2.29219
[1mStep[0m  [8/21], [94mLoss[0m : 2.32404
[1mStep[0m  [10/21], [94mLoss[0m : 2.22335
[1mStep[0m  [12/21], [94mLoss[0m : 2.26446
[1mStep[0m  [14/21], [94mLoss[0m : 2.37739
[1mStep[0m  [16/21], [94mLoss[0m : 2.35471
[1mStep[0m  [18/21], [94mLoss[0m : 2.55781
[1mStep[0m  [20/21], [94mLoss[0m : 2.44210

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.356, [92mTest[0m: 2.484, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.39424
[1mStep[0m  [2/21], [94mLoss[0m : 2.31833
[1mStep[0m  [4/21], [94mLoss[0m : 2.42038
[1mStep[0m  [6/21], [94mLoss[0m : 2.36255
[1mStep[0m  [8/21], [94mLoss[0m : 2.18772
[1mStep[0m  [10/21], [94mLoss[0m : 2.32964
[1mStep[0m  [12/21], [94mLoss[0m : 2.22686
[1mStep[0m  [14/21], [94mLoss[0m : 2.14590
[1mStep[0m  [16/21], [94mLoss[0m : 2.45439
[1mStep[0m  [18/21], [94mLoss[0m : 2.40066
[1mStep[0m  [20/21], [94mLoss[0m : 2.29614

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.356, [92mTest[0m: 2.435, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40194
[1mStep[0m  [2/21], [94mLoss[0m : 2.51103
[1mStep[0m  [4/21], [94mLoss[0m : 2.41213
[1mStep[0m  [6/21], [94mLoss[0m : 2.35650
[1mStep[0m  [8/21], [94mLoss[0m : 2.27925
[1mStep[0m  [10/21], [94mLoss[0m : 2.25627
[1mStep[0m  [12/21], [94mLoss[0m : 2.36097
[1mStep[0m  [14/21], [94mLoss[0m : 2.25658
[1mStep[0m  [16/21], [94mLoss[0m : 2.45801
[1mStep[0m  [18/21], [94mLoss[0m : 2.38670
[1mStep[0m  [20/21], [94mLoss[0m : 2.30262

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.332, [92mTest[0m: 2.459, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.32386
[1mStep[0m  [2/21], [94mLoss[0m : 2.33864
[1mStep[0m  [4/21], [94mLoss[0m : 2.41185
[1mStep[0m  [6/21], [94mLoss[0m : 2.37450
[1mStep[0m  [8/21], [94mLoss[0m : 2.28333
[1mStep[0m  [10/21], [94mLoss[0m : 2.32377
[1mStep[0m  [12/21], [94mLoss[0m : 2.39873
[1mStep[0m  [14/21], [94mLoss[0m : 2.30910
[1mStep[0m  [16/21], [94mLoss[0m : 2.57921
[1mStep[0m  [18/21], [94mLoss[0m : 2.26904
[1mStep[0m  [20/21], [94mLoss[0m : 2.39173

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.346, [92mTest[0m: 2.426, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.25385
[1mStep[0m  [2/21], [94mLoss[0m : 2.35611
[1mStep[0m  [4/21], [94mLoss[0m : 2.27638
[1mStep[0m  [6/21], [94mLoss[0m : 2.36918
[1mStep[0m  [8/21], [94mLoss[0m : 2.34087
[1mStep[0m  [10/21], [94mLoss[0m : 2.34274
[1mStep[0m  [12/21], [94mLoss[0m : 2.23890
[1mStep[0m  [14/21], [94mLoss[0m : 2.22236
[1mStep[0m  [16/21], [94mLoss[0m : 2.35608
[1mStep[0m  [18/21], [94mLoss[0m : 2.46942
[1mStep[0m  [20/21], [94mLoss[0m : 2.24549

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.332, [92mTest[0m: 2.433, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.37185
[1mStep[0m  [2/21], [94mLoss[0m : 2.37975
[1mStep[0m  [4/21], [94mLoss[0m : 2.45017
[1mStep[0m  [6/21], [94mLoss[0m : 2.26388
[1mStep[0m  [8/21], [94mLoss[0m : 2.40594
[1mStep[0m  [10/21], [94mLoss[0m : 2.40723
[1mStep[0m  [12/21], [94mLoss[0m : 2.31395
[1mStep[0m  [14/21], [94mLoss[0m : 2.21450
[1mStep[0m  [16/21], [94mLoss[0m : 2.42895
[1mStep[0m  [18/21], [94mLoss[0m : 2.39608
[1mStep[0m  [20/21], [94mLoss[0m : 2.39386

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.331, [92mTest[0m: 2.442, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.24795
[1mStep[0m  [2/21], [94mLoss[0m : 2.45970
[1mStep[0m  [4/21], [94mLoss[0m : 2.31993
[1mStep[0m  [6/21], [94mLoss[0m : 2.32758
[1mStep[0m  [8/21], [94mLoss[0m : 2.35082
[1mStep[0m  [10/21], [94mLoss[0m : 2.24199
[1mStep[0m  [12/21], [94mLoss[0m : 2.22370
[1mStep[0m  [14/21], [94mLoss[0m : 2.22370
[1mStep[0m  [16/21], [94mLoss[0m : 2.27147
[1mStep[0m  [18/21], [94mLoss[0m : 2.34754
[1mStep[0m  [20/21], [94mLoss[0m : 2.48536

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.327, [92mTest[0m: 2.440, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.37996
[1mStep[0m  [2/21], [94mLoss[0m : 2.18529
[1mStep[0m  [4/21], [94mLoss[0m : 2.28309
[1mStep[0m  [6/21], [94mLoss[0m : 2.35285
[1mStep[0m  [8/21], [94mLoss[0m : 2.35329
[1mStep[0m  [10/21], [94mLoss[0m : 2.20970
[1mStep[0m  [12/21], [94mLoss[0m : 2.30806
[1mStep[0m  [14/21], [94mLoss[0m : 2.44984
[1mStep[0m  [16/21], [94mLoss[0m : 2.45599
[1mStep[0m  [18/21], [94mLoss[0m : 2.30023
[1mStep[0m  [20/21], [94mLoss[0m : 2.28779

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.333, [92mTest[0m: 2.414, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.28293
[1mStep[0m  [2/21], [94mLoss[0m : 2.27728
[1mStep[0m  [4/21], [94mLoss[0m : 2.35191
[1mStep[0m  [6/21], [94mLoss[0m : 2.26916
[1mStep[0m  [8/21], [94mLoss[0m : 2.28494
[1mStep[0m  [10/21], [94mLoss[0m : 2.27308
[1mStep[0m  [12/21], [94mLoss[0m : 2.29472
[1mStep[0m  [14/21], [94mLoss[0m : 2.27880
[1mStep[0m  [16/21], [94mLoss[0m : 2.27167
[1mStep[0m  [18/21], [94mLoss[0m : 2.35490
[1mStep[0m  [20/21], [94mLoss[0m : 2.31958

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.316, [92mTest[0m: 2.443, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.23247
[1mStep[0m  [2/21], [94mLoss[0m : 2.35244
[1mStep[0m  [4/21], [94mLoss[0m : 2.12147
[1mStep[0m  [6/21], [94mLoss[0m : 2.44763
[1mStep[0m  [8/21], [94mLoss[0m : 2.47008
[1mStep[0m  [10/21], [94mLoss[0m : 2.34283
[1mStep[0m  [12/21], [94mLoss[0m : 2.39492
[1mStep[0m  [14/21], [94mLoss[0m : 2.37680
[1mStep[0m  [16/21], [94mLoss[0m : 2.44060
[1mStep[0m  [18/21], [94mLoss[0m : 2.36592
[1mStep[0m  [20/21], [94mLoss[0m : 2.29336

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.326, [92mTest[0m: 2.445, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.419
====================================

Phase 1 - Evaluation MAE:  2.419005904878889
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 2.30855
[1mStep[0m  [2/21], [94mLoss[0m : 2.29341
[1mStep[0m  [4/21], [94mLoss[0m : 2.25122
[1mStep[0m  [6/21], [94mLoss[0m : 2.47084
[1mStep[0m  [8/21], [94mLoss[0m : 2.41214
[1mStep[0m  [10/21], [94mLoss[0m : 2.33323
[1mStep[0m  [12/21], [94mLoss[0m : 2.37982
[1mStep[0m  [14/21], [94mLoss[0m : 2.52974
[1mStep[0m  [16/21], [94mLoss[0m : 2.47249
[1mStep[0m  [18/21], [94mLoss[0m : 2.43539
[1mStep[0m  [20/21], [94mLoss[0m : 2.26072

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.389, [92mTest[0m: 2.417, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.30673
[1mStep[0m  [2/21], [94mLoss[0m : 2.45534
[1mStep[0m  [4/21], [94mLoss[0m : 2.41985
[1mStep[0m  [6/21], [94mLoss[0m : 2.39106
[1mStep[0m  [8/21], [94mLoss[0m : 2.28262
[1mStep[0m  [10/21], [94mLoss[0m : 2.49068
[1mStep[0m  [12/21], [94mLoss[0m : 2.24932
[1mStep[0m  [14/21], [94mLoss[0m : 2.51298
[1mStep[0m  [16/21], [94mLoss[0m : 2.56138
[1mStep[0m  [18/21], [94mLoss[0m : 2.33846
[1mStep[0m  [20/21], [94mLoss[0m : 2.34946

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.356, [92mTest[0m: 3.155, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.18069
[1mStep[0m  [2/21], [94mLoss[0m : 2.37547
[1mStep[0m  [4/21], [94mLoss[0m : 2.31758
[1mStep[0m  [6/21], [94mLoss[0m : 2.19459
[1mStep[0m  [8/21], [94mLoss[0m : 2.18486
[1mStep[0m  [10/21], [94mLoss[0m : 2.34762
[1mStep[0m  [12/21], [94mLoss[0m : 2.36989
[1mStep[0m  [14/21], [94mLoss[0m : 2.25003
[1mStep[0m  [16/21], [94mLoss[0m : 2.25457
[1mStep[0m  [18/21], [94mLoss[0m : 2.23731
[1mStep[0m  [20/21], [94mLoss[0m : 2.28930

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.280, [92mTest[0m: 2.380, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.29331
[1mStep[0m  [2/21], [94mLoss[0m : 2.35215
[1mStep[0m  [4/21], [94mLoss[0m : 2.04578
[1mStep[0m  [6/21], [94mLoss[0m : 2.19944
[1mStep[0m  [8/21], [94mLoss[0m : 2.08696
[1mStep[0m  [10/21], [94mLoss[0m : 2.20831
[1mStep[0m  [12/21], [94mLoss[0m : 2.16266
[1mStep[0m  [14/21], [94mLoss[0m : 2.30458
[1mStep[0m  [16/21], [94mLoss[0m : 2.12422
[1mStep[0m  [18/21], [94mLoss[0m : 2.26824
[1mStep[0m  [20/21], [94mLoss[0m : 2.16579

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.222, [92mTest[0m: 2.784, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.15427
[1mStep[0m  [2/21], [94mLoss[0m : 2.10292
[1mStep[0m  [4/21], [94mLoss[0m : 2.21648
[1mStep[0m  [6/21], [94mLoss[0m : 2.18737
[1mStep[0m  [8/21], [94mLoss[0m : 2.21597
[1mStep[0m  [10/21], [94mLoss[0m : 2.13217
[1mStep[0m  [12/21], [94mLoss[0m : 2.22682
[1mStep[0m  [14/21], [94mLoss[0m : 2.14597
[1mStep[0m  [16/21], [94mLoss[0m : 2.29739
[1mStep[0m  [18/21], [94mLoss[0m : 2.15116
[1mStep[0m  [20/21], [94mLoss[0m : 2.24389

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.162, [92mTest[0m: 2.559, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.07373
[1mStep[0m  [2/21], [94mLoss[0m : 2.00285
[1mStep[0m  [4/21], [94mLoss[0m : 1.97415
[1mStep[0m  [6/21], [94mLoss[0m : 2.11372
[1mStep[0m  [8/21], [94mLoss[0m : 2.10132
[1mStep[0m  [10/21], [94mLoss[0m : 2.11260
[1mStep[0m  [12/21], [94mLoss[0m : 2.05919
[1mStep[0m  [14/21], [94mLoss[0m : 2.05338
[1mStep[0m  [16/21], [94mLoss[0m : 2.23969
[1mStep[0m  [18/21], [94mLoss[0m : 2.17982
[1mStep[0m  [20/21], [94mLoss[0m : 2.14443

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.101, [92mTest[0m: 2.537, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.96817
[1mStep[0m  [2/21], [94mLoss[0m : 2.09071
[1mStep[0m  [4/21], [94mLoss[0m : 2.02128
[1mStep[0m  [6/21], [94mLoss[0m : 2.09607
[1mStep[0m  [8/21], [94mLoss[0m : 2.12257
[1mStep[0m  [10/21], [94mLoss[0m : 2.11200
[1mStep[0m  [12/21], [94mLoss[0m : 2.03250
[1mStep[0m  [14/21], [94mLoss[0m : 2.09533
[1mStep[0m  [16/21], [94mLoss[0m : 2.01368
[1mStep[0m  [18/21], [94mLoss[0m : 2.10942
[1mStep[0m  [20/21], [94mLoss[0m : 2.13605

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.048, [92mTest[0m: 2.438, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.98789
[1mStep[0m  [2/21], [94mLoss[0m : 2.09751
[1mStep[0m  [4/21], [94mLoss[0m : 1.97699
[1mStep[0m  [6/21], [94mLoss[0m : 1.93325
[1mStep[0m  [8/21], [94mLoss[0m : 1.99935
[1mStep[0m  [10/21], [94mLoss[0m : 2.03027
[1mStep[0m  [12/21], [94mLoss[0m : 1.95996
[1mStep[0m  [14/21], [94mLoss[0m : 2.14458
[1mStep[0m  [16/21], [94mLoss[0m : 1.89846
[1mStep[0m  [18/21], [94mLoss[0m : 2.04836
[1mStep[0m  [20/21], [94mLoss[0m : 2.07814

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.999, [92mTest[0m: 2.413, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.92222
[1mStep[0m  [2/21], [94mLoss[0m : 1.84697
[1mStep[0m  [4/21], [94mLoss[0m : 2.09713
[1mStep[0m  [6/21], [94mLoss[0m : 2.00712
[1mStep[0m  [8/21], [94mLoss[0m : 1.99865
[1mStep[0m  [10/21], [94mLoss[0m : 1.84493
[1mStep[0m  [12/21], [94mLoss[0m : 1.85533
[1mStep[0m  [14/21], [94mLoss[0m : 1.88704
[1mStep[0m  [16/21], [94mLoss[0m : 1.94401
[1mStep[0m  [18/21], [94mLoss[0m : 1.84392
[1mStep[0m  [20/21], [94mLoss[0m : 1.93551

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.939, [92mTest[0m: 2.437, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.93137
[1mStep[0m  [2/21], [94mLoss[0m : 1.78864
[1mStep[0m  [4/21], [94mLoss[0m : 1.89330
[1mStep[0m  [6/21], [94mLoss[0m : 2.01568
[1mStep[0m  [8/21], [94mLoss[0m : 1.78221
[1mStep[0m  [10/21], [94mLoss[0m : 1.83832
[1mStep[0m  [12/21], [94mLoss[0m : 1.85253
[1mStep[0m  [14/21], [94mLoss[0m : 1.98903
[1mStep[0m  [16/21], [94mLoss[0m : 1.83424
[1mStep[0m  [18/21], [94mLoss[0m : 1.93358
[1mStep[0m  [20/21], [94mLoss[0m : 2.02240

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.893, [92mTest[0m: 2.450, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.94777
[1mStep[0m  [2/21], [94mLoss[0m : 1.90818
[1mStep[0m  [4/21], [94mLoss[0m : 1.78297
[1mStep[0m  [6/21], [94mLoss[0m : 1.74549
[1mStep[0m  [8/21], [94mLoss[0m : 1.87300
[1mStep[0m  [10/21], [94mLoss[0m : 1.82043
[1mStep[0m  [12/21], [94mLoss[0m : 1.85267
[1mStep[0m  [14/21], [94mLoss[0m : 1.88023
[1mStep[0m  [16/21], [94mLoss[0m : 1.90850
[1mStep[0m  [18/21], [94mLoss[0m : 1.76711
[1mStep[0m  [20/21], [94mLoss[0m : 1.82633

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.834, [92mTest[0m: 2.529, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.75858
[1mStep[0m  [2/21], [94mLoss[0m : 1.79934
[1mStep[0m  [4/21], [94mLoss[0m : 1.71727
[1mStep[0m  [6/21], [94mLoss[0m : 1.60389
[1mStep[0m  [8/21], [94mLoss[0m : 1.85386
[1mStep[0m  [10/21], [94mLoss[0m : 1.77061
[1mStep[0m  [12/21], [94mLoss[0m : 1.82736
[1mStep[0m  [14/21], [94mLoss[0m : 1.77316
[1mStep[0m  [16/21], [94mLoss[0m : 1.82607
[1mStep[0m  [18/21], [94mLoss[0m : 1.68248
[1mStep[0m  [20/21], [94mLoss[0m : 1.76283

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.791, [92mTest[0m: 2.440, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.57594
[1mStep[0m  [2/21], [94mLoss[0m : 1.75035
[1mStep[0m  [4/21], [94mLoss[0m : 1.83603
[1mStep[0m  [6/21], [94mLoss[0m : 1.75139
[1mStep[0m  [8/21], [94mLoss[0m : 1.68370
[1mStep[0m  [10/21], [94mLoss[0m : 1.76242
[1mStep[0m  [12/21], [94mLoss[0m : 1.91909
[1mStep[0m  [14/21], [94mLoss[0m : 1.73178
[1mStep[0m  [16/21], [94mLoss[0m : 1.75013
[1mStep[0m  [18/21], [94mLoss[0m : 1.90158
[1mStep[0m  [20/21], [94mLoss[0m : 1.73479

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.747, [92mTest[0m: 2.437, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.69504
[1mStep[0m  [2/21], [94mLoss[0m : 1.60744
[1mStep[0m  [4/21], [94mLoss[0m : 1.70286
[1mStep[0m  [6/21], [94mLoss[0m : 1.62044
[1mStep[0m  [8/21], [94mLoss[0m : 1.69605
[1mStep[0m  [10/21], [94mLoss[0m : 1.63788
[1mStep[0m  [12/21], [94mLoss[0m : 1.74513
[1mStep[0m  [14/21], [94mLoss[0m : 1.65344
[1mStep[0m  [16/21], [94mLoss[0m : 1.72048
[1mStep[0m  [18/21], [94mLoss[0m : 1.77046
[1mStep[0m  [20/21], [94mLoss[0m : 1.68677

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.702, [92mTest[0m: 2.502, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.78478
[1mStep[0m  [2/21], [94mLoss[0m : 1.64593
[1mStep[0m  [4/21], [94mLoss[0m : 1.63275
[1mStep[0m  [6/21], [94mLoss[0m : 1.60169
[1mStep[0m  [8/21], [94mLoss[0m : 1.68905
[1mStep[0m  [10/21], [94mLoss[0m : 1.52921
[1mStep[0m  [12/21], [94mLoss[0m : 1.76696
[1mStep[0m  [14/21], [94mLoss[0m : 1.67607
[1mStep[0m  [16/21], [94mLoss[0m : 1.66639
[1mStep[0m  [18/21], [94mLoss[0m : 1.72081
[1mStep[0m  [20/21], [94mLoss[0m : 1.56725

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.660, [92mTest[0m: 2.470, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.59797
[1mStep[0m  [2/21], [94mLoss[0m : 1.53782
[1mStep[0m  [4/21], [94mLoss[0m : 1.50118
[1mStep[0m  [6/21], [94mLoss[0m : 1.61870
[1mStep[0m  [8/21], [94mLoss[0m : 1.67917
[1mStep[0m  [10/21], [94mLoss[0m : 1.84517
[1mStep[0m  [12/21], [94mLoss[0m : 1.59427
[1mStep[0m  [14/21], [94mLoss[0m : 1.49145
[1mStep[0m  [16/21], [94mLoss[0m : 1.68545
[1mStep[0m  [18/21], [94mLoss[0m : 1.62501
[1mStep[0m  [20/21], [94mLoss[0m : 1.57241

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.624, [92mTest[0m: 2.504, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.52471
[1mStep[0m  [2/21], [94mLoss[0m : 1.52555
[1mStep[0m  [4/21], [94mLoss[0m : 1.54446
[1mStep[0m  [6/21], [94mLoss[0m : 1.68634
[1mStep[0m  [8/21], [94mLoss[0m : 1.56994
[1mStep[0m  [10/21], [94mLoss[0m : 1.57977
[1mStep[0m  [12/21], [94mLoss[0m : 1.73554
[1mStep[0m  [14/21], [94mLoss[0m : 1.56915
[1mStep[0m  [16/21], [94mLoss[0m : 1.54278
[1mStep[0m  [18/21], [94mLoss[0m : 1.60369
[1mStep[0m  [20/21], [94mLoss[0m : 1.63091

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.579, [92mTest[0m: 2.603, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.49806
[1mStep[0m  [2/21], [94mLoss[0m : 1.42846
[1mStep[0m  [4/21], [94mLoss[0m : 1.54780
[1mStep[0m  [6/21], [94mLoss[0m : 1.51695
[1mStep[0m  [8/21], [94mLoss[0m : 1.48662
[1mStep[0m  [10/21], [94mLoss[0m : 1.73491
[1mStep[0m  [12/21], [94mLoss[0m : 1.52560
[1mStep[0m  [14/21], [94mLoss[0m : 1.63351
[1mStep[0m  [16/21], [94mLoss[0m : 1.50173
[1mStep[0m  [18/21], [94mLoss[0m : 1.58385
[1mStep[0m  [20/21], [94mLoss[0m : 1.60073

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.526, [92mTest[0m: 2.538, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.46957
[1mStep[0m  [2/21], [94mLoss[0m : 1.48197
[1mStep[0m  [4/21], [94mLoss[0m : 1.50134
[1mStep[0m  [6/21], [94mLoss[0m : 1.57721
[1mStep[0m  [8/21], [94mLoss[0m : 1.44941
[1mStep[0m  [10/21], [94mLoss[0m : 1.62205
[1mStep[0m  [12/21], [94mLoss[0m : 1.50142
[1mStep[0m  [14/21], [94mLoss[0m : 1.51876
[1mStep[0m  [16/21], [94mLoss[0m : 1.50669
[1mStep[0m  [18/21], [94mLoss[0m : 1.44926
[1mStep[0m  [20/21], [94mLoss[0m : 1.40144

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.508, [92mTest[0m: 2.502, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.43099
[1mStep[0m  [2/21], [94mLoss[0m : 1.54459
[1mStep[0m  [4/21], [94mLoss[0m : 1.64489
[1mStep[0m  [6/21], [94mLoss[0m : 1.47012
[1mStep[0m  [8/21], [94mLoss[0m : 1.47800
[1mStep[0m  [10/21], [94mLoss[0m : 1.38669
[1mStep[0m  [12/21], [94mLoss[0m : 1.45967
[1mStep[0m  [14/21], [94mLoss[0m : 1.55833
[1mStep[0m  [16/21], [94mLoss[0m : 1.57295
[1mStep[0m  [18/21], [94mLoss[0m : 1.48211
[1mStep[0m  [20/21], [94mLoss[0m : 1.59032

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.496, [92mTest[0m: 2.520, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.39940
[1mStep[0m  [2/21], [94mLoss[0m : 1.40577
[1mStep[0m  [4/21], [94mLoss[0m : 1.52172
[1mStep[0m  [6/21], [94mLoss[0m : 1.41123
[1mStep[0m  [8/21], [94mLoss[0m : 1.31682
[1mStep[0m  [10/21], [94mLoss[0m : 1.52465
[1mStep[0m  [12/21], [94mLoss[0m : 1.52326
[1mStep[0m  [14/21], [94mLoss[0m : 1.47388
[1mStep[0m  [16/21], [94mLoss[0m : 1.46804
[1mStep[0m  [18/21], [94mLoss[0m : 1.39026
[1mStep[0m  [20/21], [94mLoss[0m : 1.43369

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.463, [92mTest[0m: 2.525, [96mlr[0m: 0.0023175
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 20 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.578
====================================

Phase 2 - Evaluation MAE:  2.5779317787715366
MAE score P1      2.419006
MAE score P2      2.577932
loss              1.463078
learning_rate     0.002575
batch_size             512
hidden_sizes         [300]
epochs                  30
activation            relu
optimizer              sgd
early stopping        True
dropout                0.2
momentum               0.9
weight_decay        0.0001
Name: 11, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 11.09701
[1mStep[0m  [4/42], [94mLoss[0m : 10.69456
[1mStep[0m  [8/42], [94mLoss[0m : 10.87298
[1mStep[0m  [12/42], [94mLoss[0m : 11.11567
[1mStep[0m  [16/42], [94mLoss[0m : 11.23005
[1mStep[0m  [20/42], [94mLoss[0m : 10.95242
[1mStep[0m  [24/42], [94mLoss[0m : 10.95306
[1mStep[0m  [28/42], [94mLoss[0m : 10.59007
[1mStep[0m  [32/42], [94mLoss[0m : 10.51703
[1mStep[0m  [36/42], [94mLoss[0m : 10.58872
[1mStep[0m  [40/42], [94mLoss[0m : 10.85741

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.844, [92mTest[0m: 11.026, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.63335
[1mStep[0m  [4/42], [94mLoss[0m : 10.62002
[1mStep[0m  [8/42], [94mLoss[0m : 10.69105
[1mStep[0m  [12/42], [94mLoss[0m : 10.54993
[1mStep[0m  [16/42], [94mLoss[0m : 10.66032
[1mStep[0m  [20/42], [94mLoss[0m : 11.06962
[1mStep[0m  [24/42], [94mLoss[0m : 10.59397
[1mStep[0m  [28/42], [94mLoss[0m : 10.65262
[1mStep[0m  [32/42], [94mLoss[0m : 10.63761
[1mStep[0m  [36/42], [94mLoss[0m : 10.66128
[1mStep[0m  [40/42], [94mLoss[0m : 10.24313

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.628, [92mTest[0m: 10.697, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.49109
[1mStep[0m  [4/42], [94mLoss[0m : 10.71266
[1mStep[0m  [8/42], [94mLoss[0m : 10.22780
[1mStep[0m  [12/42], [94mLoss[0m : 10.60724
[1mStep[0m  [16/42], [94mLoss[0m : 10.70287
[1mStep[0m  [20/42], [94mLoss[0m : 10.51635
[1mStep[0m  [24/42], [94mLoss[0m : 10.50244
[1mStep[0m  [28/42], [94mLoss[0m : 10.67510
[1mStep[0m  [32/42], [94mLoss[0m : 10.72045
[1mStep[0m  [36/42], [94mLoss[0m : 10.46247
[1mStep[0m  [40/42], [94mLoss[0m : 10.08098

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.424, [92mTest[0m: 10.389, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.12426
[1mStep[0m  [4/42], [94mLoss[0m : 10.47835
[1mStep[0m  [8/42], [94mLoss[0m : 10.06127
[1mStep[0m  [12/42], [94mLoss[0m : 10.02286
[1mStep[0m  [16/42], [94mLoss[0m : 10.13190
[1mStep[0m  [20/42], [94mLoss[0m : 10.28329
[1mStep[0m  [24/42], [94mLoss[0m : 10.07300
[1mStep[0m  [28/42], [94mLoss[0m : 9.79279
[1mStep[0m  [32/42], [94mLoss[0m : 10.29441
[1mStep[0m  [36/42], [94mLoss[0m : 9.89521
[1mStep[0m  [40/42], [94mLoss[0m : 9.86333

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.214, [92mTest[0m: 10.128, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.37160
[1mStep[0m  [4/42], [94mLoss[0m : 9.80232
[1mStep[0m  [8/42], [94mLoss[0m : 10.07066
[1mStep[0m  [12/42], [94mLoss[0m : 10.34561
[1mStep[0m  [16/42], [94mLoss[0m : 10.43488
[1mStep[0m  [20/42], [94mLoss[0m : 10.01475
[1mStep[0m  [24/42], [94mLoss[0m : 9.73144
[1mStep[0m  [28/42], [94mLoss[0m : 9.68236
[1mStep[0m  [32/42], [94mLoss[0m : 9.78664
[1mStep[0m  [36/42], [94mLoss[0m : 9.44904
[1mStep[0m  [40/42], [94mLoss[0m : 10.15133

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.010, [92mTest[0m: 9.843, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.92532
[1mStep[0m  [4/42], [94mLoss[0m : 9.54439
[1mStep[0m  [8/42], [94mLoss[0m : 9.60015
[1mStep[0m  [12/42], [94mLoss[0m : 9.75587
[1mStep[0m  [16/42], [94mLoss[0m : 9.94122
[1mStep[0m  [20/42], [94mLoss[0m : 9.80200
[1mStep[0m  [24/42], [94mLoss[0m : 9.54048
[1mStep[0m  [28/42], [94mLoss[0m : 10.14217
[1mStep[0m  [32/42], [94mLoss[0m : 10.14331
[1mStep[0m  [36/42], [94mLoss[0m : 9.90602
[1mStep[0m  [40/42], [94mLoss[0m : 9.41325

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 9.768, [92mTest[0m: 9.515, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.81505
[1mStep[0m  [4/42], [94mLoss[0m : 9.36656
[1mStep[0m  [8/42], [94mLoss[0m : 9.39196
[1mStep[0m  [12/42], [94mLoss[0m : 9.55799
[1mStep[0m  [16/42], [94mLoss[0m : 9.44978
[1mStep[0m  [20/42], [94mLoss[0m : 9.79921
[1mStep[0m  [24/42], [94mLoss[0m : 9.17224
[1mStep[0m  [28/42], [94mLoss[0m : 9.26567
[1mStep[0m  [32/42], [94mLoss[0m : 9.53654
[1mStep[0m  [36/42], [94mLoss[0m : 9.31084
[1mStep[0m  [40/42], [94mLoss[0m : 8.93460

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 9.496, [92mTest[0m: 9.161, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.44019
[1mStep[0m  [4/42], [94mLoss[0m : 9.42448
[1mStep[0m  [8/42], [94mLoss[0m : 9.57402
[1mStep[0m  [12/42], [94mLoss[0m : 9.37530
[1mStep[0m  [16/42], [94mLoss[0m : 8.96791
[1mStep[0m  [20/42], [94mLoss[0m : 9.24517
[1mStep[0m  [24/42], [94mLoss[0m : 8.94996
[1mStep[0m  [28/42], [94mLoss[0m : 8.98993
[1mStep[0m  [32/42], [94mLoss[0m : 8.93643
[1mStep[0m  [36/42], [94mLoss[0m : 8.93163
[1mStep[0m  [40/42], [94mLoss[0m : 9.41140

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 9.215, [92mTest[0m: 8.840, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.52896
[1mStep[0m  [4/42], [94mLoss[0m : 8.47078
[1mStep[0m  [8/42], [94mLoss[0m : 8.95919
[1mStep[0m  [12/42], [94mLoss[0m : 9.10150
[1mStep[0m  [16/42], [94mLoss[0m : 9.17431
[1mStep[0m  [20/42], [94mLoss[0m : 8.92996
[1mStep[0m  [24/42], [94mLoss[0m : 8.93133
[1mStep[0m  [28/42], [94mLoss[0m : 8.73317
[1mStep[0m  [32/42], [94mLoss[0m : 8.72548
[1mStep[0m  [36/42], [94mLoss[0m : 9.05267
[1mStep[0m  [40/42], [94mLoss[0m : 8.97357

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 8.918, [92mTest[0m: 8.365, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.35843
[1mStep[0m  [4/42], [94mLoss[0m : 9.16159
[1mStep[0m  [8/42], [94mLoss[0m : 8.78807
[1mStep[0m  [12/42], [94mLoss[0m : 8.53951
[1mStep[0m  [16/42], [94mLoss[0m : 8.20732
[1mStep[0m  [20/42], [94mLoss[0m : 8.50149
[1mStep[0m  [24/42], [94mLoss[0m : 8.57179
[1mStep[0m  [28/42], [94mLoss[0m : 8.68039
[1mStep[0m  [32/42], [94mLoss[0m : 8.35772
[1mStep[0m  [36/42], [94mLoss[0m : 8.21230
[1mStep[0m  [40/42], [94mLoss[0m : 8.61007

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 8.611, [92mTest[0m: 8.097, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.56554
[1mStep[0m  [4/42], [94mLoss[0m : 8.94007
[1mStep[0m  [8/42], [94mLoss[0m : 8.42555
[1mStep[0m  [12/42], [94mLoss[0m : 8.87245
[1mStep[0m  [16/42], [94mLoss[0m : 8.69747
[1mStep[0m  [20/42], [94mLoss[0m : 8.37996
[1mStep[0m  [24/42], [94mLoss[0m : 8.28269
[1mStep[0m  [28/42], [94mLoss[0m : 7.88734
[1mStep[0m  [32/42], [94mLoss[0m : 7.91380
[1mStep[0m  [36/42], [94mLoss[0m : 8.07195
[1mStep[0m  [40/42], [94mLoss[0m : 8.46294

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 8.310, [92mTest[0m: 7.645, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.35431
[1mStep[0m  [4/42], [94mLoss[0m : 7.98145
[1mStep[0m  [8/42], [94mLoss[0m : 8.38402
[1mStep[0m  [12/42], [94mLoss[0m : 8.36076
[1mStep[0m  [16/42], [94mLoss[0m : 8.13537
[1mStep[0m  [20/42], [94mLoss[0m : 8.30541
[1mStep[0m  [24/42], [94mLoss[0m : 7.93329
[1mStep[0m  [28/42], [94mLoss[0m : 7.97811
[1mStep[0m  [32/42], [94mLoss[0m : 8.20119
[1mStep[0m  [36/42], [94mLoss[0m : 7.69278
[1mStep[0m  [40/42], [94mLoss[0m : 7.90137

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 8.030, [92mTest[0m: 7.296, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.98731
[1mStep[0m  [4/42], [94mLoss[0m : 7.77649
[1mStep[0m  [8/42], [94mLoss[0m : 7.77575
[1mStep[0m  [12/42], [94mLoss[0m : 7.76559
[1mStep[0m  [16/42], [94mLoss[0m : 8.26350
[1mStep[0m  [20/42], [94mLoss[0m : 7.64228
[1mStep[0m  [24/42], [94mLoss[0m : 7.66326
[1mStep[0m  [28/42], [94mLoss[0m : 7.71415
[1mStep[0m  [32/42], [94mLoss[0m : 7.69485
[1mStep[0m  [36/42], [94mLoss[0m : 7.75915
[1mStep[0m  [40/42], [94mLoss[0m : 7.57199

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 7.754, [92mTest[0m: 6.887, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.86964
[1mStep[0m  [4/42], [94mLoss[0m : 7.92598
[1mStep[0m  [8/42], [94mLoss[0m : 7.42365
[1mStep[0m  [12/42], [94mLoss[0m : 7.72282
[1mStep[0m  [16/42], [94mLoss[0m : 7.51689
[1mStep[0m  [20/42], [94mLoss[0m : 7.81160
[1mStep[0m  [24/42], [94mLoss[0m : 7.38805
[1mStep[0m  [28/42], [94mLoss[0m : 7.74129
[1mStep[0m  [32/42], [94mLoss[0m : 7.25560
[1mStep[0m  [36/42], [94mLoss[0m : 7.58836
[1mStep[0m  [40/42], [94mLoss[0m : 7.22178

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 7.493, [92mTest[0m: 6.725, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.26105
[1mStep[0m  [4/42], [94mLoss[0m : 7.26714
[1mStep[0m  [8/42], [94mLoss[0m : 7.46498
[1mStep[0m  [12/42], [94mLoss[0m : 7.56683
[1mStep[0m  [16/42], [94mLoss[0m : 7.29297
[1mStep[0m  [20/42], [94mLoss[0m : 7.14821
[1mStep[0m  [24/42], [94mLoss[0m : 7.18393
[1mStep[0m  [28/42], [94mLoss[0m : 7.26682
[1mStep[0m  [32/42], [94mLoss[0m : 7.29601
[1mStep[0m  [36/42], [94mLoss[0m : 7.27436
[1mStep[0m  [40/42], [94mLoss[0m : 7.02613

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 7.244, [92mTest[0m: 6.528, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.17520
[1mStep[0m  [4/42], [94mLoss[0m : 7.32105
[1mStep[0m  [8/42], [94mLoss[0m : 7.25233
[1mStep[0m  [12/42], [94mLoss[0m : 7.08938
[1mStep[0m  [16/42], [94mLoss[0m : 6.99161
[1mStep[0m  [20/42], [94mLoss[0m : 6.75239
[1mStep[0m  [24/42], [94mLoss[0m : 6.82745
[1mStep[0m  [28/42], [94mLoss[0m : 6.73601
[1mStep[0m  [32/42], [94mLoss[0m : 6.91839
[1mStep[0m  [36/42], [94mLoss[0m : 7.02606
[1mStep[0m  [40/42], [94mLoss[0m : 6.83567

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 7.030, [92mTest[0m: 6.349, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.28491
[1mStep[0m  [4/42], [94mLoss[0m : 6.70541
[1mStep[0m  [8/42], [94mLoss[0m : 6.82187
[1mStep[0m  [12/42], [94mLoss[0m : 6.51646
[1mStep[0m  [16/42], [94mLoss[0m : 7.18013
[1mStep[0m  [20/42], [94mLoss[0m : 6.92427
[1mStep[0m  [24/42], [94mLoss[0m : 6.91339
[1mStep[0m  [28/42], [94mLoss[0m : 6.85703
[1mStep[0m  [32/42], [94mLoss[0m : 6.43194
[1mStep[0m  [36/42], [94mLoss[0m : 6.78502
[1mStep[0m  [40/42], [94mLoss[0m : 6.73202

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 6.796, [92mTest[0m: 6.074, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.69957
[1mStep[0m  [4/42], [94mLoss[0m : 6.56070
[1mStep[0m  [8/42], [94mLoss[0m : 6.61215
[1mStep[0m  [12/42], [94mLoss[0m : 6.72467
[1mStep[0m  [16/42], [94mLoss[0m : 6.77408
[1mStep[0m  [20/42], [94mLoss[0m : 6.20350
[1mStep[0m  [24/42], [94mLoss[0m : 6.33007
[1mStep[0m  [28/42], [94mLoss[0m : 6.68499
[1mStep[0m  [32/42], [94mLoss[0m : 6.56662
[1mStep[0m  [36/42], [94mLoss[0m : 6.32506
[1mStep[0m  [40/42], [94mLoss[0m : 6.87989

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 6.585, [92mTest[0m: 5.915, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.44112
[1mStep[0m  [4/42], [94mLoss[0m : 6.60139
[1mStep[0m  [8/42], [94mLoss[0m : 6.76211
[1mStep[0m  [12/42], [94mLoss[0m : 6.31366
[1mStep[0m  [16/42], [94mLoss[0m : 6.26997
[1mStep[0m  [20/42], [94mLoss[0m : 6.27393
[1mStep[0m  [24/42], [94mLoss[0m : 6.07314
[1mStep[0m  [28/42], [94mLoss[0m : 6.26868
[1mStep[0m  [32/42], [94mLoss[0m : 5.99876
[1mStep[0m  [36/42], [94mLoss[0m : 6.13398
[1mStep[0m  [40/42], [94mLoss[0m : 6.31419

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 6.371, [92mTest[0m: 5.641, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.73100
[1mStep[0m  [4/42], [94mLoss[0m : 6.33844
[1mStep[0m  [8/42], [94mLoss[0m : 6.46588
[1mStep[0m  [12/42], [94mLoss[0m : 6.16757
[1mStep[0m  [16/42], [94mLoss[0m : 6.42496
[1mStep[0m  [20/42], [94mLoss[0m : 6.17939
[1mStep[0m  [24/42], [94mLoss[0m : 5.72545
[1mStep[0m  [28/42], [94mLoss[0m : 6.01383
[1mStep[0m  [32/42], [94mLoss[0m : 6.06845
[1mStep[0m  [36/42], [94mLoss[0m : 6.09299
[1mStep[0m  [40/42], [94mLoss[0m : 6.05776

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 6.147, [92mTest[0m: 5.352, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.82107
[1mStep[0m  [4/42], [94mLoss[0m : 6.09295
[1mStep[0m  [8/42], [94mLoss[0m : 5.80124
[1mStep[0m  [12/42], [94mLoss[0m : 6.23837
[1mStep[0m  [16/42], [94mLoss[0m : 6.45290
[1mStep[0m  [20/42], [94mLoss[0m : 5.91548
[1mStep[0m  [24/42], [94mLoss[0m : 5.84557
[1mStep[0m  [28/42], [94mLoss[0m : 6.00616
[1mStep[0m  [32/42], [94mLoss[0m : 6.18341
[1mStep[0m  [36/42], [94mLoss[0m : 5.92770
[1mStep[0m  [40/42], [94mLoss[0m : 5.62255

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 5.916, [92mTest[0m: 5.084, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.50353
[1mStep[0m  [4/42], [94mLoss[0m : 6.07728
[1mStep[0m  [8/42], [94mLoss[0m : 5.89089
[1mStep[0m  [12/42], [94mLoss[0m : 5.52999
[1mStep[0m  [16/42], [94mLoss[0m : 5.84842
[1mStep[0m  [20/42], [94mLoss[0m : 5.46933
[1mStep[0m  [24/42], [94mLoss[0m : 5.61994
[1mStep[0m  [28/42], [94mLoss[0m : 5.85518
[1mStep[0m  [32/42], [94mLoss[0m : 5.63519
[1mStep[0m  [36/42], [94mLoss[0m : 5.49663
[1mStep[0m  [40/42], [94mLoss[0m : 5.54751

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 5.676, [92mTest[0m: 4.923, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.15878
[1mStep[0m  [4/42], [94mLoss[0m : 5.56414
[1mStep[0m  [8/42], [94mLoss[0m : 5.61940
[1mStep[0m  [12/42], [94mLoss[0m : 5.62612
[1mStep[0m  [16/42], [94mLoss[0m : 5.51597
[1mStep[0m  [20/42], [94mLoss[0m : 5.30081
[1mStep[0m  [24/42], [94mLoss[0m : 5.22866
[1mStep[0m  [28/42], [94mLoss[0m : 5.51095
[1mStep[0m  [32/42], [94mLoss[0m : 5.50555
[1mStep[0m  [36/42], [94mLoss[0m : 5.30893
[1mStep[0m  [40/42], [94mLoss[0m : 5.68032

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 5.501, [92mTest[0m: 4.734, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.55022
[1mStep[0m  [4/42], [94mLoss[0m : 5.46798
[1mStep[0m  [8/42], [94mLoss[0m : 5.35851
[1mStep[0m  [12/42], [94mLoss[0m : 5.08555
[1mStep[0m  [16/42], [94mLoss[0m : 5.30535
[1mStep[0m  [20/42], [94mLoss[0m : 4.96257
[1mStep[0m  [24/42], [94mLoss[0m : 5.06604
[1mStep[0m  [28/42], [94mLoss[0m : 5.09992
[1mStep[0m  [32/42], [94mLoss[0m : 5.24101
[1mStep[0m  [36/42], [94mLoss[0m : 4.80390
[1mStep[0m  [40/42], [94mLoss[0m : 5.36565

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 5.261, [92mTest[0m: 4.498, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.02135
[1mStep[0m  [4/42], [94mLoss[0m : 5.21236
[1mStep[0m  [8/42], [94mLoss[0m : 5.10737
[1mStep[0m  [12/42], [94mLoss[0m : 4.91586
[1mStep[0m  [16/42], [94mLoss[0m : 5.03398
[1mStep[0m  [20/42], [94mLoss[0m : 5.30468
[1mStep[0m  [24/42], [94mLoss[0m : 4.74876
[1mStep[0m  [28/42], [94mLoss[0m : 5.12052
[1mStep[0m  [32/42], [94mLoss[0m : 4.94647
[1mStep[0m  [36/42], [94mLoss[0m : 4.82624
[1mStep[0m  [40/42], [94mLoss[0m : 4.74881

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 5.027, [92mTest[0m: 4.306, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.29021
[1mStep[0m  [4/42], [94mLoss[0m : 5.04069
[1mStep[0m  [8/42], [94mLoss[0m : 4.31031
[1mStep[0m  [12/42], [94mLoss[0m : 4.66667
[1mStep[0m  [16/42], [94mLoss[0m : 4.89165
[1mStep[0m  [20/42], [94mLoss[0m : 4.81802
[1mStep[0m  [24/42], [94mLoss[0m : 4.85825
[1mStep[0m  [28/42], [94mLoss[0m : 5.10164
[1mStep[0m  [32/42], [94mLoss[0m : 4.79169
[1mStep[0m  [36/42], [94mLoss[0m : 4.71791
[1mStep[0m  [40/42], [94mLoss[0m : 4.70880

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 4.776, [92mTest[0m: 4.057, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.57460
[1mStep[0m  [4/42], [94mLoss[0m : 4.44950
[1mStep[0m  [8/42], [94mLoss[0m : 4.49686
[1mStep[0m  [12/42], [94mLoss[0m : 4.30112
[1mStep[0m  [16/42], [94mLoss[0m : 4.63457
[1mStep[0m  [20/42], [94mLoss[0m : 4.26109
[1mStep[0m  [24/42], [94mLoss[0m : 4.69507
[1mStep[0m  [28/42], [94mLoss[0m : 4.41230
[1mStep[0m  [32/42], [94mLoss[0m : 4.63331
[1mStep[0m  [36/42], [94mLoss[0m : 4.56045
[1mStep[0m  [40/42], [94mLoss[0m : 4.36753

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 4.527, [92mTest[0m: 3.850, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.13655
[1mStep[0m  [4/42], [94mLoss[0m : 4.24584
[1mStep[0m  [8/42], [94mLoss[0m : 4.52566
[1mStep[0m  [12/42], [94mLoss[0m : 4.29716
[1mStep[0m  [16/42], [94mLoss[0m : 3.95022
[1mStep[0m  [20/42], [94mLoss[0m : 4.78094
[1mStep[0m  [24/42], [94mLoss[0m : 4.01752
[1mStep[0m  [28/42], [94mLoss[0m : 4.01612
[1mStep[0m  [32/42], [94mLoss[0m : 4.01182
[1mStep[0m  [36/42], [94mLoss[0m : 4.47356
[1mStep[0m  [40/42], [94mLoss[0m : 4.39629

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 4.293, [92mTest[0m: 3.702, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.96989
[1mStep[0m  [4/42], [94mLoss[0m : 4.11318
[1mStep[0m  [8/42], [94mLoss[0m : 4.14421
[1mStep[0m  [12/42], [94mLoss[0m : 3.84725
[1mStep[0m  [16/42], [94mLoss[0m : 4.02895
[1mStep[0m  [20/42], [94mLoss[0m : 4.07295
[1mStep[0m  [24/42], [94mLoss[0m : 4.00220
[1mStep[0m  [28/42], [94mLoss[0m : 3.95226
[1mStep[0m  [32/42], [94mLoss[0m : 3.99541
[1mStep[0m  [36/42], [94mLoss[0m : 3.93573
[1mStep[0m  [40/42], [94mLoss[0m : 3.81780

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 4.025, [92mTest[0m: 3.409, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.98468
[1mStep[0m  [4/42], [94mLoss[0m : 3.65748
[1mStep[0m  [8/42], [94mLoss[0m : 3.76725
[1mStep[0m  [12/42], [94mLoss[0m : 3.60243
[1mStep[0m  [16/42], [94mLoss[0m : 3.86786
[1mStep[0m  [20/42], [94mLoss[0m : 3.67335
[1mStep[0m  [24/42], [94mLoss[0m : 3.67181
[1mStep[0m  [28/42], [94mLoss[0m : 3.44451
[1mStep[0m  [32/42], [94mLoss[0m : 3.53049
[1mStep[0m  [36/42], [94mLoss[0m : 3.68046
[1mStep[0m  [40/42], [94mLoss[0m : 3.45854

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 3.795, [92mTest[0m: 3.170, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.989
====================================

Phase 1 - Evaluation MAE:  2.9885820831571306
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 3.77206
[1mStep[0m  [4/42], [94mLoss[0m : 3.62175
[1mStep[0m  [8/42], [94mLoss[0m : 3.48898
[1mStep[0m  [12/42], [94mLoss[0m : 3.69553
[1mStep[0m  [16/42], [94mLoss[0m : 3.54644
[1mStep[0m  [20/42], [94mLoss[0m : 3.60293
[1mStep[0m  [24/42], [94mLoss[0m : 3.48425
[1mStep[0m  [28/42], [94mLoss[0m : 3.59044
[1mStep[0m  [32/42], [94mLoss[0m : 3.60480
[1mStep[0m  [36/42], [94mLoss[0m : 3.59509
[1mStep[0m  [40/42], [94mLoss[0m : 3.53558

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.579, [92mTest[0m: 2.981, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.65571
[1mStep[0m  [4/42], [94mLoss[0m : 3.61336
[1mStep[0m  [8/42], [94mLoss[0m : 3.34391
[1mStep[0m  [12/42], [94mLoss[0m : 3.20217
[1mStep[0m  [16/42], [94mLoss[0m : 3.46740
[1mStep[0m  [20/42], [94mLoss[0m : 3.15212
[1mStep[0m  [24/42], [94mLoss[0m : 3.59258
[1mStep[0m  [28/42], [94mLoss[0m : 3.37184
[1mStep[0m  [32/42], [94mLoss[0m : 2.98975
[1mStep[0m  [36/42], [94mLoss[0m : 3.21350
[1mStep[0m  [40/42], [94mLoss[0m : 3.09180

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.245, [92mTest[0m: 3.065, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.32445
[1mStep[0m  [4/42], [94mLoss[0m : 2.88056
[1mStep[0m  [8/42], [94mLoss[0m : 3.03573
[1mStep[0m  [12/42], [94mLoss[0m : 2.99723
[1mStep[0m  [16/42], [94mLoss[0m : 3.01863
[1mStep[0m  [20/42], [94mLoss[0m : 3.01478
[1mStep[0m  [24/42], [94mLoss[0m : 2.96914
[1mStep[0m  [28/42], [94mLoss[0m : 2.90351
[1mStep[0m  [32/42], [94mLoss[0m : 3.18759
[1mStep[0m  [36/42], [94mLoss[0m : 2.67838
[1mStep[0m  [40/42], [94mLoss[0m : 3.01418

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.022, [92mTest[0m: 2.700, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.16604
[1mStep[0m  [4/42], [94mLoss[0m : 3.18038
[1mStep[0m  [8/42], [94mLoss[0m : 2.82687
[1mStep[0m  [12/42], [94mLoss[0m : 2.89433
[1mStep[0m  [16/42], [94mLoss[0m : 2.96117
[1mStep[0m  [20/42], [94mLoss[0m : 2.90312
[1mStep[0m  [24/42], [94mLoss[0m : 2.66271
[1mStep[0m  [28/42], [94mLoss[0m : 3.16493
[1mStep[0m  [32/42], [94mLoss[0m : 2.93773
[1mStep[0m  [36/42], [94mLoss[0m : 2.79475
[1mStep[0m  [40/42], [94mLoss[0m : 2.88608

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.861, [92mTest[0m: 2.521, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.82007
[1mStep[0m  [4/42], [94mLoss[0m : 2.60637
[1mStep[0m  [8/42], [94mLoss[0m : 2.59427
[1mStep[0m  [12/42], [94mLoss[0m : 2.64401
[1mStep[0m  [16/42], [94mLoss[0m : 2.79908
[1mStep[0m  [20/42], [94mLoss[0m : 2.66255
[1mStep[0m  [24/42], [94mLoss[0m : 2.56276
[1mStep[0m  [28/42], [94mLoss[0m : 2.76119
[1mStep[0m  [32/42], [94mLoss[0m : 2.70566
[1mStep[0m  [36/42], [94mLoss[0m : 2.70656
[1mStep[0m  [40/42], [94mLoss[0m : 2.59189

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.772, [92mTest[0m: 2.473, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.89006
[1mStep[0m  [4/42], [94mLoss[0m : 2.54762
[1mStep[0m  [8/42], [94mLoss[0m : 2.79402
[1mStep[0m  [12/42], [94mLoss[0m : 2.76864
[1mStep[0m  [16/42], [94mLoss[0m : 2.72543
[1mStep[0m  [20/42], [94mLoss[0m : 2.63002
[1mStep[0m  [24/42], [94mLoss[0m : 2.49433
[1mStep[0m  [28/42], [94mLoss[0m : 2.65019
[1mStep[0m  [32/42], [94mLoss[0m : 2.94221
[1mStep[0m  [36/42], [94mLoss[0m : 2.54787
[1mStep[0m  [40/42], [94mLoss[0m : 2.63317

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.669, [92mTest[0m: 2.440, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44260
[1mStep[0m  [4/42], [94mLoss[0m : 2.64549
[1mStep[0m  [8/42], [94mLoss[0m : 2.66963
[1mStep[0m  [12/42], [94mLoss[0m : 2.42496
[1mStep[0m  [16/42], [94mLoss[0m : 2.71982
[1mStep[0m  [20/42], [94mLoss[0m : 2.67897
[1mStep[0m  [24/42], [94mLoss[0m : 2.79248
[1mStep[0m  [28/42], [94mLoss[0m : 2.49950
[1mStep[0m  [32/42], [94mLoss[0m : 2.58473
[1mStep[0m  [36/42], [94mLoss[0m : 2.67976
[1mStep[0m  [40/42], [94mLoss[0m : 2.77637

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.657, [92mTest[0m: 2.413, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61577
[1mStep[0m  [4/42], [94mLoss[0m : 2.58445
[1mStep[0m  [8/42], [94mLoss[0m : 2.55770
[1mStep[0m  [12/42], [94mLoss[0m : 2.65982
[1mStep[0m  [16/42], [94mLoss[0m : 2.84124
[1mStep[0m  [20/42], [94mLoss[0m : 2.56531
[1mStep[0m  [24/42], [94mLoss[0m : 2.65173
[1mStep[0m  [28/42], [94mLoss[0m : 2.48492
[1mStep[0m  [32/42], [94mLoss[0m : 2.30851
[1mStep[0m  [36/42], [94mLoss[0m : 2.69506
[1mStep[0m  [40/42], [94mLoss[0m : 2.70370

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.613, [92mTest[0m: 2.424, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44324
[1mStep[0m  [4/42], [94mLoss[0m : 2.70482
[1mStep[0m  [8/42], [94mLoss[0m : 2.57268
[1mStep[0m  [12/42], [94mLoss[0m : 2.52817
[1mStep[0m  [16/42], [94mLoss[0m : 2.56115
[1mStep[0m  [20/42], [94mLoss[0m : 2.50689
[1mStep[0m  [24/42], [94mLoss[0m : 2.77801
[1mStep[0m  [28/42], [94mLoss[0m : 2.56370
[1mStep[0m  [32/42], [94mLoss[0m : 2.35127
[1mStep[0m  [36/42], [94mLoss[0m : 2.56777
[1mStep[0m  [40/42], [94mLoss[0m : 2.54784

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.587, [92mTest[0m: 2.417, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.71155
[1mStep[0m  [4/42], [94mLoss[0m : 2.56068
[1mStep[0m  [8/42], [94mLoss[0m : 2.44259
[1mStep[0m  [12/42], [94mLoss[0m : 2.25565
[1mStep[0m  [16/42], [94mLoss[0m : 2.48345
[1mStep[0m  [20/42], [94mLoss[0m : 2.59388
[1mStep[0m  [24/42], [94mLoss[0m : 2.68569
[1mStep[0m  [28/42], [94mLoss[0m : 2.70569
[1mStep[0m  [32/42], [94mLoss[0m : 2.71753
[1mStep[0m  [36/42], [94mLoss[0m : 2.57562
[1mStep[0m  [40/42], [94mLoss[0m : 2.62920

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.557, [92mTest[0m: 2.417, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49350
[1mStep[0m  [4/42], [94mLoss[0m : 2.67652
[1mStep[0m  [8/42], [94mLoss[0m : 2.43855
[1mStep[0m  [12/42], [94mLoss[0m : 2.48318
[1mStep[0m  [16/42], [94mLoss[0m : 2.61606
[1mStep[0m  [20/42], [94mLoss[0m : 2.65690
[1mStep[0m  [24/42], [94mLoss[0m : 2.43999
[1mStep[0m  [28/42], [94mLoss[0m : 2.59991
[1mStep[0m  [32/42], [94mLoss[0m : 2.52716
[1mStep[0m  [36/42], [94mLoss[0m : 2.44805
[1mStep[0m  [40/42], [94mLoss[0m : 2.74471

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.549, [92mTest[0m: 2.431, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50382
[1mStep[0m  [4/42], [94mLoss[0m : 2.38301
[1mStep[0m  [8/42], [94mLoss[0m : 2.41414
[1mStep[0m  [12/42], [94mLoss[0m : 2.50223
[1mStep[0m  [16/42], [94mLoss[0m : 2.65911
[1mStep[0m  [20/42], [94mLoss[0m : 2.50830
[1mStep[0m  [24/42], [94mLoss[0m : 2.69597
[1mStep[0m  [28/42], [94mLoss[0m : 2.50164
[1mStep[0m  [32/42], [94mLoss[0m : 2.43029
[1mStep[0m  [36/42], [94mLoss[0m : 2.31613
[1mStep[0m  [40/42], [94mLoss[0m : 2.42662

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.447, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61855
[1mStep[0m  [4/42], [94mLoss[0m : 2.67047
[1mStep[0m  [8/42], [94mLoss[0m : 2.26843
[1mStep[0m  [12/42], [94mLoss[0m : 2.47824
[1mStep[0m  [16/42], [94mLoss[0m : 2.30609
[1mStep[0m  [20/42], [94mLoss[0m : 2.58641
[1mStep[0m  [24/42], [94mLoss[0m : 2.42497
[1mStep[0m  [28/42], [94mLoss[0m : 2.39404
[1mStep[0m  [32/42], [94mLoss[0m : 2.33002
[1mStep[0m  [36/42], [94mLoss[0m : 2.50257
[1mStep[0m  [40/42], [94mLoss[0m : 2.67107

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.417, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55519
[1mStep[0m  [4/42], [94mLoss[0m : 2.43800
[1mStep[0m  [8/42], [94mLoss[0m : 2.75712
[1mStep[0m  [12/42], [94mLoss[0m : 2.46021
[1mStep[0m  [16/42], [94mLoss[0m : 2.47648
[1mStep[0m  [20/42], [94mLoss[0m : 2.39554
[1mStep[0m  [24/42], [94mLoss[0m : 2.43825
[1mStep[0m  [28/42], [94mLoss[0m : 2.43481
[1mStep[0m  [32/42], [94mLoss[0m : 2.68714
[1mStep[0m  [36/42], [94mLoss[0m : 2.36664
[1mStep[0m  [40/42], [94mLoss[0m : 2.59154

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.454, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50413
[1mStep[0m  [4/42], [94mLoss[0m : 2.48674
[1mStep[0m  [8/42], [94mLoss[0m : 2.21874
[1mStep[0m  [12/42], [94mLoss[0m : 2.39670
[1mStep[0m  [16/42], [94mLoss[0m : 2.48511
[1mStep[0m  [20/42], [94mLoss[0m : 2.55139
[1mStep[0m  [24/42], [94mLoss[0m : 2.69191
[1mStep[0m  [28/42], [94mLoss[0m : 2.50452
[1mStep[0m  [32/42], [94mLoss[0m : 2.48991
[1mStep[0m  [36/42], [94mLoss[0m : 2.54199
[1mStep[0m  [40/42], [94mLoss[0m : 2.48407

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.437, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35806
[1mStep[0m  [4/42], [94mLoss[0m : 2.43463
[1mStep[0m  [8/42], [94mLoss[0m : 2.25227
[1mStep[0m  [12/42], [94mLoss[0m : 2.39432
[1mStep[0m  [16/42], [94mLoss[0m : 2.54590
[1mStep[0m  [20/42], [94mLoss[0m : 2.58156
[1mStep[0m  [24/42], [94mLoss[0m : 2.38950
[1mStep[0m  [28/42], [94mLoss[0m : 2.32521
[1mStep[0m  [32/42], [94mLoss[0m : 2.49881
[1mStep[0m  [36/42], [94mLoss[0m : 2.36406
[1mStep[0m  [40/42], [94mLoss[0m : 2.52160

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.469, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37252
[1mStep[0m  [4/42], [94mLoss[0m : 2.36697
[1mStep[0m  [8/42], [94mLoss[0m : 2.26098
[1mStep[0m  [12/42], [94mLoss[0m : 2.25487
[1mStep[0m  [16/42], [94mLoss[0m : 2.39688
[1mStep[0m  [20/42], [94mLoss[0m : 2.17866
[1mStep[0m  [24/42], [94mLoss[0m : 2.30607
[1mStep[0m  [28/42], [94mLoss[0m : 2.39361
[1mStep[0m  [32/42], [94mLoss[0m : 2.40057
[1mStep[0m  [36/42], [94mLoss[0m : 2.14636
[1mStep[0m  [40/42], [94mLoss[0m : 2.52732

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.410, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46072
[1mStep[0m  [4/42], [94mLoss[0m : 2.48489
[1mStep[0m  [8/42], [94mLoss[0m : 2.20960
[1mStep[0m  [12/42], [94mLoss[0m : 2.34075
[1mStep[0m  [16/42], [94mLoss[0m : 2.50960
[1mStep[0m  [20/42], [94mLoss[0m : 2.39695
[1mStep[0m  [24/42], [94mLoss[0m : 2.61189
[1mStep[0m  [28/42], [94mLoss[0m : 2.36588
[1mStep[0m  [32/42], [94mLoss[0m : 2.39996
[1mStep[0m  [36/42], [94mLoss[0m : 2.40259
[1mStep[0m  [40/42], [94mLoss[0m : 2.21039

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.398, [92mTest[0m: 2.450, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26198
[1mStep[0m  [4/42], [94mLoss[0m : 2.52521
[1mStep[0m  [8/42], [94mLoss[0m : 2.23260
[1mStep[0m  [12/42], [94mLoss[0m : 2.43780
[1mStep[0m  [16/42], [94mLoss[0m : 2.23144
[1mStep[0m  [20/42], [94mLoss[0m : 2.21575
[1mStep[0m  [24/42], [94mLoss[0m : 2.52280
[1mStep[0m  [28/42], [94mLoss[0m : 2.39853
[1mStep[0m  [32/42], [94mLoss[0m : 2.30084
[1mStep[0m  [36/42], [94mLoss[0m : 2.44559
[1mStep[0m  [40/42], [94mLoss[0m : 2.28483

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.395, [92mTest[0m: 2.404, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39462
[1mStep[0m  [4/42], [94mLoss[0m : 2.58537
[1mStep[0m  [8/42], [94mLoss[0m : 2.38908
[1mStep[0m  [12/42], [94mLoss[0m : 2.44364
[1mStep[0m  [16/42], [94mLoss[0m : 2.60025
[1mStep[0m  [20/42], [94mLoss[0m : 2.37376
[1mStep[0m  [24/42], [94mLoss[0m : 2.47339
[1mStep[0m  [28/42], [94mLoss[0m : 2.38812
[1mStep[0m  [32/42], [94mLoss[0m : 2.35442
[1mStep[0m  [36/42], [94mLoss[0m : 2.32736
[1mStep[0m  [40/42], [94mLoss[0m : 2.58757

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.366, [92mTest[0m: 2.421, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42496
[1mStep[0m  [4/42], [94mLoss[0m : 2.22064
[1mStep[0m  [8/42], [94mLoss[0m : 2.29863
[1mStep[0m  [12/42], [94mLoss[0m : 2.54800
[1mStep[0m  [16/42], [94mLoss[0m : 2.30780
[1mStep[0m  [20/42], [94mLoss[0m : 2.41534
[1mStep[0m  [24/42], [94mLoss[0m : 2.31482
[1mStep[0m  [28/42], [94mLoss[0m : 2.43838
[1mStep[0m  [32/42], [94mLoss[0m : 2.44235
[1mStep[0m  [36/42], [94mLoss[0m : 2.55609
[1mStep[0m  [40/42], [94mLoss[0m : 2.25195

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.355, [92mTest[0m: 2.432, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.09386
[1mStep[0m  [4/42], [94mLoss[0m : 2.17856
[1mStep[0m  [8/42], [94mLoss[0m : 2.31172
[1mStep[0m  [12/42], [94mLoss[0m : 2.28568
[1mStep[0m  [16/42], [94mLoss[0m : 2.57799
[1mStep[0m  [20/42], [94mLoss[0m : 2.25672
[1mStep[0m  [24/42], [94mLoss[0m : 2.17070
[1mStep[0m  [28/42], [94mLoss[0m : 2.42631
[1mStep[0m  [32/42], [94mLoss[0m : 2.37588
[1mStep[0m  [36/42], [94mLoss[0m : 2.39096
[1mStep[0m  [40/42], [94mLoss[0m : 2.36009

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.328, [92mTest[0m: 2.430, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.07749
[1mStep[0m  [4/42], [94mLoss[0m : 2.46331
[1mStep[0m  [8/42], [94mLoss[0m : 2.34269
[1mStep[0m  [12/42], [94mLoss[0m : 2.41838
[1mStep[0m  [16/42], [94mLoss[0m : 2.17486
[1mStep[0m  [20/42], [94mLoss[0m : 2.37576
[1mStep[0m  [24/42], [94mLoss[0m : 2.43037
[1mStep[0m  [28/42], [94mLoss[0m : 2.41548
[1mStep[0m  [32/42], [94mLoss[0m : 2.19421
[1mStep[0m  [36/42], [94mLoss[0m : 2.18416
[1mStep[0m  [40/42], [94mLoss[0m : 2.26131

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.319, [92mTest[0m: 2.431, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45708
[1mStep[0m  [4/42], [94mLoss[0m : 2.06907
[1mStep[0m  [8/42], [94mLoss[0m : 2.11603
[1mStep[0m  [12/42], [94mLoss[0m : 2.25745
[1mStep[0m  [16/42], [94mLoss[0m : 2.17462
[1mStep[0m  [20/42], [94mLoss[0m : 2.21663
[1mStep[0m  [24/42], [94mLoss[0m : 2.19981
[1mStep[0m  [28/42], [94mLoss[0m : 2.28064
[1mStep[0m  [32/42], [94mLoss[0m : 2.42780
[1mStep[0m  [36/42], [94mLoss[0m : 2.45692
[1mStep[0m  [40/42], [94mLoss[0m : 2.32780

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.295, [92mTest[0m: 2.448, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34519
[1mStep[0m  [4/42], [94mLoss[0m : 2.36553
[1mStep[0m  [8/42], [94mLoss[0m : 2.27010
[1mStep[0m  [12/42], [94mLoss[0m : 2.16094
[1mStep[0m  [16/42], [94mLoss[0m : 2.19671
[1mStep[0m  [20/42], [94mLoss[0m : 2.32817
[1mStep[0m  [24/42], [94mLoss[0m : 2.57558
[1mStep[0m  [28/42], [94mLoss[0m : 2.32412
[1mStep[0m  [32/42], [94mLoss[0m : 2.29975
[1mStep[0m  [36/42], [94mLoss[0m : 2.37237
[1mStep[0m  [40/42], [94mLoss[0m : 2.44226

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.284, [92mTest[0m: 2.416, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.16795
[1mStep[0m  [4/42], [94mLoss[0m : 2.12084
[1mStep[0m  [8/42], [94mLoss[0m : 2.28916
[1mStep[0m  [12/42], [94mLoss[0m : 2.25376
[1mStep[0m  [16/42], [94mLoss[0m : 2.26824
[1mStep[0m  [20/42], [94mLoss[0m : 2.18928
[1mStep[0m  [24/42], [94mLoss[0m : 2.35743
[1mStep[0m  [28/42], [94mLoss[0m : 2.21305
[1mStep[0m  [32/42], [94mLoss[0m : 2.18053
[1mStep[0m  [36/42], [94mLoss[0m : 2.07737
[1mStep[0m  [40/42], [94mLoss[0m : 2.16592

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.245, [92mTest[0m: 2.420, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.20701
[1mStep[0m  [4/42], [94mLoss[0m : 2.23866
[1mStep[0m  [8/42], [94mLoss[0m : 2.16985
[1mStep[0m  [12/42], [94mLoss[0m : 2.29456
[1mStep[0m  [16/42], [94mLoss[0m : 2.17582
[1mStep[0m  [20/42], [94mLoss[0m : 2.25438
[1mStep[0m  [24/42], [94mLoss[0m : 2.17734
[1mStep[0m  [28/42], [94mLoss[0m : 2.15003
[1mStep[0m  [32/42], [94mLoss[0m : 2.36862
[1mStep[0m  [36/42], [94mLoss[0m : 2.30683
[1mStep[0m  [40/42], [94mLoss[0m : 2.19658

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.251, [92mTest[0m: 2.436, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27330
[1mStep[0m  [4/42], [94mLoss[0m : 2.30946
[1mStep[0m  [8/42], [94mLoss[0m : 2.00944
[1mStep[0m  [12/42], [94mLoss[0m : 2.23307
[1mStep[0m  [16/42], [94mLoss[0m : 2.18783
[1mStep[0m  [20/42], [94mLoss[0m : 2.26299
[1mStep[0m  [24/42], [94mLoss[0m : 2.18523
[1mStep[0m  [28/42], [94mLoss[0m : 2.25080
[1mStep[0m  [32/42], [94mLoss[0m : 2.22646
[1mStep[0m  [36/42], [94mLoss[0m : 2.22162
[1mStep[0m  [40/42], [94mLoss[0m : 2.27281

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.220, [92mTest[0m: 2.436, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.12350
[1mStep[0m  [4/42], [94mLoss[0m : 2.23795
[1mStep[0m  [8/42], [94mLoss[0m : 2.43342
[1mStep[0m  [12/42], [94mLoss[0m : 2.24156
[1mStep[0m  [16/42], [94mLoss[0m : 2.21894
[1mStep[0m  [20/42], [94mLoss[0m : 2.10002
[1mStep[0m  [24/42], [94mLoss[0m : 2.14433
[1mStep[0m  [28/42], [94mLoss[0m : 2.16575
[1mStep[0m  [32/42], [94mLoss[0m : 2.07695
[1mStep[0m  [36/42], [94mLoss[0m : 2.31766
[1mStep[0m  [40/42], [94mLoss[0m : 2.35349

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.229, [92mTest[0m: 2.475, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.97976
[1mStep[0m  [4/42], [94mLoss[0m : 2.33937
[1mStep[0m  [8/42], [94mLoss[0m : 2.15658
[1mStep[0m  [12/42], [94mLoss[0m : 2.44055
[1mStep[0m  [16/42], [94mLoss[0m : 2.26012
[1mStep[0m  [20/42], [94mLoss[0m : 2.25430
[1mStep[0m  [24/42], [94mLoss[0m : 2.38387
[1mStep[0m  [28/42], [94mLoss[0m : 2.03175
[1mStep[0m  [32/42], [94mLoss[0m : 2.22878
[1mStep[0m  [36/42], [94mLoss[0m : 2.26408
[1mStep[0m  [40/42], [94mLoss[0m : 2.41985

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.216, [92mTest[0m: 2.438, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.440
====================================

Phase 2 - Evaluation MAE:  2.4403516394751414
MAE score P1      2.988582
MAE score P2      2.440352
loss              2.216325
learning_rate     0.002575
batch_size             256
hidden_sizes         [250]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.1
weight_decay          0.01
Name: 12, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 11.00357
[1mStep[0m  [4/42], [94mLoss[0m : 10.64396
[1mStep[0m  [8/42], [94mLoss[0m : 10.68785
[1mStep[0m  [12/42], [94mLoss[0m : 10.76207
[1mStep[0m  [16/42], [94mLoss[0m : 10.79309
[1mStep[0m  [20/42], [94mLoss[0m : 11.05954
[1mStep[0m  [24/42], [94mLoss[0m : 10.97666
[1mStep[0m  [28/42], [94mLoss[0m : 11.05447
[1mStep[0m  [32/42], [94mLoss[0m : 10.92304
[1mStep[0m  [36/42], [94mLoss[0m : 10.56294
[1mStep[0m  [40/42], [94mLoss[0m : 10.51764

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.827, [92mTest[0m: 11.043, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.44184
[1mStep[0m  [4/42], [94mLoss[0m : 10.76435
[1mStep[0m  [8/42], [94mLoss[0m : 10.39909
[1mStep[0m  [12/42], [94mLoss[0m : 10.32535
[1mStep[0m  [16/42], [94mLoss[0m : 11.02236
[1mStep[0m  [20/42], [94mLoss[0m : 10.72155
[1mStep[0m  [24/42], [94mLoss[0m : 10.26542
[1mStep[0m  [28/42], [94mLoss[0m : 10.29262
[1mStep[0m  [32/42], [94mLoss[0m : 10.03662
[1mStep[0m  [36/42], [94mLoss[0m : 10.15310
[1mStep[0m  [40/42], [94mLoss[0m : 10.34520

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.493, [92mTest[0m: 10.621, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.97414
[1mStep[0m  [4/42], [94mLoss[0m : 10.24751
[1mStep[0m  [8/42], [94mLoss[0m : 10.42586
[1mStep[0m  [12/42], [94mLoss[0m : 9.72409
[1mStep[0m  [16/42], [94mLoss[0m : 9.90159
[1mStep[0m  [20/42], [94mLoss[0m : 9.90686
[1mStep[0m  [24/42], [94mLoss[0m : 10.41050
[1mStep[0m  [28/42], [94mLoss[0m : 10.39210
[1mStep[0m  [32/42], [94mLoss[0m : 9.75883
[1mStep[0m  [36/42], [94mLoss[0m : 10.11525
[1mStep[0m  [40/42], [94mLoss[0m : 9.50764

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.138, [92mTest[0m: 10.167, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.09810
[1mStep[0m  [4/42], [94mLoss[0m : 9.87509
[1mStep[0m  [8/42], [94mLoss[0m : 9.81316
[1mStep[0m  [12/42], [94mLoss[0m : 9.85167
[1mStep[0m  [16/42], [94mLoss[0m : 9.71210
[1mStep[0m  [20/42], [94mLoss[0m : 9.88597
[1mStep[0m  [24/42], [94mLoss[0m : 9.54728
[1mStep[0m  [28/42], [94mLoss[0m : 9.62325
[1mStep[0m  [32/42], [94mLoss[0m : 9.99218
[1mStep[0m  [36/42], [94mLoss[0m : 9.82107
[1mStep[0m  [40/42], [94mLoss[0m : 9.49128

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 9.748, [92mTest[0m: 9.686, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.66303
[1mStep[0m  [4/42], [94mLoss[0m : 9.57311
[1mStep[0m  [8/42], [94mLoss[0m : 9.69255
[1mStep[0m  [12/42], [94mLoss[0m : 9.26079
[1mStep[0m  [16/42], [94mLoss[0m : 9.38327
[1mStep[0m  [20/42], [94mLoss[0m : 9.22812
[1mStep[0m  [24/42], [94mLoss[0m : 9.65391
[1mStep[0m  [28/42], [94mLoss[0m : 9.45311
[1mStep[0m  [32/42], [94mLoss[0m : 9.33757
[1mStep[0m  [36/42], [94mLoss[0m : 9.19441
[1mStep[0m  [40/42], [94mLoss[0m : 8.77409

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 9.301, [92mTest[0m: 9.171, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.95274
[1mStep[0m  [4/42], [94mLoss[0m : 9.01158
[1mStep[0m  [8/42], [94mLoss[0m : 8.96392
[1mStep[0m  [12/42], [94mLoss[0m : 9.22754
[1mStep[0m  [16/42], [94mLoss[0m : 8.89515
[1mStep[0m  [20/42], [94mLoss[0m : 8.80429
[1mStep[0m  [24/42], [94mLoss[0m : 8.67285
[1mStep[0m  [28/42], [94mLoss[0m : 8.64227
[1mStep[0m  [32/42], [94mLoss[0m : 8.50041
[1mStep[0m  [36/42], [94mLoss[0m : 8.63423
[1mStep[0m  [40/42], [94mLoss[0m : 8.31264

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 8.777, [92mTest[0m: 8.609, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.25591
[1mStep[0m  [4/42], [94mLoss[0m : 7.91754
[1mStep[0m  [8/42], [94mLoss[0m : 8.09948
[1mStep[0m  [12/42], [94mLoss[0m : 8.16842
[1mStep[0m  [16/42], [94mLoss[0m : 8.27909
[1mStep[0m  [20/42], [94mLoss[0m : 8.21673
[1mStep[0m  [24/42], [94mLoss[0m : 8.02080
[1mStep[0m  [28/42], [94mLoss[0m : 7.79381
[1mStep[0m  [32/42], [94mLoss[0m : 8.58337
[1mStep[0m  [36/42], [94mLoss[0m : 8.13114
[1mStep[0m  [40/42], [94mLoss[0m : 7.78694

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 8.179, [92mTest[0m: 7.896, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.71702
[1mStep[0m  [4/42], [94mLoss[0m : 7.99070
[1mStep[0m  [8/42], [94mLoss[0m : 7.87237
[1mStep[0m  [12/42], [94mLoss[0m : 7.75321
[1mStep[0m  [16/42], [94mLoss[0m : 7.96461
[1mStep[0m  [20/42], [94mLoss[0m : 7.36382
[1mStep[0m  [24/42], [94mLoss[0m : 7.65448
[1mStep[0m  [28/42], [94mLoss[0m : 7.47390
[1mStep[0m  [32/42], [94mLoss[0m : 7.33100
[1mStep[0m  [36/42], [94mLoss[0m : 7.49121
[1mStep[0m  [40/42], [94mLoss[0m : 7.08503

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 7.552, [92mTest[0m: 7.257, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.21873
[1mStep[0m  [4/42], [94mLoss[0m : 7.20887
[1mStep[0m  [8/42], [94mLoss[0m : 7.14378
[1mStep[0m  [12/42], [94mLoss[0m : 7.05688
[1mStep[0m  [16/42], [94mLoss[0m : 7.03537
[1mStep[0m  [20/42], [94mLoss[0m : 6.81450
[1mStep[0m  [24/42], [94mLoss[0m : 6.87443
[1mStep[0m  [28/42], [94mLoss[0m : 7.51837
[1mStep[0m  [32/42], [94mLoss[0m : 6.98960
[1mStep[0m  [36/42], [94mLoss[0m : 7.12438
[1mStep[0m  [40/42], [94mLoss[0m : 6.79541

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 6.982, [92mTest[0m: 6.516, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.10030
[1mStep[0m  [4/42], [94mLoss[0m : 6.76973
[1mStep[0m  [8/42], [94mLoss[0m : 7.16539
[1mStep[0m  [12/42], [94mLoss[0m : 6.64462
[1mStep[0m  [16/42], [94mLoss[0m : 6.68980
[1mStep[0m  [20/42], [94mLoss[0m : 6.59349
[1mStep[0m  [24/42], [94mLoss[0m : 6.13659
[1mStep[0m  [28/42], [94mLoss[0m : 6.67126
[1mStep[0m  [32/42], [94mLoss[0m : 6.33998
[1mStep[0m  [36/42], [94mLoss[0m : 6.59878
[1mStep[0m  [40/42], [94mLoss[0m : 6.18224

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 6.527, [92mTest[0m: 5.923, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.26693
[1mStep[0m  [4/42], [94mLoss[0m : 6.26122
[1mStep[0m  [8/42], [94mLoss[0m : 6.47573
[1mStep[0m  [12/42], [94mLoss[0m : 6.09685
[1mStep[0m  [16/42], [94mLoss[0m : 6.08320
[1mStep[0m  [20/42], [94mLoss[0m : 6.16527
[1mStep[0m  [24/42], [94mLoss[0m : 6.27282
[1mStep[0m  [28/42], [94mLoss[0m : 6.10937
[1mStep[0m  [32/42], [94mLoss[0m : 5.68175
[1mStep[0m  [36/42], [94mLoss[0m : 6.20999
[1mStep[0m  [40/42], [94mLoss[0m : 6.01956

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 6.101, [92mTest[0m: 5.509, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.88749
[1mStep[0m  [4/42], [94mLoss[0m : 5.74911
[1mStep[0m  [8/42], [94mLoss[0m : 5.89766
[1mStep[0m  [12/42], [94mLoss[0m : 5.54226
[1mStep[0m  [16/42], [94mLoss[0m : 5.79737
[1mStep[0m  [20/42], [94mLoss[0m : 5.31302
[1mStep[0m  [24/42], [94mLoss[0m : 5.46148
[1mStep[0m  [28/42], [94mLoss[0m : 5.88661
[1mStep[0m  [32/42], [94mLoss[0m : 5.70319
[1mStep[0m  [36/42], [94mLoss[0m : 5.74420
[1mStep[0m  [40/42], [94mLoss[0m : 5.51110

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 5.697, [92mTest[0m: 5.031, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.53671
[1mStep[0m  [4/42], [94mLoss[0m : 5.67495
[1mStep[0m  [8/42], [94mLoss[0m : 5.30335
[1mStep[0m  [12/42], [94mLoss[0m : 5.64018
[1mStep[0m  [16/42], [94mLoss[0m : 5.56393
[1mStep[0m  [20/42], [94mLoss[0m : 5.24122
[1mStep[0m  [24/42], [94mLoss[0m : 5.37401
[1mStep[0m  [28/42], [94mLoss[0m : 5.18903
[1mStep[0m  [32/42], [94mLoss[0m : 4.54030
[1mStep[0m  [36/42], [94mLoss[0m : 4.93405
[1mStep[0m  [40/42], [94mLoss[0m : 4.80302

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 5.280, [92mTest[0m: 4.562, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.15451
[1mStep[0m  [4/42], [94mLoss[0m : 5.13955
[1mStep[0m  [8/42], [94mLoss[0m : 5.00297
[1mStep[0m  [12/42], [94mLoss[0m : 4.79876
[1mStep[0m  [16/42], [94mLoss[0m : 4.69575
[1mStep[0m  [20/42], [94mLoss[0m : 5.07793
[1mStep[0m  [24/42], [94mLoss[0m : 4.77453
[1mStep[0m  [28/42], [94mLoss[0m : 4.54686
[1mStep[0m  [32/42], [94mLoss[0m : 4.79473
[1mStep[0m  [36/42], [94mLoss[0m : 4.68485
[1mStep[0m  [40/42], [94mLoss[0m : 4.62641

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 4.872, [92mTest[0m: 4.281, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.64804
[1mStep[0m  [4/42], [94mLoss[0m : 4.35880
[1mStep[0m  [8/42], [94mLoss[0m : 4.40908
[1mStep[0m  [12/42], [94mLoss[0m : 4.21732
[1mStep[0m  [16/42], [94mLoss[0m : 4.29986
[1mStep[0m  [20/42], [94mLoss[0m : 4.12531
[1mStep[0m  [24/42], [94mLoss[0m : 4.29771
[1mStep[0m  [28/42], [94mLoss[0m : 4.30072
[1mStep[0m  [32/42], [94mLoss[0m : 4.48594
[1mStep[0m  [36/42], [94mLoss[0m : 4.74811
[1mStep[0m  [40/42], [94mLoss[0m : 4.25474

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 4.438, [92mTest[0m: 3.829, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.93382
[1mStep[0m  [4/42], [94mLoss[0m : 4.17155
[1mStep[0m  [8/42], [94mLoss[0m : 3.72806
[1mStep[0m  [12/42], [94mLoss[0m : 4.04800
[1mStep[0m  [16/42], [94mLoss[0m : 4.11032
[1mStep[0m  [20/42], [94mLoss[0m : 4.13737
[1mStep[0m  [24/42], [94mLoss[0m : 4.09802
[1mStep[0m  [28/42], [94mLoss[0m : 3.64815
[1mStep[0m  [32/42], [94mLoss[0m : 3.80723
[1mStep[0m  [36/42], [94mLoss[0m : 3.72556
[1mStep[0m  [40/42], [94mLoss[0m : 3.68834

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 4.022, [92mTest[0m: 3.465, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.61655
[1mStep[0m  [4/42], [94mLoss[0m : 3.78240
[1mStep[0m  [8/42], [94mLoss[0m : 3.63077
[1mStep[0m  [12/42], [94mLoss[0m : 3.36341
[1mStep[0m  [16/42], [94mLoss[0m : 3.71288
[1mStep[0m  [20/42], [94mLoss[0m : 3.65719
[1mStep[0m  [24/42], [94mLoss[0m : 3.92243
[1mStep[0m  [28/42], [94mLoss[0m : 3.32551
[1mStep[0m  [32/42], [94mLoss[0m : 3.77593
[1mStep[0m  [36/42], [94mLoss[0m : 3.42786
[1mStep[0m  [40/42], [94mLoss[0m : 3.41289

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 3.595, [92mTest[0m: 3.038, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.29576
[1mStep[0m  [4/42], [94mLoss[0m : 3.49781
[1mStep[0m  [8/42], [94mLoss[0m : 3.52305
[1mStep[0m  [12/42], [94mLoss[0m : 3.17446
[1mStep[0m  [16/42], [94mLoss[0m : 3.40308
[1mStep[0m  [20/42], [94mLoss[0m : 3.19515
[1mStep[0m  [24/42], [94mLoss[0m : 3.22969
[1mStep[0m  [28/42], [94mLoss[0m : 3.00907
[1mStep[0m  [32/42], [94mLoss[0m : 3.04458
[1mStep[0m  [36/42], [94mLoss[0m : 3.01615
[1mStep[0m  [40/42], [94mLoss[0m : 3.11084

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 3.227, [92mTest[0m: 2.791, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.06063
[1mStep[0m  [4/42], [94mLoss[0m : 3.04201
[1mStep[0m  [8/42], [94mLoss[0m : 3.39053
[1mStep[0m  [12/42], [94mLoss[0m : 3.00784
[1mStep[0m  [16/42], [94mLoss[0m : 3.00098
[1mStep[0m  [20/42], [94mLoss[0m : 3.06140
[1mStep[0m  [24/42], [94mLoss[0m : 2.93973
[1mStep[0m  [28/42], [94mLoss[0m : 2.95780
[1mStep[0m  [32/42], [94mLoss[0m : 3.01229
[1mStep[0m  [36/42], [94mLoss[0m : 2.84142
[1mStep[0m  [40/42], [94mLoss[0m : 2.92376

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.994, [92mTest[0m: 2.579, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.68647
[1mStep[0m  [4/42], [94mLoss[0m : 2.84281
[1mStep[0m  [8/42], [94mLoss[0m : 2.96236
[1mStep[0m  [12/42], [94mLoss[0m : 2.88294
[1mStep[0m  [16/42], [94mLoss[0m : 3.10862
[1mStep[0m  [20/42], [94mLoss[0m : 2.71067
[1mStep[0m  [24/42], [94mLoss[0m : 2.71112
[1mStep[0m  [28/42], [94mLoss[0m : 2.91930
[1mStep[0m  [32/42], [94mLoss[0m : 2.89672
[1mStep[0m  [36/42], [94mLoss[0m : 2.74389
[1mStep[0m  [40/42], [94mLoss[0m : 2.76604

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.824, [92mTest[0m: 2.441, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.79459
[1mStep[0m  [4/42], [94mLoss[0m : 2.67284
[1mStep[0m  [8/42], [94mLoss[0m : 2.82420
[1mStep[0m  [12/42], [94mLoss[0m : 2.67609
[1mStep[0m  [16/42], [94mLoss[0m : 2.96026
[1mStep[0m  [20/42], [94mLoss[0m : 2.69331
[1mStep[0m  [24/42], [94mLoss[0m : 2.77267
[1mStep[0m  [28/42], [94mLoss[0m : 2.60604
[1mStep[0m  [32/42], [94mLoss[0m : 2.73017
[1mStep[0m  [36/42], [94mLoss[0m : 2.80934
[1mStep[0m  [40/42], [94mLoss[0m : 2.87700

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.724, [92mTest[0m: 2.372, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.73874
[1mStep[0m  [4/42], [94mLoss[0m : 2.65113
[1mStep[0m  [8/42], [94mLoss[0m : 2.78276
[1mStep[0m  [12/42], [94mLoss[0m : 2.56891
[1mStep[0m  [16/42], [94mLoss[0m : 2.84880
[1mStep[0m  [20/42], [94mLoss[0m : 2.68341
[1mStep[0m  [24/42], [94mLoss[0m : 2.72091
[1mStep[0m  [28/42], [94mLoss[0m : 2.70177
[1mStep[0m  [32/42], [94mLoss[0m : 2.81672
[1mStep[0m  [36/42], [94mLoss[0m : 2.58714
[1mStep[0m  [40/42], [94mLoss[0m : 2.68017

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.712, [92mTest[0m: 2.361, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.74985
[1mStep[0m  [4/42], [94mLoss[0m : 2.66980
[1mStep[0m  [8/42], [94mLoss[0m : 2.62315
[1mStep[0m  [12/42], [94mLoss[0m : 2.65383
[1mStep[0m  [16/42], [94mLoss[0m : 2.77885
[1mStep[0m  [20/42], [94mLoss[0m : 2.71003
[1mStep[0m  [24/42], [94mLoss[0m : 2.60326
[1mStep[0m  [28/42], [94mLoss[0m : 2.58094
[1mStep[0m  [32/42], [94mLoss[0m : 2.72947
[1mStep[0m  [36/42], [94mLoss[0m : 2.74838
[1mStep[0m  [40/42], [94mLoss[0m : 2.70880

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.693, [92mTest[0m: 2.346, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.69477
[1mStep[0m  [4/42], [94mLoss[0m : 2.65380
[1mStep[0m  [8/42], [94mLoss[0m : 2.69075
[1mStep[0m  [12/42], [94mLoss[0m : 2.62371
[1mStep[0m  [16/42], [94mLoss[0m : 2.71794
[1mStep[0m  [20/42], [94mLoss[0m : 2.49757
[1mStep[0m  [24/42], [94mLoss[0m : 2.60354
[1mStep[0m  [28/42], [94mLoss[0m : 2.63471
[1mStep[0m  [32/42], [94mLoss[0m : 2.71999
[1mStep[0m  [36/42], [94mLoss[0m : 2.66032
[1mStep[0m  [40/42], [94mLoss[0m : 2.50996

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.669, [92mTest[0m: 2.351, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.67178
[1mStep[0m  [4/42], [94mLoss[0m : 2.58713
[1mStep[0m  [8/42], [94mLoss[0m : 2.53076
[1mStep[0m  [12/42], [94mLoss[0m : 2.90585
[1mStep[0m  [16/42], [94mLoss[0m : 2.71514
[1mStep[0m  [20/42], [94mLoss[0m : 2.57180
[1mStep[0m  [24/42], [94mLoss[0m : 2.66627
[1mStep[0m  [28/42], [94mLoss[0m : 2.53936
[1mStep[0m  [32/42], [94mLoss[0m : 2.84687
[1mStep[0m  [36/42], [94mLoss[0m : 2.45740
[1mStep[0m  [40/42], [94mLoss[0m : 2.52776

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.660, [92mTest[0m: 2.346, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.68930
[1mStep[0m  [4/42], [94mLoss[0m : 2.52909
[1mStep[0m  [8/42], [94mLoss[0m : 2.73072
[1mStep[0m  [12/42], [94mLoss[0m : 2.59948
[1mStep[0m  [16/42], [94mLoss[0m : 2.72893
[1mStep[0m  [20/42], [94mLoss[0m : 2.56255
[1mStep[0m  [24/42], [94mLoss[0m : 2.54829
[1mStep[0m  [28/42], [94mLoss[0m : 2.63460
[1mStep[0m  [32/42], [94mLoss[0m : 2.42262
[1mStep[0m  [36/42], [94mLoss[0m : 2.40892
[1mStep[0m  [40/42], [94mLoss[0m : 2.78610

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.640, [92mTest[0m: 2.346, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.75918
[1mStep[0m  [4/42], [94mLoss[0m : 2.61171
[1mStep[0m  [8/42], [94mLoss[0m : 2.48755
[1mStep[0m  [12/42], [94mLoss[0m : 2.68993
[1mStep[0m  [16/42], [94mLoss[0m : 2.53867
[1mStep[0m  [20/42], [94mLoss[0m : 2.64706
[1mStep[0m  [24/42], [94mLoss[0m : 2.80880
[1mStep[0m  [28/42], [94mLoss[0m : 2.58160
[1mStep[0m  [32/42], [94mLoss[0m : 2.62361
[1mStep[0m  [36/42], [94mLoss[0m : 2.61942
[1mStep[0m  [40/42], [94mLoss[0m : 2.57345

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.647, [92mTest[0m: 2.374, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.75133
[1mStep[0m  [4/42], [94mLoss[0m : 2.61347
[1mStep[0m  [8/42], [94mLoss[0m : 2.38684
[1mStep[0m  [12/42], [94mLoss[0m : 2.71805
[1mStep[0m  [16/42], [94mLoss[0m : 2.47775
[1mStep[0m  [20/42], [94mLoss[0m : 2.73193
[1mStep[0m  [24/42], [94mLoss[0m : 2.54953
[1mStep[0m  [28/42], [94mLoss[0m : 2.67068
[1mStep[0m  [32/42], [94mLoss[0m : 2.79197
[1mStep[0m  [36/42], [94mLoss[0m : 2.79915
[1mStep[0m  [40/42], [94mLoss[0m : 2.53011

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.637, [92mTest[0m: 2.371, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49646
[1mStep[0m  [4/42], [94mLoss[0m : 2.39299
[1mStep[0m  [8/42], [94mLoss[0m : 2.71471
[1mStep[0m  [12/42], [94mLoss[0m : 2.74967
[1mStep[0m  [16/42], [94mLoss[0m : 2.66453
[1mStep[0m  [20/42], [94mLoss[0m : 2.50893
[1mStep[0m  [24/42], [94mLoss[0m : 2.44776
[1mStep[0m  [28/42], [94mLoss[0m : 2.63842
[1mStep[0m  [32/42], [94mLoss[0m : 2.66658
[1mStep[0m  [36/42], [94mLoss[0m : 2.54200
[1mStep[0m  [40/42], [94mLoss[0m : 2.24730

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.610, [92mTest[0m: 2.365, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.66613
[1mStep[0m  [4/42], [94mLoss[0m : 2.46405
[1mStep[0m  [8/42], [94mLoss[0m : 2.52433
[1mStep[0m  [12/42], [94mLoss[0m : 2.56630
[1mStep[0m  [16/42], [94mLoss[0m : 2.68278
[1mStep[0m  [20/42], [94mLoss[0m : 2.75043
[1mStep[0m  [24/42], [94mLoss[0m : 2.53280
[1mStep[0m  [28/42], [94mLoss[0m : 2.56857
[1mStep[0m  [32/42], [94mLoss[0m : 2.55503
[1mStep[0m  [36/42], [94mLoss[0m : 2.71769
[1mStep[0m  [40/42], [94mLoss[0m : 2.65689

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.653, [92mTest[0m: 2.366, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.366
====================================

Phase 1 - Evaluation MAE:  2.3660717521395003
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 2.67718
[1mStep[0m  [4/42], [94mLoss[0m : 2.56208
[1mStep[0m  [8/42], [94mLoss[0m : 2.76256
[1mStep[0m  [12/42], [94mLoss[0m : 2.67456
[1mStep[0m  [16/42], [94mLoss[0m : 2.61701
[1mStep[0m  [20/42], [94mLoss[0m : 2.46316
[1mStep[0m  [24/42], [94mLoss[0m : 2.72281
[1mStep[0m  [28/42], [94mLoss[0m : 2.98611
[1mStep[0m  [32/42], [94mLoss[0m : 2.40225
[1mStep[0m  [36/42], [94mLoss[0m : 2.78152
[1mStep[0m  [40/42], [94mLoss[0m : 2.66119

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.689, [92mTest[0m: 2.363, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.83262
[1mStep[0m  [4/42], [94mLoss[0m : 2.76257
[1mStep[0m  [8/42], [94mLoss[0m : 2.62528
[1mStep[0m  [12/42], [94mLoss[0m : 2.80080
[1mStep[0m  [16/42], [94mLoss[0m : 2.56490
[1mStep[0m  [20/42], [94mLoss[0m : 2.62296
[1mStep[0m  [24/42], [94mLoss[0m : 2.71597
[1mStep[0m  [28/42], [94mLoss[0m : 2.67728
[1mStep[0m  [32/42], [94mLoss[0m : 2.73748
[1mStep[0m  [36/42], [94mLoss[0m : 2.60997
[1mStep[0m  [40/42], [94mLoss[0m : 2.72388

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.650, [92mTest[0m: 2.361, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53724
[1mStep[0m  [4/42], [94mLoss[0m : 2.44964
[1mStep[0m  [8/42], [94mLoss[0m : 2.81961
[1mStep[0m  [12/42], [94mLoss[0m : 2.38170
[1mStep[0m  [16/42], [94mLoss[0m : 2.49251
[1mStep[0m  [20/42], [94mLoss[0m : 2.46350
[1mStep[0m  [24/42], [94mLoss[0m : 2.66282
[1mStep[0m  [28/42], [94mLoss[0m : 2.73722
[1mStep[0m  [32/42], [94mLoss[0m : 2.72723
[1mStep[0m  [36/42], [94mLoss[0m : 2.52565
[1mStep[0m  [40/42], [94mLoss[0m : 2.37409

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.607, [92mTest[0m: 2.403, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40917
[1mStep[0m  [4/42], [94mLoss[0m : 2.50133
[1mStep[0m  [8/42], [94mLoss[0m : 2.62672
[1mStep[0m  [12/42], [94mLoss[0m : 2.65514
[1mStep[0m  [16/42], [94mLoss[0m : 2.87330
[1mStep[0m  [20/42], [94mLoss[0m : 2.62618
[1mStep[0m  [24/42], [94mLoss[0m : 2.51897
[1mStep[0m  [28/42], [94mLoss[0m : 2.57325
[1mStep[0m  [32/42], [94mLoss[0m : 2.46929
[1mStep[0m  [36/42], [94mLoss[0m : 2.48510
[1mStep[0m  [40/42], [94mLoss[0m : 2.37903

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.587, [92mTest[0m: 2.538, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61722
[1mStep[0m  [4/42], [94mLoss[0m : 2.86548
[1mStep[0m  [8/42], [94mLoss[0m : 2.63156
[1mStep[0m  [12/42], [94mLoss[0m : 2.27338
[1mStep[0m  [16/42], [94mLoss[0m : 2.62154
[1mStep[0m  [20/42], [94mLoss[0m : 2.41588
[1mStep[0m  [24/42], [94mLoss[0m : 2.70844
[1mStep[0m  [28/42], [94mLoss[0m : 2.44019
[1mStep[0m  [32/42], [94mLoss[0m : 2.45947
[1mStep[0m  [36/42], [94mLoss[0m : 2.50140
[1mStep[0m  [40/42], [94mLoss[0m : 2.64526

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.562, [92mTest[0m: 2.585, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37543
[1mStep[0m  [4/42], [94mLoss[0m : 2.63407
[1mStep[0m  [8/42], [94mLoss[0m : 2.56026
[1mStep[0m  [12/42], [94mLoss[0m : 2.67769
[1mStep[0m  [16/42], [94mLoss[0m : 2.29328
[1mStep[0m  [20/42], [94mLoss[0m : 2.60216
[1mStep[0m  [24/42], [94mLoss[0m : 2.33545
[1mStep[0m  [28/42], [94mLoss[0m : 2.62379
[1mStep[0m  [32/42], [94mLoss[0m : 2.75018
[1mStep[0m  [36/42], [94mLoss[0m : 2.33903
[1mStep[0m  [40/42], [94mLoss[0m : 2.74852

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.530, [92mTest[0m: 2.497, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48732
[1mStep[0m  [4/42], [94mLoss[0m : 2.53772
[1mStep[0m  [8/42], [94mLoss[0m : 2.46486
[1mStep[0m  [12/42], [94mLoss[0m : 2.41104
[1mStep[0m  [16/42], [94mLoss[0m : 2.22602
[1mStep[0m  [20/42], [94mLoss[0m : 2.54676
[1mStep[0m  [24/42], [94mLoss[0m : 2.42939
[1mStep[0m  [28/42], [94mLoss[0m : 2.65430
[1mStep[0m  [32/42], [94mLoss[0m : 2.43994
[1mStep[0m  [36/42], [94mLoss[0m : 2.43287
[1mStep[0m  [40/42], [94mLoss[0m : 2.54266

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.514, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.76734
[1mStep[0m  [4/42], [94mLoss[0m : 2.35308
[1mStep[0m  [8/42], [94mLoss[0m : 2.53641
[1mStep[0m  [12/42], [94mLoss[0m : 2.37003
[1mStep[0m  [16/42], [94mLoss[0m : 2.54891
[1mStep[0m  [20/42], [94mLoss[0m : 2.52981
[1mStep[0m  [24/42], [94mLoss[0m : 2.45226
[1mStep[0m  [28/42], [94mLoss[0m : 2.28132
[1mStep[0m  [32/42], [94mLoss[0m : 2.39690
[1mStep[0m  [36/42], [94mLoss[0m : 2.52845
[1mStep[0m  [40/42], [94mLoss[0m : 2.51108

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.497, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43370
[1mStep[0m  [4/42], [94mLoss[0m : 2.63738
[1mStep[0m  [8/42], [94mLoss[0m : 2.53913
[1mStep[0m  [12/42], [94mLoss[0m : 2.49696
[1mStep[0m  [16/42], [94mLoss[0m : 2.51325
[1mStep[0m  [20/42], [94mLoss[0m : 2.43461
[1mStep[0m  [24/42], [94mLoss[0m : 2.30267
[1mStep[0m  [28/42], [94mLoss[0m : 2.47275
[1mStep[0m  [32/42], [94mLoss[0m : 2.47197
[1mStep[0m  [36/42], [94mLoss[0m : 2.64550
[1mStep[0m  [40/42], [94mLoss[0m : 2.61724

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.616, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53455
[1mStep[0m  [4/42], [94mLoss[0m : 2.36556
[1mStep[0m  [8/42], [94mLoss[0m : 2.38520
[1mStep[0m  [12/42], [94mLoss[0m : 2.43605
[1mStep[0m  [16/42], [94mLoss[0m : 2.11632
[1mStep[0m  [20/42], [94mLoss[0m : 2.52892
[1mStep[0m  [24/42], [94mLoss[0m : 2.62941
[1mStep[0m  [28/42], [94mLoss[0m : 2.47653
[1mStep[0m  [32/42], [94mLoss[0m : 2.64429
[1mStep[0m  [36/42], [94mLoss[0m : 2.45852
[1mStep[0m  [40/42], [94mLoss[0m : 2.44534

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.664, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52156
[1mStep[0m  [4/42], [94mLoss[0m : 2.51619
[1mStep[0m  [8/42], [94mLoss[0m : 2.32630
[1mStep[0m  [12/42], [94mLoss[0m : 2.38100
[1mStep[0m  [16/42], [94mLoss[0m : 2.25106
[1mStep[0m  [20/42], [94mLoss[0m : 2.31317
[1mStep[0m  [24/42], [94mLoss[0m : 2.35389
[1mStep[0m  [28/42], [94mLoss[0m : 2.46815
[1mStep[0m  [32/42], [94mLoss[0m : 2.33515
[1mStep[0m  [36/42], [94mLoss[0m : 2.44003
[1mStep[0m  [40/42], [94mLoss[0m : 2.51036

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.618, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29362
[1mStep[0m  [4/42], [94mLoss[0m : 2.41574
[1mStep[0m  [8/42], [94mLoss[0m : 2.44659
[1mStep[0m  [12/42], [94mLoss[0m : 2.32172
[1mStep[0m  [16/42], [94mLoss[0m : 2.40797
[1mStep[0m  [20/42], [94mLoss[0m : 2.40217
[1mStep[0m  [24/42], [94mLoss[0m : 2.51762
[1mStep[0m  [28/42], [94mLoss[0m : 2.49698
[1mStep[0m  [32/42], [94mLoss[0m : 2.17207
[1mStep[0m  [36/42], [94mLoss[0m : 2.37276
[1mStep[0m  [40/42], [94mLoss[0m : 2.33277

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.384, [92mTest[0m: 2.592, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41772
[1mStep[0m  [4/42], [94mLoss[0m : 2.28919
[1mStep[0m  [8/42], [94mLoss[0m : 2.32261
[1mStep[0m  [12/42], [94mLoss[0m : 2.23108
[1mStep[0m  [16/42], [94mLoss[0m : 2.40251
[1mStep[0m  [20/42], [94mLoss[0m : 2.59357
[1mStep[0m  [24/42], [94mLoss[0m : 2.43560
[1mStep[0m  [28/42], [94mLoss[0m : 2.41515
[1mStep[0m  [32/42], [94mLoss[0m : 2.36803
[1mStep[0m  [36/42], [94mLoss[0m : 2.34688
[1mStep[0m  [40/42], [94mLoss[0m : 2.38653

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.362, [92mTest[0m: 2.602, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33223
[1mStep[0m  [4/42], [94mLoss[0m : 2.29791
[1mStep[0m  [8/42], [94mLoss[0m : 2.37397
[1mStep[0m  [12/42], [94mLoss[0m : 2.36005
[1mStep[0m  [16/42], [94mLoss[0m : 2.50581
[1mStep[0m  [20/42], [94mLoss[0m : 2.47716
[1mStep[0m  [24/42], [94mLoss[0m : 2.35024
[1mStep[0m  [28/42], [94mLoss[0m : 2.30175
[1mStep[0m  [32/42], [94mLoss[0m : 1.93839
[1mStep[0m  [36/42], [94mLoss[0m : 2.44300
[1mStep[0m  [40/42], [94mLoss[0m : 2.28978

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.334, [92mTest[0m: 2.646, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31950
[1mStep[0m  [4/42], [94mLoss[0m : 2.34868
[1mStep[0m  [8/42], [94mLoss[0m : 2.40416
[1mStep[0m  [12/42], [94mLoss[0m : 2.36865
[1mStep[0m  [16/42], [94mLoss[0m : 2.28719
[1mStep[0m  [20/42], [94mLoss[0m : 2.42336
[1mStep[0m  [24/42], [94mLoss[0m : 2.40975
[1mStep[0m  [28/42], [94mLoss[0m : 2.21689
[1mStep[0m  [32/42], [94mLoss[0m : 2.01921
[1mStep[0m  [36/42], [94mLoss[0m : 2.45719
[1mStep[0m  [40/42], [94mLoss[0m : 2.20926

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.301, [92mTest[0m: 2.623, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26894
[1mStep[0m  [4/42], [94mLoss[0m : 2.56097
[1mStep[0m  [8/42], [94mLoss[0m : 2.16792
[1mStep[0m  [12/42], [94mLoss[0m : 2.36169
[1mStep[0m  [16/42], [94mLoss[0m : 2.25582
[1mStep[0m  [20/42], [94mLoss[0m : 2.00121
[1mStep[0m  [24/42], [94mLoss[0m : 2.19351
[1mStep[0m  [28/42], [94mLoss[0m : 2.25002
[1mStep[0m  [32/42], [94mLoss[0m : 2.44445
[1mStep[0m  [36/42], [94mLoss[0m : 2.27641
[1mStep[0m  [40/42], [94mLoss[0m : 2.28612

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.288, [92mTest[0m: 2.650, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41817
[1mStep[0m  [4/42], [94mLoss[0m : 2.09075
[1mStep[0m  [8/42], [94mLoss[0m : 1.96046
[1mStep[0m  [12/42], [94mLoss[0m : 2.54285
[1mStep[0m  [16/42], [94mLoss[0m : 2.27613
[1mStep[0m  [20/42], [94mLoss[0m : 2.16310
[1mStep[0m  [24/42], [94mLoss[0m : 2.03491
[1mStep[0m  [28/42], [94mLoss[0m : 2.45657
[1mStep[0m  [32/42], [94mLoss[0m : 2.39578
[1mStep[0m  [36/42], [94mLoss[0m : 2.22138
[1mStep[0m  [40/42], [94mLoss[0m : 2.27554

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.252, [92mTest[0m: 2.636, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.06526
[1mStep[0m  [4/42], [94mLoss[0m : 2.29519
[1mStep[0m  [8/42], [94mLoss[0m : 2.19175
[1mStep[0m  [12/42], [94mLoss[0m : 2.33781
[1mStep[0m  [16/42], [94mLoss[0m : 2.30506
[1mStep[0m  [20/42], [94mLoss[0m : 2.34277
[1mStep[0m  [24/42], [94mLoss[0m : 2.25157
[1mStep[0m  [28/42], [94mLoss[0m : 2.08676
[1mStep[0m  [32/42], [94mLoss[0m : 2.29121
[1mStep[0m  [36/42], [94mLoss[0m : 2.16559
[1mStep[0m  [40/42], [94mLoss[0m : 2.37332

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.227, [92mTest[0m: 2.536, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.17717
[1mStep[0m  [4/42], [94mLoss[0m : 2.09720
[1mStep[0m  [8/42], [94mLoss[0m : 2.27548
[1mStep[0m  [12/42], [94mLoss[0m : 2.13808
[1mStep[0m  [16/42], [94mLoss[0m : 2.19447
[1mStep[0m  [20/42], [94mLoss[0m : 2.21625
[1mStep[0m  [24/42], [94mLoss[0m : 2.11105
[1mStep[0m  [28/42], [94mLoss[0m : 2.08528
[1mStep[0m  [32/42], [94mLoss[0m : 2.43348
[1mStep[0m  [36/42], [94mLoss[0m : 2.32867
[1mStep[0m  [40/42], [94mLoss[0m : 2.23714

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.213, [92mTest[0m: 2.564, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.19138
[1mStep[0m  [4/42], [94mLoss[0m : 1.94964
[1mStep[0m  [8/42], [94mLoss[0m : 2.22006
[1mStep[0m  [12/42], [94mLoss[0m : 2.23496
[1mStep[0m  [16/42], [94mLoss[0m : 2.00333
[1mStep[0m  [20/42], [94mLoss[0m : 2.00648
[1mStep[0m  [24/42], [94mLoss[0m : 2.15860
[1mStep[0m  [28/42], [94mLoss[0m : 2.37257
[1mStep[0m  [32/42], [94mLoss[0m : 2.46337
[1mStep[0m  [36/42], [94mLoss[0m : 2.18109
[1mStep[0m  [40/42], [94mLoss[0m : 2.31168

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.181, [92mTest[0m: 2.617, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.11706
[1mStep[0m  [4/42], [94mLoss[0m : 2.27722
[1mStep[0m  [8/42], [94mLoss[0m : 2.10607
[1mStep[0m  [12/42], [94mLoss[0m : 2.32519
[1mStep[0m  [16/42], [94mLoss[0m : 2.13852
[1mStep[0m  [20/42], [94mLoss[0m : 2.21237
[1mStep[0m  [24/42], [94mLoss[0m : 2.39081
[1mStep[0m  [28/42], [94mLoss[0m : 2.39978
[1mStep[0m  [32/42], [94mLoss[0m : 2.22051
[1mStep[0m  [36/42], [94mLoss[0m : 2.26819
[1mStep[0m  [40/42], [94mLoss[0m : 2.11706

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.166, [92mTest[0m: 2.568, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.20619
[1mStep[0m  [4/42], [94mLoss[0m : 2.25722
[1mStep[0m  [8/42], [94mLoss[0m : 2.11987
[1mStep[0m  [12/42], [94mLoss[0m : 2.13681
[1mStep[0m  [16/42], [94mLoss[0m : 2.22486
[1mStep[0m  [20/42], [94mLoss[0m : 2.16465
[1mStep[0m  [24/42], [94mLoss[0m : 2.15639
[1mStep[0m  [28/42], [94mLoss[0m : 1.99312
[1mStep[0m  [32/42], [94mLoss[0m : 2.11084
[1mStep[0m  [36/42], [94mLoss[0m : 2.08875
[1mStep[0m  [40/42], [94mLoss[0m : 2.09925

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.140, [92mTest[0m: 2.541, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.18609
[1mStep[0m  [4/42], [94mLoss[0m : 1.91218
[1mStep[0m  [8/42], [94mLoss[0m : 1.90631
[1mStep[0m  [12/42], [94mLoss[0m : 2.08051
[1mStep[0m  [16/42], [94mLoss[0m : 2.28610
[1mStep[0m  [20/42], [94mLoss[0m : 2.06622
[1mStep[0m  [24/42], [94mLoss[0m : 2.17174
[1mStep[0m  [28/42], [94mLoss[0m : 2.18777
[1mStep[0m  [32/42], [94mLoss[0m : 2.14117
[1mStep[0m  [36/42], [94mLoss[0m : 2.11064
[1mStep[0m  [40/42], [94mLoss[0m : 2.09135

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.121, [92mTest[0m: 2.557, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.09367
[1mStep[0m  [4/42], [94mLoss[0m : 2.13619
[1mStep[0m  [8/42], [94mLoss[0m : 1.83869
[1mStep[0m  [12/42], [94mLoss[0m : 2.10333
[1mStep[0m  [16/42], [94mLoss[0m : 1.97246
[1mStep[0m  [20/42], [94mLoss[0m : 1.86894
[1mStep[0m  [24/42], [94mLoss[0m : 2.05518
[1mStep[0m  [28/42], [94mLoss[0m : 2.24114
[1mStep[0m  [32/42], [94mLoss[0m : 2.25602
[1mStep[0m  [36/42], [94mLoss[0m : 2.19182
[1mStep[0m  [40/42], [94mLoss[0m : 2.15343

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.090, [92mTest[0m: 2.536, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.12833
[1mStep[0m  [4/42], [94mLoss[0m : 2.04044
[1mStep[0m  [8/42], [94mLoss[0m : 2.01982
[1mStep[0m  [12/42], [94mLoss[0m : 1.94657
[1mStep[0m  [16/42], [94mLoss[0m : 2.16633
[1mStep[0m  [20/42], [94mLoss[0m : 2.06168
[1mStep[0m  [24/42], [94mLoss[0m : 2.10015
[1mStep[0m  [28/42], [94mLoss[0m : 1.79308
[1mStep[0m  [32/42], [94mLoss[0m : 2.15264
[1mStep[0m  [36/42], [94mLoss[0m : 2.26349
[1mStep[0m  [40/42], [94mLoss[0m : 1.95760

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.069, [92mTest[0m: 2.573, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.19977
[1mStep[0m  [4/42], [94mLoss[0m : 1.86232
[1mStep[0m  [8/42], [94mLoss[0m : 2.11124
[1mStep[0m  [12/42], [94mLoss[0m : 1.99156
[1mStep[0m  [16/42], [94mLoss[0m : 2.01124
[1mStep[0m  [20/42], [94mLoss[0m : 2.21754
[1mStep[0m  [24/42], [94mLoss[0m : 2.15994
[1mStep[0m  [28/42], [94mLoss[0m : 2.15539
[1mStep[0m  [32/42], [94mLoss[0m : 2.04944
[1mStep[0m  [36/42], [94mLoss[0m : 1.97876
[1mStep[0m  [40/42], [94mLoss[0m : 2.13396

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.061, [92mTest[0m: 2.546, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.02412
[1mStep[0m  [4/42], [94mLoss[0m : 2.05582
[1mStep[0m  [8/42], [94mLoss[0m : 1.93015
[1mStep[0m  [12/42], [94mLoss[0m : 2.01662
[1mStep[0m  [16/42], [94mLoss[0m : 2.00143
[1mStep[0m  [20/42], [94mLoss[0m : 1.87789
[1mStep[0m  [24/42], [94mLoss[0m : 2.08512
[1mStep[0m  [28/42], [94mLoss[0m : 1.85966
[1mStep[0m  [32/42], [94mLoss[0m : 2.03448
[1mStep[0m  [36/42], [94mLoss[0m : 2.07204
[1mStep[0m  [40/42], [94mLoss[0m : 1.93863

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.023, [92mTest[0m: 2.482, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.88413
[1mStep[0m  [4/42], [94mLoss[0m : 1.98639
[1mStep[0m  [8/42], [94mLoss[0m : 2.06478
[1mStep[0m  [12/42], [94mLoss[0m : 2.08584
[1mStep[0m  [16/42], [94mLoss[0m : 1.92067
[1mStep[0m  [20/42], [94mLoss[0m : 2.18024
[1mStep[0m  [24/42], [94mLoss[0m : 2.11770
[1mStep[0m  [28/42], [94mLoss[0m : 1.97446
[1mStep[0m  [32/42], [94mLoss[0m : 1.92883
[1mStep[0m  [36/42], [94mLoss[0m : 2.00884
[1mStep[0m  [40/42], [94mLoss[0m : 1.88346

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.013, [92mTest[0m: 2.596, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.97322
[1mStep[0m  [4/42], [94mLoss[0m : 1.99843
[1mStep[0m  [8/42], [94mLoss[0m : 1.85119
[1mStep[0m  [12/42], [94mLoss[0m : 2.13385
[1mStep[0m  [16/42], [94mLoss[0m : 1.89187
[1mStep[0m  [20/42], [94mLoss[0m : 2.06120
[1mStep[0m  [24/42], [94mLoss[0m : 1.91507
[1mStep[0m  [28/42], [94mLoss[0m : 1.90582
[1mStep[0m  [32/42], [94mLoss[0m : 1.85967
[1mStep[0m  [36/42], [94mLoss[0m : 1.88882
[1mStep[0m  [40/42], [94mLoss[0m : 1.93659

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.992, [92mTest[0m: 2.596, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.98717
[1mStep[0m  [4/42], [94mLoss[0m : 2.00160
[1mStep[0m  [8/42], [94mLoss[0m : 1.99647
[1mStep[0m  [12/42], [94mLoss[0m : 1.99765
[1mStep[0m  [16/42], [94mLoss[0m : 1.93128
[1mStep[0m  [20/42], [94mLoss[0m : 2.09780
[1mStep[0m  [24/42], [94mLoss[0m : 2.09659
[1mStep[0m  [28/42], [94mLoss[0m : 2.01618
[1mStep[0m  [32/42], [94mLoss[0m : 1.96323
[1mStep[0m  [36/42], [94mLoss[0m : 1.94903
[1mStep[0m  [40/42], [94mLoss[0m : 2.08021

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.987, [92mTest[0m: 2.650, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.567
====================================

Phase 2 - Evaluation MAE:  2.566973567008972
MAE score P1      2.366072
MAE score P2      2.566974
loss              1.986906
learning_rate     0.002575
batch_size             256
hidden_sizes         [100]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.5
weight_decay        0.0001
Name: 13, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/21], [94mLoss[0m : 11.33784
[1mStep[0m  [2/21], [94mLoss[0m : 10.74163
[1mStep[0m  [4/21], [94mLoss[0m : 10.29436
[1mStep[0m  [6/21], [94mLoss[0m : 10.02374
[1mStep[0m  [8/21], [94mLoss[0m : 9.70461
[1mStep[0m  [10/21], [94mLoss[0m : 8.95068
[1mStep[0m  [12/21], [94mLoss[0m : 8.46384
[1mStep[0m  [14/21], [94mLoss[0m : 8.26333
[1mStep[0m  [16/21], [94mLoss[0m : 7.58633
[1mStep[0m  [18/21], [94mLoss[0m : 7.60427
[1mStep[0m  [20/21], [94mLoss[0m : 7.00421

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.132, [92mTest[0m: 11.305, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.58068
[1mStep[0m  [2/21], [94mLoss[0m : 6.25833
[1mStep[0m  [4/21], [94mLoss[0m : 5.78270
[1mStep[0m  [6/21], [94mLoss[0m : 5.36967
[1mStep[0m  [8/21], [94mLoss[0m : 5.10208
[1mStep[0m  [10/21], [94mLoss[0m : 4.80042
[1mStep[0m  [12/21], [94mLoss[0m : 4.31967
[1mStep[0m  [14/21], [94mLoss[0m : 4.08429
[1mStep[0m  [16/21], [94mLoss[0m : 3.94846
[1mStep[0m  [18/21], [94mLoss[0m : 3.68059
[1mStep[0m  [20/21], [94mLoss[0m : 3.63654

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 4.875, [92mTest[0m: 6.710, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.31750
[1mStep[0m  [2/21], [94mLoss[0m : 3.32608
[1mStep[0m  [4/21], [94mLoss[0m : 3.22416
[1mStep[0m  [6/21], [94mLoss[0m : 3.31049
[1mStep[0m  [8/21], [94mLoss[0m : 3.11247
[1mStep[0m  [10/21], [94mLoss[0m : 2.97982
[1mStep[0m  [12/21], [94mLoss[0m : 2.81617
[1mStep[0m  [14/21], [94mLoss[0m : 3.19027
[1mStep[0m  [16/21], [94mLoss[0m : 2.96153
[1mStep[0m  [18/21], [94mLoss[0m : 2.99586
[1mStep[0m  [20/21], [94mLoss[0m : 2.87793

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.118, [92mTest[0m: 3.453, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.83919
[1mStep[0m  [2/21], [94mLoss[0m : 2.82886
[1mStep[0m  [4/21], [94mLoss[0m : 2.87962
[1mStep[0m  [6/21], [94mLoss[0m : 2.86873
[1mStep[0m  [8/21], [94mLoss[0m : 2.69899
[1mStep[0m  [10/21], [94mLoss[0m : 2.79501
[1mStep[0m  [12/21], [94mLoss[0m : 2.76735
[1mStep[0m  [14/21], [94mLoss[0m : 2.74744
[1mStep[0m  [16/21], [94mLoss[0m : 2.89080
[1mStep[0m  [18/21], [94mLoss[0m : 2.80734
[1mStep[0m  [20/21], [94mLoss[0m : 2.71953

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.817, [92mTest[0m: 2.871, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.74671
[1mStep[0m  [2/21], [94mLoss[0m : 2.73441
[1mStep[0m  [4/21], [94mLoss[0m : 2.86995
[1mStep[0m  [6/21], [94mLoss[0m : 2.66519
[1mStep[0m  [8/21], [94mLoss[0m : 2.66366
[1mStep[0m  [10/21], [94mLoss[0m : 2.60968
[1mStep[0m  [12/21], [94mLoss[0m : 2.65106
[1mStep[0m  [14/21], [94mLoss[0m : 2.66470
[1mStep[0m  [16/21], [94mLoss[0m : 2.64549
[1mStep[0m  [18/21], [94mLoss[0m : 2.80218
[1mStep[0m  [20/21], [94mLoss[0m : 2.62119

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.723, [92mTest[0m: 2.705, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.66802
[1mStep[0m  [2/21], [94mLoss[0m : 2.59739
[1mStep[0m  [4/21], [94mLoss[0m : 2.72274
[1mStep[0m  [6/21], [94mLoss[0m : 2.51527
[1mStep[0m  [8/21], [94mLoss[0m : 2.58141
[1mStep[0m  [10/21], [94mLoss[0m : 2.59068
[1mStep[0m  [12/21], [94mLoss[0m : 2.78036
[1mStep[0m  [14/21], [94mLoss[0m : 2.71489
[1mStep[0m  [16/21], [94mLoss[0m : 2.65512
[1mStep[0m  [18/21], [94mLoss[0m : 2.83803
[1mStep[0m  [20/21], [94mLoss[0m : 2.64255

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.657, [92mTest[0m: 2.642, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.76328
[1mStep[0m  [2/21], [94mLoss[0m : 2.50621
[1mStep[0m  [4/21], [94mLoss[0m : 2.57624
[1mStep[0m  [6/21], [94mLoss[0m : 2.69398
[1mStep[0m  [8/21], [94mLoss[0m : 2.53357
[1mStep[0m  [10/21], [94mLoss[0m : 2.53992
[1mStep[0m  [12/21], [94mLoss[0m : 2.63521
[1mStep[0m  [14/21], [94mLoss[0m : 2.62306
[1mStep[0m  [16/21], [94mLoss[0m : 2.67839
[1mStep[0m  [18/21], [94mLoss[0m : 2.68967
[1mStep[0m  [20/21], [94mLoss[0m : 2.61194

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.628, [92mTest[0m: 2.601, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.55901
[1mStep[0m  [2/21], [94mLoss[0m : 2.62099
[1mStep[0m  [4/21], [94mLoss[0m : 2.57103
[1mStep[0m  [6/21], [94mLoss[0m : 2.49107
[1mStep[0m  [8/21], [94mLoss[0m : 2.82439
[1mStep[0m  [10/21], [94mLoss[0m : 2.52674
[1mStep[0m  [12/21], [94mLoss[0m : 2.57190
[1mStep[0m  [14/21], [94mLoss[0m : 2.66317
[1mStep[0m  [16/21], [94mLoss[0m : 2.55552
[1mStep[0m  [18/21], [94mLoss[0m : 2.70383
[1mStep[0m  [20/21], [94mLoss[0m : 2.63471

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.595, [92mTest[0m: 2.563, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.58656
[1mStep[0m  [2/21], [94mLoss[0m : 2.49731
[1mStep[0m  [4/21], [94mLoss[0m : 2.60124
[1mStep[0m  [6/21], [94mLoss[0m : 2.53743
[1mStep[0m  [8/21], [94mLoss[0m : 2.60655
[1mStep[0m  [10/21], [94mLoss[0m : 2.53345
[1mStep[0m  [12/21], [94mLoss[0m : 2.48122
[1mStep[0m  [14/21], [94mLoss[0m : 2.43022
[1mStep[0m  [16/21], [94mLoss[0m : 2.65698
[1mStep[0m  [18/21], [94mLoss[0m : 2.43839
[1mStep[0m  [20/21], [94mLoss[0m : 2.46525

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.583, [92mTest[0m: 2.534, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.55071
[1mStep[0m  [2/21], [94mLoss[0m : 2.72765
[1mStep[0m  [4/21], [94mLoss[0m : 2.63645
[1mStep[0m  [6/21], [94mLoss[0m : 2.52291
[1mStep[0m  [8/21], [94mLoss[0m : 2.35810
[1mStep[0m  [10/21], [94mLoss[0m : 2.70107
[1mStep[0m  [12/21], [94mLoss[0m : 2.54802
[1mStep[0m  [14/21], [94mLoss[0m : 2.56637
[1mStep[0m  [16/21], [94mLoss[0m : 2.56842
[1mStep[0m  [18/21], [94mLoss[0m : 2.46344
[1mStep[0m  [20/21], [94mLoss[0m : 2.67312

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.562, [92mTest[0m: 2.522, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51661
[1mStep[0m  [2/21], [94mLoss[0m : 2.54641
[1mStep[0m  [4/21], [94mLoss[0m : 2.55012
[1mStep[0m  [6/21], [94mLoss[0m : 2.44857
[1mStep[0m  [8/21], [94mLoss[0m : 2.60695
[1mStep[0m  [10/21], [94mLoss[0m : 2.51541
[1mStep[0m  [12/21], [94mLoss[0m : 2.52645
[1mStep[0m  [14/21], [94mLoss[0m : 2.70380
[1mStep[0m  [16/21], [94mLoss[0m : 2.46283
[1mStep[0m  [18/21], [94mLoss[0m : 2.57306
[1mStep[0m  [20/21], [94mLoss[0m : 2.55474

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.553, [92mTest[0m: 2.504, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.55175
[1mStep[0m  [2/21], [94mLoss[0m : 2.52436
[1mStep[0m  [4/21], [94mLoss[0m : 2.66208
[1mStep[0m  [6/21], [94mLoss[0m : 2.59627
[1mStep[0m  [8/21], [94mLoss[0m : 2.49606
[1mStep[0m  [10/21], [94mLoss[0m : 2.57929
[1mStep[0m  [12/21], [94mLoss[0m : 2.52265
[1mStep[0m  [14/21], [94mLoss[0m : 2.55767
[1mStep[0m  [16/21], [94mLoss[0m : 2.45753
[1mStep[0m  [18/21], [94mLoss[0m : 2.35401
[1mStep[0m  [20/21], [94mLoss[0m : 2.34522

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.499, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.62572
[1mStep[0m  [2/21], [94mLoss[0m : 2.53608
[1mStep[0m  [4/21], [94mLoss[0m : 2.51012
[1mStep[0m  [6/21], [94mLoss[0m : 2.41174
[1mStep[0m  [8/21], [94mLoss[0m : 2.49384
[1mStep[0m  [10/21], [94mLoss[0m : 2.66183
[1mStep[0m  [12/21], [94mLoss[0m : 2.45454
[1mStep[0m  [14/21], [94mLoss[0m : 2.46395
[1mStep[0m  [16/21], [94mLoss[0m : 2.58749
[1mStep[0m  [18/21], [94mLoss[0m : 2.69255
[1mStep[0m  [20/21], [94mLoss[0m : 2.49684

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.491, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53261
[1mStep[0m  [2/21], [94mLoss[0m : 2.48361
[1mStep[0m  [4/21], [94mLoss[0m : 2.51276
[1mStep[0m  [6/21], [94mLoss[0m : 2.44697
[1mStep[0m  [8/21], [94mLoss[0m : 2.38454
[1mStep[0m  [10/21], [94mLoss[0m : 2.49884
[1mStep[0m  [12/21], [94mLoss[0m : 2.54944
[1mStep[0m  [14/21], [94mLoss[0m : 2.47236
[1mStep[0m  [16/21], [94mLoss[0m : 2.46731
[1mStep[0m  [18/21], [94mLoss[0m : 2.66541
[1mStep[0m  [20/21], [94mLoss[0m : 2.47821

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.479, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.39920
[1mStep[0m  [2/21], [94mLoss[0m : 2.50154
[1mStep[0m  [4/21], [94mLoss[0m : 2.53391
[1mStep[0m  [6/21], [94mLoss[0m : 2.53787
[1mStep[0m  [8/21], [94mLoss[0m : 2.57090
[1mStep[0m  [10/21], [94mLoss[0m : 2.48535
[1mStep[0m  [12/21], [94mLoss[0m : 2.56184
[1mStep[0m  [14/21], [94mLoss[0m : 2.75872
[1mStep[0m  [16/21], [94mLoss[0m : 2.42521
[1mStep[0m  [18/21], [94mLoss[0m : 2.38391
[1mStep[0m  [20/21], [94mLoss[0m : 2.55246

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.472, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.47889
[1mStep[0m  [2/21], [94mLoss[0m : 2.29171
[1mStep[0m  [4/21], [94mLoss[0m : 2.46327
[1mStep[0m  [6/21], [94mLoss[0m : 2.59209
[1mStep[0m  [8/21], [94mLoss[0m : 2.45446
[1mStep[0m  [10/21], [94mLoss[0m : 2.42227
[1mStep[0m  [12/21], [94mLoss[0m : 2.54106
[1mStep[0m  [14/21], [94mLoss[0m : 2.42828
[1mStep[0m  [16/21], [94mLoss[0m : 2.42074
[1mStep[0m  [18/21], [94mLoss[0m : 2.67961
[1mStep[0m  [20/21], [94mLoss[0m : 2.58656

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.462, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53596
[1mStep[0m  [2/21], [94mLoss[0m : 2.54244
[1mStep[0m  [4/21], [94mLoss[0m : 2.44251
[1mStep[0m  [6/21], [94mLoss[0m : 2.49972
[1mStep[0m  [8/21], [94mLoss[0m : 2.52769
[1mStep[0m  [10/21], [94mLoss[0m : 2.52915
[1mStep[0m  [12/21], [94mLoss[0m : 2.57863
[1mStep[0m  [14/21], [94mLoss[0m : 2.58223
[1mStep[0m  [16/21], [94mLoss[0m : 2.52008
[1mStep[0m  [18/21], [94mLoss[0m : 2.49791
[1mStep[0m  [20/21], [94mLoss[0m : 2.45110

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.519, [92mTest[0m: 2.452, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.55825
[1mStep[0m  [2/21], [94mLoss[0m : 2.66919
[1mStep[0m  [4/21], [94mLoss[0m : 2.42827
[1mStep[0m  [6/21], [94mLoss[0m : 2.44630
[1mStep[0m  [8/21], [94mLoss[0m : 2.40851
[1mStep[0m  [10/21], [94mLoss[0m : 2.41869
[1mStep[0m  [12/21], [94mLoss[0m : 2.44271
[1mStep[0m  [14/21], [94mLoss[0m : 2.59025
[1mStep[0m  [16/21], [94mLoss[0m : 2.58915
[1mStep[0m  [18/21], [94mLoss[0m : 2.57680
[1mStep[0m  [20/21], [94mLoss[0m : 2.43104

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.455, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51903
[1mStep[0m  [2/21], [94mLoss[0m : 2.58595
[1mStep[0m  [4/21], [94mLoss[0m : 2.46869
[1mStep[0m  [6/21], [94mLoss[0m : 2.65659
[1mStep[0m  [8/21], [94mLoss[0m : 2.49746
[1mStep[0m  [10/21], [94mLoss[0m : 2.45860
[1mStep[0m  [12/21], [94mLoss[0m : 2.49807
[1mStep[0m  [14/21], [94mLoss[0m : 2.38818
[1mStep[0m  [16/21], [94mLoss[0m : 2.41452
[1mStep[0m  [18/21], [94mLoss[0m : 2.42485
[1mStep[0m  [20/21], [94mLoss[0m : 2.46757

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.452, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56650
[1mStep[0m  [2/21], [94mLoss[0m : 2.48732
[1mStep[0m  [4/21], [94mLoss[0m : 2.66275
[1mStep[0m  [6/21], [94mLoss[0m : 2.50229
[1mStep[0m  [8/21], [94mLoss[0m : 2.42368
[1mStep[0m  [10/21], [94mLoss[0m : 2.36260
[1mStep[0m  [12/21], [94mLoss[0m : 2.59551
[1mStep[0m  [14/21], [94mLoss[0m : 2.40464
[1mStep[0m  [16/21], [94mLoss[0m : 2.52810
[1mStep[0m  [18/21], [94mLoss[0m : 2.39273
[1mStep[0m  [20/21], [94mLoss[0m : 2.41975

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.443, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46824
[1mStep[0m  [2/21], [94mLoss[0m : 2.38800
[1mStep[0m  [4/21], [94mLoss[0m : 2.39558
[1mStep[0m  [6/21], [94mLoss[0m : 2.51974
[1mStep[0m  [8/21], [94mLoss[0m : 2.66122
[1mStep[0m  [10/21], [94mLoss[0m : 2.55653
[1mStep[0m  [12/21], [94mLoss[0m : 2.45119
[1mStep[0m  [14/21], [94mLoss[0m : 2.59466
[1mStep[0m  [16/21], [94mLoss[0m : 2.48563
[1mStep[0m  [18/21], [94mLoss[0m : 2.64226
[1mStep[0m  [20/21], [94mLoss[0m : 2.39111

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.450, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.35929
[1mStep[0m  [2/21], [94mLoss[0m : 2.54643
[1mStep[0m  [4/21], [94mLoss[0m : 2.30806
[1mStep[0m  [6/21], [94mLoss[0m : 2.60671
[1mStep[0m  [8/21], [94mLoss[0m : 2.49884
[1mStep[0m  [10/21], [94mLoss[0m : 2.53104
[1mStep[0m  [12/21], [94mLoss[0m : 2.59443
[1mStep[0m  [14/21], [94mLoss[0m : 2.44547
[1mStep[0m  [16/21], [94mLoss[0m : 2.46897
[1mStep[0m  [18/21], [94mLoss[0m : 2.39127
[1mStep[0m  [20/21], [94mLoss[0m : 2.47849

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.440, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56056
[1mStep[0m  [2/21], [94mLoss[0m : 2.46982
[1mStep[0m  [4/21], [94mLoss[0m : 2.42088
[1mStep[0m  [6/21], [94mLoss[0m : 2.51761
[1mStep[0m  [8/21], [94mLoss[0m : 2.36235
[1mStep[0m  [10/21], [94mLoss[0m : 2.39596
[1mStep[0m  [12/21], [94mLoss[0m : 2.49532
[1mStep[0m  [14/21], [94mLoss[0m : 2.41237
[1mStep[0m  [16/21], [94mLoss[0m : 2.47395
[1mStep[0m  [18/21], [94mLoss[0m : 2.49199
[1mStep[0m  [20/21], [94mLoss[0m : 2.58861

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.437, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52305
[1mStep[0m  [2/21], [94mLoss[0m : 2.46144
[1mStep[0m  [4/21], [94mLoss[0m : 2.44846
[1mStep[0m  [6/21], [94mLoss[0m : 2.55069
[1mStep[0m  [8/21], [94mLoss[0m : 2.47647
[1mStep[0m  [10/21], [94mLoss[0m : 2.48178
[1mStep[0m  [12/21], [94mLoss[0m : 2.50345
[1mStep[0m  [14/21], [94mLoss[0m : 2.46564
[1mStep[0m  [16/21], [94mLoss[0m : 2.47406
[1mStep[0m  [18/21], [94mLoss[0m : 2.51144
[1mStep[0m  [20/21], [94mLoss[0m : 2.48550

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.428, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.50479
[1mStep[0m  [2/21], [94mLoss[0m : 2.40727
[1mStep[0m  [4/21], [94mLoss[0m : 2.53138
[1mStep[0m  [6/21], [94mLoss[0m : 2.50327
[1mStep[0m  [8/21], [94mLoss[0m : 2.39125
[1mStep[0m  [10/21], [94mLoss[0m : 2.48378
[1mStep[0m  [12/21], [94mLoss[0m : 2.46275
[1mStep[0m  [14/21], [94mLoss[0m : 2.59039
[1mStep[0m  [16/21], [94mLoss[0m : 2.41602
[1mStep[0m  [18/21], [94mLoss[0m : 2.40062
[1mStep[0m  [20/21], [94mLoss[0m : 2.29332

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.430, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.65738
[1mStep[0m  [2/21], [94mLoss[0m : 2.37720
[1mStep[0m  [4/21], [94mLoss[0m : 2.45245
[1mStep[0m  [6/21], [94mLoss[0m : 2.50952
[1mStep[0m  [8/21], [94mLoss[0m : 2.49839
[1mStep[0m  [10/21], [94mLoss[0m : 2.45471
[1mStep[0m  [12/21], [94mLoss[0m : 2.34782
[1mStep[0m  [14/21], [94mLoss[0m : 2.55924
[1mStep[0m  [16/21], [94mLoss[0m : 2.56179
[1mStep[0m  [18/21], [94mLoss[0m : 2.40570
[1mStep[0m  [20/21], [94mLoss[0m : 2.62737

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.433, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44134
[1mStep[0m  [2/21], [94mLoss[0m : 2.53812
[1mStep[0m  [4/21], [94mLoss[0m : 2.49890
[1mStep[0m  [6/21], [94mLoss[0m : 2.37393
[1mStep[0m  [8/21], [94mLoss[0m : 2.35398
[1mStep[0m  [10/21], [94mLoss[0m : 2.62386
[1mStep[0m  [12/21], [94mLoss[0m : 2.46970
[1mStep[0m  [14/21], [94mLoss[0m : 2.44367
[1mStep[0m  [16/21], [94mLoss[0m : 2.39841
[1mStep[0m  [18/21], [94mLoss[0m : 2.45324
[1mStep[0m  [20/21], [94mLoss[0m : 2.33318

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.427, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52184
[1mStep[0m  [2/21], [94mLoss[0m : 2.61015
[1mStep[0m  [4/21], [94mLoss[0m : 2.50484
[1mStep[0m  [6/21], [94mLoss[0m : 2.40081
[1mStep[0m  [8/21], [94mLoss[0m : 2.40770
[1mStep[0m  [10/21], [94mLoss[0m : 2.62303
[1mStep[0m  [12/21], [94mLoss[0m : 2.45684
[1mStep[0m  [14/21], [94mLoss[0m : 2.49747
[1mStep[0m  [16/21], [94mLoss[0m : 2.57277
[1mStep[0m  [18/21], [94mLoss[0m : 2.38599
[1mStep[0m  [20/21], [94mLoss[0m : 2.52819

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.420, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.41122
[1mStep[0m  [2/21], [94mLoss[0m : 2.37850
[1mStep[0m  [4/21], [94mLoss[0m : 2.44842
[1mStep[0m  [6/21], [94mLoss[0m : 2.55038
[1mStep[0m  [8/21], [94mLoss[0m : 2.52540
[1mStep[0m  [10/21], [94mLoss[0m : 2.44353
[1mStep[0m  [12/21], [94mLoss[0m : 2.60155
[1mStep[0m  [14/21], [94mLoss[0m : 2.49610
[1mStep[0m  [16/21], [94mLoss[0m : 2.32106
[1mStep[0m  [18/21], [94mLoss[0m : 2.63314
[1mStep[0m  [20/21], [94mLoss[0m : 2.39384

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.421, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.29568
[1mStep[0m  [2/21], [94mLoss[0m : 2.27380
[1mStep[0m  [4/21], [94mLoss[0m : 2.66265
[1mStep[0m  [6/21], [94mLoss[0m : 2.62481
[1mStep[0m  [8/21], [94mLoss[0m : 2.45553
[1mStep[0m  [10/21], [94mLoss[0m : 2.42671
[1mStep[0m  [12/21], [94mLoss[0m : 2.38442
[1mStep[0m  [14/21], [94mLoss[0m : 2.59524
[1mStep[0m  [16/21], [94mLoss[0m : 2.46589
[1mStep[0m  [18/21], [94mLoss[0m : 2.47888
[1mStep[0m  [20/21], [94mLoss[0m : 2.64886

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.412, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.413
====================================

Phase 1 - Evaluation MAE:  2.412783179964338
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/21], [94mLoss[0m : 2.33301
[1mStep[0m  [2/21], [94mLoss[0m : 2.55212
[1mStep[0m  [4/21], [94mLoss[0m : 2.55221
[1mStep[0m  [6/21], [94mLoss[0m : 2.51647
[1mStep[0m  [8/21], [94mLoss[0m : 2.31268
[1mStep[0m  [10/21], [94mLoss[0m : 2.43775
[1mStep[0m  [12/21], [94mLoss[0m : 2.63297
[1mStep[0m  [14/21], [94mLoss[0m : 2.63044
[1mStep[0m  [16/21], [94mLoss[0m : 2.61361
[1mStep[0m  [18/21], [94mLoss[0m : 2.57416
[1mStep[0m  [20/21], [94mLoss[0m : 2.48856

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.413, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.60134
[1mStep[0m  [2/21], [94mLoss[0m : 2.56170
[1mStep[0m  [4/21], [94mLoss[0m : 2.44618
[1mStep[0m  [6/21], [94mLoss[0m : 2.31243
[1mStep[0m  [8/21], [94mLoss[0m : 2.42655
[1mStep[0m  [10/21], [94mLoss[0m : 2.47756
[1mStep[0m  [12/21], [94mLoss[0m : 2.37888
[1mStep[0m  [14/21], [94mLoss[0m : 2.57296
[1mStep[0m  [16/21], [94mLoss[0m : 2.47662
[1mStep[0m  [18/21], [94mLoss[0m : 2.48792
[1mStep[0m  [20/21], [94mLoss[0m : 2.63582

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.405, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.39011
[1mStep[0m  [2/21], [94mLoss[0m : 2.45406
[1mStep[0m  [4/21], [94mLoss[0m : 2.58397
[1mStep[0m  [6/21], [94mLoss[0m : 2.42897
[1mStep[0m  [8/21], [94mLoss[0m : 2.45725
[1mStep[0m  [10/21], [94mLoss[0m : 2.44094
[1mStep[0m  [12/21], [94mLoss[0m : 2.48062
[1mStep[0m  [14/21], [94mLoss[0m : 2.54041
[1mStep[0m  [16/21], [94mLoss[0m : 2.54449
[1mStep[0m  [18/21], [94mLoss[0m : 2.34037
[1mStep[0m  [20/21], [94mLoss[0m : 2.45261

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.394, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.47470
[1mStep[0m  [2/21], [94mLoss[0m : 2.48656
[1mStep[0m  [4/21], [94mLoss[0m : 2.37953
[1mStep[0m  [6/21], [94mLoss[0m : 2.42422
[1mStep[0m  [8/21], [94mLoss[0m : 2.49119
[1mStep[0m  [10/21], [94mLoss[0m : 2.49117
[1mStep[0m  [12/21], [94mLoss[0m : 2.42523
[1mStep[0m  [14/21], [94mLoss[0m : 2.33978
[1mStep[0m  [16/21], [94mLoss[0m : 2.51470
[1mStep[0m  [18/21], [94mLoss[0m : 2.53104
[1mStep[0m  [20/21], [94mLoss[0m : 2.62939

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.396, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44412
[1mStep[0m  [2/21], [94mLoss[0m : 2.45027
[1mStep[0m  [4/21], [94mLoss[0m : 2.19264
[1mStep[0m  [6/21], [94mLoss[0m : 2.42734
[1mStep[0m  [8/21], [94mLoss[0m : 2.49253
[1mStep[0m  [10/21], [94mLoss[0m : 2.54937
[1mStep[0m  [12/21], [94mLoss[0m : 2.41553
[1mStep[0m  [14/21], [94mLoss[0m : 2.40976
[1mStep[0m  [16/21], [94mLoss[0m : 2.51256
[1mStep[0m  [18/21], [94mLoss[0m : 2.47743
[1mStep[0m  [20/21], [94mLoss[0m : 2.70068

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.389, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52411
[1mStep[0m  [2/21], [94mLoss[0m : 2.41998
[1mStep[0m  [4/21], [94mLoss[0m : 2.39960
[1mStep[0m  [6/21], [94mLoss[0m : 2.56838
[1mStep[0m  [8/21], [94mLoss[0m : 2.44003
[1mStep[0m  [10/21], [94mLoss[0m : 2.40521
[1mStep[0m  [12/21], [94mLoss[0m : 2.39196
[1mStep[0m  [14/21], [94mLoss[0m : 2.42728
[1mStep[0m  [16/21], [94mLoss[0m : 2.53300
[1mStep[0m  [18/21], [94mLoss[0m : 2.35574
[1mStep[0m  [20/21], [94mLoss[0m : 2.46271

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.386, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.23662
[1mStep[0m  [2/21], [94mLoss[0m : 2.49482
[1mStep[0m  [4/21], [94mLoss[0m : 2.51275
[1mStep[0m  [6/21], [94mLoss[0m : 2.40314
[1mStep[0m  [8/21], [94mLoss[0m : 2.33210
[1mStep[0m  [10/21], [94mLoss[0m : 2.38825
[1mStep[0m  [12/21], [94mLoss[0m : 2.62987
[1mStep[0m  [14/21], [94mLoss[0m : 2.27749
[1mStep[0m  [16/21], [94mLoss[0m : 2.61263
[1mStep[0m  [18/21], [94mLoss[0m : 2.40356
[1mStep[0m  [20/21], [94mLoss[0m : 2.31042

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.393, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40273
[1mStep[0m  [2/21], [94mLoss[0m : 2.26725
[1mStep[0m  [4/21], [94mLoss[0m : 2.51053
[1mStep[0m  [6/21], [94mLoss[0m : 2.35539
[1mStep[0m  [8/21], [94mLoss[0m : 2.41457
[1mStep[0m  [10/21], [94mLoss[0m : 2.44394
[1mStep[0m  [12/21], [94mLoss[0m : 2.48592
[1mStep[0m  [14/21], [94mLoss[0m : 2.36183
[1mStep[0m  [16/21], [94mLoss[0m : 2.53531
[1mStep[0m  [18/21], [94mLoss[0m : 2.43612
[1mStep[0m  [20/21], [94mLoss[0m : 2.43908

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.396, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53175
[1mStep[0m  [2/21], [94mLoss[0m : 2.36949
[1mStep[0m  [4/21], [94mLoss[0m : 2.40753
[1mStep[0m  [6/21], [94mLoss[0m : 2.41683
[1mStep[0m  [8/21], [94mLoss[0m : 2.44885
[1mStep[0m  [10/21], [94mLoss[0m : 2.49393
[1mStep[0m  [12/21], [94mLoss[0m : 2.42060
[1mStep[0m  [14/21], [94mLoss[0m : 2.33117
[1mStep[0m  [16/21], [94mLoss[0m : 2.51363
[1mStep[0m  [18/21], [94mLoss[0m : 2.41644
[1mStep[0m  [20/21], [94mLoss[0m : 2.48652

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.394, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.30191
[1mStep[0m  [2/21], [94mLoss[0m : 2.63550
[1mStep[0m  [4/21], [94mLoss[0m : 2.45572
[1mStep[0m  [6/21], [94mLoss[0m : 2.28486
[1mStep[0m  [8/21], [94mLoss[0m : 2.51409
[1mStep[0m  [10/21], [94mLoss[0m : 2.52643
[1mStep[0m  [12/21], [94mLoss[0m : 2.47394
[1mStep[0m  [14/21], [94mLoss[0m : 2.29288
[1mStep[0m  [16/21], [94mLoss[0m : 2.38614
[1mStep[0m  [18/21], [94mLoss[0m : 2.26774
[1mStep[0m  [20/21], [94mLoss[0m : 2.42161

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.368, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.34883
[1mStep[0m  [2/21], [94mLoss[0m : 2.56062
[1mStep[0m  [4/21], [94mLoss[0m : 2.40111
[1mStep[0m  [6/21], [94mLoss[0m : 2.37872
[1mStep[0m  [8/21], [94mLoss[0m : 2.38703
[1mStep[0m  [10/21], [94mLoss[0m : 2.39065
[1mStep[0m  [12/21], [94mLoss[0m : 2.29073
[1mStep[0m  [14/21], [94mLoss[0m : 2.49492
[1mStep[0m  [16/21], [94mLoss[0m : 2.38037
[1mStep[0m  [18/21], [94mLoss[0m : 2.48521
[1mStep[0m  [20/21], [94mLoss[0m : 2.57113

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.382, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.33822
[1mStep[0m  [2/21], [94mLoss[0m : 2.23531
[1mStep[0m  [4/21], [94mLoss[0m : 2.46275
[1mStep[0m  [6/21], [94mLoss[0m : 2.43944
[1mStep[0m  [8/21], [94mLoss[0m : 2.43861
[1mStep[0m  [10/21], [94mLoss[0m : 2.49832
[1mStep[0m  [12/21], [94mLoss[0m : 2.51343
[1mStep[0m  [14/21], [94mLoss[0m : 2.31198
[1mStep[0m  [16/21], [94mLoss[0m : 2.21546
[1mStep[0m  [18/21], [94mLoss[0m : 2.45588
[1mStep[0m  [20/21], [94mLoss[0m : 2.48989

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.371, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46323
[1mStep[0m  [2/21], [94mLoss[0m : 2.32235
[1mStep[0m  [4/21], [94mLoss[0m : 2.28789
[1mStep[0m  [6/21], [94mLoss[0m : 2.32308
[1mStep[0m  [8/21], [94mLoss[0m : 2.51498
[1mStep[0m  [10/21], [94mLoss[0m : 2.34567
[1mStep[0m  [12/21], [94mLoss[0m : 2.51849
[1mStep[0m  [14/21], [94mLoss[0m : 2.36935
[1mStep[0m  [16/21], [94mLoss[0m : 2.49269
[1mStep[0m  [18/21], [94mLoss[0m : 2.51053
[1mStep[0m  [20/21], [94mLoss[0m : 2.51579

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.380, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51377
[1mStep[0m  [2/21], [94mLoss[0m : 2.75146
[1mStep[0m  [4/21], [94mLoss[0m : 2.32483
[1mStep[0m  [6/21], [94mLoss[0m : 2.22839
[1mStep[0m  [8/21], [94mLoss[0m : 2.62246
[1mStep[0m  [10/21], [94mLoss[0m : 2.45015
[1mStep[0m  [12/21], [94mLoss[0m : 2.54010
[1mStep[0m  [14/21], [94mLoss[0m : 2.24684
[1mStep[0m  [16/21], [94mLoss[0m : 2.36916
[1mStep[0m  [18/21], [94mLoss[0m : 2.43168
[1mStep[0m  [20/21], [94mLoss[0m : 2.35698

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.367, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.36458
[1mStep[0m  [2/21], [94mLoss[0m : 2.24793
[1mStep[0m  [4/21], [94mLoss[0m : 2.52039
[1mStep[0m  [6/21], [94mLoss[0m : 2.50110
[1mStep[0m  [8/21], [94mLoss[0m : 2.40337
[1mStep[0m  [10/21], [94mLoss[0m : 2.26101
[1mStep[0m  [12/21], [94mLoss[0m : 2.35851
[1mStep[0m  [14/21], [94mLoss[0m : 2.41737
[1mStep[0m  [16/21], [94mLoss[0m : 2.37084
[1mStep[0m  [18/21], [94mLoss[0m : 2.37856
[1mStep[0m  [20/21], [94mLoss[0m : 2.36098

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.364, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43265
[1mStep[0m  [2/21], [94mLoss[0m : 2.32885
[1mStep[0m  [4/21], [94mLoss[0m : 2.35182
[1mStep[0m  [6/21], [94mLoss[0m : 2.45271
[1mStep[0m  [8/21], [94mLoss[0m : 2.34632
[1mStep[0m  [10/21], [94mLoss[0m : 2.40038
[1mStep[0m  [12/21], [94mLoss[0m : 2.35009
[1mStep[0m  [14/21], [94mLoss[0m : 2.41456
[1mStep[0m  [16/21], [94mLoss[0m : 2.55796
[1mStep[0m  [18/21], [94mLoss[0m : 2.37713
[1mStep[0m  [20/21], [94mLoss[0m : 2.35021

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.395, [92mTest[0m: 2.363, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51869
[1mStep[0m  [2/21], [94mLoss[0m : 2.32158
[1mStep[0m  [4/21], [94mLoss[0m : 2.33889
[1mStep[0m  [6/21], [94mLoss[0m : 2.43185
[1mStep[0m  [8/21], [94mLoss[0m : 2.40407
[1mStep[0m  [10/21], [94mLoss[0m : 2.21325
[1mStep[0m  [12/21], [94mLoss[0m : 2.25378
[1mStep[0m  [14/21], [94mLoss[0m : 2.54588
[1mStep[0m  [16/21], [94mLoss[0m : 2.42283
[1mStep[0m  [18/21], [94mLoss[0m : 2.33232
[1mStep[0m  [20/21], [94mLoss[0m : 2.42940

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.369, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40007
[1mStep[0m  [2/21], [94mLoss[0m : 2.22875
[1mStep[0m  [4/21], [94mLoss[0m : 2.46289
[1mStep[0m  [6/21], [94mLoss[0m : 2.42500
[1mStep[0m  [8/21], [94mLoss[0m : 2.37022
[1mStep[0m  [10/21], [94mLoss[0m : 2.45836
[1mStep[0m  [12/21], [94mLoss[0m : 2.27145
[1mStep[0m  [14/21], [94mLoss[0m : 2.49898
[1mStep[0m  [16/21], [94mLoss[0m : 2.58208
[1mStep[0m  [18/21], [94mLoss[0m : 2.24579
[1mStep[0m  [20/21], [94mLoss[0m : 2.42071

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.353, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.39549
[1mStep[0m  [2/21], [94mLoss[0m : 2.34516
[1mStep[0m  [4/21], [94mLoss[0m : 2.34255
[1mStep[0m  [6/21], [94mLoss[0m : 2.36436
[1mStep[0m  [8/21], [94mLoss[0m : 2.36577
[1mStep[0m  [10/21], [94mLoss[0m : 2.43079
[1mStep[0m  [12/21], [94mLoss[0m : 2.35892
[1mStep[0m  [14/21], [94mLoss[0m : 2.26727
[1mStep[0m  [16/21], [94mLoss[0m : 2.39796
[1mStep[0m  [18/21], [94mLoss[0m : 2.34725
[1mStep[0m  [20/21], [94mLoss[0m : 2.33811

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.388, [92mTest[0m: 2.360, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.24826
[1mStep[0m  [2/21], [94mLoss[0m : 2.33699
[1mStep[0m  [4/21], [94mLoss[0m : 2.33966
[1mStep[0m  [6/21], [94mLoss[0m : 2.54737
[1mStep[0m  [8/21], [94mLoss[0m : 2.35231
[1mStep[0m  [10/21], [94mLoss[0m : 2.49541
[1mStep[0m  [12/21], [94mLoss[0m : 2.44412
[1mStep[0m  [14/21], [94mLoss[0m : 2.39409
[1mStep[0m  [16/21], [94mLoss[0m : 2.51433
[1mStep[0m  [18/21], [94mLoss[0m : 2.36594
[1mStep[0m  [20/21], [94mLoss[0m : 2.25305

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.388, [92mTest[0m: 2.349, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48980
[1mStep[0m  [2/21], [94mLoss[0m : 2.39462
[1mStep[0m  [4/21], [94mLoss[0m : 2.37036
[1mStep[0m  [6/21], [94mLoss[0m : 2.40223
[1mStep[0m  [8/21], [94mLoss[0m : 2.36694
[1mStep[0m  [10/21], [94mLoss[0m : 2.39789
[1mStep[0m  [12/21], [94mLoss[0m : 2.44369
[1mStep[0m  [14/21], [94mLoss[0m : 2.36511
[1mStep[0m  [16/21], [94mLoss[0m : 2.34353
[1mStep[0m  [18/21], [94mLoss[0m : 2.41865
[1mStep[0m  [20/21], [94mLoss[0m : 2.40597

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.385, [92mTest[0m: 2.348, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46575
[1mStep[0m  [2/21], [94mLoss[0m : 2.45528
[1mStep[0m  [4/21], [94mLoss[0m : 2.30912
[1mStep[0m  [6/21], [94mLoss[0m : 2.34155
[1mStep[0m  [8/21], [94mLoss[0m : 2.37367
[1mStep[0m  [10/21], [94mLoss[0m : 2.41002
[1mStep[0m  [12/21], [94mLoss[0m : 2.34043
[1mStep[0m  [14/21], [94mLoss[0m : 2.38924
[1mStep[0m  [16/21], [94mLoss[0m : 2.38620
[1mStep[0m  [18/21], [94mLoss[0m : 2.34680
[1mStep[0m  [20/21], [94mLoss[0m : 2.29423

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.385, [92mTest[0m: 2.350, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.38309
[1mStep[0m  [2/21], [94mLoss[0m : 2.21367
[1mStep[0m  [4/21], [94mLoss[0m : 2.34406
[1mStep[0m  [6/21], [94mLoss[0m : 2.30654
[1mStep[0m  [8/21], [94mLoss[0m : 2.44833
[1mStep[0m  [10/21], [94mLoss[0m : 2.64735
[1mStep[0m  [12/21], [94mLoss[0m : 2.21760
[1mStep[0m  [14/21], [94mLoss[0m : 2.27494
[1mStep[0m  [16/21], [94mLoss[0m : 2.37126
[1mStep[0m  [18/21], [94mLoss[0m : 2.45766
[1mStep[0m  [20/21], [94mLoss[0m : 2.35808

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.377, [92mTest[0m: 2.352, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.34036
[1mStep[0m  [2/21], [94mLoss[0m : 2.28814
[1mStep[0m  [4/21], [94mLoss[0m : 2.31050
[1mStep[0m  [6/21], [94mLoss[0m : 2.44421
[1mStep[0m  [8/21], [94mLoss[0m : 2.39005
[1mStep[0m  [10/21], [94mLoss[0m : 2.30191
[1mStep[0m  [12/21], [94mLoss[0m : 2.21102
[1mStep[0m  [14/21], [94mLoss[0m : 2.34638
[1mStep[0m  [16/21], [94mLoss[0m : 2.56317
[1mStep[0m  [18/21], [94mLoss[0m : 2.23267
[1mStep[0m  [20/21], [94mLoss[0m : 2.40747

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.372, [92mTest[0m: 2.347, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.30163
[1mStep[0m  [2/21], [94mLoss[0m : 2.46248
[1mStep[0m  [4/21], [94mLoss[0m : 2.32546
[1mStep[0m  [6/21], [94mLoss[0m : 2.42216
[1mStep[0m  [8/21], [94mLoss[0m : 2.42985
[1mStep[0m  [10/21], [94mLoss[0m : 2.28836
[1mStep[0m  [12/21], [94mLoss[0m : 2.37647
[1mStep[0m  [14/21], [94mLoss[0m : 2.50127
[1mStep[0m  [16/21], [94mLoss[0m : 2.37228
[1mStep[0m  [18/21], [94mLoss[0m : 2.16920
[1mStep[0m  [20/21], [94mLoss[0m : 2.34147

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.379, [92mTest[0m: 2.366, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.37294
[1mStep[0m  [2/21], [94mLoss[0m : 2.27088
[1mStep[0m  [4/21], [94mLoss[0m : 2.35294
[1mStep[0m  [6/21], [94mLoss[0m : 2.44114
[1mStep[0m  [8/21], [94mLoss[0m : 2.30759
[1mStep[0m  [10/21], [94mLoss[0m : 2.34049
[1mStep[0m  [12/21], [94mLoss[0m : 2.39162
[1mStep[0m  [14/21], [94mLoss[0m : 2.27258
[1mStep[0m  [16/21], [94mLoss[0m : 2.42475
[1mStep[0m  [18/21], [94mLoss[0m : 2.42491
[1mStep[0m  [20/21], [94mLoss[0m : 2.31973

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.355, [92mTest[0m: 2.337, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46558
[1mStep[0m  [2/21], [94mLoss[0m : 2.38628
[1mStep[0m  [4/21], [94mLoss[0m : 2.43618
[1mStep[0m  [6/21], [94mLoss[0m : 2.29910
[1mStep[0m  [8/21], [94mLoss[0m : 2.41555
[1mStep[0m  [10/21], [94mLoss[0m : 2.34169
[1mStep[0m  [12/21], [94mLoss[0m : 2.30710
[1mStep[0m  [14/21], [94mLoss[0m : 2.32672
[1mStep[0m  [16/21], [94mLoss[0m : 2.25647
[1mStep[0m  [18/21], [94mLoss[0m : 2.35493
[1mStep[0m  [20/21], [94mLoss[0m : 2.26321

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.360, [92mTest[0m: 2.369, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54248
[1mStep[0m  [2/21], [94mLoss[0m : 2.52630
[1mStep[0m  [4/21], [94mLoss[0m : 2.22520
[1mStep[0m  [6/21], [94mLoss[0m : 2.45769
[1mStep[0m  [8/21], [94mLoss[0m : 2.34145
[1mStep[0m  [10/21], [94mLoss[0m : 2.37744
[1mStep[0m  [12/21], [94mLoss[0m : 2.26871
[1mStep[0m  [14/21], [94mLoss[0m : 2.22039
[1mStep[0m  [16/21], [94mLoss[0m : 2.40818
[1mStep[0m  [18/21], [94mLoss[0m : 2.32999
[1mStep[0m  [20/21], [94mLoss[0m : 2.23279

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.359, [92mTest[0m: 2.340, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.30990
[1mStep[0m  [2/21], [94mLoss[0m : 2.27513
[1mStep[0m  [4/21], [94mLoss[0m : 2.29594
[1mStep[0m  [6/21], [94mLoss[0m : 2.37084
[1mStep[0m  [8/21], [94mLoss[0m : 2.56240
[1mStep[0m  [10/21], [94mLoss[0m : 2.37704
[1mStep[0m  [12/21], [94mLoss[0m : 2.41369
[1mStep[0m  [14/21], [94mLoss[0m : 2.37003
[1mStep[0m  [16/21], [94mLoss[0m : 2.33342
[1mStep[0m  [18/21], [94mLoss[0m : 2.32274
[1mStep[0m  [20/21], [94mLoss[0m : 2.34943

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.362, [92mTest[0m: 2.341, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.38646
[1mStep[0m  [2/21], [94mLoss[0m : 2.34275
[1mStep[0m  [4/21], [94mLoss[0m : 2.17206
[1mStep[0m  [6/21], [94mLoss[0m : 2.28321
[1mStep[0m  [8/21], [94mLoss[0m : 2.23755
[1mStep[0m  [10/21], [94mLoss[0m : 2.50898
[1mStep[0m  [12/21], [94mLoss[0m : 2.40970
[1mStep[0m  [14/21], [94mLoss[0m : 2.46103
[1mStep[0m  [16/21], [94mLoss[0m : 2.33103
[1mStep[0m  [18/21], [94mLoss[0m : 2.26471
[1mStep[0m  [20/21], [94mLoss[0m : 2.60893

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.360, [92mTest[0m: 2.340, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.338
====================================

Phase 2 - Evaluation MAE:  2.337771075112479
MAE score P1      2.412783
MAE score P2      2.337771
loss              2.355133
learning_rate     0.002575
batch_size             512
hidden_sizes         [300]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.1
weight_decay          0.01
Name: 14, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 11.30300
[1mStep[0m  [4/42], [94mLoss[0m : 10.75263
[1mStep[0m  [8/42], [94mLoss[0m : 9.80978
[1mStep[0m  [12/42], [94mLoss[0m : 9.74837
[1mStep[0m  [16/42], [94mLoss[0m : 9.19011
[1mStep[0m  [20/42], [94mLoss[0m : 8.62570
[1mStep[0m  [24/42], [94mLoss[0m : 8.03841
[1mStep[0m  [28/42], [94mLoss[0m : 7.80572
[1mStep[0m  [32/42], [94mLoss[0m : 6.89191
[1mStep[0m  [36/42], [94mLoss[0m : 6.34203
[1mStep[0m  [40/42], [94mLoss[0m : 6.45644

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 8.563, [92mTest[0m: 10.972, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.42799
[1mStep[0m  [4/42], [94mLoss[0m : 5.67545
[1mStep[0m  [8/42], [94mLoss[0m : 5.11208
[1mStep[0m  [12/42], [94mLoss[0m : 5.17760
[1mStep[0m  [16/42], [94mLoss[0m : 4.69660
[1mStep[0m  [20/42], [94mLoss[0m : 4.57236
[1mStep[0m  [24/42], [94mLoss[0m : 4.27060
[1mStep[0m  [28/42], [94mLoss[0m : 3.66204
[1mStep[0m  [32/42], [94mLoss[0m : 3.55346
[1mStep[0m  [36/42], [94mLoss[0m : 3.38589
[1mStep[0m  [40/42], [94mLoss[0m : 3.74811

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 4.506, [92mTest[0m: 7.054, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.22741
[1mStep[0m  [4/42], [94mLoss[0m : 3.15986
[1mStep[0m  [8/42], [94mLoss[0m : 3.07685
[1mStep[0m  [12/42], [94mLoss[0m : 2.99216
[1mStep[0m  [16/42], [94mLoss[0m : 3.02981
[1mStep[0m  [20/42], [94mLoss[0m : 2.91562
[1mStep[0m  [24/42], [94mLoss[0m : 2.69923
[1mStep[0m  [28/42], [94mLoss[0m : 2.59222
[1mStep[0m  [32/42], [94mLoss[0m : 2.71715
[1mStep[0m  [36/42], [94mLoss[0m : 2.50563
[1mStep[0m  [40/42], [94mLoss[0m : 2.81185

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.917, [92mTest[0m: 4.014, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.69019
[1mStep[0m  [4/42], [94mLoss[0m : 2.64685
[1mStep[0m  [8/42], [94mLoss[0m : 2.70307
[1mStep[0m  [12/42], [94mLoss[0m : 2.80325
[1mStep[0m  [16/42], [94mLoss[0m : 2.47417
[1mStep[0m  [20/42], [94mLoss[0m : 2.59905
[1mStep[0m  [24/42], [94mLoss[0m : 2.58978
[1mStep[0m  [28/42], [94mLoss[0m : 2.52946
[1mStep[0m  [32/42], [94mLoss[0m : 2.55360
[1mStep[0m  [36/42], [94mLoss[0m : 2.72672
[1mStep[0m  [40/42], [94mLoss[0m : 2.59577

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.652, [92mTest[0m: 2.911, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.69197
[1mStep[0m  [4/42], [94mLoss[0m : 2.81050
[1mStep[0m  [8/42], [94mLoss[0m : 2.17893
[1mStep[0m  [12/42], [94mLoss[0m : 2.64452
[1mStep[0m  [16/42], [94mLoss[0m : 2.77169
[1mStep[0m  [20/42], [94mLoss[0m : 2.54272
[1mStep[0m  [24/42], [94mLoss[0m : 2.60267
[1mStep[0m  [28/42], [94mLoss[0m : 2.85877
[1mStep[0m  [32/42], [94mLoss[0m : 2.83947
[1mStep[0m  [36/42], [94mLoss[0m : 2.54918
[1mStep[0m  [40/42], [94mLoss[0m : 2.57884

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.625, [92mTest[0m: 2.712, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.81753
[1mStep[0m  [4/42], [94mLoss[0m : 2.62720
[1mStep[0m  [8/42], [94mLoss[0m : 2.40591
[1mStep[0m  [12/42], [94mLoss[0m : 2.56457
[1mStep[0m  [16/42], [94mLoss[0m : 2.64891
[1mStep[0m  [20/42], [94mLoss[0m : 2.63755
[1mStep[0m  [24/42], [94mLoss[0m : 2.61710
[1mStep[0m  [28/42], [94mLoss[0m : 2.49985
[1mStep[0m  [32/42], [94mLoss[0m : 2.84671
[1mStep[0m  [36/42], [94mLoss[0m : 2.55217
[1mStep[0m  [40/42], [94mLoss[0m : 2.49649

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.584, [92mTest[0m: 2.659, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62487
[1mStep[0m  [4/42], [94mLoss[0m : 2.44869
[1mStep[0m  [8/42], [94mLoss[0m : 2.62752
[1mStep[0m  [12/42], [94mLoss[0m : 2.69638
[1mStep[0m  [16/42], [94mLoss[0m : 2.57721
[1mStep[0m  [20/42], [94mLoss[0m : 2.86347
[1mStep[0m  [24/42], [94mLoss[0m : 2.30541
[1mStep[0m  [28/42], [94mLoss[0m : 2.50019
[1mStep[0m  [32/42], [94mLoss[0m : 2.63306
[1mStep[0m  [36/42], [94mLoss[0m : 2.40636
[1mStep[0m  [40/42], [94mLoss[0m : 2.51205

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.568, [92mTest[0m: 2.655, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.87419
[1mStep[0m  [4/42], [94mLoss[0m : 2.50415
[1mStep[0m  [8/42], [94mLoss[0m : 2.47072
[1mStep[0m  [12/42], [94mLoss[0m : 2.56930
[1mStep[0m  [16/42], [94mLoss[0m : 2.38519
[1mStep[0m  [20/42], [94mLoss[0m : 2.55354
[1mStep[0m  [24/42], [94mLoss[0m : 2.40703
[1mStep[0m  [28/42], [94mLoss[0m : 2.60398
[1mStep[0m  [32/42], [94mLoss[0m : 2.64244
[1mStep[0m  [36/42], [94mLoss[0m : 2.36629
[1mStep[0m  [40/42], [94mLoss[0m : 2.62233

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.610, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56446
[1mStep[0m  [4/42], [94mLoss[0m : 2.55236
[1mStep[0m  [8/42], [94mLoss[0m : 2.53300
[1mStep[0m  [12/42], [94mLoss[0m : 2.53101
[1mStep[0m  [16/42], [94mLoss[0m : 2.41890
[1mStep[0m  [20/42], [94mLoss[0m : 2.56563
[1mStep[0m  [24/42], [94mLoss[0m : 2.46921
[1mStep[0m  [28/42], [94mLoss[0m : 2.71879
[1mStep[0m  [32/42], [94mLoss[0m : 2.49761
[1mStep[0m  [36/42], [94mLoss[0m : 2.87359
[1mStep[0m  [40/42], [94mLoss[0m : 2.53691

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.548, [92mTest[0m: 2.620, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49946
[1mStep[0m  [4/42], [94mLoss[0m : 2.46135
[1mStep[0m  [8/42], [94mLoss[0m : 2.66046
[1mStep[0m  [12/42], [94mLoss[0m : 2.25196
[1mStep[0m  [16/42], [94mLoss[0m : 2.38753
[1mStep[0m  [20/42], [94mLoss[0m : 2.60174
[1mStep[0m  [24/42], [94mLoss[0m : 2.66818
[1mStep[0m  [28/42], [94mLoss[0m : 2.57677
[1mStep[0m  [32/42], [94mLoss[0m : 2.31386
[1mStep[0m  [36/42], [94mLoss[0m : 2.36055
[1mStep[0m  [40/42], [94mLoss[0m : 2.51175

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.597, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.72700
[1mStep[0m  [4/42], [94mLoss[0m : 2.58179
[1mStep[0m  [8/42], [94mLoss[0m : 2.45936
[1mStep[0m  [12/42], [94mLoss[0m : 2.41026
[1mStep[0m  [16/42], [94mLoss[0m : 2.47982
[1mStep[0m  [20/42], [94mLoss[0m : 2.63680
[1mStep[0m  [24/42], [94mLoss[0m : 2.44406
[1mStep[0m  [28/42], [94mLoss[0m : 2.80346
[1mStep[0m  [32/42], [94mLoss[0m : 2.42063
[1mStep[0m  [36/42], [94mLoss[0m : 2.70959
[1mStep[0m  [40/42], [94mLoss[0m : 2.77730

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.609, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44197
[1mStep[0m  [4/42], [94mLoss[0m : 2.27187
[1mStep[0m  [8/42], [94mLoss[0m : 2.61249
[1mStep[0m  [12/42], [94mLoss[0m : 2.57893
[1mStep[0m  [16/42], [94mLoss[0m : 2.38691
[1mStep[0m  [20/42], [94mLoss[0m : 2.67448
[1mStep[0m  [24/42], [94mLoss[0m : 2.56368
[1mStep[0m  [28/42], [94mLoss[0m : 2.59729
[1mStep[0m  [32/42], [94mLoss[0m : 2.72320
[1mStep[0m  [36/42], [94mLoss[0m : 2.48831
[1mStep[0m  [40/42], [94mLoss[0m : 2.59656

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.510, [92mTest[0m: 2.574, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.78325
[1mStep[0m  [4/42], [94mLoss[0m : 2.56574
[1mStep[0m  [8/42], [94mLoss[0m : 2.71625
[1mStep[0m  [12/42], [94mLoss[0m : 2.71680
[1mStep[0m  [16/42], [94mLoss[0m : 2.37116
[1mStep[0m  [20/42], [94mLoss[0m : 2.65165
[1mStep[0m  [24/42], [94mLoss[0m : 2.41052
[1mStep[0m  [28/42], [94mLoss[0m : 2.45924
[1mStep[0m  [32/42], [94mLoss[0m : 2.44029
[1mStep[0m  [36/42], [94mLoss[0m : 2.45685
[1mStep[0m  [40/42], [94mLoss[0m : 2.33535

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.559, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52943
[1mStep[0m  [4/42], [94mLoss[0m : 2.64427
[1mStep[0m  [8/42], [94mLoss[0m : 2.53035
[1mStep[0m  [12/42], [94mLoss[0m : 2.52911
[1mStep[0m  [16/42], [94mLoss[0m : 2.51757
[1mStep[0m  [20/42], [94mLoss[0m : 2.44231
[1mStep[0m  [24/42], [94mLoss[0m : 2.33836
[1mStep[0m  [28/42], [94mLoss[0m : 2.46917
[1mStep[0m  [32/42], [94mLoss[0m : 2.64423
[1mStep[0m  [36/42], [94mLoss[0m : 2.33674
[1mStep[0m  [40/42], [94mLoss[0m : 2.44574

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.551, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52124
[1mStep[0m  [4/42], [94mLoss[0m : 2.30105
[1mStep[0m  [8/42], [94mLoss[0m : 2.40878
[1mStep[0m  [12/42], [94mLoss[0m : 2.61762
[1mStep[0m  [16/42], [94mLoss[0m : 2.56458
[1mStep[0m  [20/42], [94mLoss[0m : 2.41805
[1mStep[0m  [24/42], [94mLoss[0m : 2.27262
[1mStep[0m  [28/42], [94mLoss[0m : 2.64312
[1mStep[0m  [32/42], [94mLoss[0m : 2.86461
[1mStep[0m  [36/42], [94mLoss[0m : 2.58686
[1mStep[0m  [40/42], [94mLoss[0m : 2.66530

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.561, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27692
[1mStep[0m  [4/42], [94mLoss[0m : 2.31140
[1mStep[0m  [8/42], [94mLoss[0m : 2.59224
[1mStep[0m  [12/42], [94mLoss[0m : 2.54378
[1mStep[0m  [16/42], [94mLoss[0m : 2.65538
[1mStep[0m  [20/42], [94mLoss[0m : 2.58472
[1mStep[0m  [24/42], [94mLoss[0m : 2.48036
[1mStep[0m  [28/42], [94mLoss[0m : 2.66937
[1mStep[0m  [32/42], [94mLoss[0m : 2.57806
[1mStep[0m  [36/42], [94mLoss[0m : 2.49068
[1mStep[0m  [40/42], [94mLoss[0m : 2.41134

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.569, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42152
[1mStep[0m  [4/42], [94mLoss[0m : 2.58690
[1mStep[0m  [8/42], [94mLoss[0m : 2.52932
[1mStep[0m  [12/42], [94mLoss[0m : 2.50408
[1mStep[0m  [16/42], [94mLoss[0m : 2.60911
[1mStep[0m  [20/42], [94mLoss[0m : 2.28070
[1mStep[0m  [24/42], [94mLoss[0m : 2.58808
[1mStep[0m  [28/42], [94mLoss[0m : 2.33100
[1mStep[0m  [32/42], [94mLoss[0m : 2.64271
[1mStep[0m  [36/42], [94mLoss[0m : 2.52073
[1mStep[0m  [40/42], [94mLoss[0m : 2.33240

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.543, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44537
[1mStep[0m  [4/42], [94mLoss[0m : 2.16084
[1mStep[0m  [8/42], [94mLoss[0m : 2.49060
[1mStep[0m  [12/42], [94mLoss[0m : 2.52310
[1mStep[0m  [16/42], [94mLoss[0m : 2.39230
[1mStep[0m  [20/42], [94mLoss[0m : 2.47527
[1mStep[0m  [24/42], [94mLoss[0m : 2.49318
[1mStep[0m  [28/42], [94mLoss[0m : 2.40506
[1mStep[0m  [32/42], [94mLoss[0m : 2.53622
[1mStep[0m  [36/42], [94mLoss[0m : 2.47800
[1mStep[0m  [40/42], [94mLoss[0m : 2.66223

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.555, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40400
[1mStep[0m  [4/42], [94mLoss[0m : 2.36674
[1mStep[0m  [8/42], [94mLoss[0m : 2.67343
[1mStep[0m  [12/42], [94mLoss[0m : 2.24201
[1mStep[0m  [16/42], [94mLoss[0m : 2.47177
[1mStep[0m  [20/42], [94mLoss[0m : 2.50764
[1mStep[0m  [24/42], [94mLoss[0m : 2.36898
[1mStep[0m  [28/42], [94mLoss[0m : 2.46716
[1mStep[0m  [32/42], [94mLoss[0m : 2.42525
[1mStep[0m  [36/42], [94mLoss[0m : 2.46990
[1mStep[0m  [40/42], [94mLoss[0m : 3.06613

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.563, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37136
[1mStep[0m  [4/42], [94mLoss[0m : 2.48961
[1mStep[0m  [8/42], [94mLoss[0m : 2.34880
[1mStep[0m  [12/42], [94mLoss[0m : 2.54358
[1mStep[0m  [16/42], [94mLoss[0m : 2.76939
[1mStep[0m  [20/42], [94mLoss[0m : 2.47242
[1mStep[0m  [24/42], [94mLoss[0m : 2.43346
[1mStep[0m  [28/42], [94mLoss[0m : 2.43388
[1mStep[0m  [32/42], [94mLoss[0m : 2.31692
[1mStep[0m  [36/42], [94mLoss[0m : 2.34513
[1mStep[0m  [40/42], [94mLoss[0m : 2.44405

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.546, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54007
[1mStep[0m  [4/42], [94mLoss[0m : 2.64495
[1mStep[0m  [8/42], [94mLoss[0m : 2.39177
[1mStep[0m  [12/42], [94mLoss[0m : 2.45470
[1mStep[0m  [16/42], [94mLoss[0m : 2.20997
[1mStep[0m  [20/42], [94mLoss[0m : 2.51333
[1mStep[0m  [24/42], [94mLoss[0m : 2.35897
[1mStep[0m  [28/42], [94mLoss[0m : 2.49755
[1mStep[0m  [32/42], [94mLoss[0m : 2.54245
[1mStep[0m  [36/42], [94mLoss[0m : 2.30820
[1mStep[0m  [40/42], [94mLoss[0m : 2.38544

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.533, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34665
[1mStep[0m  [4/42], [94mLoss[0m : 2.34619
[1mStep[0m  [8/42], [94mLoss[0m : 2.32339
[1mStep[0m  [12/42], [94mLoss[0m : 2.31903
[1mStep[0m  [16/42], [94mLoss[0m : 2.46238
[1mStep[0m  [20/42], [94mLoss[0m : 2.51396
[1mStep[0m  [24/42], [94mLoss[0m : 2.39253
[1mStep[0m  [28/42], [94mLoss[0m : 2.60767
[1mStep[0m  [32/42], [94mLoss[0m : 2.76149
[1mStep[0m  [36/42], [94mLoss[0m : 2.35187
[1mStep[0m  [40/42], [94mLoss[0m : 2.39768

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.539, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28561
[1mStep[0m  [4/42], [94mLoss[0m : 2.42125
[1mStep[0m  [8/42], [94mLoss[0m : 2.46900
[1mStep[0m  [12/42], [94mLoss[0m : 2.22654
[1mStep[0m  [16/42], [94mLoss[0m : 2.26868
[1mStep[0m  [20/42], [94mLoss[0m : 2.63928
[1mStep[0m  [24/42], [94mLoss[0m : 2.40927
[1mStep[0m  [28/42], [94mLoss[0m : 2.48794
[1mStep[0m  [32/42], [94mLoss[0m : 2.42480
[1mStep[0m  [36/42], [94mLoss[0m : 2.34618
[1mStep[0m  [40/42], [94mLoss[0m : 2.40279

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.551, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34462
[1mStep[0m  [4/42], [94mLoss[0m : 2.39528
[1mStep[0m  [8/42], [94mLoss[0m : 2.56138
[1mStep[0m  [12/42], [94mLoss[0m : 2.48579
[1mStep[0m  [16/42], [94mLoss[0m : 2.49965
[1mStep[0m  [20/42], [94mLoss[0m : 2.42562
[1mStep[0m  [24/42], [94mLoss[0m : 2.47395
[1mStep[0m  [28/42], [94mLoss[0m : 2.41357
[1mStep[0m  [32/42], [94mLoss[0m : 2.51491
[1mStep[0m  [36/42], [94mLoss[0m : 2.64697
[1mStep[0m  [40/42], [94mLoss[0m : 2.44256

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.539, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40987
[1mStep[0m  [4/42], [94mLoss[0m : 2.63451
[1mStep[0m  [8/42], [94mLoss[0m : 2.57324
[1mStep[0m  [12/42], [94mLoss[0m : 2.54431
[1mStep[0m  [16/42], [94mLoss[0m : 2.67442
[1mStep[0m  [20/42], [94mLoss[0m : 2.49525
[1mStep[0m  [24/42], [94mLoss[0m : 2.37630
[1mStep[0m  [28/42], [94mLoss[0m : 2.34430
[1mStep[0m  [32/42], [94mLoss[0m : 2.25012
[1mStep[0m  [36/42], [94mLoss[0m : 2.52523
[1mStep[0m  [40/42], [94mLoss[0m : 2.47194

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.561, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40617
[1mStep[0m  [4/42], [94mLoss[0m : 2.64501
[1mStep[0m  [8/42], [94mLoss[0m : 2.52527
[1mStep[0m  [12/42], [94mLoss[0m : 2.32119
[1mStep[0m  [16/42], [94mLoss[0m : 2.19233
[1mStep[0m  [20/42], [94mLoss[0m : 2.18121
[1mStep[0m  [24/42], [94mLoss[0m : 2.68025
[1mStep[0m  [28/42], [94mLoss[0m : 2.47373
[1mStep[0m  [32/42], [94mLoss[0m : 2.47893
[1mStep[0m  [36/42], [94mLoss[0m : 2.45258
[1mStep[0m  [40/42], [94mLoss[0m : 2.58923

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.544, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47443
[1mStep[0m  [4/42], [94mLoss[0m : 2.48028
[1mStep[0m  [8/42], [94mLoss[0m : 2.38897
[1mStep[0m  [12/42], [94mLoss[0m : 2.51758
[1mStep[0m  [16/42], [94mLoss[0m : 2.42611
[1mStep[0m  [20/42], [94mLoss[0m : 2.47469
[1mStep[0m  [24/42], [94mLoss[0m : 2.53157
[1mStep[0m  [28/42], [94mLoss[0m : 2.56453
[1mStep[0m  [32/42], [94mLoss[0m : 2.52166
[1mStep[0m  [36/42], [94mLoss[0m : 2.28335
[1mStep[0m  [40/42], [94mLoss[0m : 2.43118

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.538, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28914
[1mStep[0m  [4/42], [94mLoss[0m : 2.36782
[1mStep[0m  [8/42], [94mLoss[0m : 2.35662
[1mStep[0m  [12/42], [94mLoss[0m : 2.51278
[1mStep[0m  [16/42], [94mLoss[0m : 2.55132
[1mStep[0m  [20/42], [94mLoss[0m : 2.38038
[1mStep[0m  [24/42], [94mLoss[0m : 2.48991
[1mStep[0m  [28/42], [94mLoss[0m : 2.43105
[1mStep[0m  [32/42], [94mLoss[0m : 2.51361
[1mStep[0m  [36/42], [94mLoss[0m : 2.55291
[1mStep[0m  [40/42], [94mLoss[0m : 2.47508

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.535, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32703
[1mStep[0m  [4/42], [94mLoss[0m : 2.43391
[1mStep[0m  [8/42], [94mLoss[0m : 2.43327
[1mStep[0m  [12/42], [94mLoss[0m : 2.39083
[1mStep[0m  [16/42], [94mLoss[0m : 2.52713
[1mStep[0m  [20/42], [94mLoss[0m : 2.49934
[1mStep[0m  [24/42], [94mLoss[0m : 2.46422
[1mStep[0m  [28/42], [94mLoss[0m : 2.57063
[1mStep[0m  [32/42], [94mLoss[0m : 2.26285
[1mStep[0m  [36/42], [94mLoss[0m : 2.54842
[1mStep[0m  [40/42], [94mLoss[0m : 2.48670

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.540, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38267
[1mStep[0m  [4/42], [94mLoss[0m : 2.55423
[1mStep[0m  [8/42], [94mLoss[0m : 2.43480
[1mStep[0m  [12/42], [94mLoss[0m : 2.36164
[1mStep[0m  [16/42], [94mLoss[0m : 2.46689
[1mStep[0m  [20/42], [94mLoss[0m : 2.28851
[1mStep[0m  [24/42], [94mLoss[0m : 2.48658
[1mStep[0m  [28/42], [94mLoss[0m : 2.45218
[1mStep[0m  [32/42], [94mLoss[0m : 2.34301
[1mStep[0m  [36/42], [94mLoss[0m : 2.48807
[1mStep[0m  [40/42], [94mLoss[0m : 2.52707

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.554, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.551
====================================

Phase 1 - Evaluation MAE:  2.5506223099572316
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 2.62172
[1mStep[0m  [4/42], [94mLoss[0m : 2.38505
[1mStep[0m  [8/42], [94mLoss[0m : 2.47922
[1mStep[0m  [12/42], [94mLoss[0m : 2.32626
[1mStep[0m  [16/42], [94mLoss[0m : 2.23121
[1mStep[0m  [20/42], [94mLoss[0m : 2.44854
[1mStep[0m  [24/42], [94mLoss[0m : 2.56431
[1mStep[0m  [28/42], [94mLoss[0m : 2.66232
[1mStep[0m  [32/42], [94mLoss[0m : 2.49083
[1mStep[0m  [36/42], [94mLoss[0m : 2.40051
[1mStep[0m  [40/42], [94mLoss[0m : 2.41466

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.544, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35530
[1mStep[0m  [4/42], [94mLoss[0m : 2.60405
[1mStep[0m  [8/42], [94mLoss[0m : 2.60420
[1mStep[0m  [12/42], [94mLoss[0m : 2.67895
[1mStep[0m  [16/42], [94mLoss[0m : 2.31344
[1mStep[0m  [20/42], [94mLoss[0m : 2.66954
[1mStep[0m  [24/42], [94mLoss[0m : 2.65711
[1mStep[0m  [28/42], [94mLoss[0m : 2.13432
[1mStep[0m  [32/42], [94mLoss[0m : 2.36502
[1mStep[0m  [36/42], [94mLoss[0m : 2.33089
[1mStep[0m  [40/42], [94mLoss[0m : 2.38668

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.604, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49277
[1mStep[0m  [4/42], [94mLoss[0m : 2.42525
[1mStep[0m  [8/42], [94mLoss[0m : 2.47983
[1mStep[0m  [12/42], [94mLoss[0m : 2.61793
[1mStep[0m  [16/42], [94mLoss[0m : 2.49453
[1mStep[0m  [20/42], [94mLoss[0m : 2.35762
[1mStep[0m  [24/42], [94mLoss[0m : 2.67962
[1mStep[0m  [28/42], [94mLoss[0m : 2.39315
[1mStep[0m  [32/42], [94mLoss[0m : 2.19513
[1mStep[0m  [36/42], [94mLoss[0m : 2.40605
[1mStep[0m  [40/42], [94mLoss[0m : 2.52971

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.428, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47212
[1mStep[0m  [4/42], [94mLoss[0m : 2.59767
[1mStep[0m  [8/42], [94mLoss[0m : 2.41116
[1mStep[0m  [12/42], [94mLoss[0m : 2.46120
[1mStep[0m  [16/42], [94mLoss[0m : 2.30291
[1mStep[0m  [20/42], [94mLoss[0m : 2.16733
[1mStep[0m  [24/42], [94mLoss[0m : 2.49488
[1mStep[0m  [28/42], [94mLoss[0m : 2.57093
[1mStep[0m  [32/42], [94mLoss[0m : 2.39403
[1mStep[0m  [36/42], [94mLoss[0m : 2.51122
[1mStep[0m  [40/42], [94mLoss[0m : 2.52292

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.438, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.71684
[1mStep[0m  [4/42], [94mLoss[0m : 2.39597
[1mStep[0m  [8/42], [94mLoss[0m : 2.55561
[1mStep[0m  [12/42], [94mLoss[0m : 2.40250
[1mStep[0m  [16/42], [94mLoss[0m : 2.22665
[1mStep[0m  [20/42], [94mLoss[0m : 2.60076
[1mStep[0m  [24/42], [94mLoss[0m : 2.23326
[1mStep[0m  [28/42], [94mLoss[0m : 2.40015
[1mStep[0m  [32/42], [94mLoss[0m : 2.43557
[1mStep[0m  [36/42], [94mLoss[0m : 2.40660
[1mStep[0m  [40/42], [94mLoss[0m : 2.41227

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.504, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61236
[1mStep[0m  [4/42], [94mLoss[0m : 2.38238
[1mStep[0m  [8/42], [94mLoss[0m : 2.12027
[1mStep[0m  [12/42], [94mLoss[0m : 2.45021
[1mStep[0m  [16/42], [94mLoss[0m : 2.22531
[1mStep[0m  [20/42], [94mLoss[0m : 2.25998
[1mStep[0m  [24/42], [94mLoss[0m : 2.60396
[1mStep[0m  [28/42], [94mLoss[0m : 2.34293
[1mStep[0m  [32/42], [94mLoss[0m : 2.46733
[1mStep[0m  [36/42], [94mLoss[0m : 2.36964
[1mStep[0m  [40/42], [94mLoss[0m : 2.67482

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.391, [92mTest[0m: 2.638, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28384
[1mStep[0m  [4/42], [94mLoss[0m : 2.45598
[1mStep[0m  [8/42], [94mLoss[0m : 2.40231
[1mStep[0m  [12/42], [94mLoss[0m : 2.37682
[1mStep[0m  [16/42], [94mLoss[0m : 2.67537
[1mStep[0m  [20/42], [94mLoss[0m : 2.13142
[1mStep[0m  [24/42], [94mLoss[0m : 2.47086
[1mStep[0m  [28/42], [94mLoss[0m : 2.38010
[1mStep[0m  [32/42], [94mLoss[0m : 2.41948
[1mStep[0m  [36/42], [94mLoss[0m : 2.29175
[1mStep[0m  [40/42], [94mLoss[0m : 2.28767

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.392, [92mTest[0m: 2.630, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32780
[1mStep[0m  [4/42], [94mLoss[0m : 2.50004
[1mStep[0m  [8/42], [94mLoss[0m : 2.21293
[1mStep[0m  [12/42], [94mLoss[0m : 2.44447
[1mStep[0m  [16/42], [94mLoss[0m : 2.27380
[1mStep[0m  [20/42], [94mLoss[0m : 2.26410
[1mStep[0m  [24/42], [94mLoss[0m : 2.61526
[1mStep[0m  [28/42], [94mLoss[0m : 2.27929
[1mStep[0m  [32/42], [94mLoss[0m : 2.50361
[1mStep[0m  [36/42], [94mLoss[0m : 2.04837
[1mStep[0m  [40/42], [94mLoss[0m : 2.30625

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.365, [92mTest[0m: 2.694, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46515
[1mStep[0m  [4/42], [94mLoss[0m : 2.28572
[1mStep[0m  [8/42], [94mLoss[0m : 2.18025
[1mStep[0m  [12/42], [94mLoss[0m : 2.35748
[1mStep[0m  [16/42], [94mLoss[0m : 2.18780
[1mStep[0m  [20/42], [94mLoss[0m : 2.48433
[1mStep[0m  [24/42], [94mLoss[0m : 2.36244
[1mStep[0m  [28/42], [94mLoss[0m : 2.31852
[1mStep[0m  [32/42], [94mLoss[0m : 2.40451
[1mStep[0m  [36/42], [94mLoss[0m : 2.34902
[1mStep[0m  [40/42], [94mLoss[0m : 2.26912

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.344, [92mTest[0m: 2.594, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.25472
[1mStep[0m  [4/42], [94mLoss[0m : 2.27372
[1mStep[0m  [8/42], [94mLoss[0m : 2.34790
[1mStep[0m  [12/42], [94mLoss[0m : 2.18514
[1mStep[0m  [16/42], [94mLoss[0m : 2.34801
[1mStep[0m  [20/42], [94mLoss[0m : 2.43742
[1mStep[0m  [24/42], [94mLoss[0m : 2.16232
[1mStep[0m  [28/42], [94mLoss[0m : 2.51099
[1mStep[0m  [32/42], [94mLoss[0m : 2.27209
[1mStep[0m  [36/42], [94mLoss[0m : 2.67290
[1mStep[0m  [40/42], [94mLoss[0m : 2.32360

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.342, [92mTest[0m: 2.629, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.16705
[1mStep[0m  [4/42], [94mLoss[0m : 2.58380
[1mStep[0m  [8/42], [94mLoss[0m : 2.45129
[1mStep[0m  [12/42], [94mLoss[0m : 2.36069
[1mStep[0m  [16/42], [94mLoss[0m : 2.19048
[1mStep[0m  [20/42], [94mLoss[0m : 2.35202
[1mStep[0m  [24/42], [94mLoss[0m : 2.27159
[1mStep[0m  [28/42], [94mLoss[0m : 2.51914
[1mStep[0m  [32/42], [94mLoss[0m : 2.29727
[1mStep[0m  [36/42], [94mLoss[0m : 2.25481
[1mStep[0m  [40/42], [94mLoss[0m : 2.38287

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.324, [92mTest[0m: 2.632, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.20616
[1mStep[0m  [4/42], [94mLoss[0m : 2.23428
[1mStep[0m  [8/42], [94mLoss[0m : 2.36414
[1mStep[0m  [12/42], [94mLoss[0m : 2.40445
[1mStep[0m  [16/42], [94mLoss[0m : 2.43108
[1mStep[0m  [20/42], [94mLoss[0m : 2.45237
[1mStep[0m  [24/42], [94mLoss[0m : 2.24764
[1mStep[0m  [28/42], [94mLoss[0m : 2.29943
[1mStep[0m  [32/42], [94mLoss[0m : 2.37949
[1mStep[0m  [36/42], [94mLoss[0m : 2.28342
[1mStep[0m  [40/42], [94mLoss[0m : 2.20986

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.308, [92mTest[0m: 2.586, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.01790
[1mStep[0m  [4/42], [94mLoss[0m : 2.36227
[1mStep[0m  [8/42], [94mLoss[0m : 2.44895
[1mStep[0m  [12/42], [94mLoss[0m : 2.29320
[1mStep[0m  [16/42], [94mLoss[0m : 2.25753
[1mStep[0m  [20/42], [94mLoss[0m : 2.18811
[1mStep[0m  [24/42], [94mLoss[0m : 2.47268
[1mStep[0m  [28/42], [94mLoss[0m : 2.21139
[1mStep[0m  [32/42], [94mLoss[0m : 2.37335
[1mStep[0m  [36/42], [94mLoss[0m : 2.19841
[1mStep[0m  [40/42], [94mLoss[0m : 2.44027

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.277, [92mTest[0m: 2.529, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.22879
[1mStep[0m  [4/42], [94mLoss[0m : 2.33908
[1mStep[0m  [8/42], [94mLoss[0m : 2.07619
[1mStep[0m  [12/42], [94mLoss[0m : 2.38582
[1mStep[0m  [16/42], [94mLoss[0m : 2.28641
[1mStep[0m  [20/42], [94mLoss[0m : 2.02938
[1mStep[0m  [24/42], [94mLoss[0m : 2.17199
[1mStep[0m  [28/42], [94mLoss[0m : 2.40707
[1mStep[0m  [32/42], [94mLoss[0m : 2.19514
[1mStep[0m  [36/42], [94mLoss[0m : 2.50492
[1mStep[0m  [40/42], [94mLoss[0m : 2.16614

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.262, [92mTest[0m: 2.478, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38305
[1mStep[0m  [4/42], [94mLoss[0m : 2.04002
[1mStep[0m  [8/42], [94mLoss[0m : 2.05916
[1mStep[0m  [12/42], [94mLoss[0m : 2.12781
[1mStep[0m  [16/42], [94mLoss[0m : 2.11319
[1mStep[0m  [20/42], [94mLoss[0m : 2.17596
[1mStep[0m  [24/42], [94mLoss[0m : 2.12976
[1mStep[0m  [28/42], [94mLoss[0m : 2.15492
[1mStep[0m  [32/42], [94mLoss[0m : 2.31479
[1mStep[0m  [36/42], [94mLoss[0m : 2.25748
[1mStep[0m  [40/42], [94mLoss[0m : 2.31225

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.261, [92mTest[0m: 2.550, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.22713
[1mStep[0m  [4/42], [94mLoss[0m : 2.32639
[1mStep[0m  [8/42], [94mLoss[0m : 2.18673
[1mStep[0m  [12/42], [94mLoss[0m : 2.32594
[1mStep[0m  [16/42], [94mLoss[0m : 2.33791
[1mStep[0m  [20/42], [94mLoss[0m : 2.25758
[1mStep[0m  [24/42], [94mLoss[0m : 2.19441
[1mStep[0m  [28/42], [94mLoss[0m : 2.35732
[1mStep[0m  [32/42], [94mLoss[0m : 2.06737
[1mStep[0m  [36/42], [94mLoss[0m : 2.38469
[1mStep[0m  [40/42], [94mLoss[0m : 2.40220

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.251, [92mTest[0m: 2.523, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.19432
[1mStep[0m  [4/42], [94mLoss[0m : 2.29408
[1mStep[0m  [8/42], [94mLoss[0m : 2.16619
[1mStep[0m  [12/42], [94mLoss[0m : 2.30176
[1mStep[0m  [16/42], [94mLoss[0m : 2.15453
[1mStep[0m  [20/42], [94mLoss[0m : 2.12223
[1mStep[0m  [24/42], [94mLoss[0m : 2.28388
[1mStep[0m  [28/42], [94mLoss[0m : 2.15768
[1mStep[0m  [32/42], [94mLoss[0m : 2.26690
[1mStep[0m  [36/42], [94mLoss[0m : 2.06779
[1mStep[0m  [40/42], [94mLoss[0m : 2.24341

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.213, [92mTest[0m: 2.559, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.23147
[1mStep[0m  [4/42], [94mLoss[0m : 2.00789
[1mStep[0m  [8/42], [94mLoss[0m : 2.14650
[1mStep[0m  [12/42], [94mLoss[0m : 2.37040
[1mStep[0m  [16/42], [94mLoss[0m : 2.32434
[1mStep[0m  [20/42], [94mLoss[0m : 2.16829
[1mStep[0m  [24/42], [94mLoss[0m : 2.25205
[1mStep[0m  [28/42], [94mLoss[0m : 2.10647
[1mStep[0m  [32/42], [94mLoss[0m : 1.97652
[1mStep[0m  [36/42], [94mLoss[0m : 2.14457
[1mStep[0m  [40/42], [94mLoss[0m : 2.24992

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.199, [92mTest[0m: 2.550, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.03036
[1mStep[0m  [4/42], [94mLoss[0m : 2.27653
[1mStep[0m  [8/42], [94mLoss[0m : 2.25927
[1mStep[0m  [12/42], [94mLoss[0m : 2.18525
[1mStep[0m  [16/42], [94mLoss[0m : 2.25842
[1mStep[0m  [20/42], [94mLoss[0m : 2.42197
[1mStep[0m  [24/42], [94mLoss[0m : 2.11746
[1mStep[0m  [28/42], [94mLoss[0m : 2.14177
[1mStep[0m  [32/42], [94mLoss[0m : 2.20559
[1mStep[0m  [36/42], [94mLoss[0m : 2.27080
[1mStep[0m  [40/42], [94mLoss[0m : 2.21160

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.199, [92mTest[0m: 2.511, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.17822
[1mStep[0m  [4/42], [94mLoss[0m : 2.32330
[1mStep[0m  [8/42], [94mLoss[0m : 2.12707
[1mStep[0m  [12/42], [94mLoss[0m : 2.17796
[1mStep[0m  [16/42], [94mLoss[0m : 2.13549
[1mStep[0m  [20/42], [94mLoss[0m : 2.19023
[1mStep[0m  [24/42], [94mLoss[0m : 2.11924
[1mStep[0m  [28/42], [94mLoss[0m : 2.18856
[1mStep[0m  [32/42], [94mLoss[0m : 2.27015
[1mStep[0m  [36/42], [94mLoss[0m : 2.08320
[1mStep[0m  [40/42], [94mLoss[0m : 2.32531

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.180, [92mTest[0m: 2.499, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.96846
[1mStep[0m  [4/42], [94mLoss[0m : 2.02391
[1mStep[0m  [8/42], [94mLoss[0m : 2.05638
[1mStep[0m  [12/42], [94mLoss[0m : 2.01183
[1mStep[0m  [16/42], [94mLoss[0m : 2.16076
[1mStep[0m  [20/42], [94mLoss[0m : 2.23126
[1mStep[0m  [24/42], [94mLoss[0m : 2.10005
[1mStep[0m  [28/42], [94mLoss[0m : 2.08210
[1mStep[0m  [32/42], [94mLoss[0m : 2.39273
[1mStep[0m  [36/42], [94mLoss[0m : 2.13304
[1mStep[0m  [40/42], [94mLoss[0m : 1.98469

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.153, [92mTest[0m: 2.496, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.16692
[1mStep[0m  [4/42], [94mLoss[0m : 1.99343
[1mStep[0m  [8/42], [94mLoss[0m : 2.15858
[1mStep[0m  [12/42], [94mLoss[0m : 2.16752
[1mStep[0m  [16/42], [94mLoss[0m : 2.32860
[1mStep[0m  [20/42], [94mLoss[0m : 2.08502
[1mStep[0m  [24/42], [94mLoss[0m : 2.06470
[1mStep[0m  [28/42], [94mLoss[0m : 2.19930
[1mStep[0m  [32/42], [94mLoss[0m : 2.37765
[1mStep[0m  [36/42], [94mLoss[0m : 2.28912
[1mStep[0m  [40/42], [94mLoss[0m : 1.92173

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.149, [92mTest[0m: 2.499, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.06538
[1mStep[0m  [4/42], [94mLoss[0m : 2.17114
[1mStep[0m  [8/42], [94mLoss[0m : 2.03579
[1mStep[0m  [12/42], [94mLoss[0m : 2.06832
[1mStep[0m  [16/42], [94mLoss[0m : 2.09964
[1mStep[0m  [20/42], [94mLoss[0m : 2.19282
[1mStep[0m  [24/42], [94mLoss[0m : 2.05157
[1mStep[0m  [28/42], [94mLoss[0m : 2.10729
[1mStep[0m  [32/42], [94mLoss[0m : 2.14956
[1mStep[0m  [36/42], [94mLoss[0m : 2.12317
[1mStep[0m  [40/42], [94mLoss[0m : 2.15814

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.124, [92mTest[0m: 2.492, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.11647
[1mStep[0m  [4/42], [94mLoss[0m : 2.03000
[1mStep[0m  [8/42], [94mLoss[0m : 2.12808
[1mStep[0m  [12/42], [94mLoss[0m : 1.92930
[1mStep[0m  [16/42], [94mLoss[0m : 2.11462
[1mStep[0m  [20/42], [94mLoss[0m : 2.03247
[1mStep[0m  [24/42], [94mLoss[0m : 2.15998
[1mStep[0m  [28/42], [94mLoss[0m : 1.98997
[1mStep[0m  [32/42], [94mLoss[0m : 2.06769
[1mStep[0m  [36/42], [94mLoss[0m : 2.24348
[1mStep[0m  [40/42], [94mLoss[0m : 2.21622

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.108, [92mTest[0m: 2.518, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31176
[1mStep[0m  [4/42], [94mLoss[0m : 2.11429
[1mStep[0m  [8/42], [94mLoss[0m : 1.92438
[1mStep[0m  [12/42], [94mLoss[0m : 2.02280
[1mStep[0m  [16/42], [94mLoss[0m : 2.15406
[1mStep[0m  [20/42], [94mLoss[0m : 2.14463
[1mStep[0m  [24/42], [94mLoss[0m : 2.26436
[1mStep[0m  [28/42], [94mLoss[0m : 2.05171
[1mStep[0m  [32/42], [94mLoss[0m : 2.10890
[1mStep[0m  [36/42], [94mLoss[0m : 2.12478
[1mStep[0m  [40/42], [94mLoss[0m : 2.02893

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.094, [92mTest[0m: 2.511, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.19423
[1mStep[0m  [4/42], [94mLoss[0m : 2.08774
[1mStep[0m  [8/42], [94mLoss[0m : 2.07643
[1mStep[0m  [12/42], [94mLoss[0m : 2.04404
[1mStep[0m  [16/42], [94mLoss[0m : 1.99957
[1mStep[0m  [20/42], [94mLoss[0m : 2.02951
[1mStep[0m  [24/42], [94mLoss[0m : 1.92194
[1mStep[0m  [28/42], [94mLoss[0m : 2.32048
[1mStep[0m  [32/42], [94mLoss[0m : 2.19955
[1mStep[0m  [36/42], [94mLoss[0m : 2.02120
[1mStep[0m  [40/42], [94mLoss[0m : 2.05558

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.068, [92mTest[0m: 2.473, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.90217
[1mStep[0m  [4/42], [94mLoss[0m : 1.90493
[1mStep[0m  [8/42], [94mLoss[0m : 2.04569
[1mStep[0m  [12/42], [94mLoss[0m : 1.91292
[1mStep[0m  [16/42], [94mLoss[0m : 2.27564
[1mStep[0m  [20/42], [94mLoss[0m : 1.95929
[1mStep[0m  [24/42], [94mLoss[0m : 2.09748
[1mStep[0m  [28/42], [94mLoss[0m : 1.94234
[1mStep[0m  [32/42], [94mLoss[0m : 2.21370
[1mStep[0m  [36/42], [94mLoss[0m : 2.10569
[1mStep[0m  [40/42], [94mLoss[0m : 2.21263

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.069, [92mTest[0m: 2.477, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.86134
[1mStep[0m  [4/42], [94mLoss[0m : 1.97600
[1mStep[0m  [8/42], [94mLoss[0m : 2.10397
[1mStep[0m  [12/42], [94mLoss[0m : 2.00287
[1mStep[0m  [16/42], [94mLoss[0m : 2.14015
[1mStep[0m  [20/42], [94mLoss[0m : 2.16304
[1mStep[0m  [24/42], [94mLoss[0m : 1.92628
[1mStep[0m  [28/42], [94mLoss[0m : 2.15862
[1mStep[0m  [32/42], [94mLoss[0m : 2.01057
[1mStep[0m  [36/42], [94mLoss[0m : 1.90702
[1mStep[0m  [40/42], [94mLoss[0m : 1.98498

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.057, [92mTest[0m: 2.465, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.91506
[1mStep[0m  [4/42], [94mLoss[0m : 2.09169
[1mStep[0m  [8/42], [94mLoss[0m : 1.98711
[1mStep[0m  [12/42], [94mLoss[0m : 2.03517
[1mStep[0m  [16/42], [94mLoss[0m : 1.78519
[1mStep[0m  [20/42], [94mLoss[0m : 1.93326
[1mStep[0m  [24/42], [94mLoss[0m : 2.10744
[1mStep[0m  [28/42], [94mLoss[0m : 2.10605
[1mStep[0m  [32/42], [94mLoss[0m : 2.03306
[1mStep[0m  [36/42], [94mLoss[0m : 2.21701
[1mStep[0m  [40/42], [94mLoss[0m : 2.18579

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.021, [92mTest[0m: 2.446, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.89643
[1mStep[0m  [4/42], [94mLoss[0m : 2.04239
[1mStep[0m  [8/42], [94mLoss[0m : 2.02901
[1mStep[0m  [12/42], [94mLoss[0m : 1.84382
[1mStep[0m  [16/42], [94mLoss[0m : 2.12316
[1mStep[0m  [20/42], [94mLoss[0m : 2.05847
[1mStep[0m  [24/42], [94mLoss[0m : 2.06148
[1mStep[0m  [28/42], [94mLoss[0m : 1.91486
[1mStep[0m  [32/42], [94mLoss[0m : 1.98685
[1mStep[0m  [36/42], [94mLoss[0m : 1.94540
[1mStep[0m  [40/42], [94mLoss[0m : 2.09648

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.022, [92mTest[0m: 2.423, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.425
====================================

Phase 2 - Evaluation MAE:  2.4249459164483205
MAE score P1      2.550622
MAE score P2      2.424946
loss              2.020724
learning_rate     0.002575
batch_size             256
hidden_sizes         [300]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.1
weight_decay        0.0001
Name: 15, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 10.46256
[1mStep[0m  [4/42], [94mLoss[0m : 9.67326
[1mStep[0m  [8/42], [94mLoss[0m : 10.09256
[1mStep[0m  [12/42], [94mLoss[0m : 10.16156
[1mStep[0m  [16/42], [94mLoss[0m : 9.87916
[1mStep[0m  [20/42], [94mLoss[0m : 9.75790
[1mStep[0m  [24/42], [94mLoss[0m : 9.66658
[1mStep[0m  [28/42], [94mLoss[0m : 9.22625
[1mStep[0m  [32/42], [94mLoss[0m : 9.12485
[1mStep[0m  [36/42], [94mLoss[0m : 9.38218
[1mStep[0m  [40/42], [94mLoss[0m : 8.73595

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.660, [92mTest[0m: 10.917, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.51021
[1mStep[0m  [4/42], [94mLoss[0m : 8.54469
[1mStep[0m  [8/42], [94mLoss[0m : 8.36693
[1mStep[0m  [12/42], [94mLoss[0m : 8.27284
[1mStep[0m  [16/42], [94mLoss[0m : 8.18260
[1mStep[0m  [20/42], [94mLoss[0m : 7.39508
[1mStep[0m  [24/42], [94mLoss[0m : 7.27921
[1mStep[0m  [28/42], [94mLoss[0m : 7.34566
[1mStep[0m  [32/42], [94mLoss[0m : 7.36017
[1mStep[0m  [36/42], [94mLoss[0m : 6.96226
[1mStep[0m  [40/42], [94mLoss[0m : 6.56128

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 7.722, [92mTest[0m: 9.534, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.79644
[1mStep[0m  [4/42], [94mLoss[0m : 6.32713
[1mStep[0m  [8/42], [94mLoss[0m : 6.06564
[1mStep[0m  [12/42], [94mLoss[0m : 6.43534
[1mStep[0m  [16/42], [94mLoss[0m : 6.43643
[1mStep[0m  [20/42], [94mLoss[0m : 6.20640
[1mStep[0m  [24/42], [94mLoss[0m : 5.48304
[1mStep[0m  [28/42], [94mLoss[0m : 5.52456
[1mStep[0m  [32/42], [94mLoss[0m : 5.17248
[1mStep[0m  [36/42], [94mLoss[0m : 5.29196
[1mStep[0m  [40/42], [94mLoss[0m : 4.79618

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 5.917, [92mTest[0m: 8.213, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.15725
[1mStep[0m  [4/42], [94mLoss[0m : 4.89471
[1mStep[0m  [8/42], [94mLoss[0m : 4.69713
[1mStep[0m  [12/42], [94mLoss[0m : 4.54423
[1mStep[0m  [16/42], [94mLoss[0m : 4.32432
[1mStep[0m  [20/42], [94mLoss[0m : 4.75179
[1mStep[0m  [24/42], [94mLoss[0m : 4.30838
[1mStep[0m  [28/42], [94mLoss[0m : 3.92000
[1mStep[0m  [32/42], [94mLoss[0m : 3.97395
[1mStep[0m  [36/42], [94mLoss[0m : 3.79731
[1mStep[0m  [40/42], [94mLoss[0m : 3.88900

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 4.411, [92mTest[0m: 6.862, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.89409
[1mStep[0m  [4/42], [94mLoss[0m : 3.93742
[1mStep[0m  [8/42], [94mLoss[0m : 3.44303
[1mStep[0m  [12/42], [94mLoss[0m : 3.14481
[1mStep[0m  [16/42], [94mLoss[0m : 3.28452
[1mStep[0m  [20/42], [94mLoss[0m : 3.41763
[1mStep[0m  [24/42], [94mLoss[0m : 3.40555
[1mStep[0m  [28/42], [94mLoss[0m : 3.57702
[1mStep[0m  [32/42], [94mLoss[0m : 3.32767
[1mStep[0m  [36/42], [94mLoss[0m : 3.06332
[1mStep[0m  [40/42], [94mLoss[0m : 3.12819

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 3.475, [92mTest[0m: 5.427, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.90272
[1mStep[0m  [4/42], [94mLoss[0m : 3.10229
[1mStep[0m  [8/42], [94mLoss[0m : 3.19811
[1mStep[0m  [12/42], [94mLoss[0m : 3.38979
[1mStep[0m  [16/42], [94mLoss[0m : 3.27344
[1mStep[0m  [20/42], [94mLoss[0m : 2.85428
[1mStep[0m  [24/42], [94mLoss[0m : 3.05673
[1mStep[0m  [28/42], [94mLoss[0m : 3.29037
[1mStep[0m  [32/42], [94mLoss[0m : 3.17668
[1mStep[0m  [36/42], [94mLoss[0m : 2.83644
[1mStep[0m  [40/42], [94mLoss[0m : 2.86259

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 3.067, [92mTest[0m: 4.485, [96mlr[0m: 0.002575
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 5 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 3.978
====================================

Phase 1 - Evaluation MAE:  3.9780325378690446
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 3.07862
[1mStep[0m  [4/42], [94mLoss[0m : 2.90351
[1mStep[0m  [8/42], [94mLoss[0m : 2.98725
[1mStep[0m  [12/42], [94mLoss[0m : 3.01950
[1mStep[0m  [16/42], [94mLoss[0m : 2.78778
[1mStep[0m  [20/42], [94mLoss[0m : 2.85252
[1mStep[0m  [24/42], [94mLoss[0m : 3.09292
[1mStep[0m  [28/42], [94mLoss[0m : 3.21737
[1mStep[0m  [32/42], [94mLoss[0m : 2.79784
[1mStep[0m  [36/42], [94mLoss[0m : 2.68411
[1mStep[0m  [40/42], [94mLoss[0m : 3.03503

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.934, [92mTest[0m: 3.975, [96mlr[0m: 0.002575
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 0 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.983
====================================

Phase 2 - Evaluation MAE:  2.9826203414372037
MAE score P1        3.978033
MAE score P2         2.98262
loss                2.933651
learning_rate       0.002575
batch_size               256
hidden_sizes      [300, 100]
epochs                    30
activation              relu
optimizer                sgd
early stopping          True
dropout                  0.4
momentum                 0.1
weight_decay           0.001
Name: 16, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 10.63935
[1mStep[0m  [4/42], [94mLoss[0m : 10.45822
[1mStep[0m  [8/42], [94mLoss[0m : 10.18394
[1mStep[0m  [12/42], [94mLoss[0m : 10.13543
[1mStep[0m  [16/42], [94mLoss[0m : 9.87015
[1mStep[0m  [20/42], [94mLoss[0m : 9.52944
[1mStep[0m  [24/42], [94mLoss[0m : 9.14368
[1mStep[0m  [28/42], [94mLoss[0m : 8.88871
[1mStep[0m  [32/42], [94mLoss[0m : 8.72975
[1mStep[0m  [36/42], [94mLoss[0m : 8.12523
[1mStep[0m  [40/42], [94mLoss[0m : 8.64564

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.330, [92mTest[0m: 10.625, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.86913
[1mStep[0m  [4/42], [94mLoss[0m : 7.48520
[1mStep[0m  [8/42], [94mLoss[0m : 7.41345
[1mStep[0m  [12/42], [94mLoss[0m : 6.81164
[1mStep[0m  [16/42], [94mLoss[0m : 6.92934
[1mStep[0m  [20/42], [94mLoss[0m : 6.48348
[1mStep[0m  [24/42], [94mLoss[0m : 6.29964
[1mStep[0m  [28/42], [94mLoss[0m : 5.99289
[1mStep[0m  [32/42], [94mLoss[0m : 5.98513
[1mStep[0m  [36/42], [94mLoss[0m : 5.48313
[1mStep[0m  [40/42], [94mLoss[0m : 5.11363

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 6.422, [92mTest[0m: 7.820, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.71951
[1mStep[0m  [4/42], [94mLoss[0m : 4.91002
[1mStep[0m  [8/42], [94mLoss[0m : 4.70811
[1mStep[0m  [12/42], [94mLoss[0m : 4.11176
[1mStep[0m  [16/42], [94mLoss[0m : 4.42021
[1mStep[0m  [20/42], [94mLoss[0m : 4.10668
[1mStep[0m  [24/42], [94mLoss[0m : 3.84013
[1mStep[0m  [28/42], [94mLoss[0m : 3.55447
[1mStep[0m  [32/42], [94mLoss[0m : 3.80552
[1mStep[0m  [36/42], [94mLoss[0m : 3.48017
[1mStep[0m  [40/42], [94mLoss[0m : 3.26995

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 4.068, [92mTest[0m: 4.990, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.38198
