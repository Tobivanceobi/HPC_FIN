no change     /home/modelrep/sadiya/miniconda/condabin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda-env
no change     /home/modelrep/sadiya/miniconda/bin/activate
no change     /home/modelrep/sadiya/miniconda/bin/deactivate
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.sh
no change     /home/modelrep/sadiya/miniconda/etc/fish/conf.d/conda.fish
no change     /home/modelrep/sadiya/miniconda/shell/condabin/Conda.psm1
no change     /home/modelrep/sadiya/miniconda/shell/condabin/conda-hook.ps1
no change     /home/modelrep/sadiya/miniconda/lib/python3.10/site-packages/xontrib/conda.xsh
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.csh
no change     /home/modelrep/sadiya/.bashrc
No action taken.
-----------------
Process ID:  5
Number of Nodes:  1
Number of Array Tasks:  16
-----------------
Using device: cuda

AMD Instinct MI210
Memory Usage:
Allocated: 0.0 GB
Cached:    0.0 GB
theta
whole_spec
delta
alpha
beta
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/53], [94mLoss[0m : 11.22830
[1mStep[0m  [5/53], [94mLoss[0m : 10.32521
[1mStep[0m  [10/53], [94mLoss[0m : 9.76103
[1mStep[0m  [15/53], [94mLoss[0m : 7.57034
[1mStep[0m  [20/53], [94mLoss[0m : 6.49528
[1mStep[0m  [25/53], [94mLoss[0m : 4.97756
[1mStep[0m  [30/53], [94mLoss[0m : 3.22183
[1mStep[0m  [35/53], [94mLoss[0m : 2.96710
[1mStep[0m  [40/53], [94mLoss[0m : 2.77994
[1mStep[0m  [45/53], [94mLoss[0m : 2.89461
[1mStep[0m  [50/53], [94mLoss[0m : 2.86218

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 5.703, [92mTest[0m: 11.002, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.91495
[1mStep[0m  [5/53], [94mLoss[0m : 2.53545
[1mStep[0m  [10/53], [94mLoss[0m : 2.82459
[1mStep[0m  [15/53], [94mLoss[0m : 2.51100
[1mStep[0m  [20/53], [94mLoss[0m : 2.72782
[1mStep[0m  [25/53], [94mLoss[0m : 2.53383
[1mStep[0m  [30/53], [94mLoss[0m : 2.64569
[1mStep[0m  [35/53], [94mLoss[0m : 2.54119
[1mStep[0m  [40/53], [94mLoss[0m : 2.65903
[1mStep[0m  [45/53], [94mLoss[0m : 2.38456
[1mStep[0m  [50/53], [94mLoss[0m : 2.94681

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.620, [92mTest[0m: 2.890, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.44853
[1mStep[0m  [5/53], [94mLoss[0m : 2.46653
[1mStep[0m  [10/53], [94mLoss[0m : 2.62049
[1mStep[0m  [15/53], [94mLoss[0m : 2.63795
[1mStep[0m  [20/53], [94mLoss[0m : 2.50770
[1mStep[0m  [25/53], [94mLoss[0m : 2.39423
[1mStep[0m  [30/53], [94mLoss[0m : 2.64134
[1mStep[0m  [35/53], [94mLoss[0m : 2.64411
[1mStep[0m  [40/53], [94mLoss[0m : 2.59066
[1mStep[0m  [45/53], [94mLoss[0m : 2.39767
[1mStep[0m  [50/53], [94mLoss[0m : 2.48412

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.577, [92mTest[0m: 2.660, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.54248
[1mStep[0m  [5/53], [94mLoss[0m : 2.48989
[1mStep[0m  [10/53], [94mLoss[0m : 2.45454
[1mStep[0m  [15/53], [94mLoss[0m : 2.34135
[1mStep[0m  [20/53], [94mLoss[0m : 2.32365
[1mStep[0m  [25/53], [94mLoss[0m : 2.73062
[1mStep[0m  [30/53], [94mLoss[0m : 2.50885
[1mStep[0m  [35/53], [94mLoss[0m : 2.84205
[1mStep[0m  [40/53], [94mLoss[0m : 2.62704
[1mStep[0m  [45/53], [94mLoss[0m : 2.59442
[1mStep[0m  [50/53], [94mLoss[0m : 2.48903

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.501, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.58206
[1mStep[0m  [5/53], [94mLoss[0m : 2.31236
[1mStep[0m  [10/53], [94mLoss[0m : 2.49111
[1mStep[0m  [15/53], [94mLoss[0m : 2.68903
[1mStep[0m  [20/53], [94mLoss[0m : 2.69274
[1mStep[0m  [25/53], [94mLoss[0m : 2.52498
[1mStep[0m  [30/53], [94mLoss[0m : 2.56878
[1mStep[0m  [35/53], [94mLoss[0m : 2.49915
[1mStep[0m  [40/53], [94mLoss[0m : 2.58386
[1mStep[0m  [45/53], [94mLoss[0m : 2.36739
[1mStep[0m  [50/53], [94mLoss[0m : 2.51762

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.498, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.57896
[1mStep[0m  [5/53], [94mLoss[0m : 2.46009
[1mStep[0m  [10/53], [94mLoss[0m : 2.53745
[1mStep[0m  [15/53], [94mLoss[0m : 2.44978
[1mStep[0m  [20/53], [94mLoss[0m : 2.64426
[1mStep[0m  [25/53], [94mLoss[0m : 2.53580
[1mStep[0m  [30/53], [94mLoss[0m : 2.37220
[1mStep[0m  [35/53], [94mLoss[0m : 2.49206
[1mStep[0m  [40/53], [94mLoss[0m : 2.61519
[1mStep[0m  [45/53], [94mLoss[0m : 2.52199
[1mStep[0m  [50/53], [94mLoss[0m : 2.63894

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.462, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.50475
[1mStep[0m  [5/53], [94mLoss[0m : 2.71622
[1mStep[0m  [10/53], [94mLoss[0m : 2.51139
[1mStep[0m  [15/53], [94mLoss[0m : 2.56153
[1mStep[0m  [20/53], [94mLoss[0m : 2.77878
[1mStep[0m  [25/53], [94mLoss[0m : 2.69318
[1mStep[0m  [30/53], [94mLoss[0m : 2.40295
[1mStep[0m  [35/53], [94mLoss[0m : 2.51078
[1mStep[0m  [40/53], [94mLoss[0m : 2.38604
[1mStep[0m  [45/53], [94mLoss[0m : 2.33214
[1mStep[0m  [50/53], [94mLoss[0m : 2.36153

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.413, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.30465
[1mStep[0m  [5/53], [94mLoss[0m : 2.46079
[1mStep[0m  [10/53], [94mLoss[0m : 2.69444
[1mStep[0m  [15/53], [94mLoss[0m : 2.50095
[1mStep[0m  [20/53], [94mLoss[0m : 2.58829
[1mStep[0m  [25/53], [94mLoss[0m : 2.39370
[1mStep[0m  [30/53], [94mLoss[0m : 2.54015
[1mStep[0m  [35/53], [94mLoss[0m : 2.48426
[1mStep[0m  [40/53], [94mLoss[0m : 2.54675
[1mStep[0m  [45/53], [94mLoss[0m : 2.47339
[1mStep[0m  [50/53], [94mLoss[0m : 2.54919

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.456, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.75083
[1mStep[0m  [5/53], [94mLoss[0m : 2.36957
[1mStep[0m  [10/53], [94mLoss[0m : 2.23815
[1mStep[0m  [15/53], [94mLoss[0m : 2.49519
[1mStep[0m  [20/53], [94mLoss[0m : 2.30941
[1mStep[0m  [25/53], [94mLoss[0m : 2.63845
[1mStep[0m  [30/53], [94mLoss[0m : 2.29685
[1mStep[0m  [35/53], [94mLoss[0m : 2.38228
[1mStep[0m  [40/53], [94mLoss[0m : 2.54631
[1mStep[0m  [45/53], [94mLoss[0m : 2.53464
[1mStep[0m  [50/53], [94mLoss[0m : 2.43003

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.406, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.39678
[1mStep[0m  [5/53], [94mLoss[0m : 2.46659
[1mStep[0m  [10/53], [94mLoss[0m : 2.51113
[1mStep[0m  [15/53], [94mLoss[0m : 2.59828
[1mStep[0m  [20/53], [94mLoss[0m : 2.32366
[1mStep[0m  [25/53], [94mLoss[0m : 2.32188
[1mStep[0m  [30/53], [94mLoss[0m : 2.61512
[1mStep[0m  [35/53], [94mLoss[0m : 2.56013
[1mStep[0m  [40/53], [94mLoss[0m : 2.36319
[1mStep[0m  [45/53], [94mLoss[0m : 2.50953
[1mStep[0m  [50/53], [94mLoss[0m : 2.34727

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.374, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.32314
[1mStep[0m  [5/53], [94mLoss[0m : 2.39661
[1mStep[0m  [10/53], [94mLoss[0m : 2.38251
[1mStep[0m  [15/53], [94mLoss[0m : 2.32813
[1mStep[0m  [20/53], [94mLoss[0m : 2.48648
[1mStep[0m  [25/53], [94mLoss[0m : 2.45621
[1mStep[0m  [30/53], [94mLoss[0m : 2.28292
[1mStep[0m  [35/53], [94mLoss[0m : 2.31441
[1mStep[0m  [40/53], [94mLoss[0m : 2.53930
[1mStep[0m  [45/53], [94mLoss[0m : 2.33823
[1mStep[0m  [50/53], [94mLoss[0m : 2.42952

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.408, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.16934
[1mStep[0m  [5/53], [94mLoss[0m : 2.52957
[1mStep[0m  [10/53], [94mLoss[0m : 2.48624
[1mStep[0m  [15/53], [94mLoss[0m : 2.89558
[1mStep[0m  [20/53], [94mLoss[0m : 2.38633
[1mStep[0m  [25/53], [94mLoss[0m : 2.34266
[1mStep[0m  [30/53], [94mLoss[0m : 2.35519
[1mStep[0m  [35/53], [94mLoss[0m : 2.17547
[1mStep[0m  [40/53], [94mLoss[0m : 2.39709
[1mStep[0m  [45/53], [94mLoss[0m : 2.54932
[1mStep[0m  [50/53], [94mLoss[0m : 2.64401

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.360, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.09887
[1mStep[0m  [5/53], [94mLoss[0m : 2.29736
[1mStep[0m  [10/53], [94mLoss[0m : 2.41149
[1mStep[0m  [15/53], [94mLoss[0m : 2.38163
[1mStep[0m  [20/53], [94mLoss[0m : 2.29878
[1mStep[0m  [25/53], [94mLoss[0m : 2.50558
[1mStep[0m  [30/53], [94mLoss[0m : 2.19901
[1mStep[0m  [35/53], [94mLoss[0m : 2.44464
[1mStep[0m  [40/53], [94mLoss[0m : 2.45941
[1mStep[0m  [45/53], [94mLoss[0m : 2.54117
[1mStep[0m  [50/53], [94mLoss[0m : 2.22930

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.403, [92mTest[0m: 2.398, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.62288
[1mStep[0m  [5/53], [94mLoss[0m : 2.44737
[1mStep[0m  [10/53], [94mLoss[0m : 2.48460
[1mStep[0m  [15/53], [94mLoss[0m : 2.49180
[1mStep[0m  [20/53], [94mLoss[0m : 2.53511
[1mStep[0m  [25/53], [94mLoss[0m : 2.52808
[1mStep[0m  [30/53], [94mLoss[0m : 2.49774
[1mStep[0m  [35/53], [94mLoss[0m : 2.36974
[1mStep[0m  [40/53], [94mLoss[0m : 2.35341
[1mStep[0m  [45/53], [94mLoss[0m : 2.41024
[1mStep[0m  [50/53], [94mLoss[0m : 2.56517

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.392, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.28126
[1mStep[0m  [5/53], [94mLoss[0m : 2.35805
[1mStep[0m  [10/53], [94mLoss[0m : 2.45536
[1mStep[0m  [15/53], [94mLoss[0m : 2.53869
[1mStep[0m  [20/53], [94mLoss[0m : 2.48169
[1mStep[0m  [25/53], [94mLoss[0m : 2.46130
[1mStep[0m  [30/53], [94mLoss[0m : 2.39528
[1mStep[0m  [35/53], [94mLoss[0m : 2.53333
[1mStep[0m  [40/53], [94mLoss[0m : 2.30592
[1mStep[0m  [45/53], [94mLoss[0m : 2.48117
[1mStep[0m  [50/53], [94mLoss[0m : 2.30625

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.379, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.08493
[1mStep[0m  [5/53], [94mLoss[0m : 2.40408
[1mStep[0m  [10/53], [94mLoss[0m : 2.34412
[1mStep[0m  [15/53], [94mLoss[0m : 2.26805
[1mStep[0m  [20/53], [94mLoss[0m : 2.28329
[1mStep[0m  [25/53], [94mLoss[0m : 2.35399
[1mStep[0m  [30/53], [94mLoss[0m : 2.24783
[1mStep[0m  [35/53], [94mLoss[0m : 2.43401
[1mStep[0m  [40/53], [94mLoss[0m : 2.39415
[1mStep[0m  [45/53], [94mLoss[0m : 2.33976
[1mStep[0m  [50/53], [94mLoss[0m : 2.27033

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.345, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.25485
[1mStep[0m  [5/53], [94mLoss[0m : 2.25732
[1mStep[0m  [10/53], [94mLoss[0m : 2.43288
[1mStep[0m  [15/53], [94mLoss[0m : 2.52045
[1mStep[0m  [20/53], [94mLoss[0m : 2.37300
[1mStep[0m  [25/53], [94mLoss[0m : 2.16651
[1mStep[0m  [30/53], [94mLoss[0m : 2.44708
[1mStep[0m  [35/53], [94mLoss[0m : 2.21340
[1mStep[0m  [40/53], [94mLoss[0m : 2.27377
[1mStep[0m  [45/53], [94mLoss[0m : 2.59394
[1mStep[0m  [50/53], [94mLoss[0m : 2.19202

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.371, [92mTest[0m: 2.381, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.54282
[1mStep[0m  [5/53], [94mLoss[0m : 2.62157
[1mStep[0m  [10/53], [94mLoss[0m : 2.50471
[1mStep[0m  [15/53], [94mLoss[0m : 2.31100
[1mStep[0m  [20/53], [94mLoss[0m : 2.25065
[1mStep[0m  [25/53], [94mLoss[0m : 2.45277
[1mStep[0m  [30/53], [94mLoss[0m : 2.41153
[1mStep[0m  [35/53], [94mLoss[0m : 2.51856
[1mStep[0m  [40/53], [94mLoss[0m : 2.34328
[1mStep[0m  [45/53], [94mLoss[0m : 2.26168
[1mStep[0m  [50/53], [94mLoss[0m : 2.23687

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.383, [92mTest[0m: 2.376, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.30090
[1mStep[0m  [5/53], [94mLoss[0m : 2.50681
[1mStep[0m  [10/53], [94mLoss[0m : 2.38509
[1mStep[0m  [15/53], [94mLoss[0m : 2.39822
[1mStep[0m  [20/53], [94mLoss[0m : 2.40373
[1mStep[0m  [25/53], [94mLoss[0m : 2.57143
[1mStep[0m  [30/53], [94mLoss[0m : 2.49949
[1mStep[0m  [35/53], [94mLoss[0m : 2.31610
[1mStep[0m  [40/53], [94mLoss[0m : 2.44589
[1mStep[0m  [45/53], [94mLoss[0m : 2.29254
[1mStep[0m  [50/53], [94mLoss[0m : 2.19235

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.337, [92mTest[0m: 2.377, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.33917
[1mStep[0m  [5/53], [94mLoss[0m : 2.42570
[1mStep[0m  [10/53], [94mLoss[0m : 2.33843
[1mStep[0m  [15/53], [94mLoss[0m : 2.30036
[1mStep[0m  [20/53], [94mLoss[0m : 2.33334
[1mStep[0m  [25/53], [94mLoss[0m : 2.43881
[1mStep[0m  [30/53], [94mLoss[0m : 2.29102
[1mStep[0m  [35/53], [94mLoss[0m : 2.30164
[1mStep[0m  [40/53], [94mLoss[0m : 2.31512
[1mStep[0m  [45/53], [94mLoss[0m : 2.22333
[1mStep[0m  [50/53], [94mLoss[0m : 2.44813

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.348, [92mTest[0m: 2.364, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.37925
[1mStep[0m  [5/53], [94mLoss[0m : 2.32072
[1mStep[0m  [10/53], [94mLoss[0m : 2.56077
[1mStep[0m  [15/53], [94mLoss[0m : 2.26372
[1mStep[0m  [20/53], [94mLoss[0m : 2.35692
[1mStep[0m  [25/53], [94mLoss[0m : 2.42913
[1mStep[0m  [30/53], [94mLoss[0m : 2.31943
[1mStep[0m  [35/53], [94mLoss[0m : 2.33507
[1mStep[0m  [40/53], [94mLoss[0m : 2.21891
[1mStep[0m  [45/53], [94mLoss[0m : 2.29533
[1mStep[0m  [50/53], [94mLoss[0m : 2.34993

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.344, [92mTest[0m: 2.351, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.99999
[1mStep[0m  [5/53], [94mLoss[0m : 2.53562
[1mStep[0m  [10/53], [94mLoss[0m : 2.18558
[1mStep[0m  [15/53], [94mLoss[0m : 2.28238
[1mStep[0m  [20/53], [94mLoss[0m : 2.14752
[1mStep[0m  [25/53], [94mLoss[0m : 2.36280
[1mStep[0m  [30/53], [94mLoss[0m : 2.18886
[1mStep[0m  [35/53], [94mLoss[0m : 2.39369
[1mStep[0m  [40/53], [94mLoss[0m : 2.29614
[1mStep[0m  [45/53], [94mLoss[0m : 2.45074
[1mStep[0m  [50/53], [94mLoss[0m : 2.37473

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.344, [92mTest[0m: 2.360, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.36462
[1mStep[0m  [5/53], [94mLoss[0m : 2.42740
[1mStep[0m  [10/53], [94mLoss[0m : 2.42924
[1mStep[0m  [15/53], [94mLoss[0m : 2.28583
[1mStep[0m  [20/53], [94mLoss[0m : 2.42023
[1mStep[0m  [25/53], [94mLoss[0m : 2.30473
[1mStep[0m  [30/53], [94mLoss[0m : 2.41045
[1mStep[0m  [35/53], [94mLoss[0m : 2.38800
[1mStep[0m  [40/53], [94mLoss[0m : 2.16708
[1mStep[0m  [45/53], [94mLoss[0m : 2.46087
[1mStep[0m  [50/53], [94mLoss[0m : 2.20638

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.333, [92mTest[0m: 2.366, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.49057
[1mStep[0m  [5/53], [94mLoss[0m : 2.24920
[1mStep[0m  [10/53], [94mLoss[0m : 2.38104
[1mStep[0m  [15/53], [94mLoss[0m : 2.15575
[1mStep[0m  [20/53], [94mLoss[0m : 2.23283
[1mStep[0m  [25/53], [94mLoss[0m : 2.24213
[1mStep[0m  [30/53], [94mLoss[0m : 2.31134
[1mStep[0m  [35/53], [94mLoss[0m : 2.57023
[1mStep[0m  [40/53], [94mLoss[0m : 2.39574
[1mStep[0m  [45/53], [94mLoss[0m : 2.44198
[1mStep[0m  [50/53], [94mLoss[0m : 2.39342

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.319, [92mTest[0m: 2.349, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.28507
[1mStep[0m  [5/53], [94mLoss[0m : 2.34689
[1mStep[0m  [10/53], [94mLoss[0m : 2.31952
[1mStep[0m  [15/53], [94mLoss[0m : 2.28947
[1mStep[0m  [20/53], [94mLoss[0m : 2.28071
[1mStep[0m  [25/53], [94mLoss[0m : 2.43070
[1mStep[0m  [30/53], [94mLoss[0m : 2.23085
[1mStep[0m  [35/53], [94mLoss[0m : 2.34290
[1mStep[0m  [40/53], [94mLoss[0m : 2.24696
[1mStep[0m  [45/53], [94mLoss[0m : 2.32427
[1mStep[0m  [50/53], [94mLoss[0m : 2.37909

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.323, [92mTest[0m: 2.346, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.25306
[1mStep[0m  [5/53], [94mLoss[0m : 2.16324
[1mStep[0m  [10/53], [94mLoss[0m : 2.47849
[1mStep[0m  [15/53], [94mLoss[0m : 2.12358
[1mStep[0m  [20/53], [94mLoss[0m : 2.33700
[1mStep[0m  [25/53], [94mLoss[0m : 2.26236
[1mStep[0m  [30/53], [94mLoss[0m : 2.38373
[1mStep[0m  [35/53], [94mLoss[0m : 2.33687
[1mStep[0m  [40/53], [94mLoss[0m : 2.08809
[1mStep[0m  [45/53], [94mLoss[0m : 2.49266
[1mStep[0m  [50/53], [94mLoss[0m : 2.32562

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.318, [92mTest[0m: 2.352, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.25880
[1mStep[0m  [5/53], [94mLoss[0m : 2.17165
[1mStep[0m  [10/53], [94mLoss[0m : 2.28623
[1mStep[0m  [15/53], [94mLoss[0m : 2.77015
[1mStep[0m  [20/53], [94mLoss[0m : 2.16637
[1mStep[0m  [25/53], [94mLoss[0m : 2.10410
[1mStep[0m  [30/53], [94mLoss[0m : 2.32202
[1mStep[0m  [35/53], [94mLoss[0m : 2.48345
[1mStep[0m  [40/53], [94mLoss[0m : 2.24610
[1mStep[0m  [45/53], [94mLoss[0m : 2.42455
[1mStep[0m  [50/53], [94mLoss[0m : 2.63454

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.303, [92mTest[0m: 2.332, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.13329
[1mStep[0m  [5/53], [94mLoss[0m : 2.12906
[1mStep[0m  [10/53], [94mLoss[0m : 2.46273
[1mStep[0m  [15/53], [94mLoss[0m : 2.50284
[1mStep[0m  [20/53], [94mLoss[0m : 2.29753
[1mStep[0m  [25/53], [94mLoss[0m : 2.21899
[1mStep[0m  [30/53], [94mLoss[0m : 2.21084
[1mStep[0m  [35/53], [94mLoss[0m : 2.02796
[1mStep[0m  [40/53], [94mLoss[0m : 2.33212
[1mStep[0m  [45/53], [94mLoss[0m : 2.17094
[1mStep[0m  [50/53], [94mLoss[0m : 2.10331

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.308, [92mTest[0m: 2.338, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.16360
[1mStep[0m  [5/53], [94mLoss[0m : 2.41244
[1mStep[0m  [10/53], [94mLoss[0m : 2.19148
[1mStep[0m  [15/53], [94mLoss[0m : 2.12547
[1mStep[0m  [20/53], [94mLoss[0m : 2.37393
[1mStep[0m  [25/53], [94mLoss[0m : 2.25529
[1mStep[0m  [30/53], [94mLoss[0m : 2.32925
[1mStep[0m  [35/53], [94mLoss[0m : 2.18045
[1mStep[0m  [40/53], [94mLoss[0m : 2.24459
[1mStep[0m  [45/53], [94mLoss[0m : 2.48760
[1mStep[0m  [50/53], [94mLoss[0m : 2.27921

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.308, [92mTest[0m: 2.352, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.11340
[1mStep[0m  [5/53], [94mLoss[0m : 2.31341
[1mStep[0m  [10/53], [94mLoss[0m : 2.26069
[1mStep[0m  [15/53], [94mLoss[0m : 2.21211
[1mStep[0m  [20/53], [94mLoss[0m : 2.12572
[1mStep[0m  [25/53], [94mLoss[0m : 2.18125
[1mStep[0m  [30/53], [94mLoss[0m : 2.31174
[1mStep[0m  [35/53], [94mLoss[0m : 2.29143
[1mStep[0m  [40/53], [94mLoss[0m : 2.42942
[1mStep[0m  [45/53], [94mLoss[0m : 2.23557
[1mStep[0m  [50/53], [94mLoss[0m : 2.30099

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.292, [92mTest[0m: 2.349, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.339
====================================

Phase 1 - Evaluation MAE:  2.3389145044180064
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/53], [94mLoss[0m : 2.00321
[1mStep[0m  [5/53], [94mLoss[0m : 2.38543
[1mStep[0m  [10/53], [94mLoss[0m : 2.44725
[1mStep[0m  [15/53], [94mLoss[0m : 2.24779
[1mStep[0m  [20/53], [94mLoss[0m : 2.65122
[1mStep[0m  [25/53], [94mLoss[0m : 2.35569
[1mStep[0m  [30/53], [94mLoss[0m : 2.49356
[1mStep[0m  [35/53], [94mLoss[0m : 2.34882
[1mStep[0m  [40/53], [94mLoss[0m : 2.48179
[1mStep[0m  [45/53], [94mLoss[0m : 2.62073
[1mStep[0m  [50/53], [94mLoss[0m : 2.41143

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.340, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.33147
[1mStep[0m  [5/53], [94mLoss[0m : 2.03910
[1mStep[0m  [10/53], [94mLoss[0m : 2.43775
[1mStep[0m  [15/53], [94mLoss[0m : 2.27197
[1mStep[0m  [20/53], [94mLoss[0m : 2.33872
[1mStep[0m  [25/53], [94mLoss[0m : 2.13215
[1mStep[0m  [30/53], [94mLoss[0m : 2.44004
[1mStep[0m  [35/53], [94mLoss[0m : 2.46875
[1mStep[0m  [40/53], [94mLoss[0m : 2.24333
[1mStep[0m  [45/53], [94mLoss[0m : 2.56394
[1mStep[0m  [50/53], [94mLoss[0m : 2.43749

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.598, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.33213
[1mStep[0m  [5/53], [94mLoss[0m : 2.40059
[1mStep[0m  [10/53], [94mLoss[0m : 2.45522
[1mStep[0m  [15/53], [94mLoss[0m : 2.04648
[1mStep[0m  [20/53], [94mLoss[0m : 2.54641
[1mStep[0m  [25/53], [94mLoss[0m : 2.37593
[1mStep[0m  [30/53], [94mLoss[0m : 2.41315
[1mStep[0m  [35/53], [94mLoss[0m : 2.41270
[1mStep[0m  [40/53], [94mLoss[0m : 2.24591
[1mStep[0m  [45/53], [94mLoss[0m : 2.24091
[1mStep[0m  [50/53], [94mLoss[0m : 2.39443

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.322, [92mTest[0m: 2.558, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.36788
[1mStep[0m  [5/53], [94mLoss[0m : 2.32200
[1mStep[0m  [10/53], [94mLoss[0m : 2.13474
[1mStep[0m  [15/53], [94mLoss[0m : 2.18912
[1mStep[0m  [20/53], [94mLoss[0m : 2.03103
[1mStep[0m  [25/53], [94mLoss[0m : 2.27711
[1mStep[0m  [30/53], [94mLoss[0m : 2.23106
[1mStep[0m  [35/53], [94mLoss[0m : 2.31037
[1mStep[0m  [40/53], [94mLoss[0m : 2.37329
[1mStep[0m  [45/53], [94mLoss[0m : 2.25817
[1mStep[0m  [50/53], [94mLoss[0m : 2.30616

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.255, [92mTest[0m: 2.551, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.27014
[1mStep[0m  [5/53], [94mLoss[0m : 2.22650
[1mStep[0m  [10/53], [94mLoss[0m : 2.41024
[1mStep[0m  [15/53], [94mLoss[0m : 2.27988
[1mStep[0m  [20/53], [94mLoss[0m : 2.16247
[1mStep[0m  [25/53], [94mLoss[0m : 2.18649
[1mStep[0m  [30/53], [94mLoss[0m : 2.08989
[1mStep[0m  [35/53], [94mLoss[0m : 2.12549
[1mStep[0m  [40/53], [94mLoss[0m : 2.11109
[1mStep[0m  [45/53], [94mLoss[0m : 2.28317
[1mStep[0m  [50/53], [94mLoss[0m : 2.29289

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.216, [92mTest[0m: 2.669, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.26057
[1mStep[0m  [5/53], [94mLoss[0m : 2.12433
[1mStep[0m  [10/53], [94mLoss[0m : 2.13384
[1mStep[0m  [15/53], [94mLoss[0m : 2.15205
[1mStep[0m  [20/53], [94mLoss[0m : 1.91102
[1mStep[0m  [25/53], [94mLoss[0m : 2.03632
[1mStep[0m  [30/53], [94mLoss[0m : 2.08834
[1mStep[0m  [35/53], [94mLoss[0m : 2.09231
[1mStep[0m  [40/53], [94mLoss[0m : 2.20492
[1mStep[0m  [45/53], [94mLoss[0m : 2.23340
[1mStep[0m  [50/53], [94mLoss[0m : 2.10622

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.151, [92mTest[0m: 2.565, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.06969
[1mStep[0m  [5/53], [94mLoss[0m : 2.00733
[1mStep[0m  [10/53], [94mLoss[0m : 1.97667
[1mStep[0m  [15/53], [94mLoss[0m : 2.29928
[1mStep[0m  [20/53], [94mLoss[0m : 1.92793
[1mStep[0m  [25/53], [94mLoss[0m : 2.00133
[1mStep[0m  [30/53], [94mLoss[0m : 2.06963
[1mStep[0m  [35/53], [94mLoss[0m : 2.23803
[1mStep[0m  [40/53], [94mLoss[0m : 1.98866
[1mStep[0m  [45/53], [94mLoss[0m : 2.24919
[1mStep[0m  [50/53], [94mLoss[0m : 2.11718

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.092, [92mTest[0m: 2.521, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.35721
[1mStep[0m  [5/53], [94mLoss[0m : 1.94160
[1mStep[0m  [10/53], [94mLoss[0m : 2.06530
[1mStep[0m  [15/53], [94mLoss[0m : 1.94507
[1mStep[0m  [20/53], [94mLoss[0m : 2.08253
[1mStep[0m  [25/53], [94mLoss[0m : 1.98814
[1mStep[0m  [30/53], [94mLoss[0m : 2.07745
[1mStep[0m  [35/53], [94mLoss[0m : 2.05026
[1mStep[0m  [40/53], [94mLoss[0m : 2.34046
[1mStep[0m  [45/53], [94mLoss[0m : 1.97714
[1mStep[0m  [50/53], [94mLoss[0m : 2.06029

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.043, [92mTest[0m: 2.449, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.93139
[1mStep[0m  [5/53], [94mLoss[0m : 2.02099
[1mStep[0m  [10/53], [94mLoss[0m : 2.02299
[1mStep[0m  [15/53], [94mLoss[0m : 2.16927
[1mStep[0m  [20/53], [94mLoss[0m : 1.71324
[1mStep[0m  [25/53], [94mLoss[0m : 2.06873
[1mStep[0m  [30/53], [94mLoss[0m : 2.08574
[1mStep[0m  [35/53], [94mLoss[0m : 2.23032
[1mStep[0m  [40/53], [94mLoss[0m : 1.82091
[1mStep[0m  [45/53], [94mLoss[0m : 1.99741
[1mStep[0m  [50/53], [94mLoss[0m : 1.95393

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.988, [92mTest[0m: 2.400, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.94235
[1mStep[0m  [5/53], [94mLoss[0m : 1.83191
[1mStep[0m  [10/53], [94mLoss[0m : 1.72010
[1mStep[0m  [15/53], [94mLoss[0m : 1.78282
[1mStep[0m  [20/53], [94mLoss[0m : 2.13286
[1mStep[0m  [25/53], [94mLoss[0m : 1.98060
[1mStep[0m  [30/53], [94mLoss[0m : 2.01975
[1mStep[0m  [35/53], [94mLoss[0m : 1.90592
[1mStep[0m  [40/53], [94mLoss[0m : 1.93382
[1mStep[0m  [45/53], [94mLoss[0m : 1.85996
[1mStep[0m  [50/53], [94mLoss[0m : 1.96326

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.943, [92mTest[0m: 2.415, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.02209
[1mStep[0m  [5/53], [94mLoss[0m : 1.76011
[1mStep[0m  [10/53], [94mLoss[0m : 2.04636
[1mStep[0m  [15/53], [94mLoss[0m : 1.91132
[1mStep[0m  [20/53], [94mLoss[0m : 1.92018
[1mStep[0m  [25/53], [94mLoss[0m : 2.00369
[1mStep[0m  [30/53], [94mLoss[0m : 1.94450
[1mStep[0m  [35/53], [94mLoss[0m : 2.06954
[1mStep[0m  [40/53], [94mLoss[0m : 1.80987
[1mStep[0m  [45/53], [94mLoss[0m : 1.88458
[1mStep[0m  [50/53], [94mLoss[0m : 1.95883

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.897, [92mTest[0m: 2.457, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.67339
[1mStep[0m  [5/53], [94mLoss[0m : 1.85786
[1mStep[0m  [10/53], [94mLoss[0m : 1.89214
[1mStep[0m  [15/53], [94mLoss[0m : 1.85579
[1mStep[0m  [20/53], [94mLoss[0m : 1.72029
[1mStep[0m  [25/53], [94mLoss[0m : 1.77642
[1mStep[0m  [30/53], [94mLoss[0m : 2.04159
[1mStep[0m  [35/53], [94mLoss[0m : 1.70758
[1mStep[0m  [40/53], [94mLoss[0m : 1.79812
[1mStep[0m  [45/53], [94mLoss[0m : 1.92963
[1mStep[0m  [50/53], [94mLoss[0m : 1.93857

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.855, [92mTest[0m: 2.493, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.71912
[1mStep[0m  [5/53], [94mLoss[0m : 1.74089
[1mStep[0m  [10/53], [94mLoss[0m : 1.75859
[1mStep[0m  [15/53], [94mLoss[0m : 1.84215
[1mStep[0m  [20/53], [94mLoss[0m : 1.89576
[1mStep[0m  [25/53], [94mLoss[0m : 1.75327
[1mStep[0m  [30/53], [94mLoss[0m : 1.84384
[1mStep[0m  [35/53], [94mLoss[0m : 1.70845
[1mStep[0m  [40/53], [94mLoss[0m : 1.84772
[1mStep[0m  [45/53], [94mLoss[0m : 1.77018
[1mStep[0m  [50/53], [94mLoss[0m : 1.99208

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.837, [92mTest[0m: 2.488, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.73643
[1mStep[0m  [5/53], [94mLoss[0m : 1.68522
[1mStep[0m  [10/53], [94mLoss[0m : 1.65168
[1mStep[0m  [15/53], [94mLoss[0m : 1.65451
[1mStep[0m  [20/53], [94mLoss[0m : 2.04683
[1mStep[0m  [25/53], [94mLoss[0m : 1.75431
[1mStep[0m  [30/53], [94mLoss[0m : 1.81460
[1mStep[0m  [35/53], [94mLoss[0m : 1.94125
[1mStep[0m  [40/53], [94mLoss[0m : 1.86384
[1mStep[0m  [45/53], [94mLoss[0m : 1.78216
[1mStep[0m  [50/53], [94mLoss[0m : 1.73375

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.796, [92mTest[0m: 2.428, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.72096
[1mStep[0m  [5/53], [94mLoss[0m : 1.77425
[1mStep[0m  [10/53], [94mLoss[0m : 1.70993
[1mStep[0m  [15/53], [94mLoss[0m : 1.74293
[1mStep[0m  [20/53], [94mLoss[0m : 1.77776
[1mStep[0m  [25/53], [94mLoss[0m : 1.75349
[1mStep[0m  [30/53], [94mLoss[0m : 1.58042
[1mStep[0m  [35/53], [94mLoss[0m : 1.76494
[1mStep[0m  [40/53], [94mLoss[0m : 1.88226
[1mStep[0m  [45/53], [94mLoss[0m : 1.76846
[1mStep[0m  [50/53], [94mLoss[0m : 1.73364

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.744, [92mTest[0m: 2.435, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.71395
[1mStep[0m  [5/53], [94mLoss[0m : 1.66698
[1mStep[0m  [10/53], [94mLoss[0m : 1.58486
[1mStep[0m  [15/53], [94mLoss[0m : 1.63259
[1mStep[0m  [20/53], [94mLoss[0m : 1.72575
[1mStep[0m  [25/53], [94mLoss[0m : 1.65965
[1mStep[0m  [30/53], [94mLoss[0m : 1.68931
[1mStep[0m  [35/53], [94mLoss[0m : 2.11544
[1mStep[0m  [40/53], [94mLoss[0m : 1.77225
[1mStep[0m  [45/53], [94mLoss[0m : 1.68526
[1mStep[0m  [50/53], [94mLoss[0m : 1.65503

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.717, [92mTest[0m: 2.430, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.56959
[1mStep[0m  [5/53], [94mLoss[0m : 1.66077
[1mStep[0m  [10/53], [94mLoss[0m : 1.64480
[1mStep[0m  [15/53], [94mLoss[0m : 1.73179
[1mStep[0m  [20/53], [94mLoss[0m : 1.61698
[1mStep[0m  [25/53], [94mLoss[0m : 1.70145
[1mStep[0m  [30/53], [94mLoss[0m : 1.74812
[1mStep[0m  [35/53], [94mLoss[0m : 1.63048
[1mStep[0m  [40/53], [94mLoss[0m : 1.67252
[1mStep[0m  [45/53], [94mLoss[0m : 1.77868
[1mStep[0m  [50/53], [94mLoss[0m : 1.67784

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.684, [92mTest[0m: 2.533, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.67754
[1mStep[0m  [5/53], [94mLoss[0m : 1.58601
[1mStep[0m  [10/53], [94mLoss[0m : 1.58942
[1mStep[0m  [15/53], [94mLoss[0m : 1.44754
[1mStep[0m  [20/53], [94mLoss[0m : 1.73002
[1mStep[0m  [25/53], [94mLoss[0m : 1.66822
[1mStep[0m  [30/53], [94mLoss[0m : 1.64432
[1mStep[0m  [35/53], [94mLoss[0m : 1.60046
[1mStep[0m  [40/53], [94mLoss[0m : 2.02717
[1mStep[0m  [45/53], [94mLoss[0m : 1.62017
[1mStep[0m  [50/53], [94mLoss[0m : 1.86008

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.647, [92mTest[0m: 2.477, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.55764
[1mStep[0m  [5/53], [94mLoss[0m : 1.43503
[1mStep[0m  [10/53], [94mLoss[0m : 1.70080
[1mStep[0m  [15/53], [94mLoss[0m : 1.63924
[1mStep[0m  [20/53], [94mLoss[0m : 1.52539
[1mStep[0m  [25/53], [94mLoss[0m : 1.40131
[1mStep[0m  [30/53], [94mLoss[0m : 1.60690
[1mStep[0m  [35/53], [94mLoss[0m : 1.57159
[1mStep[0m  [40/53], [94mLoss[0m : 1.62124
[1mStep[0m  [45/53], [94mLoss[0m : 1.50396
[1mStep[0m  [50/53], [94mLoss[0m : 1.62259

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.623, [92mTest[0m: 2.517, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.46769
[1mStep[0m  [5/53], [94mLoss[0m : 1.73421
[1mStep[0m  [10/53], [94mLoss[0m : 1.57389
[1mStep[0m  [15/53], [94mLoss[0m : 1.56797
[1mStep[0m  [20/53], [94mLoss[0m : 1.54732
[1mStep[0m  [25/53], [94mLoss[0m : 1.63547
[1mStep[0m  [30/53], [94mLoss[0m : 1.61982
[1mStep[0m  [35/53], [94mLoss[0m : 1.65504
[1mStep[0m  [40/53], [94mLoss[0m : 1.45036
[1mStep[0m  [45/53], [94mLoss[0m : 1.58597
[1mStep[0m  [50/53], [94mLoss[0m : 1.73045

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.592, [92mTest[0m: 2.478, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.65108
[1mStep[0m  [5/53], [94mLoss[0m : 1.71063
[1mStep[0m  [10/53], [94mLoss[0m : 1.58694
[1mStep[0m  [15/53], [94mLoss[0m : 1.56618
[1mStep[0m  [20/53], [94mLoss[0m : 1.65495
[1mStep[0m  [25/53], [94mLoss[0m : 1.44949
[1mStep[0m  [30/53], [94mLoss[0m : 1.54836
[1mStep[0m  [35/53], [94mLoss[0m : 1.47928
[1mStep[0m  [40/53], [94mLoss[0m : 1.59311
[1mStep[0m  [45/53], [94mLoss[0m : 1.71116
[1mStep[0m  [50/53], [94mLoss[0m : 1.69998

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.563, [92mTest[0m: 2.551, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.48173
[1mStep[0m  [5/53], [94mLoss[0m : 1.59224
[1mStep[0m  [10/53], [94mLoss[0m : 1.45459
[1mStep[0m  [15/53], [94mLoss[0m : 1.55089
[1mStep[0m  [20/53], [94mLoss[0m : 1.46124
[1mStep[0m  [25/53], [94mLoss[0m : 1.54483
[1mStep[0m  [30/53], [94mLoss[0m : 1.52209
[1mStep[0m  [35/53], [94mLoss[0m : 1.44482
[1mStep[0m  [40/53], [94mLoss[0m : 1.47876
[1mStep[0m  [45/53], [94mLoss[0m : 1.63043
[1mStep[0m  [50/53], [94mLoss[0m : 1.59359

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.517, [92mTest[0m: 2.499, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.46726
[1mStep[0m  [5/53], [94mLoss[0m : 1.45570
[1mStep[0m  [10/53], [94mLoss[0m : 1.59748
[1mStep[0m  [15/53], [94mLoss[0m : 1.55495
[1mStep[0m  [20/53], [94mLoss[0m : 1.42561
[1mStep[0m  [25/53], [94mLoss[0m : 1.52607
[1mStep[0m  [30/53], [94mLoss[0m : 1.35679
[1mStep[0m  [35/53], [94mLoss[0m : 1.56179
[1mStep[0m  [40/53], [94mLoss[0m : 1.51335
[1mStep[0m  [45/53], [94mLoss[0m : 1.48239
[1mStep[0m  [50/53], [94mLoss[0m : 1.57521

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.512, [92mTest[0m: 2.545, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.43060
[1mStep[0m  [5/53], [94mLoss[0m : 1.48411
[1mStep[0m  [10/53], [94mLoss[0m : 1.35730
[1mStep[0m  [15/53], [94mLoss[0m : 1.48203
[1mStep[0m  [20/53], [94mLoss[0m : 1.40193
[1mStep[0m  [25/53], [94mLoss[0m : 1.43807
[1mStep[0m  [30/53], [94mLoss[0m : 1.55446
[1mStep[0m  [35/53], [94mLoss[0m : 1.60685
[1mStep[0m  [40/53], [94mLoss[0m : 1.51600
[1mStep[0m  [45/53], [94mLoss[0m : 1.56537
[1mStep[0m  [50/53], [94mLoss[0m : 1.45879

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.479, [92mTest[0m: 2.493, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.41149
[1mStep[0m  [5/53], [94mLoss[0m : 1.43853
[1mStep[0m  [10/53], [94mLoss[0m : 1.54062
[1mStep[0m  [15/53], [94mLoss[0m : 1.33012
[1mStep[0m  [20/53], [94mLoss[0m : 1.52193
[1mStep[0m  [25/53], [94mLoss[0m : 1.37809
[1mStep[0m  [30/53], [94mLoss[0m : 1.39258
[1mStep[0m  [35/53], [94mLoss[0m : 1.29395
[1mStep[0m  [40/53], [94mLoss[0m : 1.52841
[1mStep[0m  [45/53], [94mLoss[0m : 1.43273
[1mStep[0m  [50/53], [94mLoss[0m : 1.44339

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.458, [92mTest[0m: 2.549, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.40755
[1mStep[0m  [5/53], [94mLoss[0m : 1.31445
[1mStep[0m  [10/53], [94mLoss[0m : 1.52852
[1mStep[0m  [15/53], [94mLoss[0m : 1.51040
[1mStep[0m  [20/53], [94mLoss[0m : 1.39606
[1mStep[0m  [25/53], [94mLoss[0m : 1.46422
[1mStep[0m  [30/53], [94mLoss[0m : 1.44095
[1mStep[0m  [35/53], [94mLoss[0m : 1.50716
[1mStep[0m  [40/53], [94mLoss[0m : 1.40158
[1mStep[0m  [45/53], [94mLoss[0m : 1.45542
[1mStep[0m  [50/53], [94mLoss[0m : 1.46340

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.440, [92mTest[0m: 2.490, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.43916
[1mStep[0m  [5/53], [94mLoss[0m : 1.38241
[1mStep[0m  [10/53], [94mLoss[0m : 1.36764
[1mStep[0m  [15/53], [94mLoss[0m : 1.31873
[1mStep[0m  [20/53], [94mLoss[0m : 1.53202
[1mStep[0m  [25/53], [94mLoss[0m : 1.45646
[1mStep[0m  [30/53], [94mLoss[0m : 1.40729
[1mStep[0m  [35/53], [94mLoss[0m : 1.46571
[1mStep[0m  [40/53], [94mLoss[0m : 1.54129
[1mStep[0m  [45/53], [94mLoss[0m : 1.39241
[1mStep[0m  [50/53], [94mLoss[0m : 1.35939

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.417, [92mTest[0m: 2.489, [96mlr[0m: 0.0023175
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 26 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.483
====================================

Phase 2 - Evaluation MAE:  2.4827998601473293
MAE score P1        2.338915
MAE score P2          2.4828
loss                1.416945
learning_rate       0.002575
batch_size               256
hidden_sizes      [300, 100]
epochs                    30
activation              relu
optimizer                sgd
early stopping          True
dropout                  0.2
momentum                 0.9
weight_decay           0.001
Name: 0, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/26], [94mLoss[0m : 10.62031
[1mStep[0m  [2/26], [94mLoss[0m : 10.44573
[1mStep[0m  [4/26], [94mLoss[0m : 10.00120
[1mStep[0m  [6/26], [94mLoss[0m : 9.75002
[1mStep[0m  [8/26], [94mLoss[0m : 9.54204
[1mStep[0m  [10/26], [94mLoss[0m : 9.38039
[1mStep[0m  [12/26], [94mLoss[0m : 8.88226
[1mStep[0m  [14/26], [94mLoss[0m : 8.87772
[1mStep[0m  [16/26], [94mLoss[0m : 8.56117
[1mStep[0m  [18/26], [94mLoss[0m : 8.60209
[1mStep[0m  [20/26], [94mLoss[0m : 8.04951
[1mStep[0m  [22/26], [94mLoss[0m : 7.73622
[1mStep[0m  [24/26], [94mLoss[0m : 7.19516

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 8.970, [92mTest[0m: 10.871, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 7.37559
[1mStep[0m  [2/26], [94mLoss[0m : 6.98410
[1mStep[0m  [4/26], [94mLoss[0m : 6.56712
[1mStep[0m  [6/26], [94mLoss[0m : 6.40901
[1mStep[0m  [8/26], [94mLoss[0m : 6.08510
[1mStep[0m  [10/26], [94mLoss[0m : 6.11029
[1mStep[0m  [12/26], [94mLoss[0m : 6.16721
[1mStep[0m  [14/26], [94mLoss[0m : 6.09885
[1mStep[0m  [16/26], [94mLoss[0m : 5.57054
[1mStep[0m  [18/26], [94mLoss[0m : 5.28854
[1mStep[0m  [20/26], [94mLoss[0m : 5.36169
[1mStep[0m  [22/26], [94mLoss[0m : 5.04262
[1mStep[0m  [24/26], [94mLoss[0m : 5.06868

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 5.981, [92mTest[0m: 8.566, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 4.62005
[1mStep[0m  [2/26], [94mLoss[0m : 4.35279
[1mStep[0m  [4/26], [94mLoss[0m : 4.71093
[1mStep[0m  [6/26], [94mLoss[0m : 4.62844
[1mStep[0m  [8/26], [94mLoss[0m : 4.42139
[1mStep[0m  [10/26], [94mLoss[0m : 4.03299
[1mStep[0m  [12/26], [94mLoss[0m : 3.91116
[1mStep[0m  [14/26], [94mLoss[0m : 3.91041
[1mStep[0m  [16/26], [94mLoss[0m : 3.87216
[1mStep[0m  [18/26], [94mLoss[0m : 3.68698
[1mStep[0m  [20/26], [94mLoss[0m : 3.59908
[1mStep[0m  [22/26], [94mLoss[0m : 3.75817
[1mStep[0m  [24/26], [94mLoss[0m : 3.50777

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 4.051, [92mTest[0m: 5.784, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.52162
[1mStep[0m  [2/26], [94mLoss[0m : 3.34198
[1mStep[0m  [4/26], [94mLoss[0m : 3.12096
[1mStep[0m  [6/26], [94mLoss[0m : 3.16550
[1mStep[0m  [8/26], [94mLoss[0m : 3.17823
[1mStep[0m  [10/26], [94mLoss[0m : 3.30009
[1mStep[0m  [12/26], [94mLoss[0m : 3.22287
[1mStep[0m  [14/26], [94mLoss[0m : 3.15736
[1mStep[0m  [16/26], [94mLoss[0m : 2.89012
[1mStep[0m  [18/26], [94mLoss[0m : 2.80307
[1mStep[0m  [20/26], [94mLoss[0m : 3.07560
[1mStep[0m  [22/26], [94mLoss[0m : 3.10773
[1mStep[0m  [24/26], [94mLoss[0m : 2.77522

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 3.088, [92mTest[0m: 4.060, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.96507
[1mStep[0m  [2/26], [94mLoss[0m : 2.87278
[1mStep[0m  [4/26], [94mLoss[0m : 2.80896
[1mStep[0m  [6/26], [94mLoss[0m : 2.78019
[1mStep[0m  [8/26], [94mLoss[0m : 2.88385
[1mStep[0m  [10/26], [94mLoss[0m : 2.78233
[1mStep[0m  [12/26], [94mLoss[0m : 2.75744
[1mStep[0m  [14/26], [94mLoss[0m : 3.06041
[1mStep[0m  [16/26], [94mLoss[0m : 2.82124
[1mStep[0m  [18/26], [94mLoss[0m : 2.65258
[1mStep[0m  [20/26], [94mLoss[0m : 2.70339
[1mStep[0m  [22/26], [94mLoss[0m : 2.70841
[1mStep[0m  [24/26], [94mLoss[0m : 2.78893

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.777, [92mTest[0m: 3.250, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.76086
[1mStep[0m  [2/26], [94mLoss[0m : 2.77279
[1mStep[0m  [4/26], [94mLoss[0m : 2.72441
[1mStep[0m  [6/26], [94mLoss[0m : 2.50968
[1mStep[0m  [8/26], [94mLoss[0m : 2.58313
[1mStep[0m  [10/26], [94mLoss[0m : 2.70298
[1mStep[0m  [12/26], [94mLoss[0m : 2.57608
[1mStep[0m  [14/26], [94mLoss[0m : 2.70275
[1mStep[0m  [16/26], [94mLoss[0m : 2.44312
[1mStep[0m  [18/26], [94mLoss[0m : 2.58899
[1mStep[0m  [20/26], [94mLoss[0m : 2.71910
[1mStep[0m  [22/26], [94mLoss[0m : 2.77227
[1mStep[0m  [24/26], [94mLoss[0m : 2.79919

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.683, [92mTest[0m: 2.978, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.59288
[1mStep[0m  [2/26], [94mLoss[0m : 2.54385
[1mStep[0m  [4/26], [94mLoss[0m : 2.53561
[1mStep[0m  [6/26], [94mLoss[0m : 2.64067
[1mStep[0m  [8/26], [94mLoss[0m : 2.60664
[1mStep[0m  [10/26], [94mLoss[0m : 2.69497
[1mStep[0m  [12/26], [94mLoss[0m : 2.71822
[1mStep[0m  [14/26], [94mLoss[0m : 2.66343
[1mStep[0m  [16/26], [94mLoss[0m : 2.67427
[1mStep[0m  [18/26], [94mLoss[0m : 2.63721
[1mStep[0m  [20/26], [94mLoss[0m : 2.66041
[1mStep[0m  [22/26], [94mLoss[0m : 2.73256
[1mStep[0m  [24/26], [94mLoss[0m : 2.55776

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.673, [92mTest[0m: 2.871, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.64486
[1mStep[0m  [2/26], [94mLoss[0m : 2.64939
[1mStep[0m  [4/26], [94mLoss[0m : 2.61569
[1mStep[0m  [6/26], [94mLoss[0m : 2.70883
[1mStep[0m  [8/26], [94mLoss[0m : 2.63715
[1mStep[0m  [10/26], [94mLoss[0m : 2.63272
[1mStep[0m  [12/26], [94mLoss[0m : 2.78000
[1mStep[0m  [14/26], [94mLoss[0m : 2.68585
[1mStep[0m  [16/26], [94mLoss[0m : 2.40371
[1mStep[0m  [18/26], [94mLoss[0m : 2.66251
[1mStep[0m  [20/26], [94mLoss[0m : 2.63224
[1mStep[0m  [22/26], [94mLoss[0m : 2.60626
[1mStep[0m  [24/26], [94mLoss[0m : 2.55507

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.638, [92mTest[0m: 2.816, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.46641
[1mStep[0m  [2/26], [94mLoss[0m : 2.59000
[1mStep[0m  [4/26], [94mLoss[0m : 2.48837
[1mStep[0m  [6/26], [94mLoss[0m : 2.83309
[1mStep[0m  [8/26], [94mLoss[0m : 2.69648
[1mStep[0m  [10/26], [94mLoss[0m : 2.60009
[1mStep[0m  [12/26], [94mLoss[0m : 2.74357
[1mStep[0m  [14/26], [94mLoss[0m : 2.75102
[1mStep[0m  [16/26], [94mLoss[0m : 2.46377
[1mStep[0m  [18/26], [94mLoss[0m : 2.66495
[1mStep[0m  [20/26], [94mLoss[0m : 2.55468
[1mStep[0m  [22/26], [94mLoss[0m : 2.61987
[1mStep[0m  [24/26], [94mLoss[0m : 2.60629

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.618, [92mTest[0m: 2.785, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.62205
[1mStep[0m  [2/26], [94mLoss[0m : 2.66182
[1mStep[0m  [4/26], [94mLoss[0m : 2.71567
[1mStep[0m  [6/26], [94mLoss[0m : 2.72310
[1mStep[0m  [8/26], [94mLoss[0m : 2.75189
[1mStep[0m  [10/26], [94mLoss[0m : 2.60181
[1mStep[0m  [12/26], [94mLoss[0m : 2.59213
[1mStep[0m  [14/26], [94mLoss[0m : 2.55402
[1mStep[0m  [16/26], [94mLoss[0m : 2.59518
[1mStep[0m  [18/26], [94mLoss[0m : 2.56816
[1mStep[0m  [20/26], [94mLoss[0m : 2.47866
[1mStep[0m  [22/26], [94mLoss[0m : 2.37574
[1mStep[0m  [24/26], [94mLoss[0m : 2.67885

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.613, [92mTest[0m: 2.749, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.54537
[1mStep[0m  [2/26], [94mLoss[0m : 2.68816
[1mStep[0m  [4/26], [94mLoss[0m : 2.50746
[1mStep[0m  [6/26], [94mLoss[0m : 2.61474
[1mStep[0m  [8/26], [94mLoss[0m : 2.58467
[1mStep[0m  [10/26], [94mLoss[0m : 2.55576
[1mStep[0m  [12/26], [94mLoss[0m : 2.52147
[1mStep[0m  [14/26], [94mLoss[0m : 2.54260
[1mStep[0m  [16/26], [94mLoss[0m : 2.54874
[1mStep[0m  [18/26], [94mLoss[0m : 2.72164
[1mStep[0m  [20/26], [94mLoss[0m : 2.61860
[1mStep[0m  [22/26], [94mLoss[0m : 2.55215
[1mStep[0m  [24/26], [94mLoss[0m : 2.68807

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.606, [92mTest[0m: 2.736, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.51972
[1mStep[0m  [2/26], [94mLoss[0m : 2.49324
[1mStep[0m  [4/26], [94mLoss[0m : 2.58082
[1mStep[0m  [6/26], [94mLoss[0m : 2.65168
[1mStep[0m  [8/26], [94mLoss[0m : 2.54570
[1mStep[0m  [10/26], [94mLoss[0m : 2.62569
[1mStep[0m  [12/26], [94mLoss[0m : 2.59930
[1mStep[0m  [14/26], [94mLoss[0m : 2.50214
[1mStep[0m  [16/26], [94mLoss[0m : 2.58795
[1mStep[0m  [18/26], [94mLoss[0m : 2.63371
[1mStep[0m  [20/26], [94mLoss[0m : 2.61141
[1mStep[0m  [22/26], [94mLoss[0m : 2.55251
[1mStep[0m  [24/26], [94mLoss[0m : 2.60703

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.583, [92mTest[0m: 2.725, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.68071
[1mStep[0m  [2/26], [94mLoss[0m : 2.49851
[1mStep[0m  [4/26], [94mLoss[0m : 2.54530
[1mStep[0m  [6/26], [94mLoss[0m : 2.59142
[1mStep[0m  [8/26], [94mLoss[0m : 2.40042
[1mStep[0m  [10/26], [94mLoss[0m : 2.59351
[1mStep[0m  [12/26], [94mLoss[0m : 2.65213
[1mStep[0m  [14/26], [94mLoss[0m : 2.51299
[1mStep[0m  [16/26], [94mLoss[0m : 2.70529
[1mStep[0m  [18/26], [94mLoss[0m : 2.54610
[1mStep[0m  [20/26], [94mLoss[0m : 2.60165
[1mStep[0m  [22/26], [94mLoss[0m : 2.64625
[1mStep[0m  [24/26], [94mLoss[0m : 2.60983

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.578, [92mTest[0m: 2.712, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.44469
[1mStep[0m  [2/26], [94mLoss[0m : 2.65821
[1mStep[0m  [4/26], [94mLoss[0m : 2.49365
[1mStep[0m  [6/26], [94mLoss[0m : 2.56919
[1mStep[0m  [8/26], [94mLoss[0m : 2.67886
[1mStep[0m  [10/26], [94mLoss[0m : 2.55333
[1mStep[0m  [12/26], [94mLoss[0m : 2.57351
[1mStep[0m  [14/26], [94mLoss[0m : 2.64807
[1mStep[0m  [16/26], [94mLoss[0m : 2.54653
[1mStep[0m  [18/26], [94mLoss[0m : 2.43813
[1mStep[0m  [20/26], [94mLoss[0m : 2.55267
[1mStep[0m  [22/26], [94mLoss[0m : 2.60405
[1mStep[0m  [24/26], [94mLoss[0m : 2.54590

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.585, [92mTest[0m: 2.706, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.66434
[1mStep[0m  [2/26], [94mLoss[0m : 2.49392
[1mStep[0m  [4/26], [94mLoss[0m : 2.61381
[1mStep[0m  [6/26], [94mLoss[0m : 2.49670
[1mStep[0m  [8/26], [94mLoss[0m : 2.64745
[1mStep[0m  [10/26], [94mLoss[0m : 2.50064
[1mStep[0m  [12/26], [94mLoss[0m : 2.46672
[1mStep[0m  [14/26], [94mLoss[0m : 2.63159
[1mStep[0m  [16/26], [94mLoss[0m : 2.47282
[1mStep[0m  [18/26], [94mLoss[0m : 2.49423
[1mStep[0m  [20/26], [94mLoss[0m : 2.75346
[1mStep[0m  [22/26], [94mLoss[0m : 2.58102
[1mStep[0m  [24/26], [94mLoss[0m : 2.57128

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.561, [92mTest[0m: 2.699, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.47359
[1mStep[0m  [2/26], [94mLoss[0m : 2.53962
[1mStep[0m  [4/26], [94mLoss[0m : 2.50030
[1mStep[0m  [6/26], [94mLoss[0m : 2.69277
[1mStep[0m  [8/26], [94mLoss[0m : 2.47579
[1mStep[0m  [10/26], [94mLoss[0m : 2.67152
[1mStep[0m  [12/26], [94mLoss[0m : 2.54296
[1mStep[0m  [14/26], [94mLoss[0m : 2.64804
[1mStep[0m  [16/26], [94mLoss[0m : 2.56655
[1mStep[0m  [18/26], [94mLoss[0m : 2.56577
[1mStep[0m  [20/26], [94mLoss[0m : 2.54287
[1mStep[0m  [22/26], [94mLoss[0m : 2.53694
[1mStep[0m  [24/26], [94mLoss[0m : 2.49279

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.566, [92mTest[0m: 2.684, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.59985
[1mStep[0m  [2/26], [94mLoss[0m : 2.52698
[1mStep[0m  [4/26], [94mLoss[0m : 2.67062
[1mStep[0m  [6/26], [94mLoss[0m : 2.55838
[1mStep[0m  [8/26], [94mLoss[0m : 2.45928
[1mStep[0m  [10/26], [94mLoss[0m : 2.59630
[1mStep[0m  [12/26], [94mLoss[0m : 2.55612
[1mStep[0m  [14/26], [94mLoss[0m : 2.53131
[1mStep[0m  [16/26], [94mLoss[0m : 2.54356
[1mStep[0m  [18/26], [94mLoss[0m : 2.47818
[1mStep[0m  [20/26], [94mLoss[0m : 2.50549
[1mStep[0m  [22/26], [94mLoss[0m : 2.55930
[1mStep[0m  [24/26], [94mLoss[0m : 2.72483

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.562, [92mTest[0m: 2.678, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.67120
[1mStep[0m  [2/26], [94mLoss[0m : 2.64891
[1mStep[0m  [4/26], [94mLoss[0m : 2.63903
[1mStep[0m  [6/26], [94mLoss[0m : 2.64081
[1mStep[0m  [8/26], [94mLoss[0m : 2.50607
[1mStep[0m  [10/26], [94mLoss[0m : 2.63432
[1mStep[0m  [12/26], [94mLoss[0m : 2.65225
[1mStep[0m  [14/26], [94mLoss[0m : 2.52385
[1mStep[0m  [16/26], [94mLoss[0m : 2.38448
[1mStep[0m  [18/26], [94mLoss[0m : 2.42944
[1mStep[0m  [20/26], [94mLoss[0m : 2.51237
[1mStep[0m  [22/26], [94mLoss[0m : 2.48053
[1mStep[0m  [24/26], [94mLoss[0m : 2.55481

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.546, [92mTest[0m: 2.686, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.44642
[1mStep[0m  [2/26], [94mLoss[0m : 2.69315
[1mStep[0m  [4/26], [94mLoss[0m : 2.65522
[1mStep[0m  [6/26], [94mLoss[0m : 2.56091
[1mStep[0m  [8/26], [94mLoss[0m : 2.60602
[1mStep[0m  [10/26], [94mLoss[0m : 2.58931
[1mStep[0m  [12/26], [94mLoss[0m : 2.63347
[1mStep[0m  [14/26], [94mLoss[0m : 2.45026
[1mStep[0m  [16/26], [94mLoss[0m : 2.57039
[1mStep[0m  [18/26], [94mLoss[0m : 2.62512
[1mStep[0m  [20/26], [94mLoss[0m : 2.61220
[1mStep[0m  [22/26], [94mLoss[0m : 2.53010
[1mStep[0m  [24/26], [94mLoss[0m : 2.60706

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.552, [92mTest[0m: 2.673, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.69711
[1mStep[0m  [2/26], [94mLoss[0m : 2.54651
[1mStep[0m  [4/26], [94mLoss[0m : 2.53950
[1mStep[0m  [6/26], [94mLoss[0m : 2.67575
[1mStep[0m  [8/26], [94mLoss[0m : 2.51626
[1mStep[0m  [10/26], [94mLoss[0m : 2.53273
[1mStep[0m  [12/26], [94mLoss[0m : 2.66993
[1mStep[0m  [14/26], [94mLoss[0m : 2.45956
[1mStep[0m  [16/26], [94mLoss[0m : 2.51863
[1mStep[0m  [18/26], [94mLoss[0m : 2.54450
[1mStep[0m  [20/26], [94mLoss[0m : 2.74120
[1mStep[0m  [22/26], [94mLoss[0m : 2.56897
[1mStep[0m  [24/26], [94mLoss[0m : 2.37940

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.555, [92mTest[0m: 2.665, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.54836
[1mStep[0m  [2/26], [94mLoss[0m : 2.43039
[1mStep[0m  [4/26], [94mLoss[0m : 2.56212
[1mStep[0m  [6/26], [94mLoss[0m : 2.56914
[1mStep[0m  [8/26], [94mLoss[0m : 2.50276
[1mStep[0m  [10/26], [94mLoss[0m : 2.56831
[1mStep[0m  [12/26], [94mLoss[0m : 2.55080
[1mStep[0m  [14/26], [94mLoss[0m : 2.61833
[1mStep[0m  [16/26], [94mLoss[0m : 2.46416
[1mStep[0m  [18/26], [94mLoss[0m : 2.60719
[1mStep[0m  [20/26], [94mLoss[0m : 2.50554
[1mStep[0m  [22/26], [94mLoss[0m : 2.63721
[1mStep[0m  [24/26], [94mLoss[0m : 2.59189

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.535, [92mTest[0m: 2.673, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.33202
[1mStep[0m  [2/26], [94mLoss[0m : 2.50040
[1mStep[0m  [4/26], [94mLoss[0m : 2.64283
[1mStep[0m  [6/26], [94mLoss[0m : 2.47672
[1mStep[0m  [8/26], [94mLoss[0m : 2.63300
[1mStep[0m  [10/26], [94mLoss[0m : 2.53432
[1mStep[0m  [12/26], [94mLoss[0m : 2.63298
[1mStep[0m  [14/26], [94mLoss[0m : 2.60960
[1mStep[0m  [16/26], [94mLoss[0m : 2.46979
[1mStep[0m  [18/26], [94mLoss[0m : 2.56633
[1mStep[0m  [20/26], [94mLoss[0m : 2.57925
[1mStep[0m  [22/26], [94mLoss[0m : 2.51398
[1mStep[0m  [24/26], [94mLoss[0m : 2.56379

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.546, [92mTest[0m: 2.653, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.67622
[1mStep[0m  [2/26], [94mLoss[0m : 2.34140
[1mStep[0m  [4/26], [94mLoss[0m : 2.55409
[1mStep[0m  [6/26], [94mLoss[0m : 2.39872
[1mStep[0m  [8/26], [94mLoss[0m : 2.57961
[1mStep[0m  [10/26], [94mLoss[0m : 2.60894
[1mStep[0m  [12/26], [94mLoss[0m : 2.43695
[1mStep[0m  [14/26], [94mLoss[0m : 2.63463
[1mStep[0m  [16/26], [94mLoss[0m : 2.63082
[1mStep[0m  [18/26], [94mLoss[0m : 2.33667
[1mStep[0m  [20/26], [94mLoss[0m : 2.45506
[1mStep[0m  [22/26], [94mLoss[0m : 2.62557
[1mStep[0m  [24/26], [94mLoss[0m : 2.70499

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.537, [92mTest[0m: 2.666, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.71700
[1mStep[0m  [2/26], [94mLoss[0m : 2.41271
[1mStep[0m  [4/26], [94mLoss[0m : 2.46428
[1mStep[0m  [6/26], [94mLoss[0m : 2.58525
[1mStep[0m  [8/26], [94mLoss[0m : 2.33493
[1mStep[0m  [10/26], [94mLoss[0m : 2.57291
[1mStep[0m  [12/26], [94mLoss[0m : 2.65844
[1mStep[0m  [14/26], [94mLoss[0m : 2.62842
[1mStep[0m  [16/26], [94mLoss[0m : 2.44518
[1mStep[0m  [18/26], [94mLoss[0m : 2.44661
[1mStep[0m  [20/26], [94mLoss[0m : 2.39802
[1mStep[0m  [22/26], [94mLoss[0m : 2.46771
[1mStep[0m  [24/26], [94mLoss[0m : 2.68604

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.649, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.50651
[1mStep[0m  [2/26], [94mLoss[0m : 2.37594
[1mStep[0m  [4/26], [94mLoss[0m : 2.65047
[1mStep[0m  [6/26], [94mLoss[0m : 2.61374
[1mStep[0m  [8/26], [94mLoss[0m : 2.34985
[1mStep[0m  [10/26], [94mLoss[0m : 2.47250
[1mStep[0m  [12/26], [94mLoss[0m : 2.57489
[1mStep[0m  [14/26], [94mLoss[0m : 2.62732
[1mStep[0m  [16/26], [94mLoss[0m : 2.39056
[1mStep[0m  [18/26], [94mLoss[0m : 2.39074
[1mStep[0m  [20/26], [94mLoss[0m : 2.74191
[1mStep[0m  [22/26], [94mLoss[0m : 2.44350
[1mStep[0m  [24/26], [94mLoss[0m : 2.46109

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.656, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.53931
[1mStep[0m  [2/26], [94mLoss[0m : 2.42872
[1mStep[0m  [4/26], [94mLoss[0m : 2.40687
[1mStep[0m  [6/26], [94mLoss[0m : 2.68074
[1mStep[0m  [8/26], [94mLoss[0m : 2.50010
[1mStep[0m  [10/26], [94mLoss[0m : 2.56302
[1mStep[0m  [12/26], [94mLoss[0m : 2.54345
[1mStep[0m  [14/26], [94mLoss[0m : 2.61416
[1mStep[0m  [16/26], [94mLoss[0m : 2.38610
[1mStep[0m  [18/26], [94mLoss[0m : 2.42499
[1mStep[0m  [20/26], [94mLoss[0m : 2.55040
[1mStep[0m  [22/26], [94mLoss[0m : 2.67957
[1mStep[0m  [24/26], [94mLoss[0m : 2.52129

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.514, [92mTest[0m: 2.646, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.65283
[1mStep[0m  [2/26], [94mLoss[0m : 2.64646
[1mStep[0m  [4/26], [94mLoss[0m : 2.55407
[1mStep[0m  [6/26], [94mLoss[0m : 2.39328
[1mStep[0m  [8/26], [94mLoss[0m : 2.55734
[1mStep[0m  [10/26], [94mLoss[0m : 2.38091
[1mStep[0m  [12/26], [94mLoss[0m : 2.55300
[1mStep[0m  [14/26], [94mLoss[0m : 2.37549
[1mStep[0m  [16/26], [94mLoss[0m : 2.53602
[1mStep[0m  [18/26], [94mLoss[0m : 2.38076
[1mStep[0m  [20/26], [94mLoss[0m : 2.59306
[1mStep[0m  [22/26], [94mLoss[0m : 2.47140
[1mStep[0m  [24/26], [94mLoss[0m : 2.50869

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.629, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.54420
[1mStep[0m  [2/26], [94mLoss[0m : 2.54139
[1mStep[0m  [4/26], [94mLoss[0m : 2.53590
[1mStep[0m  [6/26], [94mLoss[0m : 2.46499
[1mStep[0m  [8/26], [94mLoss[0m : 2.48608
[1mStep[0m  [10/26], [94mLoss[0m : 2.65348
[1mStep[0m  [12/26], [94mLoss[0m : 2.46514
[1mStep[0m  [14/26], [94mLoss[0m : 2.57027
[1mStep[0m  [16/26], [94mLoss[0m : 2.53735
[1mStep[0m  [18/26], [94mLoss[0m : 2.53768
[1mStep[0m  [20/26], [94mLoss[0m : 2.66296
[1mStep[0m  [22/26], [94mLoss[0m : 2.54114
[1mStep[0m  [24/26], [94mLoss[0m : 2.71474

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.522, [92mTest[0m: 2.637, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.60170
[1mStep[0m  [2/26], [94mLoss[0m : 2.37233
[1mStep[0m  [4/26], [94mLoss[0m : 2.47229
[1mStep[0m  [6/26], [94mLoss[0m : 2.40543
[1mStep[0m  [8/26], [94mLoss[0m : 2.64827
[1mStep[0m  [10/26], [94mLoss[0m : 2.40039
[1mStep[0m  [12/26], [94mLoss[0m : 2.52418
[1mStep[0m  [14/26], [94mLoss[0m : 2.44654
[1mStep[0m  [16/26], [94mLoss[0m : 2.37491
[1mStep[0m  [18/26], [94mLoss[0m : 2.49412
[1mStep[0m  [20/26], [94mLoss[0m : 2.42029
[1mStep[0m  [22/26], [94mLoss[0m : 2.42391
[1mStep[0m  [24/26], [94mLoss[0m : 2.51199

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.515, [92mTest[0m: 2.639, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.44222
[1mStep[0m  [2/26], [94mLoss[0m : 2.63127
[1mStep[0m  [4/26], [94mLoss[0m : 2.58300
[1mStep[0m  [6/26], [94mLoss[0m : 2.38557
[1mStep[0m  [8/26], [94mLoss[0m : 2.49013
[1mStep[0m  [10/26], [94mLoss[0m : 2.53769
[1mStep[0m  [12/26], [94mLoss[0m : 2.53826
[1mStep[0m  [14/26], [94mLoss[0m : 2.54984
[1mStep[0m  [16/26], [94mLoss[0m : 2.65720
[1mStep[0m  [18/26], [94mLoss[0m : 2.53499
[1mStep[0m  [20/26], [94mLoss[0m : 2.55877
[1mStep[0m  [22/26], [94mLoss[0m : 2.68314
[1mStep[0m  [24/26], [94mLoss[0m : 2.40848

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.630, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.644
====================================

Phase 1 - Evaluation MAE:  2.6444552494929385
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/26], [94mLoss[0m : 2.45484
[1mStep[0m  [2/26], [94mLoss[0m : 2.42016
[1mStep[0m  [4/26], [94mLoss[0m : 2.56786
[1mStep[0m  [6/26], [94mLoss[0m : 2.44622
[1mStep[0m  [8/26], [94mLoss[0m : 2.57640
[1mStep[0m  [10/26], [94mLoss[0m : 2.46513
[1mStep[0m  [12/26], [94mLoss[0m : 2.46364
[1mStep[0m  [14/26], [94mLoss[0m : 2.54037
[1mStep[0m  [16/26], [94mLoss[0m : 2.47461
[1mStep[0m  [18/26], [94mLoss[0m : 2.38905
[1mStep[0m  [20/26], [94mLoss[0m : 2.69366
[1mStep[0m  [22/26], [94mLoss[0m : 2.51495
[1mStep[0m  [24/26], [94mLoss[0m : 2.46019

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.632, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.63545
[1mStep[0m  [2/26], [94mLoss[0m : 2.31896
[1mStep[0m  [4/26], [94mLoss[0m : 2.52429
[1mStep[0m  [6/26], [94mLoss[0m : 2.62660
[1mStep[0m  [8/26], [94mLoss[0m : 2.62845
[1mStep[0m  [10/26], [94mLoss[0m : 2.43245
[1mStep[0m  [12/26], [94mLoss[0m : 2.51736
[1mStep[0m  [14/26], [94mLoss[0m : 2.53172
[1mStep[0m  [16/26], [94mLoss[0m : 2.62378
[1mStep[0m  [18/26], [94mLoss[0m : 2.59155
[1mStep[0m  [20/26], [94mLoss[0m : 2.47252
[1mStep[0m  [22/26], [94mLoss[0m : 2.61579
[1mStep[0m  [24/26], [94mLoss[0m : 2.55376

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.533, [92mTest[0m: 3.068, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.71304
[1mStep[0m  [2/26], [94mLoss[0m : 2.73420
[1mStep[0m  [4/26], [94mLoss[0m : 2.55686
[1mStep[0m  [6/26], [94mLoss[0m : 2.60860
[1mStep[0m  [8/26], [94mLoss[0m : 2.64464
[1mStep[0m  [10/26], [94mLoss[0m : 2.39536
[1mStep[0m  [12/26], [94mLoss[0m : 2.50579
[1mStep[0m  [14/26], [94mLoss[0m : 2.55388
[1mStep[0m  [16/26], [94mLoss[0m : 2.48615
[1mStep[0m  [18/26], [94mLoss[0m : 2.62950
[1mStep[0m  [20/26], [94mLoss[0m : 2.55562
[1mStep[0m  [22/26], [94mLoss[0m : 2.60991
[1mStep[0m  [24/26], [94mLoss[0m : 2.52857

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.535, [92mTest[0m: 2.686, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.52232
[1mStep[0m  [2/26], [94mLoss[0m : 2.45267
[1mStep[0m  [4/26], [94mLoss[0m : 2.50161
[1mStep[0m  [6/26], [94mLoss[0m : 2.57498
[1mStep[0m  [8/26], [94mLoss[0m : 2.50110
[1mStep[0m  [10/26], [94mLoss[0m : 2.55230
[1mStep[0m  [12/26], [94mLoss[0m : 2.52246
[1mStep[0m  [14/26], [94mLoss[0m : 2.60226
[1mStep[0m  [16/26], [94mLoss[0m : 2.51017
[1mStep[0m  [18/26], [94mLoss[0m : 2.37162
[1mStep[0m  [20/26], [94mLoss[0m : 2.52275
[1mStep[0m  [22/26], [94mLoss[0m : 2.43372
[1mStep[0m  [24/26], [94mLoss[0m : 2.43349

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.519, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.52681
[1mStep[0m  [2/26], [94mLoss[0m : 2.52614
[1mStep[0m  [4/26], [94mLoss[0m : 2.49028
[1mStep[0m  [6/26], [94mLoss[0m : 2.44567
[1mStep[0m  [8/26], [94mLoss[0m : 2.43123
[1mStep[0m  [10/26], [94mLoss[0m : 2.51513
[1mStep[0m  [12/26], [94mLoss[0m : 2.38649
[1mStep[0m  [14/26], [94mLoss[0m : 2.37167
[1mStep[0m  [16/26], [94mLoss[0m : 2.48198
[1mStep[0m  [18/26], [94mLoss[0m : 2.61028
[1mStep[0m  [20/26], [94mLoss[0m : 2.32343
[1mStep[0m  [22/26], [94mLoss[0m : 2.43721
[1mStep[0m  [24/26], [94mLoss[0m : 2.53914

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.539, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.57188
[1mStep[0m  [2/26], [94mLoss[0m : 2.40666
[1mStep[0m  [4/26], [94mLoss[0m : 2.54374
[1mStep[0m  [6/26], [94mLoss[0m : 2.58562
[1mStep[0m  [8/26], [94mLoss[0m : 2.54974
[1mStep[0m  [10/26], [94mLoss[0m : 2.47446
[1mStep[0m  [12/26], [94mLoss[0m : 2.32528
[1mStep[0m  [14/26], [94mLoss[0m : 2.64771
[1mStep[0m  [16/26], [94mLoss[0m : 2.49444
[1mStep[0m  [18/26], [94mLoss[0m : 2.31556
[1mStep[0m  [20/26], [94mLoss[0m : 2.47221
[1mStep[0m  [22/26], [94mLoss[0m : 2.55610
[1mStep[0m  [24/26], [94mLoss[0m : 2.40634

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.515, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.54915
[1mStep[0m  [2/26], [94mLoss[0m : 2.37137
[1mStep[0m  [4/26], [94mLoss[0m : 2.40915
[1mStep[0m  [6/26], [94mLoss[0m : 2.61457
[1mStep[0m  [8/26], [94mLoss[0m : 2.34417
[1mStep[0m  [10/26], [94mLoss[0m : 2.44839
[1mStep[0m  [12/26], [94mLoss[0m : 2.46784
[1mStep[0m  [14/26], [94mLoss[0m : 2.50085
[1mStep[0m  [16/26], [94mLoss[0m : 2.45700
[1mStep[0m  [18/26], [94mLoss[0m : 2.46186
[1mStep[0m  [20/26], [94mLoss[0m : 2.63244
[1mStep[0m  [22/26], [94mLoss[0m : 2.53555
[1mStep[0m  [24/26], [94mLoss[0m : 2.68525

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.513, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.49360
[1mStep[0m  [2/26], [94mLoss[0m : 2.38608
[1mStep[0m  [4/26], [94mLoss[0m : 2.51427
[1mStep[0m  [6/26], [94mLoss[0m : 2.48384
[1mStep[0m  [8/26], [94mLoss[0m : 2.39425
[1mStep[0m  [10/26], [94mLoss[0m : 2.49308
[1mStep[0m  [12/26], [94mLoss[0m : 2.48395
[1mStep[0m  [14/26], [94mLoss[0m : 2.49380
[1mStep[0m  [16/26], [94mLoss[0m : 2.50119
[1mStep[0m  [18/26], [94mLoss[0m : 2.46469
[1mStep[0m  [20/26], [94mLoss[0m : 2.61357
[1mStep[0m  [22/26], [94mLoss[0m : 2.51914
[1mStep[0m  [24/26], [94mLoss[0m : 2.39804

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.526, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.43703
[1mStep[0m  [2/26], [94mLoss[0m : 2.41237
[1mStep[0m  [4/26], [94mLoss[0m : 2.48537
[1mStep[0m  [6/26], [94mLoss[0m : 2.55620
[1mStep[0m  [8/26], [94mLoss[0m : 2.53702
[1mStep[0m  [10/26], [94mLoss[0m : 2.64576
[1mStep[0m  [12/26], [94mLoss[0m : 2.54625
[1mStep[0m  [14/26], [94mLoss[0m : 2.36404
[1mStep[0m  [16/26], [94mLoss[0m : 2.50302
[1mStep[0m  [18/26], [94mLoss[0m : 2.42970
[1mStep[0m  [20/26], [94mLoss[0m : 2.62558
[1mStep[0m  [22/26], [94mLoss[0m : 2.46035
[1mStep[0m  [24/26], [94mLoss[0m : 2.41700

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.561, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.34422
[1mStep[0m  [2/26], [94mLoss[0m : 2.52037
[1mStep[0m  [4/26], [94mLoss[0m : 2.34204
[1mStep[0m  [6/26], [94mLoss[0m : 2.64120
[1mStep[0m  [8/26], [94mLoss[0m : 2.35866
[1mStep[0m  [10/26], [94mLoss[0m : 2.34219
[1mStep[0m  [12/26], [94mLoss[0m : 2.39205
[1mStep[0m  [14/26], [94mLoss[0m : 2.39544
[1mStep[0m  [16/26], [94mLoss[0m : 2.38294
[1mStep[0m  [18/26], [94mLoss[0m : 2.54625
[1mStep[0m  [20/26], [94mLoss[0m : 2.49916
[1mStep[0m  [22/26], [94mLoss[0m : 2.36466
[1mStep[0m  [24/26], [94mLoss[0m : 2.57127

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.539, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.51770
[1mStep[0m  [2/26], [94mLoss[0m : 2.47811
[1mStep[0m  [4/26], [94mLoss[0m : 2.41461
[1mStep[0m  [6/26], [94mLoss[0m : 2.43707
[1mStep[0m  [8/26], [94mLoss[0m : 2.39822
[1mStep[0m  [10/26], [94mLoss[0m : 2.42198
[1mStep[0m  [12/26], [94mLoss[0m : 2.67319
[1mStep[0m  [14/26], [94mLoss[0m : 2.30970
[1mStep[0m  [16/26], [94mLoss[0m : 2.48444
[1mStep[0m  [18/26], [94mLoss[0m : 2.51867
[1mStep[0m  [20/26], [94mLoss[0m : 2.47302
[1mStep[0m  [22/26], [94mLoss[0m : 2.44922
[1mStep[0m  [24/26], [94mLoss[0m : 2.52018

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.579, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.41258
[1mStep[0m  [2/26], [94mLoss[0m : 2.55695
[1mStep[0m  [4/26], [94mLoss[0m : 2.44480
[1mStep[0m  [6/26], [94mLoss[0m : 2.36420
[1mStep[0m  [8/26], [94mLoss[0m : 2.34606
[1mStep[0m  [10/26], [94mLoss[0m : 2.36667
[1mStep[0m  [12/26], [94mLoss[0m : 2.56036
[1mStep[0m  [14/26], [94mLoss[0m : 2.31929
[1mStep[0m  [16/26], [94mLoss[0m : 2.46101
[1mStep[0m  [18/26], [94mLoss[0m : 2.49484
[1mStep[0m  [20/26], [94mLoss[0m : 2.36019
[1mStep[0m  [22/26], [94mLoss[0m : 2.57091
[1mStep[0m  [24/26], [94mLoss[0m : 2.52059

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.599, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.57615
[1mStep[0m  [2/26], [94mLoss[0m : 2.36295
[1mStep[0m  [4/26], [94mLoss[0m : 2.39128
[1mStep[0m  [6/26], [94mLoss[0m : 2.26413
[1mStep[0m  [8/26], [94mLoss[0m : 2.39212
[1mStep[0m  [10/26], [94mLoss[0m : 2.56674
[1mStep[0m  [12/26], [94mLoss[0m : 2.55846
[1mStep[0m  [14/26], [94mLoss[0m : 2.22708
[1mStep[0m  [16/26], [94mLoss[0m : 2.43766
[1mStep[0m  [18/26], [94mLoss[0m : 2.34734
[1mStep[0m  [20/26], [94mLoss[0m : 2.50689
[1mStep[0m  [22/26], [94mLoss[0m : 2.39964
[1mStep[0m  [24/26], [94mLoss[0m : 2.33062

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.634, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.49640
[1mStep[0m  [2/26], [94mLoss[0m : 2.42357
[1mStep[0m  [4/26], [94mLoss[0m : 2.48561
[1mStep[0m  [6/26], [94mLoss[0m : 2.36340
[1mStep[0m  [8/26], [94mLoss[0m : 2.42942
[1mStep[0m  [10/26], [94mLoss[0m : 2.44531
[1mStep[0m  [12/26], [94mLoss[0m : 2.27330
[1mStep[0m  [14/26], [94mLoss[0m : 2.41788
[1mStep[0m  [16/26], [94mLoss[0m : 2.41895
[1mStep[0m  [18/26], [94mLoss[0m : 2.26306
[1mStep[0m  [20/26], [94mLoss[0m : 2.46338
[1mStep[0m  [22/26], [94mLoss[0m : 2.42069
[1mStep[0m  [24/26], [94mLoss[0m : 2.36096

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.545, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.39776
[1mStep[0m  [2/26], [94mLoss[0m : 2.49275
[1mStep[0m  [4/26], [94mLoss[0m : 2.39212
[1mStep[0m  [6/26], [94mLoss[0m : 2.49463
[1mStep[0m  [8/26], [94mLoss[0m : 2.31238
[1mStep[0m  [10/26], [94mLoss[0m : 2.22522
[1mStep[0m  [12/26], [94mLoss[0m : 2.59277
[1mStep[0m  [14/26], [94mLoss[0m : 2.44202
[1mStep[0m  [16/26], [94mLoss[0m : 2.37440
[1mStep[0m  [18/26], [94mLoss[0m : 2.36274
[1mStep[0m  [20/26], [94mLoss[0m : 2.54964
[1mStep[0m  [22/26], [94mLoss[0m : 2.27399
[1mStep[0m  [24/26], [94mLoss[0m : 2.31286

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.491, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.36811
[1mStep[0m  [2/26], [94mLoss[0m : 2.22493
[1mStep[0m  [4/26], [94mLoss[0m : 2.33107
[1mStep[0m  [6/26], [94mLoss[0m : 2.42886
[1mStep[0m  [8/26], [94mLoss[0m : 2.40070
[1mStep[0m  [10/26], [94mLoss[0m : 2.39577
[1mStep[0m  [12/26], [94mLoss[0m : 2.32765
[1mStep[0m  [14/26], [94mLoss[0m : 2.44209
[1mStep[0m  [16/26], [94mLoss[0m : 2.44927
[1mStep[0m  [18/26], [94mLoss[0m : 2.29364
[1mStep[0m  [20/26], [94mLoss[0m : 2.42064
[1mStep[0m  [22/26], [94mLoss[0m : 2.35954
[1mStep[0m  [24/26], [94mLoss[0m : 2.44918

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.502, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.32427
[1mStep[0m  [2/26], [94mLoss[0m : 2.49918
[1mStep[0m  [4/26], [94mLoss[0m : 2.43712
[1mStep[0m  [6/26], [94mLoss[0m : 2.49500
[1mStep[0m  [8/26], [94mLoss[0m : 2.39325
[1mStep[0m  [10/26], [94mLoss[0m : 2.25100
[1mStep[0m  [12/26], [94mLoss[0m : 2.45010
[1mStep[0m  [14/26], [94mLoss[0m : 2.29563
[1mStep[0m  [16/26], [94mLoss[0m : 2.22098
[1mStep[0m  [18/26], [94mLoss[0m : 2.38247
[1mStep[0m  [20/26], [94mLoss[0m : 2.52295
[1mStep[0m  [22/26], [94mLoss[0m : 2.36036
[1mStep[0m  [24/26], [94mLoss[0m : 2.29317

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.384, [92mTest[0m: 2.552, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.44366
[1mStep[0m  [2/26], [94mLoss[0m : 2.37915
[1mStep[0m  [4/26], [94mLoss[0m : 2.37142
[1mStep[0m  [6/26], [94mLoss[0m : 2.38296
[1mStep[0m  [8/26], [94mLoss[0m : 2.40382
[1mStep[0m  [10/26], [94mLoss[0m : 2.66712
[1mStep[0m  [12/26], [94mLoss[0m : 2.45147
[1mStep[0m  [14/26], [94mLoss[0m : 2.30808
[1mStep[0m  [16/26], [94mLoss[0m : 2.55050
[1mStep[0m  [18/26], [94mLoss[0m : 2.34345
[1mStep[0m  [20/26], [94mLoss[0m : 2.43511
[1mStep[0m  [22/26], [94mLoss[0m : 2.28014
[1mStep[0m  [24/26], [94mLoss[0m : 2.44605

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.394, [92mTest[0m: 2.567, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.28776
[1mStep[0m  [2/26], [94mLoss[0m : 2.50024
[1mStep[0m  [4/26], [94mLoss[0m : 2.43157
[1mStep[0m  [6/26], [94mLoss[0m : 2.36019
[1mStep[0m  [8/26], [94mLoss[0m : 2.33875
[1mStep[0m  [10/26], [94mLoss[0m : 2.36650
[1mStep[0m  [12/26], [94mLoss[0m : 2.28022
[1mStep[0m  [14/26], [94mLoss[0m : 2.30439
[1mStep[0m  [16/26], [94mLoss[0m : 2.27595
[1mStep[0m  [18/26], [94mLoss[0m : 2.42332
[1mStep[0m  [20/26], [94mLoss[0m : 2.40873
[1mStep[0m  [22/26], [94mLoss[0m : 2.34229
[1mStep[0m  [24/26], [94mLoss[0m : 2.56547

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.378, [92mTest[0m: 2.537, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.46281
[1mStep[0m  [2/26], [94mLoss[0m : 2.58342
[1mStep[0m  [4/26], [94mLoss[0m : 2.32152
[1mStep[0m  [6/26], [94mLoss[0m : 2.35235
[1mStep[0m  [8/26], [94mLoss[0m : 2.50251
[1mStep[0m  [10/26], [94mLoss[0m : 2.46383
[1mStep[0m  [12/26], [94mLoss[0m : 2.26371
[1mStep[0m  [14/26], [94mLoss[0m : 2.37705
[1mStep[0m  [16/26], [94mLoss[0m : 2.40800
[1mStep[0m  [18/26], [94mLoss[0m : 2.45602
[1mStep[0m  [20/26], [94mLoss[0m : 2.59493
[1mStep[0m  [22/26], [94mLoss[0m : 2.45078
[1mStep[0m  [24/26], [94mLoss[0m : 2.25399

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.393, [92mTest[0m: 2.532, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.44584
[1mStep[0m  [2/26], [94mLoss[0m : 2.43253
[1mStep[0m  [4/26], [94mLoss[0m : 2.41614
[1mStep[0m  [6/26], [94mLoss[0m : 2.37991
[1mStep[0m  [8/26], [94mLoss[0m : 2.46425
[1mStep[0m  [10/26], [94mLoss[0m : 2.22916
[1mStep[0m  [12/26], [94mLoss[0m : 2.32597
[1mStep[0m  [14/26], [94mLoss[0m : 2.27116
[1mStep[0m  [16/26], [94mLoss[0m : 2.28345
[1mStep[0m  [18/26], [94mLoss[0m : 2.34310
[1mStep[0m  [20/26], [94mLoss[0m : 2.24378
[1mStep[0m  [22/26], [94mLoss[0m : 2.33131
[1mStep[0m  [24/26], [94mLoss[0m : 2.33654

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.349, [92mTest[0m: 2.505, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.31822
[1mStep[0m  [2/26], [94mLoss[0m : 2.31696
[1mStep[0m  [4/26], [94mLoss[0m : 2.42374
[1mStep[0m  [6/26], [94mLoss[0m : 2.28097
[1mStep[0m  [8/26], [94mLoss[0m : 2.29903
[1mStep[0m  [10/26], [94mLoss[0m : 2.38997
[1mStep[0m  [12/26], [94mLoss[0m : 2.27071
[1mStep[0m  [14/26], [94mLoss[0m : 2.41110
[1mStep[0m  [16/26], [94mLoss[0m : 2.35588
[1mStep[0m  [18/26], [94mLoss[0m : 2.34418
[1mStep[0m  [20/26], [94mLoss[0m : 2.40458
[1mStep[0m  [22/26], [94mLoss[0m : 2.38570
[1mStep[0m  [24/26], [94mLoss[0m : 2.27903

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.351, [92mTest[0m: 2.562, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.26253
[1mStep[0m  [2/26], [94mLoss[0m : 2.37160
[1mStep[0m  [4/26], [94mLoss[0m : 2.20400
[1mStep[0m  [6/26], [94mLoss[0m : 2.49549
[1mStep[0m  [8/26], [94mLoss[0m : 2.39078
[1mStep[0m  [10/26], [94mLoss[0m : 2.29192
[1mStep[0m  [12/26], [94mLoss[0m : 2.21758
[1mStep[0m  [14/26], [94mLoss[0m : 2.31420
[1mStep[0m  [16/26], [94mLoss[0m : 2.50141
[1mStep[0m  [18/26], [94mLoss[0m : 2.30291
[1mStep[0m  [20/26], [94mLoss[0m : 2.40481
[1mStep[0m  [22/26], [94mLoss[0m : 2.39505
[1mStep[0m  [24/26], [94mLoss[0m : 2.29131

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.341, [92mTest[0m: 2.539, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.23521
[1mStep[0m  [2/26], [94mLoss[0m : 2.32313
[1mStep[0m  [4/26], [94mLoss[0m : 2.34338
[1mStep[0m  [6/26], [94mLoss[0m : 2.26956
[1mStep[0m  [8/26], [94mLoss[0m : 2.41348
[1mStep[0m  [10/26], [94mLoss[0m : 2.44146
[1mStep[0m  [12/26], [94mLoss[0m : 2.40060
[1mStep[0m  [14/26], [94mLoss[0m : 2.35448
[1mStep[0m  [16/26], [94mLoss[0m : 2.31519
[1mStep[0m  [18/26], [94mLoss[0m : 2.46084
[1mStep[0m  [20/26], [94mLoss[0m : 2.31182
[1mStep[0m  [22/26], [94mLoss[0m : 2.33752
[1mStep[0m  [24/26], [94mLoss[0m : 2.19601

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.339, [92mTest[0m: 2.521, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.17049
[1mStep[0m  [2/26], [94mLoss[0m : 2.28276
[1mStep[0m  [4/26], [94mLoss[0m : 2.20961
[1mStep[0m  [6/26], [94mLoss[0m : 2.20406
[1mStep[0m  [8/26], [94mLoss[0m : 2.38069
[1mStep[0m  [10/26], [94mLoss[0m : 2.28012
[1mStep[0m  [12/26], [94mLoss[0m : 2.33990
[1mStep[0m  [14/26], [94mLoss[0m : 2.41873
[1mStep[0m  [16/26], [94mLoss[0m : 2.31349
[1mStep[0m  [18/26], [94mLoss[0m : 2.31642
[1mStep[0m  [20/26], [94mLoss[0m : 2.32973
[1mStep[0m  [22/26], [94mLoss[0m : 2.38871
[1mStep[0m  [24/26], [94mLoss[0m : 2.46021

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.324, [92mTest[0m: 2.442, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.35296
[1mStep[0m  [2/26], [94mLoss[0m : 2.37025
[1mStep[0m  [4/26], [94mLoss[0m : 2.12965
[1mStep[0m  [6/26], [94mLoss[0m : 2.39593
[1mStep[0m  [8/26], [94mLoss[0m : 2.25953
[1mStep[0m  [10/26], [94mLoss[0m : 2.28307
[1mStep[0m  [12/26], [94mLoss[0m : 2.33178
[1mStep[0m  [14/26], [94mLoss[0m : 2.35672
[1mStep[0m  [16/26], [94mLoss[0m : 2.31189
[1mStep[0m  [18/26], [94mLoss[0m : 2.30886
[1mStep[0m  [20/26], [94mLoss[0m : 2.24668
[1mStep[0m  [22/26], [94mLoss[0m : 2.45768
[1mStep[0m  [24/26], [94mLoss[0m : 2.32400

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.331, [92mTest[0m: 2.450, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.41678
[1mStep[0m  [2/26], [94mLoss[0m : 2.42792
[1mStep[0m  [4/26], [94mLoss[0m : 2.29984
[1mStep[0m  [6/26], [94mLoss[0m : 2.30607
[1mStep[0m  [8/26], [94mLoss[0m : 2.27445
[1mStep[0m  [10/26], [94mLoss[0m : 2.26920
[1mStep[0m  [12/26], [94mLoss[0m : 2.32119
[1mStep[0m  [14/26], [94mLoss[0m : 2.26561
[1mStep[0m  [16/26], [94mLoss[0m : 2.19722
[1mStep[0m  [18/26], [94mLoss[0m : 2.35286
[1mStep[0m  [20/26], [94mLoss[0m : 2.37446
[1mStep[0m  [22/26], [94mLoss[0m : 2.28894
[1mStep[0m  [24/26], [94mLoss[0m : 2.16869

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.296, [92mTest[0m: 2.437, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.21645
[1mStep[0m  [2/26], [94mLoss[0m : 2.31674
[1mStep[0m  [4/26], [94mLoss[0m : 2.16817
[1mStep[0m  [6/26], [94mLoss[0m : 2.25836
[1mStep[0m  [8/26], [94mLoss[0m : 2.17119
[1mStep[0m  [10/26], [94mLoss[0m : 2.18651
[1mStep[0m  [12/26], [94mLoss[0m : 2.35437
[1mStep[0m  [14/26], [94mLoss[0m : 2.30725
[1mStep[0m  [16/26], [94mLoss[0m : 2.31333
[1mStep[0m  [18/26], [94mLoss[0m : 2.34745
[1mStep[0m  [20/26], [94mLoss[0m : 2.31227
[1mStep[0m  [22/26], [94mLoss[0m : 2.23581
[1mStep[0m  [24/26], [94mLoss[0m : 2.28575

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.283, [92mTest[0m: 2.428, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.34396
[1mStep[0m  [2/26], [94mLoss[0m : 2.27652
[1mStep[0m  [4/26], [94mLoss[0m : 2.30791
[1mStep[0m  [6/26], [94mLoss[0m : 2.28801
[1mStep[0m  [8/26], [94mLoss[0m : 2.36620
[1mStep[0m  [10/26], [94mLoss[0m : 2.47004
[1mStep[0m  [12/26], [94mLoss[0m : 2.30296
[1mStep[0m  [14/26], [94mLoss[0m : 2.19667
[1mStep[0m  [16/26], [94mLoss[0m : 2.32354
[1mStep[0m  [18/26], [94mLoss[0m : 2.32214
[1mStep[0m  [20/26], [94mLoss[0m : 2.29396
[1mStep[0m  [22/26], [94mLoss[0m : 2.15583
[1mStep[0m  [24/26], [94mLoss[0m : 2.34273

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.287, [92mTest[0m: 2.430, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.27960
[1mStep[0m  [2/26], [94mLoss[0m : 2.34820
[1mStep[0m  [4/26], [94mLoss[0m : 2.37953
[1mStep[0m  [6/26], [94mLoss[0m : 2.22270
[1mStep[0m  [8/26], [94mLoss[0m : 2.36398
[1mStep[0m  [10/26], [94mLoss[0m : 2.15452
[1mStep[0m  [12/26], [94mLoss[0m : 2.25698
[1mStep[0m  [14/26], [94mLoss[0m : 2.25209
[1mStep[0m  [16/26], [94mLoss[0m : 2.33102
[1mStep[0m  [18/26], [94mLoss[0m : 2.23091
[1mStep[0m  [20/26], [94mLoss[0m : 2.43127
[1mStep[0m  [22/26], [94mLoss[0m : 2.33973
[1mStep[0m  [24/26], [94mLoss[0m : 2.30289

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.294, [92mTest[0m: 2.416, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.411
====================================

Phase 2 - Evaluation MAE:  2.4113326989687405
MAE score P1      2.644455
MAE score P2      2.411333
loss              2.283136
learning_rate     0.002575
batch_size             512
hidden_sizes         [300]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.4
momentum               0.1
weight_decay          0.01
Name: 1, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/53], [94mLoss[0m : 10.97995
[1mStep[0m  [5/53], [94mLoss[0m : 10.58254
[1mStep[0m  [10/53], [94mLoss[0m : 11.10756
[1mStep[0m  [15/53], [94mLoss[0m : 10.61058
[1mStep[0m  [20/53], [94mLoss[0m : 10.56567
[1mStep[0m  [25/53], [94mLoss[0m : 10.49376
[1mStep[0m  [30/53], [94mLoss[0m : 10.48878
[1mStep[0m  [35/53], [94mLoss[0m : 11.23125
[1mStep[0m  [40/53], [94mLoss[0m : 10.83204
[1mStep[0m  [45/53], [94mLoss[0m : 10.66372
[1mStep[0m  [50/53], [94mLoss[0m : 10.44532

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.817, [92mTest[0m: 10.993, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.52002
[1mStep[0m  [5/53], [94mLoss[0m : 10.72211
[1mStep[0m  [10/53], [94mLoss[0m : 10.89742
[1mStep[0m  [15/53], [94mLoss[0m : 10.51313
[1mStep[0m  [20/53], [94mLoss[0m : 10.36646
[1mStep[0m  [25/53], [94mLoss[0m : 10.20159
[1mStep[0m  [30/53], [94mLoss[0m : 10.38884
[1mStep[0m  [35/53], [94mLoss[0m : 10.61852
[1mStep[0m  [40/53], [94mLoss[0m : 9.87997
[1mStep[0m  [45/53], [94mLoss[0m : 10.55910
[1mStep[0m  [50/53], [94mLoss[0m : 10.62538

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.588, [92mTest[0m: 10.687, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.56811
[1mStep[0m  [5/53], [94mLoss[0m : 10.26675
[1mStep[0m  [10/53], [94mLoss[0m : 10.27895
[1mStep[0m  [15/53], [94mLoss[0m : 10.34403
[1mStep[0m  [20/53], [94mLoss[0m : 10.27406
[1mStep[0m  [25/53], [94mLoss[0m : 10.54395
[1mStep[0m  [30/53], [94mLoss[0m : 10.19565
[1mStep[0m  [35/53], [94mLoss[0m : 10.55164
[1mStep[0m  [40/53], [94mLoss[0m : 10.00335
[1mStep[0m  [45/53], [94mLoss[0m : 10.16183
[1mStep[0m  [50/53], [94mLoss[0m : 10.31427

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.362, [92mTest[0m: 10.370, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.32156
[1mStep[0m  [5/53], [94mLoss[0m : 10.27029
[1mStep[0m  [10/53], [94mLoss[0m : 10.21253
[1mStep[0m  [15/53], [94mLoss[0m : 10.03122
[1mStep[0m  [20/53], [94mLoss[0m : 10.11825
[1mStep[0m  [25/53], [94mLoss[0m : 9.93440
[1mStep[0m  [30/53], [94mLoss[0m : 9.94832
[1mStep[0m  [35/53], [94mLoss[0m : 10.44934
[1mStep[0m  [40/53], [94mLoss[0m : 10.34126
[1mStep[0m  [45/53], [94mLoss[0m : 10.09304
[1mStep[0m  [50/53], [94mLoss[0m : 10.00714

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.135, [92mTest[0m: 10.094, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.87307
[1mStep[0m  [5/53], [94mLoss[0m : 9.57905
[1mStep[0m  [10/53], [94mLoss[0m : 10.07507
[1mStep[0m  [15/53], [94mLoss[0m : 10.19349
[1mStep[0m  [20/53], [94mLoss[0m : 9.97661
[1mStep[0m  [25/53], [94mLoss[0m : 10.42108
[1mStep[0m  [30/53], [94mLoss[0m : 9.81954
[1mStep[0m  [35/53], [94mLoss[0m : 9.70741
[1mStep[0m  [40/53], [94mLoss[0m : 9.66492
[1mStep[0m  [45/53], [94mLoss[0m : 9.83388
[1mStep[0m  [50/53], [94mLoss[0m : 9.86915

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 9.882, [92mTest[0m: 9.803, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.57833
[1mStep[0m  [5/53], [94mLoss[0m : 9.83311
[1mStep[0m  [10/53], [94mLoss[0m : 9.41218
[1mStep[0m  [15/53], [94mLoss[0m : 9.67123
[1mStep[0m  [20/53], [94mLoss[0m : 9.39053
[1mStep[0m  [25/53], [94mLoss[0m : 9.70394
[1mStep[0m  [30/53], [94mLoss[0m : 9.59172
[1mStep[0m  [35/53], [94mLoss[0m : 9.15337
[1mStep[0m  [40/53], [94mLoss[0m : 9.76978
[1mStep[0m  [45/53], [94mLoss[0m : 10.13111
[1mStep[0m  [50/53], [94mLoss[0m : 9.36865

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 9.613, [92mTest[0m: 9.429, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.43467
[1mStep[0m  [5/53], [94mLoss[0m : 9.17729
[1mStep[0m  [10/53], [94mLoss[0m : 9.16119
[1mStep[0m  [15/53], [94mLoss[0m : 9.32823
[1mStep[0m  [20/53], [94mLoss[0m : 9.52882
[1mStep[0m  [25/53], [94mLoss[0m : 9.54550
[1mStep[0m  [30/53], [94mLoss[0m : 9.16294
[1mStep[0m  [35/53], [94mLoss[0m : 9.27021
[1mStep[0m  [40/53], [94mLoss[0m : 9.21394
[1mStep[0m  [45/53], [94mLoss[0m : 9.04588
[1mStep[0m  [50/53], [94mLoss[0m : 9.39225

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 9.306, [92mTest[0m: 9.166, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.95615
[1mStep[0m  [5/53], [94mLoss[0m : 9.09764
[1mStep[0m  [10/53], [94mLoss[0m : 8.94098
[1mStep[0m  [15/53], [94mLoss[0m : 9.14895
[1mStep[0m  [20/53], [94mLoss[0m : 8.63133
[1mStep[0m  [25/53], [94mLoss[0m : 8.80654
[1mStep[0m  [30/53], [94mLoss[0m : 9.30272
[1mStep[0m  [35/53], [94mLoss[0m : 9.10895
[1mStep[0m  [40/53], [94mLoss[0m : 9.11461
[1mStep[0m  [45/53], [94mLoss[0m : 8.51335
[1mStep[0m  [50/53], [94mLoss[0m : 9.07156

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 8.973, [92mTest[0m: 8.775, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.73617
[1mStep[0m  [5/53], [94mLoss[0m : 8.82401
[1mStep[0m  [10/53], [94mLoss[0m : 8.59672
[1mStep[0m  [15/53], [94mLoss[0m : 8.63119
[1mStep[0m  [20/53], [94mLoss[0m : 8.92651
[1mStep[0m  [25/53], [94mLoss[0m : 8.86519
[1mStep[0m  [30/53], [94mLoss[0m : 8.25145
[1mStep[0m  [35/53], [94mLoss[0m : 8.53324
[1mStep[0m  [40/53], [94mLoss[0m : 8.72586
[1mStep[0m  [45/53], [94mLoss[0m : 8.05937
[1mStep[0m  [50/53], [94mLoss[0m : 8.16573

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 8.604, [92mTest[0m: 8.213, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.81347
[1mStep[0m  [5/53], [94mLoss[0m : 8.08938
[1mStep[0m  [10/53], [94mLoss[0m : 8.03533
[1mStep[0m  [15/53], [94mLoss[0m : 8.75154
[1mStep[0m  [20/53], [94mLoss[0m : 8.23609
[1mStep[0m  [25/53], [94mLoss[0m : 8.67272
[1mStep[0m  [30/53], [94mLoss[0m : 8.31618
[1mStep[0m  [35/53], [94mLoss[0m : 8.17960
[1mStep[0m  [40/53], [94mLoss[0m : 7.66697
[1mStep[0m  [45/53], [94mLoss[0m : 8.14215
[1mStep[0m  [50/53], [94mLoss[0m : 8.12615

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 8.188, [92mTest[0m: 7.892, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.03339
[1mStep[0m  [5/53], [94mLoss[0m : 8.02486
[1mStep[0m  [10/53], [94mLoss[0m : 7.69393
[1mStep[0m  [15/53], [94mLoss[0m : 7.95801
[1mStep[0m  [20/53], [94mLoss[0m : 7.97229
[1mStep[0m  [25/53], [94mLoss[0m : 7.53370
[1mStep[0m  [30/53], [94mLoss[0m : 7.58322
[1mStep[0m  [35/53], [94mLoss[0m : 7.71816
[1mStep[0m  [40/53], [94mLoss[0m : 7.65520
[1mStep[0m  [45/53], [94mLoss[0m : 7.57966
[1mStep[0m  [50/53], [94mLoss[0m : 7.22859

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 7.755, [92mTest[0m: 7.333, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.51828
[1mStep[0m  [5/53], [94mLoss[0m : 7.31250
[1mStep[0m  [10/53], [94mLoss[0m : 7.26646
[1mStep[0m  [15/53], [94mLoss[0m : 7.45835
[1mStep[0m  [20/53], [94mLoss[0m : 7.28386
[1mStep[0m  [25/53], [94mLoss[0m : 7.31079
[1mStep[0m  [30/53], [94mLoss[0m : 7.05354
[1mStep[0m  [35/53], [94mLoss[0m : 7.54698
[1mStep[0m  [40/53], [94mLoss[0m : 7.25768
[1mStep[0m  [45/53], [94mLoss[0m : 7.29917
[1mStep[0m  [50/53], [94mLoss[0m : 7.36959

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 7.327, [92mTest[0m: 6.891, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.03781
[1mStep[0m  [5/53], [94mLoss[0m : 6.94330
[1mStep[0m  [10/53], [94mLoss[0m : 6.82667
[1mStep[0m  [15/53], [94mLoss[0m : 6.94248
[1mStep[0m  [20/53], [94mLoss[0m : 6.96806
[1mStep[0m  [25/53], [94mLoss[0m : 6.69676
[1mStep[0m  [30/53], [94mLoss[0m : 6.80747
[1mStep[0m  [35/53], [94mLoss[0m : 7.01504
[1mStep[0m  [40/53], [94mLoss[0m : 6.87082
[1mStep[0m  [45/53], [94mLoss[0m : 6.97726
[1mStep[0m  [50/53], [94mLoss[0m : 6.47029

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 6.930, [92mTest[0m: 6.410, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.74872
[1mStep[0m  [5/53], [94mLoss[0m : 6.81770
[1mStep[0m  [10/53], [94mLoss[0m : 6.70400
[1mStep[0m  [15/53], [94mLoss[0m : 6.82478
[1mStep[0m  [20/53], [94mLoss[0m : 6.62348
[1mStep[0m  [25/53], [94mLoss[0m : 6.65962
[1mStep[0m  [30/53], [94mLoss[0m : 6.31870
[1mStep[0m  [35/53], [94mLoss[0m : 6.41475
[1mStep[0m  [40/53], [94mLoss[0m : 6.28059
[1mStep[0m  [45/53], [94mLoss[0m : 6.81500
[1mStep[0m  [50/53], [94mLoss[0m : 6.27666

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 6.582, [92mTest[0m: 6.061, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.35182
[1mStep[0m  [5/53], [94mLoss[0m : 6.42385
[1mStep[0m  [10/53], [94mLoss[0m : 5.76001
[1mStep[0m  [15/53], [94mLoss[0m : 6.33236
[1mStep[0m  [20/53], [94mLoss[0m : 6.47522
[1mStep[0m  [25/53], [94mLoss[0m : 6.47724
[1mStep[0m  [30/53], [94mLoss[0m : 6.22499
[1mStep[0m  [35/53], [94mLoss[0m : 6.08208
[1mStep[0m  [40/53], [94mLoss[0m : 6.17051
[1mStep[0m  [45/53], [94mLoss[0m : 6.36508
[1mStep[0m  [50/53], [94mLoss[0m : 6.09494

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 6.294, [92mTest[0m: 5.716, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.41099
[1mStep[0m  [5/53], [94mLoss[0m : 6.02877
[1mStep[0m  [10/53], [94mLoss[0m : 5.41392
[1mStep[0m  [15/53], [94mLoss[0m : 6.09448
[1mStep[0m  [20/53], [94mLoss[0m : 6.09966
[1mStep[0m  [25/53], [94mLoss[0m : 5.99833
[1mStep[0m  [30/53], [94mLoss[0m : 5.96990
[1mStep[0m  [35/53], [94mLoss[0m : 6.02620
[1mStep[0m  [40/53], [94mLoss[0m : 6.11866
[1mStep[0m  [45/53], [94mLoss[0m : 5.68525
[1mStep[0m  [50/53], [94mLoss[0m : 5.77349

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 6.000, [92mTest[0m: 5.518, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.24080
[1mStep[0m  [5/53], [94mLoss[0m : 5.84009
[1mStep[0m  [10/53], [94mLoss[0m : 5.43876
[1mStep[0m  [15/53], [94mLoss[0m : 5.85229
[1mStep[0m  [20/53], [94mLoss[0m : 5.74413
[1mStep[0m  [25/53], [94mLoss[0m : 5.61396
[1mStep[0m  [30/53], [94mLoss[0m : 6.28465
[1mStep[0m  [35/53], [94mLoss[0m : 5.70794
[1mStep[0m  [40/53], [94mLoss[0m : 5.66034
[1mStep[0m  [45/53], [94mLoss[0m : 5.33309
[1mStep[0m  [50/53], [94mLoss[0m : 5.80014

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 5.732, [92mTest[0m: 5.152, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 5.50785
[1mStep[0m  [5/53], [94mLoss[0m : 5.48818
[1mStep[0m  [10/53], [94mLoss[0m : 5.68195
[1mStep[0m  [15/53], [94mLoss[0m : 5.01613
[1mStep[0m  [20/53], [94mLoss[0m : 5.18166
[1mStep[0m  [25/53], [94mLoss[0m : 5.67123
[1mStep[0m  [30/53], [94mLoss[0m : 5.35048
[1mStep[0m  [35/53], [94mLoss[0m : 5.44662
[1mStep[0m  [40/53], [94mLoss[0m : 5.41406
[1mStep[0m  [45/53], [94mLoss[0m : 5.21682
[1mStep[0m  [50/53], [94mLoss[0m : 5.20275

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 5.442, [92mTest[0m: 4.968, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 5.31449
[1mStep[0m  [5/53], [94mLoss[0m : 5.28680
[1mStep[0m  [10/53], [94mLoss[0m : 5.40197
[1mStep[0m  [15/53], [94mLoss[0m : 5.45154
[1mStep[0m  [20/53], [94mLoss[0m : 5.38018
[1mStep[0m  [25/53], [94mLoss[0m : 5.15713
[1mStep[0m  [30/53], [94mLoss[0m : 4.97684
[1mStep[0m  [35/53], [94mLoss[0m : 5.52525
[1mStep[0m  [40/53], [94mLoss[0m : 4.58626
[1mStep[0m  [45/53], [94mLoss[0m : 5.19621
[1mStep[0m  [50/53], [94mLoss[0m : 5.05444

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 5.138, [92mTest[0m: 4.677, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 5.15520
[1mStep[0m  [5/53], [94mLoss[0m : 4.88489
[1mStep[0m  [10/53], [94mLoss[0m : 5.19964
[1mStep[0m  [15/53], [94mLoss[0m : 5.32275
[1mStep[0m  [20/53], [94mLoss[0m : 5.11967
[1mStep[0m  [25/53], [94mLoss[0m : 5.07467
[1mStep[0m  [30/53], [94mLoss[0m : 4.52860
[1mStep[0m  [35/53], [94mLoss[0m : 4.76393
[1mStep[0m  [40/53], [94mLoss[0m : 4.87848
[1mStep[0m  [45/53], [94mLoss[0m : 4.36748
[1mStep[0m  [50/53], [94mLoss[0m : 4.48845

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 4.843, [92mTest[0m: 4.410, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 4.65600
[1mStep[0m  [5/53], [94mLoss[0m : 4.72775
[1mStep[0m  [10/53], [94mLoss[0m : 4.80873
[1mStep[0m  [15/53], [94mLoss[0m : 4.37073
[1mStep[0m  [20/53], [94mLoss[0m : 4.50214
[1mStep[0m  [25/53], [94mLoss[0m : 4.61246
[1mStep[0m  [30/53], [94mLoss[0m : 4.49388
[1mStep[0m  [35/53], [94mLoss[0m : 4.41650
[1mStep[0m  [40/53], [94mLoss[0m : 4.27411
[1mStep[0m  [45/53], [94mLoss[0m : 4.43941
[1mStep[0m  [50/53], [94mLoss[0m : 4.46370

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 4.537, [92mTest[0m: 4.009, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 4.60292
[1mStep[0m  [5/53], [94mLoss[0m : 4.00748
[1mStep[0m  [10/53], [94mLoss[0m : 4.46945
[1mStep[0m  [15/53], [94mLoss[0m : 4.15037
[1mStep[0m  [20/53], [94mLoss[0m : 4.51466
[1mStep[0m  [25/53], [94mLoss[0m : 4.02468
[1mStep[0m  [30/53], [94mLoss[0m : 4.06986
[1mStep[0m  [35/53], [94mLoss[0m : 4.01215
[1mStep[0m  [40/53], [94mLoss[0m : 4.30262
[1mStep[0m  [45/53], [94mLoss[0m : 4.09041
[1mStep[0m  [50/53], [94mLoss[0m : 4.35805

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 4.270, [92mTest[0m: 3.817, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 4.06701
[1mStep[0m  [5/53], [94mLoss[0m : 3.85493
[1mStep[0m  [10/53], [94mLoss[0m : 4.05980
[1mStep[0m  [15/53], [94mLoss[0m : 3.46906
[1mStep[0m  [20/53], [94mLoss[0m : 4.42668
[1mStep[0m  [25/53], [94mLoss[0m : 3.69562
[1mStep[0m  [30/53], [94mLoss[0m : 4.25609
[1mStep[0m  [35/53], [94mLoss[0m : 4.12939
[1mStep[0m  [40/53], [94mLoss[0m : 4.23361
[1mStep[0m  [45/53], [94mLoss[0m : 4.21070
[1mStep[0m  [50/53], [94mLoss[0m : 3.98954

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 4.005, [92mTest[0m: 3.591, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 4.09915
[1mStep[0m  [5/53], [94mLoss[0m : 3.97813
[1mStep[0m  [10/53], [94mLoss[0m : 3.98838
[1mStep[0m  [15/53], [94mLoss[0m : 4.14656
[1mStep[0m  [20/53], [94mLoss[0m : 3.81344
[1mStep[0m  [25/53], [94mLoss[0m : 4.30210
[1mStep[0m  [30/53], [94mLoss[0m : 3.69396
[1mStep[0m  [35/53], [94mLoss[0m : 3.39511
[1mStep[0m  [40/53], [94mLoss[0m : 3.70053
[1mStep[0m  [45/53], [94mLoss[0m : 3.81552
[1mStep[0m  [50/53], [94mLoss[0m : 3.58628

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 3.747, [92mTest[0m: 3.361, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.61728
[1mStep[0m  [5/53], [94mLoss[0m : 3.24734
[1mStep[0m  [10/53], [94mLoss[0m : 3.79680
[1mStep[0m  [15/53], [94mLoss[0m : 3.43857
[1mStep[0m  [20/53], [94mLoss[0m : 3.65318
[1mStep[0m  [25/53], [94mLoss[0m : 3.43061
[1mStep[0m  [30/53], [94mLoss[0m : 3.48647
[1mStep[0m  [35/53], [94mLoss[0m : 3.77911
[1mStep[0m  [40/53], [94mLoss[0m : 3.41869
[1mStep[0m  [45/53], [94mLoss[0m : 3.05473
[1mStep[0m  [50/53], [94mLoss[0m : 3.52084

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 3.487, [92mTest[0m: 3.147, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.57975
[1mStep[0m  [5/53], [94mLoss[0m : 3.65778
[1mStep[0m  [10/53], [94mLoss[0m : 2.90214
[1mStep[0m  [15/53], [94mLoss[0m : 3.60789
[1mStep[0m  [20/53], [94mLoss[0m : 3.20944
[1mStep[0m  [25/53], [94mLoss[0m : 3.37886
[1mStep[0m  [30/53], [94mLoss[0m : 3.43456
[1mStep[0m  [35/53], [94mLoss[0m : 2.95717
[1mStep[0m  [40/53], [94mLoss[0m : 3.28758
[1mStep[0m  [45/53], [94mLoss[0m : 3.30726
[1mStep[0m  [50/53], [94mLoss[0m : 2.87083

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 3.267, [92mTest[0m: 2.981, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.99385
[1mStep[0m  [5/53], [94mLoss[0m : 3.15961
[1mStep[0m  [10/53], [94mLoss[0m : 3.24073
[1mStep[0m  [15/53], [94mLoss[0m : 2.75161
[1mStep[0m  [20/53], [94mLoss[0m : 3.09254
[1mStep[0m  [25/53], [94mLoss[0m : 3.19956
[1mStep[0m  [30/53], [94mLoss[0m : 3.17765
[1mStep[0m  [35/53], [94mLoss[0m : 2.79803
[1mStep[0m  [40/53], [94mLoss[0m : 3.09965
[1mStep[0m  [45/53], [94mLoss[0m : 2.73413
[1mStep[0m  [50/53], [94mLoss[0m : 3.15504

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 3.058, [92mTest[0m: 2.815, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.23000
[1mStep[0m  [5/53], [94mLoss[0m : 2.99259
[1mStep[0m  [10/53], [94mLoss[0m : 2.84529
[1mStep[0m  [15/53], [94mLoss[0m : 2.92084
[1mStep[0m  [20/53], [94mLoss[0m : 2.98106
[1mStep[0m  [25/53], [94mLoss[0m : 3.41436
[1mStep[0m  [30/53], [94mLoss[0m : 2.55136
[1mStep[0m  [35/53], [94mLoss[0m : 2.97440
[1mStep[0m  [40/53], [94mLoss[0m : 2.77085
[1mStep[0m  [45/53], [94mLoss[0m : 2.74411
[1mStep[0m  [50/53], [94mLoss[0m : 2.95231

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.927, [92mTest[0m: 2.695, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.93520
[1mStep[0m  [5/53], [94mLoss[0m : 2.73488
[1mStep[0m  [10/53], [94mLoss[0m : 2.80851
[1mStep[0m  [15/53], [94mLoss[0m : 2.67424
[1mStep[0m  [20/53], [94mLoss[0m : 2.82255
[1mStep[0m  [25/53], [94mLoss[0m : 2.87835
[1mStep[0m  [30/53], [94mLoss[0m : 2.91716
[1mStep[0m  [35/53], [94mLoss[0m : 2.79199
[1mStep[0m  [40/53], [94mLoss[0m : 2.93591
[1mStep[0m  [45/53], [94mLoss[0m : 2.96889
[1mStep[0m  [50/53], [94mLoss[0m : 2.84833

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.796, [92mTest[0m: 2.577, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.12547
[1mStep[0m  [5/53], [94mLoss[0m : 2.98327
[1mStep[0m  [10/53], [94mLoss[0m : 2.56928
[1mStep[0m  [15/53], [94mLoss[0m : 2.86676
[1mStep[0m  [20/53], [94mLoss[0m : 2.92033
[1mStep[0m  [25/53], [94mLoss[0m : 2.46290
[1mStep[0m  [30/53], [94mLoss[0m : 2.68233
[1mStep[0m  [35/53], [94mLoss[0m : 2.85685
[1mStep[0m  [40/53], [94mLoss[0m : 2.67630
[1mStep[0m  [45/53], [94mLoss[0m : 2.69589
[1mStep[0m  [50/53], [94mLoss[0m : 2.60687

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.732, [92mTest[0m: 2.513, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.492
====================================

Phase 1 - Evaluation MAE:  2.491760226396414
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/53], [94mLoss[0m : 2.62564
[1mStep[0m  [5/53], [94mLoss[0m : 2.60515
[1mStep[0m  [10/53], [94mLoss[0m : 2.88700
[1mStep[0m  [15/53], [94mLoss[0m : 2.51497
[1mStep[0m  [20/53], [94mLoss[0m : 2.64350
[1mStep[0m  [25/53], [94mLoss[0m : 2.61992
[1mStep[0m  [30/53], [94mLoss[0m : 2.73210
[1mStep[0m  [35/53], [94mLoss[0m : 2.64999
[1mStep[0m  [40/53], [94mLoss[0m : 2.54937
[1mStep[0m  [45/53], [94mLoss[0m : 2.82740
[1mStep[0m  [50/53], [94mLoss[0m : 2.74907

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.700, [92mTest[0m: 2.481, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.62981
[1mStep[0m  [5/53], [94mLoss[0m : 2.55793
[1mStep[0m  [10/53], [94mLoss[0m : 2.73043
[1mStep[0m  [15/53], [94mLoss[0m : 2.74160
[1mStep[0m  [20/53], [94mLoss[0m : 2.93032
[1mStep[0m  [25/53], [94mLoss[0m : 2.52252
[1mStep[0m  [30/53], [94mLoss[0m : 2.55813
[1mStep[0m  [35/53], [94mLoss[0m : 2.86146
[1mStep[0m  [40/53], [94mLoss[0m : 2.57480
[1mStep[0m  [45/53], [94mLoss[0m : 2.70302
[1mStep[0m  [50/53], [94mLoss[0m : 2.74621

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.635, [92mTest[0m: 2.468, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.32440
[1mStep[0m  [5/53], [94mLoss[0m : 2.41080
[1mStep[0m  [10/53], [94mLoss[0m : 2.56209
[1mStep[0m  [15/53], [94mLoss[0m : 2.40679
[1mStep[0m  [20/53], [94mLoss[0m : 2.69500
[1mStep[0m  [25/53], [94mLoss[0m : 2.53740
[1mStep[0m  [30/53], [94mLoss[0m : 2.55170
[1mStep[0m  [35/53], [94mLoss[0m : 2.84796
[1mStep[0m  [40/53], [94mLoss[0m : 2.44214
[1mStep[0m  [45/53], [94mLoss[0m : 2.75504
[1mStep[0m  [50/53], [94mLoss[0m : 2.52092

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.595, [92mTest[0m: 2.439, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.56144
[1mStep[0m  [5/53], [94mLoss[0m : 2.71522
[1mStep[0m  [10/53], [94mLoss[0m : 2.46446
[1mStep[0m  [15/53], [94mLoss[0m : 2.34689
[1mStep[0m  [20/53], [94mLoss[0m : 2.47438
[1mStep[0m  [25/53], [94mLoss[0m : 2.58702
[1mStep[0m  [30/53], [94mLoss[0m : 2.58293
[1mStep[0m  [35/53], [94mLoss[0m : 2.55617
[1mStep[0m  [40/53], [94mLoss[0m : 2.48015
[1mStep[0m  [45/53], [94mLoss[0m : 2.64149
[1mStep[0m  [50/53], [94mLoss[0m : 2.45946

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.561, [92mTest[0m: 2.444, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.52969
[1mStep[0m  [5/53], [94mLoss[0m : 2.51410
[1mStep[0m  [10/53], [94mLoss[0m : 2.51173
[1mStep[0m  [15/53], [94mLoss[0m : 2.35912
[1mStep[0m  [20/53], [94mLoss[0m : 2.77921
[1mStep[0m  [25/53], [94mLoss[0m : 2.49382
[1mStep[0m  [30/53], [94mLoss[0m : 2.59113
[1mStep[0m  [35/53], [94mLoss[0m : 2.71961
[1mStep[0m  [40/53], [94mLoss[0m : 2.51280
[1mStep[0m  [45/53], [94mLoss[0m : 2.78707
[1mStep[0m  [50/53], [94mLoss[0m : 2.70498

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.546, [92mTest[0m: 2.456, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.37852
[1mStep[0m  [5/53], [94mLoss[0m : 2.83731
[1mStep[0m  [10/53], [94mLoss[0m : 2.58714
[1mStep[0m  [15/53], [94mLoss[0m : 2.54633
[1mStep[0m  [20/53], [94mLoss[0m : 2.63754
[1mStep[0m  [25/53], [94mLoss[0m : 2.39324
[1mStep[0m  [30/53], [94mLoss[0m : 2.50100
[1mStep[0m  [35/53], [94mLoss[0m : 2.66132
[1mStep[0m  [40/53], [94mLoss[0m : 2.75426
[1mStep[0m  [45/53], [94mLoss[0m : 2.26999
[1mStep[0m  [50/53], [94mLoss[0m : 2.63923

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.479, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.42300
[1mStep[0m  [5/53], [94mLoss[0m : 2.51399
[1mStep[0m  [10/53], [94mLoss[0m : 2.42258
[1mStep[0m  [15/53], [94mLoss[0m : 2.41196
[1mStep[0m  [20/53], [94mLoss[0m : 2.38094
[1mStep[0m  [25/53], [94mLoss[0m : 2.58874
[1mStep[0m  [30/53], [94mLoss[0m : 2.37941
[1mStep[0m  [35/53], [94mLoss[0m : 2.46845
[1mStep[0m  [40/53], [94mLoss[0m : 2.35433
[1mStep[0m  [45/53], [94mLoss[0m : 2.72549
[1mStep[0m  [50/53], [94mLoss[0m : 2.18647

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.438, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.44577
[1mStep[0m  [5/53], [94mLoss[0m : 2.50995
[1mStep[0m  [10/53], [94mLoss[0m : 2.66815
[1mStep[0m  [15/53], [94mLoss[0m : 2.45440
[1mStep[0m  [20/53], [94mLoss[0m : 2.29188
[1mStep[0m  [25/53], [94mLoss[0m : 2.29372
[1mStep[0m  [30/53], [94mLoss[0m : 2.58714
[1mStep[0m  [35/53], [94mLoss[0m : 2.60034
[1mStep[0m  [40/53], [94mLoss[0m : 2.49948
[1mStep[0m  [45/53], [94mLoss[0m : 2.42604
[1mStep[0m  [50/53], [94mLoss[0m : 2.32248

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.485, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.66117
[1mStep[0m  [5/53], [94mLoss[0m : 2.22499
[1mStep[0m  [10/53], [94mLoss[0m : 2.45284
[1mStep[0m  [15/53], [94mLoss[0m : 2.44445
[1mStep[0m  [20/53], [94mLoss[0m : 2.38084
[1mStep[0m  [25/53], [94mLoss[0m : 2.49774
[1mStep[0m  [30/53], [94mLoss[0m : 2.24383
[1mStep[0m  [35/53], [94mLoss[0m : 2.70194
[1mStep[0m  [40/53], [94mLoss[0m : 2.37310
[1mStep[0m  [45/53], [94mLoss[0m : 2.29296
[1mStep[0m  [50/53], [94mLoss[0m : 2.81318

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.426, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.30622
[1mStep[0m  [5/53], [94mLoss[0m : 2.30787
[1mStep[0m  [10/53], [94mLoss[0m : 2.21348
[1mStep[0m  [15/53], [94mLoss[0m : 2.68315
[1mStep[0m  [20/53], [94mLoss[0m : 2.60476
[1mStep[0m  [25/53], [94mLoss[0m : 2.41126
[1mStep[0m  [30/53], [94mLoss[0m : 2.45871
[1mStep[0m  [35/53], [94mLoss[0m : 2.42363
[1mStep[0m  [40/53], [94mLoss[0m : 2.44994
[1mStep[0m  [45/53], [94mLoss[0m : 2.32859
[1mStep[0m  [50/53], [94mLoss[0m : 2.38709

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.440, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.35418
[1mStep[0m  [5/53], [94mLoss[0m : 2.40169
[1mStep[0m  [10/53], [94mLoss[0m : 2.54092
[1mStep[0m  [15/53], [94mLoss[0m : 2.40882
[1mStep[0m  [20/53], [94mLoss[0m : 2.37885
[1mStep[0m  [25/53], [94mLoss[0m : 2.34975
[1mStep[0m  [30/53], [94mLoss[0m : 2.29651
[1mStep[0m  [35/53], [94mLoss[0m : 2.29186
[1mStep[0m  [40/53], [94mLoss[0m : 2.41325
[1mStep[0m  [45/53], [94mLoss[0m : 2.38856
[1mStep[0m  [50/53], [94mLoss[0m : 2.56903

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.407, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.66888
[1mStep[0m  [5/53], [94mLoss[0m : 2.38556
[1mStep[0m  [10/53], [94mLoss[0m : 2.39956
[1mStep[0m  [15/53], [94mLoss[0m : 2.43883
[1mStep[0m  [20/53], [94mLoss[0m : 2.33505
[1mStep[0m  [25/53], [94mLoss[0m : 2.06748
[1mStep[0m  [30/53], [94mLoss[0m : 2.27206
[1mStep[0m  [35/53], [94mLoss[0m : 2.57059
[1mStep[0m  [40/53], [94mLoss[0m : 2.28508
[1mStep[0m  [45/53], [94mLoss[0m : 2.25018
[1mStep[0m  [50/53], [94mLoss[0m : 2.29771

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.423, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.30376
[1mStep[0m  [5/53], [94mLoss[0m : 2.25618
[1mStep[0m  [10/53], [94mLoss[0m : 2.20776
[1mStep[0m  [15/53], [94mLoss[0m : 2.36903
[1mStep[0m  [20/53], [94mLoss[0m : 2.27854
[1mStep[0m  [25/53], [94mLoss[0m : 2.33908
[1mStep[0m  [30/53], [94mLoss[0m : 2.44870
[1mStep[0m  [35/53], [94mLoss[0m : 2.61591
[1mStep[0m  [40/53], [94mLoss[0m : 2.48409
[1mStep[0m  [45/53], [94mLoss[0m : 2.72267
[1mStep[0m  [50/53], [94mLoss[0m : 2.37780

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.395, [92mTest[0m: 2.410, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.58639
[1mStep[0m  [5/53], [94mLoss[0m : 2.24576
[1mStep[0m  [10/53], [94mLoss[0m : 2.41820
[1mStep[0m  [15/53], [94mLoss[0m : 2.28694
[1mStep[0m  [20/53], [94mLoss[0m : 2.39108
[1mStep[0m  [25/53], [94mLoss[0m : 2.42909
[1mStep[0m  [30/53], [94mLoss[0m : 2.19374
[1mStep[0m  [35/53], [94mLoss[0m : 2.60719
[1mStep[0m  [40/53], [94mLoss[0m : 2.32427
[1mStep[0m  [45/53], [94mLoss[0m : 2.47152
[1mStep[0m  [50/53], [94mLoss[0m : 2.43921

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.395, [92mTest[0m: 2.395, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.28207
[1mStep[0m  [5/53], [94mLoss[0m : 2.40996
[1mStep[0m  [10/53], [94mLoss[0m : 2.28780
[1mStep[0m  [15/53], [94mLoss[0m : 2.13494
[1mStep[0m  [20/53], [94mLoss[0m : 2.15683
[1mStep[0m  [25/53], [94mLoss[0m : 2.32313
[1mStep[0m  [30/53], [94mLoss[0m : 2.43307
[1mStep[0m  [35/53], [94mLoss[0m : 2.33210
[1mStep[0m  [40/53], [94mLoss[0m : 2.52897
[1mStep[0m  [45/53], [94mLoss[0m : 2.45477
[1mStep[0m  [50/53], [94mLoss[0m : 2.37795

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.375, [92mTest[0m: 2.400, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.45464
[1mStep[0m  [5/53], [94mLoss[0m : 2.57903
[1mStep[0m  [10/53], [94mLoss[0m : 1.97868
[1mStep[0m  [15/53], [94mLoss[0m : 2.30630
[1mStep[0m  [20/53], [94mLoss[0m : 2.18326
[1mStep[0m  [25/53], [94mLoss[0m : 2.33166
[1mStep[0m  [30/53], [94mLoss[0m : 2.31505
[1mStep[0m  [35/53], [94mLoss[0m : 2.02696
[1mStep[0m  [40/53], [94mLoss[0m : 2.26696
[1mStep[0m  [45/53], [94mLoss[0m : 2.29444
[1mStep[0m  [50/53], [94mLoss[0m : 2.29830

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.359, [92mTest[0m: 2.406, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.30884
[1mStep[0m  [5/53], [94mLoss[0m : 2.32434
[1mStep[0m  [10/53], [94mLoss[0m : 2.22959
[1mStep[0m  [15/53], [94mLoss[0m : 2.51487
[1mStep[0m  [20/53], [94mLoss[0m : 2.30679
[1mStep[0m  [25/53], [94mLoss[0m : 2.30254
[1mStep[0m  [30/53], [94mLoss[0m : 2.16833
[1mStep[0m  [35/53], [94mLoss[0m : 2.37943
[1mStep[0m  [40/53], [94mLoss[0m : 2.34246
[1mStep[0m  [45/53], [94mLoss[0m : 2.10123
[1mStep[0m  [50/53], [94mLoss[0m : 2.29316

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.334, [92mTest[0m: 2.392, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.14326
[1mStep[0m  [5/53], [94mLoss[0m : 2.27405
[1mStep[0m  [10/53], [94mLoss[0m : 2.42483
[1mStep[0m  [15/53], [94mLoss[0m : 2.24507
[1mStep[0m  [20/53], [94mLoss[0m : 2.15647
[1mStep[0m  [25/53], [94mLoss[0m : 2.39204
[1mStep[0m  [30/53], [94mLoss[0m : 2.41817
[1mStep[0m  [35/53], [94mLoss[0m : 2.33426
[1mStep[0m  [40/53], [94mLoss[0m : 2.39642
[1mStep[0m  [45/53], [94mLoss[0m : 2.27789
[1mStep[0m  [50/53], [94mLoss[0m : 2.42681

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.331, [92mTest[0m: 2.418, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.17251
[1mStep[0m  [5/53], [94mLoss[0m : 2.60419
[1mStep[0m  [10/53], [94mLoss[0m : 2.07508
[1mStep[0m  [15/53], [94mLoss[0m : 2.46535
[1mStep[0m  [20/53], [94mLoss[0m : 2.45201
[1mStep[0m  [25/53], [94mLoss[0m : 2.43247
[1mStep[0m  [30/53], [94mLoss[0m : 2.22932
[1mStep[0m  [35/53], [94mLoss[0m : 2.38502
[1mStep[0m  [40/53], [94mLoss[0m : 2.41541
[1mStep[0m  [45/53], [94mLoss[0m : 2.19123
[1mStep[0m  [50/53], [94mLoss[0m : 2.46233

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.304, [92mTest[0m: 2.421, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.06853
[1mStep[0m  [5/53], [94mLoss[0m : 2.60019
[1mStep[0m  [10/53], [94mLoss[0m : 2.22577
[1mStep[0m  [15/53], [94mLoss[0m : 2.39487
[1mStep[0m  [20/53], [94mLoss[0m : 2.29325
[1mStep[0m  [25/53], [94mLoss[0m : 2.10878
[1mStep[0m  [30/53], [94mLoss[0m : 2.21107
[1mStep[0m  [35/53], [94mLoss[0m : 2.42602
[1mStep[0m  [40/53], [94mLoss[0m : 2.11829
[1mStep[0m  [45/53], [94mLoss[0m : 2.17908
[1mStep[0m  [50/53], [94mLoss[0m : 2.15277

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.286, [92mTest[0m: 2.412, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.26812
[1mStep[0m  [5/53], [94mLoss[0m : 2.50226
[1mStep[0m  [10/53], [94mLoss[0m : 2.23163
[1mStep[0m  [15/53], [94mLoss[0m : 2.43248
[1mStep[0m  [20/53], [94mLoss[0m : 2.29987
[1mStep[0m  [25/53], [94mLoss[0m : 2.28262
[1mStep[0m  [30/53], [94mLoss[0m : 2.48341
[1mStep[0m  [35/53], [94mLoss[0m : 2.24871
[1mStep[0m  [40/53], [94mLoss[0m : 2.21534
[1mStep[0m  [45/53], [94mLoss[0m : 2.42306
[1mStep[0m  [50/53], [94mLoss[0m : 2.37425

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.264, [92mTest[0m: 2.396, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.31215
[1mStep[0m  [5/53], [94mLoss[0m : 2.13809
[1mStep[0m  [10/53], [94mLoss[0m : 2.11348
[1mStep[0m  [15/53], [94mLoss[0m : 2.05619
[1mStep[0m  [20/53], [94mLoss[0m : 2.28530
[1mStep[0m  [25/53], [94mLoss[0m : 2.14289
[1mStep[0m  [30/53], [94mLoss[0m : 2.12552
[1mStep[0m  [35/53], [94mLoss[0m : 2.16911
[1mStep[0m  [40/53], [94mLoss[0m : 2.38779
[1mStep[0m  [45/53], [94mLoss[0m : 2.22067
[1mStep[0m  [50/53], [94mLoss[0m : 2.34527

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.259, [92mTest[0m: 2.387, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.30227
[1mStep[0m  [5/53], [94mLoss[0m : 2.19278
[1mStep[0m  [10/53], [94mLoss[0m : 2.18892
[1mStep[0m  [15/53], [94mLoss[0m : 2.14793
[1mStep[0m  [20/53], [94mLoss[0m : 2.20662
[1mStep[0m  [25/53], [94mLoss[0m : 2.44385
[1mStep[0m  [30/53], [94mLoss[0m : 2.35820
[1mStep[0m  [35/53], [94mLoss[0m : 2.26050
[1mStep[0m  [40/53], [94mLoss[0m : 2.20001
[1mStep[0m  [45/53], [94mLoss[0m : 2.32140
[1mStep[0m  [50/53], [94mLoss[0m : 2.44411

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.226, [92mTest[0m: 2.406, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.20691
[1mStep[0m  [5/53], [94mLoss[0m : 1.86691
[1mStep[0m  [10/53], [94mLoss[0m : 2.16995
[1mStep[0m  [15/53], [94mLoss[0m : 2.25990
[1mStep[0m  [20/53], [94mLoss[0m : 2.27604
[1mStep[0m  [25/53], [94mLoss[0m : 2.03779
[1mStep[0m  [30/53], [94mLoss[0m : 2.17456
[1mStep[0m  [35/53], [94mLoss[0m : 2.26245
[1mStep[0m  [40/53], [94mLoss[0m : 2.13771
[1mStep[0m  [45/53], [94mLoss[0m : 2.20337
[1mStep[0m  [50/53], [94mLoss[0m : 2.16489

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.202, [92mTest[0m: 2.408, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.16639
[1mStep[0m  [5/53], [94mLoss[0m : 2.22440
[1mStep[0m  [10/53], [94mLoss[0m : 2.09830
[1mStep[0m  [15/53], [94mLoss[0m : 1.93687
[1mStep[0m  [20/53], [94mLoss[0m : 2.22360
[1mStep[0m  [25/53], [94mLoss[0m : 2.18555
[1mStep[0m  [30/53], [94mLoss[0m : 2.19776
[1mStep[0m  [35/53], [94mLoss[0m : 1.93278
[1mStep[0m  [40/53], [94mLoss[0m : 2.18999
[1mStep[0m  [45/53], [94mLoss[0m : 2.27936
[1mStep[0m  [50/53], [94mLoss[0m : 2.33897

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.210, [92mTest[0m: 2.405, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.27792
[1mStep[0m  [5/53], [94mLoss[0m : 2.16015
[1mStep[0m  [10/53], [94mLoss[0m : 2.17240
[1mStep[0m  [15/53], [94mLoss[0m : 2.01134
[1mStep[0m  [20/53], [94mLoss[0m : 2.04389
[1mStep[0m  [25/53], [94mLoss[0m : 2.20885
[1mStep[0m  [30/53], [94mLoss[0m : 2.17081
[1mStep[0m  [35/53], [94mLoss[0m : 2.28144
[1mStep[0m  [40/53], [94mLoss[0m : 2.21595
[1mStep[0m  [45/53], [94mLoss[0m : 2.05410
[1mStep[0m  [50/53], [94mLoss[0m : 2.30344

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.176, [92mTest[0m: 2.394, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.15909
[1mStep[0m  [5/53], [94mLoss[0m : 2.26271
[1mStep[0m  [10/53], [94mLoss[0m : 2.09117
[1mStep[0m  [15/53], [94mLoss[0m : 2.19621
[1mStep[0m  [20/53], [94mLoss[0m : 1.90832
[1mStep[0m  [25/53], [94mLoss[0m : 2.20462
[1mStep[0m  [30/53], [94mLoss[0m : 2.22526
[1mStep[0m  [35/53], [94mLoss[0m : 2.21212
[1mStep[0m  [40/53], [94mLoss[0m : 2.31751
[1mStep[0m  [45/53], [94mLoss[0m : 2.19093
[1mStep[0m  [50/53], [94mLoss[0m : 2.31681

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.157, [92mTest[0m: 2.402, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.19514
[1mStep[0m  [5/53], [94mLoss[0m : 2.25087
[1mStep[0m  [10/53], [94mLoss[0m : 1.93972
[1mStep[0m  [15/53], [94mLoss[0m : 2.21520
[1mStep[0m  [20/53], [94mLoss[0m : 2.16689
[1mStep[0m  [25/53], [94mLoss[0m : 2.23781
[1mStep[0m  [30/53], [94mLoss[0m : 2.35509
[1mStep[0m  [35/53], [94mLoss[0m : 2.04409
[1mStep[0m  [40/53], [94mLoss[0m : 2.29753
[1mStep[0m  [45/53], [94mLoss[0m : 2.45841
[1mStep[0m  [50/53], [94mLoss[0m : 1.92761

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.144, [92mTest[0m: 2.393, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.18023
[1mStep[0m  [5/53], [94mLoss[0m : 2.16337
[1mStep[0m  [10/53], [94mLoss[0m : 2.20604
[1mStep[0m  [15/53], [94mLoss[0m : 2.24671
[1mStep[0m  [20/53], [94mLoss[0m : 2.03569
[1mStep[0m  [25/53], [94mLoss[0m : 2.20239
[1mStep[0m  [30/53], [94mLoss[0m : 2.17126
[1mStep[0m  [35/53], [94mLoss[0m : 2.03586
[1mStep[0m  [40/53], [94mLoss[0m : 2.11863
[1mStep[0m  [45/53], [94mLoss[0m : 1.99403
[1mStep[0m  [50/53], [94mLoss[0m : 2.16831

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.148, [92mTest[0m: 2.402, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.15015
[1mStep[0m  [5/53], [94mLoss[0m : 2.06281
[1mStep[0m  [10/53], [94mLoss[0m : 2.02244
[1mStep[0m  [15/53], [94mLoss[0m : 2.09420
[1mStep[0m  [20/53], [94mLoss[0m : 2.19346
[1mStep[0m  [25/53], [94mLoss[0m : 2.12677
[1mStep[0m  [30/53], [94mLoss[0m : 2.28562
[1mStep[0m  [35/53], [94mLoss[0m : 2.09399
[1mStep[0m  [40/53], [94mLoss[0m : 1.89900
[1mStep[0m  [45/53], [94mLoss[0m : 2.06557
[1mStep[0m  [50/53], [94mLoss[0m : 2.04156

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.104, [92mTest[0m: 2.407, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.402
====================================

Phase 2 - Evaluation MAE:  2.4016650915145874
MAE score P1       2.49176
MAE score P2      2.401665
loss              2.104137
learning_rate     0.002575
batch_size             256
hidden_sizes         [100]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.1
weight_decay         0.001
Name: 2, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/53], [94mLoss[0m : 10.67405
[1mStep[0m  [5/53], [94mLoss[0m : 11.06661
[1mStep[0m  [10/53], [94mLoss[0m : 10.34579
[1mStep[0m  [15/53], [94mLoss[0m : 10.78156
[1mStep[0m  [20/53], [94mLoss[0m : 10.96998
[1mStep[0m  [25/53], [94mLoss[0m : 10.99704
[1mStep[0m  [30/53], [94mLoss[0m : 11.11280
[1mStep[0m  [35/53], [94mLoss[0m : 11.00944
[1mStep[0m  [40/53], [94mLoss[0m : 11.13422
[1mStep[0m  [45/53], [94mLoss[0m : 10.77790
[1mStep[0m  [50/53], [94mLoss[0m : 10.45914

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.866, [92mTest[0m: 10.932, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.51388
[1mStep[0m  [5/53], [94mLoss[0m : 10.72494
[1mStep[0m  [10/53], [94mLoss[0m : 10.47596
[1mStep[0m  [15/53], [94mLoss[0m : 10.85175
[1mStep[0m  [20/53], [94mLoss[0m : 10.51231
[1mStep[0m  [25/53], [94mLoss[0m : 10.41158
[1mStep[0m  [30/53], [94mLoss[0m : 10.69542
[1mStep[0m  [35/53], [94mLoss[0m : 10.52941
[1mStep[0m  [40/53], [94mLoss[0m : 10.62745
[1mStep[0m  [45/53], [94mLoss[0m : 10.13145
[1mStep[0m  [50/53], [94mLoss[0m : 10.33110

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.670, [92mTest[0m: 10.706, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.95591
[1mStep[0m  [5/53], [94mLoss[0m : 10.48273
[1mStep[0m  [10/53], [94mLoss[0m : 10.29578
[1mStep[0m  [15/53], [94mLoss[0m : 11.14643
[1mStep[0m  [20/53], [94mLoss[0m : 10.77384
[1mStep[0m  [25/53], [94mLoss[0m : 10.52899
[1mStep[0m  [30/53], [94mLoss[0m : 10.55548
[1mStep[0m  [35/53], [94mLoss[0m : 10.07350
[1mStep[0m  [40/53], [94mLoss[0m : 10.34810
[1mStep[0m  [45/53], [94mLoss[0m : 10.70331
[1mStep[0m  [50/53], [94mLoss[0m : 10.43587

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.483, [92mTest[0m: 10.453, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.74403
[1mStep[0m  [5/53], [94mLoss[0m : 10.27206
[1mStep[0m  [10/53], [94mLoss[0m : 10.77848
[1mStep[0m  [15/53], [94mLoss[0m : 10.39026
[1mStep[0m  [20/53], [94mLoss[0m : 10.56529
[1mStep[0m  [25/53], [94mLoss[0m : 10.20737
[1mStep[0m  [30/53], [94mLoss[0m : 10.38808
[1mStep[0m  [35/53], [94mLoss[0m : 10.41680
[1mStep[0m  [40/53], [94mLoss[0m : 10.08986
[1mStep[0m  [45/53], [94mLoss[0m : 10.00201
[1mStep[0m  [50/53], [94mLoss[0m : 10.41233

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.288, [92mTest[0m: 10.212, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.18488
[1mStep[0m  [5/53], [94mLoss[0m : 10.52086
[1mStep[0m  [10/53], [94mLoss[0m : 10.25509
[1mStep[0m  [15/53], [94mLoss[0m : 9.88726
[1mStep[0m  [20/53], [94mLoss[0m : 9.96641
[1mStep[0m  [25/53], [94mLoss[0m : 10.16600
[1mStep[0m  [30/53], [94mLoss[0m : 9.98171
[1mStep[0m  [35/53], [94mLoss[0m : 10.19316
[1mStep[0m  [40/53], [94mLoss[0m : 9.94176
[1mStep[0m  [45/53], [94mLoss[0m : 10.56420
[1mStep[0m  [50/53], [94mLoss[0m : 9.91304

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.076, [92mTest[0m: 10.005, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.12202
[1mStep[0m  [5/53], [94mLoss[0m : 10.16805
[1mStep[0m  [10/53], [94mLoss[0m : 10.12753
[1mStep[0m  [15/53], [94mLoss[0m : 10.38290
[1mStep[0m  [20/53], [94mLoss[0m : 10.20187
[1mStep[0m  [25/53], [94mLoss[0m : 9.94409
[1mStep[0m  [30/53], [94mLoss[0m : 9.93851
[1mStep[0m  [35/53], [94mLoss[0m : 9.78005
[1mStep[0m  [40/53], [94mLoss[0m : 9.68312
[1mStep[0m  [45/53], [94mLoss[0m : 9.84125
[1mStep[0m  [50/53], [94mLoss[0m : 9.77347

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 9.868, [92mTest[0m: 9.755, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.69824
[1mStep[0m  [5/53], [94mLoss[0m : 9.40167
[1mStep[0m  [10/53], [94mLoss[0m : 9.61503
[1mStep[0m  [15/53], [94mLoss[0m : 9.75280
[1mStep[0m  [20/53], [94mLoss[0m : 9.79028
[1mStep[0m  [25/53], [94mLoss[0m : 9.86309
[1mStep[0m  [30/53], [94mLoss[0m : 9.63915
[1mStep[0m  [35/53], [94mLoss[0m : 9.67794
[1mStep[0m  [40/53], [94mLoss[0m : 9.80346
[1mStep[0m  [45/53], [94mLoss[0m : 9.63186
[1mStep[0m  [50/53], [94mLoss[0m : 9.68599

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 9.643, [92mTest[0m: 9.481, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.31242
[1mStep[0m  [5/53], [94mLoss[0m : 9.46691
[1mStep[0m  [10/53], [94mLoss[0m : 9.53835
[1mStep[0m  [15/53], [94mLoss[0m : 9.53936
[1mStep[0m  [20/53], [94mLoss[0m : 9.24088
[1mStep[0m  [25/53], [94mLoss[0m : 9.50993
[1mStep[0m  [30/53], [94mLoss[0m : 9.40883
[1mStep[0m  [35/53], [94mLoss[0m : 9.55849
[1mStep[0m  [40/53], [94mLoss[0m : 9.23116
[1mStep[0m  [45/53], [94mLoss[0m : 9.19457
[1mStep[0m  [50/53], [94mLoss[0m : 9.59702

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 9.393, [92mTest[0m: 9.168, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.12172
[1mStep[0m  [5/53], [94mLoss[0m : 9.68265
[1mStep[0m  [10/53], [94mLoss[0m : 9.27314
[1mStep[0m  [15/53], [94mLoss[0m : 9.44534
[1mStep[0m  [20/53], [94mLoss[0m : 9.33716
[1mStep[0m  [25/53], [94mLoss[0m : 9.00798
[1mStep[0m  [30/53], [94mLoss[0m : 9.07630
[1mStep[0m  [35/53], [94mLoss[0m : 9.06602
[1mStep[0m  [40/53], [94mLoss[0m : 9.07142
[1mStep[0m  [45/53], [94mLoss[0m : 8.88243
[1mStep[0m  [50/53], [94mLoss[0m : 8.92793

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 9.140, [92mTest[0m: 8.939, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.76997
[1mStep[0m  [5/53], [94mLoss[0m : 8.83446
[1mStep[0m  [10/53], [94mLoss[0m : 8.61435
[1mStep[0m  [15/53], [94mLoss[0m : 8.72952
[1mStep[0m  [20/53], [94mLoss[0m : 9.20889
[1mStep[0m  [25/53], [94mLoss[0m : 8.84120
[1mStep[0m  [30/53], [94mLoss[0m : 8.83545
[1mStep[0m  [35/53], [94mLoss[0m : 9.12692
[1mStep[0m  [40/53], [94mLoss[0m : 8.78138
[1mStep[0m  [45/53], [94mLoss[0m : 9.12862
[1mStep[0m  [50/53], [94mLoss[0m : 9.17629

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 8.842, [92mTest[0m: 8.522, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.46354
[1mStep[0m  [5/53], [94mLoss[0m : 8.79268
[1mStep[0m  [10/53], [94mLoss[0m : 8.50388
[1mStep[0m  [15/53], [94mLoss[0m : 8.39191
[1mStep[0m  [20/53], [94mLoss[0m : 8.43546
[1mStep[0m  [25/53], [94mLoss[0m : 8.42078
[1mStep[0m  [30/53], [94mLoss[0m : 8.49824
[1mStep[0m  [35/53], [94mLoss[0m : 8.92908
[1mStep[0m  [40/53], [94mLoss[0m : 8.27757
[1mStep[0m  [45/53], [94mLoss[0m : 8.31006
[1mStep[0m  [50/53], [94mLoss[0m : 7.93025

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 8.514, [92mTest[0m: 8.176, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.10284
[1mStep[0m  [5/53], [94mLoss[0m : 8.20509
[1mStep[0m  [10/53], [94mLoss[0m : 8.17096
[1mStep[0m  [15/53], [94mLoss[0m : 7.67989
[1mStep[0m  [20/53], [94mLoss[0m : 8.43751
[1mStep[0m  [25/53], [94mLoss[0m : 8.12593
[1mStep[0m  [30/53], [94mLoss[0m : 8.10886
[1mStep[0m  [35/53], [94mLoss[0m : 8.08763
[1mStep[0m  [40/53], [94mLoss[0m : 8.10018
[1mStep[0m  [45/53], [94mLoss[0m : 8.25318
[1mStep[0m  [50/53], [94mLoss[0m : 8.03093

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 8.149, [92mTest[0m: 7.792, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.10328
[1mStep[0m  [5/53], [94mLoss[0m : 7.64379
[1mStep[0m  [10/53], [94mLoss[0m : 8.20675
[1mStep[0m  [15/53], [94mLoss[0m : 7.67240
[1mStep[0m  [20/53], [94mLoss[0m : 7.72784
[1mStep[0m  [25/53], [94mLoss[0m : 7.77366
[1mStep[0m  [30/53], [94mLoss[0m : 7.79308
[1mStep[0m  [35/53], [94mLoss[0m : 8.00714
[1mStep[0m  [40/53], [94mLoss[0m : 7.59638
[1mStep[0m  [45/53], [94mLoss[0m : 7.48616
[1mStep[0m  [50/53], [94mLoss[0m : 7.64003

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 7.742, [92mTest[0m: 7.375, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.32284
[1mStep[0m  [5/53], [94mLoss[0m : 7.88764
[1mStep[0m  [10/53], [94mLoss[0m : 7.51728
[1mStep[0m  [15/53], [94mLoss[0m : 6.96770
[1mStep[0m  [20/53], [94mLoss[0m : 7.39456
[1mStep[0m  [25/53], [94mLoss[0m : 7.47423
[1mStep[0m  [30/53], [94mLoss[0m : 7.42503
[1mStep[0m  [35/53], [94mLoss[0m : 7.52307
[1mStep[0m  [40/53], [94mLoss[0m : 7.07682
[1mStep[0m  [45/53], [94mLoss[0m : 7.32274
[1mStep[0m  [50/53], [94mLoss[0m : 6.94663

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 7.341, [92mTest[0m: 6.862, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.13412
[1mStep[0m  [5/53], [94mLoss[0m : 6.93419
[1mStep[0m  [10/53], [94mLoss[0m : 7.36255
[1mStep[0m  [15/53], [94mLoss[0m : 6.57049
[1mStep[0m  [20/53], [94mLoss[0m : 7.09143
[1mStep[0m  [25/53], [94mLoss[0m : 7.07846
[1mStep[0m  [30/53], [94mLoss[0m : 7.39748
[1mStep[0m  [35/53], [94mLoss[0m : 7.58971
[1mStep[0m  [40/53], [94mLoss[0m : 6.98043
[1mStep[0m  [45/53], [94mLoss[0m : 6.81607
[1mStep[0m  [50/53], [94mLoss[0m : 6.75870

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 6.934, [92mTest[0m: 6.433, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.23837
[1mStep[0m  [5/53], [94mLoss[0m : 6.61145
[1mStep[0m  [10/53], [94mLoss[0m : 6.64029
[1mStep[0m  [15/53], [94mLoss[0m : 6.78193
[1mStep[0m  [20/53], [94mLoss[0m : 6.39469
[1mStep[0m  [25/53], [94mLoss[0m : 6.81599
[1mStep[0m  [30/53], [94mLoss[0m : 6.65449
[1mStep[0m  [35/53], [94mLoss[0m : 6.29461
[1mStep[0m  [40/53], [94mLoss[0m : 6.18643
[1mStep[0m  [45/53], [94mLoss[0m : 6.33981
[1mStep[0m  [50/53], [94mLoss[0m : 6.50569

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 6.568, [92mTest[0m: 6.088, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.43912
[1mStep[0m  [5/53], [94mLoss[0m : 6.58997
[1mStep[0m  [10/53], [94mLoss[0m : 6.20873
[1mStep[0m  [15/53], [94mLoss[0m : 6.25900
[1mStep[0m  [20/53], [94mLoss[0m : 6.24950
[1mStep[0m  [25/53], [94mLoss[0m : 6.34338
[1mStep[0m  [30/53], [94mLoss[0m : 6.10538
[1mStep[0m  [35/53], [94mLoss[0m : 6.12179
[1mStep[0m  [40/53], [94mLoss[0m : 6.12939
[1mStep[0m  [45/53], [94mLoss[0m : 6.09633
[1mStep[0m  [50/53], [94mLoss[0m : 6.30515

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 6.199, [92mTest[0m: 5.690, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.20852
[1mStep[0m  [5/53], [94mLoss[0m : 6.13249
[1mStep[0m  [10/53], [94mLoss[0m : 5.90952
[1mStep[0m  [15/53], [94mLoss[0m : 5.94356
[1mStep[0m  [20/53], [94mLoss[0m : 5.79475
[1mStep[0m  [25/53], [94mLoss[0m : 5.75175
[1mStep[0m  [30/53], [94mLoss[0m : 5.77791
[1mStep[0m  [35/53], [94mLoss[0m : 5.67487
[1mStep[0m  [40/53], [94mLoss[0m : 5.50220
[1mStep[0m  [45/53], [94mLoss[0m : 5.97961
[1mStep[0m  [50/53], [94mLoss[0m : 5.77471

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 5.858, [92mTest[0m: 5.266, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 5.71442
[1mStep[0m  [5/53], [94mLoss[0m : 5.84477
[1mStep[0m  [10/53], [94mLoss[0m : 5.88780
[1mStep[0m  [15/53], [94mLoss[0m : 5.53010
[1mStep[0m  [20/53], [94mLoss[0m : 5.44546
[1mStep[0m  [25/53], [94mLoss[0m : 5.46669
[1mStep[0m  [30/53], [94mLoss[0m : 5.32252
[1mStep[0m  [35/53], [94mLoss[0m : 5.32506
[1mStep[0m  [40/53], [94mLoss[0m : 5.06433
[1mStep[0m  [45/53], [94mLoss[0m : 5.84135
[1mStep[0m  [50/53], [94mLoss[0m : 5.69701

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 5.532, [92mTest[0m: 4.964, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 5.57987
[1mStep[0m  [5/53], [94mLoss[0m : 5.40975
[1mStep[0m  [10/53], [94mLoss[0m : 4.95878
[1mStep[0m  [15/53], [94mLoss[0m : 5.08217
[1mStep[0m  [20/53], [94mLoss[0m : 5.43569
[1mStep[0m  [25/53], [94mLoss[0m : 5.35230
[1mStep[0m  [30/53], [94mLoss[0m : 4.81474
[1mStep[0m  [35/53], [94mLoss[0m : 4.96361
[1mStep[0m  [40/53], [94mLoss[0m : 5.04253
[1mStep[0m  [45/53], [94mLoss[0m : 5.12225
[1mStep[0m  [50/53], [94mLoss[0m : 5.05451

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 5.231, [92mTest[0m: 4.771, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 4.93518
[1mStep[0m  [5/53], [94mLoss[0m : 5.02018
[1mStep[0m  [10/53], [94mLoss[0m : 4.83056
[1mStep[0m  [15/53], [94mLoss[0m : 4.83214
[1mStep[0m  [20/53], [94mLoss[0m : 4.65064
[1mStep[0m  [25/53], [94mLoss[0m : 5.13708
[1mStep[0m  [30/53], [94mLoss[0m : 4.73313
[1mStep[0m  [35/53], [94mLoss[0m : 4.91163
[1mStep[0m  [40/53], [94mLoss[0m : 4.94332
[1mStep[0m  [45/53], [94mLoss[0m : 4.64574
[1mStep[0m  [50/53], [94mLoss[0m : 4.87774

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 4.909, [92mTest[0m: 4.452, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 4.49850
[1mStep[0m  [5/53], [94mLoss[0m : 4.76152
[1mStep[0m  [10/53], [94mLoss[0m : 4.66525
[1mStep[0m  [15/53], [94mLoss[0m : 4.85496
[1mStep[0m  [20/53], [94mLoss[0m : 4.63070
[1mStep[0m  [25/53], [94mLoss[0m : 4.67019
[1mStep[0m  [30/53], [94mLoss[0m : 4.28420
[1mStep[0m  [35/53], [94mLoss[0m : 4.40526
[1mStep[0m  [40/53], [94mLoss[0m : 4.80601
[1mStep[0m  [45/53], [94mLoss[0m : 4.19576
[1mStep[0m  [50/53], [94mLoss[0m : 4.46239

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 4.610, [92mTest[0m: 4.133, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 4.61380
[1mStep[0m  [5/53], [94mLoss[0m : 4.47186
[1mStep[0m  [10/53], [94mLoss[0m : 4.27314
[1mStep[0m  [15/53], [94mLoss[0m : 4.17722
[1mStep[0m  [20/53], [94mLoss[0m : 4.33816
[1mStep[0m  [25/53], [94mLoss[0m : 4.60861
[1mStep[0m  [30/53], [94mLoss[0m : 4.38731
[1mStep[0m  [35/53], [94mLoss[0m : 4.33528
[1mStep[0m  [40/53], [94mLoss[0m : 4.39166
[1mStep[0m  [45/53], [94mLoss[0m : 4.53410
[1mStep[0m  [50/53], [94mLoss[0m : 4.42973

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 4.378, [92mTest[0m: 3.858, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 4.03859
[1mStep[0m  [5/53], [94mLoss[0m : 4.36212
[1mStep[0m  [10/53], [94mLoss[0m : 4.42216
[1mStep[0m  [15/53], [94mLoss[0m : 4.38457
[1mStep[0m  [20/53], [94mLoss[0m : 4.01558
[1mStep[0m  [25/53], [94mLoss[0m : 3.97714
[1mStep[0m  [30/53], [94mLoss[0m : 3.91588
[1mStep[0m  [35/53], [94mLoss[0m : 4.13300
[1mStep[0m  [40/53], [94mLoss[0m : 3.89557
[1mStep[0m  [45/53], [94mLoss[0m : 3.97727
[1mStep[0m  [50/53], [94mLoss[0m : 4.08055

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 4.130, [92mTest[0m: 3.801, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 4.04543
[1mStep[0m  [5/53], [94mLoss[0m : 3.95543
[1mStep[0m  [10/53], [94mLoss[0m : 3.90664
[1mStep[0m  [15/53], [94mLoss[0m : 3.84355
[1mStep[0m  [20/53], [94mLoss[0m : 3.93543
[1mStep[0m  [25/53], [94mLoss[0m : 4.10023
[1mStep[0m  [30/53], [94mLoss[0m : 3.89087
[1mStep[0m  [35/53], [94mLoss[0m : 4.13427
[1mStep[0m  [40/53], [94mLoss[0m : 3.81773
[1mStep[0m  [45/53], [94mLoss[0m : 3.92983
[1mStep[0m  [50/53], [94mLoss[0m : 3.94386

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 3.875, [92mTest[0m: 3.546, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 4.04810
[1mStep[0m  [5/53], [94mLoss[0m : 3.91800
[1mStep[0m  [10/53], [94mLoss[0m : 3.70451
[1mStep[0m  [15/53], [94mLoss[0m : 3.71777
[1mStep[0m  [20/53], [94mLoss[0m : 3.46845
[1mStep[0m  [25/53], [94mLoss[0m : 3.39290
[1mStep[0m  [30/53], [94mLoss[0m : 3.60677
[1mStep[0m  [35/53], [94mLoss[0m : 3.75049
[1mStep[0m  [40/53], [94mLoss[0m : 3.64065
[1mStep[0m  [45/53], [94mLoss[0m : 3.59185
[1mStep[0m  [50/53], [94mLoss[0m : 3.32181

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 3.633, [92mTest[0m: 3.331, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.59421
[1mStep[0m  [5/53], [94mLoss[0m : 3.64236
[1mStep[0m  [10/53], [94mLoss[0m : 3.70224
[1mStep[0m  [15/53], [94mLoss[0m : 3.52010
[1mStep[0m  [20/53], [94mLoss[0m : 3.57503
[1mStep[0m  [25/53], [94mLoss[0m : 3.38046
[1mStep[0m  [30/53], [94mLoss[0m : 3.19995
[1mStep[0m  [35/53], [94mLoss[0m : 3.21801
[1mStep[0m  [40/53], [94mLoss[0m : 3.15878
[1mStep[0m  [45/53], [94mLoss[0m : 3.19205
[1mStep[0m  [50/53], [94mLoss[0m : 3.44203

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 3.382, [92mTest[0m: 3.145, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.23534
[1mStep[0m  [5/53], [94mLoss[0m : 3.50552
[1mStep[0m  [10/53], [94mLoss[0m : 3.34817
[1mStep[0m  [15/53], [94mLoss[0m : 3.02719
[1mStep[0m  [20/53], [94mLoss[0m : 3.19814
[1mStep[0m  [25/53], [94mLoss[0m : 3.33201
[1mStep[0m  [30/53], [94mLoss[0m : 2.93097
[1mStep[0m  [35/53], [94mLoss[0m : 3.30040
[1mStep[0m  [40/53], [94mLoss[0m : 3.22304
[1mStep[0m  [45/53], [94mLoss[0m : 2.99346
[1mStep[0m  [50/53], [94mLoss[0m : 3.17781

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 3.151, [92mTest[0m: 2.938, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.08050
[1mStep[0m  [5/53], [94mLoss[0m : 2.66325
[1mStep[0m  [10/53], [94mLoss[0m : 2.90029
[1mStep[0m  [15/53], [94mLoss[0m : 2.98707
[1mStep[0m  [20/53], [94mLoss[0m : 2.99168
[1mStep[0m  [25/53], [94mLoss[0m : 3.08605
[1mStep[0m  [30/53], [94mLoss[0m : 2.80978
[1mStep[0m  [35/53], [94mLoss[0m : 2.90682
[1mStep[0m  [40/53], [94mLoss[0m : 3.12362
[1mStep[0m  [45/53], [94mLoss[0m : 3.23127
[1mStep[0m  [50/53], [94mLoss[0m : 2.61509

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 3.002, [92mTest[0m: 2.810, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.12690
[1mStep[0m  [5/53], [94mLoss[0m : 2.93398
[1mStep[0m  [10/53], [94mLoss[0m : 3.06531
[1mStep[0m  [15/53], [94mLoss[0m : 2.89636
[1mStep[0m  [20/53], [94mLoss[0m : 2.91621
[1mStep[0m  [25/53], [94mLoss[0m : 2.96860
[1mStep[0m  [30/53], [94mLoss[0m : 3.03563
[1mStep[0m  [35/53], [94mLoss[0m : 2.65033
[1mStep[0m  [40/53], [94mLoss[0m : 3.20395
[1mStep[0m  [45/53], [94mLoss[0m : 2.80707
[1mStep[0m  [50/53], [94mLoss[0m : 2.49744

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.867, [92mTest[0m: 2.665, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.598
====================================

Phase 1 - Evaluation MAE:  2.598304372567397
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/53], [94mLoss[0m : 2.63215
[1mStep[0m  [5/53], [94mLoss[0m : 2.75592
[1mStep[0m  [10/53], [94mLoss[0m : 2.83635
[1mStep[0m  [15/53], [94mLoss[0m : 2.83780
[1mStep[0m  [20/53], [94mLoss[0m : 2.63518
[1mStep[0m  [25/53], [94mLoss[0m : 2.54928
[1mStep[0m  [30/53], [94mLoss[0m : 2.76838
[1mStep[0m  [35/53], [94mLoss[0m : 2.61365
[1mStep[0m  [40/53], [94mLoss[0m : 2.84229
[1mStep[0m  [45/53], [94mLoss[0m : 2.82817
[1mStep[0m  [50/53], [94mLoss[0m : 2.98231

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.807, [92mTest[0m: 2.600, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.79007
[1mStep[0m  [5/53], [94mLoss[0m : 2.71308
[1mStep[0m  [10/53], [94mLoss[0m : 2.72022
[1mStep[0m  [15/53], [94mLoss[0m : 2.98899
[1mStep[0m  [20/53], [94mLoss[0m : 2.91958
[1mStep[0m  [25/53], [94mLoss[0m : 2.81810
[1mStep[0m  [30/53], [94mLoss[0m : 2.81175
[1mStep[0m  [35/53], [94mLoss[0m : 2.74126
[1mStep[0m  [40/53], [94mLoss[0m : 2.79000
[1mStep[0m  [45/53], [94mLoss[0m : 2.67110
[1mStep[0m  [50/53], [94mLoss[0m : 2.73148

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.752, [92mTest[0m: 2.568, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.55838
[1mStep[0m  [5/53], [94mLoss[0m : 2.66008
[1mStep[0m  [10/53], [94mLoss[0m : 2.71957
[1mStep[0m  [15/53], [94mLoss[0m : 2.87175
[1mStep[0m  [20/53], [94mLoss[0m : 2.64494
[1mStep[0m  [25/53], [94mLoss[0m : 2.63583
[1mStep[0m  [30/53], [94mLoss[0m : 2.67806
[1mStep[0m  [35/53], [94mLoss[0m : 2.76866
[1mStep[0m  [40/53], [94mLoss[0m : 2.85733
[1mStep[0m  [45/53], [94mLoss[0m : 2.79761
[1mStep[0m  [50/53], [94mLoss[0m : 2.61374

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.687, [92mTest[0m: 2.540, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.66981
[1mStep[0m  [5/53], [94mLoss[0m : 2.72506
[1mStep[0m  [10/53], [94mLoss[0m : 2.70239
[1mStep[0m  [15/53], [94mLoss[0m : 2.67238
[1mStep[0m  [20/53], [94mLoss[0m : 2.48154
[1mStep[0m  [25/53], [94mLoss[0m : 2.60044
[1mStep[0m  [30/53], [94mLoss[0m : 2.49862
[1mStep[0m  [35/53], [94mLoss[0m : 2.39443
[1mStep[0m  [40/53], [94mLoss[0m : 2.60794
[1mStep[0m  [45/53], [94mLoss[0m : 2.65377
[1mStep[0m  [50/53], [94mLoss[0m : 2.58329

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.627, [92mTest[0m: 2.561, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.67496
[1mStep[0m  [5/53], [94mLoss[0m : 2.52807
[1mStep[0m  [10/53], [94mLoss[0m : 2.94899
[1mStep[0m  [15/53], [94mLoss[0m : 2.57337
[1mStep[0m  [20/53], [94mLoss[0m : 2.55911
[1mStep[0m  [25/53], [94mLoss[0m : 2.59497
[1mStep[0m  [30/53], [94mLoss[0m : 2.69115
[1mStep[0m  [35/53], [94mLoss[0m : 2.62294
[1mStep[0m  [40/53], [94mLoss[0m : 2.44671
[1mStep[0m  [45/53], [94mLoss[0m : 2.69432
[1mStep[0m  [50/53], [94mLoss[0m : 2.59327

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.618, [92mTest[0m: 2.540, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.75031
[1mStep[0m  [5/53], [94mLoss[0m : 2.51533
[1mStep[0m  [10/53], [94mLoss[0m : 2.75432
[1mStep[0m  [15/53], [94mLoss[0m : 2.60876
[1mStep[0m  [20/53], [94mLoss[0m : 2.53597
[1mStep[0m  [25/53], [94mLoss[0m : 2.47736
[1mStep[0m  [30/53], [94mLoss[0m : 2.65013
[1mStep[0m  [35/53], [94mLoss[0m : 2.60679
[1mStep[0m  [40/53], [94mLoss[0m : 2.56219
[1mStep[0m  [45/53], [94mLoss[0m : 2.68947
[1mStep[0m  [50/53], [94mLoss[0m : 2.58472

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.577, [92mTest[0m: 2.563, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.24987
[1mStep[0m  [5/53], [94mLoss[0m : 2.67346
[1mStep[0m  [10/53], [94mLoss[0m : 2.45664
[1mStep[0m  [15/53], [94mLoss[0m : 2.60671
[1mStep[0m  [20/53], [94mLoss[0m : 2.33068
[1mStep[0m  [25/53], [94mLoss[0m : 2.37950
[1mStep[0m  [30/53], [94mLoss[0m : 2.63517
[1mStep[0m  [35/53], [94mLoss[0m : 2.74182
[1mStep[0m  [40/53], [94mLoss[0m : 2.62292
[1mStep[0m  [45/53], [94mLoss[0m : 2.75961
[1mStep[0m  [50/53], [94mLoss[0m : 2.52478

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.568, [92mTest[0m: 2.513, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.64374
[1mStep[0m  [5/53], [94mLoss[0m : 2.42410
[1mStep[0m  [10/53], [94mLoss[0m : 2.34892
[1mStep[0m  [15/53], [94mLoss[0m : 2.43040
[1mStep[0m  [20/53], [94mLoss[0m : 2.52476
[1mStep[0m  [25/53], [94mLoss[0m : 2.54872
[1mStep[0m  [30/53], [94mLoss[0m : 2.62617
[1mStep[0m  [35/53], [94mLoss[0m : 2.55539
[1mStep[0m  [40/53], [94mLoss[0m : 2.61296
[1mStep[0m  [45/53], [94mLoss[0m : 2.59310
[1mStep[0m  [50/53], [94mLoss[0m : 2.45478

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.541, [92mTest[0m: 2.480, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.62111
[1mStep[0m  [5/53], [94mLoss[0m : 2.38351
[1mStep[0m  [10/53], [94mLoss[0m : 2.39958
[1mStep[0m  [15/53], [94mLoss[0m : 2.54758
[1mStep[0m  [20/53], [94mLoss[0m : 2.37100
[1mStep[0m  [25/53], [94mLoss[0m : 2.53379
[1mStep[0m  [30/53], [94mLoss[0m : 2.45634
[1mStep[0m  [35/53], [94mLoss[0m : 2.58586
[1mStep[0m  [40/53], [94mLoss[0m : 2.48957
[1mStep[0m  [45/53], [94mLoss[0m : 2.63571
[1mStep[0m  [50/53], [94mLoss[0m : 2.47465

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.540, [92mTest[0m: 2.522, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.33413
[1mStep[0m  [5/53], [94mLoss[0m : 2.44239
[1mStep[0m  [10/53], [94mLoss[0m : 2.38733
[1mStep[0m  [15/53], [94mLoss[0m : 2.50264
[1mStep[0m  [20/53], [94mLoss[0m : 2.65606
[1mStep[0m  [25/53], [94mLoss[0m : 2.53292
[1mStep[0m  [30/53], [94mLoss[0m : 2.57919
[1mStep[0m  [35/53], [94mLoss[0m : 2.39388
[1mStep[0m  [40/53], [94mLoss[0m : 2.62172
[1mStep[0m  [45/53], [94mLoss[0m : 2.74740
[1mStep[0m  [50/53], [94mLoss[0m : 2.51817

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.442, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.50107
[1mStep[0m  [5/53], [94mLoss[0m : 2.59312
[1mStep[0m  [10/53], [94mLoss[0m : 2.22728
[1mStep[0m  [15/53], [94mLoss[0m : 2.55981
[1mStep[0m  [20/53], [94mLoss[0m : 2.38843
[1mStep[0m  [25/53], [94mLoss[0m : 2.54141
[1mStep[0m  [30/53], [94mLoss[0m : 2.58041
[1mStep[0m  [35/53], [94mLoss[0m : 2.60719
[1mStep[0m  [40/53], [94mLoss[0m : 2.50418
[1mStep[0m  [45/53], [94mLoss[0m : 2.41133
[1mStep[0m  [50/53], [94mLoss[0m : 2.76044

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.451, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.58891
[1mStep[0m  [5/53], [94mLoss[0m : 2.35302
[1mStep[0m  [10/53], [94mLoss[0m : 2.42590
[1mStep[0m  [15/53], [94mLoss[0m : 2.66789
[1mStep[0m  [20/53], [94mLoss[0m : 2.44931
[1mStep[0m  [25/53], [94mLoss[0m : 2.75302
[1mStep[0m  [30/53], [94mLoss[0m : 2.46623
[1mStep[0m  [35/53], [94mLoss[0m : 2.40950
[1mStep[0m  [40/53], [94mLoss[0m : 2.41282
[1mStep[0m  [45/53], [94mLoss[0m : 2.50029
[1mStep[0m  [50/53], [94mLoss[0m : 2.52075

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.435, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.30836
[1mStep[0m  [5/53], [94mLoss[0m : 2.51108
[1mStep[0m  [10/53], [94mLoss[0m : 2.64680
[1mStep[0m  [15/53], [94mLoss[0m : 2.58942
[1mStep[0m  [20/53], [94mLoss[0m : 2.24550
[1mStep[0m  [25/53], [94mLoss[0m : 2.37289
[1mStep[0m  [30/53], [94mLoss[0m : 2.56699
[1mStep[0m  [35/53], [94mLoss[0m : 2.58573
[1mStep[0m  [40/53], [94mLoss[0m : 2.57984
[1mStep[0m  [45/53], [94mLoss[0m : 2.62161
[1mStep[0m  [50/53], [94mLoss[0m : 2.42070

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.433, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.64749
[1mStep[0m  [5/53], [94mLoss[0m : 2.55874
[1mStep[0m  [10/53], [94mLoss[0m : 2.33020
[1mStep[0m  [15/53], [94mLoss[0m : 2.26080
[1mStep[0m  [20/53], [94mLoss[0m : 2.47957
[1mStep[0m  [25/53], [94mLoss[0m : 2.52776
[1mStep[0m  [30/53], [94mLoss[0m : 2.67830
[1mStep[0m  [35/53], [94mLoss[0m : 2.70418
[1mStep[0m  [40/53], [94mLoss[0m : 2.43720
[1mStep[0m  [45/53], [94mLoss[0m : 2.40132
[1mStep[0m  [50/53], [94mLoss[0m : 2.45476

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.458, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.45943
[1mStep[0m  [5/53], [94mLoss[0m : 2.53975
[1mStep[0m  [10/53], [94mLoss[0m : 2.60600
[1mStep[0m  [15/53], [94mLoss[0m : 2.22260
[1mStep[0m  [20/53], [94mLoss[0m : 2.46677
[1mStep[0m  [25/53], [94mLoss[0m : 2.37262
[1mStep[0m  [30/53], [94mLoss[0m : 2.35475
[1mStep[0m  [35/53], [94mLoss[0m : 2.73861
[1mStep[0m  [40/53], [94mLoss[0m : 2.45495
[1mStep[0m  [45/53], [94mLoss[0m : 2.76760
[1mStep[0m  [50/53], [94mLoss[0m : 2.58423

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.448, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.51659
[1mStep[0m  [5/53], [94mLoss[0m : 2.39093
[1mStep[0m  [10/53], [94mLoss[0m : 2.31141
[1mStep[0m  [15/53], [94mLoss[0m : 2.56521
[1mStep[0m  [20/53], [94mLoss[0m : 2.39694
[1mStep[0m  [25/53], [94mLoss[0m : 2.34607
[1mStep[0m  [30/53], [94mLoss[0m : 2.49695
[1mStep[0m  [35/53], [94mLoss[0m : 2.59230
[1mStep[0m  [40/53], [94mLoss[0m : 2.29987
[1mStep[0m  [45/53], [94mLoss[0m : 2.43623
[1mStep[0m  [50/53], [94mLoss[0m : 2.53719

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.436, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.14128
[1mStep[0m  [5/53], [94mLoss[0m : 2.42329
[1mStep[0m  [10/53], [94mLoss[0m : 2.68571
[1mStep[0m  [15/53], [94mLoss[0m : 2.30690
[1mStep[0m  [20/53], [94mLoss[0m : 2.41264
[1mStep[0m  [25/53], [94mLoss[0m : 2.43657
[1mStep[0m  [30/53], [94mLoss[0m : 2.29678
[1mStep[0m  [35/53], [94mLoss[0m : 2.53974
[1mStep[0m  [40/53], [94mLoss[0m : 2.50981
[1mStep[0m  [45/53], [94mLoss[0m : 2.31229
[1mStep[0m  [50/53], [94mLoss[0m : 2.43022

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.407, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.28242
[1mStep[0m  [5/53], [94mLoss[0m : 2.34080
[1mStep[0m  [10/53], [94mLoss[0m : 2.76729
[1mStep[0m  [15/53], [94mLoss[0m : 2.38527
[1mStep[0m  [20/53], [94mLoss[0m : 2.47673
[1mStep[0m  [25/53], [94mLoss[0m : 2.29472
[1mStep[0m  [30/53], [94mLoss[0m : 2.09208
[1mStep[0m  [35/53], [94mLoss[0m : 2.27572
[1mStep[0m  [40/53], [94mLoss[0m : 2.42415
[1mStep[0m  [45/53], [94mLoss[0m : 2.54388
[1mStep[0m  [50/53], [94mLoss[0m : 2.30406

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.399, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.07505
[1mStep[0m  [5/53], [94mLoss[0m : 2.27929
[1mStep[0m  [10/53], [94mLoss[0m : 2.48894
[1mStep[0m  [15/53], [94mLoss[0m : 2.36820
[1mStep[0m  [20/53], [94mLoss[0m : 2.53604
[1mStep[0m  [25/53], [94mLoss[0m : 2.27687
[1mStep[0m  [30/53], [94mLoss[0m : 2.31937
[1mStep[0m  [35/53], [94mLoss[0m : 2.54182
[1mStep[0m  [40/53], [94mLoss[0m : 2.18251
[1mStep[0m  [45/53], [94mLoss[0m : 2.40460
[1mStep[0m  [50/53], [94mLoss[0m : 2.52510

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.376, [92mTest[0m: 2.411, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.38646
[1mStep[0m  [5/53], [94mLoss[0m : 2.27361
[1mStep[0m  [10/53], [94mLoss[0m : 2.56553
[1mStep[0m  [15/53], [94mLoss[0m : 2.11774
[1mStep[0m  [20/53], [94mLoss[0m : 2.25500
[1mStep[0m  [25/53], [94mLoss[0m : 2.43045
[1mStep[0m  [30/53], [94mLoss[0m : 2.28041
[1mStep[0m  [35/53], [94mLoss[0m : 2.25141
[1mStep[0m  [40/53], [94mLoss[0m : 2.38832
[1mStep[0m  [45/53], [94mLoss[0m : 2.45660
[1mStep[0m  [50/53], [94mLoss[0m : 2.49653

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.379, [92mTest[0m: 2.422, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.38307
[1mStep[0m  [5/53], [94mLoss[0m : 2.39137
[1mStep[0m  [10/53], [94mLoss[0m : 2.44453
[1mStep[0m  [15/53], [94mLoss[0m : 2.48248
[1mStep[0m  [20/53], [94mLoss[0m : 2.30286
[1mStep[0m  [25/53], [94mLoss[0m : 2.22734
[1mStep[0m  [30/53], [94mLoss[0m : 2.33061
[1mStep[0m  [35/53], [94mLoss[0m : 2.44120
[1mStep[0m  [40/53], [94mLoss[0m : 2.33660
[1mStep[0m  [45/53], [94mLoss[0m : 2.43111
[1mStep[0m  [50/53], [94mLoss[0m : 2.09212

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.377, [92mTest[0m: 2.411, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.43699
[1mStep[0m  [5/53], [94mLoss[0m : 2.14484
[1mStep[0m  [10/53], [94mLoss[0m : 2.23997
[1mStep[0m  [15/53], [94mLoss[0m : 2.36534
[1mStep[0m  [20/53], [94mLoss[0m : 2.15299
[1mStep[0m  [25/53], [94mLoss[0m : 2.40777
[1mStep[0m  [30/53], [94mLoss[0m : 2.31369
[1mStep[0m  [35/53], [94mLoss[0m : 2.25335
[1mStep[0m  [40/53], [94mLoss[0m : 2.31753
[1mStep[0m  [45/53], [94mLoss[0m : 2.33729
[1mStep[0m  [50/53], [94mLoss[0m : 2.32968

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.347, [92mTest[0m: 2.402, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.34476
[1mStep[0m  [5/53], [94mLoss[0m : 2.25479
[1mStep[0m  [10/53], [94mLoss[0m : 2.35844
[1mStep[0m  [15/53], [94mLoss[0m : 2.18772
[1mStep[0m  [20/53], [94mLoss[0m : 2.15412
[1mStep[0m  [25/53], [94mLoss[0m : 2.11361
[1mStep[0m  [30/53], [94mLoss[0m : 2.29670
[1mStep[0m  [35/53], [94mLoss[0m : 2.19347
[1mStep[0m  [40/53], [94mLoss[0m : 2.31425
[1mStep[0m  [45/53], [94mLoss[0m : 2.33210
[1mStep[0m  [50/53], [94mLoss[0m : 2.47559

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.304, [92mTest[0m: 2.407, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.17548
[1mStep[0m  [5/53], [94mLoss[0m : 2.30329
[1mStep[0m  [10/53], [94mLoss[0m : 2.26792
[1mStep[0m  [15/53], [94mLoss[0m : 2.08900
[1mStep[0m  [20/53], [94mLoss[0m : 2.25501
[1mStep[0m  [25/53], [94mLoss[0m : 2.06057
[1mStep[0m  [30/53], [94mLoss[0m : 2.37650
[1mStep[0m  [35/53], [94mLoss[0m : 2.27124
[1mStep[0m  [40/53], [94mLoss[0m : 2.33756
[1mStep[0m  [45/53], [94mLoss[0m : 2.30842
[1mStep[0m  [50/53], [94mLoss[0m : 2.45008

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.299, [92mTest[0m: 2.383, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.15517
[1mStep[0m  [5/53], [94mLoss[0m : 2.32620
[1mStep[0m  [10/53], [94mLoss[0m : 2.19831
[1mStep[0m  [15/53], [94mLoss[0m : 2.22239
[1mStep[0m  [20/53], [94mLoss[0m : 2.36234
[1mStep[0m  [25/53], [94mLoss[0m : 2.29565
[1mStep[0m  [30/53], [94mLoss[0m : 2.19394
[1mStep[0m  [35/53], [94mLoss[0m : 2.11189
[1mStep[0m  [40/53], [94mLoss[0m : 2.29572
[1mStep[0m  [45/53], [94mLoss[0m : 2.26165
[1mStep[0m  [50/53], [94mLoss[0m : 2.26420

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.275, [92mTest[0m: 2.419, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.26734
[1mStep[0m  [5/53], [94mLoss[0m : 2.09317
[1mStep[0m  [10/53], [94mLoss[0m : 2.04819
[1mStep[0m  [15/53], [94mLoss[0m : 2.22207
[1mStep[0m  [20/53], [94mLoss[0m : 2.24771
[1mStep[0m  [25/53], [94mLoss[0m : 2.29605
[1mStep[0m  [30/53], [94mLoss[0m : 2.33243
[1mStep[0m  [35/53], [94mLoss[0m : 2.29315
[1mStep[0m  [40/53], [94mLoss[0m : 2.32634
[1mStep[0m  [45/53], [94mLoss[0m : 2.48414
[1mStep[0m  [50/53], [94mLoss[0m : 2.44126

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.272, [92mTest[0m: 2.385, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.36518
[1mStep[0m  [5/53], [94mLoss[0m : 2.14266
[1mStep[0m  [10/53], [94mLoss[0m : 2.03748
[1mStep[0m  [15/53], [94mLoss[0m : 2.29930
[1mStep[0m  [20/53], [94mLoss[0m : 2.10144
[1mStep[0m  [25/53], [94mLoss[0m : 2.68560
[1mStep[0m  [30/53], [94mLoss[0m : 2.55781
[1mStep[0m  [35/53], [94mLoss[0m : 2.24142
[1mStep[0m  [40/53], [94mLoss[0m : 2.22779
[1mStep[0m  [45/53], [94mLoss[0m : 2.28252
[1mStep[0m  [50/53], [94mLoss[0m : 2.22597

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.288, [92mTest[0m: 2.390, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.28551
[1mStep[0m  [5/53], [94mLoss[0m : 2.34904
[1mStep[0m  [10/53], [94mLoss[0m : 2.21832
[1mStep[0m  [15/53], [94mLoss[0m : 2.16045
[1mStep[0m  [20/53], [94mLoss[0m : 2.18185
[1mStep[0m  [25/53], [94mLoss[0m : 2.32792
[1mStep[0m  [30/53], [94mLoss[0m : 2.36290
[1mStep[0m  [35/53], [94mLoss[0m : 2.24746
[1mStep[0m  [40/53], [94mLoss[0m : 2.42288
[1mStep[0m  [45/53], [94mLoss[0m : 2.14775
[1mStep[0m  [50/53], [94mLoss[0m : 2.32929

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.259, [92mTest[0m: 2.394, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.42825
[1mStep[0m  [5/53], [94mLoss[0m : 2.31359
[1mStep[0m  [10/53], [94mLoss[0m : 2.27567
[1mStep[0m  [15/53], [94mLoss[0m : 2.30951
[1mStep[0m  [20/53], [94mLoss[0m : 2.07777
[1mStep[0m  [25/53], [94mLoss[0m : 2.37057
[1mStep[0m  [30/53], [94mLoss[0m : 2.20958
[1mStep[0m  [35/53], [94mLoss[0m : 2.02632
[1mStep[0m  [40/53], [94mLoss[0m : 2.11995
[1mStep[0m  [45/53], [94mLoss[0m : 2.32962
[1mStep[0m  [50/53], [94mLoss[0m : 2.31933

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.225, [92mTest[0m: 2.404, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.19159
[1mStep[0m  [5/53], [94mLoss[0m : 2.09640
[1mStep[0m  [10/53], [94mLoss[0m : 2.21033
[1mStep[0m  [15/53], [94mLoss[0m : 2.13288
[1mStep[0m  [20/53], [94mLoss[0m : 2.27911
[1mStep[0m  [25/53], [94mLoss[0m : 2.02017
[1mStep[0m  [30/53], [94mLoss[0m : 2.04225
[1mStep[0m  [35/53], [94mLoss[0m : 2.08341
[1mStep[0m  [40/53], [94mLoss[0m : 2.14236
[1mStep[0m  [45/53], [94mLoss[0m : 2.25748
[1mStep[0m  [50/53], [94mLoss[0m : 2.52984

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.205, [92mTest[0m: 2.433, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.397
====================================

Phase 2 - Evaluation MAE:  2.397151286785419
MAE score P1       2.598304
MAE score P2       2.397151
loss               2.205169
learning_rate      0.002575
batch_size              256
hidden_sizes      [250, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.1
weight_decay           0.01
Name: 3, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/106], [94mLoss[0m : 10.84503
[1mStep[0m  [10/106], [94mLoss[0m : 10.12927
[1mStep[0m  [20/106], [94mLoss[0m : 8.25976
[1mStep[0m  [30/106], [94mLoss[0m : 6.43426
[1mStep[0m  [40/106], [94mLoss[0m : 4.42455
[1mStep[0m  [50/106], [94mLoss[0m : 2.67610
[1mStep[0m  [60/106], [94mLoss[0m : 3.15001
[1mStep[0m  [70/106], [94mLoss[0m : 3.04817
[1mStep[0m  [80/106], [94mLoss[0m : 2.85830
[1mStep[0m  [90/106], [94mLoss[0m : 2.90093
[1mStep[0m  [100/106], [94mLoss[0m : 3.21094

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 5.003, [92mTest[0m: 11.030, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.65778
[1mStep[0m  [10/106], [94mLoss[0m : 2.97194
[1mStep[0m  [20/106], [94mLoss[0m : 2.71016
[1mStep[0m  [30/106], [94mLoss[0m : 3.11367
[1mStep[0m  [40/106], [94mLoss[0m : 3.24054
[1mStep[0m  [50/106], [94mLoss[0m : 2.89954
[1mStep[0m  [60/106], [94mLoss[0m : 3.02998
[1mStep[0m  [70/106], [94mLoss[0m : 2.67770
[1mStep[0m  [80/106], [94mLoss[0m : 2.91610
[1mStep[0m  [90/106], [94mLoss[0m : 2.59222
[1mStep[0m  [100/106], [94mLoss[0m : 2.89291

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.796, [92mTest[0m: 2.783, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.81416
[1mStep[0m  [10/106], [94mLoss[0m : 2.81577
[1mStep[0m  [20/106], [94mLoss[0m : 2.64582
[1mStep[0m  [30/106], [94mLoss[0m : 2.59082
[1mStep[0m  [40/106], [94mLoss[0m : 2.95091
[1mStep[0m  [50/106], [94mLoss[0m : 2.92267
[1mStep[0m  [60/106], [94mLoss[0m : 2.94125
[1mStep[0m  [70/106], [94mLoss[0m : 2.66447
[1mStep[0m  [80/106], [94mLoss[0m : 2.84988
[1mStep[0m  [90/106], [94mLoss[0m : 2.69527
[1mStep[0m  [100/106], [94mLoss[0m : 2.54280

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.721, [92mTest[0m: 2.542, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.76899
[1mStep[0m  [10/106], [94mLoss[0m : 2.83069
[1mStep[0m  [20/106], [94mLoss[0m : 2.53995
[1mStep[0m  [30/106], [94mLoss[0m : 2.47424
[1mStep[0m  [40/106], [94mLoss[0m : 2.69958
[1mStep[0m  [50/106], [94mLoss[0m : 2.83832
[1mStep[0m  [60/106], [94mLoss[0m : 2.83602
[1mStep[0m  [70/106], [94mLoss[0m : 2.61190
[1mStep[0m  [80/106], [94mLoss[0m : 2.35881
[1mStep[0m  [90/106], [94mLoss[0m : 2.64417
[1mStep[0m  [100/106], [94mLoss[0m : 2.67452

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.670, [92mTest[0m: 2.486, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.79681
[1mStep[0m  [10/106], [94mLoss[0m : 2.62984
[1mStep[0m  [20/106], [94mLoss[0m : 2.60756
[1mStep[0m  [30/106], [94mLoss[0m : 2.91428
[1mStep[0m  [40/106], [94mLoss[0m : 2.79188
[1mStep[0m  [50/106], [94mLoss[0m : 2.70701
[1mStep[0m  [60/106], [94mLoss[0m : 2.66697
[1mStep[0m  [70/106], [94mLoss[0m : 2.60110
[1mStep[0m  [80/106], [94mLoss[0m : 2.54276
[1mStep[0m  [90/106], [94mLoss[0m : 2.55356
[1mStep[0m  [100/106], [94mLoss[0m : 2.73268

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.647, [92mTest[0m: 2.437, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.54523
[1mStep[0m  [10/106], [94mLoss[0m : 2.68604
[1mStep[0m  [20/106], [94mLoss[0m : 2.74411
[1mStep[0m  [30/106], [94mLoss[0m : 2.54439
[1mStep[0m  [40/106], [94mLoss[0m : 2.71790
[1mStep[0m  [50/106], [94mLoss[0m : 2.70922
[1mStep[0m  [60/106], [94mLoss[0m : 2.58893
[1mStep[0m  [70/106], [94mLoss[0m : 2.75154
[1mStep[0m  [80/106], [94mLoss[0m : 3.15196
[1mStep[0m  [90/106], [94mLoss[0m : 2.97244
[1mStep[0m  [100/106], [94mLoss[0m : 2.45278

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.600, [92mTest[0m: 2.411, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.47672
[1mStep[0m  [10/106], [94mLoss[0m : 2.58021
[1mStep[0m  [20/106], [94mLoss[0m : 2.69359
[1mStep[0m  [30/106], [94mLoss[0m : 2.79298
[1mStep[0m  [40/106], [94mLoss[0m : 2.47851
[1mStep[0m  [50/106], [94mLoss[0m : 2.44004
[1mStep[0m  [60/106], [94mLoss[0m : 2.89233
[1mStep[0m  [70/106], [94mLoss[0m : 3.02266
[1mStep[0m  [80/106], [94mLoss[0m : 2.56568
[1mStep[0m  [90/106], [94mLoss[0m : 2.65921
[1mStep[0m  [100/106], [94mLoss[0m : 3.13284

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.590, [92mTest[0m: 2.403, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.58199
[1mStep[0m  [10/106], [94mLoss[0m : 2.71207
[1mStep[0m  [20/106], [94mLoss[0m : 2.65920
[1mStep[0m  [30/106], [94mLoss[0m : 2.61907
[1mStep[0m  [40/106], [94mLoss[0m : 2.64779
[1mStep[0m  [50/106], [94mLoss[0m : 2.26889
[1mStep[0m  [60/106], [94mLoss[0m : 2.92270
[1mStep[0m  [70/106], [94mLoss[0m : 2.66297
[1mStep[0m  [80/106], [94mLoss[0m : 2.81364
[1mStep[0m  [90/106], [94mLoss[0m : 2.78321
[1mStep[0m  [100/106], [94mLoss[0m : 2.69508

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.581, [92mTest[0m: 2.432, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.67654
[1mStep[0m  [10/106], [94mLoss[0m : 2.85179
[1mStep[0m  [20/106], [94mLoss[0m : 2.51697
[1mStep[0m  [30/106], [94mLoss[0m : 2.73763
[1mStep[0m  [40/106], [94mLoss[0m : 2.74032
[1mStep[0m  [50/106], [94mLoss[0m : 2.57600
[1mStep[0m  [60/106], [94mLoss[0m : 2.61058
[1mStep[0m  [70/106], [94mLoss[0m : 2.67272
[1mStep[0m  [80/106], [94mLoss[0m : 2.60644
[1mStep[0m  [90/106], [94mLoss[0m : 2.85283
[1mStep[0m  [100/106], [94mLoss[0m : 2.40754

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.571, [92mTest[0m: 2.391, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.41828
[1mStep[0m  [10/106], [94mLoss[0m : 2.42327
[1mStep[0m  [20/106], [94mLoss[0m : 2.46854
[1mStep[0m  [30/106], [94mLoss[0m : 2.90152
[1mStep[0m  [40/106], [94mLoss[0m : 2.52689
[1mStep[0m  [50/106], [94mLoss[0m : 2.59507
[1mStep[0m  [60/106], [94mLoss[0m : 2.35960
[1mStep[0m  [70/106], [94mLoss[0m : 2.95140
[1mStep[0m  [80/106], [94mLoss[0m : 2.62462
[1mStep[0m  [90/106], [94mLoss[0m : 2.70486
[1mStep[0m  [100/106], [94mLoss[0m : 2.29389

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.526, [92mTest[0m: 2.371, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.58036
[1mStep[0m  [10/106], [94mLoss[0m : 2.77731
[1mStep[0m  [20/106], [94mLoss[0m : 2.71974
[1mStep[0m  [30/106], [94mLoss[0m : 2.59544
[1mStep[0m  [40/106], [94mLoss[0m : 2.39227
[1mStep[0m  [50/106], [94mLoss[0m : 2.51438
[1mStep[0m  [60/106], [94mLoss[0m : 2.94965
[1mStep[0m  [70/106], [94mLoss[0m : 2.72928
[1mStep[0m  [80/106], [94mLoss[0m : 2.31306
[1mStep[0m  [90/106], [94mLoss[0m : 2.46271
[1mStep[0m  [100/106], [94mLoss[0m : 2.28083

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.394, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.22264
[1mStep[0m  [10/106], [94mLoss[0m : 2.54897
[1mStep[0m  [20/106], [94mLoss[0m : 2.71068
[1mStep[0m  [30/106], [94mLoss[0m : 2.49285
[1mStep[0m  [40/106], [94mLoss[0m : 2.64404
[1mStep[0m  [50/106], [94mLoss[0m : 2.34737
[1mStep[0m  [60/106], [94mLoss[0m : 2.23268
[1mStep[0m  [70/106], [94mLoss[0m : 2.36481
[1mStep[0m  [80/106], [94mLoss[0m : 2.71275
[1mStep[0m  [90/106], [94mLoss[0m : 2.48712
[1mStep[0m  [100/106], [94mLoss[0m : 2.71741

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.375, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.62816
[1mStep[0m  [10/106], [94mLoss[0m : 2.52916
[1mStep[0m  [20/106], [94mLoss[0m : 2.52532
[1mStep[0m  [30/106], [94mLoss[0m : 2.42847
[1mStep[0m  [40/106], [94mLoss[0m : 2.41999
[1mStep[0m  [50/106], [94mLoss[0m : 2.67879
[1mStep[0m  [60/106], [94mLoss[0m : 2.55152
[1mStep[0m  [70/106], [94mLoss[0m : 2.22108
[1mStep[0m  [80/106], [94mLoss[0m : 2.34867
[1mStep[0m  [90/106], [94mLoss[0m : 2.42479
[1mStep[0m  [100/106], [94mLoss[0m : 2.55053

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.374, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.53504
[1mStep[0m  [10/106], [94mLoss[0m : 2.21952
[1mStep[0m  [20/106], [94mLoss[0m : 2.39253
[1mStep[0m  [30/106], [94mLoss[0m : 2.78237
[1mStep[0m  [40/106], [94mLoss[0m : 2.51131
[1mStep[0m  [50/106], [94mLoss[0m : 2.49408
[1mStep[0m  [60/106], [94mLoss[0m : 2.63805
[1mStep[0m  [70/106], [94mLoss[0m : 2.22422
[1mStep[0m  [80/106], [94mLoss[0m : 2.63167
[1mStep[0m  [90/106], [94mLoss[0m : 2.43362
[1mStep[0m  [100/106], [94mLoss[0m : 2.37443

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.347, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.43280
[1mStep[0m  [10/106], [94mLoss[0m : 2.38793
[1mStep[0m  [20/106], [94mLoss[0m : 2.59748
[1mStep[0m  [30/106], [94mLoss[0m : 2.70853
[1mStep[0m  [40/106], [94mLoss[0m : 2.36824
[1mStep[0m  [50/106], [94mLoss[0m : 2.44107
[1mStep[0m  [60/106], [94mLoss[0m : 2.21835
[1mStep[0m  [70/106], [94mLoss[0m : 2.61330
[1mStep[0m  [80/106], [94mLoss[0m : 2.15600
[1mStep[0m  [90/106], [94mLoss[0m : 2.35225
[1mStep[0m  [100/106], [94mLoss[0m : 2.28148

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.356, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.72663
[1mStep[0m  [10/106], [94mLoss[0m : 2.11282
[1mStep[0m  [20/106], [94mLoss[0m : 2.35624
[1mStep[0m  [30/106], [94mLoss[0m : 2.63752
[1mStep[0m  [40/106], [94mLoss[0m : 2.85404
[1mStep[0m  [50/106], [94mLoss[0m : 2.58384
[1mStep[0m  [60/106], [94mLoss[0m : 2.62862
[1mStep[0m  [70/106], [94mLoss[0m : 2.69983
[1mStep[0m  [80/106], [94mLoss[0m : 2.59588
[1mStep[0m  [90/106], [94mLoss[0m : 2.40241
[1mStep[0m  [100/106], [94mLoss[0m : 2.55498

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.357, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.64965
[1mStep[0m  [10/106], [94mLoss[0m : 2.44408
[1mStep[0m  [20/106], [94mLoss[0m : 2.54086
[1mStep[0m  [30/106], [94mLoss[0m : 2.86543
[1mStep[0m  [40/106], [94mLoss[0m : 2.45902
[1mStep[0m  [50/106], [94mLoss[0m : 2.09698
[1mStep[0m  [60/106], [94mLoss[0m : 3.01430
[1mStep[0m  [70/106], [94mLoss[0m : 2.43725
[1mStep[0m  [80/106], [94mLoss[0m : 2.47106
[1mStep[0m  [90/106], [94mLoss[0m : 2.43620
[1mStep[0m  [100/106], [94mLoss[0m : 2.64291

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.357, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.59469
[1mStep[0m  [10/106], [94mLoss[0m : 2.71267
[1mStep[0m  [20/106], [94mLoss[0m : 2.32786
[1mStep[0m  [30/106], [94mLoss[0m : 2.70465
[1mStep[0m  [40/106], [94mLoss[0m : 2.34399
[1mStep[0m  [50/106], [94mLoss[0m : 2.18272
[1mStep[0m  [60/106], [94mLoss[0m : 2.20371
[1mStep[0m  [70/106], [94mLoss[0m : 2.37074
[1mStep[0m  [80/106], [94mLoss[0m : 2.37466
[1mStep[0m  [90/106], [94mLoss[0m : 2.43430
[1mStep[0m  [100/106], [94mLoss[0m : 2.41692

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.357, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.25441
[1mStep[0m  [10/106], [94mLoss[0m : 2.64982
[1mStep[0m  [20/106], [94mLoss[0m : 2.33324
[1mStep[0m  [30/106], [94mLoss[0m : 2.56952
[1mStep[0m  [40/106], [94mLoss[0m : 2.58328
[1mStep[0m  [50/106], [94mLoss[0m : 2.51077
[1mStep[0m  [60/106], [94mLoss[0m : 2.40082
[1mStep[0m  [70/106], [94mLoss[0m : 2.35834
[1mStep[0m  [80/106], [94mLoss[0m : 2.37403
[1mStep[0m  [90/106], [94mLoss[0m : 2.57840
[1mStep[0m  [100/106], [94mLoss[0m : 2.67280

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.342, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.71585
[1mStep[0m  [10/106], [94mLoss[0m : 2.54967
[1mStep[0m  [20/106], [94mLoss[0m : 2.36821
[1mStep[0m  [30/106], [94mLoss[0m : 2.59249
[1mStep[0m  [40/106], [94mLoss[0m : 2.56163
[1mStep[0m  [50/106], [94mLoss[0m : 2.39868
[1mStep[0m  [60/106], [94mLoss[0m : 2.47001
[1mStep[0m  [70/106], [94mLoss[0m : 2.67203
[1mStep[0m  [80/106], [94mLoss[0m : 2.69190
[1mStep[0m  [90/106], [94mLoss[0m : 2.52400
[1mStep[0m  [100/106], [94mLoss[0m : 2.03921

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.342, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.52349
[1mStep[0m  [10/106], [94mLoss[0m : 2.25455
[1mStep[0m  [20/106], [94mLoss[0m : 2.62441
[1mStep[0m  [30/106], [94mLoss[0m : 2.34920
[1mStep[0m  [40/106], [94mLoss[0m : 2.70280
[1mStep[0m  [50/106], [94mLoss[0m : 2.70643
[1mStep[0m  [60/106], [94mLoss[0m : 2.14148
[1mStep[0m  [70/106], [94mLoss[0m : 2.37152
[1mStep[0m  [80/106], [94mLoss[0m : 2.25936
[1mStep[0m  [90/106], [94mLoss[0m : 2.57935
[1mStep[0m  [100/106], [94mLoss[0m : 2.67681

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.353, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.46622
[1mStep[0m  [10/106], [94mLoss[0m : 2.37362
[1mStep[0m  [20/106], [94mLoss[0m : 2.41711
[1mStep[0m  [30/106], [94mLoss[0m : 2.41029
[1mStep[0m  [40/106], [94mLoss[0m : 2.38527
[1mStep[0m  [50/106], [94mLoss[0m : 2.53127
[1mStep[0m  [60/106], [94mLoss[0m : 2.46512
[1mStep[0m  [70/106], [94mLoss[0m : 2.44273
[1mStep[0m  [80/106], [94mLoss[0m : 2.36113
[1mStep[0m  [90/106], [94mLoss[0m : 2.68327
[1mStep[0m  [100/106], [94mLoss[0m : 2.14180

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.349, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.64709
[1mStep[0m  [10/106], [94mLoss[0m : 2.20459
[1mStep[0m  [20/106], [94mLoss[0m : 2.45754
[1mStep[0m  [30/106], [94mLoss[0m : 2.50811
[1mStep[0m  [40/106], [94mLoss[0m : 2.42178
[1mStep[0m  [50/106], [94mLoss[0m : 2.79946
[1mStep[0m  [60/106], [94mLoss[0m : 2.34983
[1mStep[0m  [70/106], [94mLoss[0m : 2.51536
[1mStep[0m  [80/106], [94mLoss[0m : 2.28141
[1mStep[0m  [90/106], [94mLoss[0m : 2.34529
[1mStep[0m  [100/106], [94mLoss[0m : 2.42591

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.346, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.44084
[1mStep[0m  [10/106], [94mLoss[0m : 2.38188
[1mStep[0m  [20/106], [94mLoss[0m : 2.23935
[1mStep[0m  [30/106], [94mLoss[0m : 2.45000
[1mStep[0m  [40/106], [94mLoss[0m : 2.24958
[1mStep[0m  [50/106], [94mLoss[0m : 2.59414
[1mStep[0m  [60/106], [94mLoss[0m : 2.45873
[1mStep[0m  [70/106], [94mLoss[0m : 2.40123
[1mStep[0m  [80/106], [94mLoss[0m : 2.24602
[1mStep[0m  [90/106], [94mLoss[0m : 2.13157
[1mStep[0m  [100/106], [94mLoss[0m : 2.39485

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.348, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.37615
[1mStep[0m  [10/106], [94mLoss[0m : 2.34952
[1mStep[0m  [20/106], [94mLoss[0m : 2.59491
[1mStep[0m  [30/106], [94mLoss[0m : 2.38243
[1mStep[0m  [40/106], [94mLoss[0m : 2.06144
[1mStep[0m  [50/106], [94mLoss[0m : 2.22765
[1mStep[0m  [60/106], [94mLoss[0m : 2.29582
[1mStep[0m  [70/106], [94mLoss[0m : 2.52896
[1mStep[0m  [80/106], [94mLoss[0m : 2.69551
[1mStep[0m  [90/106], [94mLoss[0m : 2.37034
[1mStep[0m  [100/106], [94mLoss[0m : 2.20701

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.345, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.47926
[1mStep[0m  [10/106], [94mLoss[0m : 2.42938
[1mStep[0m  [20/106], [94mLoss[0m : 2.14028
[1mStep[0m  [30/106], [94mLoss[0m : 2.21844
[1mStep[0m  [40/106], [94mLoss[0m : 2.58834
[1mStep[0m  [50/106], [94mLoss[0m : 2.57524
[1mStep[0m  [60/106], [94mLoss[0m : 2.31069
[1mStep[0m  [70/106], [94mLoss[0m : 2.31436
[1mStep[0m  [80/106], [94mLoss[0m : 2.30835
[1mStep[0m  [90/106], [94mLoss[0m : 2.33908
[1mStep[0m  [100/106], [94mLoss[0m : 2.58816

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.338, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.45292
[1mStep[0m  [10/106], [94mLoss[0m : 2.54621
[1mStep[0m  [20/106], [94mLoss[0m : 2.47998
[1mStep[0m  [30/106], [94mLoss[0m : 2.33243
[1mStep[0m  [40/106], [94mLoss[0m : 2.27928
[1mStep[0m  [50/106], [94mLoss[0m : 2.24837
[1mStep[0m  [60/106], [94mLoss[0m : 2.19277
[1mStep[0m  [70/106], [94mLoss[0m : 2.17587
[1mStep[0m  [80/106], [94mLoss[0m : 2.46743
[1mStep[0m  [90/106], [94mLoss[0m : 2.70959
[1mStep[0m  [100/106], [94mLoss[0m : 2.53888

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.331, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.20724
[1mStep[0m  [10/106], [94mLoss[0m : 2.29928
[1mStep[0m  [20/106], [94mLoss[0m : 2.23151
[1mStep[0m  [30/106], [94mLoss[0m : 2.58305
[1mStep[0m  [40/106], [94mLoss[0m : 2.91419
[1mStep[0m  [50/106], [94mLoss[0m : 2.32206
[1mStep[0m  [60/106], [94mLoss[0m : 2.63607
[1mStep[0m  [70/106], [94mLoss[0m : 2.48149
[1mStep[0m  [80/106], [94mLoss[0m : 2.35604
[1mStep[0m  [90/106], [94mLoss[0m : 2.45149
[1mStep[0m  [100/106], [94mLoss[0m : 2.22113

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.337, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.58068
[1mStep[0m  [10/106], [94mLoss[0m : 2.20186
[1mStep[0m  [20/106], [94mLoss[0m : 2.58965
[1mStep[0m  [30/106], [94mLoss[0m : 2.41191
[1mStep[0m  [40/106], [94mLoss[0m : 2.47946
[1mStep[0m  [50/106], [94mLoss[0m : 2.06474
[1mStep[0m  [60/106], [94mLoss[0m : 2.41434
[1mStep[0m  [70/106], [94mLoss[0m : 2.73625
[1mStep[0m  [80/106], [94mLoss[0m : 2.55344
[1mStep[0m  [90/106], [94mLoss[0m : 2.66504
[1mStep[0m  [100/106], [94mLoss[0m : 2.25928

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.395, [92mTest[0m: 2.325, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.43226
[1mStep[0m  [10/106], [94mLoss[0m : 2.38434
[1mStep[0m  [20/106], [94mLoss[0m : 2.65322
[1mStep[0m  [30/106], [94mLoss[0m : 2.55726
[1mStep[0m  [40/106], [94mLoss[0m : 2.33667
[1mStep[0m  [50/106], [94mLoss[0m : 2.45841
[1mStep[0m  [60/106], [94mLoss[0m : 2.41916
[1mStep[0m  [70/106], [94mLoss[0m : 2.50724
[1mStep[0m  [80/106], [94mLoss[0m : 2.14884
[1mStep[0m  [90/106], [94mLoss[0m : 2.38262
[1mStep[0m  [100/106], [94mLoss[0m : 2.47093

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.391, [92mTest[0m: 2.347, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.337
====================================

Phase 1 - Evaluation MAE:  2.336857309881246
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/106], [94mLoss[0m : 2.51943
[1mStep[0m  [10/106], [94mLoss[0m : 2.34900
[1mStep[0m  [20/106], [94mLoss[0m : 2.50324
[1mStep[0m  [30/106], [94mLoss[0m : 2.73756
[1mStep[0m  [40/106], [94mLoss[0m : 2.44634
[1mStep[0m  [50/106], [94mLoss[0m : 2.46835
[1mStep[0m  [60/106], [94mLoss[0m : 2.69642
[1mStep[0m  [70/106], [94mLoss[0m : 2.23921
[1mStep[0m  [80/106], [94mLoss[0m : 2.57338
[1mStep[0m  [90/106], [94mLoss[0m : 2.54474
[1mStep[0m  [100/106], [94mLoss[0m : 2.30137

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.337, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.63701
[1mStep[0m  [10/106], [94mLoss[0m : 2.55552
[1mStep[0m  [20/106], [94mLoss[0m : 2.21435
[1mStep[0m  [30/106], [94mLoss[0m : 2.44474
[1mStep[0m  [40/106], [94mLoss[0m : 2.62396
[1mStep[0m  [50/106], [94mLoss[0m : 2.61299
[1mStep[0m  [60/106], [94mLoss[0m : 2.49987
[1mStep[0m  [70/106], [94mLoss[0m : 2.44909
[1mStep[0m  [80/106], [94mLoss[0m : 2.30868
[1mStep[0m  [90/106], [94mLoss[0m : 2.41634
[1mStep[0m  [100/106], [94mLoss[0m : 2.95110

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.356, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.08917
[1mStep[0m  [10/106], [94mLoss[0m : 2.44239
[1mStep[0m  [20/106], [94mLoss[0m : 2.21160
[1mStep[0m  [30/106], [94mLoss[0m : 2.79708
[1mStep[0m  [40/106], [94mLoss[0m : 2.32798
[1mStep[0m  [50/106], [94mLoss[0m : 2.33719
[1mStep[0m  [60/106], [94mLoss[0m : 2.08215
[1mStep[0m  [70/106], [94mLoss[0m : 2.18165
[1mStep[0m  [80/106], [94mLoss[0m : 2.51015
[1mStep[0m  [90/106], [94mLoss[0m : 2.10660
[1mStep[0m  [100/106], [94mLoss[0m : 2.22549

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.354, [92mTest[0m: 2.352, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.17987
[1mStep[0m  [10/106], [94mLoss[0m : 2.17898
[1mStep[0m  [20/106], [94mLoss[0m : 2.42597
[1mStep[0m  [30/106], [94mLoss[0m : 2.19971
[1mStep[0m  [40/106], [94mLoss[0m : 2.22750
[1mStep[0m  [50/106], [94mLoss[0m : 2.31001
[1mStep[0m  [60/106], [94mLoss[0m : 2.30018
[1mStep[0m  [70/106], [94mLoss[0m : 2.41191
[1mStep[0m  [80/106], [94mLoss[0m : 1.98467
[1mStep[0m  [90/106], [94mLoss[0m : 2.30971
[1mStep[0m  [100/106], [94mLoss[0m : 2.28339

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.278, [92mTest[0m: 2.345, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.16186
[1mStep[0m  [10/106], [94mLoss[0m : 1.91645
[1mStep[0m  [20/106], [94mLoss[0m : 2.53759
[1mStep[0m  [30/106], [94mLoss[0m : 2.02127
[1mStep[0m  [40/106], [94mLoss[0m : 2.23413
[1mStep[0m  [50/106], [94mLoss[0m : 2.34133
[1mStep[0m  [60/106], [94mLoss[0m : 2.02287
[1mStep[0m  [70/106], [94mLoss[0m : 2.26281
[1mStep[0m  [80/106], [94mLoss[0m : 2.36293
[1mStep[0m  [90/106], [94mLoss[0m : 2.36563
[1mStep[0m  [100/106], [94mLoss[0m : 2.38642

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.221, [92mTest[0m: 2.363, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.12455
[1mStep[0m  [10/106], [94mLoss[0m : 2.47857
[1mStep[0m  [20/106], [94mLoss[0m : 1.97753
[1mStep[0m  [30/106], [94mLoss[0m : 2.24345
[1mStep[0m  [40/106], [94mLoss[0m : 2.16368
[1mStep[0m  [50/106], [94mLoss[0m : 2.32576
[1mStep[0m  [60/106], [94mLoss[0m : 2.61328
[1mStep[0m  [70/106], [94mLoss[0m : 2.06750
[1mStep[0m  [80/106], [94mLoss[0m : 2.15828
[1mStep[0m  [90/106], [94mLoss[0m : 2.08897
[1mStep[0m  [100/106], [94mLoss[0m : 2.10763

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.182, [92mTest[0m: 2.379, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.45012
[1mStep[0m  [10/106], [94mLoss[0m : 1.93320
[1mStep[0m  [20/106], [94mLoss[0m : 2.15976
[1mStep[0m  [30/106], [94mLoss[0m : 2.13867
[1mStep[0m  [40/106], [94mLoss[0m : 2.36994
[1mStep[0m  [50/106], [94mLoss[0m : 2.02578
[1mStep[0m  [60/106], [94mLoss[0m : 1.99449
[1mStep[0m  [70/106], [94mLoss[0m : 1.97112
[1mStep[0m  [80/106], [94mLoss[0m : 2.40961
[1mStep[0m  [90/106], [94mLoss[0m : 2.21957
[1mStep[0m  [100/106], [94mLoss[0m : 2.25805

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.133, [92mTest[0m: 2.396, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.19625
[1mStep[0m  [10/106], [94mLoss[0m : 2.03959
[1mStep[0m  [20/106], [94mLoss[0m : 2.03460
[1mStep[0m  [30/106], [94mLoss[0m : 1.96573
[1mStep[0m  [40/106], [94mLoss[0m : 1.93291
[1mStep[0m  [50/106], [94mLoss[0m : 2.27195
[1mStep[0m  [60/106], [94mLoss[0m : 2.15403
[1mStep[0m  [70/106], [94mLoss[0m : 2.14247
[1mStep[0m  [80/106], [94mLoss[0m : 1.99602
[1mStep[0m  [90/106], [94mLoss[0m : 1.93327
[1mStep[0m  [100/106], [94mLoss[0m : 1.93144

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.066, [92mTest[0m: 2.373, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.95030
[1mStep[0m  [10/106], [94mLoss[0m : 1.88655
[1mStep[0m  [20/106], [94mLoss[0m : 2.15498
[1mStep[0m  [30/106], [94mLoss[0m : 2.05675
[1mStep[0m  [40/106], [94mLoss[0m : 2.05047
[1mStep[0m  [50/106], [94mLoss[0m : 1.88109
[1mStep[0m  [60/106], [94mLoss[0m : 2.13242
[1mStep[0m  [70/106], [94mLoss[0m : 2.09208
[1mStep[0m  [80/106], [94mLoss[0m : 2.10697
[1mStep[0m  [90/106], [94mLoss[0m : 1.87646
[1mStep[0m  [100/106], [94mLoss[0m : 2.27370

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.034, [92mTest[0m: 2.407, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.16928
[1mStep[0m  [10/106], [94mLoss[0m : 1.88650
[1mStep[0m  [20/106], [94mLoss[0m : 1.94162
[1mStep[0m  [30/106], [94mLoss[0m : 1.97435
[1mStep[0m  [40/106], [94mLoss[0m : 1.86406
[1mStep[0m  [50/106], [94mLoss[0m : 1.93512
[1mStep[0m  [60/106], [94mLoss[0m : 1.94538
[1mStep[0m  [70/106], [94mLoss[0m : 2.03303
[1mStep[0m  [80/106], [94mLoss[0m : 2.28577
[1mStep[0m  [90/106], [94mLoss[0m : 1.93610
[1mStep[0m  [100/106], [94mLoss[0m : 1.85002

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.009, [92mTest[0m: 2.438, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.73104
[1mStep[0m  [10/106], [94mLoss[0m : 1.74296
[1mStep[0m  [20/106], [94mLoss[0m : 1.94995
[1mStep[0m  [30/106], [94mLoss[0m : 2.12236
[1mStep[0m  [40/106], [94mLoss[0m : 2.14869
[1mStep[0m  [50/106], [94mLoss[0m : 1.93621
[1mStep[0m  [60/106], [94mLoss[0m : 1.92525
[1mStep[0m  [70/106], [94mLoss[0m : 1.72751
[1mStep[0m  [80/106], [94mLoss[0m : 1.83166
[1mStep[0m  [90/106], [94mLoss[0m : 1.87839
[1mStep[0m  [100/106], [94mLoss[0m : 1.93221

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.966, [92mTest[0m: 2.450, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.78648
[1mStep[0m  [10/106], [94mLoss[0m : 2.06315
[1mStep[0m  [20/106], [94mLoss[0m : 1.76169
[1mStep[0m  [30/106], [94mLoss[0m : 1.74744
[1mStep[0m  [40/106], [94mLoss[0m : 1.99812
[1mStep[0m  [50/106], [94mLoss[0m : 1.78051
[1mStep[0m  [60/106], [94mLoss[0m : 2.13897
[1mStep[0m  [70/106], [94mLoss[0m : 1.70907
[1mStep[0m  [80/106], [94mLoss[0m : 2.08924
[1mStep[0m  [90/106], [94mLoss[0m : 1.75361
[1mStep[0m  [100/106], [94mLoss[0m : 1.83275

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.926, [92mTest[0m: 2.412, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.14390
[1mStep[0m  [10/106], [94mLoss[0m : 1.73480
[1mStep[0m  [20/106], [94mLoss[0m : 1.93033
[1mStep[0m  [30/106], [94mLoss[0m : 1.88855
[1mStep[0m  [40/106], [94mLoss[0m : 2.32736
[1mStep[0m  [50/106], [94mLoss[0m : 2.03134
[1mStep[0m  [60/106], [94mLoss[0m : 1.86362
[1mStep[0m  [70/106], [94mLoss[0m : 1.98664
[1mStep[0m  [80/106], [94mLoss[0m : 2.08495
[1mStep[0m  [90/106], [94mLoss[0m : 1.69966
[1mStep[0m  [100/106], [94mLoss[0m : 2.28142

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.892, [92mTest[0m: 2.432, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.83092
[1mStep[0m  [10/106], [94mLoss[0m : 1.63816
[1mStep[0m  [20/106], [94mLoss[0m : 1.76973
[1mStep[0m  [30/106], [94mLoss[0m : 1.79125
[1mStep[0m  [40/106], [94mLoss[0m : 1.71329
[1mStep[0m  [50/106], [94mLoss[0m : 1.82545
[1mStep[0m  [60/106], [94mLoss[0m : 1.59522
[1mStep[0m  [70/106], [94mLoss[0m : 1.82540
[1mStep[0m  [80/106], [94mLoss[0m : 1.96078
[1mStep[0m  [90/106], [94mLoss[0m : 1.78222
[1mStep[0m  [100/106], [94mLoss[0m : 1.79691

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.865, [92mTest[0m: 2.419, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.75288
[1mStep[0m  [10/106], [94mLoss[0m : 1.86461
[1mStep[0m  [20/106], [94mLoss[0m : 1.57153
[1mStep[0m  [30/106], [94mLoss[0m : 1.89573
[1mStep[0m  [40/106], [94mLoss[0m : 1.98455
[1mStep[0m  [50/106], [94mLoss[0m : 2.09284
[1mStep[0m  [60/106], [94mLoss[0m : 1.84279
[1mStep[0m  [70/106], [94mLoss[0m : 1.79255
[1mStep[0m  [80/106], [94mLoss[0m : 1.69815
[1mStep[0m  [90/106], [94mLoss[0m : 1.99490
[1mStep[0m  [100/106], [94mLoss[0m : 1.81216

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.855, [92mTest[0m: 2.463, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.91233
[1mStep[0m  [10/106], [94mLoss[0m : 1.99725
[1mStep[0m  [20/106], [94mLoss[0m : 2.05737
[1mStep[0m  [30/106], [94mLoss[0m : 1.73347
[1mStep[0m  [40/106], [94mLoss[0m : 1.83180
[1mStep[0m  [50/106], [94mLoss[0m : 1.99099
[1mStep[0m  [60/106], [94mLoss[0m : 1.81981
[1mStep[0m  [70/106], [94mLoss[0m : 2.12668
[1mStep[0m  [80/106], [94mLoss[0m : 1.89603
[1mStep[0m  [90/106], [94mLoss[0m : 2.05102
[1mStep[0m  [100/106], [94mLoss[0m : 1.85285

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.846, [92mTest[0m: 2.427, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.77229
[1mStep[0m  [10/106], [94mLoss[0m : 1.70707
[1mStep[0m  [20/106], [94mLoss[0m : 1.82231
[1mStep[0m  [30/106], [94mLoss[0m : 1.84546
[1mStep[0m  [40/106], [94mLoss[0m : 1.56605
[1mStep[0m  [50/106], [94mLoss[0m : 1.50840
[1mStep[0m  [60/106], [94mLoss[0m : 1.89140
[1mStep[0m  [70/106], [94mLoss[0m : 1.57635
[1mStep[0m  [80/106], [94mLoss[0m : 1.89527
[1mStep[0m  [90/106], [94mLoss[0m : 2.16066
[1mStep[0m  [100/106], [94mLoss[0m : 1.90465

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.821, [92mTest[0m: 2.457, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.66546
[1mStep[0m  [10/106], [94mLoss[0m : 1.69398
[1mStep[0m  [20/106], [94mLoss[0m : 1.70868
[1mStep[0m  [30/106], [94mLoss[0m : 1.78643
[1mStep[0m  [40/106], [94mLoss[0m : 1.70483
[1mStep[0m  [50/106], [94mLoss[0m : 1.91309
[1mStep[0m  [60/106], [94mLoss[0m : 1.87282
[1mStep[0m  [70/106], [94mLoss[0m : 2.06511
[1mStep[0m  [80/106], [94mLoss[0m : 1.87211
[1mStep[0m  [90/106], [94mLoss[0m : 1.63546
[1mStep[0m  [100/106], [94mLoss[0m : 1.82680

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.771, [92mTest[0m: 2.501, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.83932
[1mStep[0m  [10/106], [94mLoss[0m : 1.66875
[1mStep[0m  [20/106], [94mLoss[0m : 1.85506
[1mStep[0m  [30/106], [94mLoss[0m : 1.56856
[1mStep[0m  [40/106], [94mLoss[0m : 1.65140
[1mStep[0m  [50/106], [94mLoss[0m : 1.61785
[1mStep[0m  [60/106], [94mLoss[0m : 1.74669
[1mStep[0m  [70/106], [94mLoss[0m : 2.02357
[1mStep[0m  [80/106], [94mLoss[0m : 1.95947
[1mStep[0m  [90/106], [94mLoss[0m : 1.84361
[1mStep[0m  [100/106], [94mLoss[0m : 2.02058

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.758, [92mTest[0m: 2.472, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.79247
[1mStep[0m  [10/106], [94mLoss[0m : 1.91347
[1mStep[0m  [20/106], [94mLoss[0m : 1.83540
[1mStep[0m  [30/106], [94mLoss[0m : 2.04805
[1mStep[0m  [40/106], [94mLoss[0m : 1.98446
[1mStep[0m  [50/106], [94mLoss[0m : 1.57183
[1mStep[0m  [60/106], [94mLoss[0m : 1.86339
[1mStep[0m  [70/106], [94mLoss[0m : 1.69104
[1mStep[0m  [80/106], [94mLoss[0m : 1.74433
[1mStep[0m  [90/106], [94mLoss[0m : 1.67764
[1mStep[0m  [100/106], [94mLoss[0m : 1.97805

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.747, [92mTest[0m: 2.468, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.83077
[1mStep[0m  [10/106], [94mLoss[0m : 1.74807
[1mStep[0m  [20/106], [94mLoss[0m : 1.88118
[1mStep[0m  [30/106], [94mLoss[0m : 1.75115
[1mStep[0m  [40/106], [94mLoss[0m : 1.60283
[1mStep[0m  [50/106], [94mLoss[0m : 1.89314
[1mStep[0m  [60/106], [94mLoss[0m : 1.55432
[1mStep[0m  [70/106], [94mLoss[0m : 1.63113
[1mStep[0m  [80/106], [94mLoss[0m : 1.75929
[1mStep[0m  [90/106], [94mLoss[0m : 1.59515
[1mStep[0m  [100/106], [94mLoss[0m : 1.90606

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.720, [92mTest[0m: 2.479, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.74410
[1mStep[0m  [10/106], [94mLoss[0m : 1.46217
[1mStep[0m  [20/106], [94mLoss[0m : 1.64580
[1mStep[0m  [30/106], [94mLoss[0m : 1.54052
[1mStep[0m  [40/106], [94mLoss[0m : 1.74076
[1mStep[0m  [50/106], [94mLoss[0m : 1.81477
[1mStep[0m  [60/106], [94mLoss[0m : 1.62217
[1mStep[0m  [70/106], [94mLoss[0m : 1.73388
[1mStep[0m  [80/106], [94mLoss[0m : 1.66088
[1mStep[0m  [90/106], [94mLoss[0m : 1.90663
[1mStep[0m  [100/106], [94mLoss[0m : 1.75289

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.671, [92mTest[0m: 2.490, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.42723
[1mStep[0m  [10/106], [94mLoss[0m : 1.50862
[1mStep[0m  [20/106], [94mLoss[0m : 1.93113
[1mStep[0m  [30/106], [94mLoss[0m : 1.52923
[1mStep[0m  [40/106], [94mLoss[0m : 1.50358
[1mStep[0m  [50/106], [94mLoss[0m : 2.06863
[1mStep[0m  [60/106], [94mLoss[0m : 1.70748
[1mStep[0m  [70/106], [94mLoss[0m : 1.59632
[1mStep[0m  [80/106], [94mLoss[0m : 1.55429
[1mStep[0m  [90/106], [94mLoss[0m : 1.86424
[1mStep[0m  [100/106], [94mLoss[0m : 1.68139

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.648, [92mTest[0m: 2.480, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.35579
[1mStep[0m  [10/106], [94mLoss[0m : 1.51854
[1mStep[0m  [20/106], [94mLoss[0m : 1.70557
[1mStep[0m  [30/106], [94mLoss[0m : 1.57631
[1mStep[0m  [40/106], [94mLoss[0m : 1.48272
[1mStep[0m  [50/106], [94mLoss[0m : 1.54337
[1mStep[0m  [60/106], [94mLoss[0m : 1.61385
[1mStep[0m  [70/106], [94mLoss[0m : 1.70632
[1mStep[0m  [80/106], [94mLoss[0m : 1.81901
[1mStep[0m  [90/106], [94mLoss[0m : 1.69692
[1mStep[0m  [100/106], [94mLoss[0m : 1.93744

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.648, [92mTest[0m: 2.498, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.44126
[1mStep[0m  [10/106], [94mLoss[0m : 1.67367
[1mStep[0m  [20/106], [94mLoss[0m : 1.65676
[1mStep[0m  [30/106], [94mLoss[0m : 1.71320
[1mStep[0m  [40/106], [94mLoss[0m : 1.58935
[1mStep[0m  [50/106], [94mLoss[0m : 1.51158
[1mStep[0m  [60/106], [94mLoss[0m : 1.79862
[1mStep[0m  [70/106], [94mLoss[0m : 1.83345
[1mStep[0m  [80/106], [94mLoss[0m : 1.82217
[1mStep[0m  [90/106], [94mLoss[0m : 1.86894
[1mStep[0m  [100/106], [94mLoss[0m : 1.64508

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.641, [92mTest[0m: 2.487, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.49982
[1mStep[0m  [10/106], [94mLoss[0m : 1.50525
[1mStep[0m  [20/106], [94mLoss[0m : 1.76653
[1mStep[0m  [30/106], [94mLoss[0m : 1.62091
[1mStep[0m  [40/106], [94mLoss[0m : 1.83797
[1mStep[0m  [50/106], [94mLoss[0m : 1.48880
[1mStep[0m  [60/106], [94mLoss[0m : 1.60367
[1mStep[0m  [70/106], [94mLoss[0m : 1.89363
[1mStep[0m  [80/106], [94mLoss[0m : 1.46871
[1mStep[0m  [90/106], [94mLoss[0m : 1.82511
[1mStep[0m  [100/106], [94mLoss[0m : 1.53789

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.626, [92mTest[0m: 2.487, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.41225
[1mStep[0m  [10/106], [94mLoss[0m : 1.55180
[1mStep[0m  [20/106], [94mLoss[0m : 1.57277
[1mStep[0m  [30/106], [94mLoss[0m : 1.50134
[1mStep[0m  [40/106], [94mLoss[0m : 1.62767
[1mStep[0m  [50/106], [94mLoss[0m : 1.39925
[1mStep[0m  [60/106], [94mLoss[0m : 1.62507
[1mStep[0m  [70/106], [94mLoss[0m : 1.52168
[1mStep[0m  [80/106], [94mLoss[0m : 1.66152
[1mStep[0m  [90/106], [94mLoss[0m : 1.79652
[1mStep[0m  [100/106], [94mLoss[0m : 1.79043

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.619, [92mTest[0m: 2.477, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.55776
[1mStep[0m  [10/106], [94mLoss[0m : 1.44820
[1mStep[0m  [20/106], [94mLoss[0m : 1.93299
[1mStep[0m  [30/106], [94mLoss[0m : 1.40608
[1mStep[0m  [40/106], [94mLoss[0m : 1.68568
[1mStep[0m  [50/106], [94mLoss[0m : 1.53998
[1mStep[0m  [60/106], [94mLoss[0m : 1.50813
[1mStep[0m  [70/106], [94mLoss[0m : 1.55642
[1mStep[0m  [80/106], [94mLoss[0m : 1.66865
[1mStep[0m  [90/106], [94mLoss[0m : 1.59999
[1mStep[0m  [100/106], [94mLoss[0m : 1.72372

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.596, [92mTest[0m: 2.516, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.55295
[1mStep[0m  [10/106], [94mLoss[0m : 1.57840
[1mStep[0m  [20/106], [94mLoss[0m : 1.68825
[1mStep[0m  [30/106], [94mLoss[0m : 1.29594
[1mStep[0m  [40/106], [94mLoss[0m : 1.51890
[1mStep[0m  [50/106], [94mLoss[0m : 1.61661
[1mStep[0m  [60/106], [94mLoss[0m : 1.45127
[1mStep[0m  [70/106], [94mLoss[0m : 1.54907
[1mStep[0m  [80/106], [94mLoss[0m : 1.68933
[1mStep[0m  [90/106], [94mLoss[0m : 1.59004
[1mStep[0m  [100/106], [94mLoss[0m : 1.64814

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.584, [92mTest[0m: 2.535, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.62530
[1mStep[0m  [10/106], [94mLoss[0m : 1.55483
[1mStep[0m  [20/106], [94mLoss[0m : 1.65572
[1mStep[0m  [30/106], [94mLoss[0m : 1.49314
[1mStep[0m  [40/106], [94mLoss[0m : 1.52836
[1mStep[0m  [50/106], [94mLoss[0m : 1.36461
[1mStep[0m  [60/106], [94mLoss[0m : 1.42959
[1mStep[0m  [70/106], [94mLoss[0m : 1.63860
[1mStep[0m  [80/106], [94mLoss[0m : 1.46687
[1mStep[0m  [90/106], [94mLoss[0m : 1.59020
[1mStep[0m  [100/106], [94mLoss[0m : 1.88799

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.594, [92mTest[0m: 2.484, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.545
====================================

Phase 2 - Evaluation MAE:  2.544980427004256
MAE score P1       2.336857
MAE score P2        2.54498
loss               1.584163
learning_rate      0.002575
batch_size              128
hidden_sizes      [200, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.9
weight_decay           0.01
Name: 4, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/26], [94mLoss[0m : 11.02210
[1mStep[0m  [2/26], [94mLoss[0m : 11.08690
[1mStep[0m  [4/26], [94mLoss[0m : 10.60876
[1mStep[0m  [6/26], [94mLoss[0m : 10.25852
[1mStep[0m  [8/26], [94mLoss[0m : 9.91767
[1mStep[0m  [10/26], [94mLoss[0m : 9.06596
[1mStep[0m  [12/26], [94mLoss[0m : 8.45194
[1mStep[0m  [14/26], [94mLoss[0m : 8.00453
[1mStep[0m  [16/26], [94mLoss[0m : 7.17858
[1mStep[0m  [18/26], [94mLoss[0m : 6.63423
[1mStep[0m  [20/26], [94mLoss[0m : 6.09029
[1mStep[0m  [22/26], [94mLoss[0m : 5.07258
[1mStep[0m  [24/26], [94mLoss[0m : 4.68388

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 8.153, [92mTest[0m: 10.875, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 4.17643
[1mStep[0m  [2/26], [94mLoss[0m : 3.66830
[1mStep[0m  [4/26], [94mLoss[0m : 3.16897
[1mStep[0m  [6/26], [94mLoss[0m : 2.79931
[1mStep[0m  [8/26], [94mLoss[0m : 2.92271
[1mStep[0m  [10/26], [94mLoss[0m : 3.00430
[1mStep[0m  [12/26], [94mLoss[0m : 2.81569
[1mStep[0m  [14/26], [94mLoss[0m : 2.92102
[1mStep[0m  [16/26], [94mLoss[0m : 2.87428
[1mStep[0m  [18/26], [94mLoss[0m : 2.77109
[1mStep[0m  [20/26], [94mLoss[0m : 2.98265
[1mStep[0m  [22/26], [94mLoss[0m : 2.74613
[1mStep[0m  [24/26], [94mLoss[0m : 2.77129

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.979, [92mTest[0m: 6.077, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.87823
[1mStep[0m  [2/26], [94mLoss[0m : 2.75334
[1mStep[0m  [4/26], [94mLoss[0m : 2.62138
[1mStep[0m  [6/26], [94mLoss[0m : 2.63763
[1mStep[0m  [8/26], [94mLoss[0m : 2.70339
[1mStep[0m  [10/26], [94mLoss[0m : 2.58865
[1mStep[0m  [12/26], [94mLoss[0m : 2.65245
[1mStep[0m  [14/26], [94mLoss[0m : 2.48385
[1mStep[0m  [16/26], [94mLoss[0m : 2.56297
[1mStep[0m  [18/26], [94mLoss[0m : 2.49357
[1mStep[0m  [20/26], [94mLoss[0m : 2.62795
[1mStep[0m  [22/26], [94mLoss[0m : 2.35104
[1mStep[0m  [24/26], [94mLoss[0m : 2.46826

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.625, [92mTest[0m: 2.850, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.57888
[1mStep[0m  [2/26], [94mLoss[0m : 2.62856
[1mStep[0m  [4/26], [94mLoss[0m : 2.50025
[1mStep[0m  [6/26], [94mLoss[0m : 2.43471
[1mStep[0m  [8/26], [94mLoss[0m : 2.52937
[1mStep[0m  [10/26], [94mLoss[0m : 2.72282
[1mStep[0m  [12/26], [94mLoss[0m : 2.45245
[1mStep[0m  [14/26], [94mLoss[0m : 2.62347
[1mStep[0m  [16/26], [94mLoss[0m : 2.72412
[1mStep[0m  [18/26], [94mLoss[0m : 2.64550
[1mStep[0m  [20/26], [94mLoss[0m : 2.55198
[1mStep[0m  [22/26], [94mLoss[0m : 2.52536
[1mStep[0m  [24/26], [94mLoss[0m : 2.47673

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.545, [92mTest[0m: 2.806, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.53467
[1mStep[0m  [2/26], [94mLoss[0m : 2.49011
[1mStep[0m  [4/26], [94mLoss[0m : 2.60857
[1mStep[0m  [6/26], [94mLoss[0m : 2.40121
[1mStep[0m  [8/26], [94mLoss[0m : 2.37194
[1mStep[0m  [10/26], [94mLoss[0m : 2.58331
[1mStep[0m  [12/26], [94mLoss[0m : 2.62613
[1mStep[0m  [14/26], [94mLoss[0m : 2.32520
[1mStep[0m  [16/26], [94mLoss[0m : 2.49559
[1mStep[0m  [18/26], [94mLoss[0m : 2.51366
[1mStep[0m  [20/26], [94mLoss[0m : 2.59585
[1mStep[0m  [22/26], [94mLoss[0m : 2.64567
[1mStep[0m  [24/26], [94mLoss[0m : 2.42898

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.556, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.50843
[1mStep[0m  [2/26], [94mLoss[0m : 2.47843
[1mStep[0m  [4/26], [94mLoss[0m : 2.41689
[1mStep[0m  [6/26], [94mLoss[0m : 2.44399
[1mStep[0m  [8/26], [94mLoss[0m : 2.45628
[1mStep[0m  [10/26], [94mLoss[0m : 2.50137
[1mStep[0m  [12/26], [94mLoss[0m : 2.40439
[1mStep[0m  [14/26], [94mLoss[0m : 2.58156
[1mStep[0m  [16/26], [94mLoss[0m : 2.48866
[1mStep[0m  [18/26], [94mLoss[0m : 2.40403
[1mStep[0m  [20/26], [94mLoss[0m : 2.54564
[1mStep[0m  [22/26], [94mLoss[0m : 2.55665
[1mStep[0m  [24/26], [94mLoss[0m : 2.45760

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.566, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.67902
[1mStep[0m  [2/26], [94mLoss[0m : 2.40427
[1mStep[0m  [4/26], [94mLoss[0m : 2.60887
[1mStep[0m  [6/26], [94mLoss[0m : 2.45629
[1mStep[0m  [8/26], [94mLoss[0m : 2.52214
[1mStep[0m  [10/26], [94mLoss[0m : 2.45014
[1mStep[0m  [12/26], [94mLoss[0m : 2.37405
[1mStep[0m  [14/26], [94mLoss[0m : 2.44042
[1mStep[0m  [16/26], [94mLoss[0m : 2.58900
[1mStep[0m  [18/26], [94mLoss[0m : 2.61460
[1mStep[0m  [20/26], [94mLoss[0m : 2.51301
[1mStep[0m  [22/26], [94mLoss[0m : 2.46625
[1mStep[0m  [24/26], [94mLoss[0m : 2.53746

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.532, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.50384
[1mStep[0m  [2/26], [94mLoss[0m : 2.48673
[1mStep[0m  [4/26], [94mLoss[0m : 2.45377
[1mStep[0m  [6/26], [94mLoss[0m : 2.45020
[1mStep[0m  [8/26], [94mLoss[0m : 2.37292
[1mStep[0m  [10/26], [94mLoss[0m : 2.41983
[1mStep[0m  [12/26], [94mLoss[0m : 2.45263
[1mStep[0m  [14/26], [94mLoss[0m : 2.56165
[1mStep[0m  [16/26], [94mLoss[0m : 2.44870
[1mStep[0m  [18/26], [94mLoss[0m : 2.42792
[1mStep[0m  [20/26], [94mLoss[0m : 2.39881
[1mStep[0m  [22/26], [94mLoss[0m : 2.39369
[1mStep[0m  [24/26], [94mLoss[0m : 2.55231

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.488, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.29976
[1mStep[0m  [2/26], [94mLoss[0m : 2.49489
[1mStep[0m  [4/26], [94mLoss[0m : 2.40277
[1mStep[0m  [6/26], [94mLoss[0m : 2.50780
[1mStep[0m  [8/26], [94mLoss[0m : 2.46277
[1mStep[0m  [10/26], [94mLoss[0m : 2.50119
[1mStep[0m  [12/26], [94mLoss[0m : 2.61715
[1mStep[0m  [14/26], [94mLoss[0m : 2.35012
[1mStep[0m  [16/26], [94mLoss[0m : 2.44567
[1mStep[0m  [18/26], [94mLoss[0m : 2.65204
[1mStep[0m  [20/26], [94mLoss[0m : 2.53084
[1mStep[0m  [22/26], [94mLoss[0m : 2.45845
[1mStep[0m  [24/26], [94mLoss[0m : 2.42849

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.439, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.49921
[1mStep[0m  [2/26], [94mLoss[0m : 2.44036
[1mStep[0m  [4/26], [94mLoss[0m : 2.46320
[1mStep[0m  [6/26], [94mLoss[0m : 2.49994
[1mStep[0m  [8/26], [94mLoss[0m : 2.47184
[1mStep[0m  [10/26], [94mLoss[0m : 2.54659
[1mStep[0m  [12/26], [94mLoss[0m : 2.42412
[1mStep[0m  [14/26], [94mLoss[0m : 2.44233
[1mStep[0m  [16/26], [94mLoss[0m : 2.44243
[1mStep[0m  [18/26], [94mLoss[0m : 2.38105
[1mStep[0m  [20/26], [94mLoss[0m : 2.47924
[1mStep[0m  [22/26], [94mLoss[0m : 2.49421
[1mStep[0m  [24/26], [94mLoss[0m : 2.23885

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.407, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.54522
[1mStep[0m  [2/26], [94mLoss[0m : 2.47658
[1mStep[0m  [4/26], [94mLoss[0m : 2.27490
[1mStep[0m  [6/26], [94mLoss[0m : 2.43722
[1mStep[0m  [8/26], [94mLoss[0m : 2.45936
[1mStep[0m  [10/26], [94mLoss[0m : 2.36167
[1mStep[0m  [12/26], [94mLoss[0m : 2.44401
[1mStep[0m  [14/26], [94mLoss[0m : 2.48263
[1mStep[0m  [16/26], [94mLoss[0m : 2.46855
[1mStep[0m  [18/26], [94mLoss[0m : 2.46014
[1mStep[0m  [20/26], [94mLoss[0m : 2.54744
[1mStep[0m  [22/26], [94mLoss[0m : 2.45651
[1mStep[0m  [24/26], [94mLoss[0m : 2.39197

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.412, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.40273
[1mStep[0m  [2/26], [94mLoss[0m : 2.62375
[1mStep[0m  [4/26], [94mLoss[0m : 2.47703
[1mStep[0m  [6/26], [94mLoss[0m : 2.38716
[1mStep[0m  [8/26], [94mLoss[0m : 2.58132
[1mStep[0m  [10/26], [94mLoss[0m : 2.46119
[1mStep[0m  [12/26], [94mLoss[0m : 2.39673
[1mStep[0m  [14/26], [94mLoss[0m : 2.65106
[1mStep[0m  [16/26], [94mLoss[0m : 2.29783
[1mStep[0m  [18/26], [94mLoss[0m : 2.46858
[1mStep[0m  [20/26], [94mLoss[0m : 2.33941
[1mStep[0m  [22/26], [94mLoss[0m : 2.38799
[1mStep[0m  [24/26], [94mLoss[0m : 2.50272

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.432, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.49030
[1mStep[0m  [2/26], [94mLoss[0m : 2.25760
[1mStep[0m  [4/26], [94mLoss[0m : 2.38501
[1mStep[0m  [6/26], [94mLoss[0m : 2.39607
[1mStep[0m  [8/26], [94mLoss[0m : 2.38453
[1mStep[0m  [10/26], [94mLoss[0m : 2.43207
[1mStep[0m  [12/26], [94mLoss[0m : 2.45221
[1mStep[0m  [14/26], [94mLoss[0m : 2.41145
[1mStep[0m  [16/26], [94mLoss[0m : 2.33777
[1mStep[0m  [18/26], [94mLoss[0m : 2.40706
[1mStep[0m  [20/26], [94mLoss[0m : 2.46202
[1mStep[0m  [22/26], [94mLoss[0m : 2.37184
[1mStep[0m  [24/26], [94mLoss[0m : 2.43623

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.456, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.40085
[1mStep[0m  [2/26], [94mLoss[0m : 2.48286
[1mStep[0m  [4/26], [94mLoss[0m : 2.48484
[1mStep[0m  [6/26], [94mLoss[0m : 2.41663
[1mStep[0m  [8/26], [94mLoss[0m : 2.59070
[1mStep[0m  [10/26], [94mLoss[0m : 2.40956
[1mStep[0m  [12/26], [94mLoss[0m : 2.50182
[1mStep[0m  [14/26], [94mLoss[0m : 2.69503
[1mStep[0m  [16/26], [94mLoss[0m : 2.54821
[1mStep[0m  [18/26], [94mLoss[0m : 2.58154
[1mStep[0m  [20/26], [94mLoss[0m : 2.49443
[1mStep[0m  [22/26], [94mLoss[0m : 2.41937
[1mStep[0m  [24/26], [94mLoss[0m : 2.37450

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.437, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.35612
[1mStep[0m  [2/26], [94mLoss[0m : 2.34610
[1mStep[0m  [4/26], [94mLoss[0m : 2.44275
[1mStep[0m  [6/26], [94mLoss[0m : 2.52630
[1mStep[0m  [8/26], [94mLoss[0m : 2.34400
[1mStep[0m  [10/26], [94mLoss[0m : 2.61107
[1mStep[0m  [12/26], [94mLoss[0m : 2.40495
[1mStep[0m  [14/26], [94mLoss[0m : 2.47841
[1mStep[0m  [16/26], [94mLoss[0m : 2.38425
[1mStep[0m  [18/26], [94mLoss[0m : 2.25876
[1mStep[0m  [20/26], [94mLoss[0m : 2.52610
[1mStep[0m  [22/26], [94mLoss[0m : 2.49337
[1mStep[0m  [24/26], [94mLoss[0m : 2.45632

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.415, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.52917
[1mStep[0m  [2/26], [94mLoss[0m : 2.45372
[1mStep[0m  [4/26], [94mLoss[0m : 2.34453
[1mStep[0m  [6/26], [94mLoss[0m : 2.32252
[1mStep[0m  [8/26], [94mLoss[0m : 2.37757
[1mStep[0m  [10/26], [94mLoss[0m : 2.43471
[1mStep[0m  [12/26], [94mLoss[0m : 2.32440
[1mStep[0m  [14/26], [94mLoss[0m : 2.41534
[1mStep[0m  [16/26], [94mLoss[0m : 2.47881
[1mStep[0m  [18/26], [94mLoss[0m : 2.50025
[1mStep[0m  [20/26], [94mLoss[0m : 2.50052
[1mStep[0m  [22/26], [94mLoss[0m : 2.48626
[1mStep[0m  [24/26], [94mLoss[0m : 2.35289

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.411, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.29500
[1mStep[0m  [2/26], [94mLoss[0m : 2.39858
[1mStep[0m  [4/26], [94mLoss[0m : 2.44328
[1mStep[0m  [6/26], [94mLoss[0m : 2.27911
[1mStep[0m  [8/26], [94mLoss[0m : 2.23108
[1mStep[0m  [10/26], [94mLoss[0m : 2.40495
[1mStep[0m  [12/26], [94mLoss[0m : 2.39226
[1mStep[0m  [14/26], [94mLoss[0m : 2.37556
[1mStep[0m  [16/26], [94mLoss[0m : 2.31437
[1mStep[0m  [18/26], [94mLoss[0m : 2.43078
[1mStep[0m  [20/26], [94mLoss[0m : 2.35866
[1mStep[0m  [22/26], [94mLoss[0m : 2.45814
[1mStep[0m  [24/26], [94mLoss[0m : 2.42077

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.378, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.37823
[1mStep[0m  [2/26], [94mLoss[0m : 2.39185
[1mStep[0m  [4/26], [94mLoss[0m : 2.48243
[1mStep[0m  [6/26], [94mLoss[0m : 2.40556
[1mStep[0m  [8/26], [94mLoss[0m : 2.44113
[1mStep[0m  [10/26], [94mLoss[0m : 2.32858
[1mStep[0m  [12/26], [94mLoss[0m : 2.41852
[1mStep[0m  [14/26], [94mLoss[0m : 2.57794
[1mStep[0m  [16/26], [94mLoss[0m : 2.42756
[1mStep[0m  [18/26], [94mLoss[0m : 2.60359
[1mStep[0m  [20/26], [94mLoss[0m : 2.36546
[1mStep[0m  [22/26], [94mLoss[0m : 2.35500
[1mStep[0m  [24/26], [94mLoss[0m : 2.33203

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.422, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.43350
[1mStep[0m  [2/26], [94mLoss[0m : 2.46299
[1mStep[0m  [4/26], [94mLoss[0m : 2.28185
[1mStep[0m  [6/26], [94mLoss[0m : 2.48514
[1mStep[0m  [8/26], [94mLoss[0m : 2.43339
[1mStep[0m  [10/26], [94mLoss[0m : 2.35895
[1mStep[0m  [12/26], [94mLoss[0m : 2.45640
[1mStep[0m  [14/26], [94mLoss[0m : 2.34294
[1mStep[0m  [16/26], [94mLoss[0m : 2.37372
[1mStep[0m  [18/26], [94mLoss[0m : 2.25894
[1mStep[0m  [20/26], [94mLoss[0m : 2.28985
[1mStep[0m  [22/26], [94mLoss[0m : 2.36903
[1mStep[0m  [24/26], [94mLoss[0m : 2.39579

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.368, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.34083
[1mStep[0m  [2/26], [94mLoss[0m : 2.30892
[1mStep[0m  [4/26], [94mLoss[0m : 2.50566
[1mStep[0m  [6/26], [94mLoss[0m : 2.48653
[1mStep[0m  [8/26], [94mLoss[0m : 2.38684
[1mStep[0m  [10/26], [94mLoss[0m : 2.31776
[1mStep[0m  [12/26], [94mLoss[0m : 2.46080
[1mStep[0m  [14/26], [94mLoss[0m : 2.30821
[1mStep[0m  [16/26], [94mLoss[0m : 2.40343
[1mStep[0m  [18/26], [94mLoss[0m : 2.41090
[1mStep[0m  [20/26], [94mLoss[0m : 2.58882
[1mStep[0m  [22/26], [94mLoss[0m : 2.43614
[1mStep[0m  [24/26], [94mLoss[0m : 2.53859

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.403, [92mTest[0m: 2.401, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.35915
[1mStep[0m  [2/26], [94mLoss[0m : 2.47130
[1mStep[0m  [4/26], [94mLoss[0m : 2.46139
[1mStep[0m  [6/26], [94mLoss[0m : 2.31904
[1mStep[0m  [8/26], [94mLoss[0m : 2.41459
[1mStep[0m  [10/26], [94mLoss[0m : 2.33202
[1mStep[0m  [12/26], [94mLoss[0m : 2.53810
[1mStep[0m  [14/26], [94mLoss[0m : 2.48093
[1mStep[0m  [16/26], [94mLoss[0m : 2.31484
[1mStep[0m  [18/26], [94mLoss[0m : 2.45860
[1mStep[0m  [20/26], [94mLoss[0m : 2.36139
[1mStep[0m  [22/26], [94mLoss[0m : 2.52976
[1mStep[0m  [24/26], [94mLoss[0m : 2.34729

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.392, [92mTest[0m: 2.404, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.42250
[1mStep[0m  [2/26], [94mLoss[0m : 2.39852
[1mStep[0m  [4/26], [94mLoss[0m : 2.42154
[1mStep[0m  [6/26], [94mLoss[0m : 2.40227
[1mStep[0m  [8/26], [94mLoss[0m : 2.41548
[1mStep[0m  [10/26], [94mLoss[0m : 2.36814
[1mStep[0m  [12/26], [94mLoss[0m : 2.39292
[1mStep[0m  [14/26], [94mLoss[0m : 2.47715
[1mStep[0m  [16/26], [94mLoss[0m : 2.27833
[1mStep[0m  [18/26], [94mLoss[0m : 2.41386
[1mStep[0m  [20/26], [94mLoss[0m : 2.47333
[1mStep[0m  [22/26], [94mLoss[0m : 2.33657
[1mStep[0m  [24/26], [94mLoss[0m : 2.41203

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.387, [92mTest[0m: 2.410, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.39074
[1mStep[0m  [2/26], [94mLoss[0m : 2.43440
[1mStep[0m  [4/26], [94mLoss[0m : 2.27389
[1mStep[0m  [6/26], [94mLoss[0m : 2.40750
[1mStep[0m  [8/26], [94mLoss[0m : 2.27541
[1mStep[0m  [10/26], [94mLoss[0m : 2.63196
[1mStep[0m  [12/26], [94mLoss[0m : 2.35329
[1mStep[0m  [14/26], [94mLoss[0m : 2.39283
[1mStep[0m  [16/26], [94mLoss[0m : 2.57050
[1mStep[0m  [18/26], [94mLoss[0m : 2.25957
[1mStep[0m  [20/26], [94mLoss[0m : 2.38868
[1mStep[0m  [22/26], [94mLoss[0m : 2.48128
[1mStep[0m  [24/26], [94mLoss[0m : 2.28558

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.380, [92mTest[0m: 2.386, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.55981
[1mStep[0m  [2/26], [94mLoss[0m : 2.40570
[1mStep[0m  [4/26], [94mLoss[0m : 2.30328
[1mStep[0m  [6/26], [94mLoss[0m : 2.40875
[1mStep[0m  [8/26], [94mLoss[0m : 2.24464
[1mStep[0m  [10/26], [94mLoss[0m : 2.45233
[1mStep[0m  [12/26], [94mLoss[0m : 2.25581
[1mStep[0m  [14/26], [94mLoss[0m : 2.34400
[1mStep[0m  [16/26], [94mLoss[0m : 2.31641
[1mStep[0m  [18/26], [94mLoss[0m : 2.31806
[1mStep[0m  [20/26], [94mLoss[0m : 2.37736
[1mStep[0m  [22/26], [94mLoss[0m : 2.43687
[1mStep[0m  [24/26], [94mLoss[0m : 2.51611

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.383, [92mTest[0m: 2.399, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.51645
[1mStep[0m  [2/26], [94mLoss[0m : 2.43880
[1mStep[0m  [4/26], [94mLoss[0m : 2.48020
[1mStep[0m  [6/26], [94mLoss[0m : 2.33548
[1mStep[0m  [8/26], [94mLoss[0m : 2.32658
[1mStep[0m  [10/26], [94mLoss[0m : 2.43671
[1mStep[0m  [12/26], [94mLoss[0m : 2.49105
[1mStep[0m  [14/26], [94mLoss[0m : 2.23969
[1mStep[0m  [16/26], [94mLoss[0m : 2.31149
[1mStep[0m  [18/26], [94mLoss[0m : 2.44586
[1mStep[0m  [20/26], [94mLoss[0m : 2.46040
[1mStep[0m  [22/26], [94mLoss[0m : 2.46596
[1mStep[0m  [24/26], [94mLoss[0m : 2.63630

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.401, [92mTest[0m: 2.406, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.39319
[1mStep[0m  [2/26], [94mLoss[0m : 2.39141
[1mStep[0m  [4/26], [94mLoss[0m : 2.31770
[1mStep[0m  [6/26], [94mLoss[0m : 2.41508
[1mStep[0m  [8/26], [94mLoss[0m : 2.39152
[1mStep[0m  [10/26], [94mLoss[0m : 2.30521
[1mStep[0m  [12/26], [94mLoss[0m : 2.42214
[1mStep[0m  [14/26], [94mLoss[0m : 2.43286
[1mStep[0m  [16/26], [94mLoss[0m : 2.40107
[1mStep[0m  [18/26], [94mLoss[0m : 2.36201
[1mStep[0m  [20/26], [94mLoss[0m : 2.54345
[1mStep[0m  [22/26], [94mLoss[0m : 2.46549
[1mStep[0m  [24/26], [94mLoss[0m : 2.30706

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.370, [92mTest[0m: 2.383, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.33546
[1mStep[0m  [2/26], [94mLoss[0m : 2.45745
[1mStep[0m  [4/26], [94mLoss[0m : 2.48128
[1mStep[0m  [6/26], [94mLoss[0m : 2.37691
[1mStep[0m  [8/26], [94mLoss[0m : 2.45220
[1mStep[0m  [10/26], [94mLoss[0m : 2.28818
[1mStep[0m  [12/26], [94mLoss[0m : 2.51758
[1mStep[0m  [14/26], [94mLoss[0m : 2.47395
[1mStep[0m  [16/26], [94mLoss[0m : 2.33863
[1mStep[0m  [18/26], [94mLoss[0m : 2.24491
[1mStep[0m  [20/26], [94mLoss[0m : 2.42260
[1mStep[0m  [22/26], [94mLoss[0m : 2.29751
[1mStep[0m  [24/26], [94mLoss[0m : 2.36196

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.377, [92mTest[0m: 2.382, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.29761
[1mStep[0m  [2/26], [94mLoss[0m : 2.46622
[1mStep[0m  [4/26], [94mLoss[0m : 2.36442
[1mStep[0m  [6/26], [94mLoss[0m : 2.43831
[1mStep[0m  [8/26], [94mLoss[0m : 2.58517
[1mStep[0m  [10/26], [94mLoss[0m : 2.32311
[1mStep[0m  [12/26], [94mLoss[0m : 2.45536
[1mStep[0m  [14/26], [94mLoss[0m : 2.36769
[1mStep[0m  [16/26], [94mLoss[0m : 2.30390
[1mStep[0m  [18/26], [94mLoss[0m : 2.42978
[1mStep[0m  [20/26], [94mLoss[0m : 2.34847
[1mStep[0m  [22/26], [94mLoss[0m : 2.51326
[1mStep[0m  [24/26], [94mLoss[0m : 2.28310

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.366, [92mTest[0m: 2.376, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.48690
[1mStep[0m  [2/26], [94mLoss[0m : 2.40630
[1mStep[0m  [4/26], [94mLoss[0m : 2.39279
[1mStep[0m  [6/26], [94mLoss[0m : 2.35998
[1mStep[0m  [8/26], [94mLoss[0m : 2.44117
[1mStep[0m  [10/26], [94mLoss[0m : 2.32559
[1mStep[0m  [12/26], [94mLoss[0m : 2.48687
[1mStep[0m  [14/26], [94mLoss[0m : 2.33162
[1mStep[0m  [16/26], [94mLoss[0m : 2.44773
[1mStep[0m  [18/26], [94mLoss[0m : 2.44106
[1mStep[0m  [20/26], [94mLoss[0m : 2.40523
[1mStep[0m  [22/26], [94mLoss[0m : 2.31112
[1mStep[0m  [24/26], [94mLoss[0m : 2.44827

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.376, [92mTest[0m: 2.380, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.37774
[1mStep[0m  [2/26], [94mLoss[0m : 2.30431
[1mStep[0m  [4/26], [94mLoss[0m : 2.34519
[1mStep[0m  [6/26], [94mLoss[0m : 2.35997
[1mStep[0m  [8/26], [94mLoss[0m : 2.37029
[1mStep[0m  [10/26], [94mLoss[0m : 2.35825
[1mStep[0m  [12/26], [94mLoss[0m : 2.29218
[1mStep[0m  [14/26], [94mLoss[0m : 2.58974
[1mStep[0m  [16/26], [94mLoss[0m : 2.41270
[1mStep[0m  [18/26], [94mLoss[0m : 2.29304
[1mStep[0m  [20/26], [94mLoss[0m : 2.31688
[1mStep[0m  [22/26], [94mLoss[0m : 2.38682
[1mStep[0m  [24/26], [94mLoss[0m : 2.40307

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.364, [92mTest[0m: 2.368, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.393
====================================

Phase 1 - Evaluation MAE:  2.3929254091702976
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/26], [94mLoss[0m : 2.25576
[1mStep[0m  [2/26], [94mLoss[0m : 2.31875
[1mStep[0m  [4/26], [94mLoss[0m : 2.44484
[1mStep[0m  [6/26], [94mLoss[0m : 2.35255
[1mStep[0m  [8/26], [94mLoss[0m : 2.39724
[1mStep[0m  [10/26], [94mLoss[0m : 2.63720
[1mStep[0m  [12/26], [94mLoss[0m : 2.50869
[1mStep[0m  [14/26], [94mLoss[0m : 2.39409
[1mStep[0m  [16/26], [94mLoss[0m : 2.57409
[1mStep[0m  [18/26], [94mLoss[0m : 2.56315
[1mStep[0m  [20/26], [94mLoss[0m : 2.35298
[1mStep[0m  [22/26], [94mLoss[0m : 2.43540
[1mStep[0m  [24/26], [94mLoss[0m : 2.55232

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.393, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.33818
[1mStep[0m  [2/26], [94mLoss[0m : 2.43657
[1mStep[0m  [4/26], [94mLoss[0m : 2.53659
[1mStep[0m  [6/26], [94mLoss[0m : 2.46409
[1mStep[0m  [8/26], [94mLoss[0m : 2.47903
[1mStep[0m  [10/26], [94mLoss[0m : 2.41699
[1mStep[0m  [12/26], [94mLoss[0m : 2.34173
[1mStep[0m  [14/26], [94mLoss[0m : 2.35415
[1mStep[0m  [16/26], [94mLoss[0m : 2.44935
[1mStep[0m  [18/26], [94mLoss[0m : 2.43677
[1mStep[0m  [20/26], [94mLoss[0m : 2.55732
[1mStep[0m  [22/26], [94mLoss[0m : 2.34536
[1mStep[0m  [24/26], [94mLoss[0m : 2.41304

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.454, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.34255
[1mStep[0m  [2/26], [94mLoss[0m : 2.36892
[1mStep[0m  [4/26], [94mLoss[0m : 2.43046
[1mStep[0m  [6/26], [94mLoss[0m : 2.37968
[1mStep[0m  [8/26], [94mLoss[0m : 2.29410
[1mStep[0m  [10/26], [94mLoss[0m : 2.43755
[1mStep[0m  [12/26], [94mLoss[0m : 2.23810
[1mStep[0m  [14/26], [94mLoss[0m : 2.29320
[1mStep[0m  [16/26], [94mLoss[0m : 2.20226
[1mStep[0m  [18/26], [94mLoss[0m : 2.36578
[1mStep[0m  [20/26], [94mLoss[0m : 2.30325
[1mStep[0m  [22/26], [94mLoss[0m : 2.50017
[1mStep[0m  [24/26], [94mLoss[0m : 2.22809

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.347, [92mTest[0m: 2.661, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.12879
[1mStep[0m  [2/26], [94mLoss[0m : 2.24833
[1mStep[0m  [4/26], [94mLoss[0m : 2.34897
[1mStep[0m  [6/26], [94mLoss[0m : 2.23102
[1mStep[0m  [8/26], [94mLoss[0m : 2.39152
[1mStep[0m  [10/26], [94mLoss[0m : 2.19080
[1mStep[0m  [12/26], [94mLoss[0m : 2.37452
[1mStep[0m  [14/26], [94mLoss[0m : 2.34040
[1mStep[0m  [16/26], [94mLoss[0m : 2.30905
[1mStep[0m  [18/26], [94mLoss[0m : 2.32449
[1mStep[0m  [20/26], [94mLoss[0m : 2.42505
[1mStep[0m  [22/26], [94mLoss[0m : 2.36558
[1mStep[0m  [24/26], [94mLoss[0m : 2.23249

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.297, [92mTest[0m: 3.050, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.15505
[1mStep[0m  [2/26], [94mLoss[0m : 2.20142
[1mStep[0m  [4/26], [94mLoss[0m : 2.12734
[1mStep[0m  [6/26], [94mLoss[0m : 2.25986
[1mStep[0m  [8/26], [94mLoss[0m : 2.19427
[1mStep[0m  [10/26], [94mLoss[0m : 2.20583
[1mStep[0m  [12/26], [94mLoss[0m : 2.41432
[1mStep[0m  [14/26], [94mLoss[0m : 2.16720
[1mStep[0m  [16/26], [94mLoss[0m : 2.21742
[1mStep[0m  [18/26], [94mLoss[0m : 2.19602
[1mStep[0m  [20/26], [94mLoss[0m : 2.32013
[1mStep[0m  [22/26], [94mLoss[0m : 2.24877
[1mStep[0m  [24/26], [94mLoss[0m : 2.27934

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.233, [92mTest[0m: 3.017, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.19573
[1mStep[0m  [2/26], [94mLoss[0m : 2.09254
[1mStep[0m  [4/26], [94mLoss[0m : 2.24406
[1mStep[0m  [6/26], [94mLoss[0m : 2.17713
[1mStep[0m  [8/26], [94mLoss[0m : 2.15596
[1mStep[0m  [10/26], [94mLoss[0m : 2.22810
[1mStep[0m  [12/26], [94mLoss[0m : 2.26641
[1mStep[0m  [14/26], [94mLoss[0m : 2.30293
[1mStep[0m  [16/26], [94mLoss[0m : 2.14664
[1mStep[0m  [18/26], [94mLoss[0m : 2.23900
[1mStep[0m  [20/26], [94mLoss[0m : 2.27217
[1mStep[0m  [22/26], [94mLoss[0m : 2.17273
[1mStep[0m  [24/26], [94mLoss[0m : 2.19405

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.191, [92mTest[0m: 2.627, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.24825
[1mStep[0m  [2/26], [94mLoss[0m : 2.11316
[1mStep[0m  [4/26], [94mLoss[0m : 2.08616
[1mStep[0m  [6/26], [94mLoss[0m : 2.11758
[1mStep[0m  [8/26], [94mLoss[0m : 2.09175
[1mStep[0m  [10/26], [94mLoss[0m : 2.13660
[1mStep[0m  [12/26], [94mLoss[0m : 2.22904
[1mStep[0m  [14/26], [94mLoss[0m : 2.18778
[1mStep[0m  [16/26], [94mLoss[0m : 2.13602
[1mStep[0m  [18/26], [94mLoss[0m : 2.12155
[1mStep[0m  [20/26], [94mLoss[0m : 1.99878
[1mStep[0m  [22/26], [94mLoss[0m : 2.08626
[1mStep[0m  [24/26], [94mLoss[0m : 2.22725

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.140, [92mTest[0m: 2.710, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.13147
[1mStep[0m  [2/26], [94mLoss[0m : 2.04225
[1mStep[0m  [4/26], [94mLoss[0m : 2.01947
[1mStep[0m  [6/26], [94mLoss[0m : 2.15080
[1mStep[0m  [8/26], [94mLoss[0m : 2.09347
[1mStep[0m  [10/26], [94mLoss[0m : 2.22290
[1mStep[0m  [12/26], [94mLoss[0m : 2.21081
[1mStep[0m  [14/26], [94mLoss[0m : 2.03686
[1mStep[0m  [16/26], [94mLoss[0m : 2.00491
[1mStep[0m  [18/26], [94mLoss[0m : 2.21144
[1mStep[0m  [20/26], [94mLoss[0m : 2.26770
[1mStep[0m  [22/26], [94mLoss[0m : 2.16989
[1mStep[0m  [24/26], [94mLoss[0m : 2.23143

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.102, [92mTest[0m: 2.601, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.98953
[1mStep[0m  [2/26], [94mLoss[0m : 2.08048
[1mStep[0m  [4/26], [94mLoss[0m : 1.98491
[1mStep[0m  [6/26], [94mLoss[0m : 2.05891
[1mStep[0m  [8/26], [94mLoss[0m : 2.08518
[1mStep[0m  [10/26], [94mLoss[0m : 2.08172
[1mStep[0m  [12/26], [94mLoss[0m : 2.01605
[1mStep[0m  [14/26], [94mLoss[0m : 2.14875
[1mStep[0m  [16/26], [94mLoss[0m : 2.05468
[1mStep[0m  [18/26], [94mLoss[0m : 2.12014
[1mStep[0m  [20/26], [94mLoss[0m : 2.01905
[1mStep[0m  [22/26], [94mLoss[0m : 1.92774
[1mStep[0m  [24/26], [94mLoss[0m : 2.07468

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.033, [92mTest[0m: 2.633, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.03839
[1mStep[0m  [2/26], [94mLoss[0m : 1.94202
[1mStep[0m  [4/26], [94mLoss[0m : 1.96070
[1mStep[0m  [6/26], [94mLoss[0m : 1.93172
[1mStep[0m  [8/26], [94mLoss[0m : 1.98935
[1mStep[0m  [10/26], [94mLoss[0m : 2.01091
[1mStep[0m  [12/26], [94mLoss[0m : 2.04073
[1mStep[0m  [14/26], [94mLoss[0m : 1.87975
[1mStep[0m  [16/26], [94mLoss[0m : 1.87224
[1mStep[0m  [18/26], [94mLoss[0m : 2.06461
[1mStep[0m  [20/26], [94mLoss[0m : 2.12598
[1mStep[0m  [22/26], [94mLoss[0m : 2.06309
[1mStep[0m  [24/26], [94mLoss[0m : 1.96716

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.997, [92mTest[0m: 2.727, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.95974
[1mStep[0m  [2/26], [94mLoss[0m : 1.86019
[1mStep[0m  [4/26], [94mLoss[0m : 2.01366
[1mStep[0m  [6/26], [94mLoss[0m : 1.98993
[1mStep[0m  [8/26], [94mLoss[0m : 1.93684
[1mStep[0m  [10/26], [94mLoss[0m : 2.00027
[1mStep[0m  [12/26], [94mLoss[0m : 1.88201
[1mStep[0m  [14/26], [94mLoss[0m : 1.99245
[1mStep[0m  [16/26], [94mLoss[0m : 2.00317
[1mStep[0m  [18/26], [94mLoss[0m : 1.91194
[1mStep[0m  [20/26], [94mLoss[0m : 1.92898
[1mStep[0m  [22/26], [94mLoss[0m : 1.87947
[1mStep[0m  [24/26], [94mLoss[0m : 2.00858

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.959, [92mTest[0m: 2.692, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.84155
[1mStep[0m  [2/26], [94mLoss[0m : 2.03071
[1mStep[0m  [4/26], [94mLoss[0m : 1.86561
[1mStep[0m  [6/26], [94mLoss[0m : 1.93389
[1mStep[0m  [8/26], [94mLoss[0m : 1.88440
[1mStep[0m  [10/26], [94mLoss[0m : 1.80269
[1mStep[0m  [12/26], [94mLoss[0m : 1.75915
[1mStep[0m  [14/26], [94mLoss[0m : 2.01642
[1mStep[0m  [16/26], [94mLoss[0m : 1.98272
[1mStep[0m  [18/26], [94mLoss[0m : 1.91217
[1mStep[0m  [20/26], [94mLoss[0m : 1.85265
[1mStep[0m  [22/26], [94mLoss[0m : 1.93970
[1mStep[0m  [24/26], [94mLoss[0m : 1.93127

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.908, [92mTest[0m: 2.477, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.76682
[1mStep[0m  [2/26], [94mLoss[0m : 1.86620
[1mStep[0m  [4/26], [94mLoss[0m : 1.89724
[1mStep[0m  [6/26], [94mLoss[0m : 1.90822
[1mStep[0m  [8/26], [94mLoss[0m : 1.89966
[1mStep[0m  [10/26], [94mLoss[0m : 1.88812
[1mStep[0m  [12/26], [94mLoss[0m : 1.98097
[1mStep[0m  [14/26], [94mLoss[0m : 1.94130
[1mStep[0m  [16/26], [94mLoss[0m : 1.75951
[1mStep[0m  [18/26], [94mLoss[0m : 1.92349
[1mStep[0m  [20/26], [94mLoss[0m : 1.99417
[1mStep[0m  [22/26], [94mLoss[0m : 1.93276
[1mStep[0m  [24/26], [94mLoss[0m : 1.88930

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.877, [92mTest[0m: 2.531, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.70981
[1mStep[0m  [2/26], [94mLoss[0m : 1.76740
[1mStep[0m  [4/26], [94mLoss[0m : 1.97850
[1mStep[0m  [6/26], [94mLoss[0m : 1.73811
[1mStep[0m  [8/26], [94mLoss[0m : 1.71980
[1mStep[0m  [10/26], [94mLoss[0m : 1.96284
[1mStep[0m  [12/26], [94mLoss[0m : 1.80360
[1mStep[0m  [14/26], [94mLoss[0m : 1.80163
[1mStep[0m  [16/26], [94mLoss[0m : 1.86893
[1mStep[0m  [18/26], [94mLoss[0m : 1.93477
[1mStep[0m  [20/26], [94mLoss[0m : 1.85501
[1mStep[0m  [22/26], [94mLoss[0m : 1.80182
[1mStep[0m  [24/26], [94mLoss[0m : 1.82567

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.837, [92mTest[0m: 2.575, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.69772
[1mStep[0m  [2/26], [94mLoss[0m : 1.91912
[1mStep[0m  [4/26], [94mLoss[0m : 1.79316
[1mStep[0m  [6/26], [94mLoss[0m : 1.83183
[1mStep[0m  [8/26], [94mLoss[0m : 1.87477
[1mStep[0m  [10/26], [94mLoss[0m : 1.78099
[1mStep[0m  [12/26], [94mLoss[0m : 1.72036
[1mStep[0m  [14/26], [94mLoss[0m : 1.74677
[1mStep[0m  [16/26], [94mLoss[0m : 1.76297
[1mStep[0m  [18/26], [94mLoss[0m : 1.84045
[1mStep[0m  [20/26], [94mLoss[0m : 1.80606
[1mStep[0m  [22/26], [94mLoss[0m : 1.73484
[1mStep[0m  [24/26], [94mLoss[0m : 1.78646

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.792, [92mTest[0m: 2.691, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.79807
[1mStep[0m  [2/26], [94mLoss[0m : 1.73149
[1mStep[0m  [4/26], [94mLoss[0m : 1.69758
[1mStep[0m  [6/26], [94mLoss[0m : 1.74724
[1mStep[0m  [8/26], [94mLoss[0m : 1.80116
[1mStep[0m  [10/26], [94mLoss[0m : 1.78248
[1mStep[0m  [12/26], [94mLoss[0m : 1.71633
[1mStep[0m  [14/26], [94mLoss[0m : 1.67149
[1mStep[0m  [16/26], [94mLoss[0m : 1.82128
[1mStep[0m  [18/26], [94mLoss[0m : 1.80283
[1mStep[0m  [20/26], [94mLoss[0m : 1.77799
[1mStep[0m  [22/26], [94mLoss[0m : 1.83714
[1mStep[0m  [24/26], [94mLoss[0m : 1.64111

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.761, [92mTest[0m: 2.523, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.76876
[1mStep[0m  [2/26], [94mLoss[0m : 1.69839
[1mStep[0m  [4/26], [94mLoss[0m : 1.67246
[1mStep[0m  [6/26], [94mLoss[0m : 1.69114
[1mStep[0m  [8/26], [94mLoss[0m : 1.69076
[1mStep[0m  [10/26], [94mLoss[0m : 1.74725
[1mStep[0m  [12/26], [94mLoss[0m : 1.82179
[1mStep[0m  [14/26], [94mLoss[0m : 1.82206
[1mStep[0m  [16/26], [94mLoss[0m : 1.75262
[1mStep[0m  [18/26], [94mLoss[0m : 1.67572
[1mStep[0m  [20/26], [94mLoss[0m : 1.73563
[1mStep[0m  [22/26], [94mLoss[0m : 1.67928
[1mStep[0m  [24/26], [94mLoss[0m : 1.73585

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.717, [92mTest[0m: 2.630, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.70365
[1mStep[0m  [2/26], [94mLoss[0m : 1.61767
[1mStep[0m  [4/26], [94mLoss[0m : 1.79274
[1mStep[0m  [6/26], [94mLoss[0m : 1.57714
[1mStep[0m  [8/26], [94mLoss[0m : 1.62530
[1mStep[0m  [10/26], [94mLoss[0m : 1.66542
[1mStep[0m  [12/26], [94mLoss[0m : 1.62733
[1mStep[0m  [14/26], [94mLoss[0m : 1.79967
[1mStep[0m  [16/26], [94mLoss[0m : 1.58097
[1mStep[0m  [18/26], [94mLoss[0m : 1.74944
[1mStep[0m  [20/26], [94mLoss[0m : 1.62564
[1mStep[0m  [22/26], [94mLoss[0m : 1.58779
[1mStep[0m  [24/26], [94mLoss[0m : 1.71230

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.681, [92mTest[0m: 2.681, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.54698
[1mStep[0m  [2/26], [94mLoss[0m : 1.64348
[1mStep[0m  [4/26], [94mLoss[0m : 1.60695
[1mStep[0m  [6/26], [94mLoss[0m : 1.65174
[1mStep[0m  [8/26], [94mLoss[0m : 1.64721
[1mStep[0m  [10/26], [94mLoss[0m : 1.75517
[1mStep[0m  [12/26], [94mLoss[0m : 1.62061
[1mStep[0m  [14/26], [94mLoss[0m : 1.50981
[1mStep[0m  [16/26], [94mLoss[0m : 1.68746
[1mStep[0m  [18/26], [94mLoss[0m : 1.58421
[1mStep[0m  [20/26], [94mLoss[0m : 1.66687
[1mStep[0m  [22/26], [94mLoss[0m : 1.57181
[1mStep[0m  [24/26], [94mLoss[0m : 1.78092

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.653, [92mTest[0m: 2.548, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.60636
[1mStep[0m  [2/26], [94mLoss[0m : 1.54918
[1mStep[0m  [4/26], [94mLoss[0m : 1.63328
[1mStep[0m  [6/26], [94mLoss[0m : 1.62863
[1mStep[0m  [8/26], [94mLoss[0m : 1.61593
[1mStep[0m  [10/26], [94mLoss[0m : 1.68417
[1mStep[0m  [12/26], [94mLoss[0m : 1.62309
[1mStep[0m  [14/26], [94mLoss[0m : 1.60465
[1mStep[0m  [16/26], [94mLoss[0m : 1.68934
[1mStep[0m  [18/26], [94mLoss[0m : 1.56860
[1mStep[0m  [20/26], [94mLoss[0m : 1.56010
[1mStep[0m  [22/26], [94mLoss[0m : 1.64201
[1mStep[0m  [24/26], [94mLoss[0m : 1.70898

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.634, [92mTest[0m: 2.493, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.65282
[1mStep[0m  [2/26], [94mLoss[0m : 1.67325
[1mStep[0m  [4/26], [94mLoss[0m : 1.52173
[1mStep[0m  [6/26], [94mLoss[0m : 1.75262
[1mStep[0m  [8/26], [94mLoss[0m : 1.60437
[1mStep[0m  [10/26], [94mLoss[0m : 1.62347
[1mStep[0m  [12/26], [94mLoss[0m : 1.52253
[1mStep[0m  [14/26], [94mLoss[0m : 1.69931
[1mStep[0m  [16/26], [94mLoss[0m : 1.59666
[1mStep[0m  [18/26], [94mLoss[0m : 1.50267
[1mStep[0m  [20/26], [94mLoss[0m : 1.69052
[1mStep[0m  [22/26], [94mLoss[0m : 1.61033
[1mStep[0m  [24/26], [94mLoss[0m : 1.55152

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.612, [92mTest[0m: 2.508, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.54617
[1mStep[0m  [2/26], [94mLoss[0m : 1.53838
[1mStep[0m  [4/26], [94mLoss[0m : 1.62096
[1mStep[0m  [6/26], [94mLoss[0m : 1.64730
[1mStep[0m  [8/26], [94mLoss[0m : 1.59537
[1mStep[0m  [10/26], [94mLoss[0m : 1.57414
[1mStep[0m  [12/26], [94mLoss[0m : 1.56014
[1mStep[0m  [14/26], [94mLoss[0m : 1.57969
[1mStep[0m  [16/26], [94mLoss[0m : 1.62145
[1mStep[0m  [18/26], [94mLoss[0m : 1.60121
[1mStep[0m  [20/26], [94mLoss[0m : 1.55150
[1mStep[0m  [22/26], [94mLoss[0m : 1.54400
[1mStep[0m  [24/26], [94mLoss[0m : 1.56091

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.582, [92mTest[0m: 2.478, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.52942
[1mStep[0m  [2/26], [94mLoss[0m : 1.52941
[1mStep[0m  [4/26], [94mLoss[0m : 1.66443
[1mStep[0m  [6/26], [94mLoss[0m : 1.59420
[1mStep[0m  [8/26], [94mLoss[0m : 1.51373
[1mStep[0m  [10/26], [94mLoss[0m : 1.65011
[1mStep[0m  [12/26], [94mLoss[0m : 1.47364
[1mStep[0m  [14/26], [94mLoss[0m : 1.61839
[1mStep[0m  [16/26], [94mLoss[0m : 1.47245
[1mStep[0m  [18/26], [94mLoss[0m : 1.51018
[1mStep[0m  [20/26], [94mLoss[0m : 1.46114
[1mStep[0m  [22/26], [94mLoss[0m : 1.61972
[1mStep[0m  [24/26], [94mLoss[0m : 1.61611

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.562, [92mTest[0m: 2.477, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.44960
[1mStep[0m  [2/26], [94mLoss[0m : 1.40608
[1mStep[0m  [4/26], [94mLoss[0m : 1.44320
[1mStep[0m  [6/26], [94mLoss[0m : 1.56075
[1mStep[0m  [8/26], [94mLoss[0m : 1.48735
[1mStep[0m  [10/26], [94mLoss[0m : 1.55123
[1mStep[0m  [12/26], [94mLoss[0m : 1.51292
[1mStep[0m  [14/26], [94mLoss[0m : 1.54110
[1mStep[0m  [16/26], [94mLoss[0m : 1.47713
[1mStep[0m  [18/26], [94mLoss[0m : 1.41739
[1mStep[0m  [20/26], [94mLoss[0m : 1.48274
[1mStep[0m  [22/26], [94mLoss[0m : 1.48726
[1mStep[0m  [24/26], [94mLoss[0m : 1.57185

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.506, [92mTest[0m: 2.457, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.49246
[1mStep[0m  [2/26], [94mLoss[0m : 1.47967
[1mStep[0m  [4/26], [94mLoss[0m : 1.50335
[1mStep[0m  [6/26], [94mLoss[0m : 1.37591
[1mStep[0m  [8/26], [94mLoss[0m : 1.45625
[1mStep[0m  [10/26], [94mLoss[0m : 1.64609
[1mStep[0m  [12/26], [94mLoss[0m : 1.48863
[1mStep[0m  [14/26], [94mLoss[0m : 1.49771
[1mStep[0m  [16/26], [94mLoss[0m : 1.53649
[1mStep[0m  [18/26], [94mLoss[0m : 1.54873
[1mStep[0m  [20/26], [94mLoss[0m : 1.47545
[1mStep[0m  [22/26], [94mLoss[0m : 1.52579
[1mStep[0m  [24/26], [94mLoss[0m : 1.42836

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.503, [92mTest[0m: 2.440, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.42149
[1mStep[0m  [2/26], [94mLoss[0m : 1.40716
[1mStep[0m  [4/26], [94mLoss[0m : 1.55240
[1mStep[0m  [6/26], [94mLoss[0m : 1.41614
[1mStep[0m  [8/26], [94mLoss[0m : 1.51139
[1mStep[0m  [10/26], [94mLoss[0m : 1.46241
[1mStep[0m  [12/26], [94mLoss[0m : 1.49962
[1mStep[0m  [14/26], [94mLoss[0m : 1.50938
[1mStep[0m  [16/26], [94mLoss[0m : 1.56490
[1mStep[0m  [18/26], [94mLoss[0m : 1.39748
[1mStep[0m  [20/26], [94mLoss[0m : 1.44227
[1mStep[0m  [22/26], [94mLoss[0m : 1.50690
[1mStep[0m  [24/26], [94mLoss[0m : 1.43732

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.478, [92mTest[0m: 2.494, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.42164
[1mStep[0m  [2/26], [94mLoss[0m : 1.45174
[1mStep[0m  [4/26], [94mLoss[0m : 1.40386
[1mStep[0m  [6/26], [94mLoss[0m : 1.47737
[1mStep[0m  [8/26], [94mLoss[0m : 1.48017
[1mStep[0m  [10/26], [94mLoss[0m : 1.45612
[1mStep[0m  [12/26], [94mLoss[0m : 1.51404
[1mStep[0m  [14/26], [94mLoss[0m : 1.48157
[1mStep[0m  [16/26], [94mLoss[0m : 1.40398
[1mStep[0m  [18/26], [94mLoss[0m : 1.39020
[1mStep[0m  [20/26], [94mLoss[0m : 1.41265
[1mStep[0m  [22/26], [94mLoss[0m : 1.46469
[1mStep[0m  [24/26], [94mLoss[0m : 1.57608

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.447, [92mTest[0m: 2.439, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.39559
[1mStep[0m  [2/26], [94mLoss[0m : 1.41272
[1mStep[0m  [4/26], [94mLoss[0m : 1.42316
[1mStep[0m  [6/26], [94mLoss[0m : 1.41933
[1mStep[0m  [8/26], [94mLoss[0m : 1.43893
[1mStep[0m  [10/26], [94mLoss[0m : 1.38298
[1mStep[0m  [12/26], [94mLoss[0m : 1.46464
[1mStep[0m  [14/26], [94mLoss[0m : 1.51613
[1mStep[0m  [16/26], [94mLoss[0m : 1.43505
[1mStep[0m  [18/26], [94mLoss[0m : 1.46310
[1mStep[0m  [20/26], [94mLoss[0m : 1.49386
[1mStep[0m  [22/26], [94mLoss[0m : 1.51583
[1mStep[0m  [24/26], [94mLoss[0m : 1.52384

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.452, [92mTest[0m: 2.567, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.35823
[1mStep[0m  [2/26], [94mLoss[0m : 1.31117
[1mStep[0m  [4/26], [94mLoss[0m : 1.39766
[1mStep[0m  [6/26], [94mLoss[0m : 1.46275
[1mStep[0m  [8/26], [94mLoss[0m : 1.40248
[1mStep[0m  [10/26], [94mLoss[0m : 1.41787
[1mStep[0m  [12/26], [94mLoss[0m : 1.39973
[1mStep[0m  [14/26], [94mLoss[0m : 1.60331
[1mStep[0m  [16/26], [94mLoss[0m : 1.40498
[1mStep[0m  [18/26], [94mLoss[0m : 1.39167
[1mStep[0m  [20/26], [94mLoss[0m : 1.44065
[1mStep[0m  [22/26], [94mLoss[0m : 1.50146
[1mStep[0m  [24/26], [94mLoss[0m : 1.59736

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.426, [92mTest[0m: 2.449, [96mlr[0m: 0.0023175
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 28 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.459
====================================

Phase 2 - Evaluation MAE:  2.4591449224031887
MAE score P1      2.392925
MAE score P2      2.459145
loss              1.425662
learning_rate     0.002575
batch_size             512
hidden_sizes         [100]
epochs                  30
activation            relu
optimizer              sgd
early stopping        True
dropout                0.2
momentum               0.9
weight_decay         0.001
Name: 5, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/106], [94mLoss[0m : 11.00325
[1mStep[0m  [10/106], [94mLoss[0m : 11.22425
[1mStep[0m  [20/106], [94mLoss[0m : 10.78881
[1mStep[0m  [30/106], [94mLoss[0m : 10.68461
[1mStep[0m  [40/106], [94mLoss[0m : 10.61459
[1mStep[0m  [50/106], [94mLoss[0m : 10.01346
[1mStep[0m  [60/106], [94mLoss[0m : 10.53259
[1mStep[0m  [70/106], [94mLoss[0m : 10.86446
[1mStep[0m  [80/106], [94mLoss[0m : 10.45559
[1mStep[0m  [90/106], [94mLoss[0m : 10.01741
[1mStep[0m  [100/106], [94mLoss[0m : 10.19216

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.479, [92mTest[0m: 10.927, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.14191
[1mStep[0m  [10/106], [94mLoss[0m : 10.67003
[1mStep[0m  [20/106], [94mLoss[0m : 10.03003
[1mStep[0m  [30/106], [94mLoss[0m : 9.86398
[1mStep[0m  [40/106], [94mLoss[0m : 9.41858
[1mStep[0m  [50/106], [94mLoss[0m : 9.44166
[1mStep[0m  [60/106], [94mLoss[0m : 9.05995
[1mStep[0m  [70/106], [94mLoss[0m : 9.29202
[1mStep[0m  [80/106], [94mLoss[0m : 9.44293
[1mStep[0m  [90/106], [94mLoss[0m : 9.81143
[1mStep[0m  [100/106], [94mLoss[0m : 9.04162

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.746, [92mTest[0m: 9.990, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 8.72136
[1mStep[0m  [10/106], [94mLoss[0m : 8.81853
[1mStep[0m  [20/106], [94mLoss[0m : 8.75900
[1mStep[0m  [30/106], [94mLoss[0m : 8.75851
[1mStep[0m  [40/106], [94mLoss[0m : 8.73003
[1mStep[0m  [50/106], [94mLoss[0m : 9.55118
[1mStep[0m  [60/106], [94mLoss[0m : 9.00143
[1mStep[0m  [70/106], [94mLoss[0m : 8.10231
[1mStep[0m  [80/106], [94mLoss[0m : 8.87733
[1mStep[0m  [90/106], [94mLoss[0m : 8.03371
[1mStep[0m  [100/106], [94mLoss[0m : 8.36501

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 8.834, [92mTest[0m: 9.036, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 7.81965
[1mStep[0m  [10/106], [94mLoss[0m : 8.50329
[1mStep[0m  [20/106], [94mLoss[0m : 7.92588
[1mStep[0m  [30/106], [94mLoss[0m : 7.71680
[1mStep[0m  [40/106], [94mLoss[0m : 7.93495
[1mStep[0m  [50/106], [94mLoss[0m : 7.72369
[1mStep[0m  [60/106], [94mLoss[0m : 7.97935
[1mStep[0m  [70/106], [94mLoss[0m : 7.74891
[1mStep[0m  [80/106], [94mLoss[0m : 7.38711
[1mStep[0m  [90/106], [94mLoss[0m : 6.64281
[1mStep[0m  [100/106], [94mLoss[0m : 6.53130

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 7.557, [92mTest[0m: 7.759, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 7.08418
[1mStep[0m  [10/106], [94mLoss[0m : 6.81710
[1mStep[0m  [20/106], [94mLoss[0m : 5.43200
[1mStep[0m  [30/106], [94mLoss[0m : 6.86986
[1mStep[0m  [40/106], [94mLoss[0m : 5.92647
[1mStep[0m  [50/106], [94mLoss[0m : 6.78061
[1mStep[0m  [60/106], [94mLoss[0m : 5.99870
[1mStep[0m  [70/106], [94mLoss[0m : 5.71769
[1mStep[0m  [80/106], [94mLoss[0m : 5.66596
[1mStep[0m  [90/106], [94mLoss[0m : 5.75020
[1mStep[0m  [100/106], [94mLoss[0m : 5.59716

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 6.105, [92mTest[0m: 6.214, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 5.48430
[1mStep[0m  [10/106], [94mLoss[0m : 5.18633
[1mStep[0m  [20/106], [94mLoss[0m : 4.79080
[1mStep[0m  [30/106], [94mLoss[0m : 5.45854
[1mStep[0m  [40/106], [94mLoss[0m : 5.06382
[1mStep[0m  [50/106], [94mLoss[0m : 4.76776
[1mStep[0m  [60/106], [94mLoss[0m : 4.93514
[1mStep[0m  [70/106], [94mLoss[0m : 4.87452
[1mStep[0m  [80/106], [94mLoss[0m : 4.19204
[1mStep[0m  [90/106], [94mLoss[0m : 4.81011
[1mStep[0m  [100/106], [94mLoss[0m : 4.72336

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 4.900, [92mTest[0m: 4.721, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 4.15567
[1mStep[0m  [10/106], [94mLoss[0m : 4.47811
[1mStep[0m  [20/106], [94mLoss[0m : 5.06713
[1mStep[0m  [30/106], [94mLoss[0m : 4.01438
[1mStep[0m  [40/106], [94mLoss[0m : 3.75932
[1mStep[0m  [50/106], [94mLoss[0m : 3.50096
[1mStep[0m  [60/106], [94mLoss[0m : 4.12028
[1mStep[0m  [70/106], [94mLoss[0m : 3.24285
[1mStep[0m  [80/106], [94mLoss[0m : 3.47559
[1mStep[0m  [90/106], [94mLoss[0m : 3.61885
[1mStep[0m  [100/106], [94mLoss[0m : 3.80513

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 3.840, [92mTest[0m: 3.747, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.25666
[1mStep[0m  [10/106], [94mLoss[0m : 3.21760
[1mStep[0m  [20/106], [94mLoss[0m : 3.27692
[1mStep[0m  [30/106], [94mLoss[0m : 3.19755
[1mStep[0m  [40/106], [94mLoss[0m : 2.95435
[1mStep[0m  [50/106], [94mLoss[0m : 3.22802
[1mStep[0m  [60/106], [94mLoss[0m : 2.71627
[1mStep[0m  [70/106], [94mLoss[0m : 2.89561
[1mStep[0m  [80/106], [94mLoss[0m : 3.04732
[1mStep[0m  [90/106], [94mLoss[0m : 2.81861
[1mStep[0m  [100/106], [94mLoss[0m : 2.69879

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 3.090, [92mTest[0m: 3.078, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.85810
[1mStep[0m  [10/106], [94mLoss[0m : 2.87128
[1mStep[0m  [20/106], [94mLoss[0m : 2.95054
[1mStep[0m  [30/106], [94mLoss[0m : 3.28031
[1mStep[0m  [40/106], [94mLoss[0m : 2.88955
[1mStep[0m  [50/106], [94mLoss[0m : 2.53549
[1mStep[0m  [60/106], [94mLoss[0m : 2.57284
[1mStep[0m  [70/106], [94mLoss[0m : 2.90323
[1mStep[0m  [80/106], [94mLoss[0m : 2.88154
[1mStep[0m  [90/106], [94mLoss[0m : 2.61410
[1mStep[0m  [100/106], [94mLoss[0m : 2.94664

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.771, [92mTest[0m: 2.661, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.40742
[1mStep[0m  [10/106], [94mLoss[0m : 2.54624
[1mStep[0m  [20/106], [94mLoss[0m : 2.76334
[1mStep[0m  [30/106], [94mLoss[0m : 2.56361
[1mStep[0m  [40/106], [94mLoss[0m : 2.89849
[1mStep[0m  [50/106], [94mLoss[0m : 2.78097
[1mStep[0m  [60/106], [94mLoss[0m : 2.63944
[1mStep[0m  [70/106], [94mLoss[0m : 2.65143
[1mStep[0m  [80/106], [94mLoss[0m : 2.71696
[1mStep[0m  [90/106], [94mLoss[0m : 3.00544
[1mStep[0m  [100/106], [94mLoss[0m : 2.81405

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.662, [92mTest[0m: 2.518, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.77552
[1mStep[0m  [10/106], [94mLoss[0m : 2.53599
[1mStep[0m  [20/106], [94mLoss[0m : 2.40566
[1mStep[0m  [30/106], [94mLoss[0m : 2.58894
[1mStep[0m  [40/106], [94mLoss[0m : 2.67369
[1mStep[0m  [50/106], [94mLoss[0m : 2.77596
[1mStep[0m  [60/106], [94mLoss[0m : 2.77303
[1mStep[0m  [70/106], [94mLoss[0m : 2.59895
[1mStep[0m  [80/106], [94mLoss[0m : 2.52751
[1mStep[0m  [90/106], [94mLoss[0m : 2.66874
[1mStep[0m  [100/106], [94mLoss[0m : 2.55648

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.626, [92mTest[0m: 2.478, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.47171
[1mStep[0m  [10/106], [94mLoss[0m : 2.69100
[1mStep[0m  [20/106], [94mLoss[0m : 2.44679
[1mStep[0m  [30/106], [94mLoss[0m : 2.75409
[1mStep[0m  [40/106], [94mLoss[0m : 3.00306
[1mStep[0m  [50/106], [94mLoss[0m : 2.73848
[1mStep[0m  [60/106], [94mLoss[0m : 2.68920
[1mStep[0m  [70/106], [94mLoss[0m : 2.59271
[1mStep[0m  [80/106], [94mLoss[0m : 2.63563
[1mStep[0m  [90/106], [94mLoss[0m : 2.48206
[1mStep[0m  [100/106], [94mLoss[0m : 2.64792

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.603, [92mTest[0m: 2.446, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.34311
[1mStep[0m  [10/106], [94mLoss[0m : 2.35523
[1mStep[0m  [20/106], [94mLoss[0m : 2.74163
[1mStep[0m  [30/106], [94mLoss[0m : 2.48544
[1mStep[0m  [40/106], [94mLoss[0m : 2.68518
[1mStep[0m  [50/106], [94mLoss[0m : 2.83414
[1mStep[0m  [60/106], [94mLoss[0m : 2.62903
[1mStep[0m  [70/106], [94mLoss[0m : 2.73210
[1mStep[0m  [80/106], [94mLoss[0m : 2.52113
[1mStep[0m  [90/106], [94mLoss[0m : 2.66313
[1mStep[0m  [100/106], [94mLoss[0m : 2.47673

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.579, [92mTest[0m: 2.425, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.70765
[1mStep[0m  [10/106], [94mLoss[0m : 2.73108
[1mStep[0m  [20/106], [94mLoss[0m : 2.57545
[1mStep[0m  [30/106], [94mLoss[0m : 2.74501
[1mStep[0m  [40/106], [94mLoss[0m : 2.60734
[1mStep[0m  [50/106], [94mLoss[0m : 2.62648
[1mStep[0m  [60/106], [94mLoss[0m : 2.64772
[1mStep[0m  [70/106], [94mLoss[0m : 2.69150
[1mStep[0m  [80/106], [94mLoss[0m : 2.60922
[1mStep[0m  [90/106], [94mLoss[0m : 2.39238
[1mStep[0m  [100/106], [94mLoss[0m : 2.61165

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.597, [92mTest[0m: 2.441, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.89698
[1mStep[0m  [10/106], [94mLoss[0m : 2.61112
[1mStep[0m  [20/106], [94mLoss[0m : 2.73072
[1mStep[0m  [30/106], [94mLoss[0m : 2.85407
[1mStep[0m  [40/106], [94mLoss[0m : 2.64276
[1mStep[0m  [50/106], [94mLoss[0m : 2.30432
[1mStep[0m  [60/106], [94mLoss[0m : 2.58961
[1mStep[0m  [70/106], [94mLoss[0m : 2.52710
[1mStep[0m  [80/106], [94mLoss[0m : 2.50049
[1mStep[0m  [90/106], [94mLoss[0m : 2.69585
[1mStep[0m  [100/106], [94mLoss[0m : 2.53360

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.578, [92mTest[0m: 2.435, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.70683
[1mStep[0m  [10/106], [94mLoss[0m : 2.37157
[1mStep[0m  [20/106], [94mLoss[0m : 2.61249
[1mStep[0m  [30/106], [94mLoss[0m : 2.28895
[1mStep[0m  [40/106], [94mLoss[0m : 2.56240
[1mStep[0m  [50/106], [94mLoss[0m : 2.82481
[1mStep[0m  [60/106], [94mLoss[0m : 2.20843
[1mStep[0m  [70/106], [94mLoss[0m : 2.89812
[1mStep[0m  [80/106], [94mLoss[0m : 2.38816
[1mStep[0m  [90/106], [94mLoss[0m : 2.45282
[1mStep[0m  [100/106], [94mLoss[0m : 2.69358

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.581, [92mTest[0m: 2.431, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.50052
[1mStep[0m  [10/106], [94mLoss[0m : 2.43159
[1mStep[0m  [20/106], [94mLoss[0m : 2.25109
[1mStep[0m  [30/106], [94mLoss[0m : 2.55134
[1mStep[0m  [40/106], [94mLoss[0m : 2.54504
[1mStep[0m  [50/106], [94mLoss[0m : 2.59969
[1mStep[0m  [60/106], [94mLoss[0m : 2.32336
[1mStep[0m  [70/106], [94mLoss[0m : 2.47415
[1mStep[0m  [80/106], [94mLoss[0m : 2.59671
[1mStep[0m  [90/106], [94mLoss[0m : 2.77028
[1mStep[0m  [100/106], [94mLoss[0m : 2.52994

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.562, [92mTest[0m: 2.428, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.37098
[1mStep[0m  [10/106], [94mLoss[0m : 2.38878
[1mStep[0m  [20/106], [94mLoss[0m : 2.48528
[1mStep[0m  [30/106], [94mLoss[0m : 2.40502
[1mStep[0m  [40/106], [94mLoss[0m : 2.87314
[1mStep[0m  [50/106], [94mLoss[0m : 2.69463
[1mStep[0m  [60/106], [94mLoss[0m : 2.32586
[1mStep[0m  [70/106], [94mLoss[0m : 2.48503
[1mStep[0m  [80/106], [94mLoss[0m : 2.58955
[1mStep[0m  [90/106], [94mLoss[0m : 2.52362
[1mStep[0m  [100/106], [94mLoss[0m : 2.63677

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.571, [92mTest[0m: 2.397, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.81896
[1mStep[0m  [10/106], [94mLoss[0m : 2.57542
[1mStep[0m  [20/106], [94mLoss[0m : 2.62891
[1mStep[0m  [30/106], [94mLoss[0m : 2.94041
[1mStep[0m  [40/106], [94mLoss[0m : 2.68459
[1mStep[0m  [50/106], [94mLoss[0m : 2.97626
[1mStep[0m  [60/106], [94mLoss[0m : 2.86440
[1mStep[0m  [70/106], [94mLoss[0m : 2.21687
[1mStep[0m  [80/106], [94mLoss[0m : 2.34486
[1mStep[0m  [90/106], [94mLoss[0m : 2.79844
[1mStep[0m  [100/106], [94mLoss[0m : 2.45890

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.545, [92mTest[0m: 2.414, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.64174
[1mStep[0m  [10/106], [94mLoss[0m : 2.38512
[1mStep[0m  [20/106], [94mLoss[0m : 2.40086
[1mStep[0m  [30/106], [94mLoss[0m : 2.39601
[1mStep[0m  [40/106], [94mLoss[0m : 2.44109
[1mStep[0m  [50/106], [94mLoss[0m : 2.45905
[1mStep[0m  [60/106], [94mLoss[0m : 2.22374
[1mStep[0m  [70/106], [94mLoss[0m : 2.69164
[1mStep[0m  [80/106], [94mLoss[0m : 2.77667
[1mStep[0m  [90/106], [94mLoss[0m : 2.89812
[1mStep[0m  [100/106], [94mLoss[0m : 2.68038

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.537, [92mTest[0m: 2.416, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.50436
[1mStep[0m  [10/106], [94mLoss[0m : 2.36146
[1mStep[0m  [20/106], [94mLoss[0m : 2.19098
[1mStep[0m  [30/106], [94mLoss[0m : 2.58824
[1mStep[0m  [40/106], [94mLoss[0m : 2.58583
[1mStep[0m  [50/106], [94mLoss[0m : 2.33878
[1mStep[0m  [60/106], [94mLoss[0m : 2.47429
[1mStep[0m  [70/106], [94mLoss[0m : 2.74361
[1mStep[0m  [80/106], [94mLoss[0m : 2.32131
[1mStep[0m  [90/106], [94mLoss[0m : 2.57656
[1mStep[0m  [100/106], [94mLoss[0m : 2.48154

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.548, [92mTest[0m: 2.414, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.76914
[1mStep[0m  [10/106], [94mLoss[0m : 2.38802
[1mStep[0m  [20/106], [94mLoss[0m : 2.14278
[1mStep[0m  [30/106], [94mLoss[0m : 2.25405
[1mStep[0m  [40/106], [94mLoss[0m : 2.58939
[1mStep[0m  [50/106], [94mLoss[0m : 2.53729
[1mStep[0m  [60/106], [94mLoss[0m : 2.33663
[1mStep[0m  [70/106], [94mLoss[0m : 2.56553
[1mStep[0m  [80/106], [94mLoss[0m : 2.85223
[1mStep[0m  [90/106], [94mLoss[0m : 2.99272
[1mStep[0m  [100/106], [94mLoss[0m : 2.43947

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.539, [92mTest[0m: 2.412, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.48265
[1mStep[0m  [10/106], [94mLoss[0m : 2.35341
[1mStep[0m  [20/106], [94mLoss[0m : 2.51548
[1mStep[0m  [30/106], [94mLoss[0m : 2.35448
[1mStep[0m  [40/106], [94mLoss[0m : 2.42185
[1mStep[0m  [50/106], [94mLoss[0m : 2.30771
[1mStep[0m  [60/106], [94mLoss[0m : 2.47049
[1mStep[0m  [70/106], [94mLoss[0m : 2.42277
[1mStep[0m  [80/106], [94mLoss[0m : 2.59480
[1mStep[0m  [90/106], [94mLoss[0m : 2.68824
[1mStep[0m  [100/106], [94mLoss[0m : 2.49803

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.526, [92mTest[0m: 2.418, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.13004
[1mStep[0m  [10/106], [94mLoss[0m : 2.30904
[1mStep[0m  [20/106], [94mLoss[0m : 2.69828
[1mStep[0m  [30/106], [94mLoss[0m : 2.46724
[1mStep[0m  [40/106], [94mLoss[0m : 2.62102
[1mStep[0m  [50/106], [94mLoss[0m : 2.63982
[1mStep[0m  [60/106], [94mLoss[0m : 2.35274
[1mStep[0m  [70/106], [94mLoss[0m : 2.10000
[1mStep[0m  [80/106], [94mLoss[0m : 2.40010
[1mStep[0m  [90/106], [94mLoss[0m : 2.73260
[1mStep[0m  [100/106], [94mLoss[0m : 1.95965

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.517, [92mTest[0m: 2.425, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.48291
[1mStep[0m  [10/106], [94mLoss[0m : 2.43779
[1mStep[0m  [20/106], [94mLoss[0m : 2.98008
[1mStep[0m  [30/106], [94mLoss[0m : 2.56127
[1mStep[0m  [40/106], [94mLoss[0m : 2.43910
[1mStep[0m  [50/106], [94mLoss[0m : 2.56502
[1mStep[0m  [60/106], [94mLoss[0m : 2.64477
[1mStep[0m  [70/106], [94mLoss[0m : 2.69619
[1mStep[0m  [80/106], [94mLoss[0m : 2.27993
[1mStep[0m  [90/106], [94mLoss[0m : 2.46714
[1mStep[0m  [100/106], [94mLoss[0m : 2.23505

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.519, [92mTest[0m: 2.407, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.78282
[1mStep[0m  [10/106], [94mLoss[0m : 2.39381
[1mStep[0m  [20/106], [94mLoss[0m : 2.51148
[1mStep[0m  [30/106], [94mLoss[0m : 2.44774
[1mStep[0m  [40/106], [94mLoss[0m : 2.26879
[1mStep[0m  [50/106], [94mLoss[0m : 2.38419
[1mStep[0m  [60/106], [94mLoss[0m : 2.76301
[1mStep[0m  [70/106], [94mLoss[0m : 2.57899
[1mStep[0m  [80/106], [94mLoss[0m : 2.55084
[1mStep[0m  [90/106], [94mLoss[0m : 2.50835
[1mStep[0m  [100/106], [94mLoss[0m : 2.50969

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.411, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.66331
[1mStep[0m  [10/106], [94mLoss[0m : 2.50982
[1mStep[0m  [20/106], [94mLoss[0m : 2.36513
[1mStep[0m  [30/106], [94mLoss[0m : 2.46458
[1mStep[0m  [40/106], [94mLoss[0m : 2.09662
[1mStep[0m  [50/106], [94mLoss[0m : 2.78024
[1mStep[0m  [60/106], [94mLoss[0m : 2.62432
[1mStep[0m  [70/106], [94mLoss[0m : 2.47532
[1mStep[0m  [80/106], [94mLoss[0m : 2.53666
[1mStep[0m  [90/106], [94mLoss[0m : 2.58907
[1mStep[0m  [100/106], [94mLoss[0m : 2.66782

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.393, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.53288
[1mStep[0m  [10/106], [94mLoss[0m : 2.36368
[1mStep[0m  [20/106], [94mLoss[0m : 2.70046
[1mStep[0m  [30/106], [94mLoss[0m : 2.72543
[1mStep[0m  [40/106], [94mLoss[0m : 2.35471
[1mStep[0m  [50/106], [94mLoss[0m : 2.59064
[1mStep[0m  [60/106], [94mLoss[0m : 2.45872
[1mStep[0m  [70/106], [94mLoss[0m : 2.37509
[1mStep[0m  [80/106], [94mLoss[0m : 2.36956
[1mStep[0m  [90/106], [94mLoss[0m : 2.37073
[1mStep[0m  [100/106], [94mLoss[0m : 2.27789

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.389, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.36603
[1mStep[0m  [10/106], [94mLoss[0m : 2.56022
[1mStep[0m  [20/106], [94mLoss[0m : 2.61324
[1mStep[0m  [30/106], [94mLoss[0m : 2.46143
[1mStep[0m  [40/106], [94mLoss[0m : 2.61073
[1mStep[0m  [50/106], [94mLoss[0m : 2.69943
[1mStep[0m  [60/106], [94mLoss[0m : 2.20668
[1mStep[0m  [70/106], [94mLoss[0m : 2.63318
[1mStep[0m  [80/106], [94mLoss[0m : 2.51937
[1mStep[0m  [90/106], [94mLoss[0m : 2.57472
[1mStep[0m  [100/106], [94mLoss[0m : 2.60461

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.383, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.62059
[1mStep[0m  [10/106], [94mLoss[0m : 2.90268
[1mStep[0m  [20/106], [94mLoss[0m : 2.68061
[1mStep[0m  [30/106], [94mLoss[0m : 2.21872
[1mStep[0m  [40/106], [94mLoss[0m : 2.31493
[1mStep[0m  [50/106], [94mLoss[0m : 2.32621
[1mStep[0m  [60/106], [94mLoss[0m : 2.52505
[1mStep[0m  [70/106], [94mLoss[0m : 2.54699
[1mStep[0m  [80/106], [94mLoss[0m : 2.62042
[1mStep[0m  [90/106], [94mLoss[0m : 2.36318
[1mStep[0m  [100/106], [94mLoss[0m : 2.37034

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.397, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.385
====================================

Phase 1 - Evaluation MAE:  2.384874514813693
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/106], [94mLoss[0m : 2.79150
[1mStep[0m  [10/106], [94mLoss[0m : 2.68189
[1mStep[0m  [20/106], [94mLoss[0m : 2.25871
[1mStep[0m  [30/106], [94mLoss[0m : 2.45443
[1mStep[0m  [40/106], [94mLoss[0m : 2.08514
[1mStep[0m  [50/106], [94mLoss[0m : 2.90057
[1mStep[0m  [60/106], [94mLoss[0m : 2.37627
[1mStep[0m  [70/106], [94mLoss[0m : 2.61853
[1mStep[0m  [80/106], [94mLoss[0m : 2.54312
[1mStep[0m  [90/106], [94mLoss[0m : 2.29370
[1mStep[0m  [100/106], [94mLoss[0m : 2.66732

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.550, [92mTest[0m: 2.389, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.57789
[1mStep[0m  [10/106], [94mLoss[0m : 2.46858
[1mStep[0m  [20/106], [94mLoss[0m : 2.53839
[1mStep[0m  [30/106], [94mLoss[0m : 2.56158
[1mStep[0m  [40/106], [94mLoss[0m : 2.68477
[1mStep[0m  [50/106], [94mLoss[0m : 2.37843
[1mStep[0m  [60/106], [94mLoss[0m : 2.57048
[1mStep[0m  [70/106], [94mLoss[0m : 2.71549
[1mStep[0m  [80/106], [94mLoss[0m : 2.25264
[1mStep[0m  [90/106], [94mLoss[0m : 2.56046
[1mStep[0m  [100/106], [94mLoss[0m : 2.10480

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.511, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.57232
[1mStep[0m  [10/106], [94mLoss[0m : 2.54920
[1mStep[0m  [20/106], [94mLoss[0m : 2.45693
[1mStep[0m  [30/106], [94mLoss[0m : 2.51578
[1mStep[0m  [40/106], [94mLoss[0m : 2.17433
[1mStep[0m  [50/106], [94mLoss[0m : 2.49747
[1mStep[0m  [60/106], [94mLoss[0m : 2.35290
[1mStep[0m  [70/106], [94mLoss[0m : 2.36009
[1mStep[0m  [80/106], [94mLoss[0m : 2.56842
[1mStep[0m  [90/106], [94mLoss[0m : 2.60001
[1mStep[0m  [100/106], [94mLoss[0m : 2.53683

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.590, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.38270
[1mStep[0m  [10/106], [94mLoss[0m : 2.54221
[1mStep[0m  [20/106], [94mLoss[0m : 2.55842
[1mStep[0m  [30/106], [94mLoss[0m : 2.48430
[1mStep[0m  [40/106], [94mLoss[0m : 2.76203
[1mStep[0m  [50/106], [94mLoss[0m : 2.60469
[1mStep[0m  [60/106], [94mLoss[0m : 2.27688
[1mStep[0m  [70/106], [94mLoss[0m : 2.50557
[1mStep[0m  [80/106], [94mLoss[0m : 2.42749
[1mStep[0m  [90/106], [94mLoss[0m : 2.44216
[1mStep[0m  [100/106], [94mLoss[0m : 2.28733

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.660, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.17762
[1mStep[0m  [10/106], [94mLoss[0m : 2.61322
[1mStep[0m  [20/106], [94mLoss[0m : 2.55440
[1mStep[0m  [30/106], [94mLoss[0m : 2.13628
[1mStep[0m  [40/106], [94mLoss[0m : 2.29371
[1mStep[0m  [50/106], [94mLoss[0m : 2.77983
[1mStep[0m  [60/106], [94mLoss[0m : 2.47397
[1mStep[0m  [70/106], [94mLoss[0m : 2.38979
[1mStep[0m  [80/106], [94mLoss[0m : 2.51699
[1mStep[0m  [90/106], [94mLoss[0m : 2.31676
[1mStep[0m  [100/106], [94mLoss[0m : 2.33830

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.618, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.35854
[1mStep[0m  [10/106], [94mLoss[0m : 2.42654
[1mStep[0m  [20/106], [94mLoss[0m : 1.94184
[1mStep[0m  [30/106], [94mLoss[0m : 2.13673
[1mStep[0m  [40/106], [94mLoss[0m : 2.32336
[1mStep[0m  [50/106], [94mLoss[0m : 2.49559
[1mStep[0m  [60/106], [94mLoss[0m : 2.40497
[1mStep[0m  [70/106], [94mLoss[0m : 2.39339
[1mStep[0m  [80/106], [94mLoss[0m : 2.04803
[1mStep[0m  [90/106], [94mLoss[0m : 2.28471
[1mStep[0m  [100/106], [94mLoss[0m : 2.32285

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.374, [92mTest[0m: 2.517, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.33040
[1mStep[0m  [10/106], [94mLoss[0m : 2.12380
[1mStep[0m  [20/106], [94mLoss[0m : 2.46879
[1mStep[0m  [30/106], [94mLoss[0m : 2.08628
[1mStep[0m  [40/106], [94mLoss[0m : 2.23569
[1mStep[0m  [50/106], [94mLoss[0m : 2.57408
[1mStep[0m  [60/106], [94mLoss[0m : 2.38874
[1mStep[0m  [70/106], [94mLoss[0m : 2.62422
[1mStep[0m  [80/106], [94mLoss[0m : 1.83098
[1mStep[0m  [90/106], [94mLoss[0m : 2.30840
[1mStep[0m  [100/106], [94mLoss[0m : 2.27886

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.353, [92mTest[0m: 2.493, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.31803
[1mStep[0m  [10/106], [94mLoss[0m : 2.42321
[1mStep[0m  [20/106], [94mLoss[0m : 2.46022
[1mStep[0m  [30/106], [94mLoss[0m : 2.26846
[1mStep[0m  [40/106], [94mLoss[0m : 2.32266
[1mStep[0m  [50/106], [94mLoss[0m : 2.42723
[1mStep[0m  [60/106], [94mLoss[0m : 2.36915
[1mStep[0m  [70/106], [94mLoss[0m : 2.37294
[1mStep[0m  [80/106], [94mLoss[0m : 2.52748
[1mStep[0m  [90/106], [94mLoss[0m : 2.23625
[1mStep[0m  [100/106], [94mLoss[0m : 2.73498

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.293, [92mTest[0m: 2.485, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.12053
[1mStep[0m  [10/106], [94mLoss[0m : 2.47333
[1mStep[0m  [20/106], [94mLoss[0m : 2.38286
[1mStep[0m  [30/106], [94mLoss[0m : 2.01144
[1mStep[0m  [40/106], [94mLoss[0m : 2.19807
[1mStep[0m  [50/106], [94mLoss[0m : 2.27099
[1mStep[0m  [60/106], [94mLoss[0m : 2.33068
[1mStep[0m  [70/106], [94mLoss[0m : 2.17812
[1mStep[0m  [80/106], [94mLoss[0m : 2.21112
[1mStep[0m  [90/106], [94mLoss[0m : 1.99095
[1mStep[0m  [100/106], [94mLoss[0m : 2.36855

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.276, [92mTest[0m: 2.501, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.59661
[1mStep[0m  [10/106], [94mLoss[0m : 2.04158
[1mStep[0m  [20/106], [94mLoss[0m : 2.40206
[1mStep[0m  [30/106], [94mLoss[0m : 2.08548
[1mStep[0m  [40/106], [94mLoss[0m : 2.28168
[1mStep[0m  [50/106], [94mLoss[0m : 2.16237
[1mStep[0m  [60/106], [94mLoss[0m : 2.11459
[1mStep[0m  [70/106], [94mLoss[0m : 2.35665
[1mStep[0m  [80/106], [94mLoss[0m : 2.36252
[1mStep[0m  [90/106], [94mLoss[0m : 2.18949
[1mStep[0m  [100/106], [94mLoss[0m : 2.14276

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.224, [92mTest[0m: 2.466, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.88074
[1mStep[0m  [10/106], [94mLoss[0m : 2.17303
[1mStep[0m  [20/106], [94mLoss[0m : 2.14523
[1mStep[0m  [30/106], [94mLoss[0m : 2.33423
[1mStep[0m  [40/106], [94mLoss[0m : 2.04153
[1mStep[0m  [50/106], [94mLoss[0m : 2.31833
[1mStep[0m  [60/106], [94mLoss[0m : 2.13992
[1mStep[0m  [70/106], [94mLoss[0m : 2.11265
[1mStep[0m  [80/106], [94mLoss[0m : 2.21325
[1mStep[0m  [90/106], [94mLoss[0m : 2.23674
[1mStep[0m  [100/106], [94mLoss[0m : 2.46170

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.179, [92mTest[0m: 2.433, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.09879
[1mStep[0m  [10/106], [94mLoss[0m : 2.04947
[1mStep[0m  [20/106], [94mLoss[0m : 2.21595
[1mStep[0m  [30/106], [94mLoss[0m : 2.28920
[1mStep[0m  [40/106], [94mLoss[0m : 2.17453
[1mStep[0m  [50/106], [94mLoss[0m : 2.08539
[1mStep[0m  [60/106], [94mLoss[0m : 1.95480
[1mStep[0m  [70/106], [94mLoss[0m : 1.96001
[1mStep[0m  [80/106], [94mLoss[0m : 2.56429
[1mStep[0m  [90/106], [94mLoss[0m : 2.32545
[1mStep[0m  [100/106], [94mLoss[0m : 2.28033

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.143, [92mTest[0m: 2.463, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.78045
[1mStep[0m  [10/106], [94mLoss[0m : 1.97085
[1mStep[0m  [20/106], [94mLoss[0m : 2.16630
[1mStep[0m  [30/106], [94mLoss[0m : 2.06326
[1mStep[0m  [40/106], [94mLoss[0m : 2.20879
[1mStep[0m  [50/106], [94mLoss[0m : 2.15025
[1mStep[0m  [60/106], [94mLoss[0m : 1.94133
[1mStep[0m  [70/106], [94mLoss[0m : 2.17528
[1mStep[0m  [80/106], [94mLoss[0m : 1.90919
[1mStep[0m  [90/106], [94mLoss[0m : 2.05527
[1mStep[0m  [100/106], [94mLoss[0m : 2.29851

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.111, [92mTest[0m: 2.463, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.85749
[1mStep[0m  [10/106], [94mLoss[0m : 2.15839
[1mStep[0m  [20/106], [94mLoss[0m : 2.07533
[1mStep[0m  [30/106], [94mLoss[0m : 2.10320
[1mStep[0m  [40/106], [94mLoss[0m : 2.05780
[1mStep[0m  [50/106], [94mLoss[0m : 1.74423
[1mStep[0m  [60/106], [94mLoss[0m : 2.25008
[1mStep[0m  [70/106], [94mLoss[0m : 1.79503
[1mStep[0m  [80/106], [94mLoss[0m : 2.07755
[1mStep[0m  [90/106], [94mLoss[0m : 1.86113
[1mStep[0m  [100/106], [94mLoss[0m : 2.12601

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.074, [92mTest[0m: 2.477, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.89889
[1mStep[0m  [10/106], [94mLoss[0m : 1.93789
[1mStep[0m  [20/106], [94mLoss[0m : 2.05150
[1mStep[0m  [30/106], [94mLoss[0m : 1.93621
[1mStep[0m  [40/106], [94mLoss[0m : 2.07424
[1mStep[0m  [50/106], [94mLoss[0m : 2.25354
[1mStep[0m  [60/106], [94mLoss[0m : 2.01311
[1mStep[0m  [70/106], [94mLoss[0m : 2.01294
[1mStep[0m  [80/106], [94mLoss[0m : 2.18890
[1mStep[0m  [90/106], [94mLoss[0m : 2.15905
[1mStep[0m  [100/106], [94mLoss[0m : 1.95632

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.056, [92mTest[0m: 2.418, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.20115
[1mStep[0m  [10/106], [94mLoss[0m : 1.92665
[1mStep[0m  [20/106], [94mLoss[0m : 2.10436
[1mStep[0m  [30/106], [94mLoss[0m : 2.04578
[1mStep[0m  [40/106], [94mLoss[0m : 1.94535
[1mStep[0m  [50/106], [94mLoss[0m : 2.13456
[1mStep[0m  [60/106], [94mLoss[0m : 1.86621
[1mStep[0m  [70/106], [94mLoss[0m : 1.96360
[1mStep[0m  [80/106], [94mLoss[0m : 1.73174
[1mStep[0m  [90/106], [94mLoss[0m : 1.86629
[1mStep[0m  [100/106], [94mLoss[0m : 2.00501

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.018, [92mTest[0m: 2.489, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.05407
[1mStep[0m  [10/106], [94mLoss[0m : 1.82743
[1mStep[0m  [20/106], [94mLoss[0m : 2.08356
[1mStep[0m  [30/106], [94mLoss[0m : 2.06521
[1mStep[0m  [40/106], [94mLoss[0m : 1.87340
[1mStep[0m  [50/106], [94mLoss[0m : 2.14161
[1mStep[0m  [60/106], [94mLoss[0m : 2.00361
[1mStep[0m  [70/106], [94mLoss[0m : 2.10869
[1mStep[0m  [80/106], [94mLoss[0m : 1.99129
[1mStep[0m  [90/106], [94mLoss[0m : 1.77416
[1mStep[0m  [100/106], [94mLoss[0m : 2.27792

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.974, [92mTest[0m: 2.462, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.86121
[1mStep[0m  [10/106], [94mLoss[0m : 1.71071
[1mStep[0m  [20/106], [94mLoss[0m : 1.86130
[1mStep[0m  [30/106], [94mLoss[0m : 1.82901
[1mStep[0m  [40/106], [94mLoss[0m : 1.87243
[1mStep[0m  [50/106], [94mLoss[0m : 2.04632
[1mStep[0m  [60/106], [94mLoss[0m : 2.06840
[1mStep[0m  [70/106], [94mLoss[0m : 2.14734
[1mStep[0m  [80/106], [94mLoss[0m : 1.69107
[1mStep[0m  [90/106], [94mLoss[0m : 2.03094
[1mStep[0m  [100/106], [94mLoss[0m : 1.96059

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.928, [92mTest[0m: 2.435, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.02619
[1mStep[0m  [10/106], [94mLoss[0m : 1.71247
[1mStep[0m  [20/106], [94mLoss[0m : 1.75522
[1mStep[0m  [30/106], [94mLoss[0m : 1.65415
[1mStep[0m  [40/106], [94mLoss[0m : 1.87497
[1mStep[0m  [50/106], [94mLoss[0m : 1.85362
[1mStep[0m  [60/106], [94mLoss[0m : 2.21071
[1mStep[0m  [70/106], [94mLoss[0m : 1.84928
[1mStep[0m  [80/106], [94mLoss[0m : 1.98365
[1mStep[0m  [90/106], [94mLoss[0m : 1.94727
[1mStep[0m  [100/106], [94mLoss[0m : 1.95942

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.923, [92mTest[0m: 2.472, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.78316
[1mStep[0m  [10/106], [94mLoss[0m : 1.84537
[1mStep[0m  [20/106], [94mLoss[0m : 1.82718
[1mStep[0m  [30/106], [94mLoss[0m : 1.82762
[1mStep[0m  [40/106], [94mLoss[0m : 1.80748
[1mStep[0m  [50/106], [94mLoss[0m : 1.94196
[1mStep[0m  [60/106], [94mLoss[0m : 2.14187
[1mStep[0m  [70/106], [94mLoss[0m : 2.16542
[1mStep[0m  [80/106], [94mLoss[0m : 1.69628
[1mStep[0m  [90/106], [94mLoss[0m : 1.94141
[1mStep[0m  [100/106], [94mLoss[0m : 1.81797

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.901, [92mTest[0m: 2.451, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.67669
[1mStep[0m  [10/106], [94mLoss[0m : 1.85864
[1mStep[0m  [20/106], [94mLoss[0m : 1.74176
[1mStep[0m  [30/106], [94mLoss[0m : 1.78049
[1mStep[0m  [40/106], [94mLoss[0m : 1.89326
[1mStep[0m  [50/106], [94mLoss[0m : 1.77908
[1mStep[0m  [60/106], [94mLoss[0m : 1.65067
[1mStep[0m  [70/106], [94mLoss[0m : 1.53496
[1mStep[0m  [80/106], [94mLoss[0m : 1.82595
[1mStep[0m  [90/106], [94mLoss[0m : 2.07837
[1mStep[0m  [100/106], [94mLoss[0m : 1.77450

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.859, [92mTest[0m: 2.429, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.76934
[1mStep[0m  [10/106], [94mLoss[0m : 1.81722
[1mStep[0m  [20/106], [94mLoss[0m : 1.71377
[1mStep[0m  [30/106], [94mLoss[0m : 1.91379
[1mStep[0m  [40/106], [94mLoss[0m : 1.88135
[1mStep[0m  [50/106], [94mLoss[0m : 1.69794
[1mStep[0m  [60/106], [94mLoss[0m : 1.73990
[1mStep[0m  [70/106], [94mLoss[0m : 1.91292
[1mStep[0m  [80/106], [94mLoss[0m : 2.05512
[1mStep[0m  [90/106], [94mLoss[0m : 1.79981
[1mStep[0m  [100/106], [94mLoss[0m : 1.62706

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.831, [92mTest[0m: 2.483, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.62970
[1mStep[0m  [10/106], [94mLoss[0m : 1.55124
[1mStep[0m  [20/106], [94mLoss[0m : 1.88486
[1mStep[0m  [30/106], [94mLoss[0m : 1.91061
[1mStep[0m  [40/106], [94mLoss[0m : 1.81123
[1mStep[0m  [50/106], [94mLoss[0m : 1.83712
[1mStep[0m  [60/106], [94mLoss[0m : 1.61492
[1mStep[0m  [70/106], [94mLoss[0m : 1.70399
[1mStep[0m  [80/106], [94mLoss[0m : 1.68410
[1mStep[0m  [90/106], [94mLoss[0m : 1.81446
[1mStep[0m  [100/106], [94mLoss[0m : 1.70399

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.805, [92mTest[0m: 2.432, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.83080
[1mStep[0m  [10/106], [94mLoss[0m : 1.80556
[1mStep[0m  [20/106], [94mLoss[0m : 1.82897
[1mStep[0m  [30/106], [94mLoss[0m : 1.70825
[1mStep[0m  [40/106], [94mLoss[0m : 1.50582
[1mStep[0m  [50/106], [94mLoss[0m : 1.73809
[1mStep[0m  [60/106], [94mLoss[0m : 1.99476
[1mStep[0m  [70/106], [94mLoss[0m : 1.87720
[1mStep[0m  [80/106], [94mLoss[0m : 1.75256
[1mStep[0m  [90/106], [94mLoss[0m : 1.94616
[1mStep[0m  [100/106], [94mLoss[0m : 1.82854

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.778, [92mTest[0m: 2.439, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.69183
[1mStep[0m  [10/106], [94mLoss[0m : 1.70552
[1mStep[0m  [20/106], [94mLoss[0m : 1.70816
[1mStep[0m  [30/106], [94mLoss[0m : 1.62440
[1mStep[0m  [40/106], [94mLoss[0m : 2.15005
[1mStep[0m  [50/106], [94mLoss[0m : 1.82736
[1mStep[0m  [60/106], [94mLoss[0m : 1.69794
[1mStep[0m  [70/106], [94mLoss[0m : 1.74430
[1mStep[0m  [80/106], [94mLoss[0m : 1.76065
[1mStep[0m  [90/106], [94mLoss[0m : 1.79118
[1mStep[0m  [100/106], [94mLoss[0m : 1.61626

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.749, [92mTest[0m: 2.457, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.74459
[1mStep[0m  [10/106], [94mLoss[0m : 1.79043
[1mStep[0m  [20/106], [94mLoss[0m : 1.78943
[1mStep[0m  [30/106], [94mLoss[0m : 1.87933
[1mStep[0m  [40/106], [94mLoss[0m : 1.66799
[1mStep[0m  [50/106], [94mLoss[0m : 1.92546
[1mStep[0m  [60/106], [94mLoss[0m : 1.89879
[1mStep[0m  [70/106], [94mLoss[0m : 1.93243
[1mStep[0m  [80/106], [94mLoss[0m : 1.54526
[1mStep[0m  [90/106], [94mLoss[0m : 1.80103
[1mStep[0m  [100/106], [94mLoss[0m : 1.91444

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.723, [92mTest[0m: 2.436, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.58941
[1mStep[0m  [10/106], [94mLoss[0m : 1.60873
[1mStep[0m  [20/106], [94mLoss[0m : 1.72899
[1mStep[0m  [30/106], [94mLoss[0m : 1.63960
[1mStep[0m  [40/106], [94mLoss[0m : 1.74642
[1mStep[0m  [50/106], [94mLoss[0m : 1.71667
[1mStep[0m  [60/106], [94mLoss[0m : 1.83438
[1mStep[0m  [70/106], [94mLoss[0m : 1.83744
[1mStep[0m  [80/106], [94mLoss[0m : 1.94465
[1mStep[0m  [90/106], [94mLoss[0m : 1.79131
[1mStep[0m  [100/106], [94mLoss[0m : 1.84872

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.720, [92mTest[0m: 2.447, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.56184
[1mStep[0m  [10/106], [94mLoss[0m : 1.51833
[1mStep[0m  [20/106], [94mLoss[0m : 1.60990
[1mStep[0m  [30/106], [94mLoss[0m : 1.77259
[1mStep[0m  [40/106], [94mLoss[0m : 1.79790
[1mStep[0m  [50/106], [94mLoss[0m : 1.56116
[1mStep[0m  [60/106], [94mLoss[0m : 1.48198
[1mStep[0m  [70/106], [94mLoss[0m : 1.69395
[1mStep[0m  [80/106], [94mLoss[0m : 1.74407
[1mStep[0m  [90/106], [94mLoss[0m : 1.70259
[1mStep[0m  [100/106], [94mLoss[0m : 1.74429

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.692, [92mTest[0m: 2.456, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.60924
[1mStep[0m  [10/106], [94mLoss[0m : 1.62600
[1mStep[0m  [20/106], [94mLoss[0m : 1.54521
[1mStep[0m  [30/106], [94mLoss[0m : 1.70830
[1mStep[0m  [40/106], [94mLoss[0m : 1.73054
[1mStep[0m  [50/106], [94mLoss[0m : 1.66189
[1mStep[0m  [60/106], [94mLoss[0m : 1.58246
[1mStep[0m  [70/106], [94mLoss[0m : 1.59072
[1mStep[0m  [80/106], [94mLoss[0m : 1.55226
[1mStep[0m  [90/106], [94mLoss[0m : 1.88595
[1mStep[0m  [100/106], [94mLoss[0m : 1.70555

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.665, [92mTest[0m: 2.439, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.48225
[1mStep[0m  [10/106], [94mLoss[0m : 1.63190
[1mStep[0m  [20/106], [94mLoss[0m : 1.50990
[1mStep[0m  [30/106], [94mLoss[0m : 1.82752
[1mStep[0m  [40/106], [94mLoss[0m : 1.47809
[1mStep[0m  [50/106], [94mLoss[0m : 1.77124
[1mStep[0m  [60/106], [94mLoss[0m : 1.50428
[1mStep[0m  [70/106], [94mLoss[0m : 1.51085
[1mStep[0m  [80/106], [94mLoss[0m : 1.79595
[1mStep[0m  [90/106], [94mLoss[0m : 1.58661
[1mStep[0m  [100/106], [94mLoss[0m : 1.61677

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.644, [92mTest[0m: 2.437, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.448
====================================

Phase 2 - Evaluation MAE:  2.4482710316496075
MAE score P1       2.384875
MAE score P2       2.448271
loss               1.644477
learning_rate      0.002575
batch_size              128
hidden_sizes      [200, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.5
weight_decay          0.001
Name: 6, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/106], [94mLoss[0m : 10.99892
[1mStep[0m  [10/106], [94mLoss[0m : 9.31960
[1mStep[0m  [20/106], [94mLoss[0m : 6.41295
[1mStep[0m  [30/106], [94mLoss[0m : 4.05328
[1mStep[0m  [40/106], [94mLoss[0m : 3.31394
[1mStep[0m  [50/106], [94mLoss[0m : 2.81125
[1mStep[0m  [60/106], [94mLoss[0m : 2.64380
[1mStep[0m  [70/106], [94mLoss[0m : 2.96534
[1mStep[0m  [80/106], [94mLoss[0m : 2.74003
[1mStep[0m  [90/106], [94mLoss[0m : 2.66821
[1mStep[0m  [100/106], [94mLoss[0m : 2.26184

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.271, [92mTest[0m: 10.968, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.43032
[1mStep[0m  [10/106], [94mLoss[0m : 2.09754
[1mStep[0m  [20/106], [94mLoss[0m : 2.60766
[1mStep[0m  [30/106], [94mLoss[0m : 2.67840
[1mStep[0m  [40/106], [94mLoss[0m : 2.77088
[1mStep[0m  [50/106], [94mLoss[0m : 2.59986
[1mStep[0m  [60/106], [94mLoss[0m : 2.45602
[1mStep[0m  [70/106], [94mLoss[0m : 2.51266
[1mStep[0m  [80/106], [94mLoss[0m : 2.77293
[1mStep[0m  [90/106], [94mLoss[0m : 2.79596
[1mStep[0m  [100/106], [94mLoss[0m : 2.52787

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.592, [92mTest[0m: 2.464, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.60634
[1mStep[0m  [10/106], [94mLoss[0m : 2.77912
[1mStep[0m  [20/106], [94mLoss[0m : 2.29246
[1mStep[0m  [30/106], [94mLoss[0m : 2.79207
[1mStep[0m  [40/106], [94mLoss[0m : 2.83777
[1mStep[0m  [50/106], [94mLoss[0m : 2.22555
[1mStep[0m  [60/106], [94mLoss[0m : 2.79677
[1mStep[0m  [70/106], [94mLoss[0m : 2.56852
[1mStep[0m  [80/106], [94mLoss[0m : 2.46301
[1mStep[0m  [90/106], [94mLoss[0m : 2.28189
[1mStep[0m  [100/106], [94mLoss[0m : 2.63486

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.582, [92mTest[0m: 2.436, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.58236
[1mStep[0m  [10/106], [94mLoss[0m : 2.64874
[1mStep[0m  [20/106], [94mLoss[0m : 2.67432
[1mStep[0m  [30/106], [94mLoss[0m : 2.42522
[1mStep[0m  [40/106], [94mLoss[0m : 2.15358
[1mStep[0m  [50/106], [94mLoss[0m : 2.34226
[1mStep[0m  [60/106], [94mLoss[0m : 2.66578
[1mStep[0m  [70/106], [94mLoss[0m : 2.44032
[1mStep[0m  [80/106], [94mLoss[0m : 2.31374
[1mStep[0m  [90/106], [94mLoss[0m : 2.95813
[1mStep[0m  [100/106], [94mLoss[0m : 2.61019

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.576, [92mTest[0m: 2.430, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.73202
[1mStep[0m  [10/106], [94mLoss[0m : 2.24073
[1mStep[0m  [20/106], [94mLoss[0m : 2.85040
[1mStep[0m  [30/106], [94mLoss[0m : 3.13493
[1mStep[0m  [40/106], [94mLoss[0m : 2.68316
[1mStep[0m  [50/106], [94mLoss[0m : 2.66669
[1mStep[0m  [60/106], [94mLoss[0m : 2.82165
[1mStep[0m  [70/106], [94mLoss[0m : 2.66617
[1mStep[0m  [80/106], [94mLoss[0m : 2.36249
[1mStep[0m  [90/106], [94mLoss[0m : 2.75756
[1mStep[0m  [100/106], [94mLoss[0m : 2.90599

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.552, [92mTest[0m: 2.421, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.48266
[1mStep[0m  [10/106], [94mLoss[0m : 2.70399
[1mStep[0m  [20/106], [94mLoss[0m : 2.36103
[1mStep[0m  [30/106], [94mLoss[0m : 2.86145
[1mStep[0m  [40/106], [94mLoss[0m : 2.38202
[1mStep[0m  [50/106], [94mLoss[0m : 2.54043
[1mStep[0m  [60/106], [94mLoss[0m : 2.45203
[1mStep[0m  [70/106], [94mLoss[0m : 2.74356
[1mStep[0m  [80/106], [94mLoss[0m : 2.65586
[1mStep[0m  [90/106], [94mLoss[0m : 2.47986
[1mStep[0m  [100/106], [94mLoss[0m : 2.29761

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.572, [92mTest[0m: 2.420, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.78962
[1mStep[0m  [10/106], [94mLoss[0m : 2.43631
[1mStep[0m  [20/106], [94mLoss[0m : 2.63749
[1mStep[0m  [30/106], [94mLoss[0m : 2.23323
[1mStep[0m  [40/106], [94mLoss[0m : 2.66803
[1mStep[0m  [50/106], [94mLoss[0m : 2.75934
[1mStep[0m  [60/106], [94mLoss[0m : 2.29607
[1mStep[0m  [70/106], [94mLoss[0m : 2.64550
[1mStep[0m  [80/106], [94mLoss[0m : 2.46283
[1mStep[0m  [90/106], [94mLoss[0m : 2.10297
[1mStep[0m  [100/106], [94mLoss[0m : 2.37053

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.539, [92mTest[0m: 2.416, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.47229
[1mStep[0m  [10/106], [94mLoss[0m : 2.70951
[1mStep[0m  [20/106], [94mLoss[0m : 2.67993
[1mStep[0m  [30/106], [94mLoss[0m : 2.45973
[1mStep[0m  [40/106], [94mLoss[0m : 2.53673
[1mStep[0m  [50/106], [94mLoss[0m : 2.75608
[1mStep[0m  [60/106], [94mLoss[0m : 2.75792
[1mStep[0m  [70/106], [94mLoss[0m : 2.64046
[1mStep[0m  [80/106], [94mLoss[0m : 2.67559
[1mStep[0m  [90/106], [94mLoss[0m : 2.10641
[1mStep[0m  [100/106], [94mLoss[0m : 2.70367

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.541, [92mTest[0m: 2.419, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.78392
[1mStep[0m  [10/106], [94mLoss[0m : 2.46711
[1mStep[0m  [20/106], [94mLoss[0m : 2.67011
[1mStep[0m  [30/106], [94mLoss[0m : 2.61914
[1mStep[0m  [40/106], [94mLoss[0m : 2.72337
[1mStep[0m  [50/106], [94mLoss[0m : 2.74320
[1mStep[0m  [60/106], [94mLoss[0m : 2.72803
[1mStep[0m  [70/106], [94mLoss[0m : 2.39774
[1mStep[0m  [80/106], [94mLoss[0m : 2.46544
[1mStep[0m  [90/106], [94mLoss[0m : 2.59944
[1mStep[0m  [100/106], [94mLoss[0m : 2.70952

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.408, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.43262
[1mStep[0m  [10/106], [94mLoss[0m : 2.68987
[1mStep[0m  [20/106], [94mLoss[0m : 2.53497
[1mStep[0m  [30/106], [94mLoss[0m : 2.27934
[1mStep[0m  [40/106], [94mLoss[0m : 2.64995
[1mStep[0m  [50/106], [94mLoss[0m : 2.44099
[1mStep[0m  [60/106], [94mLoss[0m : 2.28679
[1mStep[0m  [70/106], [94mLoss[0m : 2.55129
[1mStep[0m  [80/106], [94mLoss[0m : 2.68876
[1mStep[0m  [90/106], [94mLoss[0m : 2.51441
[1mStep[0m  [100/106], [94mLoss[0m : 2.66514

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.536, [92mTest[0m: 2.407, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.61179
[1mStep[0m  [10/106], [94mLoss[0m : 2.52434
[1mStep[0m  [20/106], [94mLoss[0m : 2.71129
[1mStep[0m  [30/106], [94mLoss[0m : 2.91476
[1mStep[0m  [40/106], [94mLoss[0m : 2.64355
[1mStep[0m  [50/106], [94mLoss[0m : 2.64464
[1mStep[0m  [60/106], [94mLoss[0m : 2.55560
[1mStep[0m  [70/106], [94mLoss[0m : 2.27900
[1mStep[0m  [80/106], [94mLoss[0m : 2.51554
[1mStep[0m  [90/106], [94mLoss[0m : 3.11315
[1mStep[0m  [100/106], [94mLoss[0m : 2.51025

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.549, [92mTest[0m: 2.405, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.55962
[1mStep[0m  [10/106], [94mLoss[0m : 2.25173
[1mStep[0m  [20/106], [94mLoss[0m : 2.78212
[1mStep[0m  [30/106], [94mLoss[0m : 2.65571
[1mStep[0m  [40/106], [94mLoss[0m : 2.74415
[1mStep[0m  [50/106], [94mLoss[0m : 2.64362
[1mStep[0m  [60/106], [94mLoss[0m : 2.30319
[1mStep[0m  [70/106], [94mLoss[0m : 2.50810
[1mStep[0m  [80/106], [94mLoss[0m : 2.48413
[1mStep[0m  [90/106], [94mLoss[0m : 2.43565
[1mStep[0m  [100/106], [94mLoss[0m : 2.68212

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.406, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.47395
[1mStep[0m  [10/106], [94mLoss[0m : 2.48785
[1mStep[0m  [20/106], [94mLoss[0m : 2.47051
[1mStep[0m  [30/106], [94mLoss[0m : 2.65797
[1mStep[0m  [40/106], [94mLoss[0m : 2.41592
[1mStep[0m  [50/106], [94mLoss[0m : 2.69360
[1mStep[0m  [60/106], [94mLoss[0m : 2.60834
[1mStep[0m  [70/106], [94mLoss[0m : 2.54736
[1mStep[0m  [80/106], [94mLoss[0m : 2.39484
[1mStep[0m  [90/106], [94mLoss[0m : 2.33702
[1mStep[0m  [100/106], [94mLoss[0m : 2.55092

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.404, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.36185
[1mStep[0m  [10/106], [94mLoss[0m : 2.27500
[1mStep[0m  [20/106], [94mLoss[0m : 2.41683
[1mStep[0m  [30/106], [94mLoss[0m : 2.57326
[1mStep[0m  [40/106], [94mLoss[0m : 2.40389
[1mStep[0m  [50/106], [94mLoss[0m : 2.11180
[1mStep[0m  [60/106], [94mLoss[0m : 2.39080
[1mStep[0m  [70/106], [94mLoss[0m : 2.60590
[1mStep[0m  [80/106], [94mLoss[0m : 2.63308
[1mStep[0m  [90/106], [94mLoss[0m : 2.53223
[1mStep[0m  [100/106], [94mLoss[0m : 2.36184

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.404, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.35212
[1mStep[0m  [10/106], [94mLoss[0m : 2.60886
[1mStep[0m  [20/106], [94mLoss[0m : 2.50777
[1mStep[0m  [30/106], [94mLoss[0m : 2.74987
[1mStep[0m  [40/106], [94mLoss[0m : 2.37802
[1mStep[0m  [50/106], [94mLoss[0m : 2.86736
[1mStep[0m  [60/106], [94mLoss[0m : 2.40653
[1mStep[0m  [70/106], [94mLoss[0m : 2.44769
[1mStep[0m  [80/106], [94mLoss[0m : 2.36616
[1mStep[0m  [90/106], [94mLoss[0m : 2.26118
[1mStep[0m  [100/106], [94mLoss[0m : 2.77134

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.402, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.81391
[1mStep[0m  [10/106], [94mLoss[0m : 2.53331
[1mStep[0m  [20/106], [94mLoss[0m : 2.44573
[1mStep[0m  [30/106], [94mLoss[0m : 2.44565
[1mStep[0m  [40/106], [94mLoss[0m : 2.43537
[1mStep[0m  [50/106], [94mLoss[0m : 2.58905
[1mStep[0m  [60/106], [94mLoss[0m : 2.59771
[1mStep[0m  [70/106], [94mLoss[0m : 2.55451
[1mStep[0m  [80/106], [94mLoss[0m : 2.35490
[1mStep[0m  [90/106], [94mLoss[0m : 2.37203
[1mStep[0m  [100/106], [94mLoss[0m : 2.51399

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.406, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.68087
[1mStep[0m  [10/106], [94mLoss[0m : 2.37086
[1mStep[0m  [20/106], [94mLoss[0m : 2.23258
[1mStep[0m  [30/106], [94mLoss[0m : 2.60687
[1mStep[0m  [40/106], [94mLoss[0m : 2.71149
[1mStep[0m  [50/106], [94mLoss[0m : 2.16090
[1mStep[0m  [60/106], [94mLoss[0m : 2.86769
[1mStep[0m  [70/106], [94mLoss[0m : 2.40171
[1mStep[0m  [80/106], [94mLoss[0m : 2.51043
[1mStep[0m  [90/106], [94mLoss[0m : 2.52055
[1mStep[0m  [100/106], [94mLoss[0m : 2.52745

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.398, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.51894
[1mStep[0m  [10/106], [94mLoss[0m : 2.27529
[1mStep[0m  [20/106], [94mLoss[0m : 2.68664
[1mStep[0m  [30/106], [94mLoss[0m : 2.67586
[1mStep[0m  [40/106], [94mLoss[0m : 2.20007
[1mStep[0m  [50/106], [94mLoss[0m : 2.30527
[1mStep[0m  [60/106], [94mLoss[0m : 2.56390
[1mStep[0m  [70/106], [94mLoss[0m : 2.55139
[1mStep[0m  [80/106], [94mLoss[0m : 2.58124
[1mStep[0m  [90/106], [94mLoss[0m : 2.58287
[1mStep[0m  [100/106], [94mLoss[0m : 2.20995

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.401, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.60909
[1mStep[0m  [10/106], [94mLoss[0m : 2.47002
[1mStep[0m  [20/106], [94mLoss[0m : 2.38186
[1mStep[0m  [30/106], [94mLoss[0m : 2.54429
[1mStep[0m  [40/106], [94mLoss[0m : 2.62613
[1mStep[0m  [50/106], [94mLoss[0m : 2.60145
[1mStep[0m  [60/106], [94mLoss[0m : 2.14232
[1mStep[0m  [70/106], [94mLoss[0m : 2.82884
[1mStep[0m  [80/106], [94mLoss[0m : 2.39782
[1mStep[0m  [90/106], [94mLoss[0m : 2.43429
[1mStep[0m  [100/106], [94mLoss[0m : 2.69605

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.398, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.48486
[1mStep[0m  [10/106], [94mLoss[0m : 2.63514
[1mStep[0m  [20/106], [94mLoss[0m : 2.53166
[1mStep[0m  [30/106], [94mLoss[0m : 2.43496
[1mStep[0m  [40/106], [94mLoss[0m : 2.62946
[1mStep[0m  [50/106], [94mLoss[0m : 2.58581
[1mStep[0m  [60/106], [94mLoss[0m : 2.25454
[1mStep[0m  [70/106], [94mLoss[0m : 2.29035
[1mStep[0m  [80/106], [94mLoss[0m : 2.27420
[1mStep[0m  [90/106], [94mLoss[0m : 2.41320
[1mStep[0m  [100/106], [94mLoss[0m : 2.61960

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.407, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.35956
[1mStep[0m  [10/106], [94mLoss[0m : 2.51199
[1mStep[0m  [20/106], [94mLoss[0m : 2.42479
[1mStep[0m  [30/106], [94mLoss[0m : 2.53806
[1mStep[0m  [40/106], [94mLoss[0m : 2.65746
[1mStep[0m  [50/106], [94mLoss[0m : 2.62122
[1mStep[0m  [60/106], [94mLoss[0m : 2.31232
[1mStep[0m  [70/106], [94mLoss[0m : 2.74654
[1mStep[0m  [80/106], [94mLoss[0m : 2.43367
[1mStep[0m  [90/106], [94mLoss[0m : 2.36848
[1mStep[0m  [100/106], [94mLoss[0m : 2.74967

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.510, [92mTest[0m: 2.392, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.95981
[1mStep[0m  [10/106], [94mLoss[0m : 2.70638
[1mStep[0m  [20/106], [94mLoss[0m : 2.73660
[1mStep[0m  [30/106], [94mLoss[0m : 2.39756
[1mStep[0m  [40/106], [94mLoss[0m : 2.44496
[1mStep[0m  [50/106], [94mLoss[0m : 2.54529
[1mStep[0m  [60/106], [94mLoss[0m : 2.37346
[1mStep[0m  [70/106], [94mLoss[0m : 2.45065
[1mStep[0m  [80/106], [94mLoss[0m : 2.95979
[1mStep[0m  [90/106], [94mLoss[0m : 2.51128
[1mStep[0m  [100/106], [94mLoss[0m : 2.15100

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.515, [92mTest[0m: 2.397, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.63568
[1mStep[0m  [10/106], [94mLoss[0m : 2.36121
[1mStep[0m  [20/106], [94mLoss[0m : 2.46418
[1mStep[0m  [30/106], [94mLoss[0m : 2.43613
[1mStep[0m  [40/106], [94mLoss[0m : 2.53477
[1mStep[0m  [50/106], [94mLoss[0m : 2.89515
[1mStep[0m  [60/106], [94mLoss[0m : 2.40381
[1mStep[0m  [70/106], [94mLoss[0m : 2.38881
[1mStep[0m  [80/106], [94mLoss[0m : 2.64379
[1mStep[0m  [90/106], [94mLoss[0m : 2.59383
[1mStep[0m  [100/106], [94mLoss[0m : 2.40335

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.514, [92mTest[0m: 2.400, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.58003
[1mStep[0m  [10/106], [94mLoss[0m : 2.47010
[1mStep[0m  [20/106], [94mLoss[0m : 2.54417
[1mStep[0m  [30/106], [94mLoss[0m : 2.52581
[1mStep[0m  [40/106], [94mLoss[0m : 2.51559
[1mStep[0m  [50/106], [94mLoss[0m : 2.82895
[1mStep[0m  [60/106], [94mLoss[0m : 2.51308
[1mStep[0m  [70/106], [94mLoss[0m : 2.39667
[1mStep[0m  [80/106], [94mLoss[0m : 2.37347
[1mStep[0m  [90/106], [94mLoss[0m : 2.26644
[1mStep[0m  [100/106], [94mLoss[0m : 2.70774

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.399, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.70594
[1mStep[0m  [10/106], [94mLoss[0m : 2.32651
[1mStep[0m  [20/106], [94mLoss[0m : 2.61334
[1mStep[0m  [30/106], [94mLoss[0m : 2.48176
[1mStep[0m  [40/106], [94mLoss[0m : 2.34939
[1mStep[0m  [50/106], [94mLoss[0m : 2.46563
[1mStep[0m  [60/106], [94mLoss[0m : 2.75126
[1mStep[0m  [70/106], [94mLoss[0m : 2.75780
[1mStep[0m  [80/106], [94mLoss[0m : 2.34633
[1mStep[0m  [90/106], [94mLoss[0m : 2.47857
[1mStep[0m  [100/106], [94mLoss[0m : 2.51403

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.517, [92mTest[0m: 2.400, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.54285
[1mStep[0m  [10/106], [94mLoss[0m : 2.38648
[1mStep[0m  [20/106], [94mLoss[0m : 2.65902
[1mStep[0m  [30/106], [94mLoss[0m : 2.27931
[1mStep[0m  [40/106], [94mLoss[0m : 2.69116
[1mStep[0m  [50/106], [94mLoss[0m : 2.38688
[1mStep[0m  [60/106], [94mLoss[0m : 2.44090
[1mStep[0m  [70/106], [94mLoss[0m : 2.52820
[1mStep[0m  [80/106], [94mLoss[0m : 2.22017
[1mStep[0m  [90/106], [94mLoss[0m : 2.41044
[1mStep[0m  [100/106], [94mLoss[0m : 2.46999

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.397, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.74431
[1mStep[0m  [10/106], [94mLoss[0m : 2.54434
[1mStep[0m  [20/106], [94mLoss[0m : 2.49350
[1mStep[0m  [30/106], [94mLoss[0m : 2.71882
[1mStep[0m  [40/106], [94mLoss[0m : 2.80963
[1mStep[0m  [50/106], [94mLoss[0m : 2.14502
[1mStep[0m  [60/106], [94mLoss[0m : 2.64011
[1mStep[0m  [70/106], [94mLoss[0m : 2.40369
[1mStep[0m  [80/106], [94mLoss[0m : 2.24553
[1mStep[0m  [90/106], [94mLoss[0m : 2.82603
[1mStep[0m  [100/106], [94mLoss[0m : 2.26166

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.510, [92mTest[0m: 2.403, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.32805
[1mStep[0m  [10/106], [94mLoss[0m : 2.80906
[1mStep[0m  [20/106], [94mLoss[0m : 2.48094
[1mStep[0m  [30/106], [94mLoss[0m : 2.30827
[1mStep[0m  [40/106], [94mLoss[0m : 2.70824
[1mStep[0m  [50/106], [94mLoss[0m : 2.64207
[1mStep[0m  [60/106], [94mLoss[0m : 2.97827
[1mStep[0m  [70/106], [94mLoss[0m : 2.35435
[1mStep[0m  [80/106], [94mLoss[0m : 2.96663
[1mStep[0m  [90/106], [94mLoss[0m : 2.67321
[1mStep[0m  [100/106], [94mLoss[0m : 2.63986

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.396, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.33224
[1mStep[0m  [10/106], [94mLoss[0m : 2.53484
[1mStep[0m  [20/106], [94mLoss[0m : 2.44655
[1mStep[0m  [30/106], [94mLoss[0m : 2.55549
[1mStep[0m  [40/106], [94mLoss[0m : 2.38984
[1mStep[0m  [50/106], [94mLoss[0m : 2.34367
[1mStep[0m  [60/106], [94mLoss[0m : 2.42968
[1mStep[0m  [70/106], [94mLoss[0m : 2.57446
[1mStep[0m  [80/106], [94mLoss[0m : 2.63966
[1mStep[0m  [90/106], [94mLoss[0m : 2.28150
[1mStep[0m  [100/106], [94mLoss[0m : 2.47047

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.408, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.77685
[1mStep[0m  [10/106], [94mLoss[0m : 2.25772
[1mStep[0m  [20/106], [94mLoss[0m : 2.25912
[1mStep[0m  [30/106], [94mLoss[0m : 2.48609
[1mStep[0m  [40/106], [94mLoss[0m : 2.37703
[1mStep[0m  [50/106], [94mLoss[0m : 2.65314
[1mStep[0m  [60/106], [94mLoss[0m : 2.41596
[1mStep[0m  [70/106], [94mLoss[0m : 2.49194
[1mStep[0m  [80/106], [94mLoss[0m : 2.28689
[1mStep[0m  [90/106], [94mLoss[0m : 2.62137
[1mStep[0m  [100/106], [94mLoss[0m : 2.34643

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.407, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.402
====================================

Phase 1 - Evaluation MAE:  2.4018591867302947
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/106], [94mLoss[0m : 2.42131
[1mStep[0m  [10/106], [94mLoss[0m : 2.61184
[1mStep[0m  [20/106], [94mLoss[0m : 2.62609
[1mStep[0m  [30/106], [94mLoss[0m : 2.44494
[1mStep[0m  [40/106], [94mLoss[0m : 2.73629
[1mStep[0m  [50/106], [94mLoss[0m : 2.76657
[1mStep[0m  [60/106], [94mLoss[0m : 2.73415
[1mStep[0m  [70/106], [94mLoss[0m : 2.45315
[1mStep[0m  [80/106], [94mLoss[0m : 2.44802
[1mStep[0m  [90/106], [94mLoss[0m : 2.55184
[1mStep[0m  [100/106], [94mLoss[0m : 2.16609

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.401, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.26914
[1mStep[0m  [10/106], [94mLoss[0m : 2.47047
[1mStep[0m  [20/106], [94mLoss[0m : 2.24133
[1mStep[0m  [30/106], [94mLoss[0m : 2.70411
[1mStep[0m  [40/106], [94mLoss[0m : 2.29565
[1mStep[0m  [50/106], [94mLoss[0m : 2.43005
[1mStep[0m  [60/106], [94mLoss[0m : 2.19821
[1mStep[0m  [70/106], [94mLoss[0m : 2.38090
[1mStep[0m  [80/106], [94mLoss[0m : 2.54598
[1mStep[0m  [90/106], [94mLoss[0m : 2.20116
[1mStep[0m  [100/106], [94mLoss[0m : 2.52154

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.405, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.49303
[1mStep[0m  [10/106], [94mLoss[0m : 2.56485
[1mStep[0m  [20/106], [94mLoss[0m : 2.44513
[1mStep[0m  [30/106], [94mLoss[0m : 2.40318
[1mStep[0m  [40/106], [94mLoss[0m : 2.26244
[1mStep[0m  [50/106], [94mLoss[0m : 2.39341
[1mStep[0m  [60/106], [94mLoss[0m : 2.32644
[1mStep[0m  [70/106], [94mLoss[0m : 2.39842
[1mStep[0m  [80/106], [94mLoss[0m : 2.53757
[1mStep[0m  [90/106], [94mLoss[0m : 2.73824
[1mStep[0m  [100/106], [94mLoss[0m : 2.53399

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.383, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.19434
[1mStep[0m  [10/106], [94mLoss[0m : 2.33145
[1mStep[0m  [20/106], [94mLoss[0m : 2.25781
[1mStep[0m  [30/106], [94mLoss[0m : 2.38457
[1mStep[0m  [40/106], [94mLoss[0m : 2.32484
[1mStep[0m  [50/106], [94mLoss[0m : 2.49301
[1mStep[0m  [60/106], [94mLoss[0m : 2.18017
[1mStep[0m  [70/106], [94mLoss[0m : 1.99637
[1mStep[0m  [80/106], [94mLoss[0m : 2.04822
[1mStep[0m  [90/106], [94mLoss[0m : 2.35130
[1mStep[0m  [100/106], [94mLoss[0m : 2.31106

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.360, [92mTest[0m: 2.377, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.57540
[1mStep[0m  [10/106], [94mLoss[0m : 2.35250
[1mStep[0m  [20/106], [94mLoss[0m : 2.15460
[1mStep[0m  [30/106], [94mLoss[0m : 2.38628
[1mStep[0m  [40/106], [94mLoss[0m : 2.31120
[1mStep[0m  [50/106], [94mLoss[0m : 2.15263
[1mStep[0m  [60/106], [94mLoss[0m : 2.38410
[1mStep[0m  [70/106], [94mLoss[0m : 2.57490
[1mStep[0m  [80/106], [94mLoss[0m : 2.18540
[1mStep[0m  [90/106], [94mLoss[0m : 2.55467
[1mStep[0m  [100/106], [94mLoss[0m : 2.50733

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.314, [92mTest[0m: 2.369, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.24199
[1mStep[0m  [10/106], [94mLoss[0m : 1.93046
[1mStep[0m  [20/106], [94mLoss[0m : 2.21999
[1mStep[0m  [30/106], [94mLoss[0m : 1.87794
[1mStep[0m  [40/106], [94mLoss[0m : 2.32542
[1mStep[0m  [50/106], [94mLoss[0m : 2.35981
[1mStep[0m  [60/106], [94mLoss[0m : 2.09780
[1mStep[0m  [70/106], [94mLoss[0m : 2.16542
[1mStep[0m  [80/106], [94mLoss[0m : 1.96060
[1mStep[0m  [90/106], [94mLoss[0m : 2.48493
[1mStep[0m  [100/106], [94mLoss[0m : 2.03087

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.259, [92mTest[0m: 2.380, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.21587
[1mStep[0m  [10/106], [94mLoss[0m : 2.18143
[1mStep[0m  [20/106], [94mLoss[0m : 2.30994
[1mStep[0m  [30/106], [94mLoss[0m : 2.28216
[1mStep[0m  [40/106], [94mLoss[0m : 1.91253
[1mStep[0m  [50/106], [94mLoss[0m : 1.87483
[1mStep[0m  [60/106], [94mLoss[0m : 2.42400
[1mStep[0m  [70/106], [94mLoss[0m : 2.21985
[1mStep[0m  [80/106], [94mLoss[0m : 2.17894
[1mStep[0m  [90/106], [94mLoss[0m : 2.39494
[1mStep[0m  [100/106], [94mLoss[0m : 2.05131

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.217, [92mTest[0m: 2.410, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.96234
[1mStep[0m  [10/106], [94mLoss[0m : 2.35755
[1mStep[0m  [20/106], [94mLoss[0m : 2.06719
[1mStep[0m  [30/106], [94mLoss[0m : 2.08672
[1mStep[0m  [40/106], [94mLoss[0m : 2.24307
[1mStep[0m  [50/106], [94mLoss[0m : 2.08952
[1mStep[0m  [60/106], [94mLoss[0m : 2.14121
[1mStep[0m  [70/106], [94mLoss[0m : 2.20143
[1mStep[0m  [80/106], [94mLoss[0m : 2.09601
[1mStep[0m  [90/106], [94mLoss[0m : 2.36639
[1mStep[0m  [100/106], [94mLoss[0m : 2.19810

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.172, [92mTest[0m: 2.472, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.87910
[1mStep[0m  [10/106], [94mLoss[0m : 2.08656
[1mStep[0m  [20/106], [94mLoss[0m : 2.15251
[1mStep[0m  [30/106], [94mLoss[0m : 2.04702
[1mStep[0m  [40/106], [94mLoss[0m : 2.15379
[1mStep[0m  [50/106], [94mLoss[0m : 2.07294
[1mStep[0m  [60/106], [94mLoss[0m : 2.19333
[1mStep[0m  [70/106], [94mLoss[0m : 2.26529
[1mStep[0m  [80/106], [94mLoss[0m : 2.43449
[1mStep[0m  [90/106], [94mLoss[0m : 2.28409
[1mStep[0m  [100/106], [94mLoss[0m : 2.22639

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.146, [92mTest[0m: 2.417, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.07715
[1mStep[0m  [10/106], [94mLoss[0m : 1.97655
[1mStep[0m  [20/106], [94mLoss[0m : 2.15728
[1mStep[0m  [30/106], [94mLoss[0m : 2.05613
[1mStep[0m  [40/106], [94mLoss[0m : 2.28612
[1mStep[0m  [50/106], [94mLoss[0m : 2.07492
[1mStep[0m  [60/106], [94mLoss[0m : 2.02090
[1mStep[0m  [70/106], [94mLoss[0m : 2.22762
[1mStep[0m  [80/106], [94mLoss[0m : 2.08069
[1mStep[0m  [90/106], [94mLoss[0m : 2.11705
[1mStep[0m  [100/106], [94mLoss[0m : 2.03658

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.093, [92mTest[0m: 2.424, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.98381
[1mStep[0m  [10/106], [94mLoss[0m : 2.24599
[1mStep[0m  [20/106], [94mLoss[0m : 2.13411
[1mStep[0m  [30/106], [94mLoss[0m : 1.99522
[1mStep[0m  [40/106], [94mLoss[0m : 2.09025
[1mStep[0m  [50/106], [94mLoss[0m : 2.20179
[1mStep[0m  [60/106], [94mLoss[0m : 2.31272
[1mStep[0m  [70/106], [94mLoss[0m : 2.15276
[1mStep[0m  [80/106], [94mLoss[0m : 2.08576
[1mStep[0m  [90/106], [94mLoss[0m : 2.44545
[1mStep[0m  [100/106], [94mLoss[0m : 1.74266

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.065, [92mTest[0m: 2.419, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.79681
[1mStep[0m  [10/106], [94mLoss[0m : 2.09451
[1mStep[0m  [20/106], [94mLoss[0m : 2.02582
[1mStep[0m  [30/106], [94mLoss[0m : 1.97298
[1mStep[0m  [40/106], [94mLoss[0m : 2.31636
[1mStep[0m  [50/106], [94mLoss[0m : 2.12072
[1mStep[0m  [60/106], [94mLoss[0m : 2.13927
[1mStep[0m  [70/106], [94mLoss[0m : 2.08520
[1mStep[0m  [80/106], [94mLoss[0m : 1.87907
[1mStep[0m  [90/106], [94mLoss[0m : 2.38839
[1mStep[0m  [100/106], [94mLoss[0m : 2.05308

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.065, [92mTest[0m: 2.423, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.05983
[1mStep[0m  [10/106], [94mLoss[0m : 2.15294
[1mStep[0m  [20/106], [94mLoss[0m : 2.31970
[1mStep[0m  [30/106], [94mLoss[0m : 2.00733
[1mStep[0m  [40/106], [94mLoss[0m : 1.88762
[1mStep[0m  [50/106], [94mLoss[0m : 2.09134
[1mStep[0m  [60/106], [94mLoss[0m : 2.30595
[1mStep[0m  [70/106], [94mLoss[0m : 1.96418
[1mStep[0m  [80/106], [94mLoss[0m : 2.13560
[1mStep[0m  [90/106], [94mLoss[0m : 2.04795
[1mStep[0m  [100/106], [94mLoss[0m : 1.99388

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.031, [92mTest[0m: 2.426, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.67618
[1mStep[0m  [10/106], [94mLoss[0m : 1.91234
[1mStep[0m  [20/106], [94mLoss[0m : 1.99226
[1mStep[0m  [30/106], [94mLoss[0m : 1.85125
[1mStep[0m  [40/106], [94mLoss[0m : 2.11416
[1mStep[0m  [50/106], [94mLoss[0m : 1.92231
[1mStep[0m  [60/106], [94mLoss[0m : 1.92802
[1mStep[0m  [70/106], [94mLoss[0m : 2.20978
[1mStep[0m  [80/106], [94mLoss[0m : 1.78052
[1mStep[0m  [90/106], [94mLoss[0m : 1.85313
[1mStep[0m  [100/106], [94mLoss[0m : 1.89266

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.991, [92mTest[0m: 2.431, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.24145
[1mStep[0m  [10/106], [94mLoss[0m : 2.22425
[1mStep[0m  [20/106], [94mLoss[0m : 1.97511
[1mStep[0m  [30/106], [94mLoss[0m : 1.97514
[1mStep[0m  [40/106], [94mLoss[0m : 1.94466
[1mStep[0m  [50/106], [94mLoss[0m : 1.85818
[1mStep[0m  [60/106], [94mLoss[0m : 1.86471
[1mStep[0m  [70/106], [94mLoss[0m : 2.02327
[1mStep[0m  [80/106], [94mLoss[0m : 2.14087
[1mStep[0m  [90/106], [94mLoss[0m : 1.88026
[1mStep[0m  [100/106], [94mLoss[0m : 2.12144

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.984, [92mTest[0m: 2.450, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.15180
[1mStep[0m  [10/106], [94mLoss[0m : 1.70975
[1mStep[0m  [20/106], [94mLoss[0m : 1.98054
[1mStep[0m  [30/106], [94mLoss[0m : 1.93019
[1mStep[0m  [40/106], [94mLoss[0m : 2.04946
[1mStep[0m  [50/106], [94mLoss[0m : 1.98057
[1mStep[0m  [60/106], [94mLoss[0m : 2.08869
[1mStep[0m  [70/106], [94mLoss[0m : 1.89730
[1mStep[0m  [80/106], [94mLoss[0m : 1.97359
[1mStep[0m  [90/106], [94mLoss[0m : 1.84137
[1mStep[0m  [100/106], [94mLoss[0m : 2.02220

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.963, [92mTest[0m: 2.438, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.05535
[1mStep[0m  [10/106], [94mLoss[0m : 1.89334
[1mStep[0m  [20/106], [94mLoss[0m : 1.89834
[1mStep[0m  [30/106], [94mLoss[0m : 1.90866
[1mStep[0m  [40/106], [94mLoss[0m : 1.82927
[1mStep[0m  [50/106], [94mLoss[0m : 2.16682
[1mStep[0m  [60/106], [94mLoss[0m : 2.13562
[1mStep[0m  [70/106], [94mLoss[0m : 1.66218
[1mStep[0m  [80/106], [94mLoss[0m : 1.90176
[1mStep[0m  [90/106], [94mLoss[0m : 1.91870
[1mStep[0m  [100/106], [94mLoss[0m : 1.94580

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.952, [92mTest[0m: 2.480, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.82502
[1mStep[0m  [10/106], [94mLoss[0m : 1.92566
[1mStep[0m  [20/106], [94mLoss[0m : 1.80785
[1mStep[0m  [30/106], [94mLoss[0m : 2.28893
[1mStep[0m  [40/106], [94mLoss[0m : 1.87884
[1mStep[0m  [50/106], [94mLoss[0m : 2.01571
[1mStep[0m  [60/106], [94mLoss[0m : 2.12890
[1mStep[0m  [70/106], [94mLoss[0m : 1.99371
[1mStep[0m  [80/106], [94mLoss[0m : 1.71746
[1mStep[0m  [90/106], [94mLoss[0m : 2.00830
[1mStep[0m  [100/106], [94mLoss[0m : 1.95961

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.937, [92mTest[0m: 2.459, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.98573
[1mStep[0m  [10/106], [94mLoss[0m : 1.87680
[1mStep[0m  [20/106], [94mLoss[0m : 2.11828
[1mStep[0m  [30/106], [94mLoss[0m : 1.75852
[1mStep[0m  [40/106], [94mLoss[0m : 2.12771
[1mStep[0m  [50/106], [94mLoss[0m : 2.15319
[1mStep[0m  [60/106], [94mLoss[0m : 2.01810
[1mStep[0m  [70/106], [94mLoss[0m : 1.81047
[1mStep[0m  [80/106], [94mLoss[0m : 1.88999
[1mStep[0m  [90/106], [94mLoss[0m : 1.85877
[1mStep[0m  [100/106], [94mLoss[0m : 1.91029

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.926, [92mTest[0m: 2.471, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.82396
[1mStep[0m  [10/106], [94mLoss[0m : 1.78665
[1mStep[0m  [20/106], [94mLoss[0m : 1.99330
[1mStep[0m  [30/106], [94mLoss[0m : 1.85458
[1mStep[0m  [40/106], [94mLoss[0m : 2.08953
[1mStep[0m  [50/106], [94mLoss[0m : 1.83232
[1mStep[0m  [60/106], [94mLoss[0m : 2.23302
[1mStep[0m  [70/106], [94mLoss[0m : 2.01800
[1mStep[0m  [80/106], [94mLoss[0m : 1.92847
[1mStep[0m  [90/106], [94mLoss[0m : 1.95440
[1mStep[0m  [100/106], [94mLoss[0m : 2.02872

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.916, [92mTest[0m: 2.498, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.97173
[1mStep[0m  [10/106], [94mLoss[0m : 1.79243
[1mStep[0m  [20/106], [94mLoss[0m : 1.71885
[1mStep[0m  [30/106], [94mLoss[0m : 1.86699
[1mStep[0m  [40/106], [94mLoss[0m : 2.04882
[1mStep[0m  [50/106], [94mLoss[0m : 1.97182
[1mStep[0m  [60/106], [94mLoss[0m : 1.81720
[1mStep[0m  [70/106], [94mLoss[0m : 1.96915
[1mStep[0m  [80/106], [94mLoss[0m : 1.83319
[1mStep[0m  [90/106], [94mLoss[0m : 1.82112
[1mStep[0m  [100/106], [94mLoss[0m : 2.00398

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.872, [92mTest[0m: 2.448, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.87250
[1mStep[0m  [10/106], [94mLoss[0m : 2.14163
[1mStep[0m  [20/106], [94mLoss[0m : 1.90402
[1mStep[0m  [30/106], [94mLoss[0m : 1.95031
[1mStep[0m  [40/106], [94mLoss[0m : 2.17469
[1mStep[0m  [50/106], [94mLoss[0m : 1.86539
[1mStep[0m  [60/106], [94mLoss[0m : 1.95476
[1mStep[0m  [70/106], [94mLoss[0m : 1.82455
[1mStep[0m  [80/106], [94mLoss[0m : 1.93114
[1mStep[0m  [90/106], [94mLoss[0m : 2.07203
[1mStep[0m  [100/106], [94mLoss[0m : 2.06702

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.861, [92mTest[0m: 2.509, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.63759
[1mStep[0m  [10/106], [94mLoss[0m : 1.90473
[1mStep[0m  [20/106], [94mLoss[0m : 1.97406
[1mStep[0m  [30/106], [94mLoss[0m : 1.67690
[1mStep[0m  [40/106], [94mLoss[0m : 1.97222
[1mStep[0m  [50/106], [94mLoss[0m : 1.87354
[1mStep[0m  [60/106], [94mLoss[0m : 1.97395
[1mStep[0m  [70/106], [94mLoss[0m : 2.09491
[1mStep[0m  [80/106], [94mLoss[0m : 1.75441
[1mStep[0m  [90/106], [94mLoss[0m : 1.82441
[1mStep[0m  [100/106], [94mLoss[0m : 2.07973

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.860, [92mTest[0m: 2.474, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.04679
[1mStep[0m  [10/106], [94mLoss[0m : 1.70525
[1mStep[0m  [20/106], [94mLoss[0m : 1.77673
[1mStep[0m  [30/106], [94mLoss[0m : 1.87206
[1mStep[0m  [40/106], [94mLoss[0m : 2.15177
[1mStep[0m  [50/106], [94mLoss[0m : 1.85166
[1mStep[0m  [60/106], [94mLoss[0m : 1.94987
[1mStep[0m  [70/106], [94mLoss[0m : 2.22346
[1mStep[0m  [80/106], [94mLoss[0m : 2.02225
[1mStep[0m  [90/106], [94mLoss[0m : 1.80655
[1mStep[0m  [100/106], [94mLoss[0m : 1.71896

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.838, [92mTest[0m: 2.487, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.76229
[1mStep[0m  [10/106], [94mLoss[0m : 1.83635
[1mStep[0m  [20/106], [94mLoss[0m : 1.84184
[1mStep[0m  [30/106], [94mLoss[0m : 1.82809
[1mStep[0m  [40/106], [94mLoss[0m : 1.58678
[1mStep[0m  [50/106], [94mLoss[0m : 1.94298
[1mStep[0m  [60/106], [94mLoss[0m : 1.72048
[1mStep[0m  [70/106], [94mLoss[0m : 1.95889
[1mStep[0m  [80/106], [94mLoss[0m : 1.90431
[1mStep[0m  [90/106], [94mLoss[0m : 2.12911
[1mStep[0m  [100/106], [94mLoss[0m : 1.71816

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.830, [92mTest[0m: 2.470, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.79603
[1mStep[0m  [10/106], [94mLoss[0m : 1.90776
[1mStep[0m  [20/106], [94mLoss[0m : 1.78615
[1mStep[0m  [30/106], [94mLoss[0m : 1.58340
[1mStep[0m  [40/106], [94mLoss[0m : 1.90646
[1mStep[0m  [50/106], [94mLoss[0m : 1.79390
[1mStep[0m  [60/106], [94mLoss[0m : 1.55052
[1mStep[0m  [70/106], [94mLoss[0m : 1.75938
[1mStep[0m  [80/106], [94mLoss[0m : 1.66157
[1mStep[0m  [90/106], [94mLoss[0m : 1.85001
[1mStep[0m  [100/106], [94mLoss[0m : 1.90070

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.816, [92mTest[0m: 2.443, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.97706
[1mStep[0m  [10/106], [94mLoss[0m : 1.82354
[1mStep[0m  [20/106], [94mLoss[0m : 1.71231
[1mStep[0m  [30/106], [94mLoss[0m : 1.88247
[1mStep[0m  [40/106], [94mLoss[0m : 1.93723
[1mStep[0m  [50/106], [94mLoss[0m : 1.62527
[1mStep[0m  [60/106], [94mLoss[0m : 2.03774
[1mStep[0m  [70/106], [94mLoss[0m : 1.93938
[1mStep[0m  [80/106], [94mLoss[0m : 1.76783
[1mStep[0m  [90/106], [94mLoss[0m : 1.70839
[1mStep[0m  [100/106], [94mLoss[0m : 1.92757

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.813, [92mTest[0m: 2.441, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.62058
[1mStep[0m  [10/106], [94mLoss[0m : 1.76117
[1mStep[0m  [20/106], [94mLoss[0m : 1.99075
[1mStep[0m  [30/106], [94mLoss[0m : 1.73074
[1mStep[0m  [40/106], [94mLoss[0m : 1.69472
[1mStep[0m  [50/106], [94mLoss[0m : 1.77028
[1mStep[0m  [60/106], [94mLoss[0m : 1.71337
[1mStep[0m  [70/106], [94mLoss[0m : 1.70650
[1mStep[0m  [80/106], [94mLoss[0m : 1.67908
[1mStep[0m  [90/106], [94mLoss[0m : 1.94192
[1mStep[0m  [100/106], [94mLoss[0m : 1.67862

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.800, [92mTest[0m: 2.488, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.86027
[1mStep[0m  [10/106], [94mLoss[0m : 1.77604
[1mStep[0m  [20/106], [94mLoss[0m : 2.02757
[1mStep[0m  [30/106], [94mLoss[0m : 1.72880
[1mStep[0m  [40/106], [94mLoss[0m : 1.61155
[1mStep[0m  [50/106], [94mLoss[0m : 1.69454
[1mStep[0m  [60/106], [94mLoss[0m : 1.72598
[1mStep[0m  [70/106], [94mLoss[0m : 2.06794
[1mStep[0m  [80/106], [94mLoss[0m : 1.50801
[1mStep[0m  [90/106], [94mLoss[0m : 1.63915
[1mStep[0m  [100/106], [94mLoss[0m : 1.85224

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.801, [92mTest[0m: 2.497, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.80810
[1mStep[0m  [10/106], [94mLoss[0m : 1.65894
[1mStep[0m  [20/106], [94mLoss[0m : 1.86565
[1mStep[0m  [30/106], [94mLoss[0m : 1.90805
[1mStep[0m  [40/106], [94mLoss[0m : 1.79506
[1mStep[0m  [50/106], [94mLoss[0m : 1.68911
[1mStep[0m  [60/106], [94mLoss[0m : 1.90045
[1mStep[0m  [70/106], [94mLoss[0m : 1.47433
[1mStep[0m  [80/106], [94mLoss[0m : 1.79095
[1mStep[0m  [90/106], [94mLoss[0m : 1.90280
[1mStep[0m  [100/106], [94mLoss[0m : 1.88043

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.817, [92mTest[0m: 2.456, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.461
====================================

Phase 2 - Evaluation MAE:  2.4609847968479373
MAE score P1       2.401859
MAE score P2       2.460985
loss               1.800091
learning_rate      0.002575
batch_size              128
hidden_sizes      [250, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.9
weight_decay           0.01
Name: 7, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/53], [94mLoss[0m : 10.91412
[1mStep[0m  [5/53], [94mLoss[0m : 10.10697
[1mStep[0m  [10/53], [94mLoss[0m : 8.84455
[1mStep[0m  [15/53], [94mLoss[0m : 8.03725
[1mStep[0m  [20/53], [94mLoss[0m : 6.55944
[1mStep[0m  [25/53], [94mLoss[0m : 5.95348
[1mStep[0m  [30/53], [94mLoss[0m : 5.04965
[1mStep[0m  [35/53], [94mLoss[0m : 4.52074
[1mStep[0m  [40/53], [94mLoss[0m : 3.36284
[1mStep[0m  [45/53], [94mLoss[0m : 3.37685
[1mStep[0m  [50/53], [94mLoss[0m : 3.26077

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.298, [92mTest[0m: 11.019, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.94078
[1mStep[0m  [5/53], [94mLoss[0m : 2.82951
[1mStep[0m  [10/53], [94mLoss[0m : 2.89541
[1mStep[0m  [15/53], [94mLoss[0m : 2.79815
[1mStep[0m  [20/53], [94mLoss[0m : 2.57443
[1mStep[0m  [25/53], [94mLoss[0m : 2.87976
[1mStep[0m  [30/53], [94mLoss[0m : 2.59743
[1mStep[0m  [35/53], [94mLoss[0m : 2.64721
[1mStep[0m  [40/53], [94mLoss[0m : 2.76348
[1mStep[0m  [45/53], [94mLoss[0m : 2.50543
[1mStep[0m  [50/53], [94mLoss[0m : 2.59864

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.689, [92mTest[0m: 3.666, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.55000
[1mStep[0m  [5/53], [94mLoss[0m : 2.47702
[1mStep[0m  [10/53], [94mLoss[0m : 2.60767
[1mStep[0m  [15/53], [94mLoss[0m : 2.45649
[1mStep[0m  [20/53], [94mLoss[0m : 2.58541
[1mStep[0m  [25/53], [94mLoss[0m : 2.38800
[1mStep[0m  [30/53], [94mLoss[0m : 2.49575
[1mStep[0m  [35/53], [94mLoss[0m : 2.56243
[1mStep[0m  [40/53], [94mLoss[0m : 2.42550
[1mStep[0m  [45/53], [94mLoss[0m : 2.94709
[1mStep[0m  [50/53], [94mLoss[0m : 2.74659

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.578, [92mTest[0m: 2.776, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.46077
[1mStep[0m  [5/53], [94mLoss[0m : 2.52222
[1mStep[0m  [10/53], [94mLoss[0m : 2.57941
[1mStep[0m  [15/53], [94mLoss[0m : 2.48497
[1mStep[0m  [20/53], [94mLoss[0m : 2.72954
[1mStep[0m  [25/53], [94mLoss[0m : 2.48132
[1mStep[0m  [30/53], [94mLoss[0m : 2.86281
[1mStep[0m  [35/53], [94mLoss[0m : 2.59977
[1mStep[0m  [40/53], [94mLoss[0m : 2.55893
[1mStep[0m  [45/53], [94mLoss[0m : 2.52453
[1mStep[0m  [50/53], [94mLoss[0m : 2.28214

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.519, [92mTest[0m: 2.734, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.71009
[1mStep[0m  [5/53], [94mLoss[0m : 2.46527
[1mStep[0m  [10/53], [94mLoss[0m : 2.31561
[1mStep[0m  [15/53], [94mLoss[0m : 2.48773
[1mStep[0m  [20/53], [94mLoss[0m : 2.64577
[1mStep[0m  [25/53], [94mLoss[0m : 2.69195
[1mStep[0m  [30/53], [94mLoss[0m : 2.50380
[1mStep[0m  [35/53], [94mLoss[0m : 2.54417
[1mStep[0m  [40/53], [94mLoss[0m : 2.35570
[1mStep[0m  [45/53], [94mLoss[0m : 2.56022
[1mStep[0m  [50/53], [94mLoss[0m : 2.47606

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.510, [92mTest[0m: 2.678, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.71730
[1mStep[0m  [5/53], [94mLoss[0m : 2.69652
[1mStep[0m  [10/53], [94mLoss[0m : 2.41652
[1mStep[0m  [15/53], [94mLoss[0m : 2.75609
[1mStep[0m  [20/53], [94mLoss[0m : 2.41038
[1mStep[0m  [25/53], [94mLoss[0m : 2.59475
[1mStep[0m  [30/53], [94mLoss[0m : 2.47844
[1mStep[0m  [35/53], [94mLoss[0m : 2.26138
[1mStep[0m  [40/53], [94mLoss[0m : 2.58091
[1mStep[0m  [45/53], [94mLoss[0m : 2.53707
[1mStep[0m  [50/53], [94mLoss[0m : 2.45300

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.641, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.42016
[1mStep[0m  [5/53], [94mLoss[0m : 2.51120
[1mStep[0m  [10/53], [94mLoss[0m : 2.48818
[1mStep[0m  [15/53], [94mLoss[0m : 2.49236
[1mStep[0m  [20/53], [94mLoss[0m : 2.41958
[1mStep[0m  [25/53], [94mLoss[0m : 2.29584
[1mStep[0m  [30/53], [94mLoss[0m : 2.44220
[1mStep[0m  [35/53], [94mLoss[0m : 2.32754
[1mStep[0m  [40/53], [94mLoss[0m : 2.36019
[1mStep[0m  [45/53], [94mLoss[0m : 2.51483
[1mStep[0m  [50/53], [94mLoss[0m : 2.45395

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.648, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.43423
[1mStep[0m  [5/53], [94mLoss[0m : 2.30292
[1mStep[0m  [10/53], [94mLoss[0m : 2.66435
[1mStep[0m  [15/53], [94mLoss[0m : 2.53808
[1mStep[0m  [20/53], [94mLoss[0m : 2.55584
[1mStep[0m  [25/53], [94mLoss[0m : 2.43896
[1mStep[0m  [30/53], [94mLoss[0m : 2.52002
[1mStep[0m  [35/53], [94mLoss[0m : 2.42291
[1mStep[0m  [40/53], [94mLoss[0m : 2.42218
[1mStep[0m  [45/53], [94mLoss[0m : 2.66433
[1mStep[0m  [50/53], [94mLoss[0m : 2.39101

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.642, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.40716
[1mStep[0m  [5/53], [94mLoss[0m : 2.36242
[1mStep[0m  [10/53], [94mLoss[0m : 2.62355
[1mStep[0m  [15/53], [94mLoss[0m : 2.47334
[1mStep[0m  [20/53], [94mLoss[0m : 2.26990
[1mStep[0m  [25/53], [94mLoss[0m : 2.52139
[1mStep[0m  [30/53], [94mLoss[0m : 2.41799
[1mStep[0m  [35/53], [94mLoss[0m : 2.62359
[1mStep[0m  [40/53], [94mLoss[0m : 2.56720
[1mStep[0m  [45/53], [94mLoss[0m : 2.47277
[1mStep[0m  [50/53], [94mLoss[0m : 2.31839

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.641, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.33886
[1mStep[0m  [5/53], [94mLoss[0m : 2.54647
[1mStep[0m  [10/53], [94mLoss[0m : 2.60137
[1mStep[0m  [15/53], [94mLoss[0m : 2.43714
[1mStep[0m  [20/53], [94mLoss[0m : 2.46881
[1mStep[0m  [25/53], [94mLoss[0m : 2.23984
[1mStep[0m  [30/53], [94mLoss[0m : 2.38825
[1mStep[0m  [35/53], [94mLoss[0m : 2.44614
[1mStep[0m  [40/53], [94mLoss[0m : 2.43527
[1mStep[0m  [45/53], [94mLoss[0m : 2.61383
[1mStep[0m  [50/53], [94mLoss[0m : 2.31995

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.642, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.32000
[1mStep[0m  [5/53], [94mLoss[0m : 2.42561
[1mStep[0m  [10/53], [94mLoss[0m : 2.45511
[1mStep[0m  [15/53], [94mLoss[0m : 2.54729
[1mStep[0m  [20/53], [94mLoss[0m : 2.23370
[1mStep[0m  [25/53], [94mLoss[0m : 2.29953
[1mStep[0m  [30/53], [94mLoss[0m : 2.33139
[1mStep[0m  [35/53], [94mLoss[0m : 2.28098
[1mStep[0m  [40/53], [94mLoss[0m : 2.38640
[1mStep[0m  [45/53], [94mLoss[0m : 2.42773
[1mStep[0m  [50/53], [94mLoss[0m : 2.53848

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.635, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.60617
[1mStep[0m  [5/53], [94mLoss[0m : 2.47624
[1mStep[0m  [10/53], [94mLoss[0m : 2.39260
[1mStep[0m  [15/53], [94mLoss[0m : 2.43404
[1mStep[0m  [20/53], [94mLoss[0m : 2.33869
[1mStep[0m  [25/53], [94mLoss[0m : 2.55273
[1mStep[0m  [30/53], [94mLoss[0m : 2.64576
[1mStep[0m  [35/53], [94mLoss[0m : 2.43434
[1mStep[0m  [40/53], [94mLoss[0m : 2.42706
[1mStep[0m  [45/53], [94mLoss[0m : 2.29928
[1mStep[0m  [50/53], [94mLoss[0m : 2.54731

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.619, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.37945
[1mStep[0m  [5/53], [94mLoss[0m : 2.47576
[1mStep[0m  [10/53], [94mLoss[0m : 2.37196
[1mStep[0m  [15/53], [94mLoss[0m : 2.27544
[1mStep[0m  [20/53], [94mLoss[0m : 2.38936
[1mStep[0m  [25/53], [94mLoss[0m : 2.74411
[1mStep[0m  [30/53], [94mLoss[0m : 2.26574
[1mStep[0m  [35/53], [94mLoss[0m : 2.35036
[1mStep[0m  [40/53], [94mLoss[0m : 2.63530
[1mStep[0m  [45/53], [94mLoss[0m : 2.39841
[1mStep[0m  [50/53], [94mLoss[0m : 2.17365

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.609, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.37480
[1mStep[0m  [5/53], [94mLoss[0m : 2.62509
[1mStep[0m  [10/53], [94mLoss[0m : 2.56488
[1mStep[0m  [15/53], [94mLoss[0m : 2.72381
[1mStep[0m  [20/53], [94mLoss[0m : 2.24114
[1mStep[0m  [25/53], [94mLoss[0m : 2.45742
[1mStep[0m  [30/53], [94mLoss[0m : 2.06124
[1mStep[0m  [35/53], [94mLoss[0m : 2.28323
[1mStep[0m  [40/53], [94mLoss[0m : 2.31622
[1mStep[0m  [45/53], [94mLoss[0m : 2.30120
[1mStep[0m  [50/53], [94mLoss[0m : 2.33922

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.632, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.20834
[1mStep[0m  [5/53], [94mLoss[0m : 2.55558
[1mStep[0m  [10/53], [94mLoss[0m : 2.26268
[1mStep[0m  [15/53], [94mLoss[0m : 2.50680
[1mStep[0m  [20/53], [94mLoss[0m : 2.32493
[1mStep[0m  [25/53], [94mLoss[0m : 2.35193
[1mStep[0m  [30/53], [94mLoss[0m : 2.20099
[1mStep[0m  [35/53], [94mLoss[0m : 2.38806
[1mStep[0m  [40/53], [94mLoss[0m : 2.48152
[1mStep[0m  [45/53], [94mLoss[0m : 2.68228
[1mStep[0m  [50/53], [94mLoss[0m : 2.26371

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.603, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.23713
[1mStep[0m  [5/53], [94mLoss[0m : 2.50458
[1mStep[0m  [10/53], [94mLoss[0m : 2.35868
[1mStep[0m  [15/53], [94mLoss[0m : 2.26771
[1mStep[0m  [20/53], [94mLoss[0m : 2.64309
[1mStep[0m  [25/53], [94mLoss[0m : 2.51958
[1mStep[0m  [30/53], [94mLoss[0m : 2.56646
[1mStep[0m  [35/53], [94mLoss[0m : 2.45377
[1mStep[0m  [40/53], [94mLoss[0m : 2.38125
[1mStep[0m  [45/53], [94mLoss[0m : 2.14972
[1mStep[0m  [50/53], [94mLoss[0m : 2.51070

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.594, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.50847
[1mStep[0m  [5/53], [94mLoss[0m : 2.29938
[1mStep[0m  [10/53], [94mLoss[0m : 2.67679
[1mStep[0m  [15/53], [94mLoss[0m : 2.40210
[1mStep[0m  [20/53], [94mLoss[0m : 2.27106
[1mStep[0m  [25/53], [94mLoss[0m : 2.54120
[1mStep[0m  [30/53], [94mLoss[0m : 2.43058
[1mStep[0m  [35/53], [94mLoss[0m : 2.23782
[1mStep[0m  [40/53], [94mLoss[0m : 2.22014
[1mStep[0m  [45/53], [94mLoss[0m : 2.22838
[1mStep[0m  [50/53], [94mLoss[0m : 2.48017

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.599, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.68423
[1mStep[0m  [5/53], [94mLoss[0m : 2.81875
[1mStep[0m  [10/53], [94mLoss[0m : 2.33347
[1mStep[0m  [15/53], [94mLoss[0m : 2.48229
[1mStep[0m  [20/53], [94mLoss[0m : 2.37867
[1mStep[0m  [25/53], [94mLoss[0m : 2.32475
[1mStep[0m  [30/53], [94mLoss[0m : 2.26824
[1mStep[0m  [35/53], [94mLoss[0m : 2.44387
[1mStep[0m  [40/53], [94mLoss[0m : 2.26333
[1mStep[0m  [45/53], [94mLoss[0m : 2.25875
[1mStep[0m  [50/53], [94mLoss[0m : 2.44011

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.608, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.27773
[1mStep[0m  [5/53], [94mLoss[0m : 2.28353
[1mStep[0m  [10/53], [94mLoss[0m : 2.34922
[1mStep[0m  [15/53], [94mLoss[0m : 2.42197
[1mStep[0m  [20/53], [94mLoss[0m : 2.40728
[1mStep[0m  [25/53], [94mLoss[0m : 2.42639
[1mStep[0m  [30/53], [94mLoss[0m : 2.42468
[1mStep[0m  [35/53], [94mLoss[0m : 2.46593
[1mStep[0m  [40/53], [94mLoss[0m : 2.55399
[1mStep[0m  [45/53], [94mLoss[0m : 2.38642
[1mStep[0m  [50/53], [94mLoss[0m : 2.44893

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.599, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.32565
[1mStep[0m  [5/53], [94mLoss[0m : 2.59301
[1mStep[0m  [10/53], [94mLoss[0m : 2.45395
[1mStep[0m  [15/53], [94mLoss[0m : 2.50521
[1mStep[0m  [20/53], [94mLoss[0m : 2.61776
[1mStep[0m  [25/53], [94mLoss[0m : 2.36923
[1mStep[0m  [30/53], [94mLoss[0m : 2.41788
[1mStep[0m  [35/53], [94mLoss[0m : 2.57392
[1mStep[0m  [40/53], [94mLoss[0m : 2.40912
[1mStep[0m  [45/53], [94mLoss[0m : 2.27171
[1mStep[0m  [50/53], [94mLoss[0m : 2.50822

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.582, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.50455
[1mStep[0m  [5/53], [94mLoss[0m : 2.37780
[1mStep[0m  [10/53], [94mLoss[0m : 2.21073
[1mStep[0m  [15/53], [94mLoss[0m : 2.55452
[1mStep[0m  [20/53], [94mLoss[0m : 2.18816
[1mStep[0m  [25/53], [94mLoss[0m : 2.38159
[1mStep[0m  [30/53], [94mLoss[0m : 2.35226
[1mStep[0m  [35/53], [94mLoss[0m : 2.20020
[1mStep[0m  [40/53], [94mLoss[0m : 2.25302
[1mStep[0m  [45/53], [94mLoss[0m : 2.34423
[1mStep[0m  [50/53], [94mLoss[0m : 2.42032

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.590, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.37141
[1mStep[0m  [5/53], [94mLoss[0m : 2.37071
[1mStep[0m  [10/53], [94mLoss[0m : 2.59216
[1mStep[0m  [15/53], [94mLoss[0m : 2.25420
[1mStep[0m  [20/53], [94mLoss[0m : 2.36961
[1mStep[0m  [25/53], [94mLoss[0m : 2.34659
[1mStep[0m  [30/53], [94mLoss[0m : 2.35315
[1mStep[0m  [35/53], [94mLoss[0m : 2.60971
[1mStep[0m  [40/53], [94mLoss[0m : 2.33952
[1mStep[0m  [45/53], [94mLoss[0m : 2.43206
[1mStep[0m  [50/53], [94mLoss[0m : 2.32922

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.385, [92mTest[0m: 2.579, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.34698
[1mStep[0m  [5/53], [94mLoss[0m : 2.40324
[1mStep[0m  [10/53], [94mLoss[0m : 2.39711
[1mStep[0m  [15/53], [94mLoss[0m : 2.47839
[1mStep[0m  [20/53], [94mLoss[0m : 2.26845
[1mStep[0m  [25/53], [94mLoss[0m : 2.20795
[1mStep[0m  [30/53], [94mLoss[0m : 2.32491
[1mStep[0m  [35/53], [94mLoss[0m : 2.42999
[1mStep[0m  [40/53], [94mLoss[0m : 2.42822
[1mStep[0m  [45/53], [94mLoss[0m : 2.50254
[1mStep[0m  [50/53], [94mLoss[0m : 2.35776

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.593, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.58956
[1mStep[0m  [5/53], [94mLoss[0m : 2.63727
[1mStep[0m  [10/53], [94mLoss[0m : 2.37057
[1mStep[0m  [15/53], [94mLoss[0m : 2.57611
[1mStep[0m  [20/53], [94mLoss[0m : 2.46741
[1mStep[0m  [25/53], [94mLoss[0m : 2.45520
[1mStep[0m  [30/53], [94mLoss[0m : 2.44258
[1mStep[0m  [35/53], [94mLoss[0m : 2.06124
[1mStep[0m  [40/53], [94mLoss[0m : 2.24697
[1mStep[0m  [45/53], [94mLoss[0m : 2.54038
[1mStep[0m  [50/53], [94mLoss[0m : 2.34243

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.403, [92mTest[0m: 2.589, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.58569
[1mStep[0m  [5/53], [94mLoss[0m : 2.42338
[1mStep[0m  [10/53], [94mLoss[0m : 2.49401
[1mStep[0m  [15/53], [94mLoss[0m : 2.16232
[1mStep[0m  [20/53], [94mLoss[0m : 2.42560
[1mStep[0m  [25/53], [94mLoss[0m : 2.29834
[1mStep[0m  [30/53], [94mLoss[0m : 2.32372
[1mStep[0m  [35/53], [94mLoss[0m : 2.53454
[1mStep[0m  [40/53], [94mLoss[0m : 2.32573
[1mStep[0m  [45/53], [94mLoss[0m : 2.25393
[1mStep[0m  [50/53], [94mLoss[0m : 2.54654

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.385, [92mTest[0m: 2.573, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.42498
[1mStep[0m  [5/53], [94mLoss[0m : 2.52216
[1mStep[0m  [10/53], [94mLoss[0m : 2.26297
[1mStep[0m  [15/53], [94mLoss[0m : 2.62194
[1mStep[0m  [20/53], [94mLoss[0m : 2.48486
[1mStep[0m  [25/53], [94mLoss[0m : 2.31504
[1mStep[0m  [30/53], [94mLoss[0m : 2.53798
[1mStep[0m  [35/53], [94mLoss[0m : 2.22115
[1mStep[0m  [40/53], [94mLoss[0m : 2.45456
[1mStep[0m  [45/53], [94mLoss[0m : 2.50640
[1mStep[0m  [50/53], [94mLoss[0m : 2.54469

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.591, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.30863
[1mStep[0m  [5/53], [94mLoss[0m : 2.26954
[1mStep[0m  [10/53], [94mLoss[0m : 2.26447
[1mStep[0m  [15/53], [94mLoss[0m : 2.45365
[1mStep[0m  [20/53], [94mLoss[0m : 2.45783
[1mStep[0m  [25/53], [94mLoss[0m : 2.24886
[1mStep[0m  [30/53], [94mLoss[0m : 2.20643
[1mStep[0m  [35/53], [94mLoss[0m : 2.25644
[1mStep[0m  [40/53], [94mLoss[0m : 2.48235
[1mStep[0m  [45/53], [94mLoss[0m : 2.48367
[1mStep[0m  [50/53], [94mLoss[0m : 2.21837

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.389, [92mTest[0m: 2.575, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.33717
[1mStep[0m  [5/53], [94mLoss[0m : 2.38288
[1mStep[0m  [10/53], [94mLoss[0m : 2.35698
[1mStep[0m  [15/53], [94mLoss[0m : 2.29242
[1mStep[0m  [20/53], [94mLoss[0m : 2.36406
[1mStep[0m  [25/53], [94mLoss[0m : 2.30680
[1mStep[0m  [30/53], [94mLoss[0m : 2.46069
[1mStep[0m  [35/53], [94mLoss[0m : 2.62081
[1mStep[0m  [40/53], [94mLoss[0m : 2.24857
[1mStep[0m  [45/53], [94mLoss[0m : 2.29199
[1mStep[0m  [50/53], [94mLoss[0m : 2.27622

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.388, [92mTest[0m: 2.576, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.21354
[1mStep[0m  [5/53], [94mLoss[0m : 2.28019
[1mStep[0m  [10/53], [94mLoss[0m : 2.50733
[1mStep[0m  [15/53], [94mLoss[0m : 2.39177
[1mStep[0m  [20/53], [94mLoss[0m : 2.39100
[1mStep[0m  [25/53], [94mLoss[0m : 2.44932
[1mStep[0m  [30/53], [94mLoss[0m : 2.30194
[1mStep[0m  [35/53], [94mLoss[0m : 2.23350
[1mStep[0m  [40/53], [94mLoss[0m : 2.42313
[1mStep[0m  [45/53], [94mLoss[0m : 2.37338
[1mStep[0m  [50/53], [94mLoss[0m : 2.38731

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.372, [92mTest[0m: 2.595, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.34152
[1mStep[0m  [5/53], [94mLoss[0m : 2.43411
[1mStep[0m  [10/53], [94mLoss[0m : 2.52126
[1mStep[0m  [15/53], [94mLoss[0m : 2.49132
[1mStep[0m  [20/53], [94mLoss[0m : 2.29482
[1mStep[0m  [25/53], [94mLoss[0m : 2.21715
[1mStep[0m  [30/53], [94mLoss[0m : 2.30919
[1mStep[0m  [35/53], [94mLoss[0m : 2.26131
[1mStep[0m  [40/53], [94mLoss[0m : 2.56690
[1mStep[0m  [45/53], [94mLoss[0m : 2.28404
[1mStep[0m  [50/53], [94mLoss[0m : 2.40762

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.363, [92mTest[0m: 2.585, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.539
====================================

Phase 1 - Evaluation MAE:  2.539413021161006
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/53], [94mLoss[0m : 2.59374
[1mStep[0m  [5/53], [94mLoss[0m : 2.43484
[1mStep[0m  [10/53], [94mLoss[0m : 2.49692
[1mStep[0m  [15/53], [94mLoss[0m : 2.48006
[1mStep[0m  [20/53], [94mLoss[0m : 2.57918
[1mStep[0m  [25/53], [94mLoss[0m : 2.51139
[1mStep[0m  [30/53], [94mLoss[0m : 2.58791
[1mStep[0m  [35/53], [94mLoss[0m : 2.76879
[1mStep[0m  [40/53], [94mLoss[0m : 2.34269
[1mStep[0m  [45/53], [94mLoss[0m : 2.46176
[1mStep[0m  [50/53], [94mLoss[0m : 2.61780

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.539, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.41521
[1mStep[0m  [5/53], [94mLoss[0m : 2.12454
[1mStep[0m  [10/53], [94mLoss[0m : 2.45948
[1mStep[0m  [15/53], [94mLoss[0m : 2.46707
[1mStep[0m  [20/53], [94mLoss[0m : 2.53851
[1mStep[0m  [25/53], [94mLoss[0m : 2.28086
[1mStep[0m  [30/53], [94mLoss[0m : 2.29161
[1mStep[0m  [35/53], [94mLoss[0m : 2.40340
[1mStep[0m  [40/53], [94mLoss[0m : 2.37549
[1mStep[0m  [45/53], [94mLoss[0m : 2.47586
[1mStep[0m  [50/53], [94mLoss[0m : 2.32571

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.403, [92mTest[0m: 2.606, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.43541
[1mStep[0m  [5/53], [94mLoss[0m : 2.19255
[1mStep[0m  [10/53], [94mLoss[0m : 2.27831
[1mStep[0m  [15/53], [94mLoss[0m : 2.56404
[1mStep[0m  [20/53], [94mLoss[0m : 2.30265
[1mStep[0m  [25/53], [94mLoss[0m : 2.40473
[1mStep[0m  [30/53], [94mLoss[0m : 2.25505
[1mStep[0m  [35/53], [94mLoss[0m : 2.21432
[1mStep[0m  [40/53], [94mLoss[0m : 2.12712
[1mStep[0m  [45/53], [94mLoss[0m : 2.60873
[1mStep[0m  [50/53], [94mLoss[0m : 2.41602

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.369, [92mTest[0m: 2.467, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.27104
[1mStep[0m  [5/53], [94mLoss[0m : 2.49687
[1mStep[0m  [10/53], [94mLoss[0m : 2.19671
[1mStep[0m  [15/53], [94mLoss[0m : 2.25162
[1mStep[0m  [20/53], [94mLoss[0m : 2.27324
[1mStep[0m  [25/53], [94mLoss[0m : 2.17526
[1mStep[0m  [30/53], [94mLoss[0m : 2.55672
[1mStep[0m  [35/53], [94mLoss[0m : 2.53487
[1mStep[0m  [40/53], [94mLoss[0m : 2.40712
[1mStep[0m  [45/53], [94mLoss[0m : 2.26995
[1mStep[0m  [50/53], [94mLoss[0m : 2.39601

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.341, [92mTest[0m: 2.505, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.47657
[1mStep[0m  [5/53], [94mLoss[0m : 2.21240
[1mStep[0m  [10/53], [94mLoss[0m : 2.31010
[1mStep[0m  [15/53], [94mLoss[0m : 2.44503
[1mStep[0m  [20/53], [94mLoss[0m : 2.21519
[1mStep[0m  [25/53], [94mLoss[0m : 2.15658
[1mStep[0m  [30/53], [94mLoss[0m : 2.38955
[1mStep[0m  [35/53], [94mLoss[0m : 2.46561
[1mStep[0m  [40/53], [94mLoss[0m : 2.21660
[1mStep[0m  [45/53], [94mLoss[0m : 2.16278
[1mStep[0m  [50/53], [94mLoss[0m : 2.41347

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.312, [92mTest[0m: 2.446, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.44556
[1mStep[0m  [5/53], [94mLoss[0m : 2.33898
[1mStep[0m  [10/53], [94mLoss[0m : 2.34256
[1mStep[0m  [15/53], [94mLoss[0m : 2.19273
[1mStep[0m  [20/53], [94mLoss[0m : 2.19540
[1mStep[0m  [25/53], [94mLoss[0m : 2.34522
[1mStep[0m  [30/53], [94mLoss[0m : 2.37239
[1mStep[0m  [35/53], [94mLoss[0m : 2.26868
[1mStep[0m  [40/53], [94mLoss[0m : 2.17601
[1mStep[0m  [45/53], [94mLoss[0m : 2.34177
[1mStep[0m  [50/53], [94mLoss[0m : 2.14439

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.278, [92mTest[0m: 2.440, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.24901
[1mStep[0m  [5/53], [94mLoss[0m : 2.21606
[1mStep[0m  [10/53], [94mLoss[0m : 2.18091
[1mStep[0m  [15/53], [94mLoss[0m : 2.30275
[1mStep[0m  [20/53], [94mLoss[0m : 2.20125
[1mStep[0m  [25/53], [94mLoss[0m : 2.41702
[1mStep[0m  [30/53], [94mLoss[0m : 2.35816
[1mStep[0m  [35/53], [94mLoss[0m : 2.31883
[1mStep[0m  [40/53], [94mLoss[0m : 2.39050
[1mStep[0m  [45/53], [94mLoss[0m : 2.32681
[1mStep[0m  [50/53], [94mLoss[0m : 2.11073

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.264, [92mTest[0m: 2.441, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.18202
[1mStep[0m  [5/53], [94mLoss[0m : 2.20801
[1mStep[0m  [10/53], [94mLoss[0m : 2.03621
[1mStep[0m  [15/53], [94mLoss[0m : 2.32139
[1mStep[0m  [20/53], [94mLoss[0m : 2.28807
[1mStep[0m  [25/53], [94mLoss[0m : 2.23749
[1mStep[0m  [30/53], [94mLoss[0m : 2.18030
[1mStep[0m  [35/53], [94mLoss[0m : 2.22718
[1mStep[0m  [40/53], [94mLoss[0m : 2.18250
[1mStep[0m  [45/53], [94mLoss[0m : 2.01788
[1mStep[0m  [50/53], [94mLoss[0m : 2.24018

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.238, [92mTest[0m: 2.423, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.28676
[1mStep[0m  [5/53], [94mLoss[0m : 2.08859
[1mStep[0m  [10/53], [94mLoss[0m : 2.08885
[1mStep[0m  [15/53], [94mLoss[0m : 2.21093
[1mStep[0m  [20/53], [94mLoss[0m : 2.12848
[1mStep[0m  [25/53], [94mLoss[0m : 2.26066
[1mStep[0m  [30/53], [94mLoss[0m : 2.12170
[1mStep[0m  [35/53], [94mLoss[0m : 2.04925
[1mStep[0m  [40/53], [94mLoss[0m : 2.13489
[1mStep[0m  [45/53], [94mLoss[0m : 2.26599
[1mStep[0m  [50/53], [94mLoss[0m : 2.17029

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.211, [92mTest[0m: 2.452, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.17994
[1mStep[0m  [5/53], [94mLoss[0m : 2.24704
[1mStep[0m  [10/53], [94mLoss[0m : 2.20661
[1mStep[0m  [15/53], [94mLoss[0m : 2.16322
[1mStep[0m  [20/53], [94mLoss[0m : 2.26988
[1mStep[0m  [25/53], [94mLoss[0m : 2.21644
[1mStep[0m  [30/53], [94mLoss[0m : 2.01532
[1mStep[0m  [35/53], [94mLoss[0m : 2.16698
[1mStep[0m  [40/53], [94mLoss[0m : 2.33309
[1mStep[0m  [45/53], [94mLoss[0m : 2.10781
[1mStep[0m  [50/53], [94mLoss[0m : 2.37144

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.195, [92mTest[0m: 2.425, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.00936
[1mStep[0m  [5/53], [94mLoss[0m : 2.15388
[1mStep[0m  [10/53], [94mLoss[0m : 2.24710
[1mStep[0m  [15/53], [94mLoss[0m : 2.45639
[1mStep[0m  [20/53], [94mLoss[0m : 1.95467
[1mStep[0m  [25/53], [94mLoss[0m : 2.09579
[1mStep[0m  [30/53], [94mLoss[0m : 2.31550
[1mStep[0m  [35/53], [94mLoss[0m : 2.30575
[1mStep[0m  [40/53], [94mLoss[0m : 2.21243
[1mStep[0m  [45/53], [94mLoss[0m : 2.02593
[1mStep[0m  [50/53], [94mLoss[0m : 2.07400

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.143, [92mTest[0m: 2.436, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.12525
[1mStep[0m  [5/53], [94mLoss[0m : 2.02313
[1mStep[0m  [10/53], [94mLoss[0m : 1.95536
[1mStep[0m  [15/53], [94mLoss[0m : 2.05285
[1mStep[0m  [20/53], [94mLoss[0m : 2.01433
[1mStep[0m  [25/53], [94mLoss[0m : 2.04633
[1mStep[0m  [30/53], [94mLoss[0m : 2.13929
[1mStep[0m  [35/53], [94mLoss[0m : 2.11168
[1mStep[0m  [40/53], [94mLoss[0m : 1.98774
[1mStep[0m  [45/53], [94mLoss[0m : 2.21943
[1mStep[0m  [50/53], [94mLoss[0m : 1.97549

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.116, [92mTest[0m: 2.419, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.20265
[1mStep[0m  [5/53], [94mLoss[0m : 2.09512
[1mStep[0m  [10/53], [94mLoss[0m : 2.06620
[1mStep[0m  [15/53], [94mLoss[0m : 2.15812
[1mStep[0m  [20/53], [94mLoss[0m : 2.09973
[1mStep[0m  [25/53], [94mLoss[0m : 2.17913
[1mStep[0m  [30/53], [94mLoss[0m : 2.00146
[1mStep[0m  [35/53], [94mLoss[0m : 2.21607
[1mStep[0m  [40/53], [94mLoss[0m : 2.32408
[1mStep[0m  [45/53], [94mLoss[0m : 2.19554
[1mStep[0m  [50/53], [94mLoss[0m : 2.04909

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.096, [92mTest[0m: 2.404, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.15568
[1mStep[0m  [5/53], [94mLoss[0m : 2.13081
[1mStep[0m  [10/53], [94mLoss[0m : 2.06430
[1mStep[0m  [15/53], [94mLoss[0m : 2.02999
[1mStep[0m  [20/53], [94mLoss[0m : 2.05328
[1mStep[0m  [25/53], [94mLoss[0m : 1.97131
[1mStep[0m  [30/53], [94mLoss[0m : 2.09485
[1mStep[0m  [35/53], [94mLoss[0m : 1.92085
[1mStep[0m  [40/53], [94mLoss[0m : 2.29662
[1mStep[0m  [45/53], [94mLoss[0m : 2.15230
[1mStep[0m  [50/53], [94mLoss[0m : 1.78940

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.064, [92mTest[0m: 2.418, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.00735
[1mStep[0m  [5/53], [94mLoss[0m : 2.13003
[1mStep[0m  [10/53], [94mLoss[0m : 1.90473
[1mStep[0m  [15/53], [94mLoss[0m : 1.95609
[1mStep[0m  [20/53], [94mLoss[0m : 1.85801
[1mStep[0m  [25/53], [94mLoss[0m : 2.22969
[1mStep[0m  [30/53], [94mLoss[0m : 1.99390
[1mStep[0m  [35/53], [94mLoss[0m : 1.81858
[1mStep[0m  [40/53], [94mLoss[0m : 2.15493
[1mStep[0m  [45/53], [94mLoss[0m : 2.10601
[1mStep[0m  [50/53], [94mLoss[0m : 2.10448

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.052, [92mTest[0m: 2.422, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.22552
[1mStep[0m  [5/53], [94mLoss[0m : 2.07892
[1mStep[0m  [10/53], [94mLoss[0m : 1.92109
[1mStep[0m  [15/53], [94mLoss[0m : 1.95878
[1mStep[0m  [20/53], [94mLoss[0m : 1.79983
[1mStep[0m  [25/53], [94mLoss[0m : 2.07781
[1mStep[0m  [30/53], [94mLoss[0m : 1.96178
[1mStep[0m  [35/53], [94mLoss[0m : 2.03776
[1mStep[0m  [40/53], [94mLoss[0m : 2.13542
[1mStep[0m  [45/53], [94mLoss[0m : 2.09244
[1mStep[0m  [50/53], [94mLoss[0m : 1.98192

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.006, [92mTest[0m: 2.425, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.94373
[1mStep[0m  [5/53], [94mLoss[0m : 2.12686
[1mStep[0m  [10/53], [94mLoss[0m : 1.88745
[1mStep[0m  [15/53], [94mLoss[0m : 2.15075
[1mStep[0m  [20/53], [94mLoss[0m : 1.93552
[1mStep[0m  [25/53], [94mLoss[0m : 1.78065
[1mStep[0m  [30/53], [94mLoss[0m : 2.08372
[1mStep[0m  [35/53], [94mLoss[0m : 1.94889
[1mStep[0m  [40/53], [94mLoss[0m : 2.00646
[1mStep[0m  [45/53], [94mLoss[0m : 1.94565
[1mStep[0m  [50/53], [94mLoss[0m : 1.99872

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.966, [92mTest[0m: 2.468, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.03462
[1mStep[0m  [5/53], [94mLoss[0m : 1.94939
[1mStep[0m  [10/53], [94mLoss[0m : 2.03367
[1mStep[0m  [15/53], [94mLoss[0m : 1.90736
[1mStep[0m  [20/53], [94mLoss[0m : 1.84250
[1mStep[0m  [25/53], [94mLoss[0m : 2.00000
[1mStep[0m  [30/53], [94mLoss[0m : 2.12179
[1mStep[0m  [35/53], [94mLoss[0m : 2.24771
[1mStep[0m  [40/53], [94mLoss[0m : 1.97247
[1mStep[0m  [45/53], [94mLoss[0m : 2.04618
[1mStep[0m  [50/53], [94mLoss[0m : 1.89104

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.944, [92mTest[0m: 2.488, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.84555
[1mStep[0m  [5/53], [94mLoss[0m : 1.96679
[1mStep[0m  [10/53], [94mLoss[0m : 1.72610
[1mStep[0m  [15/53], [94mLoss[0m : 1.89424
[1mStep[0m  [20/53], [94mLoss[0m : 1.85758
[1mStep[0m  [25/53], [94mLoss[0m : 1.82968
[1mStep[0m  [30/53], [94mLoss[0m : 1.82180
[1mStep[0m  [35/53], [94mLoss[0m : 1.97923
[1mStep[0m  [40/53], [94mLoss[0m : 1.83439
[1mStep[0m  [45/53], [94mLoss[0m : 2.04031
[1mStep[0m  [50/53], [94mLoss[0m : 1.78062

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.900, [92mTest[0m: 2.499, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.73837
[1mStep[0m  [5/53], [94mLoss[0m : 1.67255
[1mStep[0m  [10/53], [94mLoss[0m : 1.80295
[1mStep[0m  [15/53], [94mLoss[0m : 1.91438
[1mStep[0m  [20/53], [94mLoss[0m : 1.85549
[1mStep[0m  [25/53], [94mLoss[0m : 1.83126
[1mStep[0m  [30/53], [94mLoss[0m : 1.77560
[1mStep[0m  [35/53], [94mLoss[0m : 2.03535
[1mStep[0m  [40/53], [94mLoss[0m : 1.77314
[1mStep[0m  [45/53], [94mLoss[0m : 1.80353
[1mStep[0m  [50/53], [94mLoss[0m : 2.08295

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.870, [92mTest[0m: 2.520, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.91830
[1mStep[0m  [5/53], [94mLoss[0m : 1.91303
[1mStep[0m  [10/53], [94mLoss[0m : 1.93757
[1mStep[0m  [15/53], [94mLoss[0m : 1.92556
[1mStep[0m  [20/53], [94mLoss[0m : 1.82347
[1mStep[0m  [25/53], [94mLoss[0m : 1.84992
[1mStep[0m  [30/53], [94mLoss[0m : 1.95611
[1mStep[0m  [35/53], [94mLoss[0m : 1.94164
[1mStep[0m  [40/53], [94mLoss[0m : 1.80656
[1mStep[0m  [45/53], [94mLoss[0m : 1.91196
[1mStep[0m  [50/53], [94mLoss[0m : 1.75906

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.854, [92mTest[0m: 2.474, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.78307
[1mStep[0m  [5/53], [94mLoss[0m : 1.80749
[1mStep[0m  [10/53], [94mLoss[0m : 1.81607
[1mStep[0m  [15/53], [94mLoss[0m : 1.82662
[1mStep[0m  [20/53], [94mLoss[0m : 1.98887
[1mStep[0m  [25/53], [94mLoss[0m : 1.86013
[1mStep[0m  [30/53], [94mLoss[0m : 1.87657
[1mStep[0m  [35/53], [94mLoss[0m : 1.94698
[1mStep[0m  [40/53], [94mLoss[0m : 1.79779
[1mStep[0m  [45/53], [94mLoss[0m : 1.66134
[1mStep[0m  [50/53], [94mLoss[0m : 1.93622

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.810, [92mTest[0m: 2.611, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.76346
[1mStep[0m  [5/53], [94mLoss[0m : 1.84565
[1mStep[0m  [10/53], [94mLoss[0m : 1.74147
[1mStep[0m  [15/53], [94mLoss[0m : 1.79575
[1mStep[0m  [20/53], [94mLoss[0m : 1.74082
[1mStep[0m  [25/53], [94mLoss[0m : 1.86527
[1mStep[0m  [30/53], [94mLoss[0m : 1.92135
[1mStep[0m  [35/53], [94mLoss[0m : 1.88645
[1mStep[0m  [40/53], [94mLoss[0m : 1.66720
[1mStep[0m  [45/53], [94mLoss[0m : 1.81236
[1mStep[0m  [50/53], [94mLoss[0m : 1.94238

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.798, [92mTest[0m: 2.591, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.78951
[1mStep[0m  [5/53], [94mLoss[0m : 1.78163
[1mStep[0m  [10/53], [94mLoss[0m : 1.71134
[1mStep[0m  [15/53], [94mLoss[0m : 1.80570
[1mStep[0m  [20/53], [94mLoss[0m : 1.74353
[1mStep[0m  [25/53], [94mLoss[0m : 1.89522
[1mStep[0m  [30/53], [94mLoss[0m : 1.68514
[1mStep[0m  [35/53], [94mLoss[0m : 1.87238
[1mStep[0m  [40/53], [94mLoss[0m : 1.72454
[1mStep[0m  [45/53], [94mLoss[0m : 1.55834
[1mStep[0m  [50/53], [94mLoss[0m : 1.81470

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.774, [92mTest[0m: 2.573, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.67070
[1mStep[0m  [5/53], [94mLoss[0m : 1.85402
[1mStep[0m  [10/53], [94mLoss[0m : 1.59405
[1mStep[0m  [15/53], [94mLoss[0m : 1.90570
[1mStep[0m  [20/53], [94mLoss[0m : 1.82169
[1mStep[0m  [25/53], [94mLoss[0m : 1.70372
[1mStep[0m  [30/53], [94mLoss[0m : 1.77616
[1mStep[0m  [35/53], [94mLoss[0m : 1.75086
[1mStep[0m  [40/53], [94mLoss[0m : 1.76922
[1mStep[0m  [45/53], [94mLoss[0m : 1.64253
[1mStep[0m  [50/53], [94mLoss[0m : 1.87937

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.760, [92mTest[0m: 2.547, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.77735
[1mStep[0m  [5/53], [94mLoss[0m : 1.67166
[1mStep[0m  [10/53], [94mLoss[0m : 1.71459
[1mStep[0m  [15/53], [94mLoss[0m : 1.67540
[1mStep[0m  [20/53], [94mLoss[0m : 1.77360
[1mStep[0m  [25/53], [94mLoss[0m : 1.59978
[1mStep[0m  [30/53], [94mLoss[0m : 1.51641
[1mStep[0m  [35/53], [94mLoss[0m : 1.83448
[1mStep[0m  [40/53], [94mLoss[0m : 1.81732
[1mStep[0m  [45/53], [94mLoss[0m : 1.75336
[1mStep[0m  [50/53], [94mLoss[0m : 1.83959

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.716, [92mTest[0m: 2.707, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.72504
[1mStep[0m  [5/53], [94mLoss[0m : 1.71785
[1mStep[0m  [10/53], [94mLoss[0m : 1.71173
[1mStep[0m  [15/53], [94mLoss[0m : 1.59119
[1mStep[0m  [20/53], [94mLoss[0m : 1.86368
[1mStep[0m  [25/53], [94mLoss[0m : 1.69116
[1mStep[0m  [30/53], [94mLoss[0m : 1.62357
[1mStep[0m  [35/53], [94mLoss[0m : 1.53365
[1mStep[0m  [40/53], [94mLoss[0m : 1.65976
[1mStep[0m  [45/53], [94mLoss[0m : 1.75289
[1mStep[0m  [50/53], [94mLoss[0m : 1.75058

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.687, [92mTest[0m: 2.654, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.63348
[1mStep[0m  [5/53], [94mLoss[0m : 1.55033
[1mStep[0m  [10/53], [94mLoss[0m : 1.61766
[1mStep[0m  [15/53], [94mLoss[0m : 1.59554
[1mStep[0m  [20/53], [94mLoss[0m : 1.60687
[1mStep[0m  [25/53], [94mLoss[0m : 1.51275
[1mStep[0m  [30/53], [94mLoss[0m : 1.64020
[1mStep[0m  [35/53], [94mLoss[0m : 1.70257
[1mStep[0m  [40/53], [94mLoss[0m : 1.62853
[1mStep[0m  [45/53], [94mLoss[0m : 1.76682
[1mStep[0m  [50/53], [94mLoss[0m : 1.67145

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.668, [92mTest[0m: 2.594, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.77729
[1mStep[0m  [5/53], [94mLoss[0m : 1.60918
[1mStep[0m  [10/53], [94mLoss[0m : 1.68357
[1mStep[0m  [15/53], [94mLoss[0m : 1.51268
[1mStep[0m  [20/53], [94mLoss[0m : 1.42131
[1mStep[0m  [25/53], [94mLoss[0m : 1.51660
[1mStep[0m  [30/53], [94mLoss[0m : 1.62557
[1mStep[0m  [35/53], [94mLoss[0m : 1.59359
[1mStep[0m  [40/53], [94mLoss[0m : 1.68945
[1mStep[0m  [45/53], [94mLoss[0m : 1.53115
[1mStep[0m  [50/53], [94mLoss[0m : 1.56143

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.646, [92mTest[0m: 2.594, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.64988
[1mStep[0m  [5/53], [94mLoss[0m : 1.72611
[1mStep[0m  [10/53], [94mLoss[0m : 1.65231
[1mStep[0m  [15/53], [94mLoss[0m : 1.63575
[1mStep[0m  [20/53], [94mLoss[0m : 1.59818
[1mStep[0m  [25/53], [94mLoss[0m : 1.55159
[1mStep[0m  [30/53], [94mLoss[0m : 1.67495
[1mStep[0m  [35/53], [94mLoss[0m : 1.61432
[1mStep[0m  [40/53], [94mLoss[0m : 1.58757
[1mStep[0m  [45/53], [94mLoss[0m : 1.79604
[1mStep[0m  [50/53], [94mLoss[0m : 1.57323

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.635, [92mTest[0m: 2.644, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.579
====================================

Phase 2 - Evaluation MAE:  2.5789913947765646
MAE score P1      2.539413
MAE score P2      2.578991
loss              1.635088
learning_rate     0.002575
batch_size             256
hidden_sizes         [300]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.5
weight_decay          0.01
Name: 8, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/53], [94mLoss[0m : 10.77897
[1mStep[0m  [5/53], [94mLoss[0m : 11.05833
[1mStep[0m  [10/53], [94mLoss[0m : 10.99696
[1mStep[0m  [15/53], [94mLoss[0m : 11.06077
[1mStep[0m  [20/53], [94mLoss[0m : 10.56884
[1mStep[0m  [25/53], [94mLoss[0m : 10.62949
[1mStep[0m  [30/53], [94mLoss[0m : 10.60854
[1mStep[0m  [35/53], [94mLoss[0m : 10.68304
[1mStep[0m  [40/53], [94mLoss[0m : 10.46133
[1mStep[0m  [45/53], [94mLoss[0m : 9.95279
[1mStep[0m  [50/53], [94mLoss[0m : 10.36633

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.669, [92mTest[0m: 10.889, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.50052
[1mStep[0m  [5/53], [94mLoss[0m : 10.30954
[1mStep[0m  [10/53], [94mLoss[0m : 10.39665
[1mStep[0m  [15/53], [94mLoss[0m : 10.10410
[1mStep[0m  [20/53], [94mLoss[0m : 10.43861
[1mStep[0m  [25/53], [94mLoss[0m : 10.35411
[1mStep[0m  [30/53], [94mLoss[0m : 10.30999
[1mStep[0m  [35/53], [94mLoss[0m : 9.66809
[1mStep[0m  [40/53], [94mLoss[0m : 10.20902
[1mStep[0m  [45/53], [94mLoss[0m : 10.04109
[1mStep[0m  [50/53], [94mLoss[0m : 10.35355

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.198, [92mTest[0m: 10.351, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.88993
[1mStep[0m  [5/53], [94mLoss[0m : 9.87623
[1mStep[0m  [10/53], [94mLoss[0m : 9.99183
[1mStep[0m  [15/53], [94mLoss[0m : 9.61410
[1mStep[0m  [20/53], [94mLoss[0m : 10.16753
[1mStep[0m  [25/53], [94mLoss[0m : 9.52116
[1mStep[0m  [30/53], [94mLoss[0m : 9.40126
[1mStep[0m  [35/53], [94mLoss[0m : 9.57996
[1mStep[0m  [40/53], [94mLoss[0m : 9.81513
[1mStep[0m  [45/53], [94mLoss[0m : 9.21858
[1mStep[0m  [50/53], [94mLoss[0m : 9.92463

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.660, [92mTest[0m: 9.676, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.02134
[1mStep[0m  [5/53], [94mLoss[0m : 9.04298
[1mStep[0m  [10/53], [94mLoss[0m : 9.13706
[1mStep[0m  [15/53], [94mLoss[0m : 8.64351
[1mStep[0m  [20/53], [94mLoss[0m : 8.90084
[1mStep[0m  [25/53], [94mLoss[0m : 9.32541
[1mStep[0m  [30/53], [94mLoss[0m : 9.19396
[1mStep[0m  [35/53], [94mLoss[0m : 8.85379
[1mStep[0m  [40/53], [94mLoss[0m : 8.88853
[1mStep[0m  [45/53], [94mLoss[0m : 9.09707
[1mStep[0m  [50/53], [94mLoss[0m : 8.77318

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 9.017, [92mTest[0m: 8.886, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.90696
[1mStep[0m  [5/53], [94mLoss[0m : 8.38493
[1mStep[0m  [10/53], [94mLoss[0m : 8.80794
[1mStep[0m  [15/53], [94mLoss[0m : 8.27206
[1mStep[0m  [20/53], [94mLoss[0m : 8.03243
[1mStep[0m  [25/53], [94mLoss[0m : 8.08068
[1mStep[0m  [30/53], [94mLoss[0m : 8.44433
[1mStep[0m  [35/53], [94mLoss[0m : 7.96930
[1mStep[0m  [40/53], [94mLoss[0m : 7.85619
[1mStep[0m  [45/53], [94mLoss[0m : 8.08154
[1mStep[0m  [50/53], [94mLoss[0m : 7.68160

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 8.335, [92mTest[0m: 8.116, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.14996
[1mStep[0m  [5/53], [94mLoss[0m : 7.88691
[1mStep[0m  [10/53], [94mLoss[0m : 7.74377
[1mStep[0m  [15/53], [94mLoss[0m : 7.63331
[1mStep[0m  [20/53], [94mLoss[0m : 7.65866
[1mStep[0m  [25/53], [94mLoss[0m : 7.79320
[1mStep[0m  [30/53], [94mLoss[0m : 7.92782
[1mStep[0m  [35/53], [94mLoss[0m : 7.50330
[1mStep[0m  [40/53], [94mLoss[0m : 7.13955
[1mStep[0m  [45/53], [94mLoss[0m : 7.69691
[1mStep[0m  [50/53], [94mLoss[0m : 7.07933

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 7.657, [92mTest[0m: 7.126, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.19881
[1mStep[0m  [5/53], [94mLoss[0m : 6.98987
[1mStep[0m  [10/53], [94mLoss[0m : 7.38968
[1mStep[0m  [15/53], [94mLoss[0m : 6.98581
[1mStep[0m  [20/53], [94mLoss[0m : 7.09485
[1mStep[0m  [25/53], [94mLoss[0m : 7.41263
[1mStep[0m  [30/53], [94mLoss[0m : 7.08637
[1mStep[0m  [35/53], [94mLoss[0m : 7.00959
[1mStep[0m  [40/53], [94mLoss[0m : 6.88327
[1mStep[0m  [45/53], [94mLoss[0m : 6.68676
[1mStep[0m  [50/53], [94mLoss[0m : 7.27267

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 7.094, [92mTest[0m: 6.637, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.34760
[1mStep[0m  [5/53], [94mLoss[0m : 7.07891
[1mStep[0m  [10/53], [94mLoss[0m : 6.92250
[1mStep[0m  [15/53], [94mLoss[0m : 6.51074
[1mStep[0m  [20/53], [94mLoss[0m : 6.73697
[1mStep[0m  [25/53], [94mLoss[0m : 6.70476
[1mStep[0m  [30/53], [94mLoss[0m : 6.34793
[1mStep[0m  [35/53], [94mLoss[0m : 6.98465
[1mStep[0m  [40/53], [94mLoss[0m : 6.10951
[1mStep[0m  [45/53], [94mLoss[0m : 6.61656
[1mStep[0m  [50/53], [94mLoss[0m : 6.36193

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 6.573, [92mTest[0m: 6.150, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.56422
[1mStep[0m  [5/53], [94mLoss[0m : 6.42710
[1mStep[0m  [10/53], [94mLoss[0m : 6.38285
[1mStep[0m  [15/53], [94mLoss[0m : 6.47237
[1mStep[0m  [20/53], [94mLoss[0m : 6.07911
[1mStep[0m  [25/53], [94mLoss[0m : 6.29222
[1mStep[0m  [30/53], [94mLoss[0m : 6.33504
[1mStep[0m  [35/53], [94mLoss[0m : 5.60765
[1mStep[0m  [40/53], [94mLoss[0m : 5.74428
[1mStep[0m  [45/53], [94mLoss[0m : 5.51679
[1mStep[0m  [50/53], [94mLoss[0m : 5.76862

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 6.072, [92mTest[0m: 5.371, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 5.74857
[1mStep[0m  [5/53], [94mLoss[0m : 5.75993
[1mStep[0m  [10/53], [94mLoss[0m : 5.75616
[1mStep[0m  [15/53], [94mLoss[0m : 5.45087
[1mStep[0m  [20/53], [94mLoss[0m : 5.91648
[1mStep[0m  [25/53], [94mLoss[0m : 5.74486
[1mStep[0m  [30/53], [94mLoss[0m : 5.16202
[1mStep[0m  [35/53], [94mLoss[0m : 5.58399
[1mStep[0m  [40/53], [94mLoss[0m : 5.73643
[1mStep[0m  [45/53], [94mLoss[0m : 5.44781
[1mStep[0m  [50/53], [94mLoss[0m : 5.50794

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 5.560, [92mTest[0m: 5.056, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 5.53223
[1mStep[0m  [5/53], [94mLoss[0m : 5.32804
[1mStep[0m  [10/53], [94mLoss[0m : 5.47606
[1mStep[0m  [15/53], [94mLoss[0m : 5.10047
[1mStep[0m  [20/53], [94mLoss[0m : 4.76881
[1mStep[0m  [25/53], [94mLoss[0m : 4.29675
[1mStep[0m  [30/53], [94mLoss[0m : 5.13760
[1mStep[0m  [35/53], [94mLoss[0m : 4.46741
[1mStep[0m  [40/53], [94mLoss[0m : 4.91273
[1mStep[0m  [45/53], [94mLoss[0m : 4.66662
[1mStep[0m  [50/53], [94mLoss[0m : 5.25445

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 4.988, [92mTest[0m: 4.571, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 4.59352
[1mStep[0m  [5/53], [94mLoss[0m : 4.45668
[1mStep[0m  [10/53], [94mLoss[0m : 4.68441
[1mStep[0m  [15/53], [94mLoss[0m : 4.83222
[1mStep[0m  [20/53], [94mLoss[0m : 4.45834
[1mStep[0m  [25/53], [94mLoss[0m : 4.27248
[1mStep[0m  [30/53], [94mLoss[0m : 4.32568
[1mStep[0m  [35/53], [94mLoss[0m : 4.40223
[1mStep[0m  [40/53], [94mLoss[0m : 4.12991
[1mStep[0m  [45/53], [94mLoss[0m : 4.04121
[1mStep[0m  [50/53], [94mLoss[0m : 4.52203

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 4.393, [92mTest[0m: 4.028, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.46975
[1mStep[0m  [5/53], [94mLoss[0m : 3.82993
[1mStep[0m  [10/53], [94mLoss[0m : 3.93222
[1mStep[0m  [15/53], [94mLoss[0m : 3.95969
[1mStep[0m  [20/53], [94mLoss[0m : 3.85644
[1mStep[0m  [25/53], [94mLoss[0m : 3.99616
[1mStep[0m  [30/53], [94mLoss[0m : 3.55743
[1mStep[0m  [35/53], [94mLoss[0m : 3.63617
[1mStep[0m  [40/53], [94mLoss[0m : 3.64452
[1mStep[0m  [45/53], [94mLoss[0m : 3.63339
[1mStep[0m  [50/53], [94mLoss[0m : 3.56023

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 3.769, [92mTest[0m: 3.445, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.18243
[1mStep[0m  [5/53], [94mLoss[0m : 3.78172
[1mStep[0m  [10/53], [94mLoss[0m : 3.23569
[1mStep[0m  [15/53], [94mLoss[0m : 3.27463
[1mStep[0m  [20/53], [94mLoss[0m : 3.16638
[1mStep[0m  [25/53], [94mLoss[0m : 3.00819
[1mStep[0m  [30/53], [94mLoss[0m : 3.19064
[1mStep[0m  [35/53], [94mLoss[0m : 3.16155
[1mStep[0m  [40/53], [94mLoss[0m : 3.18380
[1mStep[0m  [45/53], [94mLoss[0m : 3.29156
[1mStep[0m  [50/53], [94mLoss[0m : 2.91661

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 3.232, [92mTest[0m: 3.025, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.93017
[1mStep[0m  [5/53], [94mLoss[0m : 2.86804
[1mStep[0m  [10/53], [94mLoss[0m : 2.92274
[1mStep[0m  [15/53], [94mLoss[0m : 3.30647
[1mStep[0m  [20/53], [94mLoss[0m : 2.79684
[1mStep[0m  [25/53], [94mLoss[0m : 2.68556
[1mStep[0m  [30/53], [94mLoss[0m : 2.68220
[1mStep[0m  [35/53], [94mLoss[0m : 2.84230
[1mStep[0m  [40/53], [94mLoss[0m : 2.63125
[1mStep[0m  [45/53], [94mLoss[0m : 2.62957
[1mStep[0m  [50/53], [94mLoss[0m : 3.00879

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.854, [92mTest[0m: 2.663, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.54273
[1mStep[0m  [5/53], [94mLoss[0m : 2.79531
[1mStep[0m  [10/53], [94mLoss[0m : 2.64301
[1mStep[0m  [15/53], [94mLoss[0m : 2.95789
[1mStep[0m  [20/53], [94mLoss[0m : 2.58600
[1mStep[0m  [25/53], [94mLoss[0m : 2.57822
[1mStep[0m  [30/53], [94mLoss[0m : 2.75513
[1mStep[0m  [35/53], [94mLoss[0m : 2.60952
[1mStep[0m  [40/53], [94mLoss[0m : 2.78892
[1mStep[0m  [45/53], [94mLoss[0m : 2.57160
[1mStep[0m  [50/53], [94mLoss[0m : 2.47381

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.667, [92mTest[0m: 2.500, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.53948
[1mStep[0m  [5/53], [94mLoss[0m : 2.59533
[1mStep[0m  [10/53], [94mLoss[0m : 2.68112
[1mStep[0m  [15/53], [94mLoss[0m : 2.78679
[1mStep[0m  [20/53], [94mLoss[0m : 2.56046
[1mStep[0m  [25/53], [94mLoss[0m : 2.81906
[1mStep[0m  [30/53], [94mLoss[0m : 2.70753
[1mStep[0m  [35/53], [94mLoss[0m : 2.71472
[1mStep[0m  [40/53], [94mLoss[0m : 2.47652
[1mStep[0m  [45/53], [94mLoss[0m : 2.41042
[1mStep[0m  [50/53], [94mLoss[0m : 2.60806

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.605, [92mTest[0m: 2.471, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.56279
[1mStep[0m  [5/53], [94mLoss[0m : 2.59215
[1mStep[0m  [10/53], [94mLoss[0m : 2.58774
[1mStep[0m  [15/53], [94mLoss[0m : 2.64609
[1mStep[0m  [20/53], [94mLoss[0m : 2.29549
[1mStep[0m  [25/53], [94mLoss[0m : 2.39687
[1mStep[0m  [30/53], [94mLoss[0m : 2.49275
[1mStep[0m  [35/53], [94mLoss[0m : 2.53085
[1mStep[0m  [40/53], [94mLoss[0m : 2.52785
[1mStep[0m  [45/53], [94mLoss[0m : 2.43719
[1mStep[0m  [50/53], [94mLoss[0m : 2.60661

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.571, [92mTest[0m: 2.468, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.65464
[1mStep[0m  [5/53], [94mLoss[0m : 2.34028
[1mStep[0m  [10/53], [94mLoss[0m : 2.62920
[1mStep[0m  [15/53], [94mLoss[0m : 2.33379
[1mStep[0m  [20/53], [94mLoss[0m : 2.61704
[1mStep[0m  [25/53], [94mLoss[0m : 2.59087
[1mStep[0m  [30/53], [94mLoss[0m : 2.77746
[1mStep[0m  [35/53], [94mLoss[0m : 2.52716
[1mStep[0m  [40/53], [94mLoss[0m : 2.60023
[1mStep[0m  [45/53], [94mLoss[0m : 2.54883
[1mStep[0m  [50/53], [94mLoss[0m : 2.50722

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.560, [92mTest[0m: 2.467, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.36032
[1mStep[0m  [5/53], [94mLoss[0m : 2.78095
[1mStep[0m  [10/53], [94mLoss[0m : 2.43099
[1mStep[0m  [15/53], [94mLoss[0m : 2.69654
[1mStep[0m  [20/53], [94mLoss[0m : 2.52886
[1mStep[0m  [25/53], [94mLoss[0m : 2.50986
[1mStep[0m  [30/53], [94mLoss[0m : 2.36058
[1mStep[0m  [35/53], [94mLoss[0m : 2.66052
[1mStep[0m  [40/53], [94mLoss[0m : 2.61405
[1mStep[0m  [45/53], [94mLoss[0m : 2.52338
[1mStep[0m  [50/53], [94mLoss[0m : 2.62913

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.565, [92mTest[0m: 2.465, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.50725
[1mStep[0m  [5/53], [94mLoss[0m : 2.67443
[1mStep[0m  [10/53], [94mLoss[0m : 2.56385
[1mStep[0m  [15/53], [94mLoss[0m : 2.38270
[1mStep[0m  [20/53], [94mLoss[0m : 2.56972
[1mStep[0m  [25/53], [94mLoss[0m : 2.45036
[1mStep[0m  [30/53], [94mLoss[0m : 2.38179
[1mStep[0m  [35/53], [94mLoss[0m : 2.40719
[1mStep[0m  [40/53], [94mLoss[0m : 2.51657
[1mStep[0m  [45/53], [94mLoss[0m : 2.54710
[1mStep[0m  [50/53], [94mLoss[0m : 2.52085

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.558, [92mTest[0m: 2.469, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.52202
[1mStep[0m  [5/53], [94mLoss[0m : 2.40247
[1mStep[0m  [10/53], [94mLoss[0m : 2.58028
[1mStep[0m  [15/53], [94mLoss[0m : 2.41339
[1mStep[0m  [20/53], [94mLoss[0m : 2.51395
[1mStep[0m  [25/53], [94mLoss[0m : 2.66691
[1mStep[0m  [30/53], [94mLoss[0m : 2.32499
[1mStep[0m  [35/53], [94mLoss[0m : 2.24198
[1mStep[0m  [40/53], [94mLoss[0m : 2.56752
[1mStep[0m  [45/53], [94mLoss[0m : 2.42920
[1mStep[0m  [50/53], [94mLoss[0m : 2.33485

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.542, [92mTest[0m: 2.443, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.62005
[1mStep[0m  [5/53], [94mLoss[0m : 2.53304
[1mStep[0m  [10/53], [94mLoss[0m : 2.59961
[1mStep[0m  [15/53], [94mLoss[0m : 2.65466
[1mStep[0m  [20/53], [94mLoss[0m : 2.52904
[1mStep[0m  [25/53], [94mLoss[0m : 2.55195
[1mStep[0m  [30/53], [94mLoss[0m : 2.52057
[1mStep[0m  [35/53], [94mLoss[0m : 2.45344
[1mStep[0m  [40/53], [94mLoss[0m : 2.54901
[1mStep[0m  [45/53], [94mLoss[0m : 2.28554
[1mStep[0m  [50/53], [94mLoss[0m : 2.68850

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.536, [92mTest[0m: 2.466, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.77043
[1mStep[0m  [5/53], [94mLoss[0m : 2.47872
[1mStep[0m  [10/53], [94mLoss[0m : 2.37800
[1mStep[0m  [15/53], [94mLoss[0m : 2.49912
[1mStep[0m  [20/53], [94mLoss[0m : 2.57701
[1mStep[0m  [25/53], [94mLoss[0m : 2.54945
[1mStep[0m  [30/53], [94mLoss[0m : 2.60122
[1mStep[0m  [35/53], [94mLoss[0m : 2.52394
[1mStep[0m  [40/53], [94mLoss[0m : 2.50508
[1mStep[0m  [45/53], [94mLoss[0m : 2.55104
[1mStep[0m  [50/53], [94mLoss[0m : 2.69829

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.540, [92mTest[0m: 2.497, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.57285
[1mStep[0m  [5/53], [94mLoss[0m : 2.40368
[1mStep[0m  [10/53], [94mLoss[0m : 2.52244
[1mStep[0m  [15/53], [94mLoss[0m : 2.41743
[1mStep[0m  [20/53], [94mLoss[0m : 2.36073
[1mStep[0m  [25/53], [94mLoss[0m : 2.64864
[1mStep[0m  [30/53], [94mLoss[0m : 2.43621
[1mStep[0m  [35/53], [94mLoss[0m : 2.39152
[1mStep[0m  [40/53], [94mLoss[0m : 2.49853
[1mStep[0m  [45/53], [94mLoss[0m : 2.49587
[1mStep[0m  [50/53], [94mLoss[0m : 2.45018

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.450, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.46517
[1mStep[0m  [5/53], [94mLoss[0m : 2.65618
[1mStep[0m  [10/53], [94mLoss[0m : 2.65412
[1mStep[0m  [15/53], [94mLoss[0m : 2.30159
[1mStep[0m  [20/53], [94mLoss[0m : 2.62494
[1mStep[0m  [25/53], [94mLoss[0m : 2.57068
[1mStep[0m  [30/53], [94mLoss[0m : 2.52410
[1mStep[0m  [35/53], [94mLoss[0m : 2.55544
[1mStep[0m  [40/53], [94mLoss[0m : 2.44349
[1mStep[0m  [45/53], [94mLoss[0m : 2.44924
[1mStep[0m  [50/53], [94mLoss[0m : 2.52768

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.546, [92mTest[0m: 2.506, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.41470
[1mStep[0m  [5/53], [94mLoss[0m : 2.45029
[1mStep[0m  [10/53], [94mLoss[0m : 2.50799
[1mStep[0m  [15/53], [94mLoss[0m : 2.51310
[1mStep[0m  [20/53], [94mLoss[0m : 2.52562
[1mStep[0m  [25/53], [94mLoss[0m : 2.74180
[1mStep[0m  [30/53], [94mLoss[0m : 2.41329
[1mStep[0m  [35/53], [94mLoss[0m : 2.46760
[1mStep[0m  [40/53], [94mLoss[0m : 2.32688
[1mStep[0m  [45/53], [94mLoss[0m : 2.50353
[1mStep[0m  [50/53], [94mLoss[0m : 2.58868

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.514, [92mTest[0m: 2.457, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.52300
[1mStep[0m  [5/53], [94mLoss[0m : 2.56274
[1mStep[0m  [10/53], [94mLoss[0m : 2.70198
[1mStep[0m  [15/53], [94mLoss[0m : 2.50334
[1mStep[0m  [20/53], [94mLoss[0m : 2.50072
[1mStep[0m  [25/53], [94mLoss[0m : 2.76197
[1mStep[0m  [30/53], [94mLoss[0m : 2.64375
[1mStep[0m  [35/53], [94mLoss[0m : 2.43695
[1mStep[0m  [40/53], [94mLoss[0m : 2.63751
[1mStep[0m  [45/53], [94mLoss[0m : 2.61269
[1mStep[0m  [50/53], [94mLoss[0m : 2.55878

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.456, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.28859
[1mStep[0m  [5/53], [94mLoss[0m : 2.54877
[1mStep[0m  [10/53], [94mLoss[0m : 2.32861
[1mStep[0m  [15/53], [94mLoss[0m : 2.53785
[1mStep[0m  [20/53], [94mLoss[0m : 2.39950
[1mStep[0m  [25/53], [94mLoss[0m : 2.52288
[1mStep[0m  [30/53], [94mLoss[0m : 2.53025
[1mStep[0m  [35/53], [94mLoss[0m : 2.49177
[1mStep[0m  [40/53], [94mLoss[0m : 2.47950
[1mStep[0m  [45/53], [94mLoss[0m : 2.48263
[1mStep[0m  [50/53], [94mLoss[0m : 2.67102

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.419, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.64551
[1mStep[0m  [5/53], [94mLoss[0m : 2.63005
[1mStep[0m  [10/53], [94mLoss[0m : 2.78883
[1mStep[0m  [15/53], [94mLoss[0m : 2.32774
[1mStep[0m  [20/53], [94mLoss[0m : 2.48446
[1mStep[0m  [25/53], [94mLoss[0m : 2.34416
[1mStep[0m  [30/53], [94mLoss[0m : 2.44555
[1mStep[0m  [35/53], [94mLoss[0m : 2.66032
[1mStep[0m  [40/53], [94mLoss[0m : 2.72234
[1mStep[0m  [45/53], [94mLoss[0m : 2.59549
[1mStep[0m  [50/53], [94mLoss[0m : 2.45001

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.425, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.438
====================================

Phase 1 - Evaluation MAE:  2.4383400678634644
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/53], [94mLoss[0m : 2.43640
[1mStep[0m  [5/53], [94mLoss[0m : 2.43106
[1mStep[0m  [10/53], [94mLoss[0m : 2.49427
[1mStep[0m  [15/53], [94mLoss[0m : 2.55641
[1mStep[0m  [20/53], [94mLoss[0m : 2.43034
[1mStep[0m  [25/53], [94mLoss[0m : 2.72545
[1mStep[0m  [30/53], [94mLoss[0m : 2.41450
[1mStep[0m  [35/53], [94mLoss[0m : 2.77816
[1mStep[0m  [40/53], [94mLoss[0m : 2.41725
[1mStep[0m  [45/53], [94mLoss[0m : 2.51737
[1mStep[0m  [50/53], [94mLoss[0m : 2.61521

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.564, [92mTest[0m: 2.442, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.57489
[1mStep[0m  [5/53], [94mLoss[0m : 2.74130
[1mStep[0m  [10/53], [94mLoss[0m : 2.52579
[1mStep[0m  [15/53], [94mLoss[0m : 2.79235
[1mStep[0m  [20/53], [94mLoss[0m : 2.70660
[1mStep[0m  [25/53], [94mLoss[0m : 2.33819
[1mStep[0m  [30/53], [94mLoss[0m : 2.29378
[1mStep[0m  [35/53], [94mLoss[0m : 2.45879
[1mStep[0m  [40/53], [94mLoss[0m : 2.29145
[1mStep[0m  [45/53], [94mLoss[0m : 2.41821
[1mStep[0m  [50/53], [94mLoss[0m : 2.57389

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.570, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.42399
[1mStep[0m  [5/53], [94mLoss[0m : 2.33805
[1mStep[0m  [10/53], [94mLoss[0m : 2.47290
[1mStep[0m  [15/53], [94mLoss[0m : 2.51717
[1mStep[0m  [20/53], [94mLoss[0m : 2.40844
[1mStep[0m  [25/53], [94mLoss[0m : 2.52062
[1mStep[0m  [30/53], [94mLoss[0m : 2.54365
[1mStep[0m  [35/53], [94mLoss[0m : 2.51877
[1mStep[0m  [40/53], [94mLoss[0m : 2.42472
[1mStep[0m  [45/53], [94mLoss[0m : 2.52212
[1mStep[0m  [50/53], [94mLoss[0m : 2.59570

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.591, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.52740
[1mStep[0m  [5/53], [94mLoss[0m : 2.25114
[1mStep[0m  [10/53], [94mLoss[0m : 2.54727
[1mStep[0m  [15/53], [94mLoss[0m : 2.47003
[1mStep[0m  [20/53], [94mLoss[0m : 2.55293
[1mStep[0m  [25/53], [94mLoss[0m : 2.39606
[1mStep[0m  [30/53], [94mLoss[0m : 2.58407
[1mStep[0m  [35/53], [94mLoss[0m : 2.56744
[1mStep[0m  [40/53], [94mLoss[0m : 2.58982
[1mStep[0m  [45/53], [94mLoss[0m : 2.53018
[1mStep[0m  [50/53], [94mLoss[0m : 2.64299

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.653, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.40065
[1mStep[0m  [5/53], [94mLoss[0m : 2.44443
[1mStep[0m  [10/53], [94mLoss[0m : 2.56037
[1mStep[0m  [15/53], [94mLoss[0m : 2.44594
[1mStep[0m  [20/53], [94mLoss[0m : 2.46403
[1mStep[0m  [25/53], [94mLoss[0m : 2.43131
[1mStep[0m  [30/53], [94mLoss[0m : 2.49553
[1mStep[0m  [35/53], [94mLoss[0m : 2.46047
[1mStep[0m  [40/53], [94mLoss[0m : 2.63011
[1mStep[0m  [45/53], [94mLoss[0m : 2.45541
[1mStep[0m  [50/53], [94mLoss[0m : 2.35304

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.618, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.37512
[1mStep[0m  [5/53], [94mLoss[0m : 2.43463
[1mStep[0m  [10/53], [94mLoss[0m : 2.43233
[1mStep[0m  [15/53], [94mLoss[0m : 2.41351
[1mStep[0m  [20/53], [94mLoss[0m : 2.51007
[1mStep[0m  [25/53], [94mLoss[0m : 2.26390
[1mStep[0m  [30/53], [94mLoss[0m : 2.28190
[1mStep[0m  [35/53], [94mLoss[0m : 2.23837
[1mStep[0m  [40/53], [94mLoss[0m : 2.61616
[1mStep[0m  [45/53], [94mLoss[0m : 2.23145
[1mStep[0m  [50/53], [94mLoss[0m : 2.51262

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.693, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.58372
[1mStep[0m  [5/53], [94mLoss[0m : 2.35268
[1mStep[0m  [10/53], [94mLoss[0m : 2.64067
[1mStep[0m  [15/53], [94mLoss[0m : 2.34040
[1mStep[0m  [20/53], [94mLoss[0m : 2.33820
[1mStep[0m  [25/53], [94mLoss[0m : 2.33118
[1mStep[0m  [30/53], [94mLoss[0m : 2.41781
[1mStep[0m  [35/53], [94mLoss[0m : 2.45237
[1mStep[0m  [40/53], [94mLoss[0m : 2.19766
[1mStep[0m  [45/53], [94mLoss[0m : 2.35448
[1mStep[0m  [50/53], [94mLoss[0m : 2.48848

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.709, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.34137
[1mStep[0m  [5/53], [94mLoss[0m : 2.46653
[1mStep[0m  [10/53], [94mLoss[0m : 2.32148
[1mStep[0m  [15/53], [94mLoss[0m : 2.43421
[1mStep[0m  [20/53], [94mLoss[0m : 2.46933
[1mStep[0m  [25/53], [94mLoss[0m : 2.42308
[1mStep[0m  [30/53], [94mLoss[0m : 2.43757
[1mStep[0m  [35/53], [94mLoss[0m : 2.34188
[1mStep[0m  [40/53], [94mLoss[0m : 2.47972
[1mStep[0m  [45/53], [94mLoss[0m : 2.25249
[1mStep[0m  [50/53], [94mLoss[0m : 2.53683

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.381, [92mTest[0m: 2.624, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.38996
[1mStep[0m  [5/53], [94mLoss[0m : 2.41472
[1mStep[0m  [10/53], [94mLoss[0m : 2.30762
[1mStep[0m  [15/53], [94mLoss[0m : 2.36601
[1mStep[0m  [20/53], [94mLoss[0m : 2.39905
[1mStep[0m  [25/53], [94mLoss[0m : 2.28392
[1mStep[0m  [30/53], [94mLoss[0m : 2.18673
[1mStep[0m  [35/53], [94mLoss[0m : 2.34993
[1mStep[0m  [40/53], [94mLoss[0m : 2.29096
[1mStep[0m  [45/53], [94mLoss[0m : 2.49368
[1mStep[0m  [50/53], [94mLoss[0m : 2.13678

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.354, [92mTest[0m: 2.560, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.14016
[1mStep[0m  [5/53], [94mLoss[0m : 2.45588
[1mStep[0m  [10/53], [94mLoss[0m : 2.33899
[1mStep[0m  [15/53], [94mLoss[0m : 2.47023
[1mStep[0m  [20/53], [94mLoss[0m : 2.60312
[1mStep[0m  [25/53], [94mLoss[0m : 2.34442
[1mStep[0m  [30/53], [94mLoss[0m : 2.17224
[1mStep[0m  [35/53], [94mLoss[0m : 2.27443
[1mStep[0m  [40/53], [94mLoss[0m : 2.42026
[1mStep[0m  [45/53], [94mLoss[0m : 2.23150
[1mStep[0m  [50/53], [94mLoss[0m : 2.39544

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.312, [92mTest[0m: 2.692, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.10837
[1mStep[0m  [5/53], [94mLoss[0m : 2.46057
[1mStep[0m  [10/53], [94mLoss[0m : 2.31764
[1mStep[0m  [15/53], [94mLoss[0m : 2.18005
[1mStep[0m  [20/53], [94mLoss[0m : 2.40286
[1mStep[0m  [25/53], [94mLoss[0m : 2.38169
[1mStep[0m  [30/53], [94mLoss[0m : 2.55991
[1mStep[0m  [35/53], [94mLoss[0m : 2.63906
[1mStep[0m  [40/53], [94mLoss[0m : 2.32505
[1mStep[0m  [45/53], [94mLoss[0m : 2.12936
[1mStep[0m  [50/53], [94mLoss[0m : 2.55482

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.310, [92mTest[0m: 2.595, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.34383
[1mStep[0m  [5/53], [94mLoss[0m : 2.14257
[1mStep[0m  [10/53], [94mLoss[0m : 2.64734
[1mStep[0m  [15/53], [94mLoss[0m : 2.31288
[1mStep[0m  [20/53], [94mLoss[0m : 2.07053
[1mStep[0m  [25/53], [94mLoss[0m : 2.16217
[1mStep[0m  [30/53], [94mLoss[0m : 2.42829
[1mStep[0m  [35/53], [94mLoss[0m : 2.10223
[1mStep[0m  [40/53], [94mLoss[0m : 2.37000
[1mStep[0m  [45/53], [94mLoss[0m : 2.18820
[1mStep[0m  [50/53], [94mLoss[0m : 2.14274

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.260, [92mTest[0m: 2.526, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.32685
[1mStep[0m  [5/53], [94mLoss[0m : 2.15173
[1mStep[0m  [10/53], [94mLoss[0m : 2.16018
[1mStep[0m  [15/53], [94mLoss[0m : 2.22927
[1mStep[0m  [20/53], [94mLoss[0m : 2.37297
[1mStep[0m  [25/53], [94mLoss[0m : 2.25035
[1mStep[0m  [30/53], [94mLoss[0m : 2.31477
[1mStep[0m  [35/53], [94mLoss[0m : 2.53994
[1mStep[0m  [40/53], [94mLoss[0m : 2.24495
[1mStep[0m  [45/53], [94mLoss[0m : 2.22006
[1mStep[0m  [50/53], [94mLoss[0m : 2.41343

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.256, [92mTest[0m: 2.566, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.27709
[1mStep[0m  [5/53], [94mLoss[0m : 2.05615
[1mStep[0m  [10/53], [94mLoss[0m : 2.10149
[1mStep[0m  [15/53], [94mLoss[0m : 2.16937
[1mStep[0m  [20/53], [94mLoss[0m : 2.02351
[1mStep[0m  [25/53], [94mLoss[0m : 2.34560
[1mStep[0m  [30/53], [94mLoss[0m : 2.19460
[1mStep[0m  [35/53], [94mLoss[0m : 2.31085
[1mStep[0m  [40/53], [94mLoss[0m : 2.25791
[1mStep[0m  [45/53], [94mLoss[0m : 2.20439
[1mStep[0m  [50/53], [94mLoss[0m : 2.17778

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.195, [92mTest[0m: 2.582, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.05651
[1mStep[0m  [5/53], [94mLoss[0m : 2.08749
[1mStep[0m  [10/53], [94mLoss[0m : 2.30258
[1mStep[0m  [15/53], [94mLoss[0m : 2.14257
[1mStep[0m  [20/53], [94mLoss[0m : 2.32067
[1mStep[0m  [25/53], [94mLoss[0m : 2.11492
[1mStep[0m  [30/53], [94mLoss[0m : 2.33469
[1mStep[0m  [35/53], [94mLoss[0m : 2.30632
[1mStep[0m  [40/53], [94mLoss[0m : 1.94163
[1mStep[0m  [45/53], [94mLoss[0m : 2.08171
[1mStep[0m  [50/53], [94mLoss[0m : 2.05179

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.195, [92mTest[0m: 2.710, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.10946
[1mStep[0m  [5/53], [94mLoss[0m : 2.33949
[1mStep[0m  [10/53], [94mLoss[0m : 2.16857
[1mStep[0m  [15/53], [94mLoss[0m : 2.13915
[1mStep[0m  [20/53], [94mLoss[0m : 2.11758
[1mStep[0m  [25/53], [94mLoss[0m : 2.42567
[1mStep[0m  [30/53], [94mLoss[0m : 2.12843
[1mStep[0m  [35/53], [94mLoss[0m : 2.46667
[1mStep[0m  [40/53], [94mLoss[0m : 2.21735
[1mStep[0m  [45/53], [94mLoss[0m : 2.18579
[1mStep[0m  [50/53], [94mLoss[0m : 2.31909

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.172, [92mTest[0m: 2.659, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.09180
[1mStep[0m  [5/53], [94mLoss[0m : 1.89296
[1mStep[0m  [10/53], [94mLoss[0m : 2.13294
[1mStep[0m  [15/53], [94mLoss[0m : 2.30569
[1mStep[0m  [20/53], [94mLoss[0m : 2.10584
[1mStep[0m  [25/53], [94mLoss[0m : 1.78457
[1mStep[0m  [30/53], [94mLoss[0m : 2.22258
[1mStep[0m  [35/53], [94mLoss[0m : 2.37876
[1mStep[0m  [40/53], [94mLoss[0m : 2.28337
[1mStep[0m  [45/53], [94mLoss[0m : 2.26400
[1mStep[0m  [50/53], [94mLoss[0m : 1.98733

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.136, [92mTest[0m: 2.639, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.97590
[1mStep[0m  [5/53], [94mLoss[0m : 2.02485
[1mStep[0m  [10/53], [94mLoss[0m : 2.12219
[1mStep[0m  [15/53], [94mLoss[0m : 1.91123
[1mStep[0m  [20/53], [94mLoss[0m : 2.18119
[1mStep[0m  [25/53], [94mLoss[0m : 2.11320
[1mStep[0m  [30/53], [94mLoss[0m : 2.11129
[1mStep[0m  [35/53], [94mLoss[0m : 2.00260
[1mStep[0m  [40/53], [94mLoss[0m : 2.09627
[1mStep[0m  [45/53], [94mLoss[0m : 2.22351
[1mStep[0m  [50/53], [94mLoss[0m : 2.23141

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.101, [92mTest[0m: 2.585, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.08259
[1mStep[0m  [5/53], [94mLoss[0m : 2.08912
[1mStep[0m  [10/53], [94mLoss[0m : 1.99768
[1mStep[0m  [15/53], [94mLoss[0m : 1.96584
[1mStep[0m  [20/53], [94mLoss[0m : 2.23884
[1mStep[0m  [25/53], [94mLoss[0m : 2.18646
[1mStep[0m  [30/53], [94mLoss[0m : 2.20086
[1mStep[0m  [35/53], [94mLoss[0m : 2.19706
[1mStep[0m  [40/53], [94mLoss[0m : 2.08750
[1mStep[0m  [45/53], [94mLoss[0m : 1.98862
[1mStep[0m  [50/53], [94mLoss[0m : 1.97473

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.075, [92mTest[0m: 2.630, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.96944
[1mStep[0m  [5/53], [94mLoss[0m : 1.99694
[1mStep[0m  [10/53], [94mLoss[0m : 1.96300
[1mStep[0m  [15/53], [94mLoss[0m : 2.21294
[1mStep[0m  [20/53], [94mLoss[0m : 1.93114
[1mStep[0m  [25/53], [94mLoss[0m : 1.94722
[1mStep[0m  [30/53], [94mLoss[0m : 2.13469
[1mStep[0m  [35/53], [94mLoss[0m : 1.99569
[1mStep[0m  [40/53], [94mLoss[0m : 1.94003
[1mStep[0m  [45/53], [94mLoss[0m : 1.99477
[1mStep[0m  [50/53], [94mLoss[0m : 2.06839

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.056, [92mTest[0m: 2.554, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.79384
[1mStep[0m  [5/53], [94mLoss[0m : 1.96278
[1mStep[0m  [10/53], [94mLoss[0m : 2.03456
[1mStep[0m  [15/53], [94mLoss[0m : 2.00995
[1mStep[0m  [20/53], [94mLoss[0m : 2.07329
[1mStep[0m  [25/53], [94mLoss[0m : 1.97205
[1mStep[0m  [30/53], [94mLoss[0m : 2.09765
[1mStep[0m  [35/53], [94mLoss[0m : 1.80285
[1mStep[0m  [40/53], [94mLoss[0m : 1.99356
[1mStep[0m  [45/53], [94mLoss[0m : 1.73800
[1mStep[0m  [50/53], [94mLoss[0m : 2.14621

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.024, [92mTest[0m: 2.548, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.96264
[1mStep[0m  [5/53], [94mLoss[0m : 1.84833
[1mStep[0m  [10/53], [94mLoss[0m : 2.08249
[1mStep[0m  [15/53], [94mLoss[0m : 2.09149
[1mStep[0m  [20/53], [94mLoss[0m : 1.90752
[1mStep[0m  [25/53], [94mLoss[0m : 2.21775
[1mStep[0m  [30/53], [94mLoss[0m : 1.91557
[1mStep[0m  [35/53], [94mLoss[0m : 1.93634
[1mStep[0m  [40/53], [94mLoss[0m : 1.84435
[1mStep[0m  [45/53], [94mLoss[0m : 1.91703
[1mStep[0m  [50/53], [94mLoss[0m : 2.07532

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.005, [92mTest[0m: 2.580, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.90998
[1mStep[0m  [5/53], [94mLoss[0m : 1.80265
[1mStep[0m  [10/53], [94mLoss[0m : 2.01425
[1mStep[0m  [15/53], [94mLoss[0m : 2.06616
[1mStep[0m  [20/53], [94mLoss[0m : 2.09456
[1mStep[0m  [25/53], [94mLoss[0m : 2.05201
[1mStep[0m  [30/53], [94mLoss[0m : 1.97479
[1mStep[0m  [35/53], [94mLoss[0m : 2.01191
[1mStep[0m  [40/53], [94mLoss[0m : 1.92569
[1mStep[0m  [45/53], [94mLoss[0m : 1.99385
[1mStep[0m  [50/53], [94mLoss[0m : 1.91915

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.984, [92mTest[0m: 2.772, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.94012
[1mStep[0m  [5/53], [94mLoss[0m : 1.81236
[1mStep[0m  [10/53], [94mLoss[0m : 1.91218
[1mStep[0m  [15/53], [94mLoss[0m : 2.11524
[1mStep[0m  [20/53], [94mLoss[0m : 1.93183
[1mStep[0m  [25/53], [94mLoss[0m : 1.83693
[1mStep[0m  [30/53], [94mLoss[0m : 2.00084
[1mStep[0m  [35/53], [94mLoss[0m : 1.76354
[1mStep[0m  [40/53], [94mLoss[0m : 1.93503
[1mStep[0m  [45/53], [94mLoss[0m : 1.99940
[1mStep[0m  [50/53], [94mLoss[0m : 1.93198

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.940, [92mTest[0m: 2.594, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.02657
[1mStep[0m  [5/53], [94mLoss[0m : 1.87182
[1mStep[0m  [10/53], [94mLoss[0m : 1.87532
[1mStep[0m  [15/53], [94mLoss[0m : 1.94913
[1mStep[0m  [20/53], [94mLoss[0m : 1.82815
[1mStep[0m  [25/53], [94mLoss[0m : 1.95090
[1mStep[0m  [30/53], [94mLoss[0m : 1.89918
[1mStep[0m  [35/53], [94mLoss[0m : 1.94078
[1mStep[0m  [40/53], [94mLoss[0m : 1.95172
[1mStep[0m  [45/53], [94mLoss[0m : 1.87921
[1mStep[0m  [50/53], [94mLoss[0m : 2.03045

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.935, [92mTest[0m: 2.672, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.79703
[1mStep[0m  [5/53], [94mLoss[0m : 1.90286
[1mStep[0m  [10/53], [94mLoss[0m : 1.82230
[1mStep[0m  [15/53], [94mLoss[0m : 1.81274
[1mStep[0m  [20/53], [94mLoss[0m : 1.97684
[1mStep[0m  [25/53], [94mLoss[0m : 1.96973
[1mStep[0m  [30/53], [94mLoss[0m : 1.76354
[1mStep[0m  [35/53], [94mLoss[0m : 2.02595
[1mStep[0m  [40/53], [94mLoss[0m : 1.80247
[1mStep[0m  [45/53], [94mLoss[0m : 2.02107
[1mStep[0m  [50/53], [94mLoss[0m : 2.04527

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.919, [92mTest[0m: 2.692, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.79805
[1mStep[0m  [5/53], [94mLoss[0m : 1.88732
[1mStep[0m  [10/53], [94mLoss[0m : 1.96599
[1mStep[0m  [15/53], [94mLoss[0m : 1.96026
[1mStep[0m  [20/53], [94mLoss[0m : 1.86629
[1mStep[0m  [25/53], [94mLoss[0m : 1.82064
[1mStep[0m  [30/53], [94mLoss[0m : 1.81058
[1mStep[0m  [35/53], [94mLoss[0m : 1.93815
[1mStep[0m  [40/53], [94mLoss[0m : 1.86381
[1mStep[0m  [45/53], [94mLoss[0m : 1.73677
[1mStep[0m  [50/53], [94mLoss[0m : 2.00507

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.885, [92mTest[0m: 2.570, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.86458
[1mStep[0m  [5/53], [94mLoss[0m : 1.97387
[1mStep[0m  [10/53], [94mLoss[0m : 2.06207
[1mStep[0m  [15/53], [94mLoss[0m : 1.88552
[1mStep[0m  [20/53], [94mLoss[0m : 1.91975
[1mStep[0m  [25/53], [94mLoss[0m : 1.91155
[1mStep[0m  [30/53], [94mLoss[0m : 1.99533
[1mStep[0m  [35/53], [94mLoss[0m : 2.04520
[1mStep[0m  [40/53], [94mLoss[0m : 1.79557
[1mStep[0m  [45/53], [94mLoss[0m : 1.95971
[1mStep[0m  [50/53], [94mLoss[0m : 1.94321

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.872, [92mTest[0m: 2.535, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.76221
[1mStep[0m  [5/53], [94mLoss[0m : 1.80887
[1mStep[0m  [10/53], [94mLoss[0m : 1.93688
[1mStep[0m  [15/53], [94mLoss[0m : 2.01340
[1mStep[0m  [20/53], [94mLoss[0m : 1.96870
[1mStep[0m  [25/53], [94mLoss[0m : 1.94040
[1mStep[0m  [30/53], [94mLoss[0m : 1.90826
[1mStep[0m  [35/53], [94mLoss[0m : 1.67519
[1mStep[0m  [40/53], [94mLoss[0m : 1.84551
[1mStep[0m  [45/53], [94mLoss[0m : 1.83875
[1mStep[0m  [50/53], [94mLoss[0m : 1.85851

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.851, [92mTest[0m: 2.556, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.74488
[1mStep[0m  [5/53], [94mLoss[0m : 1.78666
[1mStep[0m  [10/53], [94mLoss[0m : 1.73229
[1mStep[0m  [15/53], [94mLoss[0m : 1.96410
[1mStep[0m  [20/53], [94mLoss[0m : 1.87552
[1mStep[0m  [25/53], [94mLoss[0m : 1.87227
[1mStep[0m  [30/53], [94mLoss[0m : 1.70875
[1mStep[0m  [35/53], [94mLoss[0m : 1.85024
[1mStep[0m  [40/53], [94mLoss[0m : 1.70162
[1mStep[0m  [45/53], [94mLoss[0m : 1.84577
[1mStep[0m  [50/53], [94mLoss[0m : 1.90527

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.830, [92mTest[0m: 2.662, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.548
====================================

Phase 2 - Evaluation MAE:  2.548172483077416
MAE score P1       2.43834
MAE score P2      2.548172
loss                1.8298
learning_rate     0.002575
batch_size             256
hidden_sizes         [250]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.5
weight_decay        0.0001
Name: 9, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/26], [94mLoss[0m : 10.65949
[1mStep[0m  [2/26], [94mLoss[0m : 10.05769
[1mStep[0m  [4/26], [94mLoss[0m : 8.72120
[1mStep[0m  [6/26], [94mLoss[0m : 7.43025
[1mStep[0m  [8/26], [94mLoss[0m : 4.99461
[1mStep[0m  [10/26], [94mLoss[0m : 3.31230
[1mStep[0m  [12/26], [94mLoss[0m : 2.86117
[1mStep[0m  [14/26], [94mLoss[0m : 3.16086
[1mStep[0m  [16/26], [94mLoss[0m : 3.70066
[1mStep[0m  [18/26], [94mLoss[0m : 4.16110
[1mStep[0m  [20/26], [94mLoss[0m : 4.00838
[1mStep[0m  [22/26], [94mLoss[0m : 3.66900
[1mStep[0m  [24/26], [94mLoss[0m : 3.46436

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 5.259, [92mTest[0m: 10.666, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.83613
[1mStep[0m  [2/26], [94mLoss[0m : 2.68364
[1mStep[0m  [4/26], [94mLoss[0m : 2.65159
[1mStep[0m  [6/26], [94mLoss[0m : 2.68573
[1mStep[0m  [8/26], [94mLoss[0m : 2.78010
[1mStep[0m  [10/26], [94mLoss[0m : 2.75383
[1mStep[0m  [12/26], [94mLoss[0m : 3.10330
[1mStep[0m  [14/26], [94mLoss[0m : 2.42240
[1mStep[0m  [16/26], [94mLoss[0m : 2.46608
[1mStep[0m  [18/26], [94mLoss[0m : 2.48661
[1mStep[0m  [20/26], [94mLoss[0m : 2.60656
[1mStep[0m  [22/26], [94mLoss[0m : 2.72387
[1mStep[0m  [24/26], [94mLoss[0m : 2.57995

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.657, [92mTest[0m: 2.890, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.46971
[1mStep[0m  [2/26], [94mLoss[0m : 2.31598
[1mStep[0m  [4/26], [94mLoss[0m : 2.55427
[1mStep[0m  [6/26], [94mLoss[0m : 2.58401
[1mStep[0m  [8/26], [94mLoss[0m : 2.42808
[1mStep[0m  [10/26], [94mLoss[0m : 2.58795
[1mStep[0m  [12/26], [94mLoss[0m : 2.42220
[1mStep[0m  [14/26], [94mLoss[0m : 2.58378
[1mStep[0m  [16/26], [94mLoss[0m : 2.74012
[1mStep[0m  [18/26], [94mLoss[0m : 2.52006
[1mStep[0m  [20/26], [94mLoss[0m : 2.54328
[1mStep[0m  [22/26], [94mLoss[0m : 2.55520
[1mStep[0m  [24/26], [94mLoss[0m : 2.51838

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.598, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.56132
[1mStep[0m  [2/26], [94mLoss[0m : 2.55437
[1mStep[0m  [4/26], [94mLoss[0m : 2.39665
[1mStep[0m  [6/26], [94mLoss[0m : 2.41688
[1mStep[0m  [8/26], [94mLoss[0m : 2.56549
[1mStep[0m  [10/26], [94mLoss[0m : 2.41751
[1mStep[0m  [12/26], [94mLoss[0m : 2.37194
[1mStep[0m  [14/26], [94mLoss[0m : 2.37796
[1mStep[0m  [16/26], [94mLoss[0m : 2.59808
[1mStep[0m  [18/26], [94mLoss[0m : 2.49726
[1mStep[0m  [20/26], [94mLoss[0m : 2.31847
[1mStep[0m  [22/26], [94mLoss[0m : 2.48759
[1mStep[0m  [24/26], [94mLoss[0m : 2.56130

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.500, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.36516
[1mStep[0m  [2/26], [94mLoss[0m : 2.45090
[1mStep[0m  [4/26], [94mLoss[0m : 2.41586
[1mStep[0m  [6/26], [94mLoss[0m : 2.41515
[1mStep[0m  [8/26], [94mLoss[0m : 2.55779
[1mStep[0m  [10/26], [94mLoss[0m : 2.51036
[1mStep[0m  [12/26], [94mLoss[0m : 2.50674
[1mStep[0m  [14/26], [94mLoss[0m : 2.41418
[1mStep[0m  [16/26], [94mLoss[0m : 2.46999
[1mStep[0m  [18/26], [94mLoss[0m : 2.39084
[1mStep[0m  [20/26], [94mLoss[0m : 2.59614
[1mStep[0m  [22/26], [94mLoss[0m : 2.45567
[1mStep[0m  [24/26], [94mLoss[0m : 2.31357

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.468, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.44648
[1mStep[0m  [2/26], [94mLoss[0m : 2.57721
[1mStep[0m  [4/26], [94mLoss[0m : 2.43625
[1mStep[0m  [6/26], [94mLoss[0m : 2.50321
[1mStep[0m  [8/26], [94mLoss[0m : 2.55866
[1mStep[0m  [10/26], [94mLoss[0m : 2.45963
[1mStep[0m  [12/26], [94mLoss[0m : 2.58079
[1mStep[0m  [14/26], [94mLoss[0m : 2.45485
[1mStep[0m  [16/26], [94mLoss[0m : 2.58898
[1mStep[0m  [18/26], [94mLoss[0m : 2.41283
[1mStep[0m  [20/26], [94mLoss[0m : 2.48120
[1mStep[0m  [22/26], [94mLoss[0m : 2.43066
[1mStep[0m  [24/26], [94mLoss[0m : 2.45770

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.465, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.40505
[1mStep[0m  [2/26], [94mLoss[0m : 2.28258
[1mStep[0m  [4/26], [94mLoss[0m : 2.53932
[1mStep[0m  [6/26], [94mLoss[0m : 2.58031
[1mStep[0m  [8/26], [94mLoss[0m : 2.39568
[1mStep[0m  [10/26], [94mLoss[0m : 2.54324
[1mStep[0m  [12/26], [94mLoss[0m : 2.65854
[1mStep[0m  [14/26], [94mLoss[0m : 2.59991
[1mStep[0m  [16/26], [94mLoss[0m : 2.48836
[1mStep[0m  [18/26], [94mLoss[0m : 2.55627
[1mStep[0m  [20/26], [94mLoss[0m : 2.37654
[1mStep[0m  [22/26], [94mLoss[0m : 2.45666
[1mStep[0m  [24/26], [94mLoss[0m : 2.28132

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.440, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.44942
[1mStep[0m  [2/26], [94mLoss[0m : 2.60410
[1mStep[0m  [4/26], [94mLoss[0m : 2.53432
[1mStep[0m  [6/26], [94mLoss[0m : 2.51226
[1mStep[0m  [8/26], [94mLoss[0m : 2.61261
[1mStep[0m  [10/26], [94mLoss[0m : 2.40684
[1mStep[0m  [12/26], [94mLoss[0m : 2.39416
[1mStep[0m  [14/26], [94mLoss[0m : 2.36027
[1mStep[0m  [16/26], [94mLoss[0m : 2.48742
[1mStep[0m  [18/26], [94mLoss[0m : 2.55278
[1mStep[0m  [20/26], [94mLoss[0m : 2.54635
[1mStep[0m  [22/26], [94mLoss[0m : 2.42547
[1mStep[0m  [24/26], [94mLoss[0m : 2.27799

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.431, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.45923
[1mStep[0m  [2/26], [94mLoss[0m : 2.53538
[1mStep[0m  [4/26], [94mLoss[0m : 2.48257
[1mStep[0m  [6/26], [94mLoss[0m : 2.53746
[1mStep[0m  [8/26], [94mLoss[0m : 2.34804
[1mStep[0m  [10/26], [94mLoss[0m : 2.52080
[1mStep[0m  [12/26], [94mLoss[0m : 2.44890
[1mStep[0m  [14/26], [94mLoss[0m : 2.46764
[1mStep[0m  [16/26], [94mLoss[0m : 2.35929
[1mStep[0m  [18/26], [94mLoss[0m : 2.49745
[1mStep[0m  [20/26], [94mLoss[0m : 2.39287
[1mStep[0m  [22/26], [94mLoss[0m : 2.58165
[1mStep[0m  [24/26], [94mLoss[0m : 2.33495

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.429, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.39596
[1mStep[0m  [2/26], [94mLoss[0m : 2.57579
[1mStep[0m  [4/26], [94mLoss[0m : 2.45662
[1mStep[0m  [6/26], [94mLoss[0m : 2.42053
[1mStep[0m  [8/26], [94mLoss[0m : 2.38405
[1mStep[0m  [10/26], [94mLoss[0m : 2.49095
[1mStep[0m  [12/26], [94mLoss[0m : 2.47262
[1mStep[0m  [14/26], [94mLoss[0m : 2.57530
[1mStep[0m  [16/26], [94mLoss[0m : 2.40088
[1mStep[0m  [18/26], [94mLoss[0m : 2.52917
[1mStep[0m  [20/26], [94mLoss[0m : 2.44329
[1mStep[0m  [22/26], [94mLoss[0m : 2.39427
[1mStep[0m  [24/26], [94mLoss[0m : 2.48609

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.443, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.34724
[1mStep[0m  [2/26], [94mLoss[0m : 2.48812
[1mStep[0m  [4/26], [94mLoss[0m : 2.76417
[1mStep[0m  [6/26], [94mLoss[0m : 2.37723
[1mStep[0m  [8/26], [94mLoss[0m : 2.66498
[1mStep[0m  [10/26], [94mLoss[0m : 2.46404
[1mStep[0m  [12/26], [94mLoss[0m : 2.46962
[1mStep[0m  [14/26], [94mLoss[0m : 2.48823
[1mStep[0m  [16/26], [94mLoss[0m : 2.44530
[1mStep[0m  [18/26], [94mLoss[0m : 2.57112
[1mStep[0m  [20/26], [94mLoss[0m : 2.53623
[1mStep[0m  [22/26], [94mLoss[0m : 2.27690
[1mStep[0m  [24/26], [94mLoss[0m : 2.39250

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.435, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.43301
[1mStep[0m  [2/26], [94mLoss[0m : 2.45553
[1mStep[0m  [4/26], [94mLoss[0m : 2.53519
[1mStep[0m  [6/26], [94mLoss[0m : 2.28796
[1mStep[0m  [8/26], [94mLoss[0m : 2.43563
[1mStep[0m  [10/26], [94mLoss[0m : 2.40554
[1mStep[0m  [12/26], [94mLoss[0m : 2.43810
[1mStep[0m  [14/26], [94mLoss[0m : 2.42803
[1mStep[0m  [16/26], [94mLoss[0m : 2.49465
[1mStep[0m  [18/26], [94mLoss[0m : 2.40534
[1mStep[0m  [20/26], [94mLoss[0m : 2.45935
[1mStep[0m  [22/26], [94mLoss[0m : 2.38197
[1mStep[0m  [24/26], [94mLoss[0m : 2.60792

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.430, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.63878
[1mStep[0m  [2/26], [94mLoss[0m : 2.41174
[1mStep[0m  [4/26], [94mLoss[0m : 2.37765
[1mStep[0m  [6/26], [94mLoss[0m : 2.47799
[1mStep[0m  [8/26], [94mLoss[0m : 2.31668
[1mStep[0m  [10/26], [94mLoss[0m : 2.52586
[1mStep[0m  [12/26], [94mLoss[0m : 2.45823
[1mStep[0m  [14/26], [94mLoss[0m : 2.33191
[1mStep[0m  [16/26], [94mLoss[0m : 2.45494
[1mStep[0m  [18/26], [94mLoss[0m : 2.51055
[1mStep[0m  [20/26], [94mLoss[0m : 2.45477
[1mStep[0m  [22/26], [94mLoss[0m : 2.58123
[1mStep[0m  [24/26], [94mLoss[0m : 2.44516

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.425, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.48005
[1mStep[0m  [2/26], [94mLoss[0m : 2.42000
[1mStep[0m  [4/26], [94mLoss[0m : 2.45461
[1mStep[0m  [6/26], [94mLoss[0m : 2.41750
[1mStep[0m  [8/26], [94mLoss[0m : 2.34937
[1mStep[0m  [10/26], [94mLoss[0m : 2.38271
[1mStep[0m  [12/26], [94mLoss[0m : 2.46282
[1mStep[0m  [14/26], [94mLoss[0m : 2.46218
[1mStep[0m  [16/26], [94mLoss[0m : 2.45366
[1mStep[0m  [18/26], [94mLoss[0m : 2.54930
[1mStep[0m  [20/26], [94mLoss[0m : 2.43791
[1mStep[0m  [22/26], [94mLoss[0m : 2.61883
[1mStep[0m  [24/26], [94mLoss[0m : 2.53675

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.421, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.43101
[1mStep[0m  [2/26], [94mLoss[0m : 2.33726
[1mStep[0m  [4/26], [94mLoss[0m : 2.41687
[1mStep[0m  [6/26], [94mLoss[0m : 2.55324
[1mStep[0m  [8/26], [94mLoss[0m : 2.47428
[1mStep[0m  [10/26], [94mLoss[0m : 2.51808
[1mStep[0m  [12/26], [94mLoss[0m : 2.41453
[1mStep[0m  [14/26], [94mLoss[0m : 2.39232
[1mStep[0m  [16/26], [94mLoss[0m : 2.35925
[1mStep[0m  [18/26], [94mLoss[0m : 2.47558
[1mStep[0m  [20/26], [94mLoss[0m : 2.37166
[1mStep[0m  [22/26], [94mLoss[0m : 2.33388
[1mStep[0m  [24/26], [94mLoss[0m : 2.55358

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.419, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.48139
[1mStep[0m  [2/26], [94mLoss[0m : 2.36490
[1mStep[0m  [4/26], [94mLoss[0m : 2.31075
[1mStep[0m  [6/26], [94mLoss[0m : 2.35147
[1mStep[0m  [8/26], [94mLoss[0m : 2.36917
[1mStep[0m  [10/26], [94mLoss[0m : 2.44184
[1mStep[0m  [12/26], [94mLoss[0m : 2.50007
[1mStep[0m  [14/26], [94mLoss[0m : 2.40711
[1mStep[0m  [16/26], [94mLoss[0m : 2.52652
[1mStep[0m  [18/26], [94mLoss[0m : 2.42160
[1mStep[0m  [20/26], [94mLoss[0m : 2.38251
[1mStep[0m  [22/26], [94mLoss[0m : 2.42123
[1mStep[0m  [24/26], [94mLoss[0m : 2.33210

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.414, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.43279
[1mStep[0m  [2/26], [94mLoss[0m : 2.46694
[1mStep[0m  [4/26], [94mLoss[0m : 2.56721
[1mStep[0m  [6/26], [94mLoss[0m : 2.50706
[1mStep[0m  [8/26], [94mLoss[0m : 2.41877
[1mStep[0m  [10/26], [94mLoss[0m : 2.35495
[1mStep[0m  [12/26], [94mLoss[0m : 2.37977
[1mStep[0m  [14/26], [94mLoss[0m : 2.45762
[1mStep[0m  [16/26], [94mLoss[0m : 2.44764
[1mStep[0m  [18/26], [94mLoss[0m : 2.53933
[1mStep[0m  [20/26], [94mLoss[0m : 2.37125
[1mStep[0m  [22/26], [94mLoss[0m : 2.36097
[1mStep[0m  [24/26], [94mLoss[0m : 2.43280

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.419, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.47603
[1mStep[0m  [2/26], [94mLoss[0m : 2.44281
[1mStep[0m  [4/26], [94mLoss[0m : 2.49942
[1mStep[0m  [6/26], [94mLoss[0m : 2.64453
[1mStep[0m  [8/26], [94mLoss[0m : 2.28079
[1mStep[0m  [10/26], [94mLoss[0m : 2.51690
[1mStep[0m  [12/26], [94mLoss[0m : 2.42030
[1mStep[0m  [14/26], [94mLoss[0m : 2.37436
[1mStep[0m  [16/26], [94mLoss[0m : 2.40736
[1mStep[0m  [18/26], [94mLoss[0m : 2.50128
[1mStep[0m  [20/26], [94mLoss[0m : 2.45710
[1mStep[0m  [22/26], [94mLoss[0m : 2.35486
[1mStep[0m  [24/26], [94mLoss[0m : 2.39314

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.417, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.40685
[1mStep[0m  [2/26], [94mLoss[0m : 2.45624
[1mStep[0m  [4/26], [94mLoss[0m : 2.48339
[1mStep[0m  [6/26], [94mLoss[0m : 2.30253
[1mStep[0m  [8/26], [94mLoss[0m : 2.42877
[1mStep[0m  [10/26], [94mLoss[0m : 2.35149
[1mStep[0m  [12/26], [94mLoss[0m : 2.41988
[1mStep[0m  [14/26], [94mLoss[0m : 2.39578
[1mStep[0m  [16/26], [94mLoss[0m : 2.57953
[1mStep[0m  [18/26], [94mLoss[0m : 2.52488
[1mStep[0m  [20/26], [94mLoss[0m : 2.32970
[1mStep[0m  [22/26], [94mLoss[0m : 2.35393
[1mStep[0m  [24/26], [94mLoss[0m : 2.50258

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.419, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.34583
[1mStep[0m  [2/26], [94mLoss[0m : 2.50531
[1mStep[0m  [4/26], [94mLoss[0m : 2.50148
[1mStep[0m  [6/26], [94mLoss[0m : 2.55001
[1mStep[0m  [8/26], [94mLoss[0m : 2.55880
[1mStep[0m  [10/26], [94mLoss[0m : 2.45745
[1mStep[0m  [12/26], [94mLoss[0m : 2.62281
[1mStep[0m  [14/26], [94mLoss[0m : 2.42153
[1mStep[0m  [16/26], [94mLoss[0m : 2.40356
[1mStep[0m  [18/26], [94mLoss[0m : 2.41908
[1mStep[0m  [20/26], [94mLoss[0m : 2.37995
[1mStep[0m  [22/26], [94mLoss[0m : 2.52683
[1mStep[0m  [24/26], [94mLoss[0m : 2.46427

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.417, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.53740
[1mStep[0m  [2/26], [94mLoss[0m : 2.50267
[1mStep[0m  [4/26], [94mLoss[0m : 2.41371
[1mStep[0m  [6/26], [94mLoss[0m : 2.34310
[1mStep[0m  [8/26], [94mLoss[0m : 2.47580
[1mStep[0m  [10/26], [94mLoss[0m : 2.46600
[1mStep[0m  [12/26], [94mLoss[0m : 2.36312
[1mStep[0m  [14/26], [94mLoss[0m : 2.52104
[1mStep[0m  [16/26], [94mLoss[0m : 2.39118
[1mStep[0m  [18/26], [94mLoss[0m : 2.47829
[1mStep[0m  [20/26], [94mLoss[0m : 2.28327
[1mStep[0m  [22/26], [94mLoss[0m : 2.44217
[1mStep[0m  [24/26], [94mLoss[0m : 2.42305

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.414, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.43106
[1mStep[0m  [2/26], [94mLoss[0m : 2.32120
[1mStep[0m  [4/26], [94mLoss[0m : 2.51178
[1mStep[0m  [6/26], [94mLoss[0m : 2.43621
[1mStep[0m  [8/26], [94mLoss[0m : 2.50137
[1mStep[0m  [10/26], [94mLoss[0m : 2.42905
[1mStep[0m  [12/26], [94mLoss[0m : 2.23014
[1mStep[0m  [14/26], [94mLoss[0m : 2.41501
[1mStep[0m  [16/26], [94mLoss[0m : 2.36367
[1mStep[0m  [18/26], [94mLoss[0m : 2.47157
[1mStep[0m  [20/26], [94mLoss[0m : 2.50249
[1mStep[0m  [22/26], [94mLoss[0m : 2.48376
[1mStep[0m  [24/26], [94mLoss[0m : 2.51163

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.412, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.53944
[1mStep[0m  [2/26], [94mLoss[0m : 2.55104
[1mStep[0m  [4/26], [94mLoss[0m : 2.46095
[1mStep[0m  [6/26], [94mLoss[0m : 2.44909
[1mStep[0m  [8/26], [94mLoss[0m : 2.39909
[1mStep[0m  [10/26], [94mLoss[0m : 2.37488
[1mStep[0m  [12/26], [94mLoss[0m : 2.42897
[1mStep[0m  [14/26], [94mLoss[0m : 2.38985
[1mStep[0m  [16/26], [94mLoss[0m : 2.53979
[1mStep[0m  [18/26], [94mLoss[0m : 2.42886
[1mStep[0m  [20/26], [94mLoss[0m : 2.42732
[1mStep[0m  [22/26], [94mLoss[0m : 2.26666
[1mStep[0m  [24/26], [94mLoss[0m : 2.31043

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.414, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.21449
[1mStep[0m  [2/26], [94mLoss[0m : 2.46569
[1mStep[0m  [4/26], [94mLoss[0m : 2.43730
[1mStep[0m  [6/26], [94mLoss[0m : 2.29689
[1mStep[0m  [8/26], [94mLoss[0m : 2.51944
[1mStep[0m  [10/26], [94mLoss[0m : 2.49955
[1mStep[0m  [12/26], [94mLoss[0m : 2.44996
[1mStep[0m  [14/26], [94mLoss[0m : 2.58896
[1mStep[0m  [16/26], [94mLoss[0m : 2.42723
[1mStep[0m  [18/26], [94mLoss[0m : 2.52554
[1mStep[0m  [20/26], [94mLoss[0m : 2.35175
[1mStep[0m  [22/26], [94mLoss[0m : 2.33990
[1mStep[0m  [24/26], [94mLoss[0m : 2.32663

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.415, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.34096
[1mStep[0m  [2/26], [94mLoss[0m : 2.50471
[1mStep[0m  [4/26], [94mLoss[0m : 2.34283
[1mStep[0m  [6/26], [94mLoss[0m : 2.49205
[1mStep[0m  [8/26], [94mLoss[0m : 2.59760
[1mStep[0m  [10/26], [94mLoss[0m : 2.42291
[1mStep[0m  [12/26], [94mLoss[0m : 2.54531
[1mStep[0m  [14/26], [94mLoss[0m : 2.38856
[1mStep[0m  [16/26], [94mLoss[0m : 2.40297
[1mStep[0m  [18/26], [94mLoss[0m : 2.35786
[1mStep[0m  [20/26], [94mLoss[0m : 2.53003
[1mStep[0m  [22/26], [94mLoss[0m : 2.38781
[1mStep[0m  [24/26], [94mLoss[0m : 2.34594

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.408, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.50515
[1mStep[0m  [2/26], [94mLoss[0m : 2.27916
[1mStep[0m  [4/26], [94mLoss[0m : 2.46736
[1mStep[0m  [6/26], [94mLoss[0m : 2.56379
[1mStep[0m  [8/26], [94mLoss[0m : 2.49248
[1mStep[0m  [10/26], [94mLoss[0m : 2.39070
[1mStep[0m  [12/26], [94mLoss[0m : 2.56803
[1mStep[0m  [14/26], [94mLoss[0m : 2.35349
[1mStep[0m  [16/26], [94mLoss[0m : 2.34259
[1mStep[0m  [18/26], [94mLoss[0m : 2.45911
[1mStep[0m  [20/26], [94mLoss[0m : 2.42423
[1mStep[0m  [22/26], [94mLoss[0m : 2.37593
[1mStep[0m  [24/26], [94mLoss[0m : 2.39721

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.412, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.55830
[1mStep[0m  [2/26], [94mLoss[0m : 2.36999
[1mStep[0m  [4/26], [94mLoss[0m : 2.49837
[1mStep[0m  [6/26], [94mLoss[0m : 2.34549
[1mStep[0m  [8/26], [94mLoss[0m : 2.47853
[1mStep[0m  [10/26], [94mLoss[0m : 2.38231
[1mStep[0m  [12/26], [94mLoss[0m : 2.27966
[1mStep[0m  [14/26], [94mLoss[0m : 2.45616
[1mStep[0m  [16/26], [94mLoss[0m : 2.34089
[1mStep[0m  [18/26], [94mLoss[0m : 2.47406
[1mStep[0m  [20/26], [94mLoss[0m : 2.27354
[1mStep[0m  [22/26], [94mLoss[0m : 2.42068
[1mStep[0m  [24/26], [94mLoss[0m : 2.42057

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.411, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.42785
[1mStep[0m  [2/26], [94mLoss[0m : 2.48895
[1mStep[0m  [4/26], [94mLoss[0m : 2.48002
[1mStep[0m  [6/26], [94mLoss[0m : 2.41310
[1mStep[0m  [8/26], [94mLoss[0m : 2.37476
[1mStep[0m  [10/26], [94mLoss[0m : 2.68944
[1mStep[0m  [12/26], [94mLoss[0m : 2.32485
[1mStep[0m  [14/26], [94mLoss[0m : 2.28282
[1mStep[0m  [16/26], [94mLoss[0m : 2.55714
[1mStep[0m  [18/26], [94mLoss[0m : 2.42604
[1mStep[0m  [20/26], [94mLoss[0m : 2.49938
[1mStep[0m  [22/26], [94mLoss[0m : 2.58952
[1mStep[0m  [24/26], [94mLoss[0m : 2.59417

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.427, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.58180
[1mStep[0m  [2/26], [94mLoss[0m : 2.42448
[1mStep[0m  [4/26], [94mLoss[0m : 2.52972
[1mStep[0m  [6/26], [94mLoss[0m : 2.49489
[1mStep[0m  [8/26], [94mLoss[0m : 2.37937
[1mStep[0m  [10/26], [94mLoss[0m : 2.55746
[1mStep[0m  [12/26], [94mLoss[0m : 2.41930
[1mStep[0m  [14/26], [94mLoss[0m : 2.49600
[1mStep[0m  [16/26], [94mLoss[0m : 2.39416
[1mStep[0m  [18/26], [94mLoss[0m : 2.29456
[1mStep[0m  [20/26], [94mLoss[0m : 2.53527
[1mStep[0m  [22/26], [94mLoss[0m : 2.35274
[1mStep[0m  [24/26], [94mLoss[0m : 2.37066

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.415, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.37380
[1mStep[0m  [2/26], [94mLoss[0m : 2.38759
[1mStep[0m  [4/26], [94mLoss[0m : 2.54973
[1mStep[0m  [6/26], [94mLoss[0m : 2.48461
[1mStep[0m  [8/26], [94mLoss[0m : 2.54594
[1mStep[0m  [10/26], [94mLoss[0m : 2.33153
[1mStep[0m  [12/26], [94mLoss[0m : 2.25509
[1mStep[0m  [14/26], [94mLoss[0m : 2.36865
[1mStep[0m  [16/26], [94mLoss[0m : 2.37426
[1mStep[0m  [18/26], [94mLoss[0m : 2.41558
[1mStep[0m  [20/26], [94mLoss[0m : 2.41021
[1mStep[0m  [22/26], [94mLoss[0m : 2.37329
[1mStep[0m  [24/26], [94mLoss[0m : 2.32235

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.406, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.408
====================================

Phase 1 - Evaluation MAE:  2.407605996498695
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/26], [94mLoss[0m : 2.39382
[1mStep[0m  [2/26], [94mLoss[0m : 2.30106
[1mStep[0m  [4/26], [94mLoss[0m : 2.44985
[1mStep[0m  [6/26], [94mLoss[0m : 2.70752
[1mStep[0m  [8/26], [94mLoss[0m : 2.57956
[1mStep[0m  [10/26], [94mLoss[0m : 2.39800
[1mStep[0m  [12/26], [94mLoss[0m : 2.24651
[1mStep[0m  [14/26], [94mLoss[0m : 2.46192
[1mStep[0m  [16/26], [94mLoss[0m : 2.54466
[1mStep[0m  [18/26], [94mLoss[0m : 2.45251
[1mStep[0m  [20/26], [94mLoss[0m : 2.44135
[1mStep[0m  [22/26], [94mLoss[0m : 2.44263
[1mStep[0m  [24/26], [94mLoss[0m : 2.56399

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.402, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.51025
[1mStep[0m  [2/26], [94mLoss[0m : 2.36907
[1mStep[0m  [4/26], [94mLoss[0m : 2.50797
[1mStep[0m  [6/26], [94mLoss[0m : 2.35301
[1mStep[0m  [8/26], [94mLoss[0m : 2.53670
[1mStep[0m  [10/26], [94mLoss[0m : 2.37700
[1mStep[0m  [12/26], [94mLoss[0m : 2.45263
[1mStep[0m  [14/26], [94mLoss[0m : 2.36720
[1mStep[0m  [16/26], [94mLoss[0m : 2.29292
[1mStep[0m  [18/26], [94mLoss[0m : 2.43160
[1mStep[0m  [20/26], [94mLoss[0m : 2.23745
[1mStep[0m  [22/26], [94mLoss[0m : 2.41337
[1mStep[0m  [24/26], [94mLoss[0m : 2.37105

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.491, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.52613
[1mStep[0m  [2/26], [94mLoss[0m : 2.26161
[1mStep[0m  [4/26], [94mLoss[0m : 2.40696
[1mStep[0m  [6/26], [94mLoss[0m : 2.46915
[1mStep[0m  [8/26], [94mLoss[0m : 2.38441
[1mStep[0m  [10/26], [94mLoss[0m : 2.25202
[1mStep[0m  [12/26], [94mLoss[0m : 2.42480
[1mStep[0m  [14/26], [94mLoss[0m : 2.30673
[1mStep[0m  [16/26], [94mLoss[0m : 2.35878
[1mStep[0m  [18/26], [94mLoss[0m : 2.32888
[1mStep[0m  [20/26], [94mLoss[0m : 2.42817
[1mStep[0m  [22/26], [94mLoss[0m : 2.25011
[1mStep[0m  [24/26], [94mLoss[0m : 2.28447

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.393, [92mTest[0m: 2.466, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.40339
[1mStep[0m  [2/26], [94mLoss[0m : 2.33775
[1mStep[0m  [4/26], [94mLoss[0m : 2.46494
[1mStep[0m  [6/26], [94mLoss[0m : 2.34996
[1mStep[0m  [8/26], [94mLoss[0m : 2.36596
[1mStep[0m  [10/26], [94mLoss[0m : 2.42694
[1mStep[0m  [12/26], [94mLoss[0m : 2.49038
[1mStep[0m  [14/26], [94mLoss[0m : 2.46971
[1mStep[0m  [16/26], [94mLoss[0m : 2.31197
[1mStep[0m  [18/26], [94mLoss[0m : 2.37741
[1mStep[0m  [20/26], [94mLoss[0m : 2.38229
[1mStep[0m  [22/26], [94mLoss[0m : 2.26538
[1mStep[0m  [24/26], [94mLoss[0m : 2.46077

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.357, [92mTest[0m: 2.418, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.37370
[1mStep[0m  [2/26], [94mLoss[0m : 2.36769
[1mStep[0m  [4/26], [94mLoss[0m : 2.27795
[1mStep[0m  [6/26], [94mLoss[0m : 2.17237
[1mStep[0m  [8/26], [94mLoss[0m : 2.17252
[1mStep[0m  [10/26], [94mLoss[0m : 2.33968
[1mStep[0m  [12/26], [94mLoss[0m : 2.31382
[1mStep[0m  [14/26], [94mLoss[0m : 2.12218
[1mStep[0m  [16/26], [94mLoss[0m : 2.22178
[1mStep[0m  [18/26], [94mLoss[0m : 2.29343
[1mStep[0m  [20/26], [94mLoss[0m : 2.38773
[1mStep[0m  [22/26], [94mLoss[0m : 2.32559
[1mStep[0m  [24/26], [94mLoss[0m : 2.45657

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.337, [92mTest[0m: 2.423, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.28315
[1mStep[0m  [2/26], [94mLoss[0m : 2.50835
[1mStep[0m  [4/26], [94mLoss[0m : 2.27092
[1mStep[0m  [6/26], [94mLoss[0m : 2.33125
[1mStep[0m  [8/26], [94mLoss[0m : 2.34945
[1mStep[0m  [10/26], [94mLoss[0m : 2.34485
[1mStep[0m  [12/26], [94mLoss[0m : 2.17192
[1mStep[0m  [14/26], [94mLoss[0m : 2.24366
[1mStep[0m  [16/26], [94mLoss[0m : 2.40030
[1mStep[0m  [18/26], [94mLoss[0m : 2.18554
[1mStep[0m  [20/26], [94mLoss[0m : 2.31878
[1mStep[0m  [22/26], [94mLoss[0m : 2.44222
[1mStep[0m  [24/26], [94mLoss[0m : 2.34154

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.306, [92mTest[0m: 2.440, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.38033
[1mStep[0m  [2/26], [94mLoss[0m : 2.22170
[1mStep[0m  [4/26], [94mLoss[0m : 2.15374
[1mStep[0m  [6/26], [94mLoss[0m : 2.33466
[1mStep[0m  [8/26], [94mLoss[0m : 2.19096
[1mStep[0m  [10/26], [94mLoss[0m : 2.27753
[1mStep[0m  [12/26], [94mLoss[0m : 2.26219
[1mStep[0m  [14/26], [94mLoss[0m : 2.39721
[1mStep[0m  [16/26], [94mLoss[0m : 2.35220
[1mStep[0m  [18/26], [94mLoss[0m : 2.30044
[1mStep[0m  [20/26], [94mLoss[0m : 2.34035
[1mStep[0m  [22/26], [94mLoss[0m : 2.32013
[1mStep[0m  [24/26], [94mLoss[0m : 2.23500

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.264, [92mTest[0m: 2.393, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.24792
[1mStep[0m  [2/26], [94mLoss[0m : 2.10702
[1mStep[0m  [4/26], [94mLoss[0m : 2.42288
[1mStep[0m  [6/26], [94mLoss[0m : 2.24295
[1mStep[0m  [8/26], [94mLoss[0m : 2.13240
[1mStep[0m  [10/26], [94mLoss[0m : 2.21380
[1mStep[0m  [12/26], [94mLoss[0m : 2.11547
[1mStep[0m  [14/26], [94mLoss[0m : 2.20421
[1mStep[0m  [16/26], [94mLoss[0m : 2.21995
[1mStep[0m  [18/26], [94mLoss[0m : 2.17873
[1mStep[0m  [20/26], [94mLoss[0m : 2.22818
[1mStep[0m  [22/26], [94mLoss[0m : 2.21303
[1mStep[0m  [24/26], [94mLoss[0m : 2.26003

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.217, [92mTest[0m: 2.379, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.26475
[1mStep[0m  [2/26], [94mLoss[0m : 2.14088
[1mStep[0m  [4/26], [94mLoss[0m : 2.13733
[1mStep[0m  [6/26], [94mLoss[0m : 2.04317
[1mStep[0m  [8/26], [94mLoss[0m : 2.13643
[1mStep[0m  [10/26], [94mLoss[0m : 2.15504
[1mStep[0m  [12/26], [94mLoss[0m : 2.34831
[1mStep[0m  [14/26], [94mLoss[0m : 2.24422
[1mStep[0m  [16/26], [94mLoss[0m : 2.23114
[1mStep[0m  [18/26], [94mLoss[0m : 2.13194
[1mStep[0m  [20/26], [94mLoss[0m : 2.17290
[1mStep[0m  [22/26], [94mLoss[0m : 2.19511
[1mStep[0m  [24/26], [94mLoss[0m : 2.16615

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.176, [92mTest[0m: 2.400, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.12469
[1mStep[0m  [2/26], [94mLoss[0m : 2.05435
[1mStep[0m  [4/26], [94mLoss[0m : 2.07344
[1mStep[0m  [6/26], [94mLoss[0m : 2.10675
[1mStep[0m  [8/26], [94mLoss[0m : 2.22392
[1mStep[0m  [10/26], [94mLoss[0m : 2.13344
[1mStep[0m  [12/26], [94mLoss[0m : 2.03167
[1mStep[0m  [14/26], [94mLoss[0m : 2.23468
[1mStep[0m  [16/26], [94mLoss[0m : 2.12546
[1mStep[0m  [18/26], [94mLoss[0m : 2.13869
[1mStep[0m  [20/26], [94mLoss[0m : 2.13513
[1mStep[0m  [22/26], [94mLoss[0m : 2.13646
[1mStep[0m  [24/26], [94mLoss[0m : 2.15007

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.125, [92mTest[0m: 2.385, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.90539
[1mStep[0m  [2/26], [94mLoss[0m : 2.08642
[1mStep[0m  [4/26], [94mLoss[0m : 2.22739
[1mStep[0m  [6/26], [94mLoss[0m : 1.97154
[1mStep[0m  [8/26], [94mLoss[0m : 2.12377
[1mStep[0m  [10/26], [94mLoss[0m : 1.90337
[1mStep[0m  [12/26], [94mLoss[0m : 2.02283
[1mStep[0m  [14/26], [94mLoss[0m : 1.97686
[1mStep[0m  [16/26], [94mLoss[0m : 2.13308
[1mStep[0m  [18/26], [94mLoss[0m : 2.08480
[1mStep[0m  [20/26], [94mLoss[0m : 2.00759
[1mStep[0m  [22/26], [94mLoss[0m : 2.20917
[1mStep[0m  [24/26], [94mLoss[0m : 2.11304

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.072, [92mTest[0m: 2.377, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.95690
[1mStep[0m  [2/26], [94mLoss[0m : 2.01770
[1mStep[0m  [4/26], [94mLoss[0m : 2.06819
[1mStep[0m  [6/26], [94mLoss[0m : 2.12245
[1mStep[0m  [8/26], [94mLoss[0m : 2.02035
[1mStep[0m  [10/26], [94mLoss[0m : 2.12900
[1mStep[0m  [12/26], [94mLoss[0m : 1.81110
[1mStep[0m  [14/26], [94mLoss[0m : 2.11666
[1mStep[0m  [16/26], [94mLoss[0m : 2.03843
[1mStep[0m  [18/26], [94mLoss[0m : 2.07332
[1mStep[0m  [20/26], [94mLoss[0m : 2.00815
[1mStep[0m  [22/26], [94mLoss[0m : 2.14190
[1mStep[0m  [24/26], [94mLoss[0m : 2.03031

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.046, [92mTest[0m: 2.404, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.02732
[1mStep[0m  [2/26], [94mLoss[0m : 1.94813
[1mStep[0m  [4/26], [94mLoss[0m : 1.83784
[1mStep[0m  [6/26], [94mLoss[0m : 1.87527
[1mStep[0m  [8/26], [94mLoss[0m : 2.10471
[1mStep[0m  [10/26], [94mLoss[0m : 2.03791
[1mStep[0m  [12/26], [94mLoss[0m : 2.02512
[1mStep[0m  [14/26], [94mLoss[0m : 2.01138
[1mStep[0m  [16/26], [94mLoss[0m : 1.88893
[1mStep[0m  [18/26], [94mLoss[0m : 2.02982
[1mStep[0m  [20/26], [94mLoss[0m : 1.97452
[1mStep[0m  [22/26], [94mLoss[0m : 1.97310
[1mStep[0m  [24/26], [94mLoss[0m : 2.05094

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.978, [92mTest[0m: 2.380, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.84069
[1mStep[0m  [2/26], [94mLoss[0m : 1.83845
[1mStep[0m  [4/26], [94mLoss[0m : 1.94943
[1mStep[0m  [6/26], [94mLoss[0m : 1.85071
[1mStep[0m  [8/26], [94mLoss[0m : 1.88799
[1mStep[0m  [10/26], [94mLoss[0m : 1.82197
[1mStep[0m  [12/26], [94mLoss[0m : 1.86321
[1mStep[0m  [14/26], [94mLoss[0m : 1.98952
[1mStep[0m  [16/26], [94mLoss[0m : 1.95514
[1mStep[0m  [18/26], [94mLoss[0m : 1.85522
[1mStep[0m  [20/26], [94mLoss[0m : 1.91528
[1mStep[0m  [22/26], [94mLoss[0m : 1.97742
[1mStep[0m  [24/26], [94mLoss[0m : 1.85408

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.928, [92mTest[0m: 2.382, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.84200
[1mStep[0m  [2/26], [94mLoss[0m : 1.82933
[1mStep[0m  [4/26], [94mLoss[0m : 1.88975
[1mStep[0m  [6/26], [94mLoss[0m : 1.86978
[1mStep[0m  [8/26], [94mLoss[0m : 1.97135
[1mStep[0m  [10/26], [94mLoss[0m : 1.70997
[1mStep[0m  [12/26], [94mLoss[0m : 1.82500
[1mStep[0m  [14/26], [94mLoss[0m : 1.86033
[1mStep[0m  [16/26], [94mLoss[0m : 1.89635
[1mStep[0m  [18/26], [94mLoss[0m : 1.82973
[1mStep[0m  [20/26], [94mLoss[0m : 2.00679
[1mStep[0m  [22/26], [94mLoss[0m : 1.98923
[1mStep[0m  [24/26], [94mLoss[0m : 1.86663

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.883, [92mTest[0m: 2.388, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.78423
[1mStep[0m  [2/26], [94mLoss[0m : 1.86748
[1mStep[0m  [4/26], [94mLoss[0m : 1.81875
[1mStep[0m  [6/26], [94mLoss[0m : 1.76850
[1mStep[0m  [8/26], [94mLoss[0m : 1.76368
[1mStep[0m  [10/26], [94mLoss[0m : 1.92618
[1mStep[0m  [12/26], [94mLoss[0m : 1.97505
[1mStep[0m  [14/26], [94mLoss[0m : 1.84840
[1mStep[0m  [16/26], [94mLoss[0m : 1.77653
[1mStep[0m  [18/26], [94mLoss[0m : 1.80813
[1mStep[0m  [20/26], [94mLoss[0m : 1.97419
[1mStep[0m  [22/26], [94mLoss[0m : 1.91221
[1mStep[0m  [24/26], [94mLoss[0m : 2.00632

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.857, [92mTest[0m: 2.442, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.73074
[1mStep[0m  [2/26], [94mLoss[0m : 1.88396
[1mStep[0m  [4/26], [94mLoss[0m : 1.88291
[1mStep[0m  [6/26], [94mLoss[0m : 1.74346
[1mStep[0m  [8/26], [94mLoss[0m : 1.86292
[1mStep[0m  [10/26], [94mLoss[0m : 1.74934
[1mStep[0m  [12/26], [94mLoss[0m : 1.79723
[1mStep[0m  [14/26], [94mLoss[0m : 1.80394
[1mStep[0m  [16/26], [94mLoss[0m : 1.99182
[1mStep[0m  [18/26], [94mLoss[0m : 1.71812
[1mStep[0m  [20/26], [94mLoss[0m : 1.92324
[1mStep[0m  [22/26], [94mLoss[0m : 1.75061
[1mStep[0m  [24/26], [94mLoss[0m : 1.71061

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.826, [92mTest[0m: 2.381, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.94771
[1mStep[0m  [2/26], [94mLoss[0m : 1.63583
[1mStep[0m  [4/26], [94mLoss[0m : 1.81338
[1mStep[0m  [6/26], [94mLoss[0m : 1.79742
[1mStep[0m  [8/26], [94mLoss[0m : 1.70114
[1mStep[0m  [10/26], [94mLoss[0m : 1.88837
[1mStep[0m  [12/26], [94mLoss[0m : 1.68515
[1mStep[0m  [14/26], [94mLoss[0m : 1.83432
[1mStep[0m  [16/26], [94mLoss[0m : 1.74210
[1mStep[0m  [18/26], [94mLoss[0m : 1.66934
[1mStep[0m  [20/26], [94mLoss[0m : 1.74373
[1mStep[0m  [22/26], [94mLoss[0m : 1.80196
[1mStep[0m  [24/26], [94mLoss[0m : 1.88175

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.770, [92mTest[0m: 2.465, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.65574
[1mStep[0m  [2/26], [94mLoss[0m : 1.81173
[1mStep[0m  [4/26], [94mLoss[0m : 1.71047
[1mStep[0m  [6/26], [94mLoss[0m : 1.63046
[1mStep[0m  [8/26], [94mLoss[0m : 1.64367
[1mStep[0m  [10/26], [94mLoss[0m : 1.76300
[1mStep[0m  [12/26], [94mLoss[0m : 1.80311
[1mStep[0m  [14/26], [94mLoss[0m : 1.78919
[1mStep[0m  [16/26], [94mLoss[0m : 1.77513
[1mStep[0m  [18/26], [94mLoss[0m : 1.73532
[1mStep[0m  [20/26], [94mLoss[0m : 1.73056
[1mStep[0m  [22/26], [94mLoss[0m : 1.85172
[1mStep[0m  [24/26], [94mLoss[0m : 1.70689

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.736, [92mTest[0m: 2.446, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.61697
[1mStep[0m  [2/26], [94mLoss[0m : 1.71211
[1mStep[0m  [4/26], [94mLoss[0m : 1.68489
[1mStep[0m  [6/26], [94mLoss[0m : 1.66490
[1mStep[0m  [8/26], [94mLoss[0m : 1.74925
[1mStep[0m  [10/26], [94mLoss[0m : 1.59469
[1mStep[0m  [12/26], [94mLoss[0m : 1.74212
[1mStep[0m  [14/26], [94mLoss[0m : 1.70957
[1mStep[0m  [16/26], [94mLoss[0m : 1.67986
[1mStep[0m  [18/26], [94mLoss[0m : 1.74592
[1mStep[0m  [20/26], [94mLoss[0m : 1.62904
[1mStep[0m  [22/26], [94mLoss[0m : 1.69614
[1mStep[0m  [24/26], [94mLoss[0m : 1.69055

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.701, [92mTest[0m: 2.487, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.63414
[1mStep[0m  [2/26], [94mLoss[0m : 1.57843
[1mStep[0m  [4/26], [94mLoss[0m : 1.62312
[1mStep[0m  [6/26], [94mLoss[0m : 1.65959
[1mStep[0m  [8/26], [94mLoss[0m : 1.72406
[1mStep[0m  [10/26], [94mLoss[0m : 1.62809
[1mStep[0m  [12/26], [94mLoss[0m : 1.56985
[1mStep[0m  [14/26], [94mLoss[0m : 1.67979
[1mStep[0m  [16/26], [94mLoss[0m : 1.58719
[1mStep[0m  [18/26], [94mLoss[0m : 1.70873
[1mStep[0m  [20/26], [94mLoss[0m : 1.64588
[1mStep[0m  [22/26], [94mLoss[0m : 1.71704
[1mStep[0m  [24/26], [94mLoss[0m : 1.70982

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.664, [92mTest[0m: 2.523, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.59281
[1mStep[0m  [2/26], [94mLoss[0m : 1.68754
[1mStep[0m  [4/26], [94mLoss[0m : 1.65815
[1mStep[0m  [6/26], [94mLoss[0m : 1.62214
[1mStep[0m  [8/26], [94mLoss[0m : 1.49385
[1mStep[0m  [10/26], [94mLoss[0m : 1.61213
[1mStep[0m  [12/26], [94mLoss[0m : 1.62069
[1mStep[0m  [14/26], [94mLoss[0m : 1.53973
[1mStep[0m  [16/26], [94mLoss[0m : 1.73405
[1mStep[0m  [18/26], [94mLoss[0m : 1.55098
[1mStep[0m  [20/26], [94mLoss[0m : 1.58685
[1mStep[0m  [22/26], [94mLoss[0m : 1.60886
[1mStep[0m  [24/26], [94mLoss[0m : 1.56627

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.624, [92mTest[0m: 2.425, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.54264
[1mStep[0m  [2/26], [94mLoss[0m : 1.63436
[1mStep[0m  [4/26], [94mLoss[0m : 1.55530
[1mStep[0m  [6/26], [94mLoss[0m : 1.62887
[1mStep[0m  [8/26], [94mLoss[0m : 1.56677
[1mStep[0m  [10/26], [94mLoss[0m : 1.60662
[1mStep[0m  [12/26], [94mLoss[0m : 1.57382
[1mStep[0m  [14/26], [94mLoss[0m : 1.63900
[1mStep[0m  [16/26], [94mLoss[0m : 1.53018
[1mStep[0m  [18/26], [94mLoss[0m : 1.65429
[1mStep[0m  [20/26], [94mLoss[0m : 1.57506
[1mStep[0m  [22/26], [94mLoss[0m : 1.54172
[1mStep[0m  [24/26], [94mLoss[0m : 1.54116

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.604, [92mTest[0m: 2.437, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.56638
[1mStep[0m  [2/26], [94mLoss[0m : 1.48634
[1mStep[0m  [4/26], [94mLoss[0m : 1.60361
[1mStep[0m  [6/26], [94mLoss[0m : 1.52702
[1mStep[0m  [8/26], [94mLoss[0m : 1.56459
[1mStep[0m  [10/26], [94mLoss[0m : 1.68942
[1mStep[0m  [12/26], [94mLoss[0m : 1.57476
[1mStep[0m  [14/26], [94mLoss[0m : 1.53666
[1mStep[0m  [16/26], [94mLoss[0m : 1.68916
[1mStep[0m  [18/26], [94mLoss[0m : 1.53787
[1mStep[0m  [20/26], [94mLoss[0m : 1.61996
[1mStep[0m  [22/26], [94mLoss[0m : 1.48572
[1mStep[0m  [24/26], [94mLoss[0m : 1.73615

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.580, [92mTest[0m: 2.450, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.68324
[1mStep[0m  [2/26], [94mLoss[0m : 1.66459
[1mStep[0m  [4/26], [94mLoss[0m : 1.55099
[1mStep[0m  [6/26], [94mLoss[0m : 1.75034
[1mStep[0m  [8/26], [94mLoss[0m : 1.54435
[1mStep[0m  [10/26], [94mLoss[0m : 1.55026
[1mStep[0m  [12/26], [94mLoss[0m : 1.60795
[1mStep[0m  [14/26], [94mLoss[0m : 1.64311
[1mStep[0m  [16/26], [94mLoss[0m : 1.70180
[1mStep[0m  [18/26], [94mLoss[0m : 1.48670
[1mStep[0m  [20/26], [94mLoss[0m : 1.44578
[1mStep[0m  [22/26], [94mLoss[0m : 1.65441
[1mStep[0m  [24/26], [94mLoss[0m : 1.57630

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.572, [92mTest[0m: 2.422, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.45168
[1mStep[0m  [2/26], [94mLoss[0m : 1.55458
[1mStep[0m  [4/26], [94mLoss[0m : 1.47883
[1mStep[0m  [6/26], [94mLoss[0m : 1.58206
[1mStep[0m  [8/26], [94mLoss[0m : 1.59513
[1mStep[0m  [10/26], [94mLoss[0m : 1.47905
[1mStep[0m  [12/26], [94mLoss[0m : 1.38266
[1mStep[0m  [14/26], [94mLoss[0m : 1.57105
[1mStep[0m  [16/26], [94mLoss[0m : 1.52663
[1mStep[0m  [18/26], [94mLoss[0m : 1.53543
[1mStep[0m  [20/26], [94mLoss[0m : 1.61610
[1mStep[0m  [22/26], [94mLoss[0m : 1.69706
[1mStep[0m  [24/26], [94mLoss[0m : 1.45062

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.554, [92mTest[0m: 2.467, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.57101
[1mStep[0m  [2/26], [94mLoss[0m : 1.50851
[1mStep[0m  [4/26], [94mLoss[0m : 1.68488
[1mStep[0m  [6/26], [94mLoss[0m : 1.46816
[1mStep[0m  [8/26], [94mLoss[0m : 1.52368
[1mStep[0m  [10/26], [94mLoss[0m : 1.60373
[1mStep[0m  [12/26], [94mLoss[0m : 1.44978
[1mStep[0m  [14/26], [94mLoss[0m : 1.59575
[1mStep[0m  [16/26], [94mLoss[0m : 1.57220
[1mStep[0m  [18/26], [94mLoss[0m : 1.50240
[1mStep[0m  [20/26], [94mLoss[0m : 1.47463
[1mStep[0m  [22/26], [94mLoss[0m : 1.71499
[1mStep[0m  [24/26], [94mLoss[0m : 1.53770

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.533, [92mTest[0m: 2.598, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.56185
[1mStep[0m  [2/26], [94mLoss[0m : 1.44604
[1mStep[0m  [4/26], [94mLoss[0m : 1.38859
[1mStep[0m  [6/26], [94mLoss[0m : 1.50620
[1mStep[0m  [8/26], [94mLoss[0m : 1.46412
[1mStep[0m  [10/26], [94mLoss[0m : 1.43152
[1mStep[0m  [12/26], [94mLoss[0m : 1.46200
[1mStep[0m  [14/26], [94mLoss[0m : 1.48533
[1mStep[0m  [16/26], [94mLoss[0m : 1.54538
[1mStep[0m  [18/26], [94mLoss[0m : 1.50421
[1mStep[0m  [20/26], [94mLoss[0m : 1.45106
[1mStep[0m  [22/26], [94mLoss[0m : 1.45041
[1mStep[0m  [24/26], [94mLoss[0m : 1.44712

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.476, [92mTest[0m: 2.485, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.37253
[1mStep[0m  [2/26], [94mLoss[0m : 1.48187
[1mStep[0m  [4/26], [94mLoss[0m : 1.50556
[1mStep[0m  [6/26], [94mLoss[0m : 1.49286
[1mStep[0m  [8/26], [94mLoss[0m : 1.46568
[1mStep[0m  [10/26], [94mLoss[0m : 1.36733
[1mStep[0m  [12/26], [94mLoss[0m : 1.43012
[1mStep[0m  [14/26], [94mLoss[0m : 1.48639
[1mStep[0m  [16/26], [94mLoss[0m : 1.43438
[1mStep[0m  [18/26], [94mLoss[0m : 1.40726
[1mStep[0m  [20/26], [94mLoss[0m : 1.41964
[1mStep[0m  [22/26], [94mLoss[0m : 1.42797
[1mStep[0m  [24/26], [94mLoss[0m : 1.45058

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.455, [92mTest[0m: 2.509, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.45415
[1mStep[0m  [2/26], [94mLoss[0m : 1.50272
[1mStep[0m  [4/26], [94mLoss[0m : 1.32246
[1mStep[0m  [6/26], [94mLoss[0m : 1.40383
[1mStep[0m  [8/26], [94mLoss[0m : 1.41903
[1mStep[0m  [10/26], [94mLoss[0m : 1.50405
[1mStep[0m  [12/26], [94mLoss[0m : 1.45416
[1mStep[0m  [14/26], [94mLoss[0m : 1.49841
[1mStep[0m  [16/26], [94mLoss[0m : 1.47055
[1mStep[0m  [18/26], [94mLoss[0m : 1.42831
[1mStep[0m  [20/26], [94mLoss[0m : 1.37066
[1mStep[0m  [22/26], [94mLoss[0m : 1.48900
[1mStep[0m  [24/26], [94mLoss[0m : 1.45568

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.450, [92mTest[0m: 2.520, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.456
====================================

Phase 2 - Evaluation MAE:  2.455798625946045
MAE score P1      2.407606
MAE score P2      2.455799
loss              1.449765
learning_rate     0.002575
batch_size             512
hidden_sizes         [300]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.4
momentum               0.9
weight_decay          0.01
Name: 10, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/53], [94mLoss[0m : 11.05509
[1mStep[0m  [5/53], [94mLoss[0m : 10.16666
[1mStep[0m  [10/53], [94mLoss[0m : 9.22879
[1mStep[0m  [15/53], [94mLoss[0m : 8.67812
[1mStep[0m  [20/53], [94mLoss[0m : 7.68665
[1mStep[0m  [25/53], [94mLoss[0m : 6.24163
[1mStep[0m  [30/53], [94mLoss[0m : 5.82219
[1mStep[0m  [35/53], [94mLoss[0m : 4.67553
[1mStep[0m  [40/53], [94mLoss[0m : 4.20498
[1mStep[0m  [45/53], [94mLoss[0m : 3.72621
[1mStep[0m  [50/53], [94mLoss[0m : 3.50961

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.762, [92mTest[0m: 11.233, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.55969
[1mStep[0m  [5/53], [94mLoss[0m : 3.00682
[1mStep[0m  [10/53], [94mLoss[0m : 2.94614
[1mStep[0m  [15/53], [94mLoss[0m : 3.08780
[1mStep[0m  [20/53], [94mLoss[0m : 2.90101
[1mStep[0m  [25/53], [94mLoss[0m : 2.93298
[1mStep[0m  [30/53], [94mLoss[0m : 3.10651
[1mStep[0m  [35/53], [94mLoss[0m : 2.91578
[1mStep[0m  [40/53], [94mLoss[0m : 2.68999
[1mStep[0m  [45/53], [94mLoss[0m : 2.86891
[1mStep[0m  [50/53], [94mLoss[0m : 2.77189

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.919, [92mTest[0m: 3.338, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.80120
[1mStep[0m  [5/53], [94mLoss[0m : 2.66152
[1mStep[0m  [10/53], [94mLoss[0m : 2.82427
[1mStep[0m  [15/53], [94mLoss[0m : 2.45676
[1mStep[0m  [20/53], [94mLoss[0m : 2.56646
[1mStep[0m  [25/53], [94mLoss[0m : 2.64381
[1mStep[0m  [30/53], [94mLoss[0m : 2.77841
[1mStep[0m  [35/53], [94mLoss[0m : 2.49162
[1mStep[0m  [40/53], [94mLoss[0m : 2.42835
[1mStep[0m  [45/53], [94mLoss[0m : 2.62841
[1mStep[0m  [50/53], [94mLoss[0m : 2.64905

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.667, [92mTest[0m: 2.653, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.54085
[1mStep[0m  [5/53], [94mLoss[0m : 2.61162
[1mStep[0m  [10/53], [94mLoss[0m : 2.73331
[1mStep[0m  [15/53], [94mLoss[0m : 2.74512
[1mStep[0m  [20/53], [94mLoss[0m : 2.64723
[1mStep[0m  [25/53], [94mLoss[0m : 2.58085
[1mStep[0m  [30/53], [94mLoss[0m : 2.50464
[1mStep[0m  [35/53], [94mLoss[0m : 2.70230
[1mStep[0m  [40/53], [94mLoss[0m : 2.54969
[1mStep[0m  [45/53], [94mLoss[0m : 2.66177
[1mStep[0m  [50/53], [94mLoss[0m : 2.40753

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.596, [92mTest[0m: 2.572, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.47153
[1mStep[0m  [5/53], [94mLoss[0m : 2.64613
[1mStep[0m  [10/53], [94mLoss[0m : 2.63193
[1mStep[0m  [15/53], [94mLoss[0m : 2.35800
[1mStep[0m  [20/53], [94mLoss[0m : 2.59610
[1mStep[0m  [25/53], [94mLoss[0m : 2.44718
[1mStep[0m  [30/53], [94mLoss[0m : 2.60875
[1mStep[0m  [35/53], [94mLoss[0m : 2.53977
[1mStep[0m  [40/53], [94mLoss[0m : 2.63716
[1mStep[0m  [45/53], [94mLoss[0m : 2.61440
[1mStep[0m  [50/53], [94mLoss[0m : 2.67937

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.575, [92mTest[0m: 2.530, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.60737
[1mStep[0m  [5/53], [94mLoss[0m : 2.35414
[1mStep[0m  [10/53], [94mLoss[0m : 2.41535
[1mStep[0m  [15/53], [94mLoss[0m : 2.67031
[1mStep[0m  [20/53], [94mLoss[0m : 2.57581
[1mStep[0m  [25/53], [94mLoss[0m : 2.53376
[1mStep[0m  [30/53], [94mLoss[0m : 2.15348
[1mStep[0m  [35/53], [94mLoss[0m : 2.57513
[1mStep[0m  [40/53], [94mLoss[0m : 2.64251
[1mStep[0m  [45/53], [94mLoss[0m : 2.60966
[1mStep[0m  [50/53], [94mLoss[0m : 2.60502

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.545, [92mTest[0m: 2.511, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.24493
[1mStep[0m  [5/53], [94mLoss[0m : 2.49253
[1mStep[0m  [10/53], [94mLoss[0m : 2.43588
[1mStep[0m  [15/53], [94mLoss[0m : 2.30360
[1mStep[0m  [20/53], [94mLoss[0m : 2.54757
[1mStep[0m  [25/53], [94mLoss[0m : 2.57466
[1mStep[0m  [30/53], [94mLoss[0m : 2.33082
[1mStep[0m  [35/53], [94mLoss[0m : 2.51837
[1mStep[0m  [40/53], [94mLoss[0m : 2.49374
[1mStep[0m  [45/53], [94mLoss[0m : 2.59740
[1mStep[0m  [50/53], [94mLoss[0m : 2.41562

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.538, [92mTest[0m: 2.503, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.70429
[1mStep[0m  [5/53], [94mLoss[0m : 2.63928
[1mStep[0m  [10/53], [94mLoss[0m : 2.50985
[1mStep[0m  [15/53], [94mLoss[0m : 2.55886
[1mStep[0m  [20/53], [94mLoss[0m : 2.35989
[1mStep[0m  [25/53], [94mLoss[0m : 2.44165
[1mStep[0m  [30/53], [94mLoss[0m : 2.61032
[1mStep[0m  [35/53], [94mLoss[0m : 2.53577
[1mStep[0m  [40/53], [94mLoss[0m : 2.68459
[1mStep[0m  [45/53], [94mLoss[0m : 2.48437
[1mStep[0m  [50/53], [94mLoss[0m : 2.67246

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.531, [92mTest[0m: 2.493, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.48530
[1mStep[0m  [5/53], [94mLoss[0m : 2.58768
[1mStep[0m  [10/53], [94mLoss[0m : 2.62228
[1mStep[0m  [15/53], [94mLoss[0m : 2.70805
[1mStep[0m  [20/53], [94mLoss[0m : 2.63264
[1mStep[0m  [25/53], [94mLoss[0m : 2.40088
[1mStep[0m  [30/53], [94mLoss[0m : 2.46972
[1mStep[0m  [35/53], [94mLoss[0m : 2.38987
[1mStep[0m  [40/53], [94mLoss[0m : 2.57975
[1mStep[0m  [45/53], [94mLoss[0m : 2.45904
[1mStep[0m  [50/53], [94mLoss[0m : 2.61855

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.480, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.52316
[1mStep[0m  [5/53], [94mLoss[0m : 2.39483
[1mStep[0m  [10/53], [94mLoss[0m : 2.70453
[1mStep[0m  [15/53], [94mLoss[0m : 2.43031
[1mStep[0m  [20/53], [94mLoss[0m : 2.66970
[1mStep[0m  [25/53], [94mLoss[0m : 2.73766
[1mStep[0m  [30/53], [94mLoss[0m : 2.30208
[1mStep[0m  [35/53], [94mLoss[0m : 2.47225
[1mStep[0m  [40/53], [94mLoss[0m : 2.71493
[1mStep[0m  [45/53], [94mLoss[0m : 2.42659
[1mStep[0m  [50/53], [94mLoss[0m : 2.69326

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.517, [92mTest[0m: 2.480, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.40942
[1mStep[0m  [5/53], [94mLoss[0m : 2.39682
[1mStep[0m  [10/53], [94mLoss[0m : 2.26195
[1mStep[0m  [15/53], [94mLoss[0m : 2.55088
[1mStep[0m  [20/53], [94mLoss[0m : 2.38345
[1mStep[0m  [25/53], [94mLoss[0m : 2.75633
[1mStep[0m  [30/53], [94mLoss[0m : 2.59389
[1mStep[0m  [35/53], [94mLoss[0m : 2.64466
[1mStep[0m  [40/53], [94mLoss[0m : 2.59339
[1mStep[0m  [45/53], [94mLoss[0m : 2.49131
[1mStep[0m  [50/53], [94mLoss[0m : 2.53375

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.515, [92mTest[0m: 2.482, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.48396
[1mStep[0m  [5/53], [94mLoss[0m : 2.61985
[1mStep[0m  [10/53], [94mLoss[0m : 2.50335
[1mStep[0m  [15/53], [94mLoss[0m : 2.42263
[1mStep[0m  [20/53], [94mLoss[0m : 2.46414
[1mStep[0m  [25/53], [94mLoss[0m : 2.60416
[1mStep[0m  [30/53], [94mLoss[0m : 2.62082
[1mStep[0m  [35/53], [94mLoss[0m : 2.79792
[1mStep[0m  [40/53], [94mLoss[0m : 2.54907
[1mStep[0m  [45/53], [94mLoss[0m : 2.38447
[1mStep[0m  [50/53], [94mLoss[0m : 2.29031

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.476, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.64977
[1mStep[0m  [5/53], [94mLoss[0m : 2.86724
[1mStep[0m  [10/53], [94mLoss[0m : 2.41863
[1mStep[0m  [15/53], [94mLoss[0m : 2.59331
[1mStep[0m  [20/53], [94mLoss[0m : 2.61445
[1mStep[0m  [25/53], [94mLoss[0m : 2.48088
[1mStep[0m  [30/53], [94mLoss[0m : 2.50161
[1mStep[0m  [35/53], [94mLoss[0m : 2.55457
[1mStep[0m  [40/53], [94mLoss[0m : 2.47326
[1mStep[0m  [45/53], [94mLoss[0m : 2.51848
[1mStep[0m  [50/53], [94mLoss[0m : 2.45304

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.463, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.28539
[1mStep[0m  [5/53], [94mLoss[0m : 2.47579
[1mStep[0m  [10/53], [94mLoss[0m : 2.63519
[1mStep[0m  [15/53], [94mLoss[0m : 2.61316
[1mStep[0m  [20/53], [94mLoss[0m : 2.57670
[1mStep[0m  [25/53], [94mLoss[0m : 2.56842
[1mStep[0m  [30/53], [94mLoss[0m : 2.28723
[1mStep[0m  [35/53], [94mLoss[0m : 2.51664
[1mStep[0m  [40/53], [94mLoss[0m : 2.69513
[1mStep[0m  [45/53], [94mLoss[0m : 2.53527
[1mStep[0m  [50/53], [94mLoss[0m : 2.43307

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.472, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.59985
[1mStep[0m  [5/53], [94mLoss[0m : 2.59013
[1mStep[0m  [10/53], [94mLoss[0m : 2.62552
[1mStep[0m  [15/53], [94mLoss[0m : 2.44150
[1mStep[0m  [20/53], [94mLoss[0m : 2.33572
[1mStep[0m  [25/53], [94mLoss[0m : 2.48810
[1mStep[0m  [30/53], [94mLoss[0m : 2.69055
[1mStep[0m  [35/53], [94mLoss[0m : 2.51405
[1mStep[0m  [40/53], [94mLoss[0m : 2.66643
[1mStep[0m  [45/53], [94mLoss[0m : 2.35757
[1mStep[0m  [50/53], [94mLoss[0m : 2.39727

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.459, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.58589
[1mStep[0m  [5/53], [94mLoss[0m : 2.54824
[1mStep[0m  [10/53], [94mLoss[0m : 2.49940
[1mStep[0m  [15/53], [94mLoss[0m : 2.20057
[1mStep[0m  [20/53], [94mLoss[0m : 2.45591
[1mStep[0m  [25/53], [94mLoss[0m : 2.33461
[1mStep[0m  [30/53], [94mLoss[0m : 2.37780
[1mStep[0m  [35/53], [94mLoss[0m : 2.41107
[1mStep[0m  [40/53], [94mLoss[0m : 2.47024
[1mStep[0m  [45/53], [94mLoss[0m : 2.62025
[1mStep[0m  [50/53], [94mLoss[0m : 2.46194

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.464, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.50686
[1mStep[0m  [5/53], [94mLoss[0m : 2.47811
[1mStep[0m  [10/53], [94mLoss[0m : 2.24350
[1mStep[0m  [15/53], [94mLoss[0m : 2.49818
[1mStep[0m  [20/53], [94mLoss[0m : 2.46938
[1mStep[0m  [25/53], [94mLoss[0m : 2.46204
[1mStep[0m  [30/53], [94mLoss[0m : 2.48910
[1mStep[0m  [35/53], [94mLoss[0m : 2.54996
[1mStep[0m  [40/53], [94mLoss[0m : 2.43673
[1mStep[0m  [45/53], [94mLoss[0m : 2.41628
[1mStep[0m  [50/53], [94mLoss[0m : 2.48201

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.461, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.55537
[1mStep[0m  [5/53], [94mLoss[0m : 2.65191
[1mStep[0m  [10/53], [94mLoss[0m : 2.50519
[1mStep[0m  [15/53], [94mLoss[0m : 2.51715
[1mStep[0m  [20/53], [94mLoss[0m : 2.49516
[1mStep[0m  [25/53], [94mLoss[0m : 2.46644
[1mStep[0m  [30/53], [94mLoss[0m : 2.46978
[1mStep[0m  [35/53], [94mLoss[0m : 2.40898
[1mStep[0m  [40/53], [94mLoss[0m : 2.48565
[1mStep[0m  [45/53], [94mLoss[0m : 2.45946
[1mStep[0m  [50/53], [94mLoss[0m : 2.37101

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.455, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.67711
[1mStep[0m  [5/53], [94mLoss[0m : 2.45103
[1mStep[0m  [10/53], [94mLoss[0m : 2.51307
[1mStep[0m  [15/53], [94mLoss[0m : 2.56474
[1mStep[0m  [20/53], [94mLoss[0m : 2.20943
[1mStep[0m  [25/53], [94mLoss[0m : 2.74638
[1mStep[0m  [30/53], [94mLoss[0m : 2.34163
[1mStep[0m  [35/53], [94mLoss[0m : 2.51803
[1mStep[0m  [40/53], [94mLoss[0m : 2.41232
[1mStep[0m  [45/53], [94mLoss[0m : 2.49121
[1mStep[0m  [50/53], [94mLoss[0m : 2.46535

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.457, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.62314
[1mStep[0m  [5/53], [94mLoss[0m : 2.20611
[1mStep[0m  [10/53], [94mLoss[0m : 2.56869
[1mStep[0m  [15/53], [94mLoss[0m : 2.30533
[1mStep[0m  [20/53], [94mLoss[0m : 2.48065
[1mStep[0m  [25/53], [94mLoss[0m : 2.49172
[1mStep[0m  [30/53], [94mLoss[0m : 2.82296
[1mStep[0m  [35/53], [94mLoss[0m : 2.38254
[1mStep[0m  [40/53], [94mLoss[0m : 2.36157
[1mStep[0m  [45/53], [94mLoss[0m : 2.47372
[1mStep[0m  [50/53], [94mLoss[0m : 2.43144

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.447, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.40753
[1mStep[0m  [5/53], [94mLoss[0m : 2.47549
[1mStep[0m  [10/53], [94mLoss[0m : 2.47833
[1mStep[0m  [15/53], [94mLoss[0m : 2.60669
[1mStep[0m  [20/53], [94mLoss[0m : 2.48067
[1mStep[0m  [25/53], [94mLoss[0m : 2.42404
[1mStep[0m  [30/53], [94mLoss[0m : 2.47801
[1mStep[0m  [35/53], [94mLoss[0m : 2.32813
[1mStep[0m  [40/53], [94mLoss[0m : 2.52618
[1mStep[0m  [45/53], [94mLoss[0m : 2.37674
[1mStep[0m  [50/53], [94mLoss[0m : 2.65366

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.453, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.58719
[1mStep[0m  [5/53], [94mLoss[0m : 2.56207
[1mStep[0m  [10/53], [94mLoss[0m : 2.42861
[1mStep[0m  [15/53], [94mLoss[0m : 2.50358
[1mStep[0m  [20/53], [94mLoss[0m : 2.38127
[1mStep[0m  [25/53], [94mLoss[0m : 2.53678
[1mStep[0m  [30/53], [94mLoss[0m : 2.49660
[1mStep[0m  [35/53], [94mLoss[0m : 2.50997
[1mStep[0m  [40/53], [94mLoss[0m : 2.50281
[1mStep[0m  [45/53], [94mLoss[0m : 2.50826
[1mStep[0m  [50/53], [94mLoss[0m : 2.62763

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.452, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.57926
[1mStep[0m  [5/53], [94mLoss[0m : 2.67225
[1mStep[0m  [10/53], [94mLoss[0m : 2.49546
[1mStep[0m  [15/53], [94mLoss[0m : 2.45891
[1mStep[0m  [20/53], [94mLoss[0m : 2.36859
[1mStep[0m  [25/53], [94mLoss[0m : 2.33652
[1mStep[0m  [30/53], [94mLoss[0m : 2.53917
[1mStep[0m  [35/53], [94mLoss[0m : 2.42143
[1mStep[0m  [40/53], [94mLoss[0m : 2.68625
[1mStep[0m  [45/53], [94mLoss[0m : 2.57933
[1mStep[0m  [50/53], [94mLoss[0m : 2.48152

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.445, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.43695
[1mStep[0m  [5/53], [94mLoss[0m : 2.47323
[1mStep[0m  [10/53], [94mLoss[0m : 2.64962
[1mStep[0m  [15/53], [94mLoss[0m : 2.38822
[1mStep[0m  [20/53], [94mLoss[0m : 2.59389
[1mStep[0m  [25/53], [94mLoss[0m : 2.39223
[1mStep[0m  [30/53], [94mLoss[0m : 2.36822
[1mStep[0m  [35/53], [94mLoss[0m : 2.50070
[1mStep[0m  [40/53], [94mLoss[0m : 2.60863
[1mStep[0m  [45/53], [94mLoss[0m : 2.49737
[1mStep[0m  [50/53], [94mLoss[0m : 2.53215

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.444, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.26241
[1mStep[0m  [5/53], [94mLoss[0m : 2.60224
[1mStep[0m  [10/53], [94mLoss[0m : 2.57665
[1mStep[0m  [15/53], [94mLoss[0m : 2.54435
[1mStep[0m  [20/53], [94mLoss[0m : 2.36211
[1mStep[0m  [25/53], [94mLoss[0m : 2.41540
[1mStep[0m  [30/53], [94mLoss[0m : 2.38540
[1mStep[0m  [35/53], [94mLoss[0m : 2.53755
[1mStep[0m  [40/53], [94mLoss[0m : 2.51716
[1mStep[0m  [45/53], [94mLoss[0m : 2.46775
[1mStep[0m  [50/53], [94mLoss[0m : 2.29200

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.433, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.51086
[1mStep[0m  [5/53], [94mLoss[0m : 2.44448
[1mStep[0m  [10/53], [94mLoss[0m : 2.62410
[1mStep[0m  [15/53], [94mLoss[0m : 2.37400
[1mStep[0m  [20/53], [94mLoss[0m : 2.45183
[1mStep[0m  [25/53], [94mLoss[0m : 2.31293
[1mStep[0m  [30/53], [94mLoss[0m : 2.44880
[1mStep[0m  [35/53], [94mLoss[0m : 2.59372
[1mStep[0m  [40/53], [94mLoss[0m : 2.35270
[1mStep[0m  [45/53], [94mLoss[0m : 2.36910
[1mStep[0m  [50/53], [94mLoss[0m : 2.43259

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.442, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.49335
[1mStep[0m  [5/53], [94mLoss[0m : 2.70299
[1mStep[0m  [10/53], [94mLoss[0m : 2.43054
[1mStep[0m  [15/53], [94mLoss[0m : 2.45714
[1mStep[0m  [20/53], [94mLoss[0m : 2.24582
[1mStep[0m  [25/53], [94mLoss[0m : 2.50653
[1mStep[0m  [30/53], [94mLoss[0m : 2.41435
[1mStep[0m  [35/53], [94mLoss[0m : 2.42612
[1mStep[0m  [40/53], [94mLoss[0m : 2.31751
[1mStep[0m  [45/53], [94mLoss[0m : 2.36532
[1mStep[0m  [50/53], [94mLoss[0m : 2.59194

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.433, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.64677
[1mStep[0m  [5/53], [94mLoss[0m : 2.48076
[1mStep[0m  [10/53], [94mLoss[0m : 2.64899
[1mStep[0m  [15/53], [94mLoss[0m : 2.60894
[1mStep[0m  [20/53], [94mLoss[0m : 2.59040
[1mStep[0m  [25/53], [94mLoss[0m : 2.57985
[1mStep[0m  [30/53], [94mLoss[0m : 2.45631
[1mStep[0m  [35/53], [94mLoss[0m : 2.36557
[1mStep[0m  [40/53], [94mLoss[0m : 2.17614
[1mStep[0m  [45/53], [94mLoss[0m : 2.47295
[1mStep[0m  [50/53], [94mLoss[0m : 2.58666

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.436, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.42811
[1mStep[0m  [5/53], [94mLoss[0m : 2.48090
[1mStep[0m  [10/53], [94mLoss[0m : 2.59245
[1mStep[0m  [15/53], [94mLoss[0m : 2.39963
[1mStep[0m  [20/53], [94mLoss[0m : 2.61190
[1mStep[0m  [25/53], [94mLoss[0m : 2.52116
[1mStep[0m  [30/53], [94mLoss[0m : 2.41083
[1mStep[0m  [35/53], [94mLoss[0m : 2.40312
[1mStep[0m  [40/53], [94mLoss[0m : 2.43287
[1mStep[0m  [45/53], [94mLoss[0m : 2.66052
[1mStep[0m  [50/53], [94mLoss[0m : 2.72212

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.433, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.44742
[1mStep[0m  [5/53], [94mLoss[0m : 2.51497
[1mStep[0m  [10/53], [94mLoss[0m : 2.47837
[1mStep[0m  [15/53], [94mLoss[0m : 2.47346
[1mStep[0m  [20/53], [94mLoss[0m : 2.54002
[1mStep[0m  [25/53], [94mLoss[0m : 2.55767
[1mStep[0m  [30/53], [94mLoss[0m : 2.58539
[1mStep[0m  [35/53], [94mLoss[0m : 2.45094
[1mStep[0m  [40/53], [94mLoss[0m : 2.49769
[1mStep[0m  [45/53], [94mLoss[0m : 2.55902
[1mStep[0m  [50/53], [94mLoss[0m : 2.53653

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.437, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.438
====================================

Phase 1 - Evaluation MAE:  2.437650653032156
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/53], [94mLoss[0m : 2.54355
[1mStep[0m  [5/53], [94mLoss[0m : 2.46509
[1mStep[0m  [10/53], [94mLoss[0m : 2.51525
[1mStep[0m  [15/53], [94mLoss[0m : 2.56031
[1mStep[0m  [20/53], [94mLoss[0m : 2.56091
[1mStep[0m  [25/53], [94mLoss[0m : 2.59068
[1mStep[0m  [30/53], [94mLoss[0m : 2.49931
[1mStep[0m  [35/53], [94mLoss[0m : 2.47653
[1mStep[0m  [40/53], [94mLoss[0m : 2.44358
[1mStep[0m  [45/53], [94mLoss[0m : 2.59750
[1mStep[0m  [50/53], [94mLoss[0m : 2.26563

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.432, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.44187
[1mStep[0m  [5/53], [94mLoss[0m : 2.40065
[1mStep[0m  [10/53], [94mLoss[0m : 2.45807
[1mStep[0m  [15/53], [94mLoss[0m : 2.39606
[1mStep[0m  [20/53], [94mLoss[0m : 2.50571
[1mStep[0m  [25/53], [94mLoss[0m : 2.65572
[1mStep[0m  [30/53], [94mLoss[0m : 2.58313
[1mStep[0m  [35/53], [94mLoss[0m : 2.33556
[1mStep[0m  [40/53], [94mLoss[0m : 2.38796
[1mStep[0m  [45/53], [94mLoss[0m : 2.52878
[1mStep[0m  [50/53], [94mLoss[0m : 2.44519

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.413, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.68075
[1mStep[0m  [5/53], [94mLoss[0m : 2.38775
[1mStep[0m  [10/53], [94mLoss[0m : 2.43744
[1mStep[0m  [15/53], [94mLoss[0m : 2.47249
[1mStep[0m  [20/53], [94mLoss[0m : 2.45972
[1mStep[0m  [25/53], [94mLoss[0m : 2.57080
[1mStep[0m  [30/53], [94mLoss[0m : 2.30137
[1mStep[0m  [35/53], [94mLoss[0m : 2.41452
[1mStep[0m  [40/53], [94mLoss[0m : 2.45948
[1mStep[0m  [45/53], [94mLoss[0m : 2.47494
[1mStep[0m  [50/53], [94mLoss[0m : 2.49199

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.421, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.39409
[1mStep[0m  [5/53], [94mLoss[0m : 2.45400
[1mStep[0m  [10/53], [94mLoss[0m : 2.76894
[1mStep[0m  [15/53], [94mLoss[0m : 2.31923
[1mStep[0m  [20/53], [94mLoss[0m : 2.51765
[1mStep[0m  [25/53], [94mLoss[0m : 2.58410
[1mStep[0m  [30/53], [94mLoss[0m : 2.55316
[1mStep[0m  [35/53], [94mLoss[0m : 2.46883
[1mStep[0m  [40/53], [94mLoss[0m : 2.67635
[1mStep[0m  [45/53], [94mLoss[0m : 2.22924
[1mStep[0m  [50/53], [94mLoss[0m : 2.55811

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.432, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.36476
[1mStep[0m  [5/53], [94mLoss[0m : 2.40704
[1mStep[0m  [10/53], [94mLoss[0m : 2.51319
[1mStep[0m  [15/53], [94mLoss[0m : 2.49933
[1mStep[0m  [20/53], [94mLoss[0m : 2.38838
[1mStep[0m  [25/53], [94mLoss[0m : 2.27283
[1mStep[0m  [30/53], [94mLoss[0m : 2.40515
[1mStep[0m  [35/53], [94mLoss[0m : 2.53252
[1mStep[0m  [40/53], [94mLoss[0m : 2.32583
[1mStep[0m  [45/53], [94mLoss[0m : 2.57679
[1mStep[0m  [50/53], [94mLoss[0m : 2.62788

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.424, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.44384
[1mStep[0m  [5/53], [94mLoss[0m : 2.38617
[1mStep[0m  [10/53], [94mLoss[0m : 2.39808
[1mStep[0m  [15/53], [94mLoss[0m : 2.53702
[1mStep[0m  [20/53], [94mLoss[0m : 2.55311
[1mStep[0m  [25/53], [94mLoss[0m : 2.51726
[1mStep[0m  [30/53], [94mLoss[0m : 2.20752
[1mStep[0m  [35/53], [94mLoss[0m : 2.44516
[1mStep[0m  [40/53], [94mLoss[0m : 2.46838
[1mStep[0m  [45/53], [94mLoss[0m : 2.59594
[1mStep[0m  [50/53], [94mLoss[0m : 2.45310

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.425, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.43705
[1mStep[0m  [5/53], [94mLoss[0m : 2.41614
[1mStep[0m  [10/53], [94mLoss[0m : 2.36976
[1mStep[0m  [15/53], [94mLoss[0m : 2.40493
[1mStep[0m  [20/53], [94mLoss[0m : 2.31399
[1mStep[0m  [25/53], [94mLoss[0m : 2.44791
[1mStep[0m  [30/53], [94mLoss[0m : 2.42535
[1mStep[0m  [35/53], [94mLoss[0m : 2.34754
[1mStep[0m  [40/53], [94mLoss[0m : 2.43463
[1mStep[0m  [45/53], [94mLoss[0m : 2.46752
[1mStep[0m  [50/53], [94mLoss[0m : 2.34582

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.420, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.52415
[1mStep[0m  [5/53], [94mLoss[0m : 2.43623
[1mStep[0m  [10/53], [94mLoss[0m : 2.27875
[1mStep[0m  [15/53], [94mLoss[0m : 2.52365
[1mStep[0m  [20/53], [94mLoss[0m : 2.38966
[1mStep[0m  [25/53], [94mLoss[0m : 2.50698
[1mStep[0m  [30/53], [94mLoss[0m : 2.44836
[1mStep[0m  [35/53], [94mLoss[0m : 2.42449
[1mStep[0m  [40/53], [94mLoss[0m : 2.38535
[1mStep[0m  [45/53], [94mLoss[0m : 2.38131
[1mStep[0m  [50/53], [94mLoss[0m : 2.40868

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.469, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.40085
[1mStep[0m  [5/53], [94mLoss[0m : 2.43036
[1mStep[0m  [10/53], [94mLoss[0m : 2.63185
[1mStep[0m  [15/53], [94mLoss[0m : 2.36529
[1mStep[0m  [20/53], [94mLoss[0m : 2.41528
[1mStep[0m  [25/53], [94mLoss[0m : 2.35600
[1mStep[0m  [30/53], [94mLoss[0m : 2.47392
[1mStep[0m  [35/53], [94mLoss[0m : 2.48409
[1mStep[0m  [40/53], [94mLoss[0m : 2.50570
[1mStep[0m  [45/53], [94mLoss[0m : 2.19005
[1mStep[0m  [50/53], [94mLoss[0m : 2.42589

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.479, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.39726
[1mStep[0m  [5/53], [94mLoss[0m : 2.46325
[1mStep[0m  [10/53], [94mLoss[0m : 2.49769
[1mStep[0m  [15/53], [94mLoss[0m : 2.46430
[1mStep[0m  [20/53], [94mLoss[0m : 2.33719
[1mStep[0m  [25/53], [94mLoss[0m : 2.26077
[1mStep[0m  [30/53], [94mLoss[0m : 2.52105
[1mStep[0m  [35/53], [94mLoss[0m : 2.51204
[1mStep[0m  [40/53], [94mLoss[0m : 2.50483
[1mStep[0m  [45/53], [94mLoss[0m : 2.38864
[1mStep[0m  [50/53], [94mLoss[0m : 2.64865

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.421, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.19535
[1mStep[0m  [5/53], [94mLoss[0m : 2.43963
[1mStep[0m  [10/53], [94mLoss[0m : 2.54062
[1mStep[0m  [15/53], [94mLoss[0m : 2.50541
[1mStep[0m  [20/53], [94mLoss[0m : 2.44676
[1mStep[0m  [25/53], [94mLoss[0m : 2.36744
[1mStep[0m  [30/53], [94mLoss[0m : 2.51963
[1mStep[0m  [35/53], [94mLoss[0m : 2.35533
[1mStep[0m  [40/53], [94mLoss[0m : 2.42168
[1mStep[0m  [45/53], [94mLoss[0m : 2.48836
[1mStep[0m  [50/53], [94mLoss[0m : 2.41913

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.403, [92mTest[0m: 2.434, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.47839
[1mStep[0m  [5/53], [94mLoss[0m : 2.35785
[1mStep[0m  [10/53], [94mLoss[0m : 2.41286
[1mStep[0m  [15/53], [94mLoss[0m : 2.55032
[1mStep[0m  [20/53], [94mLoss[0m : 2.51568
[1mStep[0m  [25/53], [94mLoss[0m : 2.21670
[1mStep[0m  [30/53], [94mLoss[0m : 2.27051
[1mStep[0m  [35/53], [94mLoss[0m : 2.64938
[1mStep[0m  [40/53], [94mLoss[0m : 2.50017
[1mStep[0m  [45/53], [94mLoss[0m : 2.31909
[1mStep[0m  [50/53], [94mLoss[0m : 2.18935

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.395, [92mTest[0m: 2.449, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.47851
[1mStep[0m  [5/53], [94mLoss[0m : 2.32364
[1mStep[0m  [10/53], [94mLoss[0m : 2.26547
[1mStep[0m  [15/53], [94mLoss[0m : 2.47896
[1mStep[0m  [20/53], [94mLoss[0m : 2.38519
[1mStep[0m  [25/53], [94mLoss[0m : 2.56108
[1mStep[0m  [30/53], [94mLoss[0m : 2.29135
[1mStep[0m  [35/53], [94mLoss[0m : 2.33133
[1mStep[0m  [40/53], [94mLoss[0m : 2.34839
[1mStep[0m  [45/53], [94mLoss[0m : 2.40185
[1mStep[0m  [50/53], [94mLoss[0m : 2.38628

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.390, [92mTest[0m: 2.442, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.41569
[1mStep[0m  [5/53], [94mLoss[0m : 2.31499
[1mStep[0m  [10/53], [94mLoss[0m : 2.35140
[1mStep[0m  [15/53], [94mLoss[0m : 2.60202
[1mStep[0m  [20/53], [94mLoss[0m : 2.51033
[1mStep[0m  [25/53], [94mLoss[0m : 2.27922
[1mStep[0m  [30/53], [94mLoss[0m : 2.34300
[1mStep[0m  [35/53], [94mLoss[0m : 2.22200
[1mStep[0m  [40/53], [94mLoss[0m : 2.36016
[1mStep[0m  [45/53], [94mLoss[0m : 2.34354
[1mStep[0m  [50/53], [94mLoss[0m : 2.26978

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.385, [92mTest[0m: 2.499, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.52637
[1mStep[0m  [5/53], [94mLoss[0m : 2.46029
[1mStep[0m  [10/53], [94mLoss[0m : 2.35154
[1mStep[0m  [15/53], [94mLoss[0m : 2.12805
[1mStep[0m  [20/53], [94mLoss[0m : 2.36743
[1mStep[0m  [25/53], [94mLoss[0m : 2.41718
[1mStep[0m  [30/53], [94mLoss[0m : 2.41356
[1mStep[0m  [35/53], [94mLoss[0m : 2.47057
[1mStep[0m  [40/53], [94mLoss[0m : 2.55153
[1mStep[0m  [45/53], [94mLoss[0m : 2.59243
[1mStep[0m  [50/53], [94mLoss[0m : 2.38414

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.368, [92mTest[0m: 2.434, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.15290
[1mStep[0m  [5/53], [94mLoss[0m : 2.27750
[1mStep[0m  [10/53], [94mLoss[0m : 2.38847
[1mStep[0m  [15/53], [94mLoss[0m : 2.32426
[1mStep[0m  [20/53], [94mLoss[0m : 2.46594
[1mStep[0m  [25/53], [94mLoss[0m : 2.31641
[1mStep[0m  [30/53], [94mLoss[0m : 2.54487
[1mStep[0m  [35/53], [94mLoss[0m : 2.31189
[1mStep[0m  [40/53], [94mLoss[0m : 2.27900
[1mStep[0m  [45/53], [94mLoss[0m : 2.25474
[1mStep[0m  [50/53], [94mLoss[0m : 2.42236

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.365, [92mTest[0m: 2.466, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.54134
[1mStep[0m  [5/53], [94mLoss[0m : 2.47039
[1mStep[0m  [10/53], [94mLoss[0m : 2.12136
[1mStep[0m  [15/53], [94mLoss[0m : 2.41521
[1mStep[0m  [20/53], [94mLoss[0m : 2.24190
[1mStep[0m  [25/53], [94mLoss[0m : 2.24002
[1mStep[0m  [30/53], [94mLoss[0m : 2.13244
[1mStep[0m  [35/53], [94mLoss[0m : 2.41477
[1mStep[0m  [40/53], [94mLoss[0m : 2.31265
[1mStep[0m  [45/53], [94mLoss[0m : 2.39539
[1mStep[0m  [50/53], [94mLoss[0m : 2.34929

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.355, [92mTest[0m: 2.459, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.22299
[1mStep[0m  [5/53], [94mLoss[0m : 2.41171
[1mStep[0m  [10/53], [94mLoss[0m : 2.26959
[1mStep[0m  [15/53], [94mLoss[0m : 2.34595
[1mStep[0m  [20/53], [94mLoss[0m : 2.42731
[1mStep[0m  [25/53], [94mLoss[0m : 2.45298
[1mStep[0m  [30/53], [94mLoss[0m : 2.26622
[1mStep[0m  [35/53], [94mLoss[0m : 2.28119
[1mStep[0m  [40/53], [94mLoss[0m : 2.50970
[1mStep[0m  [45/53], [94mLoss[0m : 2.23872
[1mStep[0m  [50/53], [94mLoss[0m : 2.35375

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.345, [92mTest[0m: 2.461, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.40632
[1mStep[0m  [5/53], [94mLoss[0m : 2.28059
[1mStep[0m  [10/53], [94mLoss[0m : 2.25630
[1mStep[0m  [15/53], [94mLoss[0m : 2.50500
[1mStep[0m  [20/53], [94mLoss[0m : 2.37681
[1mStep[0m  [25/53], [94mLoss[0m : 2.16954
[1mStep[0m  [30/53], [94mLoss[0m : 2.09223
[1mStep[0m  [35/53], [94mLoss[0m : 2.44071
[1mStep[0m  [40/53], [94mLoss[0m : 2.41059
[1mStep[0m  [45/53], [94mLoss[0m : 2.20438
[1mStep[0m  [50/53], [94mLoss[0m : 2.44278

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.354, [92mTest[0m: 2.510, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.22015
[1mStep[0m  [5/53], [94mLoss[0m : 2.33027
[1mStep[0m  [10/53], [94mLoss[0m : 2.27263
[1mStep[0m  [15/53], [94mLoss[0m : 2.49888
[1mStep[0m  [20/53], [94mLoss[0m : 2.23494
[1mStep[0m  [25/53], [94mLoss[0m : 2.37705
[1mStep[0m  [30/53], [94mLoss[0m : 2.32594
[1mStep[0m  [35/53], [94mLoss[0m : 2.52447
[1mStep[0m  [40/53], [94mLoss[0m : 2.40126
[1mStep[0m  [45/53], [94mLoss[0m : 2.41042
[1mStep[0m  [50/53], [94mLoss[0m : 2.32303

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.343, [92mTest[0m: 2.449, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.22454
[1mStep[0m  [5/53], [94mLoss[0m : 2.26563
[1mStep[0m  [10/53], [94mLoss[0m : 2.17756
[1mStep[0m  [15/53], [94mLoss[0m : 2.26650
[1mStep[0m  [20/53], [94mLoss[0m : 2.37594
[1mStep[0m  [25/53], [94mLoss[0m : 2.37225
[1mStep[0m  [30/53], [94mLoss[0m : 2.62343
[1mStep[0m  [35/53], [94mLoss[0m : 2.24214
[1mStep[0m  [40/53], [94mLoss[0m : 2.41710
[1mStep[0m  [45/53], [94mLoss[0m : 2.30004
[1mStep[0m  [50/53], [94mLoss[0m : 2.37135

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.329, [92mTest[0m: 2.526, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.45343
[1mStep[0m  [5/53], [94mLoss[0m : 2.36137
[1mStep[0m  [10/53], [94mLoss[0m : 2.28281
[1mStep[0m  [15/53], [94mLoss[0m : 2.38411
[1mStep[0m  [20/53], [94mLoss[0m : 2.47049
[1mStep[0m  [25/53], [94mLoss[0m : 2.37172
[1mStep[0m  [30/53], [94mLoss[0m : 2.25268
[1mStep[0m  [35/53], [94mLoss[0m : 2.30510
[1mStep[0m  [40/53], [94mLoss[0m : 2.32409
[1mStep[0m  [45/53], [94mLoss[0m : 2.38498
[1mStep[0m  [50/53], [94mLoss[0m : 2.36269

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.319, [92mTest[0m: 2.544, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.19499
[1mStep[0m  [5/53], [94mLoss[0m : 2.47935
[1mStep[0m  [10/53], [94mLoss[0m : 2.42920
[1mStep[0m  [15/53], [94mLoss[0m : 2.23402
[1mStep[0m  [20/53], [94mLoss[0m : 2.36967
[1mStep[0m  [25/53], [94mLoss[0m : 2.29710
[1mStep[0m  [30/53], [94mLoss[0m : 2.14560
[1mStep[0m  [35/53], [94mLoss[0m : 2.16218
[1mStep[0m  [40/53], [94mLoss[0m : 2.11136
[1mStep[0m  [45/53], [94mLoss[0m : 2.41753
[1mStep[0m  [50/53], [94mLoss[0m : 2.31949

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.309, [92mTest[0m: 2.515, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.12887
[1mStep[0m  [5/53], [94mLoss[0m : 2.12057
[1mStep[0m  [10/53], [94mLoss[0m : 2.37882
[1mStep[0m  [15/53], [94mLoss[0m : 2.35234
[1mStep[0m  [20/53], [94mLoss[0m : 2.22692
[1mStep[0m  [25/53], [94mLoss[0m : 2.52191
[1mStep[0m  [30/53], [94mLoss[0m : 2.27309
[1mStep[0m  [35/53], [94mLoss[0m : 2.17534
[1mStep[0m  [40/53], [94mLoss[0m : 2.30324
[1mStep[0m  [45/53], [94mLoss[0m : 2.12883
[1mStep[0m  [50/53], [94mLoss[0m : 2.38406

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.293, [92mTest[0m: 2.492, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.28833
[1mStep[0m  [5/53], [94mLoss[0m : 2.26948
[1mStep[0m  [10/53], [94mLoss[0m : 2.45698
[1mStep[0m  [15/53], [94mLoss[0m : 2.23237
[1mStep[0m  [20/53], [94mLoss[0m : 2.25362
[1mStep[0m  [25/53], [94mLoss[0m : 2.25652
[1mStep[0m  [30/53], [94mLoss[0m : 2.19510
[1mStep[0m  [35/53], [94mLoss[0m : 2.33409
[1mStep[0m  [40/53], [94mLoss[0m : 2.35108
[1mStep[0m  [45/53], [94mLoss[0m : 2.43896
[1mStep[0m  [50/53], [94mLoss[0m : 2.44742

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.291, [92mTest[0m: 2.490, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.36276
[1mStep[0m  [5/53], [94mLoss[0m : 2.47330
[1mStep[0m  [10/53], [94mLoss[0m : 2.25681
[1mStep[0m  [15/53], [94mLoss[0m : 2.20773
[1mStep[0m  [20/53], [94mLoss[0m : 2.51137
[1mStep[0m  [25/53], [94mLoss[0m : 2.33107
[1mStep[0m  [30/53], [94mLoss[0m : 2.12852
[1mStep[0m  [35/53], [94mLoss[0m : 2.19163
[1mStep[0m  [40/53], [94mLoss[0m : 2.49278
[1mStep[0m  [45/53], [94mLoss[0m : 2.35068
[1mStep[0m  [50/53], [94mLoss[0m : 2.31620

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.277, [92mTest[0m: 2.470, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.38385
[1mStep[0m  [5/53], [94mLoss[0m : 2.08158
[1mStep[0m  [10/53], [94mLoss[0m : 2.06375
[1mStep[0m  [15/53], [94mLoss[0m : 2.22943
[1mStep[0m  [20/53], [94mLoss[0m : 2.32627
[1mStep[0m  [25/53], [94mLoss[0m : 2.17991
[1mStep[0m  [30/53], [94mLoss[0m : 2.11109
[1mStep[0m  [35/53], [94mLoss[0m : 1.97344
[1mStep[0m  [40/53], [94mLoss[0m : 2.39283
[1mStep[0m  [45/53], [94mLoss[0m : 2.43758
[1mStep[0m  [50/53], [94mLoss[0m : 2.27988

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.268, [92mTest[0m: 2.504, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.13491
[1mStep[0m  [5/53], [94mLoss[0m : 2.29120
[1mStep[0m  [10/53], [94mLoss[0m : 2.19707
[1mStep[0m  [15/53], [94mLoss[0m : 2.41647
[1mStep[0m  [20/53], [94mLoss[0m : 2.13258
[1mStep[0m  [25/53], [94mLoss[0m : 2.30199
[1mStep[0m  [30/53], [94mLoss[0m : 2.14734
[1mStep[0m  [35/53], [94mLoss[0m : 2.40521
[1mStep[0m  [40/53], [94mLoss[0m : 2.35825
[1mStep[0m  [45/53], [94mLoss[0m : 2.46677
[1mStep[0m  [50/53], [94mLoss[0m : 2.28879

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.257, [92mTest[0m: 2.479, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.32246
[1mStep[0m  [5/53], [94mLoss[0m : 2.15761
[1mStep[0m  [10/53], [94mLoss[0m : 2.39212
[1mStep[0m  [15/53], [94mLoss[0m : 2.42086
[1mStep[0m  [20/53], [94mLoss[0m : 2.28842
[1mStep[0m  [25/53], [94mLoss[0m : 2.25281
[1mStep[0m  [30/53], [94mLoss[0m : 2.27037
[1mStep[0m  [35/53], [94mLoss[0m : 2.18440
[1mStep[0m  [40/53], [94mLoss[0m : 2.13783
[1mStep[0m  [45/53], [94mLoss[0m : 2.18630
[1mStep[0m  [50/53], [94mLoss[0m : 2.10586

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.257, [92mTest[0m: 2.477, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.08015
[1mStep[0m  [5/53], [94mLoss[0m : 2.27546
[1mStep[0m  [10/53], [94mLoss[0m : 2.33006
[1mStep[0m  [15/53], [94mLoss[0m : 2.29578
[1mStep[0m  [20/53], [94mLoss[0m : 2.27278
[1mStep[0m  [25/53], [94mLoss[0m : 2.21802
[1mStep[0m  [30/53], [94mLoss[0m : 2.14084
[1mStep[0m  [35/53], [94mLoss[0m : 2.41831
[1mStep[0m  [40/53], [94mLoss[0m : 2.08786
[1mStep[0m  [45/53], [94mLoss[0m : 2.17244
[1mStep[0m  [50/53], [94mLoss[0m : 2.30899

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.236, [92mTest[0m: 2.448, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.433
====================================

Phase 2 - Evaluation MAE:  2.433039215894846
MAE score P1      2.437651
MAE score P2      2.433039
loss               2.23595
learning_rate     0.002575
batch_size             256
hidden_sizes         [250]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.4
momentum               0.1
weight_decay          0.01
Name: 11, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/53], [94mLoss[0m : 10.87599
[1mStep[0m  [5/53], [94mLoss[0m : 10.21946
[1mStep[0m  [10/53], [94mLoss[0m : 9.18912
[1mStep[0m  [15/53], [94mLoss[0m : 8.05465
[1mStep[0m  [20/53], [94mLoss[0m : 6.61518
[1mStep[0m  [25/53], [94mLoss[0m : 4.89469
[1mStep[0m  [30/53], [94mLoss[0m : 3.50132
[1mStep[0m  [35/53], [94mLoss[0m : 3.54964
[1mStep[0m  [40/53], [94mLoss[0m : 3.03109
[1mStep[0m  [45/53], [94mLoss[0m : 2.85005
[1mStep[0m  [50/53], [94mLoss[0m : 2.93384

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 5.785, [92mTest[0m: 10.742, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.86234
[1mStep[0m  [5/53], [94mLoss[0m : 2.77053
[1mStep[0m  [10/53], [94mLoss[0m : 2.81185
[1mStep[0m  [15/53], [94mLoss[0m : 2.73385
[1mStep[0m  [20/53], [94mLoss[0m : 2.71289
[1mStep[0m  [25/53], [94mLoss[0m : 2.48684
[1mStep[0m  [30/53], [94mLoss[0m : 2.71694
[1mStep[0m  [35/53], [94mLoss[0m : 2.61500
[1mStep[0m  [40/53], [94mLoss[0m : 2.90140
[1mStep[0m  [45/53], [94mLoss[0m : 2.58726
[1mStep[0m  [50/53], [94mLoss[0m : 2.72192

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.713, [92mTest[0m: 2.608, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.70438
[1mStep[0m  [5/53], [94mLoss[0m : 2.62370
[1mStep[0m  [10/53], [94mLoss[0m : 2.62844
[1mStep[0m  [15/53], [94mLoss[0m : 2.68443
[1mStep[0m  [20/53], [94mLoss[0m : 2.70114
[1mStep[0m  [25/53], [94mLoss[0m : 2.97461
[1mStep[0m  [30/53], [94mLoss[0m : 2.56871
[1mStep[0m  [35/53], [94mLoss[0m : 2.51794
[1mStep[0m  [40/53], [94mLoss[0m : 2.68006
[1mStep[0m  [45/53], [94mLoss[0m : 2.62173
[1mStep[0m  [50/53], [94mLoss[0m : 2.67670

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.689, [92mTest[0m: 2.475, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.54625
[1mStep[0m  [5/53], [94mLoss[0m : 2.59131
[1mStep[0m  [10/53], [94mLoss[0m : 2.81464
[1mStep[0m  [15/53], [94mLoss[0m : 2.59623
[1mStep[0m  [20/53], [94mLoss[0m : 2.75070
[1mStep[0m  [25/53], [94mLoss[0m : 2.43778
[1mStep[0m  [30/53], [94mLoss[0m : 2.63398
[1mStep[0m  [35/53], [94mLoss[0m : 2.61326
[1mStep[0m  [40/53], [94mLoss[0m : 2.70530
[1mStep[0m  [45/53], [94mLoss[0m : 2.67563
[1mStep[0m  [50/53], [94mLoss[0m : 2.68123

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.665, [92mTest[0m: 2.451, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.52327
[1mStep[0m  [5/53], [94mLoss[0m : 2.47668
[1mStep[0m  [10/53], [94mLoss[0m : 2.66199
[1mStep[0m  [15/53], [94mLoss[0m : 2.64016
[1mStep[0m  [20/53], [94mLoss[0m : 2.52180
[1mStep[0m  [25/53], [94mLoss[0m : 2.87476
[1mStep[0m  [30/53], [94mLoss[0m : 2.71789
[1mStep[0m  [35/53], [94mLoss[0m : 2.64185
[1mStep[0m  [40/53], [94mLoss[0m : 2.62514
[1mStep[0m  [45/53], [94mLoss[0m : 2.81097
[1mStep[0m  [50/53], [94mLoss[0m : 2.82634

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.635, [92mTest[0m: 2.439, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.78325
[1mStep[0m  [5/53], [94mLoss[0m : 2.60659
[1mStep[0m  [10/53], [94mLoss[0m : 2.75508
[1mStep[0m  [15/53], [94mLoss[0m : 2.51057
[1mStep[0m  [20/53], [94mLoss[0m : 2.62119
[1mStep[0m  [25/53], [94mLoss[0m : 2.70398
[1mStep[0m  [30/53], [94mLoss[0m : 2.68185
[1mStep[0m  [35/53], [94mLoss[0m : 2.60140
[1mStep[0m  [40/53], [94mLoss[0m : 2.63569
[1mStep[0m  [45/53], [94mLoss[0m : 2.63593
[1mStep[0m  [50/53], [94mLoss[0m : 2.57256

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.633, [92mTest[0m: 2.434, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.51782
[1mStep[0m  [5/53], [94mLoss[0m : 2.68474
[1mStep[0m  [10/53], [94mLoss[0m : 2.88585
[1mStep[0m  [15/53], [94mLoss[0m : 2.75466
[1mStep[0m  [20/53], [94mLoss[0m : 2.57714
[1mStep[0m  [25/53], [94mLoss[0m : 2.69165
[1mStep[0m  [30/53], [94mLoss[0m : 2.71055
[1mStep[0m  [35/53], [94mLoss[0m : 2.46130
[1mStep[0m  [40/53], [94mLoss[0m : 2.56910
[1mStep[0m  [45/53], [94mLoss[0m : 2.71525
[1mStep[0m  [50/53], [94mLoss[0m : 2.75332

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.633, [92mTest[0m: 2.426, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.73128
[1mStep[0m  [5/53], [94mLoss[0m : 2.66253
[1mStep[0m  [10/53], [94mLoss[0m : 2.53511
[1mStep[0m  [15/53], [94mLoss[0m : 2.78896
[1mStep[0m  [20/53], [94mLoss[0m : 2.73086
[1mStep[0m  [25/53], [94mLoss[0m : 2.76169
[1mStep[0m  [30/53], [94mLoss[0m : 2.46624
[1mStep[0m  [35/53], [94mLoss[0m : 2.68430
[1mStep[0m  [40/53], [94mLoss[0m : 2.67655
[1mStep[0m  [45/53], [94mLoss[0m : 2.48344
[1mStep[0m  [50/53], [94mLoss[0m : 2.21071

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.617, [92mTest[0m: 2.428, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.55619
[1mStep[0m  [5/53], [94mLoss[0m : 2.51338
[1mStep[0m  [10/53], [94mLoss[0m : 2.77904
[1mStep[0m  [15/53], [94mLoss[0m : 2.60580
[1mStep[0m  [20/53], [94mLoss[0m : 2.59080
[1mStep[0m  [25/53], [94mLoss[0m : 2.77527
[1mStep[0m  [30/53], [94mLoss[0m : 2.40259
[1mStep[0m  [35/53], [94mLoss[0m : 2.83736
[1mStep[0m  [40/53], [94mLoss[0m : 2.75334
[1mStep[0m  [45/53], [94mLoss[0m : 2.65657
[1mStep[0m  [50/53], [94mLoss[0m : 2.66846

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.618, [92mTest[0m: 2.421, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.46592
[1mStep[0m  [5/53], [94mLoss[0m : 2.63242
[1mStep[0m  [10/53], [94mLoss[0m : 2.47517
[1mStep[0m  [15/53], [94mLoss[0m : 2.60567
[1mStep[0m  [20/53], [94mLoss[0m : 2.37059
[1mStep[0m  [25/53], [94mLoss[0m : 2.68312
[1mStep[0m  [30/53], [94mLoss[0m : 2.85345
[1mStep[0m  [35/53], [94mLoss[0m : 2.76782
[1mStep[0m  [40/53], [94mLoss[0m : 2.71740
[1mStep[0m  [45/53], [94mLoss[0m : 2.61324
[1mStep[0m  [50/53], [94mLoss[0m : 2.68054

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.617, [92mTest[0m: 2.416, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.62574
[1mStep[0m  [5/53], [94mLoss[0m : 2.37171
[1mStep[0m  [10/53], [94mLoss[0m : 2.80338
[1mStep[0m  [15/53], [94mLoss[0m : 2.69819
[1mStep[0m  [20/53], [94mLoss[0m : 2.57012
[1mStep[0m  [25/53], [94mLoss[0m : 2.53106
[1mStep[0m  [30/53], [94mLoss[0m : 2.53895
[1mStep[0m  [35/53], [94mLoss[0m : 2.61876
[1mStep[0m  [40/53], [94mLoss[0m : 2.50287
[1mStep[0m  [45/53], [94mLoss[0m : 2.54811
[1mStep[0m  [50/53], [94mLoss[0m : 2.56415

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.604, [92mTest[0m: 2.422, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.53042
[1mStep[0m  [5/53], [94mLoss[0m : 2.57176
[1mStep[0m  [10/53], [94mLoss[0m : 2.60251
[1mStep[0m  [15/53], [94mLoss[0m : 2.76811
[1mStep[0m  [20/53], [94mLoss[0m : 2.50961
[1mStep[0m  [25/53], [94mLoss[0m : 2.68668
[1mStep[0m  [30/53], [94mLoss[0m : 2.67356
[1mStep[0m  [35/53], [94mLoss[0m : 2.91352
[1mStep[0m  [40/53], [94mLoss[0m : 2.79724
[1mStep[0m  [45/53], [94mLoss[0m : 2.44874
[1mStep[0m  [50/53], [94mLoss[0m : 2.74538

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.600, [92mTest[0m: 2.416, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.77797
[1mStep[0m  [5/53], [94mLoss[0m : 2.74789
[1mStep[0m  [10/53], [94mLoss[0m : 2.84459
[1mStep[0m  [15/53], [94mLoss[0m : 2.49550
[1mStep[0m  [20/53], [94mLoss[0m : 2.87879
[1mStep[0m  [25/53], [94mLoss[0m : 2.78782
[1mStep[0m  [30/53], [94mLoss[0m : 2.50532
[1mStep[0m  [35/53], [94mLoss[0m : 2.78080
[1mStep[0m  [40/53], [94mLoss[0m : 2.68991
[1mStep[0m  [45/53], [94mLoss[0m : 2.54081
[1mStep[0m  [50/53], [94mLoss[0m : 2.83776

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.603, [92mTest[0m: 2.418, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.74090
[1mStep[0m  [5/53], [94mLoss[0m : 2.57173
[1mStep[0m  [10/53], [94mLoss[0m : 2.81372
[1mStep[0m  [15/53], [94mLoss[0m : 2.70392
[1mStep[0m  [20/53], [94mLoss[0m : 2.31270
[1mStep[0m  [25/53], [94mLoss[0m : 2.70137
[1mStep[0m  [30/53], [94mLoss[0m : 2.43292
[1mStep[0m  [35/53], [94mLoss[0m : 2.65824
[1mStep[0m  [40/53], [94mLoss[0m : 2.43406
[1mStep[0m  [45/53], [94mLoss[0m : 2.80129
[1mStep[0m  [50/53], [94mLoss[0m : 2.49468

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.598, [92mTest[0m: 2.416, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.58118
[1mStep[0m  [5/53], [94mLoss[0m : 2.44072
[1mStep[0m  [10/53], [94mLoss[0m : 2.54777
[1mStep[0m  [15/53], [94mLoss[0m : 2.75549
[1mStep[0m  [20/53], [94mLoss[0m : 2.71255
[1mStep[0m  [25/53], [94mLoss[0m : 2.52244
[1mStep[0m  [30/53], [94mLoss[0m : 2.72843
[1mStep[0m  [35/53], [94mLoss[0m : 2.71372
[1mStep[0m  [40/53], [94mLoss[0m : 2.72302
[1mStep[0m  [45/53], [94mLoss[0m : 2.65108
[1mStep[0m  [50/53], [94mLoss[0m : 2.65091

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.609, [92mTest[0m: 2.414, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.55836
[1mStep[0m  [5/53], [94mLoss[0m : 2.62879
[1mStep[0m  [10/53], [94mLoss[0m : 2.73182
[1mStep[0m  [15/53], [94mLoss[0m : 2.69292
[1mStep[0m  [20/53], [94mLoss[0m : 2.44999
[1mStep[0m  [25/53], [94mLoss[0m : 2.56212
[1mStep[0m  [30/53], [94mLoss[0m : 2.52108
[1mStep[0m  [35/53], [94mLoss[0m : 2.78607
[1mStep[0m  [40/53], [94mLoss[0m : 2.71519
[1mStep[0m  [45/53], [94mLoss[0m : 2.76920
[1mStep[0m  [50/53], [94mLoss[0m : 2.62302

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.618, [92mTest[0m: 2.408, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.84840
[1mStep[0m  [5/53], [94mLoss[0m : 2.63228
[1mStep[0m  [10/53], [94mLoss[0m : 2.70782
[1mStep[0m  [15/53], [94mLoss[0m : 2.63699
[1mStep[0m  [20/53], [94mLoss[0m : 2.60153
[1mStep[0m  [25/53], [94mLoss[0m : 2.63681
[1mStep[0m  [30/53], [94mLoss[0m : 2.50225
[1mStep[0m  [35/53], [94mLoss[0m : 2.54770
[1mStep[0m  [40/53], [94mLoss[0m : 2.39469
[1mStep[0m  [45/53], [94mLoss[0m : 2.46643
[1mStep[0m  [50/53], [94mLoss[0m : 2.41327

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.590, [92mTest[0m: 2.412, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.58877
[1mStep[0m  [5/53], [94mLoss[0m : 2.59858
[1mStep[0m  [10/53], [94mLoss[0m : 2.68625
[1mStep[0m  [15/53], [94mLoss[0m : 2.59445
[1mStep[0m  [20/53], [94mLoss[0m : 2.85389
[1mStep[0m  [25/53], [94mLoss[0m : 2.76744
[1mStep[0m  [30/53], [94mLoss[0m : 2.64526
[1mStep[0m  [35/53], [94mLoss[0m : 2.73450
[1mStep[0m  [40/53], [94mLoss[0m : 2.32774
[1mStep[0m  [45/53], [94mLoss[0m : 2.68275
[1mStep[0m  [50/53], [94mLoss[0m : 2.74062

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.572, [92mTest[0m: 2.404, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.54371
[1mStep[0m  [5/53], [94mLoss[0m : 2.70680
[1mStep[0m  [10/53], [94mLoss[0m : 2.70690
[1mStep[0m  [15/53], [94mLoss[0m : 2.90428
[1mStep[0m  [20/53], [94mLoss[0m : 2.48324
[1mStep[0m  [25/53], [94mLoss[0m : 2.41032
[1mStep[0m  [30/53], [94mLoss[0m : 2.58116
[1mStep[0m  [35/53], [94mLoss[0m : 2.78158
[1mStep[0m  [40/53], [94mLoss[0m : 2.67958
[1mStep[0m  [45/53], [94mLoss[0m : 2.57607
[1mStep[0m  [50/53], [94mLoss[0m : 2.51736

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.590, [92mTest[0m: 2.411, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.45689
[1mStep[0m  [5/53], [94mLoss[0m : 2.35359
[1mStep[0m  [10/53], [94mLoss[0m : 2.57633
[1mStep[0m  [15/53], [94mLoss[0m : 2.52621
[1mStep[0m  [20/53], [94mLoss[0m : 2.56657
[1mStep[0m  [25/53], [94mLoss[0m : 2.62575
[1mStep[0m  [30/53], [94mLoss[0m : 2.89150
[1mStep[0m  [35/53], [94mLoss[0m : 2.50383
[1mStep[0m  [40/53], [94mLoss[0m : 2.52133
[1mStep[0m  [45/53], [94mLoss[0m : 2.48963
[1mStep[0m  [50/53], [94mLoss[0m : 2.69576

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.590, [92mTest[0m: 2.415, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.53909
[1mStep[0m  [5/53], [94mLoss[0m : 2.67637
[1mStep[0m  [10/53], [94mLoss[0m : 2.68201
[1mStep[0m  [15/53], [94mLoss[0m : 2.74458
[1mStep[0m  [20/53], [94mLoss[0m : 2.39244
[1mStep[0m  [25/53], [94mLoss[0m : 2.69277
[1mStep[0m  [30/53], [94mLoss[0m : 2.73320
[1mStep[0m  [35/53], [94mLoss[0m : 2.79168
[1mStep[0m  [40/53], [94mLoss[0m : 2.29672
[1mStep[0m  [45/53], [94mLoss[0m : 2.71049
[1mStep[0m  [50/53], [94mLoss[0m : 2.72722

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.584, [92mTest[0m: 2.406, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.64372
[1mStep[0m  [5/53], [94mLoss[0m : 2.63984
[1mStep[0m  [10/53], [94mLoss[0m : 2.72983
[1mStep[0m  [15/53], [94mLoss[0m : 2.48430
[1mStep[0m  [20/53], [94mLoss[0m : 2.51182
[1mStep[0m  [25/53], [94mLoss[0m : 2.39323
[1mStep[0m  [30/53], [94mLoss[0m : 2.46509
[1mStep[0m  [35/53], [94mLoss[0m : 2.39528
[1mStep[0m  [40/53], [94mLoss[0m : 2.44260
[1mStep[0m  [45/53], [94mLoss[0m : 2.42511
[1mStep[0m  [50/53], [94mLoss[0m : 2.52367

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.582, [92mTest[0m: 2.408, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.57381
[1mStep[0m  [5/53], [94mLoss[0m : 2.54453
[1mStep[0m  [10/53], [94mLoss[0m : 2.45931
[1mStep[0m  [15/53], [94mLoss[0m : 2.42508
[1mStep[0m  [20/53], [94mLoss[0m : 2.54923
[1mStep[0m  [25/53], [94mLoss[0m : 2.70379
[1mStep[0m  [30/53], [94mLoss[0m : 2.88444
[1mStep[0m  [35/53], [94mLoss[0m : 2.40617
[1mStep[0m  [40/53], [94mLoss[0m : 2.45099
[1mStep[0m  [45/53], [94mLoss[0m : 2.57600
[1mStep[0m  [50/53], [94mLoss[0m : 2.67647

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.583, [92mTest[0m: 2.408, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.47981
[1mStep[0m  [5/53], [94mLoss[0m : 2.44100
[1mStep[0m  [10/53], [94mLoss[0m : 2.37559
[1mStep[0m  [15/53], [94mLoss[0m : 2.60002
[1mStep[0m  [20/53], [94mLoss[0m : 2.54558
[1mStep[0m  [25/53], [94mLoss[0m : 2.60166
[1mStep[0m  [30/53], [94mLoss[0m : 2.57881
[1mStep[0m  [35/53], [94mLoss[0m : 2.51400
[1mStep[0m  [40/53], [94mLoss[0m : 2.90746
[1mStep[0m  [45/53], [94mLoss[0m : 2.74188
[1mStep[0m  [50/53], [94mLoss[0m : 2.76606

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.582, [92mTest[0m: 2.407, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.54584
[1mStep[0m  [5/53], [94mLoss[0m : 2.50583
[1mStep[0m  [10/53], [94mLoss[0m : 2.55431
[1mStep[0m  [15/53], [94mLoss[0m : 2.44147
[1mStep[0m  [20/53], [94mLoss[0m : 2.38056
[1mStep[0m  [25/53], [94mLoss[0m : 2.53783
[1mStep[0m  [30/53], [94mLoss[0m : 2.54819
[1mStep[0m  [35/53], [94mLoss[0m : 2.66706
[1mStep[0m  [40/53], [94mLoss[0m : 2.51575
[1mStep[0m  [45/53], [94mLoss[0m : 2.46245
[1mStep[0m  [50/53], [94mLoss[0m : 2.45052

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.569, [92mTest[0m: 2.406, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.52959
[1mStep[0m  [5/53], [94mLoss[0m : 2.51645
[1mStep[0m  [10/53], [94mLoss[0m : 2.45398
[1mStep[0m  [15/53], [94mLoss[0m : 2.58503
[1mStep[0m  [20/53], [94mLoss[0m : 2.40789
[1mStep[0m  [25/53], [94mLoss[0m : 2.67343
[1mStep[0m  [30/53], [94mLoss[0m : 2.50066
[1mStep[0m  [35/53], [94mLoss[0m : 2.65939
[1mStep[0m  [40/53], [94mLoss[0m : 2.67619
[1mStep[0m  [45/53], [94mLoss[0m : 2.67632
[1mStep[0m  [50/53], [94mLoss[0m : 2.41696

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.576, [92mTest[0m: 2.405, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.46826
[1mStep[0m  [5/53], [94mLoss[0m : 2.82010
[1mStep[0m  [10/53], [94mLoss[0m : 2.40264
[1mStep[0m  [15/53], [94mLoss[0m : 2.62981
[1mStep[0m  [20/53], [94mLoss[0m : 2.44179
[1mStep[0m  [25/53], [94mLoss[0m : 2.67904
[1mStep[0m  [30/53], [94mLoss[0m : 2.64214
[1mStep[0m  [35/53], [94mLoss[0m : 2.42051
[1mStep[0m  [40/53], [94mLoss[0m : 2.53616
[1mStep[0m  [45/53], [94mLoss[0m : 2.54687
[1mStep[0m  [50/53], [94mLoss[0m : 2.45343

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.556, [92mTest[0m: 2.398, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.73088
[1mStep[0m  [5/53], [94mLoss[0m : 2.63409
[1mStep[0m  [10/53], [94mLoss[0m : 2.41484
[1mStep[0m  [15/53], [94mLoss[0m : 2.54908
[1mStep[0m  [20/53], [94mLoss[0m : 2.59937
[1mStep[0m  [25/53], [94mLoss[0m : 2.49407
[1mStep[0m  [30/53], [94mLoss[0m : 2.46045
[1mStep[0m  [35/53], [94mLoss[0m : 2.61966
[1mStep[0m  [40/53], [94mLoss[0m : 2.64233
[1mStep[0m  [45/53], [94mLoss[0m : 2.65755
[1mStep[0m  [50/53], [94mLoss[0m : 2.61741

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.565, [92mTest[0m: 2.406, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.56983
[1mStep[0m  [5/53], [94mLoss[0m : 2.62483
[1mStep[0m  [10/53], [94mLoss[0m : 2.63747
[1mStep[0m  [15/53], [94mLoss[0m : 2.55289
[1mStep[0m  [20/53], [94mLoss[0m : 2.57553
[1mStep[0m  [25/53], [94mLoss[0m : 2.64988
[1mStep[0m  [30/53], [94mLoss[0m : 2.55472
[1mStep[0m  [35/53], [94mLoss[0m : 2.58440
[1mStep[0m  [40/53], [94mLoss[0m : 2.47146
[1mStep[0m  [45/53], [94mLoss[0m : 2.75283
[1mStep[0m  [50/53], [94mLoss[0m : 2.70975

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.574, [92mTest[0m: 2.404, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.46508
[1mStep[0m  [5/53], [94mLoss[0m : 2.54943
[1mStep[0m  [10/53], [94mLoss[0m : 2.46442
[1mStep[0m  [15/53], [94mLoss[0m : 2.39353
[1mStep[0m  [20/53], [94mLoss[0m : 2.54530
[1mStep[0m  [25/53], [94mLoss[0m : 2.52659
[1mStep[0m  [30/53], [94mLoss[0m : 2.52023
[1mStep[0m  [35/53], [94mLoss[0m : 2.86581
[1mStep[0m  [40/53], [94mLoss[0m : 2.59458
[1mStep[0m  [45/53], [94mLoss[0m : 2.68494
[1mStep[0m  [50/53], [94mLoss[0m : 2.59693

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.563, [92mTest[0m: 2.401, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.405
====================================

Phase 1 - Evaluation MAE:  2.4047305492254405
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/53], [94mLoss[0m : 2.65519
[1mStep[0m  [5/53], [94mLoss[0m : 2.51170
[1mStep[0m  [10/53], [94mLoss[0m : 2.46961
[1mStep[0m  [15/53], [94mLoss[0m : 2.61155
[1mStep[0m  [20/53], [94mLoss[0m : 2.45730
[1mStep[0m  [25/53], [94mLoss[0m : 2.63168
[1mStep[0m  [30/53], [94mLoss[0m : 2.54863
[1mStep[0m  [35/53], [94mLoss[0m : 2.34090
[1mStep[0m  [40/53], [94mLoss[0m : 2.78820
[1mStep[0m  [45/53], [94mLoss[0m : 2.45370
[1mStep[0m  [50/53], [94mLoss[0m : 2.57891

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.573, [92mTest[0m: 2.402, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.53082
[1mStep[0m  [5/53], [94mLoss[0m : 2.56479
[1mStep[0m  [10/53], [94mLoss[0m : 2.72808
[1mStep[0m  [15/53], [94mLoss[0m : 2.62713
[1mStep[0m  [20/53], [94mLoss[0m : 2.54568
[1mStep[0m  [25/53], [94mLoss[0m : 2.70330
[1mStep[0m  [30/53], [94mLoss[0m : 2.65657
[1mStep[0m  [35/53], [94mLoss[0m : 2.33328
[1mStep[0m  [40/53], [94mLoss[0m : 2.53873
[1mStep[0m  [45/53], [94mLoss[0m : 2.31920
[1mStep[0m  [50/53], [94mLoss[0m : 2.75691

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.547, [92mTest[0m: 2.398, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.44633
[1mStep[0m  [5/53], [94mLoss[0m : 2.47265
[1mStep[0m  [10/53], [94mLoss[0m : 2.48127
[1mStep[0m  [15/53], [94mLoss[0m : 2.67250
[1mStep[0m  [20/53], [94mLoss[0m : 2.64950
[1mStep[0m  [25/53], [94mLoss[0m : 2.30411
[1mStep[0m  [30/53], [94mLoss[0m : 2.39561
[1mStep[0m  [35/53], [94mLoss[0m : 2.54950
[1mStep[0m  [40/53], [94mLoss[0m : 2.39398
[1mStep[0m  [45/53], [94mLoss[0m : 2.74829
[1mStep[0m  [50/53], [94mLoss[0m : 2.36450

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.373, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.44326
[1mStep[0m  [5/53], [94mLoss[0m : 2.38215
[1mStep[0m  [10/53], [94mLoss[0m : 2.43414
[1mStep[0m  [15/53], [94mLoss[0m : 2.55016
[1mStep[0m  [20/53], [94mLoss[0m : 2.47260
[1mStep[0m  [25/53], [94mLoss[0m : 2.58624
[1mStep[0m  [30/53], [94mLoss[0m : 2.53685
[1mStep[0m  [35/53], [94mLoss[0m : 2.60057
[1mStep[0m  [40/53], [94mLoss[0m : 2.48091
[1mStep[0m  [45/53], [94mLoss[0m : 2.35834
[1mStep[0m  [50/53], [94mLoss[0m : 2.60336

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.382, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.22168
[1mStep[0m  [5/53], [94mLoss[0m : 2.52601
[1mStep[0m  [10/53], [94mLoss[0m : 2.53564
[1mStep[0m  [15/53], [94mLoss[0m : 2.56948
[1mStep[0m  [20/53], [94mLoss[0m : 2.48407
[1mStep[0m  [25/53], [94mLoss[0m : 2.43989
[1mStep[0m  [30/53], [94mLoss[0m : 2.44959
[1mStep[0m  [35/53], [94mLoss[0m : 2.72218
[1mStep[0m  [40/53], [94mLoss[0m : 2.50506
[1mStep[0m  [45/53], [94mLoss[0m : 2.32659
[1mStep[0m  [50/53], [94mLoss[0m : 2.54536

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.373, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.41248
[1mStep[0m  [5/53], [94mLoss[0m : 2.34392
[1mStep[0m  [10/53], [94mLoss[0m : 2.34692
[1mStep[0m  [15/53], [94mLoss[0m : 2.26701
[1mStep[0m  [20/53], [94mLoss[0m : 2.42335
[1mStep[0m  [25/53], [94mLoss[0m : 2.42476
[1mStep[0m  [30/53], [94mLoss[0m : 2.32074
[1mStep[0m  [35/53], [94mLoss[0m : 2.43333
[1mStep[0m  [40/53], [94mLoss[0m : 2.51296
[1mStep[0m  [45/53], [94mLoss[0m : 2.33031
[1mStep[0m  [50/53], [94mLoss[0m : 2.39198

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.366, [92mTest[0m: 2.372, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.35939
[1mStep[0m  [5/53], [94mLoss[0m : 2.13789
[1mStep[0m  [10/53], [94mLoss[0m : 2.27372
[1mStep[0m  [15/53], [94mLoss[0m : 2.21085
[1mStep[0m  [20/53], [94mLoss[0m : 2.34753
[1mStep[0m  [25/53], [94mLoss[0m : 2.33001
[1mStep[0m  [30/53], [94mLoss[0m : 2.22851
[1mStep[0m  [35/53], [94mLoss[0m : 2.30692
[1mStep[0m  [40/53], [94mLoss[0m : 2.37966
[1mStep[0m  [45/53], [94mLoss[0m : 2.52585
[1mStep[0m  [50/53], [94mLoss[0m : 2.49044

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.324, [92mTest[0m: 2.365, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.20397
[1mStep[0m  [5/53], [94mLoss[0m : 2.37683
[1mStep[0m  [10/53], [94mLoss[0m : 2.31630
[1mStep[0m  [15/53], [94mLoss[0m : 2.28954
[1mStep[0m  [20/53], [94mLoss[0m : 2.26605
[1mStep[0m  [25/53], [94mLoss[0m : 2.16048
[1mStep[0m  [30/53], [94mLoss[0m : 2.28470
[1mStep[0m  [35/53], [94mLoss[0m : 2.54208
[1mStep[0m  [40/53], [94mLoss[0m : 2.07865
[1mStep[0m  [45/53], [94mLoss[0m : 2.21770
[1mStep[0m  [50/53], [94mLoss[0m : 2.42520

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.288, [92mTest[0m: 2.369, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.31326
[1mStep[0m  [5/53], [94mLoss[0m : 2.24470
[1mStep[0m  [10/53], [94mLoss[0m : 2.42967
[1mStep[0m  [15/53], [94mLoss[0m : 2.28210
[1mStep[0m  [20/53], [94mLoss[0m : 2.21788
[1mStep[0m  [25/53], [94mLoss[0m : 2.19921
[1mStep[0m  [30/53], [94mLoss[0m : 2.31403
[1mStep[0m  [35/53], [94mLoss[0m : 2.23112
[1mStep[0m  [40/53], [94mLoss[0m : 2.24407
[1mStep[0m  [45/53], [94mLoss[0m : 2.05036
[1mStep[0m  [50/53], [94mLoss[0m : 2.29584

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.229, [92mTest[0m: 2.364, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.21068
[1mStep[0m  [5/53], [94mLoss[0m : 2.02783
[1mStep[0m  [10/53], [94mLoss[0m : 2.22424
[1mStep[0m  [15/53], [94mLoss[0m : 2.09847
[1mStep[0m  [20/53], [94mLoss[0m : 2.06775
[1mStep[0m  [25/53], [94mLoss[0m : 2.17004
[1mStep[0m  [30/53], [94mLoss[0m : 2.38021
[1mStep[0m  [35/53], [94mLoss[0m : 2.26473
[1mStep[0m  [40/53], [94mLoss[0m : 2.06211
[1mStep[0m  [45/53], [94mLoss[0m : 2.16473
[1mStep[0m  [50/53], [94mLoss[0m : 1.95900

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.171, [92mTest[0m: 2.416, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.11502
[1mStep[0m  [5/53], [94mLoss[0m : 2.19470
[1mStep[0m  [10/53], [94mLoss[0m : 2.13759
[1mStep[0m  [15/53], [94mLoss[0m : 2.14684
[1mStep[0m  [20/53], [94mLoss[0m : 2.09100
[1mStep[0m  [25/53], [94mLoss[0m : 2.11559
[1mStep[0m  [30/53], [94mLoss[0m : 1.89491
[1mStep[0m  [35/53], [94mLoss[0m : 2.26311
[1mStep[0m  [40/53], [94mLoss[0m : 2.19218
[1mStep[0m  [45/53], [94mLoss[0m : 2.10687
[1mStep[0m  [50/53], [94mLoss[0m : 2.34318

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.161, [92mTest[0m: 2.390, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.06975
[1mStep[0m  [5/53], [94mLoss[0m : 2.29937
[1mStep[0m  [10/53], [94mLoss[0m : 2.12012
[1mStep[0m  [15/53], [94mLoss[0m : 2.16084
[1mStep[0m  [20/53], [94mLoss[0m : 2.10381
[1mStep[0m  [25/53], [94mLoss[0m : 2.16483
[1mStep[0m  [30/53], [94mLoss[0m : 2.43012
[1mStep[0m  [35/53], [94mLoss[0m : 2.04049
[1mStep[0m  [40/53], [94mLoss[0m : 2.24304
[1mStep[0m  [45/53], [94mLoss[0m : 2.10698
[1mStep[0m  [50/53], [94mLoss[0m : 2.18196

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.136, [92mTest[0m: 2.389, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.18335
[1mStep[0m  [5/53], [94mLoss[0m : 1.93977
[1mStep[0m  [10/53], [94mLoss[0m : 2.24060
[1mStep[0m  [15/53], [94mLoss[0m : 1.99815
[1mStep[0m  [20/53], [94mLoss[0m : 2.05593
[1mStep[0m  [25/53], [94mLoss[0m : 2.17797
[1mStep[0m  [30/53], [94mLoss[0m : 2.05775
[1mStep[0m  [35/53], [94mLoss[0m : 2.16381
[1mStep[0m  [40/53], [94mLoss[0m : 2.24798
[1mStep[0m  [45/53], [94mLoss[0m : 2.22897
[1mStep[0m  [50/53], [94mLoss[0m : 2.30376

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.110, [92mTest[0m: 2.441, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.98014
[1mStep[0m  [5/53], [94mLoss[0m : 1.98139
[1mStep[0m  [10/53], [94mLoss[0m : 1.91141
[1mStep[0m  [15/53], [94mLoss[0m : 2.00759
[1mStep[0m  [20/53], [94mLoss[0m : 2.01097
[1mStep[0m  [25/53], [94mLoss[0m : 1.84843
[1mStep[0m  [30/53], [94mLoss[0m : 2.14309
[1mStep[0m  [35/53], [94mLoss[0m : 2.04388
[1mStep[0m  [40/53], [94mLoss[0m : 2.07025
[1mStep[0m  [45/53], [94mLoss[0m : 2.20172
[1mStep[0m  [50/53], [94mLoss[0m : 2.03216

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.060, [92mTest[0m: 2.419, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.09090
[1mStep[0m  [5/53], [94mLoss[0m : 1.81542
[1mStep[0m  [10/53], [94mLoss[0m : 1.88522
[1mStep[0m  [15/53], [94mLoss[0m : 2.10143
[1mStep[0m  [20/53], [94mLoss[0m : 2.05737
[1mStep[0m  [25/53], [94mLoss[0m : 2.03676
[1mStep[0m  [30/53], [94mLoss[0m : 2.04657
[1mStep[0m  [35/53], [94mLoss[0m : 2.04502
[1mStep[0m  [40/53], [94mLoss[0m : 1.85071
[1mStep[0m  [45/53], [94mLoss[0m : 2.08799
[1mStep[0m  [50/53], [94mLoss[0m : 2.06592

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.028, [92mTest[0m: 2.507, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.80705
[1mStep[0m  [5/53], [94mLoss[0m : 1.95819
[1mStep[0m  [10/53], [94mLoss[0m : 2.11955
[1mStep[0m  [15/53], [94mLoss[0m : 2.12730
[1mStep[0m  [20/53], [94mLoss[0m : 1.98172
[1mStep[0m  [25/53], [94mLoss[0m : 2.18561
[1mStep[0m  [30/53], [94mLoss[0m : 2.08676
[1mStep[0m  [35/53], [94mLoss[0m : 2.09504
[1mStep[0m  [40/53], [94mLoss[0m : 1.85244
[1mStep[0m  [45/53], [94mLoss[0m : 2.20612
[1mStep[0m  [50/53], [94mLoss[0m : 2.30455

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.015, [92mTest[0m: 2.442, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.92668
[1mStep[0m  [5/53], [94mLoss[0m : 2.15967
[1mStep[0m  [10/53], [94mLoss[0m : 2.08270
[1mStep[0m  [15/53], [94mLoss[0m : 2.09203
[1mStep[0m  [20/53], [94mLoss[0m : 2.09853
[1mStep[0m  [25/53], [94mLoss[0m : 1.90064
[1mStep[0m  [30/53], [94mLoss[0m : 2.04071
[1mStep[0m  [35/53], [94mLoss[0m : 2.12605
[1mStep[0m  [40/53], [94mLoss[0m : 1.96366
[1mStep[0m  [45/53], [94mLoss[0m : 2.02835
[1mStep[0m  [50/53], [94mLoss[0m : 1.98108

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.990, [92mTest[0m: 2.428, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.76664
[1mStep[0m  [5/53], [94mLoss[0m : 1.94685
[1mStep[0m  [10/53], [94mLoss[0m : 1.79414
[1mStep[0m  [15/53], [94mLoss[0m : 1.68933
[1mStep[0m  [20/53], [94mLoss[0m : 2.11703
[1mStep[0m  [25/53], [94mLoss[0m : 1.89023
[1mStep[0m  [30/53], [94mLoss[0m : 1.98797
[1mStep[0m  [35/53], [94mLoss[0m : 2.05795
[1mStep[0m  [40/53], [94mLoss[0m : 1.93607
[1mStep[0m  [45/53], [94mLoss[0m : 1.88521
[1mStep[0m  [50/53], [94mLoss[0m : 2.18148

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.976, [92mTest[0m: 2.496, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.80822
[1mStep[0m  [5/53], [94mLoss[0m : 1.96659
[1mStep[0m  [10/53], [94mLoss[0m : 2.02433
[1mStep[0m  [15/53], [94mLoss[0m : 2.02063
[1mStep[0m  [20/53], [94mLoss[0m : 1.92904
[1mStep[0m  [25/53], [94mLoss[0m : 1.93470
[1mStep[0m  [30/53], [94mLoss[0m : 1.74686
[1mStep[0m  [35/53], [94mLoss[0m : 2.06610
[1mStep[0m  [40/53], [94mLoss[0m : 1.97111
[1mStep[0m  [45/53], [94mLoss[0m : 1.86952
[1mStep[0m  [50/53], [94mLoss[0m : 2.08086

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.937, [92mTest[0m: 2.431, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.97887
[1mStep[0m  [5/53], [94mLoss[0m : 1.85290
[1mStep[0m  [10/53], [94mLoss[0m : 1.88936
[1mStep[0m  [15/53], [94mLoss[0m : 1.92757
[1mStep[0m  [20/53], [94mLoss[0m : 2.02496
[1mStep[0m  [25/53], [94mLoss[0m : 2.04572
[1mStep[0m  [30/53], [94mLoss[0m : 1.83021
[1mStep[0m  [35/53], [94mLoss[0m : 1.82753
[1mStep[0m  [40/53], [94mLoss[0m : 1.90958
[1mStep[0m  [45/53], [94mLoss[0m : 1.90979
[1mStep[0m  [50/53], [94mLoss[0m : 1.90422

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.925, [92mTest[0m: 2.441, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.88024
[1mStep[0m  [5/53], [94mLoss[0m : 1.99653
[1mStep[0m  [10/53], [94mLoss[0m : 1.67931
[1mStep[0m  [15/53], [94mLoss[0m : 1.85436
[1mStep[0m  [20/53], [94mLoss[0m : 1.71404
[1mStep[0m  [25/53], [94mLoss[0m : 1.95437
[1mStep[0m  [30/53], [94mLoss[0m : 1.97091
[1mStep[0m  [35/53], [94mLoss[0m : 1.87019
[1mStep[0m  [40/53], [94mLoss[0m : 1.87263
[1mStep[0m  [45/53], [94mLoss[0m : 1.99877
[1mStep[0m  [50/53], [94mLoss[0m : 1.81757

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.889, [92mTest[0m: 2.466, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.86047
[1mStep[0m  [5/53], [94mLoss[0m : 1.84406
[1mStep[0m  [10/53], [94mLoss[0m : 1.89072
[1mStep[0m  [15/53], [94mLoss[0m : 2.03320
[1mStep[0m  [20/53], [94mLoss[0m : 1.87243
[1mStep[0m  [25/53], [94mLoss[0m : 1.84958
[1mStep[0m  [30/53], [94mLoss[0m : 1.91614
[1mStep[0m  [35/53], [94mLoss[0m : 1.83592
[1mStep[0m  [40/53], [94mLoss[0m : 1.96238
[1mStep[0m  [45/53], [94mLoss[0m : 2.16727
[1mStep[0m  [50/53], [94mLoss[0m : 1.97814

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.871, [92mTest[0m: 2.415, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.83951
[1mStep[0m  [5/53], [94mLoss[0m : 1.85622
[1mStep[0m  [10/53], [94mLoss[0m : 1.73141
[1mStep[0m  [15/53], [94mLoss[0m : 1.58279
[1mStep[0m  [20/53], [94mLoss[0m : 1.83980
[1mStep[0m  [25/53], [94mLoss[0m : 1.92248
[1mStep[0m  [30/53], [94mLoss[0m : 1.99580
[1mStep[0m  [35/53], [94mLoss[0m : 2.14447
[1mStep[0m  [40/53], [94mLoss[0m : 1.95568
[1mStep[0m  [45/53], [94mLoss[0m : 1.98556
[1mStep[0m  [50/53], [94mLoss[0m : 1.90970

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.858, [92mTest[0m: 2.396, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.68564
[1mStep[0m  [5/53], [94mLoss[0m : 1.98256
[1mStep[0m  [10/53], [94mLoss[0m : 1.87085
[1mStep[0m  [15/53], [94mLoss[0m : 1.99440
[1mStep[0m  [20/53], [94mLoss[0m : 1.80726
[1mStep[0m  [25/53], [94mLoss[0m : 1.75872
[1mStep[0m  [30/53], [94mLoss[0m : 1.86528
[1mStep[0m  [35/53], [94mLoss[0m : 2.05111
[1mStep[0m  [40/53], [94mLoss[0m : 1.95326
[1mStep[0m  [45/53], [94mLoss[0m : 1.81205
[1mStep[0m  [50/53], [94mLoss[0m : 1.83891

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.871, [92mTest[0m: 2.436, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.67941
[1mStep[0m  [5/53], [94mLoss[0m : 1.77357
[1mStep[0m  [10/53], [94mLoss[0m : 1.69486
[1mStep[0m  [15/53], [94mLoss[0m : 1.76469
[1mStep[0m  [20/53], [94mLoss[0m : 1.63887
[1mStep[0m  [25/53], [94mLoss[0m : 1.91879
[1mStep[0m  [30/53], [94mLoss[0m : 1.82476
[1mStep[0m  [35/53], [94mLoss[0m : 1.82170
[1mStep[0m  [40/53], [94mLoss[0m : 1.98250
[1mStep[0m  [45/53], [94mLoss[0m : 1.89784
[1mStep[0m  [50/53], [94mLoss[0m : 1.83826

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.831, [92mTest[0m: 2.459, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.87776
[1mStep[0m  [5/53], [94mLoss[0m : 1.63785
[1mStep[0m  [10/53], [94mLoss[0m : 1.82494
[1mStep[0m  [15/53], [94mLoss[0m : 1.91385
[1mStep[0m  [20/53], [94mLoss[0m : 1.86484
[1mStep[0m  [25/53], [94mLoss[0m : 1.81394
[1mStep[0m  [30/53], [94mLoss[0m : 1.92287
[1mStep[0m  [35/53], [94mLoss[0m : 1.80223
[1mStep[0m  [40/53], [94mLoss[0m : 1.76582
[1mStep[0m  [45/53], [94mLoss[0m : 1.82980
[1mStep[0m  [50/53], [94mLoss[0m : 1.77216

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.828, [92mTest[0m: 2.480, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.84848
[1mStep[0m  [5/53], [94mLoss[0m : 1.69796
[1mStep[0m  [10/53], [94mLoss[0m : 1.78684
[1mStep[0m  [15/53], [94mLoss[0m : 1.74053
[1mStep[0m  [20/53], [94mLoss[0m : 1.83821
[1mStep[0m  [25/53], [94mLoss[0m : 1.79182
[1mStep[0m  [30/53], [94mLoss[0m : 1.75201
[1mStep[0m  [35/53], [94mLoss[0m : 1.85262
[1mStep[0m  [40/53], [94mLoss[0m : 1.70618
[1mStep[0m  [45/53], [94mLoss[0m : 1.74160
[1mStep[0m  [50/53], [94mLoss[0m : 1.77263

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.808, [92mTest[0m: 2.439, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.78614
[1mStep[0m  [5/53], [94mLoss[0m : 1.85959
[1mStep[0m  [10/53], [94mLoss[0m : 1.79154
[1mStep[0m  [15/53], [94mLoss[0m : 1.79803
[1mStep[0m  [20/53], [94mLoss[0m : 1.70739
[1mStep[0m  [25/53], [94mLoss[0m : 1.73231
[1mStep[0m  [30/53], [94mLoss[0m : 1.79032
[1mStep[0m  [35/53], [94mLoss[0m : 1.92172
[1mStep[0m  [40/53], [94mLoss[0m : 1.74289
[1mStep[0m  [45/53], [94mLoss[0m : 1.93805
[1mStep[0m  [50/53], [94mLoss[0m : 1.90152

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.808, [92mTest[0m: 2.489, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.85797
[1mStep[0m  [5/53], [94mLoss[0m : 1.67482
[1mStep[0m  [10/53], [94mLoss[0m : 1.83873
[1mStep[0m  [15/53], [94mLoss[0m : 1.81544
[1mStep[0m  [20/53], [94mLoss[0m : 1.58410
[1mStep[0m  [25/53], [94mLoss[0m : 1.86702
[1mStep[0m  [30/53], [94mLoss[0m : 1.61221
[1mStep[0m  [35/53], [94mLoss[0m : 1.86383
[1mStep[0m  [40/53], [94mLoss[0m : 1.79747
[1mStep[0m  [45/53], [94mLoss[0m : 1.85424
[1mStep[0m  [50/53], [94mLoss[0m : 1.88088

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.778, [92mTest[0m: 2.477, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.79623
[1mStep[0m  [5/53], [94mLoss[0m : 1.64798
[1mStep[0m  [10/53], [94mLoss[0m : 1.65559
[1mStep[0m  [15/53], [94mLoss[0m : 1.82263
[1mStep[0m  [20/53], [94mLoss[0m : 1.68798
[1mStep[0m  [25/53], [94mLoss[0m : 1.82918
[1mStep[0m  [30/53], [94mLoss[0m : 1.80960
[1mStep[0m  [35/53], [94mLoss[0m : 1.78846
[1mStep[0m  [40/53], [94mLoss[0m : 1.73195
[1mStep[0m  [45/53], [94mLoss[0m : 1.76934
[1mStep[0m  [50/53], [94mLoss[0m : 1.67893

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.785, [92mTest[0m: 2.430, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.426
====================================

Phase 2 - Evaluation MAE:  2.4261010885238647
MAE score P1       2.404731
MAE score P2       2.426101
loss               1.778057
learning_rate      0.002575
batch_size              256
hidden_sizes      [200, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.9
weight_decay           0.01
Name: 12, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/26], [94mLoss[0m : 10.84632
[1mStep[0m  [2/26], [94mLoss[0m : 10.86993
[1mStep[0m  [4/26], [94mLoss[0m : 11.11698
[1mStep[0m  [6/26], [94mLoss[0m : 10.64436
[1mStep[0m  [8/26], [94mLoss[0m : 10.43022
[1mStep[0m  [10/26], [94mLoss[0m : 10.70106
[1mStep[0m  [12/26], [94mLoss[0m : 10.76098
[1mStep[0m  [14/26], [94mLoss[0m : 10.51741
[1mStep[0m  [16/26], [94mLoss[0m : 10.28797
[1mStep[0m  [18/26], [94mLoss[0m : 10.22679
[1mStep[0m  [20/26], [94mLoss[0m : 10.29099
[1mStep[0m  [22/26], [94mLoss[0m : 10.39662
[1mStep[0m  [24/26], [94mLoss[0m : 10.38629

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.586, [92mTest[0m: 10.949, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.03409
[1mStep[0m  [2/26], [94mLoss[0m : 10.32703
[1mStep[0m  [4/26], [94mLoss[0m : 9.97699
[1mStep[0m  [6/26], [94mLoss[0m : 9.74361
[1mStep[0m  [8/26], [94mLoss[0m : 9.80850
[1mStep[0m  [10/26], [94mLoss[0m : 9.77837
[1mStep[0m  [12/26], [94mLoss[0m : 9.72507
[1mStep[0m  [14/26], [94mLoss[0m : 9.33476
[1mStep[0m  [16/26], [94mLoss[0m : 9.14892
[1mStep[0m  [18/26], [94mLoss[0m : 9.19900
[1mStep[0m  [20/26], [94mLoss[0m : 9.05193
[1mStep[0m  [22/26], [94mLoss[0m : 8.78849
[1mStep[0m  [24/26], [94mLoss[0m : 9.03277

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.521, [92mTest[0m: 10.015, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.90377
[1mStep[0m  [2/26], [94mLoss[0m : 8.49070
[1mStep[0m  [4/26], [94mLoss[0m : 8.66663
[1mStep[0m  [6/26], [94mLoss[0m : 8.29178
[1mStep[0m  [8/26], [94mLoss[0m : 8.44129
[1mStep[0m  [10/26], [94mLoss[0m : 8.40752
[1mStep[0m  [12/26], [94mLoss[0m : 8.07726
[1mStep[0m  [14/26], [94mLoss[0m : 7.84549
[1mStep[0m  [16/26], [94mLoss[0m : 7.94591
[1mStep[0m  [18/26], [94mLoss[0m : 7.63088
[1mStep[0m  [20/26], [94mLoss[0m : 7.75672
[1mStep[0m  [22/26], [94mLoss[0m : 7.64148
[1mStep[0m  [24/26], [94mLoss[0m : 7.38311

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 8.131, [92mTest[0m: 8.540, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 7.44866
[1mStep[0m  [2/26], [94mLoss[0m : 7.08432
[1mStep[0m  [4/26], [94mLoss[0m : 7.07158
[1mStep[0m  [6/26], [94mLoss[0m : 7.35119
[1mStep[0m  [8/26], [94mLoss[0m : 7.19450
[1mStep[0m  [10/26], [94mLoss[0m : 7.10979
[1mStep[0m  [12/26], [94mLoss[0m : 7.17891
[1mStep[0m  [14/26], [94mLoss[0m : 6.46854
[1mStep[0m  [16/26], [94mLoss[0m : 6.78759
[1mStep[0m  [18/26], [94mLoss[0m : 6.49939
[1mStep[0m  [20/26], [94mLoss[0m : 6.86723
[1mStep[0m  [22/26], [94mLoss[0m : 6.08110
[1mStep[0m  [24/26], [94mLoss[0m : 6.33180

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 6.863, [92mTest[0m: 7.442, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 6.45875
[1mStep[0m  [2/26], [94mLoss[0m : 6.17278
[1mStep[0m  [4/26], [94mLoss[0m : 6.19249
[1mStep[0m  [6/26], [94mLoss[0m : 5.98901
[1mStep[0m  [8/26], [94mLoss[0m : 5.65678
[1mStep[0m  [10/26], [94mLoss[0m : 5.82954
[1mStep[0m  [12/26], [94mLoss[0m : 5.85154
[1mStep[0m  [14/26], [94mLoss[0m : 5.91032
[1mStep[0m  [16/26], [94mLoss[0m : 5.82909
[1mStep[0m  [18/26], [94mLoss[0m : 5.40545
[1mStep[0m  [20/26], [94mLoss[0m : 5.43657
[1mStep[0m  [22/26], [94mLoss[0m : 5.34858
[1mStep[0m  [24/26], [94mLoss[0m : 5.13053

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 5.753, [92mTest[0m: 5.529, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 5.14977
[1mStep[0m  [2/26], [94mLoss[0m : 4.99381
[1mStep[0m  [4/26], [94mLoss[0m : 4.81152
[1mStep[0m  [6/26], [94mLoss[0m : 4.77423
[1mStep[0m  [8/26], [94mLoss[0m : 4.70903
[1mStep[0m  [10/26], [94mLoss[0m : 4.56259
[1mStep[0m  [12/26], [94mLoss[0m : 4.42506
[1mStep[0m  [14/26], [94mLoss[0m : 4.45206
[1mStep[0m  [16/26], [94mLoss[0m : 4.29868
[1mStep[0m  [18/26], [94mLoss[0m : 4.15565
[1mStep[0m  [20/26], [94mLoss[0m : 4.09312
[1mStep[0m  [22/26], [94mLoss[0m : 4.03368
[1mStep[0m  [24/26], [94mLoss[0m : 3.83476

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 4.522, [92mTest[0m: 4.043, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.76842
[1mStep[0m  [2/26], [94mLoss[0m : 3.49703
[1mStep[0m  [4/26], [94mLoss[0m : 3.66959
[1mStep[0m  [6/26], [94mLoss[0m : 3.56489
[1mStep[0m  [8/26], [94mLoss[0m : 3.25145
[1mStep[0m  [10/26], [94mLoss[0m : 3.23564
[1mStep[0m  [12/26], [94mLoss[0m : 3.38759
[1mStep[0m  [14/26], [94mLoss[0m : 3.27501
[1mStep[0m  [16/26], [94mLoss[0m : 3.11564
[1mStep[0m  [18/26], [94mLoss[0m : 3.10341
[1mStep[0m  [20/26], [94mLoss[0m : 3.03909
[1mStep[0m  [22/26], [94mLoss[0m : 2.92505
[1mStep[0m  [24/26], [94mLoss[0m : 2.96671

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 3.290, [92mTest[0m: 3.104, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.77263
[1mStep[0m  [2/26], [94mLoss[0m : 2.67451
[1mStep[0m  [4/26], [94mLoss[0m : 2.63880
[1mStep[0m  [6/26], [94mLoss[0m : 2.56371
[1mStep[0m  [8/26], [94mLoss[0m : 2.67763
[1mStep[0m  [10/26], [94mLoss[0m : 2.73652
[1mStep[0m  [12/26], [94mLoss[0m : 2.64171
[1mStep[0m  [14/26], [94mLoss[0m : 2.77306
[1mStep[0m  [16/26], [94mLoss[0m : 2.73953
[1mStep[0m  [18/26], [94mLoss[0m : 2.68758
[1mStep[0m  [20/26], [94mLoss[0m : 2.68086
[1mStep[0m  [22/26], [94mLoss[0m : 2.60849
[1mStep[0m  [24/26], [94mLoss[0m : 2.42707

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.681, [92mTest[0m: 2.572, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.82301
[1mStep[0m  [2/26], [94mLoss[0m : 2.66611
[1mStep[0m  [4/26], [94mLoss[0m : 2.63943
[1mStep[0m  [6/26], [94mLoss[0m : 2.49351
[1mStep[0m  [8/26], [94mLoss[0m : 2.59230
[1mStep[0m  [10/26], [94mLoss[0m : 2.64638
[1mStep[0m  [12/26], [94mLoss[0m : 2.74768
[1mStep[0m  [14/26], [94mLoss[0m : 2.66925
[1mStep[0m  [16/26], [94mLoss[0m : 2.60870
[1mStep[0m  [18/26], [94mLoss[0m : 2.72102
[1mStep[0m  [20/26], [94mLoss[0m : 2.81156
[1mStep[0m  [22/26], [94mLoss[0m : 2.65352
[1mStep[0m  [24/26], [94mLoss[0m : 2.54242

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.638, [92mTest[0m: 2.629, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.59705
[1mStep[0m  [2/26], [94mLoss[0m : 2.60419
[1mStep[0m  [4/26], [94mLoss[0m : 2.62530
[1mStep[0m  [6/26], [94mLoss[0m : 2.56921
[1mStep[0m  [8/26], [94mLoss[0m : 2.81244
[1mStep[0m  [10/26], [94mLoss[0m : 2.58226
[1mStep[0m  [12/26], [94mLoss[0m : 2.75733
[1mStep[0m  [14/26], [94mLoss[0m : 2.40834
[1mStep[0m  [16/26], [94mLoss[0m : 2.76824
[1mStep[0m  [18/26], [94mLoss[0m : 2.47097
[1mStep[0m  [20/26], [94mLoss[0m : 2.48979
[1mStep[0m  [22/26], [94mLoss[0m : 2.56322
[1mStep[0m  [24/26], [94mLoss[0m : 2.50724

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.608, [92mTest[0m: 2.454, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.49658
[1mStep[0m  [2/26], [94mLoss[0m : 2.42602
[1mStep[0m  [4/26], [94mLoss[0m : 2.59955
[1mStep[0m  [6/26], [94mLoss[0m : 2.65148
[1mStep[0m  [8/26], [94mLoss[0m : 2.64425
[1mStep[0m  [10/26], [94mLoss[0m : 2.47885
[1mStep[0m  [12/26], [94mLoss[0m : 2.59950
[1mStep[0m  [14/26], [94mLoss[0m : 2.67162
[1mStep[0m  [16/26], [94mLoss[0m : 2.74956
[1mStep[0m  [18/26], [94mLoss[0m : 2.63367
[1mStep[0m  [20/26], [94mLoss[0m : 2.52535
[1mStep[0m  [22/26], [94mLoss[0m : 2.79669
[1mStep[0m  [24/26], [94mLoss[0m : 2.66907

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.597, [92mTest[0m: 2.442, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.68064
[1mStep[0m  [2/26], [94mLoss[0m : 2.60730
[1mStep[0m  [4/26], [94mLoss[0m : 2.53914
[1mStep[0m  [6/26], [94mLoss[0m : 2.66899
[1mStep[0m  [8/26], [94mLoss[0m : 2.52294
[1mStep[0m  [10/26], [94mLoss[0m : 2.69029
[1mStep[0m  [12/26], [94mLoss[0m : 2.51422
[1mStep[0m  [14/26], [94mLoss[0m : 2.63855
[1mStep[0m  [16/26], [94mLoss[0m : 2.58020
[1mStep[0m  [18/26], [94mLoss[0m : 2.57737
[1mStep[0m  [20/26], [94mLoss[0m : 2.50644
[1mStep[0m  [22/26], [94mLoss[0m : 2.54507
[1mStep[0m  [24/26], [94mLoss[0m : 2.68197

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.585, [92mTest[0m: 2.403, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.64888
[1mStep[0m  [2/26], [94mLoss[0m : 2.64837
[1mStep[0m  [4/26], [94mLoss[0m : 2.63148
[1mStep[0m  [6/26], [94mLoss[0m : 2.46638
[1mStep[0m  [8/26], [94mLoss[0m : 2.57693
[1mStep[0m  [10/26], [94mLoss[0m : 2.47823
[1mStep[0m  [12/26], [94mLoss[0m : 2.37255
[1mStep[0m  [14/26], [94mLoss[0m : 2.49506
[1mStep[0m  [16/26], [94mLoss[0m : 2.41364
[1mStep[0m  [18/26], [94mLoss[0m : 2.64379
[1mStep[0m  [20/26], [94mLoss[0m : 2.64052
[1mStep[0m  [22/26], [94mLoss[0m : 2.55790
[1mStep[0m  [24/26], [94mLoss[0m : 2.69915

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.573, [92mTest[0m: 2.421, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.76019
[1mStep[0m  [2/26], [94mLoss[0m : 2.49340
[1mStep[0m  [4/26], [94mLoss[0m : 2.63325
[1mStep[0m  [6/26], [94mLoss[0m : 2.53925
[1mStep[0m  [8/26], [94mLoss[0m : 2.70936
[1mStep[0m  [10/26], [94mLoss[0m : 2.52344
[1mStep[0m  [12/26], [94mLoss[0m : 2.65549
[1mStep[0m  [14/26], [94mLoss[0m : 2.45591
[1mStep[0m  [16/26], [94mLoss[0m : 2.69063
[1mStep[0m  [18/26], [94mLoss[0m : 2.60033
[1mStep[0m  [20/26], [94mLoss[0m : 2.62480
[1mStep[0m  [22/26], [94mLoss[0m : 2.51221
[1mStep[0m  [24/26], [94mLoss[0m : 2.35886

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.564, [92mTest[0m: 2.473, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.49525
[1mStep[0m  [2/26], [94mLoss[0m : 2.55791
[1mStep[0m  [4/26], [94mLoss[0m : 2.52112
[1mStep[0m  [6/26], [94mLoss[0m : 2.62226
[1mStep[0m  [8/26], [94mLoss[0m : 2.48895
[1mStep[0m  [10/26], [94mLoss[0m : 2.48108
[1mStep[0m  [12/26], [94mLoss[0m : 2.61089
[1mStep[0m  [14/26], [94mLoss[0m : 2.70408
[1mStep[0m  [16/26], [94mLoss[0m : 2.52685
[1mStep[0m  [18/26], [94mLoss[0m : 2.40378
[1mStep[0m  [20/26], [94mLoss[0m : 2.43608
[1mStep[0m  [22/26], [94mLoss[0m : 2.67404
[1mStep[0m  [24/26], [94mLoss[0m : 2.53687

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.552, [92mTest[0m: 2.411, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.55994
[1mStep[0m  [2/26], [94mLoss[0m : 2.53057
[1mStep[0m  [4/26], [94mLoss[0m : 2.68192
[1mStep[0m  [6/26], [94mLoss[0m : 2.48514
[1mStep[0m  [8/26], [94mLoss[0m : 2.67713
[1mStep[0m  [10/26], [94mLoss[0m : 2.37256
[1mStep[0m  [12/26], [94mLoss[0m : 2.53388
[1mStep[0m  [14/26], [94mLoss[0m : 2.55314
[1mStep[0m  [16/26], [94mLoss[0m : 2.60185
[1mStep[0m  [18/26], [94mLoss[0m : 2.58971
[1mStep[0m  [20/26], [94mLoss[0m : 2.64120
[1mStep[0m  [22/26], [94mLoss[0m : 2.53351
[1mStep[0m  [24/26], [94mLoss[0m : 2.51860

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.547, [92mTest[0m: 2.440, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.63876
[1mStep[0m  [2/26], [94mLoss[0m : 2.54974
[1mStep[0m  [4/26], [94mLoss[0m : 2.56528
[1mStep[0m  [6/26], [94mLoss[0m : 2.59515
[1mStep[0m  [8/26], [94mLoss[0m : 2.61053
[1mStep[0m  [10/26], [94mLoss[0m : 2.51212
[1mStep[0m  [12/26], [94mLoss[0m : 2.66985
[1mStep[0m  [14/26], [94mLoss[0m : 2.23558
[1mStep[0m  [16/26], [94mLoss[0m : 2.57694
[1mStep[0m  [18/26], [94mLoss[0m : 2.47037
[1mStep[0m  [20/26], [94mLoss[0m : 2.55458
[1mStep[0m  [22/26], [94mLoss[0m : 2.43097
[1mStep[0m  [24/26], [94mLoss[0m : 2.70464

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.399, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.44320
[1mStep[0m  [2/26], [94mLoss[0m : 2.57449
[1mStep[0m  [4/26], [94mLoss[0m : 2.61610
[1mStep[0m  [6/26], [94mLoss[0m : 2.47374
[1mStep[0m  [8/26], [94mLoss[0m : 2.29771
[1mStep[0m  [10/26], [94mLoss[0m : 2.47521
[1mStep[0m  [12/26], [94mLoss[0m : 2.55193
[1mStep[0m  [14/26], [94mLoss[0m : 2.53069
[1mStep[0m  [16/26], [94mLoss[0m : 2.43741
[1mStep[0m  [18/26], [94mLoss[0m : 2.59105
[1mStep[0m  [20/26], [94mLoss[0m : 2.44465
[1mStep[0m  [22/26], [94mLoss[0m : 2.54167
[1mStep[0m  [24/26], [94mLoss[0m : 2.54859

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.390, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.37022
[1mStep[0m  [2/26], [94mLoss[0m : 2.66559
[1mStep[0m  [4/26], [94mLoss[0m : 2.43097
[1mStep[0m  [6/26], [94mLoss[0m : 2.73004
[1mStep[0m  [8/26], [94mLoss[0m : 2.54777
[1mStep[0m  [10/26], [94mLoss[0m : 2.41339
[1mStep[0m  [12/26], [94mLoss[0m : 2.48981
[1mStep[0m  [14/26], [94mLoss[0m : 2.51794
[1mStep[0m  [16/26], [94mLoss[0m : 2.61773
[1mStep[0m  [18/26], [94mLoss[0m : 2.49002
[1mStep[0m  [20/26], [94mLoss[0m : 2.42934
[1mStep[0m  [22/26], [94mLoss[0m : 2.52124
[1mStep[0m  [24/26], [94mLoss[0m : 2.45663

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.515, [92mTest[0m: 2.425, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.54018
[1mStep[0m  [2/26], [94mLoss[0m : 2.60827
[1mStep[0m  [4/26], [94mLoss[0m : 2.51568
[1mStep[0m  [6/26], [94mLoss[0m : 2.55318
[1mStep[0m  [8/26], [94mLoss[0m : 2.42122
[1mStep[0m  [10/26], [94mLoss[0m : 2.60155
[1mStep[0m  [12/26], [94mLoss[0m : 2.55165
[1mStep[0m  [14/26], [94mLoss[0m : 2.48557
[1mStep[0m  [16/26], [94mLoss[0m : 2.60385
[1mStep[0m  [18/26], [94mLoss[0m : 2.44847
[1mStep[0m  [20/26], [94mLoss[0m : 2.58432
[1mStep[0m  [22/26], [94mLoss[0m : 2.42727
[1mStep[0m  [24/26], [94mLoss[0m : 2.63473

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.392, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.44613
[1mStep[0m  [2/26], [94mLoss[0m : 2.30688
[1mStep[0m  [4/26], [94mLoss[0m : 2.56526
[1mStep[0m  [6/26], [94mLoss[0m : 2.66607
[1mStep[0m  [8/26], [94mLoss[0m : 2.63802
[1mStep[0m  [10/26], [94mLoss[0m : 2.61783
[1mStep[0m  [12/26], [94mLoss[0m : 2.33970
[1mStep[0m  [14/26], [94mLoss[0m : 2.57790
[1mStep[0m  [16/26], [94mLoss[0m : 2.59267
[1mStep[0m  [18/26], [94mLoss[0m : 2.55665
[1mStep[0m  [20/26], [94mLoss[0m : 2.62001
[1mStep[0m  [22/26], [94mLoss[0m : 2.56929
[1mStep[0m  [24/26], [94mLoss[0m : 2.46645

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.514, [92mTest[0m: 2.406, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.44443
[1mStep[0m  [2/26], [94mLoss[0m : 2.48245
[1mStep[0m  [4/26], [94mLoss[0m : 2.56332
[1mStep[0m  [6/26], [94mLoss[0m : 2.50067
[1mStep[0m  [8/26], [94mLoss[0m : 2.44252
[1mStep[0m  [10/26], [94mLoss[0m : 2.47417
[1mStep[0m  [12/26], [94mLoss[0m : 2.45017
[1mStep[0m  [14/26], [94mLoss[0m : 2.41319
[1mStep[0m  [16/26], [94mLoss[0m : 2.37278
[1mStep[0m  [18/26], [94mLoss[0m : 2.48834
[1mStep[0m  [20/26], [94mLoss[0m : 2.33855
[1mStep[0m  [22/26], [94mLoss[0m : 2.48852
[1mStep[0m  [24/26], [94mLoss[0m : 2.55731

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.397, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.59307
[1mStep[0m  [2/26], [94mLoss[0m : 2.59137
[1mStep[0m  [4/26], [94mLoss[0m : 2.62418
[1mStep[0m  [6/26], [94mLoss[0m : 2.41987
[1mStep[0m  [8/26], [94mLoss[0m : 2.58283
[1mStep[0m  [10/26], [94mLoss[0m : 2.52086
[1mStep[0m  [12/26], [94mLoss[0m : 2.44675
[1mStep[0m  [14/26], [94mLoss[0m : 2.49398
[1mStep[0m  [16/26], [94mLoss[0m : 2.30970
[1mStep[0m  [18/26], [94mLoss[0m : 2.45353
[1mStep[0m  [20/26], [94mLoss[0m : 2.43926
[1mStep[0m  [22/26], [94mLoss[0m : 2.47554
[1mStep[0m  [24/26], [94mLoss[0m : 2.28055

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.385, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.50103
[1mStep[0m  [2/26], [94mLoss[0m : 2.47156
[1mStep[0m  [4/26], [94mLoss[0m : 2.41593
[1mStep[0m  [6/26], [94mLoss[0m : 2.50203
[1mStep[0m  [8/26], [94mLoss[0m : 2.48428
[1mStep[0m  [10/26], [94mLoss[0m : 2.49199
[1mStep[0m  [12/26], [94mLoss[0m : 2.59146
[1mStep[0m  [14/26], [94mLoss[0m : 2.64669
[1mStep[0m  [16/26], [94mLoss[0m : 2.39352
[1mStep[0m  [18/26], [94mLoss[0m : 2.56159
[1mStep[0m  [20/26], [94mLoss[0m : 2.45419
[1mStep[0m  [22/26], [94mLoss[0m : 2.50638
[1mStep[0m  [24/26], [94mLoss[0m : 2.35440

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.388, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.70185
[1mStep[0m  [2/26], [94mLoss[0m : 2.32605
[1mStep[0m  [4/26], [94mLoss[0m : 2.52353
[1mStep[0m  [6/26], [94mLoss[0m : 2.53465
[1mStep[0m  [8/26], [94mLoss[0m : 2.51989
[1mStep[0m  [10/26], [94mLoss[0m : 2.61209
[1mStep[0m  [12/26], [94mLoss[0m : 2.56795
[1mStep[0m  [14/26], [94mLoss[0m : 2.58605
[1mStep[0m  [16/26], [94mLoss[0m : 2.60271
[1mStep[0m  [18/26], [94mLoss[0m : 2.39008
[1mStep[0m  [20/26], [94mLoss[0m : 2.40723
[1mStep[0m  [22/26], [94mLoss[0m : 2.48540
[1mStep[0m  [24/26], [94mLoss[0m : 2.58229

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.379, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.51866
[1mStep[0m  [2/26], [94mLoss[0m : 2.52287
[1mStep[0m  [4/26], [94mLoss[0m : 2.54327
[1mStep[0m  [6/26], [94mLoss[0m : 2.48369
[1mStep[0m  [8/26], [94mLoss[0m : 2.43165
[1mStep[0m  [10/26], [94mLoss[0m : 2.47375
[1mStep[0m  [12/26], [94mLoss[0m : 2.48939
[1mStep[0m  [14/26], [94mLoss[0m : 2.40328
[1mStep[0m  [16/26], [94mLoss[0m : 2.41150
[1mStep[0m  [18/26], [94mLoss[0m : 2.40322
[1mStep[0m  [20/26], [94mLoss[0m : 2.59959
[1mStep[0m  [22/26], [94mLoss[0m : 2.58183
[1mStep[0m  [24/26], [94mLoss[0m : 2.46400

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.398, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.54213
[1mStep[0m  [2/26], [94mLoss[0m : 2.50846
[1mStep[0m  [4/26], [94mLoss[0m : 2.33805
[1mStep[0m  [6/26], [94mLoss[0m : 2.40421
[1mStep[0m  [8/26], [94mLoss[0m : 2.48568
[1mStep[0m  [10/26], [94mLoss[0m : 2.44655
[1mStep[0m  [12/26], [94mLoss[0m : 2.51993
[1mStep[0m  [14/26], [94mLoss[0m : 2.49688
[1mStep[0m  [16/26], [94mLoss[0m : 2.41136
[1mStep[0m  [18/26], [94mLoss[0m : 2.35986
[1mStep[0m  [20/26], [94mLoss[0m : 2.43675
[1mStep[0m  [22/26], [94mLoss[0m : 2.45042
[1mStep[0m  [24/26], [94mLoss[0m : 2.58269

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.379, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.50691
[1mStep[0m  [2/26], [94mLoss[0m : 2.47137
[1mStep[0m  [4/26], [94mLoss[0m : 2.43536
[1mStep[0m  [6/26], [94mLoss[0m : 2.65406
[1mStep[0m  [8/26], [94mLoss[0m : 2.43768
[1mStep[0m  [10/26], [94mLoss[0m : 2.53618
[1mStep[0m  [12/26], [94mLoss[0m : 2.44192
[1mStep[0m  [14/26], [94mLoss[0m : 2.53433
[1mStep[0m  [16/26], [94mLoss[0m : 2.32911
[1mStep[0m  [18/26], [94mLoss[0m : 2.52990
[1mStep[0m  [20/26], [94mLoss[0m : 2.57778
[1mStep[0m  [22/26], [94mLoss[0m : 2.47008
[1mStep[0m  [24/26], [94mLoss[0m : 2.42527

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.380, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.42386
[1mStep[0m  [2/26], [94mLoss[0m : 2.49289
[1mStep[0m  [4/26], [94mLoss[0m : 2.43765
[1mStep[0m  [6/26], [94mLoss[0m : 2.51400
[1mStep[0m  [8/26], [94mLoss[0m : 2.44074
[1mStep[0m  [10/26], [94mLoss[0m : 2.40848
[1mStep[0m  [12/26], [94mLoss[0m : 2.35140
[1mStep[0m  [14/26], [94mLoss[0m : 2.60332
[1mStep[0m  [16/26], [94mLoss[0m : 2.44615
[1mStep[0m  [18/26], [94mLoss[0m : 2.54153
[1mStep[0m  [20/26], [94mLoss[0m : 2.47969
[1mStep[0m  [22/26], [94mLoss[0m : 2.43320
[1mStep[0m  [24/26], [94mLoss[0m : 2.39495

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.416, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.50051
[1mStep[0m  [2/26], [94mLoss[0m : 2.42343
[1mStep[0m  [4/26], [94mLoss[0m : 2.53224
[1mStep[0m  [6/26], [94mLoss[0m : 2.40316
[1mStep[0m  [8/26], [94mLoss[0m : 2.46148
[1mStep[0m  [10/26], [94mLoss[0m : 2.48786
[1mStep[0m  [12/26], [94mLoss[0m : 2.46964
[1mStep[0m  [14/26], [94mLoss[0m : 2.34506
[1mStep[0m  [16/26], [94mLoss[0m : 2.51290
[1mStep[0m  [18/26], [94mLoss[0m : 2.45884
[1mStep[0m  [20/26], [94mLoss[0m : 2.55848
[1mStep[0m  [22/26], [94mLoss[0m : 2.46654
[1mStep[0m  [24/26], [94mLoss[0m : 2.56368

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.375, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.375
====================================

Phase 1 - Evaluation MAE:  2.37520694732666
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/26], [94mLoss[0m : 2.37324
[1mStep[0m  [2/26], [94mLoss[0m : 2.46284
[1mStep[0m  [4/26], [94mLoss[0m : 2.46699
[1mStep[0m  [6/26], [94mLoss[0m : 2.46825
[1mStep[0m  [8/26], [94mLoss[0m : 2.54226
[1mStep[0m  [10/26], [94mLoss[0m : 2.75833
[1mStep[0m  [12/26], [94mLoss[0m : 2.59232
[1mStep[0m  [14/26], [94mLoss[0m : 2.65020
[1mStep[0m  [16/26], [94mLoss[0m : 2.54866
[1mStep[0m  [18/26], [94mLoss[0m : 2.45938
[1mStep[0m  [20/26], [94mLoss[0m : 2.53602
[1mStep[0m  [22/26], [94mLoss[0m : 2.46242
[1mStep[0m  [24/26], [94mLoss[0m : 2.53999

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.526, [92mTest[0m: 2.371, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.39473
[1mStep[0m  [2/26], [94mLoss[0m : 2.51795
[1mStep[0m  [4/26], [94mLoss[0m : 2.38662
[1mStep[0m  [6/26], [94mLoss[0m : 2.52076
[1mStep[0m  [8/26], [94mLoss[0m : 2.59536
[1mStep[0m  [10/26], [94mLoss[0m : 2.39357
[1mStep[0m  [12/26], [94mLoss[0m : 2.50217
[1mStep[0m  [14/26], [94mLoss[0m : 2.37369
[1mStep[0m  [16/26], [94mLoss[0m : 2.56890
[1mStep[0m  [18/26], [94mLoss[0m : 2.48514
[1mStep[0m  [20/26], [94mLoss[0m : 2.37555
[1mStep[0m  [22/26], [94mLoss[0m : 2.38195
[1mStep[0m  [24/26], [94mLoss[0m : 2.65625

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.440, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.49396
[1mStep[0m  [2/26], [94mLoss[0m : 2.51134
[1mStep[0m  [4/26], [94mLoss[0m : 2.56216
[1mStep[0m  [6/26], [94mLoss[0m : 2.36967
[1mStep[0m  [8/26], [94mLoss[0m : 2.53573
[1mStep[0m  [10/26], [94mLoss[0m : 2.33814
[1mStep[0m  [12/26], [94mLoss[0m : 2.42172
[1mStep[0m  [14/26], [94mLoss[0m : 2.47050
[1mStep[0m  [16/26], [94mLoss[0m : 2.31644
[1mStep[0m  [18/26], [94mLoss[0m : 2.40062
[1mStep[0m  [20/26], [94mLoss[0m : 2.47611
[1mStep[0m  [22/26], [94mLoss[0m : 2.31026
[1mStep[0m  [24/26], [94mLoss[0m : 2.38583

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.387, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.35656
[1mStep[0m  [2/26], [94mLoss[0m : 2.31569
[1mStep[0m  [4/26], [94mLoss[0m : 2.25677
[1mStep[0m  [6/26], [94mLoss[0m : 2.39814
[1mStep[0m  [8/26], [94mLoss[0m : 2.45771
[1mStep[0m  [10/26], [94mLoss[0m : 2.34093
[1mStep[0m  [12/26], [94mLoss[0m : 2.27784
[1mStep[0m  [14/26], [94mLoss[0m : 2.38006
[1mStep[0m  [16/26], [94mLoss[0m : 2.55277
[1mStep[0m  [18/26], [94mLoss[0m : 2.38802
[1mStep[0m  [20/26], [94mLoss[0m : 2.40245
[1mStep[0m  [22/26], [94mLoss[0m : 2.46095
[1mStep[0m  [24/26], [94mLoss[0m : 2.53181

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.386, [92mTest[0m: 2.398, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.29444
[1mStep[0m  [2/26], [94mLoss[0m : 2.46490
[1mStep[0m  [4/26], [94mLoss[0m : 2.26113
[1mStep[0m  [6/26], [94mLoss[0m : 2.33093
[1mStep[0m  [8/26], [94mLoss[0m : 2.33465
[1mStep[0m  [10/26], [94mLoss[0m : 2.50704
[1mStep[0m  [12/26], [94mLoss[0m : 2.24129
[1mStep[0m  [14/26], [94mLoss[0m : 2.42749
[1mStep[0m  [16/26], [94mLoss[0m : 2.42492
[1mStep[0m  [18/26], [94mLoss[0m : 2.33949
[1mStep[0m  [20/26], [94mLoss[0m : 2.23630
[1mStep[0m  [22/26], [94mLoss[0m : 2.28127
[1mStep[0m  [24/26], [94mLoss[0m : 2.22764

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.312, [92mTest[0m: 2.369, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.29601
[1mStep[0m  [2/26], [94mLoss[0m : 2.33806
[1mStep[0m  [4/26], [94mLoss[0m : 2.14415
[1mStep[0m  [6/26], [94mLoss[0m : 2.28525
[1mStep[0m  [8/26], [94mLoss[0m : 2.16747
[1mStep[0m  [10/26], [94mLoss[0m : 2.22535
[1mStep[0m  [12/26], [94mLoss[0m : 2.21227
[1mStep[0m  [14/26], [94mLoss[0m : 2.26080
[1mStep[0m  [16/26], [94mLoss[0m : 2.22504
[1mStep[0m  [18/26], [94mLoss[0m : 2.36850
[1mStep[0m  [20/26], [94mLoss[0m : 2.14747
[1mStep[0m  [22/26], [94mLoss[0m : 2.36933
[1mStep[0m  [24/26], [94mLoss[0m : 2.29494

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.275, [92mTest[0m: 2.385, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.28613
[1mStep[0m  [2/26], [94mLoss[0m : 2.21133
[1mStep[0m  [4/26], [94mLoss[0m : 2.20202
[1mStep[0m  [6/26], [94mLoss[0m : 2.26141
[1mStep[0m  [8/26], [94mLoss[0m : 2.15585
[1mStep[0m  [10/26], [94mLoss[0m : 2.25963
[1mStep[0m  [12/26], [94mLoss[0m : 2.20177
[1mStep[0m  [14/26], [94mLoss[0m : 2.22723
[1mStep[0m  [16/26], [94mLoss[0m : 2.11280
[1mStep[0m  [18/26], [94mLoss[0m : 2.18236
[1mStep[0m  [20/26], [94mLoss[0m : 2.39699
[1mStep[0m  [22/26], [94mLoss[0m : 2.31702
[1mStep[0m  [24/26], [94mLoss[0m : 2.29187

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.232, [92mTest[0m: 2.353, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.20592
[1mStep[0m  [2/26], [94mLoss[0m : 2.20231
[1mStep[0m  [4/26], [94mLoss[0m : 2.15422
[1mStep[0m  [6/26], [94mLoss[0m : 2.27964
[1mStep[0m  [8/26], [94mLoss[0m : 2.10864
[1mStep[0m  [10/26], [94mLoss[0m : 2.25682
[1mStep[0m  [12/26], [94mLoss[0m : 2.33387
[1mStep[0m  [14/26], [94mLoss[0m : 2.18471
[1mStep[0m  [16/26], [94mLoss[0m : 2.17817
[1mStep[0m  [18/26], [94mLoss[0m : 2.19349
[1mStep[0m  [20/26], [94mLoss[0m : 2.18477
[1mStep[0m  [22/26], [94mLoss[0m : 2.16762
[1mStep[0m  [24/26], [94mLoss[0m : 2.09068

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.191, [92mTest[0m: 2.380, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.19659
[1mStep[0m  [2/26], [94mLoss[0m : 2.18208
[1mStep[0m  [4/26], [94mLoss[0m : 2.18758
[1mStep[0m  [6/26], [94mLoss[0m : 1.95946
[1mStep[0m  [8/26], [94mLoss[0m : 2.18595
[1mStep[0m  [10/26], [94mLoss[0m : 2.16042
[1mStep[0m  [12/26], [94mLoss[0m : 2.05987
[1mStep[0m  [14/26], [94mLoss[0m : 2.17268
[1mStep[0m  [16/26], [94mLoss[0m : 2.11237
[1mStep[0m  [18/26], [94mLoss[0m : 2.38247
[1mStep[0m  [20/26], [94mLoss[0m : 2.29145
[1mStep[0m  [22/26], [94mLoss[0m : 2.18939
[1mStep[0m  [24/26], [94mLoss[0m : 2.08455

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.143, [92mTest[0m: 2.385, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.98935
[1mStep[0m  [2/26], [94mLoss[0m : 2.04192
[1mStep[0m  [4/26], [94mLoss[0m : 2.09531
[1mStep[0m  [6/26], [94mLoss[0m : 1.96479
[1mStep[0m  [8/26], [94mLoss[0m : 2.11820
[1mStep[0m  [10/26], [94mLoss[0m : 2.12026
[1mStep[0m  [12/26], [94mLoss[0m : 2.11869
[1mStep[0m  [14/26], [94mLoss[0m : 1.94325
[1mStep[0m  [16/26], [94mLoss[0m : 1.97007
[1mStep[0m  [18/26], [94mLoss[0m : 2.07463
[1mStep[0m  [20/26], [94mLoss[0m : 2.06140
[1mStep[0m  [22/26], [94mLoss[0m : 2.09066
[1mStep[0m  [24/26], [94mLoss[0m : 2.01483

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.068, [92mTest[0m: 2.401, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.14719
[1mStep[0m  [2/26], [94mLoss[0m : 2.00995
[1mStep[0m  [4/26], [94mLoss[0m : 1.91688
[1mStep[0m  [6/26], [94mLoss[0m : 1.87460
[1mStep[0m  [8/26], [94mLoss[0m : 1.88524
[1mStep[0m  [10/26], [94mLoss[0m : 2.05121
[1mStep[0m  [12/26], [94mLoss[0m : 1.93932
[1mStep[0m  [14/26], [94mLoss[0m : 2.07189
[1mStep[0m  [16/26], [94mLoss[0m : 2.12111
[1mStep[0m  [18/26], [94mLoss[0m : 2.07107
[1mStep[0m  [20/26], [94mLoss[0m : 2.21322
[1mStep[0m  [22/26], [94mLoss[0m : 2.11419
[1mStep[0m  [24/26], [94mLoss[0m : 2.00860

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.027, [92mTest[0m: 2.405, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.16934
[1mStep[0m  [2/26], [94mLoss[0m : 2.25906
[1mStep[0m  [4/26], [94mLoss[0m : 1.96261
[1mStep[0m  [6/26], [94mLoss[0m : 1.89352
[1mStep[0m  [8/26], [94mLoss[0m : 2.09465
[1mStep[0m  [10/26], [94mLoss[0m : 1.99585
[1mStep[0m  [12/26], [94mLoss[0m : 1.96418
[1mStep[0m  [14/26], [94mLoss[0m : 2.02220
[1mStep[0m  [16/26], [94mLoss[0m : 1.96857
[1mStep[0m  [18/26], [94mLoss[0m : 2.01122
[1mStep[0m  [20/26], [94mLoss[0m : 1.92558
[1mStep[0m  [22/26], [94mLoss[0m : 1.93824
[1mStep[0m  [24/26], [94mLoss[0m : 2.03506

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.024, [92mTest[0m: 2.404, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.96036
[1mStep[0m  [2/26], [94mLoss[0m : 1.89472
[1mStep[0m  [4/26], [94mLoss[0m : 1.96218
[1mStep[0m  [6/26], [94mLoss[0m : 1.86500
[1mStep[0m  [8/26], [94mLoss[0m : 1.76145
[1mStep[0m  [10/26], [94mLoss[0m : 1.94990
[1mStep[0m  [12/26], [94mLoss[0m : 1.97191
[1mStep[0m  [14/26], [94mLoss[0m : 1.92242
[1mStep[0m  [16/26], [94mLoss[0m : 2.02129
[1mStep[0m  [18/26], [94mLoss[0m : 1.97746
[1mStep[0m  [20/26], [94mLoss[0m : 1.88126
[1mStep[0m  [22/26], [94mLoss[0m : 2.06294
[1mStep[0m  [24/26], [94mLoss[0m : 2.10115

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.951, [92mTest[0m: 2.409, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.85681
[1mStep[0m  [2/26], [94mLoss[0m : 1.92204
[1mStep[0m  [4/26], [94mLoss[0m : 1.98607
[1mStep[0m  [6/26], [94mLoss[0m : 1.82219
[1mStep[0m  [8/26], [94mLoss[0m : 1.92866
[1mStep[0m  [10/26], [94mLoss[0m : 1.88562
[1mStep[0m  [12/26], [94mLoss[0m : 1.90532
[1mStep[0m  [14/26], [94mLoss[0m : 1.76423
[1mStep[0m  [16/26], [94mLoss[0m : 2.08515
[1mStep[0m  [18/26], [94mLoss[0m : 2.07656
[1mStep[0m  [20/26], [94mLoss[0m : 1.92383
[1mStep[0m  [22/26], [94mLoss[0m : 1.83531
[1mStep[0m  [24/26], [94mLoss[0m : 2.02653

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.896, [92mTest[0m: 2.430, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.81041
[1mStep[0m  [2/26], [94mLoss[0m : 1.73064
[1mStep[0m  [4/26], [94mLoss[0m : 1.92292
[1mStep[0m  [6/26], [94mLoss[0m : 1.88535
[1mStep[0m  [8/26], [94mLoss[0m : 1.89429
[1mStep[0m  [10/26], [94mLoss[0m : 1.78111
[1mStep[0m  [12/26], [94mLoss[0m : 1.94235
[1mStep[0m  [14/26], [94mLoss[0m : 1.93621
[1mStep[0m  [16/26], [94mLoss[0m : 1.83143
[1mStep[0m  [18/26], [94mLoss[0m : 1.73931
[1mStep[0m  [20/26], [94mLoss[0m : 1.97976
[1mStep[0m  [22/26], [94mLoss[0m : 1.90311
[1mStep[0m  [24/26], [94mLoss[0m : 1.98108

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.863, [92mTest[0m: 2.411, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.80109
[1mStep[0m  [2/26], [94mLoss[0m : 1.85622
[1mStep[0m  [4/26], [94mLoss[0m : 1.84526
[1mStep[0m  [6/26], [94mLoss[0m : 1.97294
[1mStep[0m  [8/26], [94mLoss[0m : 1.81259
[1mStep[0m  [10/26], [94mLoss[0m : 1.85439
[1mStep[0m  [12/26], [94mLoss[0m : 1.95320
[1mStep[0m  [14/26], [94mLoss[0m : 1.80789
[1mStep[0m  [16/26], [94mLoss[0m : 1.79436
[1mStep[0m  [18/26], [94mLoss[0m : 1.86317
[1mStep[0m  [20/26], [94mLoss[0m : 1.89414
[1mStep[0m  [22/26], [94mLoss[0m : 1.88782
[1mStep[0m  [24/26], [94mLoss[0m : 1.96617

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.847, [92mTest[0m: 2.446, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.87130
[1mStep[0m  [2/26], [94mLoss[0m : 1.85186
[1mStep[0m  [4/26], [94mLoss[0m : 1.76109
[1mStep[0m  [6/26], [94mLoss[0m : 1.83849
[1mStep[0m  [8/26], [94mLoss[0m : 1.86622
[1mStep[0m  [10/26], [94mLoss[0m : 1.63286
[1mStep[0m  [12/26], [94mLoss[0m : 1.74095
[1mStep[0m  [14/26], [94mLoss[0m : 1.75021
[1mStep[0m  [16/26], [94mLoss[0m : 1.78850
[1mStep[0m  [18/26], [94mLoss[0m : 1.89293
[1mStep[0m  [20/26], [94mLoss[0m : 1.81748
[1mStep[0m  [22/26], [94mLoss[0m : 1.86173
[1mStep[0m  [24/26], [94mLoss[0m : 1.91544

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.801, [92mTest[0m: 2.409, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.81704
[1mStep[0m  [2/26], [94mLoss[0m : 1.70873
[1mStep[0m  [4/26], [94mLoss[0m : 1.85509
[1mStep[0m  [6/26], [94mLoss[0m : 1.83905
[1mStep[0m  [8/26], [94mLoss[0m : 1.76982
[1mStep[0m  [10/26], [94mLoss[0m : 1.70419
[1mStep[0m  [12/26], [94mLoss[0m : 1.75895
[1mStep[0m  [14/26], [94mLoss[0m : 1.80895
[1mStep[0m  [16/26], [94mLoss[0m : 1.73940
[1mStep[0m  [18/26], [94mLoss[0m : 1.82568
[1mStep[0m  [20/26], [94mLoss[0m : 1.76390
[1mStep[0m  [22/26], [94mLoss[0m : 1.81450
[1mStep[0m  [24/26], [94mLoss[0m : 1.88709

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.776, [92mTest[0m: 2.479, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.72857
[1mStep[0m  [2/26], [94mLoss[0m : 1.79832
[1mStep[0m  [4/26], [94mLoss[0m : 1.71330
[1mStep[0m  [6/26], [94mLoss[0m : 1.72610
[1mStep[0m  [8/26], [94mLoss[0m : 1.61611
[1mStep[0m  [10/26], [94mLoss[0m : 1.80787
[1mStep[0m  [12/26], [94mLoss[0m : 1.61049
[1mStep[0m  [14/26], [94mLoss[0m : 1.65965
[1mStep[0m  [16/26], [94mLoss[0m : 1.78737
[1mStep[0m  [18/26], [94mLoss[0m : 1.69866
[1mStep[0m  [20/26], [94mLoss[0m : 1.85060
[1mStep[0m  [22/26], [94mLoss[0m : 1.76000
[1mStep[0m  [24/26], [94mLoss[0m : 1.71022

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.729, [92mTest[0m: 2.539, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.70455
[1mStep[0m  [2/26], [94mLoss[0m : 1.69812
[1mStep[0m  [4/26], [94mLoss[0m : 1.66469
[1mStep[0m  [6/26], [94mLoss[0m : 1.66565
[1mStep[0m  [8/26], [94mLoss[0m : 1.74648
[1mStep[0m  [10/26], [94mLoss[0m : 1.68726
[1mStep[0m  [12/26], [94mLoss[0m : 1.64534
[1mStep[0m  [14/26], [94mLoss[0m : 1.68536
[1mStep[0m  [16/26], [94mLoss[0m : 1.78847
[1mStep[0m  [18/26], [94mLoss[0m : 1.80048
[1mStep[0m  [20/26], [94mLoss[0m : 1.73424
[1mStep[0m  [22/26], [94mLoss[0m : 1.64552
[1mStep[0m  [24/26], [94mLoss[0m : 1.81564

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.707, [92mTest[0m: 2.433, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.62305
[1mStep[0m  [2/26], [94mLoss[0m : 1.70085
[1mStep[0m  [4/26], [94mLoss[0m : 1.58548
[1mStep[0m  [6/26], [94mLoss[0m : 1.59332
[1mStep[0m  [8/26], [94mLoss[0m : 1.67070
[1mStep[0m  [10/26], [94mLoss[0m : 1.63049
[1mStep[0m  [12/26], [94mLoss[0m : 1.78743
[1mStep[0m  [14/26], [94mLoss[0m : 1.70391
[1mStep[0m  [16/26], [94mLoss[0m : 1.63908
[1mStep[0m  [18/26], [94mLoss[0m : 1.65356
[1mStep[0m  [20/26], [94mLoss[0m : 1.72644
[1mStep[0m  [22/26], [94mLoss[0m : 1.64873
[1mStep[0m  [24/26], [94mLoss[0m : 1.68529

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.660, [92mTest[0m: 2.409, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.55517
[1mStep[0m  [2/26], [94mLoss[0m : 1.57615
[1mStep[0m  [4/26], [94mLoss[0m : 1.67048
[1mStep[0m  [6/26], [94mLoss[0m : 1.70596
[1mStep[0m  [8/26], [94mLoss[0m : 1.72511
[1mStep[0m  [10/26], [94mLoss[0m : 1.53154
[1mStep[0m  [12/26], [94mLoss[0m : 1.63949
[1mStep[0m  [14/26], [94mLoss[0m : 1.81142
[1mStep[0m  [16/26], [94mLoss[0m : 1.81558
[1mStep[0m  [18/26], [94mLoss[0m : 1.63298
[1mStep[0m  [20/26], [94mLoss[0m : 1.70475
[1mStep[0m  [22/26], [94mLoss[0m : 1.74692
[1mStep[0m  [24/26], [94mLoss[0m : 1.61179

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.644, [92mTest[0m: 2.461, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.53908
[1mStep[0m  [2/26], [94mLoss[0m : 1.60723
[1mStep[0m  [4/26], [94mLoss[0m : 1.61367
[1mStep[0m  [6/26], [94mLoss[0m : 1.50909
[1mStep[0m  [8/26], [94mLoss[0m : 1.59599
[1mStep[0m  [10/26], [94mLoss[0m : 1.63224
[1mStep[0m  [12/26], [94mLoss[0m : 1.64608
[1mStep[0m  [14/26], [94mLoss[0m : 1.58657
[1mStep[0m  [16/26], [94mLoss[0m : 1.49571
[1mStep[0m  [18/26], [94mLoss[0m : 1.52648
[1mStep[0m  [20/26], [94mLoss[0m : 1.57553
[1mStep[0m  [22/26], [94mLoss[0m : 1.53571
[1mStep[0m  [24/26], [94mLoss[0m : 1.58298

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.591, [92mTest[0m: 2.466, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.61200
[1mStep[0m  [2/26], [94mLoss[0m : 1.55778
[1mStep[0m  [4/26], [94mLoss[0m : 1.59037
[1mStep[0m  [6/26], [94mLoss[0m : 1.59506
[1mStep[0m  [8/26], [94mLoss[0m : 1.53513
[1mStep[0m  [10/26], [94mLoss[0m : 1.57455
[1mStep[0m  [12/26], [94mLoss[0m : 1.53142
[1mStep[0m  [14/26], [94mLoss[0m : 1.63698
[1mStep[0m  [16/26], [94mLoss[0m : 1.68469
[1mStep[0m  [18/26], [94mLoss[0m : 1.59499
[1mStep[0m  [20/26], [94mLoss[0m : 1.58607
[1mStep[0m  [22/26], [94mLoss[0m : 1.73184
[1mStep[0m  [24/26], [94mLoss[0m : 1.53706

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.584, [92mTest[0m: 2.456, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.49405
[1mStep[0m  [2/26], [94mLoss[0m : 1.46874
[1mStep[0m  [4/26], [94mLoss[0m : 1.65950
[1mStep[0m  [6/26], [94mLoss[0m : 1.46920
[1mStep[0m  [8/26], [94mLoss[0m : 1.47712
[1mStep[0m  [10/26], [94mLoss[0m : 1.49705
[1mStep[0m  [12/26], [94mLoss[0m : 1.48440
[1mStep[0m  [14/26], [94mLoss[0m : 1.60182
[1mStep[0m  [16/26], [94mLoss[0m : 1.67610
[1mStep[0m  [18/26], [94mLoss[0m : 1.50559
[1mStep[0m  [20/26], [94mLoss[0m : 1.54125
[1mStep[0m  [22/26], [94mLoss[0m : 1.56579
[1mStep[0m  [24/26], [94mLoss[0m : 1.71286

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.554, [92mTest[0m: 2.424, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.61555
[1mStep[0m  [2/26], [94mLoss[0m : 1.59894
[1mStep[0m  [4/26], [94mLoss[0m : 1.56726
[1mStep[0m  [6/26], [94mLoss[0m : 1.53762
[1mStep[0m  [8/26], [94mLoss[0m : 1.57459
[1mStep[0m  [10/26], [94mLoss[0m : 1.52192
[1mStep[0m  [12/26], [94mLoss[0m : 1.50057
[1mStep[0m  [14/26], [94mLoss[0m : 1.55646
[1mStep[0m  [16/26], [94mLoss[0m : 1.47443
[1mStep[0m  [18/26], [94mLoss[0m : 1.65538
[1mStep[0m  [20/26], [94mLoss[0m : 1.54487
[1mStep[0m  [22/26], [94mLoss[0m : 1.52421
[1mStep[0m  [24/26], [94mLoss[0m : 1.57100

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.559, [92mTest[0m: 2.538, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.37570
[1mStep[0m  [2/26], [94mLoss[0m : 1.72325
[1mStep[0m  [4/26], [94mLoss[0m : 1.46220
[1mStep[0m  [6/26], [94mLoss[0m : 1.50368
[1mStep[0m  [8/26], [94mLoss[0m : 1.46736
[1mStep[0m  [10/26], [94mLoss[0m : 1.42877
[1mStep[0m  [12/26], [94mLoss[0m : 1.48221
[1mStep[0m  [14/26], [94mLoss[0m : 1.50778
[1mStep[0m  [16/26], [94mLoss[0m : 1.46818
[1mStep[0m  [18/26], [94mLoss[0m : 1.50859
[1mStep[0m  [20/26], [94mLoss[0m : 1.59020
[1mStep[0m  [22/26], [94mLoss[0m : 1.52532
[1mStep[0m  [24/26], [94mLoss[0m : 1.51031

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.516, [92mTest[0m: 2.416, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.35124
[1mStep[0m  [2/26], [94mLoss[0m : 1.40874
[1mStep[0m  [4/26], [94mLoss[0m : 1.42175
[1mStep[0m  [6/26], [94mLoss[0m : 1.54890
[1mStep[0m  [8/26], [94mLoss[0m : 1.50207
[1mStep[0m  [10/26], [94mLoss[0m : 1.44201
[1mStep[0m  [12/26], [94mLoss[0m : 1.48497
[1mStep[0m  [14/26], [94mLoss[0m : 1.46715
[1mStep[0m  [16/26], [94mLoss[0m : 1.53042
[1mStep[0m  [18/26], [94mLoss[0m : 1.45628
[1mStep[0m  [20/26], [94mLoss[0m : 1.55432
[1mStep[0m  [22/26], [94mLoss[0m : 1.52682
[1mStep[0m  [24/26], [94mLoss[0m : 1.60575

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.516, [92mTest[0m: 2.476, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.49721
[1mStep[0m  [2/26], [94mLoss[0m : 1.53514
[1mStep[0m  [4/26], [94mLoss[0m : 1.47113
[1mStep[0m  [6/26], [94mLoss[0m : 1.51486
[1mStep[0m  [8/26], [94mLoss[0m : 1.34418
[1mStep[0m  [10/26], [94mLoss[0m : 1.41309
[1mStep[0m  [12/26], [94mLoss[0m : 1.39518
[1mStep[0m  [14/26], [94mLoss[0m : 1.39837
[1mStep[0m  [16/26], [94mLoss[0m : 1.44708
[1mStep[0m  [18/26], [94mLoss[0m : 1.42260
[1mStep[0m  [20/26], [94mLoss[0m : 1.45590
[1mStep[0m  [22/26], [94mLoss[0m : 1.51644
[1mStep[0m  [24/26], [94mLoss[0m : 1.62286

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.464, [92mTest[0m: 2.423, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.49751
[1mStep[0m  [2/26], [94mLoss[0m : 1.49684
[1mStep[0m  [4/26], [94mLoss[0m : 1.44808
[1mStep[0m  [6/26], [94mLoss[0m : 1.35902
[1mStep[0m  [8/26], [94mLoss[0m : 1.54590
[1mStep[0m  [10/26], [94mLoss[0m : 1.50105
[1mStep[0m  [12/26], [94mLoss[0m : 1.57358
[1mStep[0m  [14/26], [94mLoss[0m : 1.47412
[1mStep[0m  [16/26], [94mLoss[0m : 1.35407
[1mStep[0m  [18/26], [94mLoss[0m : 1.47298
[1mStep[0m  [20/26], [94mLoss[0m : 1.46255
[1mStep[0m  [22/26], [94mLoss[0m : 1.44296
[1mStep[0m  [24/26], [94mLoss[0m : 1.46991

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.459, [92mTest[0m: 2.498, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.470
====================================

Phase 2 - Evaluation MAE:  2.469561320084792
MAE score P1      2.375207
MAE score P2      2.469561
loss              1.459187
learning_rate     0.002575
batch_size             512
hidden_sizes         [300]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.9
weight_decay          0.01
Name: 13, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/53], [94mLoss[0m : 10.37055
[1mStep[0m  [5/53], [94mLoss[0m : 10.55175
[1mStep[0m  [10/53], [94mLoss[0m : 9.47351
[1mStep[0m  [15/53], [94mLoss[0m : 9.06131
[1mStep[0m  [20/53], [94mLoss[0m : 8.35868
[1mStep[0m  [25/53], [94mLoss[0m : 7.19379
[1mStep[0m  [30/53], [94mLoss[0m : 6.14178
[1mStep[0m  [35/53], [94mLoss[0m : 5.10852
[1mStep[0m  [40/53], [94mLoss[0m : 4.02562
[1mStep[0m  [45/53], [94mLoss[0m : 3.46334
[1mStep[0m  [50/53], [94mLoss[0m : 3.16586

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.740, [92mTest[0m: 10.754, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.01889
[1mStep[0m  [5/53], [94mLoss[0m : 2.73808
[1mStep[0m  [10/53], [94mLoss[0m : 2.93582
[1mStep[0m  [15/53], [94mLoss[0m : 2.80821
[1mStep[0m  [20/53], [94mLoss[0m : 2.89125
[1mStep[0m  [25/53], [94mLoss[0m : 2.64323
[1mStep[0m  [30/53], [94mLoss[0m : 2.88221
[1mStep[0m  [35/53], [94mLoss[0m : 2.70963
[1mStep[0m  [40/53], [94mLoss[0m : 3.08705
[1mStep[0m  [45/53], [94mLoss[0m : 2.63497
[1mStep[0m  [50/53], [94mLoss[0m : 2.82115

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.850, [92mTest[0m: 3.470, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.99678
[1mStep[0m  [5/53], [94mLoss[0m : 2.69719
[1mStep[0m  [10/53], [94mLoss[0m : 2.68115
[1mStep[0m  [15/53], [94mLoss[0m : 2.65972
[1mStep[0m  [20/53], [94mLoss[0m : 2.76506
[1mStep[0m  [25/53], [94mLoss[0m : 2.82391
[1mStep[0m  [30/53], [94mLoss[0m : 2.97697
[1mStep[0m  [35/53], [94mLoss[0m : 2.79024
[1mStep[0m  [40/53], [94mLoss[0m : 2.66327
[1mStep[0m  [45/53], [94mLoss[0m : 3.03431
[1mStep[0m  [50/53], [94mLoss[0m : 2.81388

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.760, [92mTest[0m: 2.577, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.70670
[1mStep[0m  [5/53], [94mLoss[0m : 2.55017
[1mStep[0m  [10/53], [94mLoss[0m : 2.76750
[1mStep[0m  [15/53], [94mLoss[0m : 2.73782
[1mStep[0m  [20/53], [94mLoss[0m : 2.53164
[1mStep[0m  [25/53], [94mLoss[0m : 2.84057
[1mStep[0m  [30/53], [94mLoss[0m : 2.85092
[1mStep[0m  [35/53], [94mLoss[0m : 2.61043
[1mStep[0m  [40/53], [94mLoss[0m : 2.54027
[1mStep[0m  [45/53], [94mLoss[0m : 2.67845
[1mStep[0m  [50/53], [94mLoss[0m : 2.82456

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.706, [92mTest[0m: 2.527, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.63494
[1mStep[0m  [5/53], [94mLoss[0m : 2.76844
[1mStep[0m  [10/53], [94mLoss[0m : 2.66362
[1mStep[0m  [15/53], [94mLoss[0m : 2.62397
[1mStep[0m  [20/53], [94mLoss[0m : 2.63748
[1mStep[0m  [25/53], [94mLoss[0m : 2.61476
[1mStep[0m  [30/53], [94mLoss[0m : 2.80855
[1mStep[0m  [35/53], [94mLoss[0m : 2.71917
[1mStep[0m  [40/53], [94mLoss[0m : 2.68927
[1mStep[0m  [45/53], [94mLoss[0m : 2.70750
[1mStep[0m  [50/53], [94mLoss[0m : 2.76106

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.675, [92mTest[0m: 2.457, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.70889
[1mStep[0m  [5/53], [94mLoss[0m : 2.55211
[1mStep[0m  [10/53], [94mLoss[0m : 2.60401
[1mStep[0m  [15/53], [94mLoss[0m : 2.50784
[1mStep[0m  [20/53], [94mLoss[0m : 2.69815
[1mStep[0m  [25/53], [94mLoss[0m : 2.75223
[1mStep[0m  [30/53], [94mLoss[0m : 2.82052
[1mStep[0m  [35/53], [94mLoss[0m : 2.58635
[1mStep[0m  [40/53], [94mLoss[0m : 2.70752
[1mStep[0m  [45/53], [94mLoss[0m : 2.66567
[1mStep[0m  [50/53], [94mLoss[0m : 2.71311

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.664, [92mTest[0m: 2.447, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.72743
[1mStep[0m  [5/53], [94mLoss[0m : 2.53887
[1mStep[0m  [10/53], [94mLoss[0m : 2.75087
[1mStep[0m  [15/53], [94mLoss[0m : 2.51544
[1mStep[0m  [20/53], [94mLoss[0m : 2.70503
[1mStep[0m  [25/53], [94mLoss[0m : 2.56020
[1mStep[0m  [30/53], [94mLoss[0m : 2.41969
[1mStep[0m  [35/53], [94mLoss[0m : 2.62047
[1mStep[0m  [40/53], [94mLoss[0m : 2.56642
[1mStep[0m  [45/53], [94mLoss[0m : 2.67871
[1mStep[0m  [50/53], [94mLoss[0m : 2.48791

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.621, [92mTest[0m: 2.403, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.67515
[1mStep[0m  [5/53], [94mLoss[0m : 2.78452
[1mStep[0m  [10/53], [94mLoss[0m : 2.56714
[1mStep[0m  [15/53], [94mLoss[0m : 2.81579
[1mStep[0m  [20/53], [94mLoss[0m : 2.64196
[1mStep[0m  [25/53], [94mLoss[0m : 2.75788
[1mStep[0m  [30/53], [94mLoss[0m : 2.70452
[1mStep[0m  [35/53], [94mLoss[0m : 2.42788
[1mStep[0m  [40/53], [94mLoss[0m : 2.52550
[1mStep[0m  [45/53], [94mLoss[0m : 2.66714
[1mStep[0m  [50/53], [94mLoss[0m : 2.53302

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.603, [92mTest[0m: 2.404, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.53465
[1mStep[0m  [5/53], [94mLoss[0m : 2.61587
[1mStep[0m  [10/53], [94mLoss[0m : 2.70068
[1mStep[0m  [15/53], [94mLoss[0m : 2.52274
[1mStep[0m  [20/53], [94mLoss[0m : 2.74709
[1mStep[0m  [25/53], [94mLoss[0m : 2.60750
[1mStep[0m  [30/53], [94mLoss[0m : 2.87513
[1mStep[0m  [35/53], [94mLoss[0m : 2.36692
[1mStep[0m  [40/53], [94mLoss[0m : 2.49126
[1mStep[0m  [45/53], [94mLoss[0m : 2.39229
[1mStep[0m  [50/53], [94mLoss[0m : 2.64654

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.594, [92mTest[0m: 2.417, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.55886
[1mStep[0m  [5/53], [94mLoss[0m : 2.52232
[1mStep[0m  [10/53], [94mLoss[0m : 2.58513
[1mStep[0m  [15/53], [94mLoss[0m : 2.58789
[1mStep[0m  [20/53], [94mLoss[0m : 2.58333
[1mStep[0m  [25/53], [94mLoss[0m : 2.29626
[1mStep[0m  [30/53], [94mLoss[0m : 2.58738
[1mStep[0m  [35/53], [94mLoss[0m : 2.44387
[1mStep[0m  [40/53], [94mLoss[0m : 2.71378
[1mStep[0m  [45/53], [94mLoss[0m : 2.54834
[1mStep[0m  [50/53], [94mLoss[0m : 2.61648

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.591, [92mTest[0m: 2.423, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.62819
[1mStep[0m  [5/53], [94mLoss[0m : 2.52151
[1mStep[0m  [10/53], [94mLoss[0m : 2.66651
[1mStep[0m  [15/53], [94mLoss[0m : 2.81886
[1mStep[0m  [20/53], [94mLoss[0m : 2.71344
[1mStep[0m  [25/53], [94mLoss[0m : 2.58653
[1mStep[0m  [30/53], [94mLoss[0m : 2.68019
[1mStep[0m  [35/53], [94mLoss[0m : 2.42054
[1mStep[0m  [40/53], [94mLoss[0m : 2.78629
[1mStep[0m  [45/53], [94mLoss[0m : 2.52236
[1mStep[0m  [50/53], [94mLoss[0m : 2.62122

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.581, [92mTest[0m: 2.394, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.52281
[1mStep[0m  [5/53], [94mLoss[0m : 2.36815
[1mStep[0m  [10/53], [94mLoss[0m : 2.37033
[1mStep[0m  [15/53], [94mLoss[0m : 2.72314
[1mStep[0m  [20/53], [94mLoss[0m : 2.71871
[1mStep[0m  [25/53], [94mLoss[0m : 2.68275
[1mStep[0m  [30/53], [94mLoss[0m : 2.40354
[1mStep[0m  [35/53], [94mLoss[0m : 2.38961
[1mStep[0m  [40/53], [94mLoss[0m : 2.48683
[1mStep[0m  [45/53], [94mLoss[0m : 2.69472
[1mStep[0m  [50/53], [94mLoss[0m : 2.43845

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.578, [92mTest[0m: 2.371, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.53590
[1mStep[0m  [5/53], [94mLoss[0m : 2.53632
[1mStep[0m  [10/53], [94mLoss[0m : 2.41969
[1mStep[0m  [15/53], [94mLoss[0m : 2.48883
[1mStep[0m  [20/53], [94mLoss[0m : 2.45500
[1mStep[0m  [25/53], [94mLoss[0m : 2.35955
[1mStep[0m  [30/53], [94mLoss[0m : 2.47197
[1mStep[0m  [35/53], [94mLoss[0m : 2.44063
[1mStep[0m  [40/53], [94mLoss[0m : 2.58223
[1mStep[0m  [45/53], [94mLoss[0m : 2.51058
[1mStep[0m  [50/53], [94mLoss[0m : 2.55764

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.534, [92mTest[0m: 2.376, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.70636
[1mStep[0m  [5/53], [94mLoss[0m : 2.52315
[1mStep[0m  [10/53], [94mLoss[0m : 2.44107
[1mStep[0m  [15/53], [94mLoss[0m : 2.37550
[1mStep[0m  [20/53], [94mLoss[0m : 2.48790
[1mStep[0m  [25/53], [94mLoss[0m : 2.56829
[1mStep[0m  [30/53], [94mLoss[0m : 2.48646
[1mStep[0m  [35/53], [94mLoss[0m : 2.58494
[1mStep[0m  [40/53], [94mLoss[0m : 2.48433
[1mStep[0m  [45/53], [94mLoss[0m : 2.30038
[1mStep[0m  [50/53], [94mLoss[0m : 2.53158

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.531, [92mTest[0m: 2.418, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.72999
[1mStep[0m  [5/53], [94mLoss[0m : 2.21729
[1mStep[0m  [10/53], [94mLoss[0m : 2.45806
[1mStep[0m  [15/53], [94mLoss[0m : 2.30313
[1mStep[0m  [20/53], [94mLoss[0m : 2.49673
[1mStep[0m  [25/53], [94mLoss[0m : 2.46863
[1mStep[0m  [30/53], [94mLoss[0m : 2.48059
[1mStep[0m  [35/53], [94mLoss[0m : 2.52627
[1mStep[0m  [40/53], [94mLoss[0m : 2.53719
[1mStep[0m  [45/53], [94mLoss[0m : 2.64277
[1mStep[0m  [50/53], [94mLoss[0m : 2.64599

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.380, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.36278
[1mStep[0m  [5/53], [94mLoss[0m : 2.50359
[1mStep[0m  [10/53], [94mLoss[0m : 2.37667
[1mStep[0m  [15/53], [94mLoss[0m : 2.68274
[1mStep[0m  [20/53], [94mLoss[0m : 2.50428
[1mStep[0m  [25/53], [94mLoss[0m : 2.44441
[1mStep[0m  [30/53], [94mLoss[0m : 2.60891
[1mStep[0m  [35/53], [94mLoss[0m : 2.78091
[1mStep[0m  [40/53], [94mLoss[0m : 2.34835
[1mStep[0m  [45/53], [94mLoss[0m : 2.53209
[1mStep[0m  [50/53], [94mLoss[0m : 2.47623

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.528, [92mTest[0m: 2.373, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.65842
[1mStep[0m  [5/53], [94mLoss[0m : 2.57781
[1mStep[0m  [10/53], [94mLoss[0m : 2.56650
[1mStep[0m  [15/53], [94mLoss[0m : 2.59166
[1mStep[0m  [20/53], [94mLoss[0m : 2.64296
[1mStep[0m  [25/53], [94mLoss[0m : 2.21801
[1mStep[0m  [30/53], [94mLoss[0m : 2.54498
[1mStep[0m  [35/53], [94mLoss[0m : 2.49166
[1mStep[0m  [40/53], [94mLoss[0m : 2.64943
[1mStep[0m  [45/53], [94mLoss[0m : 2.49161
[1mStep[0m  [50/53], [94mLoss[0m : 2.20596

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.517, [92mTest[0m: 2.359, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.69074
[1mStep[0m  [5/53], [94mLoss[0m : 2.37843
[1mStep[0m  [10/53], [94mLoss[0m : 2.60682
[1mStep[0m  [15/53], [94mLoss[0m : 2.44990
[1mStep[0m  [20/53], [94mLoss[0m : 2.36369
[1mStep[0m  [25/53], [94mLoss[0m : 2.47058
[1mStep[0m  [30/53], [94mLoss[0m : 2.58990
[1mStep[0m  [35/53], [94mLoss[0m : 2.58285
[1mStep[0m  [40/53], [94mLoss[0m : 2.77676
[1mStep[0m  [45/53], [94mLoss[0m : 2.59347
[1mStep[0m  [50/53], [94mLoss[0m : 2.38800

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.386, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.58993
[1mStep[0m  [5/53], [94mLoss[0m : 2.26519
[1mStep[0m  [10/53], [94mLoss[0m : 2.77749
[1mStep[0m  [15/53], [94mLoss[0m : 2.27932
[1mStep[0m  [20/53], [94mLoss[0m : 2.55981
[1mStep[0m  [25/53], [94mLoss[0m : 2.21263
[1mStep[0m  [30/53], [94mLoss[0m : 2.34354
[1mStep[0m  [35/53], [94mLoss[0m : 2.46341
[1mStep[0m  [40/53], [94mLoss[0m : 2.70216
[1mStep[0m  [45/53], [94mLoss[0m : 2.68443
[1mStep[0m  [50/53], [94mLoss[0m : 2.66065

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.366, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.26717
[1mStep[0m  [5/53], [94mLoss[0m : 2.49411
[1mStep[0m  [10/53], [94mLoss[0m : 2.63492
[1mStep[0m  [15/53], [94mLoss[0m : 2.43361
[1mStep[0m  [20/53], [94mLoss[0m : 2.51275
[1mStep[0m  [25/53], [94mLoss[0m : 2.35940
[1mStep[0m  [30/53], [94mLoss[0m : 2.42304
[1mStep[0m  [35/53], [94mLoss[0m : 2.57023
[1mStep[0m  [40/53], [94mLoss[0m : 2.33774
[1mStep[0m  [45/53], [94mLoss[0m : 2.34624
[1mStep[0m  [50/53], [94mLoss[0m : 2.41472

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.372, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.31257
[1mStep[0m  [5/53], [94mLoss[0m : 2.43891
[1mStep[0m  [10/53], [94mLoss[0m : 2.53722
[1mStep[0m  [15/53], [94mLoss[0m : 2.59572
[1mStep[0m  [20/53], [94mLoss[0m : 2.28056
[1mStep[0m  [25/53], [94mLoss[0m : 2.36806
[1mStep[0m  [30/53], [94mLoss[0m : 2.39251
[1mStep[0m  [35/53], [94mLoss[0m : 2.50174
[1mStep[0m  [40/53], [94mLoss[0m : 2.49657
[1mStep[0m  [45/53], [94mLoss[0m : 2.57235
[1mStep[0m  [50/53], [94mLoss[0m : 2.49570

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.346, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.32001
[1mStep[0m  [5/53], [94mLoss[0m : 2.42965
[1mStep[0m  [10/53], [94mLoss[0m : 2.33027
[1mStep[0m  [15/53], [94mLoss[0m : 2.30887
[1mStep[0m  [20/53], [94mLoss[0m : 2.44321
[1mStep[0m  [25/53], [94mLoss[0m : 2.15103
[1mStep[0m  [30/53], [94mLoss[0m : 2.48512
[1mStep[0m  [35/53], [94mLoss[0m : 2.44214
[1mStep[0m  [40/53], [94mLoss[0m : 2.72091
[1mStep[0m  [45/53], [94mLoss[0m : 2.60878
[1mStep[0m  [50/53], [94mLoss[0m : 2.41091

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.363, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.43770
[1mStep[0m  [5/53], [94mLoss[0m : 2.56456
[1mStep[0m  [10/53], [94mLoss[0m : 2.41490
[1mStep[0m  [15/53], [94mLoss[0m : 2.45122
[1mStep[0m  [20/53], [94mLoss[0m : 2.39340
[1mStep[0m  [25/53], [94mLoss[0m : 2.47246
[1mStep[0m  [30/53], [94mLoss[0m : 2.59854
[1mStep[0m  [35/53], [94mLoss[0m : 2.20130
[1mStep[0m  [40/53], [94mLoss[0m : 2.53165
[1mStep[0m  [45/53], [94mLoss[0m : 2.37764
[1mStep[0m  [50/53], [94mLoss[0m : 2.37366

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.359, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.41901
[1mStep[0m  [5/53], [94mLoss[0m : 2.52412
[1mStep[0m  [10/53], [94mLoss[0m : 2.53640
[1mStep[0m  [15/53], [94mLoss[0m : 2.55674
[1mStep[0m  [20/53], [94mLoss[0m : 2.28314
[1mStep[0m  [25/53], [94mLoss[0m : 2.53328
[1mStep[0m  [30/53], [94mLoss[0m : 2.48089
[1mStep[0m  [35/53], [94mLoss[0m : 2.17754
[1mStep[0m  [40/53], [94mLoss[0m : 2.41607
[1mStep[0m  [45/53], [94mLoss[0m : 2.23846
[1mStep[0m  [50/53], [94mLoss[0m : 2.24110

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.347, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.55661
[1mStep[0m  [5/53], [94mLoss[0m : 2.40691
[1mStep[0m  [10/53], [94mLoss[0m : 2.55328
[1mStep[0m  [15/53], [94mLoss[0m : 2.29285
[1mStep[0m  [20/53], [94mLoss[0m : 2.47031
[1mStep[0m  [25/53], [94mLoss[0m : 2.43036
[1mStep[0m  [30/53], [94mLoss[0m : 2.48786
[1mStep[0m  [35/53], [94mLoss[0m : 2.51568
[1mStep[0m  [40/53], [94mLoss[0m : 2.39115
[1mStep[0m  [45/53], [94mLoss[0m : 2.32662
[1mStep[0m  [50/53], [94mLoss[0m : 2.62256

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.356, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.25062
[1mStep[0m  [5/53], [94mLoss[0m : 2.69651
[1mStep[0m  [10/53], [94mLoss[0m : 2.61849
[1mStep[0m  [15/53], [94mLoss[0m : 2.59835
[1mStep[0m  [20/53], [94mLoss[0m : 2.20094
[1mStep[0m  [25/53], [94mLoss[0m : 2.38013
[1mStep[0m  [30/53], [94mLoss[0m : 2.71537
[1mStep[0m  [35/53], [94mLoss[0m : 2.31281
[1mStep[0m  [40/53], [94mLoss[0m : 2.39455
[1mStep[0m  [45/53], [94mLoss[0m : 2.50947
[1mStep[0m  [50/53], [94mLoss[0m : 2.27615

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.342, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.44486
[1mStep[0m  [5/53], [94mLoss[0m : 2.35999
[1mStep[0m  [10/53], [94mLoss[0m : 2.50981
[1mStep[0m  [15/53], [94mLoss[0m : 2.64000
[1mStep[0m  [20/53], [94mLoss[0m : 2.42761
[1mStep[0m  [25/53], [94mLoss[0m : 2.55129
[1mStep[0m  [30/53], [94mLoss[0m : 2.29391
[1mStep[0m  [35/53], [94mLoss[0m : 2.20536
[1mStep[0m  [40/53], [94mLoss[0m : 2.56835
[1mStep[0m  [45/53], [94mLoss[0m : 2.36358
[1mStep[0m  [50/53], [94mLoss[0m : 2.43184

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.349, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.39521
[1mStep[0m  [5/53], [94mLoss[0m : 2.28945
[1mStep[0m  [10/53], [94mLoss[0m : 2.44592
[1mStep[0m  [15/53], [94mLoss[0m : 2.60692
[1mStep[0m  [20/53], [94mLoss[0m : 2.54058
[1mStep[0m  [25/53], [94mLoss[0m : 2.17252
[1mStep[0m  [30/53], [94mLoss[0m : 2.52854
[1mStep[0m  [35/53], [94mLoss[0m : 2.41594
[1mStep[0m  [40/53], [94mLoss[0m : 2.38166
[1mStep[0m  [45/53], [94mLoss[0m : 2.24345
[1mStep[0m  [50/53], [94mLoss[0m : 2.47815

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.350, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.49501
[1mStep[0m  [5/53], [94mLoss[0m : 2.56069
[1mStep[0m  [10/53], [94mLoss[0m : 2.51519
[1mStep[0m  [15/53], [94mLoss[0m : 2.49434
[1mStep[0m  [20/53], [94mLoss[0m : 2.28167
[1mStep[0m  [25/53], [94mLoss[0m : 2.33852
[1mStep[0m  [30/53], [94mLoss[0m : 2.22753
[1mStep[0m  [35/53], [94mLoss[0m : 2.80166
[1mStep[0m  [40/53], [94mLoss[0m : 2.58939
[1mStep[0m  [45/53], [94mLoss[0m : 2.54661
[1mStep[0m  [50/53], [94mLoss[0m : 2.44782

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.358, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.45330
[1mStep[0m  [5/53], [94mLoss[0m : 2.25935
[1mStep[0m  [10/53], [94mLoss[0m : 2.42112
[1mStep[0m  [15/53], [94mLoss[0m : 2.56630
[1mStep[0m  [20/53], [94mLoss[0m : 2.60014
[1mStep[0m  [25/53], [94mLoss[0m : 2.58311
[1mStep[0m  [30/53], [94mLoss[0m : 2.32014
[1mStep[0m  [35/53], [94mLoss[0m : 2.58205
[1mStep[0m  [40/53], [94mLoss[0m : 2.63376
[1mStep[0m  [45/53], [94mLoss[0m : 2.51223
[1mStep[0m  [50/53], [94mLoss[0m : 2.48340

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.351, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.349
====================================

Phase 1 - Evaluation MAE:  2.3485023700273953
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/53], [94mLoss[0m : 2.37561
[1mStep[0m  [5/53], [94mLoss[0m : 2.50285
[1mStep[0m  [10/53], [94mLoss[0m : 2.42410
[1mStep[0m  [15/53], [94mLoss[0m : 2.26916
[1mStep[0m  [20/53], [94mLoss[0m : 2.51325
[1mStep[0m  [25/53], [94mLoss[0m : 2.62692
[1mStep[0m  [30/53], [94mLoss[0m : 2.45240
[1mStep[0m  [35/53], [94mLoss[0m : 2.52352
[1mStep[0m  [40/53], [94mLoss[0m : 2.62867
[1mStep[0m  [45/53], [94mLoss[0m : 2.65591
[1mStep[0m  [50/53], [94mLoss[0m : 2.48880

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.534, [92mTest[0m: 2.346, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.20882
[1mStep[0m  [5/53], [94mLoss[0m : 2.49246
[1mStep[0m  [10/53], [94mLoss[0m : 2.54036
[1mStep[0m  [15/53], [94mLoss[0m : 2.42653
[1mStep[0m  [20/53], [94mLoss[0m : 2.50292
[1mStep[0m  [25/53], [94mLoss[0m : 2.56052
[1mStep[0m  [30/53], [94mLoss[0m : 2.41681
[1mStep[0m  [35/53], [94mLoss[0m : 2.40074
[1mStep[0m  [40/53], [94mLoss[0m : 2.52986
[1mStep[0m  [45/53], [94mLoss[0m : 2.51595
[1mStep[0m  [50/53], [94mLoss[0m : 2.45546

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.346, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.50837
[1mStep[0m  [5/53], [94mLoss[0m : 2.56578
[1mStep[0m  [10/53], [94mLoss[0m : 2.14276
[1mStep[0m  [15/53], [94mLoss[0m : 2.47623
[1mStep[0m  [20/53], [94mLoss[0m : 2.20925
[1mStep[0m  [25/53], [94mLoss[0m : 2.61254
[1mStep[0m  [30/53], [94mLoss[0m : 2.61420
[1mStep[0m  [35/53], [94mLoss[0m : 2.36740
[1mStep[0m  [40/53], [94mLoss[0m : 2.43477
[1mStep[0m  [45/53], [94mLoss[0m : 2.43148
[1mStep[0m  [50/53], [94mLoss[0m : 2.34102

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.366, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.42131
[1mStep[0m  [5/53], [94mLoss[0m : 2.42841
[1mStep[0m  [10/53], [94mLoss[0m : 2.30802
[1mStep[0m  [15/53], [94mLoss[0m : 2.34601
[1mStep[0m  [20/53], [94mLoss[0m : 2.22776
[1mStep[0m  [25/53], [94mLoss[0m : 2.55187
[1mStep[0m  [30/53], [94mLoss[0m : 2.41824
[1mStep[0m  [35/53], [94mLoss[0m : 2.42976
[1mStep[0m  [40/53], [94mLoss[0m : 2.23793
[1mStep[0m  [45/53], [94mLoss[0m : 2.33873
[1mStep[0m  [50/53], [94mLoss[0m : 2.38138

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.341, [92mTest[0m: 2.336, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.29421
[1mStep[0m  [5/53], [94mLoss[0m : 2.33611
[1mStep[0m  [10/53], [94mLoss[0m : 2.15542
[1mStep[0m  [15/53], [94mLoss[0m : 2.29817
[1mStep[0m  [20/53], [94mLoss[0m : 2.31339
[1mStep[0m  [25/53], [94mLoss[0m : 2.33075
[1mStep[0m  [30/53], [94mLoss[0m : 2.29699
[1mStep[0m  [35/53], [94mLoss[0m : 2.16628
[1mStep[0m  [40/53], [94mLoss[0m : 2.40095
[1mStep[0m  [45/53], [94mLoss[0m : 2.49398
[1mStep[0m  [50/53], [94mLoss[0m : 2.31282

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.284, [92mTest[0m: 2.354, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.07476
[1mStep[0m  [5/53], [94mLoss[0m : 1.98846
[1mStep[0m  [10/53], [94mLoss[0m : 2.18938
[1mStep[0m  [15/53], [94mLoss[0m : 2.13403
[1mStep[0m  [20/53], [94mLoss[0m : 2.28419
[1mStep[0m  [25/53], [94mLoss[0m : 2.32107
[1mStep[0m  [30/53], [94mLoss[0m : 2.27652
[1mStep[0m  [35/53], [94mLoss[0m : 2.13231
[1mStep[0m  [40/53], [94mLoss[0m : 2.14712
[1mStep[0m  [45/53], [94mLoss[0m : 2.36298
[1mStep[0m  [50/53], [94mLoss[0m : 2.20756

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.246, [92mTest[0m: 2.335, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.19056
[1mStep[0m  [5/53], [94mLoss[0m : 2.11710
[1mStep[0m  [10/53], [94mLoss[0m : 2.36329
[1mStep[0m  [15/53], [94mLoss[0m : 2.06899
[1mStep[0m  [20/53], [94mLoss[0m : 2.10025
[1mStep[0m  [25/53], [94mLoss[0m : 2.11335
[1mStep[0m  [30/53], [94mLoss[0m : 2.22436
[1mStep[0m  [35/53], [94mLoss[0m : 2.26200
[1mStep[0m  [40/53], [94mLoss[0m : 2.03262
[1mStep[0m  [45/53], [94mLoss[0m : 2.10402
[1mStep[0m  [50/53], [94mLoss[0m : 2.21850

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.179, [92mTest[0m: 2.354, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.12425
[1mStep[0m  [5/53], [94mLoss[0m : 2.05303
[1mStep[0m  [10/53], [94mLoss[0m : 1.87263
[1mStep[0m  [15/53], [94mLoss[0m : 2.12002
[1mStep[0m  [20/53], [94mLoss[0m : 1.98830
[1mStep[0m  [25/53], [94mLoss[0m : 2.22508
[1mStep[0m  [30/53], [94mLoss[0m : 2.04465
[1mStep[0m  [35/53], [94mLoss[0m : 2.38091
[1mStep[0m  [40/53], [94mLoss[0m : 2.13770
[1mStep[0m  [45/53], [94mLoss[0m : 2.17123
[1mStep[0m  [50/53], [94mLoss[0m : 2.08108

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.147, [92mTest[0m: 2.371, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.17621
[1mStep[0m  [5/53], [94mLoss[0m : 1.98877
[1mStep[0m  [10/53], [94mLoss[0m : 1.97985
[1mStep[0m  [15/53], [94mLoss[0m : 1.92960
[1mStep[0m  [20/53], [94mLoss[0m : 2.22399
[1mStep[0m  [25/53], [94mLoss[0m : 1.88512
[1mStep[0m  [30/53], [94mLoss[0m : 2.00501
[1mStep[0m  [35/53], [94mLoss[0m : 1.99170
[1mStep[0m  [40/53], [94mLoss[0m : 2.33125
[1mStep[0m  [45/53], [94mLoss[0m : 2.07462
[1mStep[0m  [50/53], [94mLoss[0m : 2.09364

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.084, [92mTest[0m: 2.373, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.08018
[1mStep[0m  [5/53], [94mLoss[0m : 2.04838
[1mStep[0m  [10/53], [94mLoss[0m : 2.29291
[1mStep[0m  [15/53], [94mLoss[0m : 2.12664
[1mStep[0m  [20/53], [94mLoss[0m : 2.11971
[1mStep[0m  [25/53], [94mLoss[0m : 2.03956
[1mStep[0m  [30/53], [94mLoss[0m : 1.90358
[1mStep[0m  [35/53], [94mLoss[0m : 2.12530
[1mStep[0m  [40/53], [94mLoss[0m : 1.99919
[1mStep[0m  [45/53], [94mLoss[0m : 1.93036
[1mStep[0m  [50/53], [94mLoss[0m : 2.29575

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.032, [92mTest[0m: 2.388, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.92682
[1mStep[0m  [5/53], [94mLoss[0m : 1.92661
[1mStep[0m  [10/53], [94mLoss[0m : 2.08500
[1mStep[0m  [15/53], [94mLoss[0m : 2.08258
[1mStep[0m  [20/53], [94mLoss[0m : 1.85754
[1mStep[0m  [25/53], [94mLoss[0m : 2.05682
[1mStep[0m  [30/53], [94mLoss[0m : 1.83662
[1mStep[0m  [35/53], [94mLoss[0m : 2.07384
[1mStep[0m  [40/53], [94mLoss[0m : 2.21234
[1mStep[0m  [45/53], [94mLoss[0m : 1.93341
[1mStep[0m  [50/53], [94mLoss[0m : 1.93817

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.016, [92mTest[0m: 2.405, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.84200
[1mStep[0m  [5/53], [94mLoss[0m : 1.87630
[1mStep[0m  [10/53], [94mLoss[0m : 1.73385
[1mStep[0m  [15/53], [94mLoss[0m : 1.71697
[1mStep[0m  [20/53], [94mLoss[0m : 1.98322
[1mStep[0m  [25/53], [94mLoss[0m : 1.91070
[1mStep[0m  [30/53], [94mLoss[0m : 1.94938
[1mStep[0m  [35/53], [94mLoss[0m : 1.84636
[1mStep[0m  [40/53], [94mLoss[0m : 2.11684
[1mStep[0m  [45/53], [94mLoss[0m : 1.98641
[1mStep[0m  [50/53], [94mLoss[0m : 1.96643

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.962, [92mTest[0m: 2.423, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.90135
[1mStep[0m  [5/53], [94mLoss[0m : 1.94256
[1mStep[0m  [10/53], [94mLoss[0m : 1.85601
[1mStep[0m  [15/53], [94mLoss[0m : 2.03790
[1mStep[0m  [20/53], [94mLoss[0m : 1.89818
[1mStep[0m  [25/53], [94mLoss[0m : 1.74236
[1mStep[0m  [30/53], [94mLoss[0m : 1.87019
[1mStep[0m  [35/53], [94mLoss[0m : 1.80584
[1mStep[0m  [40/53], [94mLoss[0m : 1.97113
[1mStep[0m  [45/53], [94mLoss[0m : 1.97528
[1mStep[0m  [50/53], [94mLoss[0m : 2.07696

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.932, [92mTest[0m: 2.414, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.71702
[1mStep[0m  [5/53], [94mLoss[0m : 1.82053
[1mStep[0m  [10/53], [94mLoss[0m : 1.92586
[1mStep[0m  [15/53], [94mLoss[0m : 1.99864
[1mStep[0m  [20/53], [94mLoss[0m : 2.03897
[1mStep[0m  [25/53], [94mLoss[0m : 1.97443
[1mStep[0m  [30/53], [94mLoss[0m : 1.85598
[1mStep[0m  [35/53], [94mLoss[0m : 1.86575
[1mStep[0m  [40/53], [94mLoss[0m : 1.89187
[1mStep[0m  [45/53], [94mLoss[0m : 1.94960
[1mStep[0m  [50/53], [94mLoss[0m : 2.01601

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.903, [92mTest[0m: 2.426, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.78138
[1mStep[0m  [5/53], [94mLoss[0m : 1.86852
[1mStep[0m  [10/53], [94mLoss[0m : 1.72716
[1mStep[0m  [15/53], [94mLoss[0m : 1.94677
[1mStep[0m  [20/53], [94mLoss[0m : 1.89840
[1mStep[0m  [25/53], [94mLoss[0m : 1.81067
[1mStep[0m  [30/53], [94mLoss[0m : 1.76234
[1mStep[0m  [35/53], [94mLoss[0m : 1.82704
[1mStep[0m  [40/53], [94mLoss[0m : 1.95616
[1mStep[0m  [45/53], [94mLoss[0m : 1.91568
[1mStep[0m  [50/53], [94mLoss[0m : 1.86883

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.862, [92mTest[0m: 2.425, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.76890
[1mStep[0m  [5/53], [94mLoss[0m : 1.96506
[1mStep[0m  [10/53], [94mLoss[0m : 1.79925
[1mStep[0m  [15/53], [94mLoss[0m : 1.76440
[1mStep[0m  [20/53], [94mLoss[0m : 1.86877
[1mStep[0m  [25/53], [94mLoss[0m : 1.80869
[1mStep[0m  [30/53], [94mLoss[0m : 1.68155
[1mStep[0m  [35/53], [94mLoss[0m : 1.84804
[1mStep[0m  [40/53], [94mLoss[0m : 1.88103
[1mStep[0m  [45/53], [94mLoss[0m : 1.98837
[1mStep[0m  [50/53], [94mLoss[0m : 1.92956

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.823, [92mTest[0m: 2.488, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.73273
[1mStep[0m  [5/53], [94mLoss[0m : 1.70447
[1mStep[0m  [10/53], [94mLoss[0m : 1.72337
[1mStep[0m  [15/53], [94mLoss[0m : 1.73373
[1mStep[0m  [20/53], [94mLoss[0m : 2.00144
[1mStep[0m  [25/53], [94mLoss[0m : 1.76815
[1mStep[0m  [30/53], [94mLoss[0m : 1.59351
[1mStep[0m  [35/53], [94mLoss[0m : 1.74939
[1mStep[0m  [40/53], [94mLoss[0m : 1.85615
[1mStep[0m  [45/53], [94mLoss[0m : 1.87800
[1mStep[0m  [50/53], [94mLoss[0m : 1.81785

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.806, [92mTest[0m: 2.463, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.84030
[1mStep[0m  [5/53], [94mLoss[0m : 1.82116
[1mStep[0m  [10/53], [94mLoss[0m : 1.68442
[1mStep[0m  [15/53], [94mLoss[0m : 1.81828
[1mStep[0m  [20/53], [94mLoss[0m : 1.71898
[1mStep[0m  [25/53], [94mLoss[0m : 1.67138
[1mStep[0m  [30/53], [94mLoss[0m : 1.86932
[1mStep[0m  [35/53], [94mLoss[0m : 1.77409
[1mStep[0m  [40/53], [94mLoss[0m : 1.74348
[1mStep[0m  [45/53], [94mLoss[0m : 1.77995
[1mStep[0m  [50/53], [94mLoss[0m : 2.03358

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.783, [92mTest[0m: 2.458, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.76538
[1mStep[0m  [5/53], [94mLoss[0m : 1.92585
[1mStep[0m  [10/53], [94mLoss[0m : 1.69488
[1mStep[0m  [15/53], [94mLoss[0m : 1.75656
[1mStep[0m  [20/53], [94mLoss[0m : 1.73994
[1mStep[0m  [25/53], [94mLoss[0m : 1.66754
[1mStep[0m  [30/53], [94mLoss[0m : 1.78336
[1mStep[0m  [35/53], [94mLoss[0m : 1.97387
[1mStep[0m  [40/53], [94mLoss[0m : 1.55961
[1mStep[0m  [45/53], [94mLoss[0m : 1.78838
[1mStep[0m  [50/53], [94mLoss[0m : 1.79090

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.741, [92mTest[0m: 2.478, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.71511
[1mStep[0m  [5/53], [94mLoss[0m : 1.75970
[1mStep[0m  [10/53], [94mLoss[0m : 1.67739
[1mStep[0m  [15/53], [94mLoss[0m : 1.68385
[1mStep[0m  [20/53], [94mLoss[0m : 1.83567
[1mStep[0m  [25/53], [94mLoss[0m : 1.68500
[1mStep[0m  [30/53], [94mLoss[0m : 1.69706
[1mStep[0m  [35/53], [94mLoss[0m : 1.86178
[1mStep[0m  [40/53], [94mLoss[0m : 1.66316
[1mStep[0m  [45/53], [94mLoss[0m : 1.81335
[1mStep[0m  [50/53], [94mLoss[0m : 1.76020

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.714, [92mTest[0m: 2.471, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.55166
[1mStep[0m  [5/53], [94mLoss[0m : 1.44417
[1mStep[0m  [10/53], [94mLoss[0m : 1.81600
[1mStep[0m  [15/53], [94mLoss[0m : 1.72563
[1mStep[0m  [20/53], [94mLoss[0m : 1.54249
[1mStep[0m  [25/53], [94mLoss[0m : 1.68901
[1mStep[0m  [30/53], [94mLoss[0m : 1.80391
[1mStep[0m  [35/53], [94mLoss[0m : 1.73613
[1mStep[0m  [40/53], [94mLoss[0m : 1.50907
[1mStep[0m  [45/53], [94mLoss[0m : 1.91196
[1mStep[0m  [50/53], [94mLoss[0m : 1.59259

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.698, [92mTest[0m: 2.476, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.59096
[1mStep[0m  [5/53], [94mLoss[0m : 1.66985
[1mStep[0m  [10/53], [94mLoss[0m : 1.44155
[1mStep[0m  [15/53], [94mLoss[0m : 1.56135
[1mStep[0m  [20/53], [94mLoss[0m : 1.73124
[1mStep[0m  [25/53], [94mLoss[0m : 1.67409
[1mStep[0m  [30/53], [94mLoss[0m : 1.62255
[1mStep[0m  [35/53], [94mLoss[0m : 1.55001
[1mStep[0m  [40/53], [94mLoss[0m : 1.70414
[1mStep[0m  [45/53], [94mLoss[0m : 1.62642
[1mStep[0m  [50/53], [94mLoss[0m : 1.74588

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.674, [92mTest[0m: 2.478, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.50870
[1mStep[0m  [5/53], [94mLoss[0m : 1.60302
[1mStep[0m  [10/53], [94mLoss[0m : 1.55598
[1mStep[0m  [15/53], [94mLoss[0m : 1.67702
[1mStep[0m  [20/53], [94mLoss[0m : 1.49739
[1mStep[0m  [25/53], [94mLoss[0m : 1.73949
[1mStep[0m  [30/53], [94mLoss[0m : 1.72324
[1mStep[0m  [35/53], [94mLoss[0m : 1.78428
[1mStep[0m  [40/53], [94mLoss[0m : 1.60544
[1mStep[0m  [45/53], [94mLoss[0m : 1.72990
[1mStep[0m  [50/53], [94mLoss[0m : 1.49917

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.639, [92mTest[0m: 2.465, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.52653
[1mStep[0m  [5/53], [94mLoss[0m : 1.56036
[1mStep[0m  [10/53], [94mLoss[0m : 1.67833
[1mStep[0m  [15/53], [94mLoss[0m : 1.60870
[1mStep[0m  [20/53], [94mLoss[0m : 1.54636
[1mStep[0m  [25/53], [94mLoss[0m : 1.58335
[1mStep[0m  [30/53], [94mLoss[0m : 1.60237
[1mStep[0m  [35/53], [94mLoss[0m : 1.66267
[1mStep[0m  [40/53], [94mLoss[0m : 1.69362
[1mStep[0m  [45/53], [94mLoss[0m : 1.58283
[1mStep[0m  [50/53], [94mLoss[0m : 1.58913

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.627, [92mTest[0m: 2.475, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.51967
[1mStep[0m  [5/53], [94mLoss[0m : 1.56348
[1mStep[0m  [10/53], [94mLoss[0m : 1.55472
[1mStep[0m  [15/53], [94mLoss[0m : 1.63301
[1mStep[0m  [20/53], [94mLoss[0m : 1.53353
[1mStep[0m  [25/53], [94mLoss[0m : 1.55006
[1mStep[0m  [30/53], [94mLoss[0m : 1.58902
[1mStep[0m  [35/53], [94mLoss[0m : 1.83478
[1mStep[0m  [40/53], [94mLoss[0m : 1.51972
[1mStep[0m  [45/53], [94mLoss[0m : 1.65307
[1mStep[0m  [50/53], [94mLoss[0m : 1.59832

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.594, [92mTest[0m: 2.504, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.49917
[1mStep[0m  [5/53], [94mLoss[0m : 1.48722
[1mStep[0m  [10/53], [94mLoss[0m : 1.48126
[1mStep[0m  [15/53], [94mLoss[0m : 1.56101
[1mStep[0m  [20/53], [94mLoss[0m : 1.47831
[1mStep[0m  [25/53], [94mLoss[0m : 1.55488
[1mStep[0m  [30/53], [94mLoss[0m : 1.36104
[1mStep[0m  [35/53], [94mLoss[0m : 1.54278
[1mStep[0m  [40/53], [94mLoss[0m : 1.50735
[1mStep[0m  [45/53], [94mLoss[0m : 1.60105
[1mStep[0m  [50/53], [94mLoss[0m : 1.62652

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.575, [92mTest[0m: 2.480, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.64305
[1mStep[0m  [5/53], [94mLoss[0m : 1.48581
[1mStep[0m  [10/53], [94mLoss[0m : 1.62737
[1mStep[0m  [15/53], [94mLoss[0m : 1.56931
[1mStep[0m  [20/53], [94mLoss[0m : 1.49201
[1mStep[0m  [25/53], [94mLoss[0m : 1.61531
[1mStep[0m  [30/53], [94mLoss[0m : 1.58138
[1mStep[0m  [35/53], [94mLoss[0m : 1.47344
[1mStep[0m  [40/53], [94mLoss[0m : 1.62779
[1mStep[0m  [45/53], [94mLoss[0m : 1.48337
[1mStep[0m  [50/53], [94mLoss[0m : 1.65229

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.577, [92mTest[0m: 2.496, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.46272
[1mStep[0m  [5/53], [94mLoss[0m : 1.54812
[1mStep[0m  [10/53], [94mLoss[0m : 1.68315
[1mStep[0m  [15/53], [94mLoss[0m : 1.58668
[1mStep[0m  [20/53], [94mLoss[0m : 1.47597
[1mStep[0m  [25/53], [94mLoss[0m : 1.53396
[1mStep[0m  [30/53], [94mLoss[0m : 1.59889
[1mStep[0m  [35/53], [94mLoss[0m : 1.46503
[1mStep[0m  [40/53], [94mLoss[0m : 1.53573
[1mStep[0m  [45/53], [94mLoss[0m : 1.49746
[1mStep[0m  [50/53], [94mLoss[0m : 1.57528

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.545, [92mTest[0m: 2.526, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.46282
[1mStep[0m  [5/53], [94mLoss[0m : 1.45686
[1mStep[0m  [10/53], [94mLoss[0m : 1.42813
[1mStep[0m  [15/53], [94mLoss[0m : 1.56203
[1mStep[0m  [20/53], [94mLoss[0m : 1.41447
[1mStep[0m  [25/53], [94mLoss[0m : 1.55825
[1mStep[0m  [30/53], [94mLoss[0m : 1.53115
[1mStep[0m  [35/53], [94mLoss[0m : 1.31704
[1mStep[0m  [40/53], [94mLoss[0m : 1.54548
[1mStep[0m  [45/53], [94mLoss[0m : 1.62370
[1mStep[0m  [50/53], [94mLoss[0m : 1.56827

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.529, [92mTest[0m: 2.484, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.51758
[1mStep[0m  [5/53], [94mLoss[0m : 1.44849
[1mStep[0m  [10/53], [94mLoss[0m : 1.41925
[1mStep[0m  [15/53], [94mLoss[0m : 1.54343
[1mStep[0m  [20/53], [94mLoss[0m : 1.59607
[1mStep[0m  [25/53], [94mLoss[0m : 1.57763
[1mStep[0m  [30/53], [94mLoss[0m : 1.50844
[1mStep[0m  [35/53], [94mLoss[0m : 1.58385
[1mStep[0m  [40/53], [94mLoss[0m : 1.59169
[1mStep[0m  [45/53], [94mLoss[0m : 1.64193
[1mStep[0m  [50/53], [94mLoss[0m : 1.50048

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.541, [92mTest[0m: 2.472, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.443
====================================

Phase 2 - Evaluation MAE:  2.442835706930894
MAE score P1       2.348502
MAE score P2       2.442836
loss               1.529356
learning_rate      0.002575
batch_size              256
hidden_sizes      [250, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.9
weight_decay           0.01
Name: 14, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/53], [94mLoss[0m : 11.04431
[1mStep[0m  [5/53], [94mLoss[0m : 11.30195
[1mStep[0m  [10/53], [94mLoss[0m : 10.21364
[1mStep[0m  [15/53], [94mLoss[0m : 10.48025
[1mStep[0m  [20/53], [94mLoss[0m : 10.10977
[1mStep[0m  [25/53], [94mLoss[0m : 10.09435
[1mStep[0m  [30/53], [94mLoss[0m : 9.84136
[1mStep[0m  [35/53], [94mLoss[0m : 9.31191
[1mStep[0m  [40/53], [94mLoss[0m : 9.07168
[1mStep[0m  [45/53], [94mLoss[0m : 9.15507
[1mStep[0m  [50/53], [94mLoss[0m : 8.64708

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.998, [92mTest[0m: 10.998, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.61808
[1mStep[0m  [5/53], [94mLoss[0m : 8.23980
[1mStep[0m  [10/53], [94mLoss[0m : 8.61723
[1mStep[0m  [15/53], [94mLoss[0m : 8.08457
[1mStep[0m  [20/53], [94mLoss[0m : 8.02116
[1mStep[0m  [25/53], [94mLoss[0m : 7.54589
[1mStep[0m  [30/53], [94mLoss[0m : 7.31016
[1mStep[0m  [35/53], [94mLoss[0m : 7.42028
[1mStep[0m  [40/53], [94mLoss[0m : 6.95034
[1mStep[0m  [45/53], [94mLoss[0m : 6.86109
[1mStep[0m  [50/53], [94mLoss[0m : 6.28546

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 7.566, [92mTest[0m: 9.622, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.39592
[1mStep[0m  [5/53], [94mLoss[0m : 5.59124
[1mStep[0m  [10/53], [94mLoss[0m : 6.03917
[1mStep[0m  [15/53], [94mLoss[0m : 5.68076
[1mStep[0m  [20/53], [94mLoss[0m : 5.20569
[1mStep[0m  [25/53], [94mLoss[0m : 4.83430
[1mStep[0m  [30/53], [94mLoss[0m : 4.92628
[1mStep[0m  [35/53], [94mLoss[0m : 4.48998
[1mStep[0m  [40/53], [94mLoss[0m : 4.74887
[1mStep[0m  [45/53], [94mLoss[0m : 4.47830
[1mStep[0m  [50/53], [94mLoss[0m : 4.17660

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 5.080, [92mTest[0m: 7.790, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.98296
[1mStep[0m  [5/53], [94mLoss[0m : 4.12588
[1mStep[0m  [10/53], [94mLoss[0m : 3.54315
[1mStep[0m  [15/53], [94mLoss[0m : 3.89014
[1mStep[0m  [20/53], [94mLoss[0m : 3.40207
[1mStep[0m  [25/53], [94mLoss[0m : 3.45139
[1mStep[0m  [30/53], [94mLoss[0m : 3.65226
[1mStep[0m  [35/53], [94mLoss[0m : 3.22141
[1mStep[0m  [40/53], [94mLoss[0m : 3.24104
[1mStep[0m  [45/53], [94mLoss[0m : 3.09471
[1mStep[0m  [50/53], [94mLoss[0m : 2.98678

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 3.522, [92mTest[0m: 5.434, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.22841
[1mStep[0m  [5/53], [94mLoss[0m : 3.06480
[1mStep[0m  [10/53], [94mLoss[0m : 3.15537
[1mStep[0m  [15/53], [94mLoss[0m : 2.87975
[1mStep[0m  [20/53], [94mLoss[0m : 3.17286
[1mStep[0m  [25/53], [94mLoss[0m : 3.19303
[1mStep[0m  [30/53], [94mLoss[0m : 3.12107
[1mStep[0m  [35/53], [94mLoss[0m : 2.80365
[1mStep[0m  [40/53], [94mLoss[0m : 2.83929
[1mStep[0m  [45/53], [94mLoss[0m : 2.89230
[1mStep[0m  [50/53], [94mLoss[0m : 3.01116

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 3.001, [92mTest[0m: 3.922, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.57859
[1mStep[0m  [5/53], [94mLoss[0m : 2.82041
[1mStep[0m  [10/53], [94mLoss[0m : 2.78888
[1mStep[0m  [15/53], [94mLoss[0m : 2.90393
[1mStep[0m  [20/53], [94mLoss[0m : 2.98194
[1mStep[0m  [25/53], [94mLoss[0m : 2.58974
[1mStep[0m  [30/53], [94mLoss[0m : 2.81228
[1mStep[0m  [35/53], [94mLoss[0m : 3.04295
[1mStep[0m  [40/53], [94mLoss[0m : 2.97731
[1mStep[0m  [45/53], [94mLoss[0m : 2.94216
[1mStep[0m  [50/53], [94mLoss[0m : 2.97159

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.860, [92mTest[0m: 3.292, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.72312
[1mStep[0m  [5/53], [94mLoss[0m : 2.92445
[1mStep[0m  [10/53], [94mLoss[0m : 2.60610
[1mStep[0m  [15/53], [94mLoss[0m : 2.50634
[1mStep[0m  [20/53], [94mLoss[0m : 2.90254
[1mStep[0m  [25/53], [94mLoss[0m : 2.73172
[1mStep[0m  [30/53], [94mLoss[0m : 2.88604
[1mStep[0m  [35/53], [94mLoss[0m : 2.97915
[1mStep[0m  [40/53], [94mLoss[0m : 2.74431
[1mStep[0m  [45/53], [94mLoss[0m : 2.63685
[1mStep[0m  [50/53], [94mLoss[0m : 2.85144

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.832, [92mTest[0m: 3.028, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.92120
[1mStep[0m  [5/53], [94mLoss[0m : 2.97598
[1mStep[0m  [10/53], [94mLoss[0m : 2.83810
[1mStep[0m  [15/53], [94mLoss[0m : 2.85792
[1mStep[0m  [20/53], [94mLoss[0m : 2.87318
[1mStep[0m  [25/53], [94mLoss[0m : 2.64961
[1mStep[0m  [30/53], [94mLoss[0m : 2.81325
[1mStep[0m  [35/53], [94mLoss[0m : 2.76336
[1mStep[0m  [40/53], [94mLoss[0m : 2.73419
[1mStep[0m  [45/53], [94mLoss[0m : 2.86597
[1mStep[0m  [50/53], [94mLoss[0m : 2.77516

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.805, [92mTest[0m: 2.817, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.54769
[1mStep[0m  [5/53], [94mLoss[0m : 2.87696
[1mStep[0m  [10/53], [94mLoss[0m : 2.60483
[1mStep[0m  [15/53], [94mLoss[0m : 2.82462
[1mStep[0m  [20/53], [94mLoss[0m : 2.79777
[1mStep[0m  [25/53], [94mLoss[0m : 2.82464
[1mStep[0m  [30/53], [94mLoss[0m : 2.68813
[1mStep[0m  [35/53], [94mLoss[0m : 2.47763
[1mStep[0m  [40/53], [94mLoss[0m : 2.60253
[1mStep[0m  [45/53], [94mLoss[0m : 2.63727
[1mStep[0m  [50/53], [94mLoss[0m : 2.89516

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.767, [92mTest[0m: 2.718, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.80426
[1mStep[0m  [5/53], [94mLoss[0m : 2.65230
[1mStep[0m  [10/53], [94mLoss[0m : 3.03639
[1mStep[0m  [15/53], [94mLoss[0m : 2.72029
[1mStep[0m  [20/53], [94mLoss[0m : 2.69596
[1mStep[0m  [25/53], [94mLoss[0m : 2.61339
[1mStep[0m  [30/53], [94mLoss[0m : 2.62958
[1mStep[0m  [35/53], [94mLoss[0m : 2.67246
[1mStep[0m  [40/53], [94mLoss[0m : 2.69735
[1mStep[0m  [45/53], [94mLoss[0m : 2.76415
[1mStep[0m  [50/53], [94mLoss[0m : 2.91558

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.752, [92mTest[0m: 2.683, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.66520
[1mStep[0m  [5/53], [94mLoss[0m : 2.85920
[1mStep[0m  [10/53], [94mLoss[0m : 2.80209
[1mStep[0m  [15/53], [94mLoss[0m : 2.81350
[1mStep[0m  [20/53], [94mLoss[0m : 2.74352
[1mStep[0m  [25/53], [94mLoss[0m : 2.80157
[1mStep[0m  [30/53], [94mLoss[0m : 2.87111
[1mStep[0m  [35/53], [94mLoss[0m : 2.77384
[1mStep[0m  [40/53], [94mLoss[0m : 2.63382
[1mStep[0m  [45/53], [94mLoss[0m : 2.76968
[1mStep[0m  [50/53], [94mLoss[0m : 2.71050

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.765, [92mTest[0m: 2.667, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.61033
[1mStep[0m  [5/53], [94mLoss[0m : 2.87399
[1mStep[0m  [10/53], [94mLoss[0m : 2.74175
[1mStep[0m  [15/53], [94mLoss[0m : 2.62411
[1mStep[0m  [20/53], [94mLoss[0m : 2.77532
[1mStep[0m  [25/53], [94mLoss[0m : 3.02293
[1mStep[0m  [30/53], [94mLoss[0m : 2.82323
[1mStep[0m  [35/53], [94mLoss[0m : 2.71898
[1mStep[0m  [40/53], [94mLoss[0m : 2.78221
[1mStep[0m  [45/53], [94mLoss[0m : 2.60865
[1mStep[0m  [50/53], [94mLoss[0m : 2.69647

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.722, [92mTest[0m: 2.630, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.86155
[1mStep[0m  [5/53], [94mLoss[0m : 2.71160
[1mStep[0m  [10/53], [94mLoss[0m : 2.72590
[1mStep[0m  [15/53], [94mLoss[0m : 2.64727
[1mStep[0m  [20/53], [94mLoss[0m : 2.78631
[1mStep[0m  [25/53], [94mLoss[0m : 2.65213
[1mStep[0m  [30/53], [94mLoss[0m : 2.63969
[1mStep[0m  [35/53], [94mLoss[0m : 2.74863
[1mStep[0m  [40/53], [94mLoss[0m : 2.82747
[1mStep[0m  [45/53], [94mLoss[0m : 2.89144
[1mStep[0m  [50/53], [94mLoss[0m : 2.71707

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.758, [92mTest[0m: 2.614, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.57059
[1mStep[0m  [5/53], [94mLoss[0m : 2.70366
[1mStep[0m  [10/53], [94mLoss[0m : 3.20435
[1mStep[0m  [15/53], [94mLoss[0m : 2.74509
[1mStep[0m  [20/53], [94mLoss[0m : 2.73342
[1mStep[0m  [25/53], [94mLoss[0m : 2.76857
[1mStep[0m  [30/53], [94mLoss[0m : 2.65372
[1mStep[0m  [35/53], [94mLoss[0m : 3.00710
[1mStep[0m  [40/53], [94mLoss[0m : 2.86835
[1mStep[0m  [45/53], [94mLoss[0m : 2.69316
[1mStep[0m  [50/53], [94mLoss[0m : 2.83202

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.726, [92mTest[0m: 2.568, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.82686
[1mStep[0m  [5/53], [94mLoss[0m : 2.93310
[1mStep[0m  [10/53], [94mLoss[0m : 2.63993
[1mStep[0m  [15/53], [94mLoss[0m : 2.67877
[1mStep[0m  [20/53], [94mLoss[0m : 2.96734
[1mStep[0m  [25/53], [94mLoss[0m : 2.74190
[1mStep[0m  [30/53], [94mLoss[0m : 2.67997
[1mStep[0m  [35/53], [94mLoss[0m : 2.73315
[1mStep[0m  [40/53], [94mLoss[0m : 2.65330
[1mStep[0m  [45/53], [94mLoss[0m : 2.72027
[1mStep[0m  [50/53], [94mLoss[0m : 2.68773

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.727, [92mTest[0m: 2.576, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.89629
[1mStep[0m  [5/53], [94mLoss[0m : 2.67579
[1mStep[0m  [10/53], [94mLoss[0m : 2.54273
[1mStep[0m  [15/53], [94mLoss[0m : 2.59495
[1mStep[0m  [20/53], [94mLoss[0m : 2.75572
[1mStep[0m  [25/53], [94mLoss[0m : 2.72019
[1mStep[0m  [30/53], [94mLoss[0m : 2.73024
[1mStep[0m  [35/53], [94mLoss[0m : 2.92634
[1mStep[0m  [40/53], [94mLoss[0m : 2.61726
[1mStep[0m  [45/53], [94mLoss[0m : 2.66804
[1mStep[0m  [50/53], [94mLoss[0m : 2.95455

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.692, [92mTest[0m: 2.532, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.77290
[1mStep[0m  [5/53], [94mLoss[0m : 2.90755
[1mStep[0m  [10/53], [94mLoss[0m : 2.76599
[1mStep[0m  [15/53], [94mLoss[0m : 2.60220
[1mStep[0m  [20/53], [94mLoss[0m : 3.02392
[1mStep[0m  [25/53], [94mLoss[0m : 2.86425
[1mStep[0m  [30/53], [94mLoss[0m : 2.72466
[1mStep[0m  [35/53], [94mLoss[0m : 2.65004
[1mStep[0m  [40/53], [94mLoss[0m : 2.60969
[1mStep[0m  [45/53], [94mLoss[0m : 2.45800
[1mStep[0m  [50/53], [94mLoss[0m : 2.54565

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.694, [92mTest[0m: 2.513, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.57783
[1mStep[0m  [5/53], [94mLoss[0m : 2.75021
[1mStep[0m  [10/53], [94mLoss[0m : 2.72270
[1mStep[0m  [15/53], [94mLoss[0m : 2.51599
[1mStep[0m  [20/53], [94mLoss[0m : 2.84394
[1mStep[0m  [25/53], [94mLoss[0m : 2.71268
[1mStep[0m  [30/53], [94mLoss[0m : 2.51432
[1mStep[0m  [35/53], [94mLoss[0m : 2.70804
[1mStep[0m  [40/53], [94mLoss[0m : 2.53477
[1mStep[0m  [45/53], [94mLoss[0m : 2.86451
[1mStep[0m  [50/53], [94mLoss[0m : 2.70233

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.673, [92mTest[0m: 2.509, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.68594
[1mStep[0m  [5/53], [94mLoss[0m : 2.65891
[1mStep[0m  [10/53], [94mLoss[0m : 2.69029
[1mStep[0m  [15/53], [94mLoss[0m : 2.62430
[1mStep[0m  [20/53], [94mLoss[0m : 2.71193
[1mStep[0m  [25/53], [94mLoss[0m : 2.79434
[1mStep[0m  [30/53], [94mLoss[0m : 2.72043
[1mStep[0m  [35/53], [94mLoss[0m : 2.78173
[1mStep[0m  [40/53], [94mLoss[0m : 2.64620
[1mStep[0m  [45/53], [94mLoss[0m : 2.71457
[1mStep[0m  [50/53], [94mLoss[0m : 2.69289

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.682, [92mTest[0m: 2.506, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.51910
[1mStep[0m  [5/53], [94mLoss[0m : 2.62095
[1mStep[0m  [10/53], [94mLoss[0m : 2.73204
[1mStep[0m  [15/53], [94mLoss[0m : 2.72315
[1mStep[0m  [20/53], [94mLoss[0m : 2.71258
[1mStep[0m  [25/53], [94mLoss[0m : 2.62551
[1mStep[0m  [30/53], [94mLoss[0m : 2.74137
[1mStep[0m  [35/53], [94mLoss[0m : 2.56701
[1mStep[0m  [40/53], [94mLoss[0m : 2.44883
[1mStep[0m  [45/53], [94mLoss[0m : 2.76917
[1mStep[0m  [50/53], [94mLoss[0m : 2.65129

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.681, [92mTest[0m: 2.509, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.83094
[1mStep[0m  [5/53], [94mLoss[0m : 2.69339
[1mStep[0m  [10/53], [94mLoss[0m : 2.64432
[1mStep[0m  [15/53], [94mLoss[0m : 2.67133
[1mStep[0m  [20/53], [94mLoss[0m : 2.75630
[1mStep[0m  [25/53], [94mLoss[0m : 2.72057
[1mStep[0m  [30/53], [94mLoss[0m : 2.63156
[1mStep[0m  [35/53], [94mLoss[0m : 2.57880
[1mStep[0m  [40/53], [94mLoss[0m : 2.53146
[1mStep[0m  [45/53], [94mLoss[0m : 2.58705
[1mStep[0m  [50/53], [94mLoss[0m : 2.71725

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.673, [92mTest[0m: 2.491, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.67508
[1mStep[0m  [5/53], [94mLoss[0m : 2.71227
[1mStep[0m  [10/53], [94mLoss[0m : 2.82243
[1mStep[0m  [15/53], [94mLoss[0m : 2.61784
[1mStep[0m  [20/53], [94mLoss[0m : 2.38260
[1mStep[0m  [25/53], [94mLoss[0m : 2.90147
[1mStep[0m  [30/53], [94mLoss[0m : 2.57462
[1mStep[0m  [35/53], [94mLoss[0m : 2.65752
[1mStep[0m  [40/53], [94mLoss[0m : 2.59128
[1mStep[0m  [45/53], [94mLoss[0m : 2.53051
[1mStep[0m  [50/53], [94mLoss[0m : 2.81170

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.673, [92mTest[0m: 2.474, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.70163
[1mStep[0m  [5/53], [94mLoss[0m : 2.56533
[1mStep[0m  [10/53], [94mLoss[0m : 2.84561
[1mStep[0m  [15/53], [94mLoss[0m : 2.77912
[1mStep[0m  [20/53], [94mLoss[0m : 2.86547
[1mStep[0m  [25/53], [94mLoss[0m : 2.54545
[1mStep[0m  [30/53], [94mLoss[0m : 2.69923
[1mStep[0m  [35/53], [94mLoss[0m : 2.66207
[1mStep[0m  [40/53], [94mLoss[0m : 2.68534
[1mStep[0m  [45/53], [94mLoss[0m : 2.61534
[1mStep[0m  [50/53], [94mLoss[0m : 3.08586

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.673, [92mTest[0m: 2.459, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.67101
[1mStep[0m  [5/53], [94mLoss[0m : 2.58488
[1mStep[0m  [10/53], [94mLoss[0m : 2.43882
[1mStep[0m  [15/53], [94mLoss[0m : 2.73575
[1mStep[0m  [20/53], [94mLoss[0m : 2.73352
[1mStep[0m  [25/53], [94mLoss[0m : 2.54540
[1mStep[0m  [30/53], [94mLoss[0m : 2.66133
[1mStep[0m  [35/53], [94mLoss[0m : 2.53354
[1mStep[0m  [40/53], [94mLoss[0m : 2.43653
[1mStep[0m  [45/53], [94mLoss[0m : 2.56833
[1mStep[0m  [50/53], [94mLoss[0m : 2.53536

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.643, [92mTest[0m: 2.457, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.80621
[1mStep[0m  [5/53], [94mLoss[0m : 2.57020
[1mStep[0m  [10/53], [94mLoss[0m : 2.62806
[1mStep[0m  [15/53], [94mLoss[0m : 2.66391
[1mStep[0m  [20/53], [94mLoss[0m : 2.72926
[1mStep[0m  [25/53], [94mLoss[0m : 2.69023
[1mStep[0m  [30/53], [94mLoss[0m : 2.85756
[1mStep[0m  [35/53], [94mLoss[0m : 2.79159
[1mStep[0m  [40/53], [94mLoss[0m : 2.50774
[1mStep[0m  [45/53], [94mLoss[0m : 2.68574
[1mStep[0m  [50/53], [94mLoss[0m : 2.81280

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.644, [92mTest[0m: 2.472, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.70908
[1mStep[0m  [5/53], [94mLoss[0m : 2.83895
[1mStep[0m  [10/53], [94mLoss[0m : 2.74707
[1mStep[0m  [15/53], [94mLoss[0m : 2.90242
[1mStep[0m  [20/53], [94mLoss[0m : 2.28569
[1mStep[0m  [25/53], [94mLoss[0m : 2.68199
[1mStep[0m  [30/53], [94mLoss[0m : 2.71547
[1mStep[0m  [35/53], [94mLoss[0m : 2.67205
[1mStep[0m  [40/53], [94mLoss[0m : 2.90635
[1mStep[0m  [45/53], [94mLoss[0m : 2.67854
[1mStep[0m  [50/53], [94mLoss[0m : 2.67650

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.659, [92mTest[0m: 2.433, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.59604
[1mStep[0m  [5/53], [94mLoss[0m : 2.72708
[1mStep[0m  [10/53], [94mLoss[0m : 2.75467
[1mStep[0m  [15/53], [94mLoss[0m : 2.64551
[1mStep[0m  [20/53], [94mLoss[0m : 2.73499
[1mStep[0m  [25/53], [94mLoss[0m : 2.72782
[1mStep[0m  [30/53], [94mLoss[0m : 2.62524
[1mStep[0m  [35/53], [94mLoss[0m : 2.59777
[1mStep[0m  [40/53], [94mLoss[0m : 2.83232
[1mStep[0m  [45/53], [94mLoss[0m : 2.67927
[1mStep[0m  [50/53], [94mLoss[0m : 2.41339

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.663, [92mTest[0m: 2.432, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.52062
[1mStep[0m  [5/53], [94mLoss[0m : 2.78774
[1mStep[0m  [10/53], [94mLoss[0m : 2.47951
[1mStep[0m  [15/53], [94mLoss[0m : 2.62367
[1mStep[0m  [20/53], [94mLoss[0m : 2.57728
[1mStep[0m  [25/53], [94mLoss[0m : 2.52599
[1mStep[0m  [30/53], [94mLoss[0m : 2.60364
[1mStep[0m  [35/53], [94mLoss[0m : 2.56358
[1mStep[0m  [40/53], [94mLoss[0m : 2.49024
[1mStep[0m  [45/53], [94mLoss[0m : 2.72011
[1mStep[0m  [50/53], [94mLoss[0m : 2.65441

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.627, [92mTest[0m: 2.433, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.81370
[1mStep[0m  [5/53], [94mLoss[0m : 2.65593
[1mStep[0m  [10/53], [94mLoss[0m : 2.79926
[1mStep[0m  [15/53], [94mLoss[0m : 2.62947
[1mStep[0m  [20/53], [94mLoss[0m : 2.46783
[1mStep[0m  [25/53], [94mLoss[0m : 2.65294
[1mStep[0m  [30/53], [94mLoss[0m : 2.72016
[1mStep[0m  [35/53], [94mLoss[0m : 2.56970
[1mStep[0m  [40/53], [94mLoss[0m : 2.36429
[1mStep[0m  [45/53], [94mLoss[0m : 2.80134
[1mStep[0m  [50/53], [94mLoss[0m : 2.61762

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.636, [92mTest[0m: 2.427, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.77300
[1mStep[0m  [5/53], [94mLoss[0m : 2.49521
[1mStep[0m  [10/53], [94mLoss[0m : 2.56680
[1mStep[0m  [15/53], [94mLoss[0m : 2.56544
[1mStep[0m  [20/53], [94mLoss[0m : 2.74390
[1mStep[0m  [25/53], [94mLoss[0m : 2.62675
[1mStep[0m  [30/53], [94mLoss[0m : 2.62596
[1mStep[0m  [35/53], [94mLoss[0m : 2.75446
[1mStep[0m  [40/53], [94mLoss[0m : 2.67697
[1mStep[0m  [45/53], [94mLoss[0m : 2.68136
[1mStep[0m  [50/53], [94mLoss[0m : 2.60746

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.629, [92mTest[0m: 2.453, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.430
====================================

Phase 1 - Evaluation MAE:  2.429964670768151
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
