no change     /home/modelrep/sadiya/miniconda/condabin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda-env
no change     /home/modelrep/sadiya/miniconda/bin/activate
no change     /home/modelrep/sadiya/miniconda/bin/deactivate
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.sh
no change     /home/modelrep/sadiya/miniconda/etc/fish/conf.d/conda.fish
no change     /home/modelrep/sadiya/miniconda/shell/condabin/Conda.psm1
no change     /home/modelrep/sadiya/miniconda/shell/condabin/conda-hook.ps1
no change     /home/modelrep/sadiya/miniconda/lib/python3.10/site-packages/xontrib/conda.xsh
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.csh
no change     /home/modelrep/sadiya/.bashrc
No action taken.
-----------------
Process ID:  5
Number of Nodes:  1
Number of Array Tasks:  16
-----------------
Using device: cuda

AMD Instinct MI210
Memory Usage:
Allocated: 0.0 GB
Cached:    0.0 GB
theta
whole_spec
delta
alpha
beta
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 11.09549
[1mStep[0m  [4/42], [94mLoss[0m : 10.27882
[1mStep[0m  [8/42], [94mLoss[0m : 9.17595
[1mStep[0m  [12/42], [94mLoss[0m : 8.65239
[1mStep[0m  [16/42], [94mLoss[0m : 7.12941
[1mStep[0m  [20/42], [94mLoss[0m : 7.02326
[1mStep[0m  [24/42], [94mLoss[0m : 5.52746
[1mStep[0m  [28/42], [94mLoss[0m : 5.11140
[1mStep[0m  [32/42], [94mLoss[0m : 4.71706
[1mStep[0m  [36/42], [94mLoss[0m : 4.01731
[1mStep[0m  [40/42], [94mLoss[0m : 3.79819

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.818, [92mTest[0m: 11.037, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.58091
[1mStep[0m  [4/42], [94mLoss[0m : 3.15506
[1mStep[0m  [8/42], [94mLoss[0m : 2.96377
[1mStep[0m  [12/42], [94mLoss[0m : 3.06302
[1mStep[0m  [16/42], [94mLoss[0m : 3.08670
[1mStep[0m  [20/42], [94mLoss[0m : 3.04098
[1mStep[0m  [24/42], [94mLoss[0m : 2.97825
[1mStep[0m  [28/42], [94mLoss[0m : 2.69790
[1mStep[0m  [32/42], [94mLoss[0m : 2.96646
[1mStep[0m  [36/42], [94mLoss[0m : 2.85404
[1mStep[0m  [40/42], [94mLoss[0m : 2.65633

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.972, [92mTest[0m: 3.425, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.88861
[1mStep[0m  [4/42], [94mLoss[0m : 2.51497
[1mStep[0m  [8/42], [94mLoss[0m : 2.67461
[1mStep[0m  [12/42], [94mLoss[0m : 2.79422
[1mStep[0m  [16/42], [94mLoss[0m : 2.82735
[1mStep[0m  [20/42], [94mLoss[0m : 2.69465
[1mStep[0m  [24/42], [94mLoss[0m : 2.76698
[1mStep[0m  [28/42], [94mLoss[0m : 2.79620
[1mStep[0m  [32/42], [94mLoss[0m : 2.67707
[1mStep[0m  [36/42], [94mLoss[0m : 2.76767
[1mStep[0m  [40/42], [94mLoss[0m : 2.74615

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.708, [92mTest[0m: 2.724, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62204
[1mStep[0m  [4/42], [94mLoss[0m : 2.57619
[1mStep[0m  [8/42], [94mLoss[0m : 2.50191
[1mStep[0m  [12/42], [94mLoss[0m : 2.52262
[1mStep[0m  [16/42], [94mLoss[0m : 2.45061
[1mStep[0m  [20/42], [94mLoss[0m : 2.77646
[1mStep[0m  [24/42], [94mLoss[0m : 2.45507
[1mStep[0m  [28/42], [94mLoss[0m : 2.72507
[1mStep[0m  [32/42], [94mLoss[0m : 2.76558
[1mStep[0m  [36/42], [94mLoss[0m : 2.57906
[1mStep[0m  [40/42], [94mLoss[0m : 2.63958

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.635, [92mTest[0m: 2.592, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24940
[1mStep[0m  [4/42], [94mLoss[0m : 2.63722
[1mStep[0m  [8/42], [94mLoss[0m : 2.50487
[1mStep[0m  [12/42], [94mLoss[0m : 2.83015
[1mStep[0m  [16/42], [94mLoss[0m : 2.47565
[1mStep[0m  [20/42], [94mLoss[0m : 2.70062
[1mStep[0m  [24/42], [94mLoss[0m : 2.53977
[1mStep[0m  [28/42], [94mLoss[0m : 2.38690
[1mStep[0m  [32/42], [94mLoss[0m : 2.66075
[1mStep[0m  [36/42], [94mLoss[0m : 2.61430
[1mStep[0m  [40/42], [94mLoss[0m : 2.80037

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.580, [92mTest[0m: 2.540, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.79473
[1mStep[0m  [4/42], [94mLoss[0m : 2.58653
[1mStep[0m  [8/42], [94mLoss[0m : 2.44494
[1mStep[0m  [12/42], [94mLoss[0m : 2.73452
[1mStep[0m  [16/42], [94mLoss[0m : 2.62585
[1mStep[0m  [20/42], [94mLoss[0m : 2.51405
[1mStep[0m  [24/42], [94mLoss[0m : 2.60882
[1mStep[0m  [28/42], [94mLoss[0m : 2.50920
[1mStep[0m  [32/42], [94mLoss[0m : 2.54005
[1mStep[0m  [36/42], [94mLoss[0m : 2.50013
[1mStep[0m  [40/42], [94mLoss[0m : 2.55005

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.565, [92mTest[0m: 2.507, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44216
[1mStep[0m  [4/42], [94mLoss[0m : 2.64800
[1mStep[0m  [8/42], [94mLoss[0m : 2.60009
[1mStep[0m  [12/42], [94mLoss[0m : 2.53395
[1mStep[0m  [16/42], [94mLoss[0m : 2.53530
[1mStep[0m  [20/42], [94mLoss[0m : 2.24692
[1mStep[0m  [24/42], [94mLoss[0m : 2.52378
[1mStep[0m  [28/42], [94mLoss[0m : 2.63714
[1mStep[0m  [32/42], [94mLoss[0m : 2.30431
[1mStep[0m  [36/42], [94mLoss[0m : 2.67334
[1mStep[0m  [40/42], [94mLoss[0m : 2.52592

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.537, [92mTest[0m: 2.478, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.63452
[1mStep[0m  [4/42], [94mLoss[0m : 2.72985
[1mStep[0m  [8/42], [94mLoss[0m : 2.51406
[1mStep[0m  [12/42], [94mLoss[0m : 2.68679
[1mStep[0m  [16/42], [94mLoss[0m : 2.39410
[1mStep[0m  [20/42], [94mLoss[0m : 2.60588
[1mStep[0m  [24/42], [94mLoss[0m : 2.57864
[1mStep[0m  [28/42], [94mLoss[0m : 2.32411
[1mStep[0m  [32/42], [94mLoss[0m : 2.60635
[1mStep[0m  [36/42], [94mLoss[0m : 2.64640
[1mStep[0m  [40/42], [94mLoss[0m : 2.59736

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.544, [92mTest[0m: 2.467, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59579
[1mStep[0m  [4/42], [94mLoss[0m : 2.57507
[1mStep[0m  [8/42], [94mLoss[0m : 2.50426
[1mStep[0m  [12/42], [94mLoss[0m : 2.37717
[1mStep[0m  [16/42], [94mLoss[0m : 2.67096
[1mStep[0m  [20/42], [94mLoss[0m : 2.48257
[1mStep[0m  [24/42], [94mLoss[0m : 2.48416
[1mStep[0m  [28/42], [94mLoss[0m : 2.44441
[1mStep[0m  [32/42], [94mLoss[0m : 2.64410
[1mStep[0m  [36/42], [94mLoss[0m : 2.28851
[1mStep[0m  [40/42], [94mLoss[0m : 2.61785

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.528, [92mTest[0m: 2.445, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64041
[1mStep[0m  [4/42], [94mLoss[0m : 2.25394
[1mStep[0m  [8/42], [94mLoss[0m : 2.55606
[1mStep[0m  [12/42], [94mLoss[0m : 2.52831
[1mStep[0m  [16/42], [94mLoss[0m : 2.40036
[1mStep[0m  [20/42], [94mLoss[0m : 2.49356
[1mStep[0m  [24/42], [94mLoss[0m : 2.46431
[1mStep[0m  [28/42], [94mLoss[0m : 2.46983
[1mStep[0m  [32/42], [94mLoss[0m : 2.44072
[1mStep[0m  [36/42], [94mLoss[0m : 2.37375
[1mStep[0m  [40/42], [94mLoss[0m : 2.50398

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.442, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48355
[1mStep[0m  [4/42], [94mLoss[0m : 2.63872
[1mStep[0m  [8/42], [94mLoss[0m : 2.44137
[1mStep[0m  [12/42], [94mLoss[0m : 2.52571
[1mStep[0m  [16/42], [94mLoss[0m : 2.62308
[1mStep[0m  [20/42], [94mLoss[0m : 2.72235
[1mStep[0m  [24/42], [94mLoss[0m : 2.67424
[1mStep[0m  [28/42], [94mLoss[0m : 2.68107
[1mStep[0m  [32/42], [94mLoss[0m : 2.43320
[1mStep[0m  [36/42], [94mLoss[0m : 2.49857
[1mStep[0m  [40/42], [94mLoss[0m : 2.51484

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.422, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53255
[1mStep[0m  [4/42], [94mLoss[0m : 2.41335
[1mStep[0m  [8/42], [94mLoss[0m : 2.45912
[1mStep[0m  [12/42], [94mLoss[0m : 2.38720
[1mStep[0m  [16/42], [94mLoss[0m : 2.55081
[1mStep[0m  [20/42], [94mLoss[0m : 2.49757
[1mStep[0m  [24/42], [94mLoss[0m : 2.40475
[1mStep[0m  [28/42], [94mLoss[0m : 2.37948
[1mStep[0m  [32/42], [94mLoss[0m : 2.56744
[1mStep[0m  [36/42], [94mLoss[0m : 2.66653
[1mStep[0m  [40/42], [94mLoss[0m : 2.55926

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.428, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57534
[1mStep[0m  [4/42], [94mLoss[0m : 2.58476
[1mStep[0m  [8/42], [94mLoss[0m : 2.51375
[1mStep[0m  [12/42], [94mLoss[0m : 2.50077
[1mStep[0m  [16/42], [94mLoss[0m : 2.57597
[1mStep[0m  [20/42], [94mLoss[0m : 2.40712
[1mStep[0m  [24/42], [94mLoss[0m : 2.44319
[1mStep[0m  [28/42], [94mLoss[0m : 2.40641
[1mStep[0m  [32/42], [94mLoss[0m : 2.51578
[1mStep[0m  [36/42], [94mLoss[0m : 2.76769
[1mStep[0m  [40/42], [94mLoss[0m : 2.44609

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.428, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46447
[1mStep[0m  [4/42], [94mLoss[0m : 2.75991
[1mStep[0m  [8/42], [94mLoss[0m : 2.49986
[1mStep[0m  [12/42], [94mLoss[0m : 2.35963
[1mStep[0m  [16/42], [94mLoss[0m : 2.51252
[1mStep[0m  [20/42], [94mLoss[0m : 2.54799
[1mStep[0m  [24/42], [94mLoss[0m : 2.35294
[1mStep[0m  [28/42], [94mLoss[0m : 2.33792
[1mStep[0m  [32/42], [94mLoss[0m : 2.58527
[1mStep[0m  [36/42], [94mLoss[0m : 2.55845
[1mStep[0m  [40/42], [94mLoss[0m : 2.68760

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.413, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.65579
[1mStep[0m  [4/42], [94mLoss[0m : 2.36670
[1mStep[0m  [8/42], [94mLoss[0m : 2.35050
[1mStep[0m  [12/42], [94mLoss[0m : 2.31148
[1mStep[0m  [16/42], [94mLoss[0m : 2.47409
[1mStep[0m  [20/42], [94mLoss[0m : 2.64980
[1mStep[0m  [24/42], [94mLoss[0m : 2.37347
[1mStep[0m  [28/42], [94mLoss[0m : 2.75177
[1mStep[0m  [32/42], [94mLoss[0m : 2.67344
[1mStep[0m  [36/42], [94mLoss[0m : 2.53081
[1mStep[0m  [40/42], [94mLoss[0m : 2.37325

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.405, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52222
[1mStep[0m  [4/42], [94mLoss[0m : 2.50651
[1mStep[0m  [8/42], [94mLoss[0m : 2.46268
[1mStep[0m  [12/42], [94mLoss[0m : 2.53005
[1mStep[0m  [16/42], [94mLoss[0m : 2.41261
[1mStep[0m  [20/42], [94mLoss[0m : 2.68462
[1mStep[0m  [24/42], [94mLoss[0m : 2.36972
[1mStep[0m  [28/42], [94mLoss[0m : 2.60704
[1mStep[0m  [32/42], [94mLoss[0m : 2.81822
[1mStep[0m  [36/42], [94mLoss[0m : 2.33075
[1mStep[0m  [40/42], [94mLoss[0m : 2.39883

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.394, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42096
[1mStep[0m  [4/42], [94mLoss[0m : 2.48847
[1mStep[0m  [8/42], [94mLoss[0m : 2.26561
[1mStep[0m  [12/42], [94mLoss[0m : 2.32797
[1mStep[0m  [16/42], [94mLoss[0m : 2.49769
[1mStep[0m  [20/42], [94mLoss[0m : 2.47771
[1mStep[0m  [24/42], [94mLoss[0m : 2.47030
[1mStep[0m  [28/42], [94mLoss[0m : 2.53928
[1mStep[0m  [32/42], [94mLoss[0m : 2.45188
[1mStep[0m  [36/42], [94mLoss[0m : 2.50084
[1mStep[0m  [40/42], [94mLoss[0m : 2.37363

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.399, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42877
[1mStep[0m  [4/42], [94mLoss[0m : 2.55264
[1mStep[0m  [8/42], [94mLoss[0m : 2.76151
[1mStep[0m  [12/42], [94mLoss[0m : 2.54597
[1mStep[0m  [16/42], [94mLoss[0m : 2.45667
[1mStep[0m  [20/42], [94mLoss[0m : 2.51466
[1mStep[0m  [24/42], [94mLoss[0m : 2.47669
[1mStep[0m  [28/42], [94mLoss[0m : 2.38760
[1mStep[0m  [32/42], [94mLoss[0m : 2.57851
[1mStep[0m  [36/42], [94mLoss[0m : 2.58782
[1mStep[0m  [40/42], [94mLoss[0m : 2.47426

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.395, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52108
[1mStep[0m  [4/42], [94mLoss[0m : 2.45917
[1mStep[0m  [8/42], [94mLoss[0m : 2.37677
[1mStep[0m  [12/42], [94mLoss[0m : 2.61832
[1mStep[0m  [16/42], [94mLoss[0m : 2.54130
[1mStep[0m  [20/42], [94mLoss[0m : 2.50246
[1mStep[0m  [24/42], [94mLoss[0m : 2.53270
[1mStep[0m  [28/42], [94mLoss[0m : 2.28312
[1mStep[0m  [32/42], [94mLoss[0m : 2.56471
[1mStep[0m  [36/42], [94mLoss[0m : 2.44219
[1mStep[0m  [40/42], [94mLoss[0m : 2.39028

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.387, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62700
[1mStep[0m  [4/42], [94mLoss[0m : 2.38866
[1mStep[0m  [8/42], [94mLoss[0m : 2.59883
[1mStep[0m  [12/42], [94mLoss[0m : 2.45344
[1mStep[0m  [16/42], [94mLoss[0m : 2.31688
[1mStep[0m  [20/42], [94mLoss[0m : 2.44767
[1mStep[0m  [24/42], [94mLoss[0m : 2.49019
[1mStep[0m  [28/42], [94mLoss[0m : 2.31278
[1mStep[0m  [32/42], [94mLoss[0m : 2.57140
[1mStep[0m  [36/42], [94mLoss[0m : 2.40102
[1mStep[0m  [40/42], [94mLoss[0m : 2.37042

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.392, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58611
[1mStep[0m  [4/42], [94mLoss[0m : 2.20473
[1mStep[0m  [8/42], [94mLoss[0m : 2.37767
[1mStep[0m  [12/42], [94mLoss[0m : 2.59529
[1mStep[0m  [16/42], [94mLoss[0m : 2.38096
[1mStep[0m  [20/42], [94mLoss[0m : 2.44908
[1mStep[0m  [24/42], [94mLoss[0m : 2.45133
[1mStep[0m  [28/42], [94mLoss[0m : 2.59081
[1mStep[0m  [32/42], [94mLoss[0m : 2.62366
[1mStep[0m  [36/42], [94mLoss[0m : 2.41226
[1mStep[0m  [40/42], [94mLoss[0m : 2.61887

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.380, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37798
[1mStep[0m  [4/42], [94mLoss[0m : 2.40859
[1mStep[0m  [8/42], [94mLoss[0m : 2.30872
[1mStep[0m  [12/42], [94mLoss[0m : 2.31694
[1mStep[0m  [16/42], [94mLoss[0m : 2.58783
[1mStep[0m  [20/42], [94mLoss[0m : 2.40354
[1mStep[0m  [24/42], [94mLoss[0m : 2.63467
[1mStep[0m  [28/42], [94mLoss[0m : 2.50425
[1mStep[0m  [32/42], [94mLoss[0m : 2.41578
[1mStep[0m  [36/42], [94mLoss[0m : 2.50571
[1mStep[0m  [40/42], [94mLoss[0m : 2.54016

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.387, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46775
[1mStep[0m  [4/42], [94mLoss[0m : 2.36228
[1mStep[0m  [8/42], [94mLoss[0m : 2.57297
[1mStep[0m  [12/42], [94mLoss[0m : 2.51022
[1mStep[0m  [16/42], [94mLoss[0m : 2.35995
[1mStep[0m  [20/42], [94mLoss[0m : 2.36703
[1mStep[0m  [24/42], [94mLoss[0m : 2.59790
[1mStep[0m  [28/42], [94mLoss[0m : 2.62478
[1mStep[0m  [32/42], [94mLoss[0m : 2.41814
[1mStep[0m  [36/42], [94mLoss[0m : 2.38258
[1mStep[0m  [40/42], [94mLoss[0m : 2.69224

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.382, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41180
[1mStep[0m  [4/42], [94mLoss[0m : 2.62979
[1mStep[0m  [8/42], [94mLoss[0m : 2.27911
[1mStep[0m  [12/42], [94mLoss[0m : 2.39914
[1mStep[0m  [16/42], [94mLoss[0m : 2.49506
[1mStep[0m  [20/42], [94mLoss[0m : 2.26413
[1mStep[0m  [24/42], [94mLoss[0m : 2.56177
[1mStep[0m  [28/42], [94mLoss[0m : 2.49158
[1mStep[0m  [32/42], [94mLoss[0m : 2.18102
[1mStep[0m  [36/42], [94mLoss[0m : 2.58196
[1mStep[0m  [40/42], [94mLoss[0m : 2.64386

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.383, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36248
[1mStep[0m  [4/42], [94mLoss[0m : 2.43685
[1mStep[0m  [8/42], [94mLoss[0m : 2.34236
[1mStep[0m  [12/42], [94mLoss[0m : 2.52938
[1mStep[0m  [16/42], [94mLoss[0m : 2.51911
[1mStep[0m  [20/42], [94mLoss[0m : 2.71296
[1mStep[0m  [24/42], [94mLoss[0m : 2.31923
[1mStep[0m  [28/42], [94mLoss[0m : 2.74772
[1mStep[0m  [32/42], [94mLoss[0m : 2.46115
[1mStep[0m  [36/42], [94mLoss[0m : 2.45292
[1mStep[0m  [40/42], [94mLoss[0m : 2.57434

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.371, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42177
[1mStep[0m  [4/42], [94mLoss[0m : 2.33681
[1mStep[0m  [8/42], [94mLoss[0m : 2.37402
[1mStep[0m  [12/42], [94mLoss[0m : 2.62182
[1mStep[0m  [16/42], [94mLoss[0m : 2.64172
[1mStep[0m  [20/42], [94mLoss[0m : 2.46672
[1mStep[0m  [24/42], [94mLoss[0m : 2.66692
[1mStep[0m  [28/42], [94mLoss[0m : 2.53247
[1mStep[0m  [32/42], [94mLoss[0m : 2.57676
[1mStep[0m  [36/42], [94mLoss[0m : 2.40861
[1mStep[0m  [40/42], [94mLoss[0m : 2.16980

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.373, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52105
[1mStep[0m  [4/42], [94mLoss[0m : 2.26654
[1mStep[0m  [8/42], [94mLoss[0m : 2.41097
[1mStep[0m  [12/42], [94mLoss[0m : 2.66794
[1mStep[0m  [16/42], [94mLoss[0m : 2.55695
[1mStep[0m  [20/42], [94mLoss[0m : 2.70865
[1mStep[0m  [24/42], [94mLoss[0m : 2.36258
[1mStep[0m  [28/42], [94mLoss[0m : 2.52239
[1mStep[0m  [32/42], [94mLoss[0m : 2.62676
[1mStep[0m  [36/42], [94mLoss[0m : 2.32969
[1mStep[0m  [40/42], [94mLoss[0m : 2.42275

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.371, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48591
[1mStep[0m  [4/42], [94mLoss[0m : 2.47542
[1mStep[0m  [8/42], [94mLoss[0m : 2.51932
[1mStep[0m  [12/42], [94mLoss[0m : 2.30453
[1mStep[0m  [16/42], [94mLoss[0m : 2.48725
[1mStep[0m  [20/42], [94mLoss[0m : 2.59230
[1mStep[0m  [24/42], [94mLoss[0m : 2.35824
[1mStep[0m  [28/42], [94mLoss[0m : 2.38648
[1mStep[0m  [32/42], [94mLoss[0m : 2.61685
[1mStep[0m  [36/42], [94mLoss[0m : 2.72279
[1mStep[0m  [40/42], [94mLoss[0m : 2.37822

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.368, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59162
[1mStep[0m  [4/42], [94mLoss[0m : 2.27607
[1mStep[0m  [8/42], [94mLoss[0m : 2.34448
[1mStep[0m  [12/42], [94mLoss[0m : 2.24453
[1mStep[0m  [16/42], [94mLoss[0m : 2.47471
[1mStep[0m  [20/42], [94mLoss[0m : 2.24800
[1mStep[0m  [24/42], [94mLoss[0m : 2.38708
[1mStep[0m  [28/42], [94mLoss[0m : 2.42064
[1mStep[0m  [32/42], [94mLoss[0m : 2.51745
[1mStep[0m  [36/42], [94mLoss[0m : 2.52316
[1mStep[0m  [40/42], [94mLoss[0m : 2.41727

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.363, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55594
[1mStep[0m  [4/42], [94mLoss[0m : 2.51281
[1mStep[0m  [8/42], [94mLoss[0m : 2.37653
[1mStep[0m  [12/42], [94mLoss[0m : 2.49735
[1mStep[0m  [16/42], [94mLoss[0m : 2.58068
[1mStep[0m  [20/42], [94mLoss[0m : 2.33097
[1mStep[0m  [24/42], [94mLoss[0m : 2.60231
[1mStep[0m  [28/42], [94mLoss[0m : 2.58982
[1mStep[0m  [32/42], [94mLoss[0m : 2.52604
[1mStep[0m  [36/42], [94mLoss[0m : 2.51594
[1mStep[0m  [40/42], [94mLoss[0m : 2.30445

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.367, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.368
====================================

Phase 1 - Evaluation MAE:  2.368193711553301
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 2.73817
[1mStep[0m  [4/42], [94mLoss[0m : 2.50285
[1mStep[0m  [8/42], [94mLoss[0m : 2.34520
[1mStep[0m  [12/42], [94mLoss[0m : 2.35367
[1mStep[0m  [16/42], [94mLoss[0m : 2.21945
[1mStep[0m  [20/42], [94mLoss[0m : 2.39623
[1mStep[0m  [24/42], [94mLoss[0m : 2.34793
[1mStep[0m  [28/42], [94mLoss[0m : 2.66151
[1mStep[0m  [32/42], [94mLoss[0m : 2.65566
[1mStep[0m  [36/42], [94mLoss[0m : 2.45492
[1mStep[0m  [40/42], [94mLoss[0m : 2.51083

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.362, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57991
[1mStep[0m  [4/42], [94mLoss[0m : 2.49102
[1mStep[0m  [8/42], [94mLoss[0m : 2.27792
[1mStep[0m  [12/42], [94mLoss[0m : 2.37355
[1mStep[0m  [16/42], [94mLoss[0m : 2.58978
[1mStep[0m  [20/42], [94mLoss[0m : 2.41262
[1mStep[0m  [24/42], [94mLoss[0m : 2.37177
[1mStep[0m  [28/42], [94mLoss[0m : 2.66505
[1mStep[0m  [32/42], [94mLoss[0m : 2.33138
[1mStep[0m  [36/42], [94mLoss[0m : 2.68063
[1mStep[0m  [40/42], [94mLoss[0m : 2.60524

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.353, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47540
[1mStep[0m  [4/42], [94mLoss[0m : 2.38365
[1mStep[0m  [8/42], [94mLoss[0m : 2.42389
[1mStep[0m  [12/42], [94mLoss[0m : 2.32260
[1mStep[0m  [16/42], [94mLoss[0m : 2.35931
[1mStep[0m  [20/42], [94mLoss[0m : 2.62900
[1mStep[0m  [24/42], [94mLoss[0m : 2.46893
[1mStep[0m  [28/42], [94mLoss[0m : 2.47102
[1mStep[0m  [32/42], [94mLoss[0m : 2.49298
[1mStep[0m  [36/42], [94mLoss[0m : 2.46270
[1mStep[0m  [40/42], [94mLoss[0m : 2.39739

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.384, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43811
[1mStep[0m  [4/42], [94mLoss[0m : 2.63956
[1mStep[0m  [8/42], [94mLoss[0m : 2.50854
[1mStep[0m  [12/42], [94mLoss[0m : 2.38240
[1mStep[0m  [16/42], [94mLoss[0m : 2.40069
[1mStep[0m  [20/42], [94mLoss[0m : 2.44205
[1mStep[0m  [24/42], [94mLoss[0m : 2.48698
[1mStep[0m  [28/42], [94mLoss[0m : 2.40132
[1mStep[0m  [32/42], [94mLoss[0m : 2.24634
[1mStep[0m  [36/42], [94mLoss[0m : 2.37277
[1mStep[0m  [40/42], [94mLoss[0m : 2.23292

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.383, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49031
[1mStep[0m  [4/42], [94mLoss[0m : 2.21035
[1mStep[0m  [8/42], [94mLoss[0m : 2.48783
[1mStep[0m  [12/42], [94mLoss[0m : 2.45345
[1mStep[0m  [16/42], [94mLoss[0m : 2.66105
[1mStep[0m  [20/42], [94mLoss[0m : 2.39801
[1mStep[0m  [24/42], [94mLoss[0m : 2.46441
[1mStep[0m  [28/42], [94mLoss[0m : 2.41603
[1mStep[0m  [32/42], [94mLoss[0m : 2.27615
[1mStep[0m  [36/42], [94mLoss[0m : 2.56501
[1mStep[0m  [40/42], [94mLoss[0m : 2.53430

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.369, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51695
[1mStep[0m  [4/42], [94mLoss[0m : 2.57298
[1mStep[0m  [8/42], [94mLoss[0m : 2.42969
[1mStep[0m  [12/42], [94mLoss[0m : 2.52889
[1mStep[0m  [16/42], [94mLoss[0m : 2.58469
[1mStep[0m  [20/42], [94mLoss[0m : 2.32870
[1mStep[0m  [24/42], [94mLoss[0m : 2.40651
[1mStep[0m  [28/42], [94mLoss[0m : 2.60607
[1mStep[0m  [32/42], [94mLoss[0m : 2.23690
[1mStep[0m  [36/42], [94mLoss[0m : 2.25434
[1mStep[0m  [40/42], [94mLoss[0m : 2.25681

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.383, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51104
[1mStep[0m  [4/42], [94mLoss[0m : 2.41373
[1mStep[0m  [8/42], [94mLoss[0m : 2.49175
[1mStep[0m  [12/42], [94mLoss[0m : 2.40590
[1mStep[0m  [16/42], [94mLoss[0m : 2.42272
[1mStep[0m  [20/42], [94mLoss[0m : 2.48172
[1mStep[0m  [24/42], [94mLoss[0m : 2.30887
[1mStep[0m  [28/42], [94mLoss[0m : 2.37412
[1mStep[0m  [32/42], [94mLoss[0m : 2.71544
[1mStep[0m  [36/42], [94mLoss[0m : 2.49821
[1mStep[0m  [40/42], [94mLoss[0m : 2.36976

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.408, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43253
[1mStep[0m  [4/42], [94mLoss[0m : 2.24994
[1mStep[0m  [8/42], [94mLoss[0m : 2.42866
[1mStep[0m  [12/42], [94mLoss[0m : 2.27859
[1mStep[0m  [16/42], [94mLoss[0m : 2.31538
[1mStep[0m  [20/42], [94mLoss[0m : 2.42236
[1mStep[0m  [24/42], [94mLoss[0m : 2.51347
[1mStep[0m  [28/42], [94mLoss[0m : 2.37959
[1mStep[0m  [32/42], [94mLoss[0m : 2.47845
[1mStep[0m  [36/42], [94mLoss[0m : 2.34085
[1mStep[0m  [40/42], [94mLoss[0m : 2.66384

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.383, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41526
[1mStep[0m  [4/42], [94mLoss[0m : 2.64059
[1mStep[0m  [8/42], [94mLoss[0m : 2.51565
[1mStep[0m  [12/42], [94mLoss[0m : 2.32817
[1mStep[0m  [16/42], [94mLoss[0m : 2.23941
[1mStep[0m  [20/42], [94mLoss[0m : 2.30787
[1mStep[0m  [24/42], [94mLoss[0m : 2.26480
[1mStep[0m  [28/42], [94mLoss[0m : 2.23926
[1mStep[0m  [32/42], [94mLoss[0m : 2.41596
[1mStep[0m  [36/42], [94mLoss[0m : 2.45297
[1mStep[0m  [40/42], [94mLoss[0m : 2.36796

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.365, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35264
[1mStep[0m  [4/42], [94mLoss[0m : 2.31839
[1mStep[0m  [8/42], [94mLoss[0m : 2.42360
[1mStep[0m  [12/42], [94mLoss[0m : 2.38165
[1mStep[0m  [16/42], [94mLoss[0m : 2.40735
[1mStep[0m  [20/42], [94mLoss[0m : 2.45048
[1mStep[0m  [24/42], [94mLoss[0m : 2.12706
[1mStep[0m  [28/42], [94mLoss[0m : 2.36824
[1mStep[0m  [32/42], [94mLoss[0m : 2.37528
[1mStep[0m  [36/42], [94mLoss[0m : 2.44339
[1mStep[0m  [40/42], [94mLoss[0m : 2.37132

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.403, [92mTest[0m: 2.386, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26221
[1mStep[0m  [4/42], [94mLoss[0m : 2.31144
[1mStep[0m  [8/42], [94mLoss[0m : 2.45164
[1mStep[0m  [12/42], [94mLoss[0m : 2.25194
[1mStep[0m  [16/42], [94mLoss[0m : 2.24529
[1mStep[0m  [20/42], [94mLoss[0m : 2.49457
[1mStep[0m  [24/42], [94mLoss[0m : 2.37520
[1mStep[0m  [28/42], [94mLoss[0m : 2.32176
[1mStep[0m  [32/42], [94mLoss[0m : 2.45217
[1mStep[0m  [36/42], [94mLoss[0m : 2.25300
[1mStep[0m  [40/42], [94mLoss[0m : 2.32895

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.390, [92mTest[0m: 2.407, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55819
[1mStep[0m  [4/42], [94mLoss[0m : 2.27888
[1mStep[0m  [8/42], [94mLoss[0m : 2.32084
[1mStep[0m  [12/42], [94mLoss[0m : 2.48400
[1mStep[0m  [16/42], [94mLoss[0m : 2.31054
[1mStep[0m  [20/42], [94mLoss[0m : 2.61640
[1mStep[0m  [24/42], [94mLoss[0m : 2.59447
[1mStep[0m  [28/42], [94mLoss[0m : 2.34587
[1mStep[0m  [32/42], [94mLoss[0m : 2.39366
[1mStep[0m  [36/42], [94mLoss[0m : 2.44943
[1mStep[0m  [40/42], [94mLoss[0m : 2.43818

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.401, [92mTest[0m: 2.431, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56620
[1mStep[0m  [4/42], [94mLoss[0m : 2.25757
[1mStep[0m  [8/42], [94mLoss[0m : 2.32840
[1mStep[0m  [12/42], [94mLoss[0m : 2.34889
[1mStep[0m  [16/42], [94mLoss[0m : 2.35584
[1mStep[0m  [20/42], [94mLoss[0m : 2.34537
[1mStep[0m  [24/42], [94mLoss[0m : 2.36158
[1mStep[0m  [28/42], [94mLoss[0m : 2.34080
[1mStep[0m  [32/42], [94mLoss[0m : 2.42336
[1mStep[0m  [36/42], [94mLoss[0m : 2.42493
[1mStep[0m  [40/42], [94mLoss[0m : 2.34157

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.387, [92mTest[0m: 2.401, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28313
[1mStep[0m  [4/42], [94mLoss[0m : 2.26165
[1mStep[0m  [8/42], [94mLoss[0m : 2.58510
[1mStep[0m  [12/42], [94mLoss[0m : 2.18660
[1mStep[0m  [16/42], [94mLoss[0m : 2.18539
[1mStep[0m  [20/42], [94mLoss[0m : 2.17792
[1mStep[0m  [24/42], [94mLoss[0m : 2.14277
[1mStep[0m  [28/42], [94mLoss[0m : 2.40823
[1mStep[0m  [32/42], [94mLoss[0m : 2.39255
[1mStep[0m  [36/42], [94mLoss[0m : 2.35516
[1mStep[0m  [40/42], [94mLoss[0m : 2.30752

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.365, [92mTest[0m: 2.371, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51867
[1mStep[0m  [4/42], [94mLoss[0m : 2.23643
[1mStep[0m  [8/42], [94mLoss[0m : 2.39037
[1mStep[0m  [12/42], [94mLoss[0m : 2.45078
[1mStep[0m  [16/42], [94mLoss[0m : 2.42079
[1mStep[0m  [20/42], [94mLoss[0m : 2.35292
[1mStep[0m  [24/42], [94mLoss[0m : 2.31654
[1mStep[0m  [28/42], [94mLoss[0m : 2.26747
[1mStep[0m  [32/42], [94mLoss[0m : 2.37338
[1mStep[0m  [36/42], [94mLoss[0m : 2.16509
[1mStep[0m  [40/42], [94mLoss[0m : 2.24695

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.373, [92mTest[0m: 2.426, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24172
[1mStep[0m  [4/42], [94mLoss[0m : 2.42607
[1mStep[0m  [8/42], [94mLoss[0m : 2.27409
[1mStep[0m  [12/42], [94mLoss[0m : 2.30012
[1mStep[0m  [16/42], [94mLoss[0m : 2.34155
[1mStep[0m  [20/42], [94mLoss[0m : 2.36009
[1mStep[0m  [24/42], [94mLoss[0m : 2.33457
[1mStep[0m  [28/42], [94mLoss[0m : 2.39268
[1mStep[0m  [32/42], [94mLoss[0m : 2.44828
[1mStep[0m  [36/42], [94mLoss[0m : 2.44078
[1mStep[0m  [40/42], [94mLoss[0m : 2.44380

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.375, [92mTest[0m: 2.416, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31304
[1mStep[0m  [4/42], [94mLoss[0m : 2.35790
[1mStep[0m  [8/42], [94mLoss[0m : 2.16674
[1mStep[0m  [12/42], [94mLoss[0m : 2.44445
[1mStep[0m  [16/42], [94mLoss[0m : 2.23772
[1mStep[0m  [20/42], [94mLoss[0m : 2.13368
[1mStep[0m  [24/42], [94mLoss[0m : 2.35852
[1mStep[0m  [28/42], [94mLoss[0m : 2.43289
[1mStep[0m  [32/42], [94mLoss[0m : 2.34247
[1mStep[0m  [36/42], [94mLoss[0m : 2.36536
[1mStep[0m  [40/42], [94mLoss[0m : 2.25755

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.364, [92mTest[0m: 2.389, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41665
[1mStep[0m  [4/42], [94mLoss[0m : 2.29146
[1mStep[0m  [8/42], [94mLoss[0m : 2.62503
[1mStep[0m  [12/42], [94mLoss[0m : 2.37951
[1mStep[0m  [16/42], [94mLoss[0m : 2.38108
[1mStep[0m  [20/42], [94mLoss[0m : 2.32128
[1mStep[0m  [24/42], [94mLoss[0m : 2.43122
[1mStep[0m  [28/42], [94mLoss[0m : 2.54559
[1mStep[0m  [32/42], [94mLoss[0m : 2.28310
[1mStep[0m  [36/42], [94mLoss[0m : 2.39708
[1mStep[0m  [40/42], [94mLoss[0m : 2.31974

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.357, [92mTest[0m: 2.434, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.16926
[1mStep[0m  [4/42], [94mLoss[0m : 2.50835
[1mStep[0m  [8/42], [94mLoss[0m : 2.43320
[1mStep[0m  [12/42], [94mLoss[0m : 2.33133
[1mStep[0m  [16/42], [94mLoss[0m : 2.32706
[1mStep[0m  [20/42], [94mLoss[0m : 2.42471
[1mStep[0m  [24/42], [94mLoss[0m : 2.33821
[1mStep[0m  [28/42], [94mLoss[0m : 2.26072
[1mStep[0m  [32/42], [94mLoss[0m : 2.27890
[1mStep[0m  [36/42], [94mLoss[0m : 2.36824
[1mStep[0m  [40/42], [94mLoss[0m : 2.14517

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.355, [92mTest[0m: 2.421, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45320
[1mStep[0m  [4/42], [94mLoss[0m : 2.37642
[1mStep[0m  [8/42], [94mLoss[0m : 2.26200
[1mStep[0m  [12/42], [94mLoss[0m : 2.20909
[1mStep[0m  [16/42], [94mLoss[0m : 2.45682
[1mStep[0m  [20/42], [94mLoss[0m : 2.47050
[1mStep[0m  [24/42], [94mLoss[0m : 2.32648
[1mStep[0m  [28/42], [94mLoss[0m : 2.27415
[1mStep[0m  [32/42], [94mLoss[0m : 2.32634
[1mStep[0m  [36/42], [94mLoss[0m : 2.45881
[1mStep[0m  [40/42], [94mLoss[0m : 2.22061

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.340, [92mTest[0m: 2.387, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42490
[1mStep[0m  [4/42], [94mLoss[0m : 2.28239
[1mStep[0m  [8/42], [94mLoss[0m : 2.31001
[1mStep[0m  [12/42], [94mLoss[0m : 2.31238
[1mStep[0m  [16/42], [94mLoss[0m : 2.39545
[1mStep[0m  [20/42], [94mLoss[0m : 2.21721
[1mStep[0m  [24/42], [94mLoss[0m : 2.30596
[1mStep[0m  [28/42], [94mLoss[0m : 2.31695
[1mStep[0m  [32/42], [94mLoss[0m : 2.53868
[1mStep[0m  [36/42], [94mLoss[0m : 2.40564
[1mStep[0m  [40/42], [94mLoss[0m : 2.22068

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.316, [92mTest[0m: 2.380, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42221
[1mStep[0m  [4/42], [94mLoss[0m : 2.15893
[1mStep[0m  [8/42], [94mLoss[0m : 2.39052
[1mStep[0m  [12/42], [94mLoss[0m : 2.42880
[1mStep[0m  [16/42], [94mLoss[0m : 2.22663
[1mStep[0m  [20/42], [94mLoss[0m : 2.15735
[1mStep[0m  [24/42], [94mLoss[0m : 2.41466
[1mStep[0m  [28/42], [94mLoss[0m : 2.56875
[1mStep[0m  [32/42], [94mLoss[0m : 2.27489
[1mStep[0m  [36/42], [94mLoss[0m : 2.42258
[1mStep[0m  [40/42], [94mLoss[0m : 2.42937

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.317, [92mTest[0m: 2.421, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35271
[1mStep[0m  [4/42], [94mLoss[0m : 2.24408
[1mStep[0m  [8/42], [94mLoss[0m : 2.33344
[1mStep[0m  [12/42], [94mLoss[0m : 2.36024
[1mStep[0m  [16/42], [94mLoss[0m : 2.35841
[1mStep[0m  [20/42], [94mLoss[0m : 2.34210
[1mStep[0m  [24/42], [94mLoss[0m : 2.46132
[1mStep[0m  [28/42], [94mLoss[0m : 2.28195
[1mStep[0m  [32/42], [94mLoss[0m : 2.21043
[1mStep[0m  [36/42], [94mLoss[0m : 2.26446
[1mStep[0m  [40/42], [94mLoss[0m : 2.42627

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.313, [92mTest[0m: 2.419, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27359
[1mStep[0m  [4/42], [94mLoss[0m : 2.05954
[1mStep[0m  [8/42], [94mLoss[0m : 2.20507
[1mStep[0m  [12/42], [94mLoss[0m : 2.45472
[1mStep[0m  [16/42], [94mLoss[0m : 2.60009
[1mStep[0m  [20/42], [94mLoss[0m : 2.27370
[1mStep[0m  [24/42], [94mLoss[0m : 2.29399
[1mStep[0m  [28/42], [94mLoss[0m : 2.03883
[1mStep[0m  [32/42], [94mLoss[0m : 2.33152
[1mStep[0m  [36/42], [94mLoss[0m : 2.22538
[1mStep[0m  [40/42], [94mLoss[0m : 2.44211

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.318, [92mTest[0m: 2.460, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28528
[1mStep[0m  [4/42], [94mLoss[0m : 2.26344
[1mStep[0m  [8/42], [94mLoss[0m : 2.26908
[1mStep[0m  [12/42], [94mLoss[0m : 2.10278
[1mStep[0m  [16/42], [94mLoss[0m : 2.15505
[1mStep[0m  [20/42], [94mLoss[0m : 2.26287
[1mStep[0m  [24/42], [94mLoss[0m : 2.41661
[1mStep[0m  [28/42], [94mLoss[0m : 2.59731
[1mStep[0m  [32/42], [94mLoss[0m : 2.28611
[1mStep[0m  [36/42], [94mLoss[0m : 2.37922
[1mStep[0m  [40/42], [94mLoss[0m : 2.27836

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.292, [92mTest[0m: 2.484, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.30784
[1mStep[0m  [4/42], [94mLoss[0m : 2.25518
[1mStep[0m  [8/42], [94mLoss[0m : 2.33449
[1mStep[0m  [12/42], [94mLoss[0m : 2.21071
[1mStep[0m  [16/42], [94mLoss[0m : 2.22338
[1mStep[0m  [20/42], [94mLoss[0m : 2.37154
[1mStep[0m  [24/42], [94mLoss[0m : 2.50511
[1mStep[0m  [28/42], [94mLoss[0m : 2.38308
[1mStep[0m  [32/42], [94mLoss[0m : 2.33104
[1mStep[0m  [36/42], [94mLoss[0m : 2.03037
[1mStep[0m  [40/42], [94mLoss[0m : 2.30272

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.299, [92mTest[0m: 2.435, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26795
[1mStep[0m  [4/42], [94mLoss[0m : 2.37569
[1mStep[0m  [8/42], [94mLoss[0m : 2.67026
[1mStep[0m  [12/42], [94mLoss[0m : 2.13578
[1mStep[0m  [16/42], [94mLoss[0m : 2.43172
[1mStep[0m  [20/42], [94mLoss[0m : 2.16190
[1mStep[0m  [24/42], [94mLoss[0m : 2.14089
[1mStep[0m  [28/42], [94mLoss[0m : 2.40793
[1mStep[0m  [32/42], [94mLoss[0m : 2.41723
[1mStep[0m  [36/42], [94mLoss[0m : 2.39414
[1mStep[0m  [40/42], [94mLoss[0m : 2.46838

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.280, [92mTest[0m: 2.379, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.99248
[1mStep[0m  [4/42], [94mLoss[0m : 2.19706
[1mStep[0m  [8/42], [94mLoss[0m : 2.44039
[1mStep[0m  [12/42], [94mLoss[0m : 2.23087
[1mStep[0m  [16/42], [94mLoss[0m : 2.22637
[1mStep[0m  [20/42], [94mLoss[0m : 2.52164
[1mStep[0m  [24/42], [94mLoss[0m : 2.04779
[1mStep[0m  [28/42], [94mLoss[0m : 2.36963
[1mStep[0m  [32/42], [94mLoss[0m : 2.10519
[1mStep[0m  [36/42], [94mLoss[0m : 2.30336
[1mStep[0m  [40/42], [94mLoss[0m : 2.28960

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.283, [92mTest[0m: 2.473, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.11046
[1mStep[0m  [4/42], [94mLoss[0m : 2.22223
[1mStep[0m  [8/42], [94mLoss[0m : 2.40657
[1mStep[0m  [12/42], [94mLoss[0m : 2.37020
[1mStep[0m  [16/42], [94mLoss[0m : 2.04428
[1mStep[0m  [20/42], [94mLoss[0m : 2.01092
[1mStep[0m  [24/42], [94mLoss[0m : 2.28763
[1mStep[0m  [28/42], [94mLoss[0m : 2.23913
[1mStep[0m  [32/42], [94mLoss[0m : 2.49374
[1mStep[0m  [36/42], [94mLoss[0m : 2.30647
[1mStep[0m  [40/42], [94mLoss[0m : 2.30488

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.268, [92mTest[0m: 2.421, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.23645
[1mStep[0m  [4/42], [94mLoss[0m : 2.32196
[1mStep[0m  [8/42], [94mLoss[0m : 2.36287
[1mStep[0m  [12/42], [94mLoss[0m : 2.47714
[1mStep[0m  [16/42], [94mLoss[0m : 2.29040
[1mStep[0m  [20/42], [94mLoss[0m : 2.31413
[1mStep[0m  [24/42], [94mLoss[0m : 2.43932
[1mStep[0m  [28/42], [94mLoss[0m : 2.33171
[1mStep[0m  [32/42], [94mLoss[0m : 2.57126
[1mStep[0m  [36/42], [94mLoss[0m : 2.55742
[1mStep[0m  [40/42], [94mLoss[0m : 2.28673

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.262, [92mTest[0m: 2.432, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.397
====================================

Phase 2 - Evaluation MAE:  2.3974788870130266
MAE score P1      2.368194
MAE score P2      2.397479
loss              2.262086
learning_rate     0.002575
batch_size             256
hidden_sizes         [300]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.4
momentum               0.1
weight_decay        0.0001
Name: 0, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 10.50564
[1mStep[0m  [4/42], [94mLoss[0m : 10.07259
[1mStep[0m  [8/42], [94mLoss[0m : 9.60701
[1mStep[0m  [12/42], [94mLoss[0m : 8.25445
[1mStep[0m  [16/42], [94mLoss[0m : 8.39481
[1mStep[0m  [20/42], [94mLoss[0m : 7.61201
[1mStep[0m  [24/42], [94mLoss[0m : 7.32286
[1mStep[0m  [28/42], [94mLoss[0m : 6.90955
[1mStep[0m  [32/42], [94mLoss[0m : 6.48936
[1mStep[0m  [36/42], [94mLoss[0m : 5.17173
[1mStep[0m  [40/42], [94mLoss[0m : 5.13743

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 7.709, [92mTest[0m: 10.241, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.88285
[1mStep[0m  [4/42], [94mLoss[0m : 4.30109
[1mStep[0m  [8/42], [94mLoss[0m : 4.15056
[1mStep[0m  [12/42], [94mLoss[0m : 4.21111
[1mStep[0m  [16/42], [94mLoss[0m : 3.67923
[1mStep[0m  [20/42], [94mLoss[0m : 3.59254
[1mStep[0m  [24/42], [94mLoss[0m : 3.17672
[1mStep[0m  [28/42], [94mLoss[0m : 3.12292
[1mStep[0m  [32/42], [94mLoss[0m : 3.19597
[1mStep[0m  [36/42], [94mLoss[0m : 3.05078
[1mStep[0m  [40/42], [94mLoss[0m : 3.25986

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.605, [92mTest[0m: 4.897, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54142
[1mStep[0m  [4/42], [94mLoss[0m : 2.87407
[1mStep[0m  [8/42], [94mLoss[0m : 3.01033
[1mStep[0m  [12/42], [94mLoss[0m : 2.66653
[1mStep[0m  [16/42], [94mLoss[0m : 2.96115
[1mStep[0m  [20/42], [94mLoss[0m : 2.56680
[1mStep[0m  [24/42], [94mLoss[0m : 2.62861
[1mStep[0m  [28/42], [94mLoss[0m : 2.72454
[1mStep[0m  [32/42], [94mLoss[0m : 2.61464
[1mStep[0m  [36/42], [94mLoss[0m : 2.51911
[1mStep[0m  [40/42], [94mLoss[0m : 2.67484

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.748, [92mTest[0m: 2.833, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27392
[1mStep[0m  [4/42], [94mLoss[0m : 2.57933
[1mStep[0m  [8/42], [94mLoss[0m : 2.80572
[1mStep[0m  [12/42], [94mLoss[0m : 2.75006
[1mStep[0m  [16/42], [94mLoss[0m : 2.45992
[1mStep[0m  [20/42], [94mLoss[0m : 2.50252
[1mStep[0m  [24/42], [94mLoss[0m : 2.59072
[1mStep[0m  [28/42], [94mLoss[0m : 2.49038
[1mStep[0m  [32/42], [94mLoss[0m : 2.79335
[1mStep[0m  [36/42], [94mLoss[0m : 2.48192
[1mStep[0m  [40/42], [94mLoss[0m : 2.50195

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.582, [92mTest[0m: 2.488, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54330
[1mStep[0m  [4/42], [94mLoss[0m : 2.42734
[1mStep[0m  [8/42], [94mLoss[0m : 2.60683
[1mStep[0m  [12/42], [94mLoss[0m : 2.57974
[1mStep[0m  [16/42], [94mLoss[0m : 2.39322
[1mStep[0m  [20/42], [94mLoss[0m : 2.54153
[1mStep[0m  [24/42], [94mLoss[0m : 2.43494
[1mStep[0m  [28/42], [94mLoss[0m : 2.53488
[1mStep[0m  [32/42], [94mLoss[0m : 2.82227
[1mStep[0m  [36/42], [94mLoss[0m : 2.36403
[1mStep[0m  [40/42], [94mLoss[0m : 2.58434

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.441, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51228
[1mStep[0m  [4/42], [94mLoss[0m : 2.66280
[1mStep[0m  [8/42], [94mLoss[0m : 2.49104
[1mStep[0m  [12/42], [94mLoss[0m : 2.65562
[1mStep[0m  [16/42], [94mLoss[0m : 2.39735
[1mStep[0m  [20/42], [94mLoss[0m : 2.48136
[1mStep[0m  [24/42], [94mLoss[0m : 2.43434
[1mStep[0m  [28/42], [94mLoss[0m : 2.68788
[1mStep[0m  [32/42], [94mLoss[0m : 2.52976
[1mStep[0m  [36/42], [94mLoss[0m : 2.28510
[1mStep[0m  [40/42], [94mLoss[0m : 2.63638

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.522, [92mTest[0m: 2.428, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32755
[1mStep[0m  [4/42], [94mLoss[0m : 2.61850
[1mStep[0m  [8/42], [94mLoss[0m : 2.60648
[1mStep[0m  [12/42], [94mLoss[0m : 2.56570
[1mStep[0m  [16/42], [94mLoss[0m : 2.61099
[1mStep[0m  [20/42], [94mLoss[0m : 2.76920
[1mStep[0m  [24/42], [94mLoss[0m : 2.52324
[1mStep[0m  [28/42], [94mLoss[0m : 2.52806
[1mStep[0m  [32/42], [94mLoss[0m : 2.47743
[1mStep[0m  [36/42], [94mLoss[0m : 2.56178
[1mStep[0m  [40/42], [94mLoss[0m : 2.67067

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.519, [92mTest[0m: 2.418, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64216
[1mStep[0m  [4/42], [94mLoss[0m : 2.41308
[1mStep[0m  [8/42], [94mLoss[0m : 2.50392
[1mStep[0m  [12/42], [94mLoss[0m : 2.44694
[1mStep[0m  [16/42], [94mLoss[0m : 2.40740
[1mStep[0m  [20/42], [94mLoss[0m : 2.44815
[1mStep[0m  [24/42], [94mLoss[0m : 2.42710
[1mStep[0m  [28/42], [94mLoss[0m : 2.46096
[1mStep[0m  [32/42], [94mLoss[0m : 2.41153
[1mStep[0m  [36/42], [94mLoss[0m : 2.62740
[1mStep[0m  [40/42], [94mLoss[0m : 2.47960

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.409, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37662
[1mStep[0m  [4/42], [94mLoss[0m : 2.60032
[1mStep[0m  [8/42], [94mLoss[0m : 2.45977
[1mStep[0m  [12/42], [94mLoss[0m : 2.41634
[1mStep[0m  [16/42], [94mLoss[0m : 2.55383
[1mStep[0m  [20/42], [94mLoss[0m : 2.27668
[1mStep[0m  [24/42], [94mLoss[0m : 2.51890
[1mStep[0m  [28/42], [94mLoss[0m : 2.66118
[1mStep[0m  [32/42], [94mLoss[0m : 2.82813
[1mStep[0m  [36/42], [94mLoss[0m : 2.45988
[1mStep[0m  [40/42], [94mLoss[0m : 2.40977

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.396, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55169
[1mStep[0m  [4/42], [94mLoss[0m : 2.52312
[1mStep[0m  [8/42], [94mLoss[0m : 2.31508
[1mStep[0m  [12/42], [94mLoss[0m : 2.64899
[1mStep[0m  [16/42], [94mLoss[0m : 2.45860
[1mStep[0m  [20/42], [94mLoss[0m : 2.34043
[1mStep[0m  [24/42], [94mLoss[0m : 2.30018
[1mStep[0m  [28/42], [94mLoss[0m : 2.66063
[1mStep[0m  [32/42], [94mLoss[0m : 2.47089
[1mStep[0m  [36/42], [94mLoss[0m : 2.31479
[1mStep[0m  [40/42], [94mLoss[0m : 2.52033

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.385, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61496
[1mStep[0m  [4/42], [94mLoss[0m : 2.39149
[1mStep[0m  [8/42], [94mLoss[0m : 2.31776
[1mStep[0m  [12/42], [94mLoss[0m : 2.37409
[1mStep[0m  [16/42], [94mLoss[0m : 2.42369
[1mStep[0m  [20/42], [94mLoss[0m : 2.29960
[1mStep[0m  [24/42], [94mLoss[0m : 2.71391
[1mStep[0m  [28/42], [94mLoss[0m : 2.55795
[1mStep[0m  [32/42], [94mLoss[0m : 2.60060
[1mStep[0m  [36/42], [94mLoss[0m : 2.42051
[1mStep[0m  [40/42], [94mLoss[0m : 2.82170

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.381, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45719
[1mStep[0m  [4/42], [94mLoss[0m : 2.39308
[1mStep[0m  [8/42], [94mLoss[0m : 2.64075
[1mStep[0m  [12/42], [94mLoss[0m : 2.39814
[1mStep[0m  [16/42], [94mLoss[0m : 2.57692
[1mStep[0m  [20/42], [94mLoss[0m : 2.59237
[1mStep[0m  [24/42], [94mLoss[0m : 2.45171
[1mStep[0m  [28/42], [94mLoss[0m : 2.54640
[1mStep[0m  [32/42], [94mLoss[0m : 2.51565
[1mStep[0m  [36/42], [94mLoss[0m : 2.39797
[1mStep[0m  [40/42], [94mLoss[0m : 2.44639

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.375, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56305
[1mStep[0m  [4/42], [94mLoss[0m : 2.41996
[1mStep[0m  [8/42], [94mLoss[0m : 2.18226
[1mStep[0m  [12/42], [94mLoss[0m : 2.38528
[1mStep[0m  [16/42], [94mLoss[0m : 2.58861
[1mStep[0m  [20/42], [94mLoss[0m : 2.52049
[1mStep[0m  [24/42], [94mLoss[0m : 2.28838
[1mStep[0m  [28/42], [94mLoss[0m : 2.47088
[1mStep[0m  [32/42], [94mLoss[0m : 2.48462
[1mStep[0m  [36/42], [94mLoss[0m : 2.54447
[1mStep[0m  [40/42], [94mLoss[0m : 2.37284

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.370, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37305
[1mStep[0m  [4/42], [94mLoss[0m : 2.38825
[1mStep[0m  [8/42], [94mLoss[0m : 2.54194
[1mStep[0m  [12/42], [94mLoss[0m : 2.53039
[1mStep[0m  [16/42], [94mLoss[0m : 2.41987
[1mStep[0m  [20/42], [94mLoss[0m : 2.32559
[1mStep[0m  [24/42], [94mLoss[0m : 2.39218
[1mStep[0m  [28/42], [94mLoss[0m : 2.34350
[1mStep[0m  [32/42], [94mLoss[0m : 2.53229
[1mStep[0m  [36/42], [94mLoss[0m : 2.71946
[1mStep[0m  [40/42], [94mLoss[0m : 2.65167

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.361, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50414
[1mStep[0m  [4/42], [94mLoss[0m : 2.63590
[1mStep[0m  [8/42], [94mLoss[0m : 2.52114
[1mStep[0m  [12/42], [94mLoss[0m : 2.32654
[1mStep[0m  [16/42], [94mLoss[0m : 2.27316
[1mStep[0m  [20/42], [94mLoss[0m : 2.35106
[1mStep[0m  [24/42], [94mLoss[0m : 2.48293
[1mStep[0m  [28/42], [94mLoss[0m : 2.56094
[1mStep[0m  [32/42], [94mLoss[0m : 2.39918
[1mStep[0m  [36/42], [94mLoss[0m : 2.59527
[1mStep[0m  [40/42], [94mLoss[0m : 2.40295

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.374, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56001
[1mStep[0m  [4/42], [94mLoss[0m : 2.42267
[1mStep[0m  [8/42], [94mLoss[0m : 2.47225
[1mStep[0m  [12/42], [94mLoss[0m : 2.59923
[1mStep[0m  [16/42], [94mLoss[0m : 2.35603
[1mStep[0m  [20/42], [94mLoss[0m : 2.51024
[1mStep[0m  [24/42], [94mLoss[0m : 2.36835
[1mStep[0m  [28/42], [94mLoss[0m : 2.38331
[1mStep[0m  [32/42], [94mLoss[0m : 2.57044
[1mStep[0m  [36/42], [94mLoss[0m : 2.36806
[1mStep[0m  [40/42], [94mLoss[0m : 2.25370

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.357, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58524
[1mStep[0m  [4/42], [94mLoss[0m : 2.42850
[1mStep[0m  [8/42], [94mLoss[0m : 2.39568
[1mStep[0m  [12/42], [94mLoss[0m : 2.52012
[1mStep[0m  [16/42], [94mLoss[0m : 2.43181
[1mStep[0m  [20/42], [94mLoss[0m : 2.45004
[1mStep[0m  [24/42], [94mLoss[0m : 2.44446
[1mStep[0m  [28/42], [94mLoss[0m : 2.26595
[1mStep[0m  [32/42], [94mLoss[0m : 2.71252
[1mStep[0m  [36/42], [94mLoss[0m : 2.31353
[1mStep[0m  [40/42], [94mLoss[0m : 2.77182

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.360, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41935
[1mStep[0m  [4/42], [94mLoss[0m : 2.51592
[1mStep[0m  [8/42], [94mLoss[0m : 2.38655
[1mStep[0m  [12/42], [94mLoss[0m : 2.55975
[1mStep[0m  [16/42], [94mLoss[0m : 2.26701
[1mStep[0m  [20/42], [94mLoss[0m : 2.34033
[1mStep[0m  [24/42], [94mLoss[0m : 2.51435
[1mStep[0m  [28/42], [94mLoss[0m : 2.52244
[1mStep[0m  [32/42], [94mLoss[0m : 2.35670
[1mStep[0m  [36/42], [94mLoss[0m : 2.28910
[1mStep[0m  [40/42], [94mLoss[0m : 2.57615

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.356, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40318
[1mStep[0m  [4/42], [94mLoss[0m : 2.26258
[1mStep[0m  [8/42], [94mLoss[0m : 2.55127
[1mStep[0m  [12/42], [94mLoss[0m : 2.35358
[1mStep[0m  [16/42], [94mLoss[0m : 2.53178
[1mStep[0m  [20/42], [94mLoss[0m : 2.35994
[1mStep[0m  [24/42], [94mLoss[0m : 2.38729
[1mStep[0m  [28/42], [94mLoss[0m : 2.51870
[1mStep[0m  [32/42], [94mLoss[0m : 2.51692
[1mStep[0m  [36/42], [94mLoss[0m : 2.30678
[1mStep[0m  [40/42], [94mLoss[0m : 2.53219

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.349, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45075
[1mStep[0m  [4/42], [94mLoss[0m : 2.42222
[1mStep[0m  [8/42], [94mLoss[0m : 2.34375
[1mStep[0m  [12/42], [94mLoss[0m : 2.26082
[1mStep[0m  [16/42], [94mLoss[0m : 2.47302
[1mStep[0m  [20/42], [94mLoss[0m : 2.27999
[1mStep[0m  [24/42], [94mLoss[0m : 2.50894
[1mStep[0m  [28/42], [94mLoss[0m : 2.40992
[1mStep[0m  [32/42], [94mLoss[0m : 2.60282
[1mStep[0m  [36/42], [94mLoss[0m : 2.70525
[1mStep[0m  [40/42], [94mLoss[0m : 2.36690

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.351, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46476
[1mStep[0m  [4/42], [94mLoss[0m : 2.51787
[1mStep[0m  [8/42], [94mLoss[0m : 2.25308
[1mStep[0m  [12/42], [94mLoss[0m : 2.43439
[1mStep[0m  [16/42], [94mLoss[0m : 2.42568
[1mStep[0m  [20/42], [94mLoss[0m : 2.43099
[1mStep[0m  [24/42], [94mLoss[0m : 2.62474
[1mStep[0m  [28/42], [94mLoss[0m : 2.51222
[1mStep[0m  [32/42], [94mLoss[0m : 2.60006
[1mStep[0m  [36/42], [94mLoss[0m : 2.52661
[1mStep[0m  [40/42], [94mLoss[0m : 2.51981

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.351, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33593
[1mStep[0m  [4/42], [94mLoss[0m : 2.29091
[1mStep[0m  [8/42], [94mLoss[0m : 2.53222
[1mStep[0m  [12/42], [94mLoss[0m : 2.44753
[1mStep[0m  [16/42], [94mLoss[0m : 2.53718
[1mStep[0m  [20/42], [94mLoss[0m : 2.35007
[1mStep[0m  [24/42], [94mLoss[0m : 2.53684
[1mStep[0m  [28/42], [94mLoss[0m : 2.61102
[1mStep[0m  [32/42], [94mLoss[0m : 2.73953
[1mStep[0m  [36/42], [94mLoss[0m : 2.34741
[1mStep[0m  [40/42], [94mLoss[0m : 2.43912

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.350, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.25792
[1mStep[0m  [4/42], [94mLoss[0m : 2.42576
[1mStep[0m  [8/42], [94mLoss[0m : 2.48195
[1mStep[0m  [12/42], [94mLoss[0m : 2.47270
[1mStep[0m  [16/42], [94mLoss[0m : 2.43955
[1mStep[0m  [20/42], [94mLoss[0m : 2.45566
[1mStep[0m  [24/42], [94mLoss[0m : 2.43130
[1mStep[0m  [28/42], [94mLoss[0m : 2.30257
[1mStep[0m  [32/42], [94mLoss[0m : 2.35642
[1mStep[0m  [36/42], [94mLoss[0m : 2.25705
[1mStep[0m  [40/42], [94mLoss[0m : 2.35327

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.352, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41081
[1mStep[0m  [4/42], [94mLoss[0m : 2.32482
[1mStep[0m  [8/42], [94mLoss[0m : 2.53804
[1mStep[0m  [12/42], [94mLoss[0m : 2.34097
[1mStep[0m  [16/42], [94mLoss[0m : 2.45977
[1mStep[0m  [20/42], [94mLoss[0m : 2.21173
[1mStep[0m  [24/42], [94mLoss[0m : 2.15247
[1mStep[0m  [28/42], [94mLoss[0m : 2.21887
[1mStep[0m  [32/42], [94mLoss[0m : 2.39857
[1mStep[0m  [36/42], [94mLoss[0m : 2.41939
[1mStep[0m  [40/42], [94mLoss[0m : 2.40355

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.348, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50146
[1mStep[0m  [4/42], [94mLoss[0m : 2.37958
[1mStep[0m  [8/42], [94mLoss[0m : 2.18108
[1mStep[0m  [12/42], [94mLoss[0m : 2.32655
[1mStep[0m  [16/42], [94mLoss[0m : 2.49381
[1mStep[0m  [20/42], [94mLoss[0m : 2.50422
[1mStep[0m  [24/42], [94mLoss[0m : 2.58601
[1mStep[0m  [28/42], [94mLoss[0m : 2.50372
[1mStep[0m  [32/42], [94mLoss[0m : 2.58063
[1mStep[0m  [36/42], [94mLoss[0m : 2.32585
[1mStep[0m  [40/42], [94mLoss[0m : 2.55655

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.344, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33299
[1mStep[0m  [4/42], [94mLoss[0m : 2.27160
[1mStep[0m  [8/42], [94mLoss[0m : 2.14077
[1mStep[0m  [12/42], [94mLoss[0m : 2.53037
[1mStep[0m  [16/42], [94mLoss[0m : 2.59961
[1mStep[0m  [20/42], [94mLoss[0m : 2.48226
[1mStep[0m  [24/42], [94mLoss[0m : 2.58656
[1mStep[0m  [28/42], [94mLoss[0m : 2.47792
[1mStep[0m  [32/42], [94mLoss[0m : 2.51124
[1mStep[0m  [36/42], [94mLoss[0m : 2.40057
[1mStep[0m  [40/42], [94mLoss[0m : 2.63010

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.345, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59164
[1mStep[0m  [4/42], [94mLoss[0m : 2.34681
[1mStep[0m  [8/42], [94mLoss[0m : 2.32480
[1mStep[0m  [12/42], [94mLoss[0m : 2.40544
[1mStep[0m  [16/42], [94mLoss[0m : 2.40051
[1mStep[0m  [20/42], [94mLoss[0m : 2.45467
[1mStep[0m  [24/42], [94mLoss[0m : 2.44374
[1mStep[0m  [28/42], [94mLoss[0m : 2.54801
[1mStep[0m  [32/42], [94mLoss[0m : 2.50968
[1mStep[0m  [36/42], [94mLoss[0m : 2.48514
[1mStep[0m  [40/42], [94mLoss[0m : 2.60887

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.346, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.22069
[1mStep[0m  [4/42], [94mLoss[0m : 2.41628
[1mStep[0m  [8/42], [94mLoss[0m : 2.35393
[1mStep[0m  [12/42], [94mLoss[0m : 2.47133
[1mStep[0m  [16/42], [94mLoss[0m : 2.47300
[1mStep[0m  [20/42], [94mLoss[0m : 2.33332
[1mStep[0m  [24/42], [94mLoss[0m : 2.46631
[1mStep[0m  [28/42], [94mLoss[0m : 2.47881
[1mStep[0m  [32/42], [94mLoss[0m : 2.65351
[1mStep[0m  [36/42], [94mLoss[0m : 2.39086
[1mStep[0m  [40/42], [94mLoss[0m : 2.52883

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.348, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49996
[1mStep[0m  [4/42], [94mLoss[0m : 2.45814
[1mStep[0m  [8/42], [94mLoss[0m : 2.18313
[1mStep[0m  [12/42], [94mLoss[0m : 2.42277
[1mStep[0m  [16/42], [94mLoss[0m : 2.42791
[1mStep[0m  [20/42], [94mLoss[0m : 2.50143
[1mStep[0m  [24/42], [94mLoss[0m : 2.51074
[1mStep[0m  [28/42], [94mLoss[0m : 2.61253
[1mStep[0m  [32/42], [94mLoss[0m : 2.62808
[1mStep[0m  [36/42], [94mLoss[0m : 2.65598
[1mStep[0m  [40/42], [94mLoss[0m : 2.59432

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.344, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35609
[1mStep[0m  [4/42], [94mLoss[0m : 2.64472
[1mStep[0m  [8/42], [94mLoss[0m : 2.42364
[1mStep[0m  [12/42], [94mLoss[0m : 2.58435
[1mStep[0m  [16/42], [94mLoss[0m : 2.53111
[1mStep[0m  [20/42], [94mLoss[0m : 2.27217
[1mStep[0m  [24/42], [94mLoss[0m : 2.38162
[1mStep[0m  [28/42], [94mLoss[0m : 2.44886
[1mStep[0m  [32/42], [94mLoss[0m : 2.35052
[1mStep[0m  [36/42], [94mLoss[0m : 2.37741
[1mStep[0m  [40/42], [94mLoss[0m : 2.47646

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.333, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.338
====================================

Phase 1 - Evaluation MAE:  2.338475772312709
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 2.34048
[1mStep[0m  [4/42], [94mLoss[0m : 2.30114
[1mStep[0m  [8/42], [94mLoss[0m : 2.38530
[1mStep[0m  [12/42], [94mLoss[0m : 2.40836
[1mStep[0m  [16/42], [94mLoss[0m : 2.58282
[1mStep[0m  [20/42], [94mLoss[0m : 2.63190
[1mStep[0m  [24/42], [94mLoss[0m : 2.55589
[1mStep[0m  [28/42], [94mLoss[0m : 2.56947
[1mStep[0m  [32/42], [94mLoss[0m : 2.45878
[1mStep[0m  [36/42], [94mLoss[0m : 2.50424
[1mStep[0m  [40/42], [94mLoss[0m : 2.40149

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.339, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.66563
[1mStep[0m  [4/42], [94mLoss[0m : 2.48488
[1mStep[0m  [8/42], [94mLoss[0m : 2.28723
[1mStep[0m  [12/42], [94mLoss[0m : 2.36399
[1mStep[0m  [16/42], [94mLoss[0m : 2.53695
[1mStep[0m  [20/42], [94mLoss[0m : 2.44003
[1mStep[0m  [24/42], [94mLoss[0m : 2.74560
[1mStep[0m  [28/42], [94mLoss[0m : 2.58040
[1mStep[0m  [32/42], [94mLoss[0m : 2.56189
[1mStep[0m  [36/42], [94mLoss[0m : 2.47366
[1mStep[0m  [40/42], [94mLoss[0m : 2.51220

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.334, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37762
[1mStep[0m  [4/42], [94mLoss[0m : 2.56438
[1mStep[0m  [8/42], [94mLoss[0m : 2.53589
[1mStep[0m  [12/42], [94mLoss[0m : 2.31530
[1mStep[0m  [16/42], [94mLoss[0m : 2.25591
[1mStep[0m  [20/42], [94mLoss[0m : 2.56075
[1mStep[0m  [24/42], [94mLoss[0m : 2.62502
[1mStep[0m  [28/42], [94mLoss[0m : 2.40504
[1mStep[0m  [32/42], [94mLoss[0m : 2.62712
[1mStep[0m  [36/42], [94mLoss[0m : 2.34417
[1mStep[0m  [40/42], [94mLoss[0m : 2.37355

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.382, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32217
[1mStep[0m  [4/42], [94mLoss[0m : 2.61287
[1mStep[0m  [8/42], [94mLoss[0m : 2.36463
[1mStep[0m  [12/42], [94mLoss[0m : 2.44795
[1mStep[0m  [16/42], [94mLoss[0m : 2.39576
[1mStep[0m  [20/42], [94mLoss[0m : 2.48627
[1mStep[0m  [24/42], [94mLoss[0m : 2.59970
[1mStep[0m  [28/42], [94mLoss[0m : 2.48898
[1mStep[0m  [32/42], [94mLoss[0m : 2.49138
[1mStep[0m  [36/42], [94mLoss[0m : 2.46943
[1mStep[0m  [40/42], [94mLoss[0m : 2.59667

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.403, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35679
[1mStep[0m  [4/42], [94mLoss[0m : 2.65283
[1mStep[0m  [8/42], [94mLoss[0m : 2.04211
[1mStep[0m  [12/42], [94mLoss[0m : 2.31630
[1mStep[0m  [16/42], [94mLoss[0m : 2.44254
[1mStep[0m  [20/42], [94mLoss[0m : 2.49918
[1mStep[0m  [24/42], [94mLoss[0m : 2.32220
[1mStep[0m  [28/42], [94mLoss[0m : 2.48704
[1mStep[0m  [32/42], [94mLoss[0m : 2.40122
[1mStep[0m  [36/42], [94mLoss[0m : 2.39712
[1mStep[0m  [40/42], [94mLoss[0m : 2.33279

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.441, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52105
[1mStep[0m  [4/42], [94mLoss[0m : 2.34161
[1mStep[0m  [8/42], [94mLoss[0m : 2.42227
[1mStep[0m  [12/42], [94mLoss[0m : 2.33542
[1mStep[0m  [16/42], [94mLoss[0m : 2.22873
[1mStep[0m  [20/42], [94mLoss[0m : 2.56338
[1mStep[0m  [24/42], [94mLoss[0m : 2.47859
[1mStep[0m  [28/42], [94mLoss[0m : 2.38203
[1mStep[0m  [32/42], [94mLoss[0m : 2.29812
[1mStep[0m  [36/42], [94mLoss[0m : 2.39683
[1mStep[0m  [40/42], [94mLoss[0m : 2.37043

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.468, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43571
[1mStep[0m  [4/42], [94mLoss[0m : 2.52766
[1mStep[0m  [8/42], [94mLoss[0m : 2.37238
[1mStep[0m  [12/42], [94mLoss[0m : 2.26369
[1mStep[0m  [16/42], [94mLoss[0m : 2.39103
[1mStep[0m  [20/42], [94mLoss[0m : 2.59493
[1mStep[0m  [24/42], [94mLoss[0m : 2.35905
[1mStep[0m  [28/42], [94mLoss[0m : 2.22437
[1mStep[0m  [32/42], [94mLoss[0m : 2.22396
[1mStep[0m  [36/42], [94mLoss[0m : 2.35551
[1mStep[0m  [40/42], [94mLoss[0m : 2.47402

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.378, [92mTest[0m: 2.499, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48513
[1mStep[0m  [4/42], [94mLoss[0m : 2.47081
[1mStep[0m  [8/42], [94mLoss[0m : 2.34994
[1mStep[0m  [12/42], [94mLoss[0m : 2.09593
[1mStep[0m  [16/42], [94mLoss[0m : 2.26206
[1mStep[0m  [20/42], [94mLoss[0m : 2.18350
[1mStep[0m  [24/42], [94mLoss[0m : 2.49890
[1mStep[0m  [28/42], [94mLoss[0m : 2.14353
[1mStep[0m  [32/42], [94mLoss[0m : 2.30682
[1mStep[0m  [36/42], [94mLoss[0m : 2.36438
[1mStep[0m  [40/42], [94mLoss[0m : 2.43172

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.364, [92mTest[0m: 2.492, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29756
[1mStep[0m  [4/42], [94mLoss[0m : 2.36223
[1mStep[0m  [8/42], [94mLoss[0m : 2.31777
[1mStep[0m  [12/42], [94mLoss[0m : 2.47978
[1mStep[0m  [16/42], [94mLoss[0m : 2.58454
[1mStep[0m  [20/42], [94mLoss[0m : 2.40015
[1mStep[0m  [24/42], [94mLoss[0m : 2.48129
[1mStep[0m  [28/42], [94mLoss[0m : 2.49000
[1mStep[0m  [32/42], [94mLoss[0m : 2.47108
[1mStep[0m  [36/42], [94mLoss[0m : 2.26756
[1mStep[0m  [40/42], [94mLoss[0m : 2.45606

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.367, [92mTest[0m: 2.568, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36987
[1mStep[0m  [4/42], [94mLoss[0m : 2.28743
[1mStep[0m  [8/42], [94mLoss[0m : 2.25502
[1mStep[0m  [12/42], [94mLoss[0m : 2.42360
[1mStep[0m  [16/42], [94mLoss[0m : 2.45192
[1mStep[0m  [20/42], [94mLoss[0m : 2.30602
[1mStep[0m  [24/42], [94mLoss[0m : 2.21474
[1mStep[0m  [28/42], [94mLoss[0m : 2.44744
[1mStep[0m  [32/42], [94mLoss[0m : 2.40109
[1mStep[0m  [36/42], [94mLoss[0m : 2.23183
[1mStep[0m  [40/42], [94mLoss[0m : 2.49650

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.338, [92mTest[0m: 2.540, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31476
[1mStep[0m  [4/42], [94mLoss[0m : 2.25444
[1mStep[0m  [8/42], [94mLoss[0m : 2.37183
[1mStep[0m  [12/42], [94mLoss[0m : 2.39125
[1mStep[0m  [16/42], [94mLoss[0m : 2.29694
[1mStep[0m  [20/42], [94mLoss[0m : 2.15718
[1mStep[0m  [24/42], [94mLoss[0m : 2.26236
[1mStep[0m  [28/42], [94mLoss[0m : 2.34655
[1mStep[0m  [32/42], [94mLoss[0m : 2.40376
[1mStep[0m  [36/42], [94mLoss[0m : 2.38182
[1mStep[0m  [40/42], [94mLoss[0m : 2.24499

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.334, [92mTest[0m: 2.577, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34466
[1mStep[0m  [4/42], [94mLoss[0m : 2.49576
[1mStep[0m  [8/42], [94mLoss[0m : 2.27529
[1mStep[0m  [12/42], [94mLoss[0m : 2.21810
[1mStep[0m  [16/42], [94mLoss[0m : 2.14321
[1mStep[0m  [20/42], [94mLoss[0m : 2.54092
[1mStep[0m  [24/42], [94mLoss[0m : 2.48004
[1mStep[0m  [28/42], [94mLoss[0m : 2.16464
[1mStep[0m  [32/42], [94mLoss[0m : 2.32251
[1mStep[0m  [36/42], [94mLoss[0m : 2.23258
[1mStep[0m  [40/42], [94mLoss[0m : 2.45704

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.319, [92mTest[0m: 2.558, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39249
[1mStep[0m  [4/42], [94mLoss[0m : 2.27689
[1mStep[0m  [8/42], [94mLoss[0m : 2.42809
[1mStep[0m  [12/42], [94mLoss[0m : 2.27285
[1mStep[0m  [16/42], [94mLoss[0m : 2.27208
[1mStep[0m  [20/42], [94mLoss[0m : 2.32604
[1mStep[0m  [24/42], [94mLoss[0m : 2.36689
[1mStep[0m  [28/42], [94mLoss[0m : 2.30879
[1mStep[0m  [32/42], [94mLoss[0m : 2.21708
[1mStep[0m  [36/42], [94mLoss[0m : 2.17700
[1mStep[0m  [40/42], [94mLoss[0m : 2.32785

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.297, [92mTest[0m: 2.582, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24591
[1mStep[0m  [4/42], [94mLoss[0m : 2.35001
[1mStep[0m  [8/42], [94mLoss[0m : 2.28359
[1mStep[0m  [12/42], [94mLoss[0m : 2.38708
[1mStep[0m  [16/42], [94mLoss[0m : 2.43084
[1mStep[0m  [20/42], [94mLoss[0m : 2.04474
[1mStep[0m  [24/42], [94mLoss[0m : 2.25996
[1mStep[0m  [28/42], [94mLoss[0m : 1.96988
[1mStep[0m  [32/42], [94mLoss[0m : 2.14069
[1mStep[0m  [36/42], [94mLoss[0m : 2.24401
[1mStep[0m  [40/42], [94mLoss[0m : 2.37906

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.277, [92mTest[0m: 2.646, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31729
[1mStep[0m  [4/42], [94mLoss[0m : 2.22258
[1mStep[0m  [8/42], [94mLoss[0m : 2.42326
[1mStep[0m  [12/42], [94mLoss[0m : 2.40527
[1mStep[0m  [16/42], [94mLoss[0m : 2.16391
[1mStep[0m  [20/42], [94mLoss[0m : 2.09069
[1mStep[0m  [24/42], [94mLoss[0m : 2.18766
[1mStep[0m  [28/42], [94mLoss[0m : 2.42831
[1mStep[0m  [32/42], [94mLoss[0m : 2.15049
[1mStep[0m  [36/42], [94mLoss[0m : 1.97916
[1mStep[0m  [40/42], [94mLoss[0m : 2.41223

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.256, [92mTest[0m: 2.572, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26447
[1mStep[0m  [4/42], [94mLoss[0m : 2.19559
[1mStep[0m  [8/42], [94mLoss[0m : 2.07043
[1mStep[0m  [12/42], [94mLoss[0m : 2.31567
[1mStep[0m  [16/42], [94mLoss[0m : 2.17517
[1mStep[0m  [20/42], [94mLoss[0m : 2.46412
[1mStep[0m  [24/42], [94mLoss[0m : 2.26511
[1mStep[0m  [28/42], [94mLoss[0m : 2.19091
[1mStep[0m  [32/42], [94mLoss[0m : 2.27855
[1mStep[0m  [36/42], [94mLoss[0m : 2.22919
[1mStep[0m  [40/42], [94mLoss[0m : 2.04297

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.247, [92mTest[0m: 2.634, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32344
[1mStep[0m  [4/42], [94mLoss[0m : 2.20347
[1mStep[0m  [8/42], [94mLoss[0m : 2.30553
[1mStep[0m  [12/42], [94mLoss[0m : 2.11718
[1mStep[0m  [16/42], [94mLoss[0m : 1.97751
[1mStep[0m  [20/42], [94mLoss[0m : 2.19365
[1mStep[0m  [24/42], [94mLoss[0m : 2.32979
[1mStep[0m  [28/42], [94mLoss[0m : 2.18217
[1mStep[0m  [32/42], [94mLoss[0m : 2.27996
[1mStep[0m  [36/42], [94mLoss[0m : 2.16448
[1mStep[0m  [40/42], [94mLoss[0m : 2.02243

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.225, [92mTest[0m: 2.666, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.18708
[1mStep[0m  [4/42], [94mLoss[0m : 2.03291
[1mStep[0m  [8/42], [94mLoss[0m : 2.25470
[1mStep[0m  [12/42], [94mLoss[0m : 2.08434
[1mStep[0m  [16/42], [94mLoss[0m : 2.11420
[1mStep[0m  [20/42], [94mLoss[0m : 2.40161
[1mStep[0m  [24/42], [94mLoss[0m : 2.34010
[1mStep[0m  [28/42], [94mLoss[0m : 1.86926
[1mStep[0m  [32/42], [94mLoss[0m : 2.38200
[1mStep[0m  [36/42], [94mLoss[0m : 2.35589
[1mStep[0m  [40/42], [94mLoss[0m : 2.26845

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.201, [92mTest[0m: 2.646, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.00009
[1mStep[0m  [4/42], [94mLoss[0m : 2.43235
[1mStep[0m  [8/42], [94mLoss[0m : 1.96013
[1mStep[0m  [12/42], [94mLoss[0m : 2.20916
[1mStep[0m  [16/42], [94mLoss[0m : 2.09532
[1mStep[0m  [20/42], [94mLoss[0m : 2.20876
[1mStep[0m  [24/42], [94mLoss[0m : 2.31868
[1mStep[0m  [28/42], [94mLoss[0m : 2.09121
[1mStep[0m  [32/42], [94mLoss[0m : 2.26046
[1mStep[0m  [36/42], [94mLoss[0m : 2.06929
[1mStep[0m  [40/42], [94mLoss[0m : 2.18486

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.189, [92mTest[0m: 2.600, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.17647
[1mStep[0m  [4/42], [94mLoss[0m : 2.12924
[1mStep[0m  [8/42], [94mLoss[0m : 2.19165
[1mStep[0m  [12/42], [94mLoss[0m : 2.09976
[1mStep[0m  [16/42], [94mLoss[0m : 2.20844
[1mStep[0m  [20/42], [94mLoss[0m : 2.25292
[1mStep[0m  [24/42], [94mLoss[0m : 2.02922
[1mStep[0m  [28/42], [94mLoss[0m : 2.32669
[1mStep[0m  [32/42], [94mLoss[0m : 2.16138
[1mStep[0m  [36/42], [94mLoss[0m : 2.12066
[1mStep[0m  [40/42], [94mLoss[0m : 2.07156

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.159, [92mTest[0m: 2.623, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.98932
[1mStep[0m  [4/42], [94mLoss[0m : 2.48136
[1mStep[0m  [8/42], [94mLoss[0m : 1.88010
[1mStep[0m  [12/42], [94mLoss[0m : 2.19448
[1mStep[0m  [16/42], [94mLoss[0m : 2.23025
[1mStep[0m  [20/42], [94mLoss[0m : 2.16795
[1mStep[0m  [24/42], [94mLoss[0m : 2.18806
[1mStep[0m  [28/42], [94mLoss[0m : 2.07855
[1mStep[0m  [32/42], [94mLoss[0m : 2.11989
[1mStep[0m  [36/42], [94mLoss[0m : 2.07066
[1mStep[0m  [40/42], [94mLoss[0m : 2.16480

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.147, [92mTest[0m: 2.631, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.89645
[1mStep[0m  [4/42], [94mLoss[0m : 2.02218
[1mStep[0m  [8/42], [94mLoss[0m : 2.14658
[1mStep[0m  [12/42], [94mLoss[0m : 2.20184
[1mStep[0m  [16/42], [94mLoss[0m : 2.05995
[1mStep[0m  [20/42], [94mLoss[0m : 2.03505
[1mStep[0m  [24/42], [94mLoss[0m : 2.06072
[1mStep[0m  [28/42], [94mLoss[0m : 2.09757
[1mStep[0m  [32/42], [94mLoss[0m : 2.07849
[1mStep[0m  [36/42], [94mLoss[0m : 2.27565
[1mStep[0m  [40/42], [94mLoss[0m : 2.08228

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.122, [92mTest[0m: 2.614, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.13797
[1mStep[0m  [4/42], [94mLoss[0m : 2.07469
[1mStep[0m  [8/42], [94mLoss[0m : 2.03238
[1mStep[0m  [12/42], [94mLoss[0m : 2.09417
[1mStep[0m  [16/42], [94mLoss[0m : 2.10624
[1mStep[0m  [20/42], [94mLoss[0m : 2.18173
[1mStep[0m  [24/42], [94mLoss[0m : 2.13659
[1mStep[0m  [28/42], [94mLoss[0m : 2.01602
[1mStep[0m  [32/42], [94mLoss[0m : 2.02061
[1mStep[0m  [36/42], [94mLoss[0m : 2.10008
[1mStep[0m  [40/42], [94mLoss[0m : 2.10301

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.102, [92mTest[0m: 2.600, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.02236
[1mStep[0m  [4/42], [94mLoss[0m : 2.23289
[1mStep[0m  [8/42], [94mLoss[0m : 2.17143
[1mStep[0m  [12/42], [94mLoss[0m : 2.06775
[1mStep[0m  [16/42], [94mLoss[0m : 2.04091
[1mStep[0m  [20/42], [94mLoss[0m : 1.98981
[1mStep[0m  [24/42], [94mLoss[0m : 2.09909
[1mStep[0m  [28/42], [94mLoss[0m : 2.07989
[1mStep[0m  [32/42], [94mLoss[0m : 1.95964
[1mStep[0m  [36/42], [94mLoss[0m : 1.91453
[1mStep[0m  [40/42], [94mLoss[0m : 2.12106

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.082, [92mTest[0m: 2.646, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.00239
[1mStep[0m  [4/42], [94mLoss[0m : 2.01293
[1mStep[0m  [8/42], [94mLoss[0m : 1.99069
[1mStep[0m  [12/42], [94mLoss[0m : 2.07532
[1mStep[0m  [16/42], [94mLoss[0m : 1.99465
[1mStep[0m  [20/42], [94mLoss[0m : 2.00540
[1mStep[0m  [24/42], [94mLoss[0m : 2.08176
[1mStep[0m  [28/42], [94mLoss[0m : 2.08300
[1mStep[0m  [32/42], [94mLoss[0m : 2.09463
[1mStep[0m  [36/42], [94mLoss[0m : 2.17396
[1mStep[0m  [40/42], [94mLoss[0m : 2.19450

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.065, [92mTest[0m: 2.724, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.00278
[1mStep[0m  [4/42], [94mLoss[0m : 1.91385
[1mStep[0m  [8/42], [94mLoss[0m : 2.22396
[1mStep[0m  [12/42], [94mLoss[0m : 2.19996
[1mStep[0m  [16/42], [94mLoss[0m : 2.27663
[1mStep[0m  [20/42], [94mLoss[0m : 2.13008
[1mStep[0m  [24/42], [94mLoss[0m : 2.03416
[1mStep[0m  [28/42], [94mLoss[0m : 1.95545
[1mStep[0m  [32/42], [94mLoss[0m : 2.21101
[1mStep[0m  [36/42], [94mLoss[0m : 2.02241
[1mStep[0m  [40/42], [94mLoss[0m : 1.93450

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.037, [92mTest[0m: 2.654, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.88906
[1mStep[0m  [4/42], [94mLoss[0m : 2.05219
[1mStep[0m  [8/42], [94mLoss[0m : 1.84766
[1mStep[0m  [12/42], [94mLoss[0m : 1.98405
[1mStep[0m  [16/42], [94mLoss[0m : 1.99730
[1mStep[0m  [20/42], [94mLoss[0m : 1.99564
[1mStep[0m  [24/42], [94mLoss[0m : 2.13050
[1mStep[0m  [28/42], [94mLoss[0m : 2.14278
[1mStep[0m  [32/42], [94mLoss[0m : 2.08081
[1mStep[0m  [36/42], [94mLoss[0m : 2.20110
[1mStep[0m  [40/42], [94mLoss[0m : 2.05776

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.014, [92mTest[0m: 2.611, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.96659
[1mStep[0m  [4/42], [94mLoss[0m : 2.12001
[1mStep[0m  [8/42], [94mLoss[0m : 1.98570
[1mStep[0m  [12/42], [94mLoss[0m : 1.98695
[1mStep[0m  [16/42], [94mLoss[0m : 1.88840
[1mStep[0m  [20/42], [94mLoss[0m : 2.12826
[1mStep[0m  [24/42], [94mLoss[0m : 1.77791
[1mStep[0m  [28/42], [94mLoss[0m : 1.96968
[1mStep[0m  [32/42], [94mLoss[0m : 2.05269
[1mStep[0m  [36/42], [94mLoss[0m : 1.82025
[1mStep[0m  [40/42], [94mLoss[0m : 1.98261

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.988, [92mTest[0m: 2.702, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.98033
[1mStep[0m  [4/42], [94mLoss[0m : 1.94101
[1mStep[0m  [8/42], [94mLoss[0m : 1.93200
[1mStep[0m  [12/42], [94mLoss[0m : 2.18806
[1mStep[0m  [16/42], [94mLoss[0m : 2.08590
[1mStep[0m  [20/42], [94mLoss[0m : 1.95321
[1mStep[0m  [24/42], [94mLoss[0m : 1.95894
[1mStep[0m  [28/42], [94mLoss[0m : 2.06173
[1mStep[0m  [32/42], [94mLoss[0m : 1.76830
[1mStep[0m  [36/42], [94mLoss[0m : 2.02567
[1mStep[0m  [40/42], [94mLoss[0m : 1.90005

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.947, [92mTest[0m: 2.584, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.89531
[1mStep[0m  [4/42], [94mLoss[0m : 2.17807
[1mStep[0m  [8/42], [94mLoss[0m : 2.02560
[1mStep[0m  [12/42], [94mLoss[0m : 1.93945
[1mStep[0m  [16/42], [94mLoss[0m : 1.96678
[1mStep[0m  [20/42], [94mLoss[0m : 2.06921
[1mStep[0m  [24/42], [94mLoss[0m : 2.08601
[1mStep[0m  [28/42], [94mLoss[0m : 1.83321
[1mStep[0m  [32/42], [94mLoss[0m : 1.76823
[1mStep[0m  [36/42], [94mLoss[0m : 2.00459
[1mStep[0m  [40/42], [94mLoss[0m : 1.82385

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.940, [92mTest[0m: 2.614, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.661
====================================

Phase 2 - Evaluation MAE:  2.660521490233285
MAE score P1        2.338476
MAE score P2        2.660521
loss                1.940107
learning_rate       0.002575
batch_size               256
hidden_sizes      [300, 100]
epochs                    30
activation           sigmoid
optimizer                sgd
early stopping         False
dropout                  0.2
momentum                 0.5
weight_decay          0.0001
Name: 1, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 10.59168
[1mStep[0m  [4/42], [94mLoss[0m : 9.56887
[1mStep[0m  [8/42], [94mLoss[0m : 8.42067
[1mStep[0m  [12/42], [94mLoss[0m : 7.19966
[1mStep[0m  [16/42], [94mLoss[0m : 5.84954
[1mStep[0m  [20/42], [94mLoss[0m : 4.49807
[1mStep[0m  [24/42], [94mLoss[0m : 3.99695
[1mStep[0m  [28/42], [94mLoss[0m : 3.23872
[1mStep[0m  [32/42], [94mLoss[0m : 3.12203
[1mStep[0m  [36/42], [94mLoss[0m : 2.71479
[1mStep[0m  [40/42], [94mLoss[0m : 2.83624

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 5.544, [92mTest[0m: 10.696, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.85824
[1mStep[0m  [4/42], [94mLoss[0m : 2.76661
[1mStep[0m  [8/42], [94mLoss[0m : 2.89992
[1mStep[0m  [12/42], [94mLoss[0m : 2.49409
[1mStep[0m  [16/42], [94mLoss[0m : 2.67110
[1mStep[0m  [20/42], [94mLoss[0m : 2.65828
[1mStep[0m  [24/42], [94mLoss[0m : 2.78943
[1mStep[0m  [28/42], [94mLoss[0m : 2.75283
[1mStep[0m  [32/42], [94mLoss[0m : 2.76818
[1mStep[0m  [36/42], [94mLoss[0m : 2.68680
[1mStep[0m  [40/42], [94mLoss[0m : 2.59348

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.705, [92mTest[0m: 2.854, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58434
[1mStep[0m  [4/42], [94mLoss[0m : 2.46594
[1mStep[0m  [8/42], [94mLoss[0m : 2.63319
[1mStep[0m  [12/42], [94mLoss[0m : 2.63834
[1mStep[0m  [16/42], [94mLoss[0m : 2.53516
[1mStep[0m  [20/42], [94mLoss[0m : 2.80398
[1mStep[0m  [24/42], [94mLoss[0m : 2.69237
[1mStep[0m  [28/42], [94mLoss[0m : 2.67962
[1mStep[0m  [32/42], [94mLoss[0m : 2.34620
[1mStep[0m  [36/42], [94mLoss[0m : 2.57092
[1mStep[0m  [40/42], [94mLoss[0m : 2.42419

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.577, [92mTest[0m: 2.544, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42623
[1mStep[0m  [4/42], [94mLoss[0m : 2.61399
[1mStep[0m  [8/42], [94mLoss[0m : 2.58721
[1mStep[0m  [12/42], [94mLoss[0m : 2.39309
[1mStep[0m  [16/42], [94mLoss[0m : 2.39077
[1mStep[0m  [20/42], [94mLoss[0m : 2.47702
[1mStep[0m  [24/42], [94mLoss[0m : 2.55456
[1mStep[0m  [28/42], [94mLoss[0m : 2.51944
[1mStep[0m  [32/42], [94mLoss[0m : 2.37460
[1mStep[0m  [36/42], [94mLoss[0m : 2.51668
[1mStep[0m  [40/42], [94mLoss[0m : 2.61973

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.483, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.65252
[1mStep[0m  [4/42], [94mLoss[0m : 2.42908
[1mStep[0m  [8/42], [94mLoss[0m : 2.53684
[1mStep[0m  [12/42], [94mLoss[0m : 2.54461
[1mStep[0m  [16/42], [94mLoss[0m : 2.52789
[1mStep[0m  [20/42], [94mLoss[0m : 2.50451
[1mStep[0m  [24/42], [94mLoss[0m : 2.49802
[1mStep[0m  [28/42], [94mLoss[0m : 2.60486
[1mStep[0m  [32/42], [94mLoss[0m : 2.47463
[1mStep[0m  [36/42], [94mLoss[0m : 2.58518
[1mStep[0m  [40/42], [94mLoss[0m : 2.62095

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.517, [92mTest[0m: 2.451, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56634
[1mStep[0m  [4/42], [94mLoss[0m : 2.48083
[1mStep[0m  [8/42], [94mLoss[0m : 2.53161
[1mStep[0m  [12/42], [94mLoss[0m : 2.40693
[1mStep[0m  [16/42], [94mLoss[0m : 2.76070
[1mStep[0m  [20/42], [94mLoss[0m : 2.40091
[1mStep[0m  [24/42], [94mLoss[0m : 2.65594
[1mStep[0m  [28/42], [94mLoss[0m : 2.45953
[1mStep[0m  [32/42], [94mLoss[0m : 2.49582
[1mStep[0m  [36/42], [94mLoss[0m : 2.53103
[1mStep[0m  [40/42], [94mLoss[0m : 2.54953

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.435, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31348
[1mStep[0m  [4/42], [94mLoss[0m : 2.42405
[1mStep[0m  [8/42], [94mLoss[0m : 2.56380
[1mStep[0m  [12/42], [94mLoss[0m : 2.36378
[1mStep[0m  [16/42], [94mLoss[0m : 2.33122
[1mStep[0m  [20/42], [94mLoss[0m : 2.56958
[1mStep[0m  [24/42], [94mLoss[0m : 2.60209
[1mStep[0m  [28/42], [94mLoss[0m : 2.58518
[1mStep[0m  [32/42], [94mLoss[0m : 2.47324
[1mStep[0m  [36/42], [94mLoss[0m : 2.44026
[1mStep[0m  [40/42], [94mLoss[0m : 2.74255

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.423, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42222
[1mStep[0m  [4/42], [94mLoss[0m : 2.53858
[1mStep[0m  [8/42], [94mLoss[0m : 2.41859
[1mStep[0m  [12/42], [94mLoss[0m : 2.48650
[1mStep[0m  [16/42], [94mLoss[0m : 2.38585
[1mStep[0m  [20/42], [94mLoss[0m : 2.60501
[1mStep[0m  [24/42], [94mLoss[0m : 2.71434
[1mStep[0m  [28/42], [94mLoss[0m : 2.51099
[1mStep[0m  [32/42], [94mLoss[0m : 2.53872
[1mStep[0m  [36/42], [94mLoss[0m : 2.24081
[1mStep[0m  [40/42], [94mLoss[0m : 2.72769

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.421, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45769
[1mStep[0m  [4/42], [94mLoss[0m : 2.56436
[1mStep[0m  [8/42], [94mLoss[0m : 2.49865
[1mStep[0m  [12/42], [94mLoss[0m : 2.57309
[1mStep[0m  [16/42], [94mLoss[0m : 2.48970
[1mStep[0m  [20/42], [94mLoss[0m : 2.44797
[1mStep[0m  [24/42], [94mLoss[0m : 2.62387
[1mStep[0m  [28/42], [94mLoss[0m : 2.47372
[1mStep[0m  [32/42], [94mLoss[0m : 2.54123
[1mStep[0m  [36/42], [94mLoss[0m : 2.38570
[1mStep[0m  [40/42], [94mLoss[0m : 2.49980

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.398, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.66140
[1mStep[0m  [4/42], [94mLoss[0m : 2.37212
[1mStep[0m  [8/42], [94mLoss[0m : 2.55972
[1mStep[0m  [12/42], [94mLoss[0m : 2.64570
[1mStep[0m  [16/42], [94mLoss[0m : 2.57339
[1mStep[0m  [20/42], [94mLoss[0m : 2.53409
[1mStep[0m  [24/42], [94mLoss[0m : 2.42697
[1mStep[0m  [28/42], [94mLoss[0m : 2.43763
[1mStep[0m  [32/42], [94mLoss[0m : 2.39503
[1mStep[0m  [36/42], [94mLoss[0m : 2.36653
[1mStep[0m  [40/42], [94mLoss[0m : 2.24438

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.394, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58183
[1mStep[0m  [4/42], [94mLoss[0m : 2.34292
[1mStep[0m  [8/42], [94mLoss[0m : 2.50898
[1mStep[0m  [12/42], [94mLoss[0m : 2.66520
[1mStep[0m  [16/42], [94mLoss[0m : 2.72306
[1mStep[0m  [20/42], [94mLoss[0m : 2.48188
[1mStep[0m  [24/42], [94mLoss[0m : 2.64285
[1mStep[0m  [28/42], [94mLoss[0m : 2.42456
[1mStep[0m  [32/42], [94mLoss[0m : 2.33984
[1mStep[0m  [36/42], [94mLoss[0m : 2.51906
[1mStep[0m  [40/42], [94mLoss[0m : 2.63149

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.388, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44226
[1mStep[0m  [4/42], [94mLoss[0m : 2.65205
[1mStep[0m  [8/42], [94mLoss[0m : 2.40877
[1mStep[0m  [12/42], [94mLoss[0m : 2.56100
[1mStep[0m  [16/42], [94mLoss[0m : 2.51471
[1mStep[0m  [20/42], [94mLoss[0m : 2.48300
[1mStep[0m  [24/42], [94mLoss[0m : 2.17078
[1mStep[0m  [28/42], [94mLoss[0m : 2.53303
[1mStep[0m  [32/42], [94mLoss[0m : 2.47618
[1mStep[0m  [36/42], [94mLoss[0m : 2.54104
[1mStep[0m  [40/42], [94mLoss[0m : 2.37888

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.383, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44352
[1mStep[0m  [4/42], [94mLoss[0m : 2.46927
[1mStep[0m  [8/42], [94mLoss[0m : 2.66615
[1mStep[0m  [12/42], [94mLoss[0m : 2.37344
[1mStep[0m  [16/42], [94mLoss[0m : 2.55693
[1mStep[0m  [20/42], [94mLoss[0m : 2.67888
[1mStep[0m  [24/42], [94mLoss[0m : 2.38894
[1mStep[0m  [28/42], [94mLoss[0m : 2.51620
[1mStep[0m  [32/42], [94mLoss[0m : 2.42289
[1mStep[0m  [36/42], [94mLoss[0m : 2.24547
[1mStep[0m  [40/42], [94mLoss[0m : 2.49171

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.374, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50774
[1mStep[0m  [4/42], [94mLoss[0m : 2.33320
[1mStep[0m  [8/42], [94mLoss[0m : 2.27645
[1mStep[0m  [12/42], [94mLoss[0m : 2.45695
[1mStep[0m  [16/42], [94mLoss[0m : 2.52371
[1mStep[0m  [20/42], [94mLoss[0m : 2.35333
[1mStep[0m  [24/42], [94mLoss[0m : 2.73948
[1mStep[0m  [28/42], [94mLoss[0m : 2.69496
[1mStep[0m  [32/42], [94mLoss[0m : 2.47443
[1mStep[0m  [36/42], [94mLoss[0m : 2.42396
[1mStep[0m  [40/42], [94mLoss[0m : 2.31372

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.371, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39696
[1mStep[0m  [4/42], [94mLoss[0m : 2.41224
[1mStep[0m  [8/42], [94mLoss[0m : 2.54376
[1mStep[0m  [12/42], [94mLoss[0m : 2.50962
[1mStep[0m  [16/42], [94mLoss[0m : 2.26422
[1mStep[0m  [20/42], [94mLoss[0m : 2.50024
[1mStep[0m  [24/42], [94mLoss[0m : 2.28537
[1mStep[0m  [28/42], [94mLoss[0m : 2.62965
[1mStep[0m  [32/42], [94mLoss[0m : 2.56889
[1mStep[0m  [36/42], [94mLoss[0m : 2.41328
[1mStep[0m  [40/42], [94mLoss[0m : 2.38259

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.373, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41683
[1mStep[0m  [4/42], [94mLoss[0m : 2.50890
[1mStep[0m  [8/42], [94mLoss[0m : 2.52010
[1mStep[0m  [12/42], [94mLoss[0m : 2.52095
[1mStep[0m  [16/42], [94mLoss[0m : 2.65101
[1mStep[0m  [20/42], [94mLoss[0m : 2.37548
[1mStep[0m  [24/42], [94mLoss[0m : 2.71285
[1mStep[0m  [28/42], [94mLoss[0m : 2.27651
[1mStep[0m  [32/42], [94mLoss[0m : 2.27036
[1mStep[0m  [36/42], [94mLoss[0m : 2.48055
[1mStep[0m  [40/42], [94mLoss[0m : 2.19534

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.373, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50237
[1mStep[0m  [4/42], [94mLoss[0m : 2.60247
[1mStep[0m  [8/42], [94mLoss[0m : 2.30480
[1mStep[0m  [12/42], [94mLoss[0m : 2.44690
[1mStep[0m  [16/42], [94mLoss[0m : 2.45079
[1mStep[0m  [20/42], [94mLoss[0m : 2.46412
[1mStep[0m  [24/42], [94mLoss[0m : 2.64656
[1mStep[0m  [28/42], [94mLoss[0m : 2.29511
[1mStep[0m  [32/42], [94mLoss[0m : 2.40152
[1mStep[0m  [36/42], [94mLoss[0m : 2.61545
[1mStep[0m  [40/42], [94mLoss[0m : 2.30692

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.365, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41383
[1mStep[0m  [4/42], [94mLoss[0m : 2.31397
[1mStep[0m  [8/42], [94mLoss[0m : 2.38291
[1mStep[0m  [12/42], [94mLoss[0m : 2.33540
[1mStep[0m  [16/42], [94mLoss[0m : 2.42999
[1mStep[0m  [20/42], [94mLoss[0m : 2.51442
[1mStep[0m  [24/42], [94mLoss[0m : 2.63314
[1mStep[0m  [28/42], [94mLoss[0m : 2.42764
[1mStep[0m  [32/42], [94mLoss[0m : 2.46251
[1mStep[0m  [36/42], [94mLoss[0m : 2.46838
[1mStep[0m  [40/42], [94mLoss[0m : 2.23383

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.368, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64459
[1mStep[0m  [4/42], [94mLoss[0m : 2.33351
[1mStep[0m  [8/42], [94mLoss[0m : 2.52887
[1mStep[0m  [12/42], [94mLoss[0m : 2.24584
[1mStep[0m  [16/42], [94mLoss[0m : 2.37067
[1mStep[0m  [20/42], [94mLoss[0m : 2.53046
[1mStep[0m  [24/42], [94mLoss[0m : 2.62818
[1mStep[0m  [28/42], [94mLoss[0m : 2.54778
[1mStep[0m  [32/42], [94mLoss[0m : 2.42357
[1mStep[0m  [36/42], [94mLoss[0m : 2.65080
[1mStep[0m  [40/42], [94mLoss[0m : 2.64385

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.361, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38410
[1mStep[0m  [4/42], [94mLoss[0m : 2.36008
[1mStep[0m  [8/42], [94mLoss[0m : 2.58783
[1mStep[0m  [12/42], [94mLoss[0m : 2.57074
[1mStep[0m  [16/42], [94mLoss[0m : 2.56023
[1mStep[0m  [20/42], [94mLoss[0m : 2.32152
[1mStep[0m  [24/42], [94mLoss[0m : 2.49658
[1mStep[0m  [28/42], [94mLoss[0m : 2.35694
[1mStep[0m  [32/42], [94mLoss[0m : 2.65566
[1mStep[0m  [36/42], [94mLoss[0m : 2.52865
[1mStep[0m  [40/42], [94mLoss[0m : 2.84723

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.357, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41928
[1mStep[0m  [4/42], [94mLoss[0m : 2.35921
[1mStep[0m  [8/42], [94mLoss[0m : 2.46619
[1mStep[0m  [12/42], [94mLoss[0m : 2.55882
[1mStep[0m  [16/42], [94mLoss[0m : 2.40459
[1mStep[0m  [20/42], [94mLoss[0m : 2.41450
[1mStep[0m  [24/42], [94mLoss[0m : 2.53279
[1mStep[0m  [28/42], [94mLoss[0m : 2.46020
[1mStep[0m  [32/42], [94mLoss[0m : 2.34015
[1mStep[0m  [36/42], [94mLoss[0m : 2.27327
[1mStep[0m  [40/42], [94mLoss[0m : 2.48122

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.348, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31616
[1mStep[0m  [4/42], [94mLoss[0m : 2.53210
[1mStep[0m  [8/42], [94mLoss[0m : 2.35129
[1mStep[0m  [12/42], [94mLoss[0m : 2.57467
[1mStep[0m  [16/42], [94mLoss[0m : 2.39296
[1mStep[0m  [20/42], [94mLoss[0m : 2.51615
[1mStep[0m  [24/42], [94mLoss[0m : 2.41265
[1mStep[0m  [28/42], [94mLoss[0m : 2.49906
[1mStep[0m  [32/42], [94mLoss[0m : 2.56808
[1mStep[0m  [36/42], [94mLoss[0m : 2.37804
[1mStep[0m  [40/42], [94mLoss[0m : 2.27869

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.345, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37565
[1mStep[0m  [4/42], [94mLoss[0m : 2.52801
[1mStep[0m  [8/42], [94mLoss[0m : 2.49974
[1mStep[0m  [12/42], [94mLoss[0m : 2.52168
[1mStep[0m  [16/42], [94mLoss[0m : 2.35333
[1mStep[0m  [20/42], [94mLoss[0m : 2.54051
[1mStep[0m  [24/42], [94mLoss[0m : 2.49095
[1mStep[0m  [28/42], [94mLoss[0m : 2.27045
[1mStep[0m  [32/42], [94mLoss[0m : 2.42472
[1mStep[0m  [36/42], [94mLoss[0m : 2.58458
[1mStep[0m  [40/42], [94mLoss[0m : 2.35379

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.357, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53116
[1mStep[0m  [4/42], [94mLoss[0m : 2.42664
[1mStep[0m  [8/42], [94mLoss[0m : 2.50104
[1mStep[0m  [12/42], [94mLoss[0m : 2.37890
[1mStep[0m  [16/42], [94mLoss[0m : 2.37440
[1mStep[0m  [20/42], [94mLoss[0m : 2.32039
[1mStep[0m  [24/42], [94mLoss[0m : 2.40011
[1mStep[0m  [28/42], [94mLoss[0m : 2.31183
[1mStep[0m  [32/42], [94mLoss[0m : 2.41963
[1mStep[0m  [36/42], [94mLoss[0m : 2.72392
[1mStep[0m  [40/42], [94mLoss[0m : 2.48550

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.352, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64060
[1mStep[0m  [4/42], [94mLoss[0m : 2.72821
[1mStep[0m  [8/42], [94mLoss[0m : 2.37865
[1mStep[0m  [12/42], [94mLoss[0m : 2.29620
[1mStep[0m  [16/42], [94mLoss[0m : 2.36237
[1mStep[0m  [20/42], [94mLoss[0m : 2.52842
[1mStep[0m  [24/42], [94mLoss[0m : 2.42661
[1mStep[0m  [28/42], [94mLoss[0m : 2.33946
[1mStep[0m  [32/42], [94mLoss[0m : 2.42837
[1mStep[0m  [36/42], [94mLoss[0m : 2.43657
[1mStep[0m  [40/42], [94mLoss[0m : 2.49249

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.348, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.30204
[1mStep[0m  [4/42], [94mLoss[0m : 2.30534
[1mStep[0m  [8/42], [94mLoss[0m : 2.50553
[1mStep[0m  [12/42], [94mLoss[0m : 2.49644
[1mStep[0m  [16/42], [94mLoss[0m : 2.20380
[1mStep[0m  [20/42], [94mLoss[0m : 2.71847
[1mStep[0m  [24/42], [94mLoss[0m : 2.30011
[1mStep[0m  [28/42], [94mLoss[0m : 2.46887
[1mStep[0m  [32/42], [94mLoss[0m : 2.36829
[1mStep[0m  [36/42], [94mLoss[0m : 2.61173
[1mStep[0m  [40/42], [94mLoss[0m : 2.49094

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.348, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40086
[1mStep[0m  [4/42], [94mLoss[0m : 2.62578
[1mStep[0m  [8/42], [94mLoss[0m : 2.51488
[1mStep[0m  [12/42], [94mLoss[0m : 2.48142
[1mStep[0m  [16/42], [94mLoss[0m : 2.33864
[1mStep[0m  [20/42], [94mLoss[0m : 2.47462
[1mStep[0m  [24/42], [94mLoss[0m : 2.29694
[1mStep[0m  [28/42], [94mLoss[0m : 2.60409
[1mStep[0m  [32/42], [94mLoss[0m : 2.24806
[1mStep[0m  [36/42], [94mLoss[0m : 2.11281
[1mStep[0m  [40/42], [94mLoss[0m : 2.41485

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.347, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36257
[1mStep[0m  [4/42], [94mLoss[0m : 2.24363
[1mStep[0m  [8/42], [94mLoss[0m : 2.53445
[1mStep[0m  [12/42], [94mLoss[0m : 2.44723
[1mStep[0m  [16/42], [94mLoss[0m : 2.28925
[1mStep[0m  [20/42], [94mLoss[0m : 2.41923
[1mStep[0m  [24/42], [94mLoss[0m : 2.52206
[1mStep[0m  [28/42], [94mLoss[0m : 2.36540
[1mStep[0m  [32/42], [94mLoss[0m : 2.37336
[1mStep[0m  [36/42], [94mLoss[0m : 2.22284
[1mStep[0m  [40/42], [94mLoss[0m : 2.56371

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.341, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36831
[1mStep[0m  [4/42], [94mLoss[0m : 2.30415
[1mStep[0m  [8/42], [94mLoss[0m : 2.45282
[1mStep[0m  [12/42], [94mLoss[0m : 2.42079
[1mStep[0m  [16/42], [94mLoss[0m : 2.54066
[1mStep[0m  [20/42], [94mLoss[0m : 2.42383
[1mStep[0m  [24/42], [94mLoss[0m : 2.35358
[1mStep[0m  [28/42], [94mLoss[0m : 2.53327
[1mStep[0m  [32/42], [94mLoss[0m : 2.49977
[1mStep[0m  [36/42], [94mLoss[0m : 2.58484
[1mStep[0m  [40/42], [94mLoss[0m : 2.29224

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.349, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34372
[1mStep[0m  [4/42], [94mLoss[0m : 2.42306
[1mStep[0m  [8/42], [94mLoss[0m : 2.47910
[1mStep[0m  [12/42], [94mLoss[0m : 2.33760
[1mStep[0m  [16/42], [94mLoss[0m : 2.50099
[1mStep[0m  [20/42], [94mLoss[0m : 2.51339
[1mStep[0m  [24/42], [94mLoss[0m : 2.41620
[1mStep[0m  [28/42], [94mLoss[0m : 2.43543
[1mStep[0m  [32/42], [94mLoss[0m : 2.69235
[1mStep[0m  [36/42], [94mLoss[0m : 2.42090
[1mStep[0m  [40/42], [94mLoss[0m : 2.46025

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.342, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.345
====================================

Phase 1 - Evaluation MAE:  2.3450064148221696
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 2.55974
[1mStep[0m  [4/42], [94mLoss[0m : 2.53573
[1mStep[0m  [8/42], [94mLoss[0m : 2.29674
[1mStep[0m  [12/42], [94mLoss[0m : 2.50498
[1mStep[0m  [16/42], [94mLoss[0m : 2.42682
[1mStep[0m  [20/42], [94mLoss[0m : 2.32907
[1mStep[0m  [24/42], [94mLoss[0m : 2.60198
[1mStep[0m  [28/42], [94mLoss[0m : 2.53344
[1mStep[0m  [32/42], [94mLoss[0m : 2.35969
[1mStep[0m  [36/42], [94mLoss[0m : 2.41889
[1mStep[0m  [40/42], [94mLoss[0m : 2.44205

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.345, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40277
[1mStep[0m  [4/42], [94mLoss[0m : 2.52591
[1mStep[0m  [8/42], [94mLoss[0m : 2.57142
[1mStep[0m  [12/42], [94mLoss[0m : 2.61413
[1mStep[0m  [16/42], [94mLoss[0m : 2.34104
[1mStep[0m  [20/42], [94mLoss[0m : 2.36095
[1mStep[0m  [24/42], [94mLoss[0m : 2.20256
[1mStep[0m  [28/42], [94mLoss[0m : 2.43603
[1mStep[0m  [32/42], [94mLoss[0m : 2.47010
[1mStep[0m  [36/42], [94mLoss[0m : 2.52428
[1mStep[0m  [40/42], [94mLoss[0m : 2.62501

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.352, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34809
[1mStep[0m  [4/42], [94mLoss[0m : 2.39369
[1mStep[0m  [8/42], [94mLoss[0m : 2.37204
[1mStep[0m  [12/42], [94mLoss[0m : 2.35988
[1mStep[0m  [16/42], [94mLoss[0m : 2.36399
[1mStep[0m  [20/42], [94mLoss[0m : 2.36257
[1mStep[0m  [24/42], [94mLoss[0m : 2.37797
[1mStep[0m  [28/42], [94mLoss[0m : 2.41444
[1mStep[0m  [32/42], [94mLoss[0m : 2.25706
[1mStep[0m  [36/42], [94mLoss[0m : 2.46389
[1mStep[0m  [40/42], [94mLoss[0m : 2.68142

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.334, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46253
[1mStep[0m  [4/42], [94mLoss[0m : 2.31885
[1mStep[0m  [8/42], [94mLoss[0m : 2.38762
[1mStep[0m  [12/42], [94mLoss[0m : 2.43282
[1mStep[0m  [16/42], [94mLoss[0m : 2.48319
[1mStep[0m  [20/42], [94mLoss[0m : 2.40373
[1mStep[0m  [24/42], [94mLoss[0m : 2.37636
[1mStep[0m  [28/42], [94mLoss[0m : 2.44989
[1mStep[0m  [32/42], [94mLoss[0m : 2.38211
[1mStep[0m  [36/42], [94mLoss[0m : 2.46298
[1mStep[0m  [40/42], [94mLoss[0m : 2.43913

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.329, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43276
[1mStep[0m  [4/42], [94mLoss[0m : 2.27918
[1mStep[0m  [8/42], [94mLoss[0m : 2.47377
[1mStep[0m  [12/42], [94mLoss[0m : 2.17638
[1mStep[0m  [16/42], [94mLoss[0m : 2.13601
[1mStep[0m  [20/42], [94mLoss[0m : 2.26169
[1mStep[0m  [24/42], [94mLoss[0m : 2.42016
[1mStep[0m  [28/42], [94mLoss[0m : 2.49446
[1mStep[0m  [32/42], [94mLoss[0m : 2.38843
[1mStep[0m  [36/42], [94mLoss[0m : 2.45552
[1mStep[0m  [40/42], [94mLoss[0m : 2.32814

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.383, [92mTest[0m: 2.329, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.13597
[1mStep[0m  [4/42], [94mLoss[0m : 2.57709
[1mStep[0m  [8/42], [94mLoss[0m : 2.29874
[1mStep[0m  [12/42], [94mLoss[0m : 2.32845
[1mStep[0m  [16/42], [94mLoss[0m : 2.10840
[1mStep[0m  [20/42], [94mLoss[0m : 2.61538
[1mStep[0m  [24/42], [94mLoss[0m : 2.34728
[1mStep[0m  [28/42], [94mLoss[0m : 2.27882
[1mStep[0m  [32/42], [94mLoss[0m : 2.48488
[1mStep[0m  [36/42], [94mLoss[0m : 2.28661
[1mStep[0m  [40/42], [94mLoss[0m : 2.41905

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.367, [92mTest[0m: 2.314, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24666
[1mStep[0m  [4/42], [94mLoss[0m : 2.63634
[1mStep[0m  [8/42], [94mLoss[0m : 2.35126
[1mStep[0m  [12/42], [94mLoss[0m : 2.20246
[1mStep[0m  [16/42], [94mLoss[0m : 2.34132
[1mStep[0m  [20/42], [94mLoss[0m : 2.56702
[1mStep[0m  [24/42], [94mLoss[0m : 2.55876
[1mStep[0m  [28/42], [94mLoss[0m : 2.34248
[1mStep[0m  [32/42], [94mLoss[0m : 2.47779
[1mStep[0m  [36/42], [94mLoss[0m : 2.26323
[1mStep[0m  [40/42], [94mLoss[0m : 2.45056

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.370, [92mTest[0m: 2.324, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33721
[1mStep[0m  [4/42], [94mLoss[0m : 2.35244
[1mStep[0m  [8/42], [94mLoss[0m : 2.24798
[1mStep[0m  [12/42], [94mLoss[0m : 2.53622
[1mStep[0m  [16/42], [94mLoss[0m : 2.42017
[1mStep[0m  [20/42], [94mLoss[0m : 2.22137
[1mStep[0m  [24/42], [94mLoss[0m : 2.38649
[1mStep[0m  [28/42], [94mLoss[0m : 2.32876
[1mStep[0m  [32/42], [94mLoss[0m : 2.30138
[1mStep[0m  [36/42], [94mLoss[0m : 2.41481
[1mStep[0m  [40/42], [94mLoss[0m : 2.36922

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.344, [92mTest[0m: 2.366, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26278
[1mStep[0m  [4/42], [94mLoss[0m : 2.03093
[1mStep[0m  [8/42], [94mLoss[0m : 2.51430
[1mStep[0m  [12/42], [94mLoss[0m : 2.39897
[1mStep[0m  [16/42], [94mLoss[0m : 2.35097
[1mStep[0m  [20/42], [94mLoss[0m : 2.17937
[1mStep[0m  [24/42], [94mLoss[0m : 2.17134
[1mStep[0m  [28/42], [94mLoss[0m : 2.29291
[1mStep[0m  [32/42], [94mLoss[0m : 2.31366
[1mStep[0m  [36/42], [94mLoss[0m : 2.45774
[1mStep[0m  [40/42], [94mLoss[0m : 2.59805

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.332, [92mTest[0m: 2.359, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49179
[1mStep[0m  [4/42], [94mLoss[0m : 2.40279
[1mStep[0m  [8/42], [94mLoss[0m : 2.39491
[1mStep[0m  [12/42], [94mLoss[0m : 2.19950
[1mStep[0m  [16/42], [94mLoss[0m : 2.50040
[1mStep[0m  [20/42], [94mLoss[0m : 2.25510
[1mStep[0m  [24/42], [94mLoss[0m : 2.41693
[1mStep[0m  [28/42], [94mLoss[0m : 2.32882
[1mStep[0m  [32/42], [94mLoss[0m : 2.16110
[1mStep[0m  [36/42], [94mLoss[0m : 2.35005
[1mStep[0m  [40/42], [94mLoss[0m : 2.28838

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.315, [92mTest[0m: 2.398, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31881
[1mStep[0m  [4/42], [94mLoss[0m : 2.38828
[1mStep[0m  [8/42], [94mLoss[0m : 2.38347
[1mStep[0m  [12/42], [94mLoss[0m : 2.32527
[1mStep[0m  [16/42], [94mLoss[0m : 2.57125
[1mStep[0m  [20/42], [94mLoss[0m : 2.27021
[1mStep[0m  [24/42], [94mLoss[0m : 2.15906
[1mStep[0m  [28/42], [94mLoss[0m : 2.51160
[1mStep[0m  [32/42], [94mLoss[0m : 2.40334
[1mStep[0m  [36/42], [94mLoss[0m : 2.15276
[1mStep[0m  [40/42], [94mLoss[0m : 2.20174

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.295, [92mTest[0m: 2.392, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.17952
[1mStep[0m  [4/42], [94mLoss[0m : 2.20990
[1mStep[0m  [8/42], [94mLoss[0m : 2.56407
[1mStep[0m  [12/42], [94mLoss[0m : 2.09541
[1mStep[0m  [16/42], [94mLoss[0m : 2.44794
[1mStep[0m  [20/42], [94mLoss[0m : 2.07256
[1mStep[0m  [24/42], [94mLoss[0m : 2.21684
[1mStep[0m  [28/42], [94mLoss[0m : 2.22207
[1mStep[0m  [32/42], [94mLoss[0m : 2.32159
[1mStep[0m  [36/42], [94mLoss[0m : 2.24813
[1mStep[0m  [40/42], [94mLoss[0m : 2.24251

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.290, [92mTest[0m: 2.354, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42181
[1mStep[0m  [4/42], [94mLoss[0m : 2.18061
[1mStep[0m  [8/42], [94mLoss[0m : 2.10329
[1mStep[0m  [12/42], [94mLoss[0m : 2.34105
[1mStep[0m  [16/42], [94mLoss[0m : 2.41209
[1mStep[0m  [20/42], [94mLoss[0m : 2.27713
[1mStep[0m  [24/42], [94mLoss[0m : 2.39977
[1mStep[0m  [28/42], [94mLoss[0m : 2.24618
[1mStep[0m  [32/42], [94mLoss[0m : 2.34806
[1mStep[0m  [36/42], [94mLoss[0m : 2.66811
[1mStep[0m  [40/42], [94mLoss[0m : 2.31962

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.270, [92mTest[0m: 2.367, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.18920
[1mStep[0m  [4/42], [94mLoss[0m : 1.99981
[1mStep[0m  [8/42], [94mLoss[0m : 2.37411
[1mStep[0m  [12/42], [94mLoss[0m : 2.32063
[1mStep[0m  [16/42], [94mLoss[0m : 2.16188
[1mStep[0m  [20/42], [94mLoss[0m : 2.20288
[1mStep[0m  [24/42], [94mLoss[0m : 2.30820
[1mStep[0m  [28/42], [94mLoss[0m : 2.30464
[1mStep[0m  [32/42], [94mLoss[0m : 2.24061
[1mStep[0m  [36/42], [94mLoss[0m : 2.33616
[1mStep[0m  [40/42], [94mLoss[0m : 2.03905

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.259, [92mTest[0m: 2.364, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40353
[1mStep[0m  [4/42], [94mLoss[0m : 2.15411
[1mStep[0m  [8/42], [94mLoss[0m : 2.47866
[1mStep[0m  [12/42], [94mLoss[0m : 2.30278
[1mStep[0m  [16/42], [94mLoss[0m : 2.19875
[1mStep[0m  [20/42], [94mLoss[0m : 2.18543
[1mStep[0m  [24/42], [94mLoss[0m : 2.28743
[1mStep[0m  [28/42], [94mLoss[0m : 2.39624
[1mStep[0m  [32/42], [94mLoss[0m : 2.08692
[1mStep[0m  [36/42], [94mLoss[0m : 2.28532
[1mStep[0m  [40/42], [94mLoss[0m : 2.25763

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.249, [92mTest[0m: 2.379, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.16962
[1mStep[0m  [4/42], [94mLoss[0m : 2.23794
[1mStep[0m  [8/42], [94mLoss[0m : 2.00549
[1mStep[0m  [12/42], [94mLoss[0m : 2.11375
[1mStep[0m  [16/42], [94mLoss[0m : 2.20172
[1mStep[0m  [20/42], [94mLoss[0m : 2.30224
[1mStep[0m  [24/42], [94mLoss[0m : 2.17331
[1mStep[0m  [28/42], [94mLoss[0m : 2.37140
[1mStep[0m  [32/42], [94mLoss[0m : 2.39743
[1mStep[0m  [36/42], [94mLoss[0m : 2.09792
[1mStep[0m  [40/42], [94mLoss[0m : 2.52812

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.224, [92mTest[0m: 2.385, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.11108
[1mStep[0m  [4/42], [94mLoss[0m : 2.25167
[1mStep[0m  [8/42], [94mLoss[0m : 2.32287
[1mStep[0m  [12/42], [94mLoss[0m : 2.10470
[1mStep[0m  [16/42], [94mLoss[0m : 2.29090
[1mStep[0m  [20/42], [94mLoss[0m : 2.38633
[1mStep[0m  [24/42], [94mLoss[0m : 2.35326
[1mStep[0m  [28/42], [94mLoss[0m : 2.25498
[1mStep[0m  [32/42], [94mLoss[0m : 2.04976
[1mStep[0m  [36/42], [94mLoss[0m : 2.17332
[1mStep[0m  [40/42], [94mLoss[0m : 2.25985

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.208, [92mTest[0m: 2.391, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.00048
[1mStep[0m  [4/42], [94mLoss[0m : 2.28024
[1mStep[0m  [8/42], [94mLoss[0m : 2.04264
[1mStep[0m  [12/42], [94mLoss[0m : 2.14599
[1mStep[0m  [16/42], [94mLoss[0m : 2.20430
[1mStep[0m  [20/42], [94mLoss[0m : 2.29084
[1mStep[0m  [24/42], [94mLoss[0m : 2.18210
[1mStep[0m  [28/42], [94mLoss[0m : 2.27627
[1mStep[0m  [32/42], [94mLoss[0m : 2.05308
[1mStep[0m  [36/42], [94mLoss[0m : 2.30018
[1mStep[0m  [40/42], [94mLoss[0m : 2.32247

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.179, [92mTest[0m: 2.383, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.22788
[1mStep[0m  [4/42], [94mLoss[0m : 2.13985
[1mStep[0m  [8/42], [94mLoss[0m : 2.11302
[1mStep[0m  [12/42], [94mLoss[0m : 2.04248
[1mStep[0m  [16/42], [94mLoss[0m : 2.06552
[1mStep[0m  [20/42], [94mLoss[0m : 2.04802
[1mStep[0m  [24/42], [94mLoss[0m : 2.00018
[1mStep[0m  [28/42], [94mLoss[0m : 1.99740
[1mStep[0m  [32/42], [94mLoss[0m : 2.06001
[1mStep[0m  [36/42], [94mLoss[0m : 2.18554
[1mStep[0m  [40/42], [94mLoss[0m : 2.40211

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.157, [92mTest[0m: 2.390, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.17040
[1mStep[0m  [4/42], [94mLoss[0m : 2.12665
[1mStep[0m  [8/42], [94mLoss[0m : 1.79866
[1mStep[0m  [12/42], [94mLoss[0m : 2.34324
[1mStep[0m  [16/42], [94mLoss[0m : 2.21846
[1mStep[0m  [20/42], [94mLoss[0m : 2.19177
[1mStep[0m  [24/42], [94mLoss[0m : 2.28204
[1mStep[0m  [28/42], [94mLoss[0m : 2.28067
[1mStep[0m  [32/42], [94mLoss[0m : 2.19678
[1mStep[0m  [36/42], [94mLoss[0m : 2.14757
[1mStep[0m  [40/42], [94mLoss[0m : 2.25639

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.142, [92mTest[0m: 2.409, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.15949
[1mStep[0m  [4/42], [94mLoss[0m : 2.13305
[1mStep[0m  [8/42], [94mLoss[0m : 2.14958
[1mStep[0m  [12/42], [94mLoss[0m : 2.18303
[1mStep[0m  [16/42], [94mLoss[0m : 2.35456
[1mStep[0m  [20/42], [94mLoss[0m : 2.07874
[1mStep[0m  [24/42], [94mLoss[0m : 2.10481
[1mStep[0m  [28/42], [94mLoss[0m : 2.27932
[1mStep[0m  [32/42], [94mLoss[0m : 2.01644
[1mStep[0m  [36/42], [94mLoss[0m : 2.14066
[1mStep[0m  [40/42], [94mLoss[0m : 2.09457

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.134, [92mTest[0m: 2.385, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.16672
[1mStep[0m  [4/42], [94mLoss[0m : 2.00079
[1mStep[0m  [8/42], [94mLoss[0m : 1.95085
[1mStep[0m  [12/42], [94mLoss[0m : 2.26851
[1mStep[0m  [16/42], [94mLoss[0m : 1.89279
[1mStep[0m  [20/42], [94mLoss[0m : 2.19336
[1mStep[0m  [24/42], [94mLoss[0m : 2.15806
[1mStep[0m  [28/42], [94mLoss[0m : 1.95665
[1mStep[0m  [32/42], [94mLoss[0m : 2.09538
[1mStep[0m  [36/42], [94mLoss[0m : 2.00160
[1mStep[0m  [40/42], [94mLoss[0m : 2.27795

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.088, [92mTest[0m: 2.426, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.15656
[1mStep[0m  [4/42], [94mLoss[0m : 2.11173
[1mStep[0m  [8/42], [94mLoss[0m : 2.08660
[1mStep[0m  [12/42], [94mLoss[0m : 1.99645
[1mStep[0m  [16/42], [94mLoss[0m : 2.05973
[1mStep[0m  [20/42], [94mLoss[0m : 2.07696
[1mStep[0m  [24/42], [94mLoss[0m : 1.98127
[1mStep[0m  [28/42], [94mLoss[0m : 2.01270
[1mStep[0m  [32/42], [94mLoss[0m : 2.19836
[1mStep[0m  [36/42], [94mLoss[0m : 2.11568
[1mStep[0m  [40/42], [94mLoss[0m : 2.18656

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.089, [92mTest[0m: 2.400, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.01811
[1mStep[0m  [4/42], [94mLoss[0m : 2.20555
[1mStep[0m  [8/42], [94mLoss[0m : 2.03291
[1mStep[0m  [12/42], [94mLoss[0m : 1.87469
[1mStep[0m  [16/42], [94mLoss[0m : 2.31739
[1mStep[0m  [20/42], [94mLoss[0m : 2.14672
[1mStep[0m  [24/42], [94mLoss[0m : 1.95418
[1mStep[0m  [28/42], [94mLoss[0m : 2.13712
[1mStep[0m  [32/42], [94mLoss[0m : 1.93669
[1mStep[0m  [36/42], [94mLoss[0m : 1.81058
[1mStep[0m  [40/42], [94mLoss[0m : 2.05637

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.064, [92mTest[0m: 2.394, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.03579
[1mStep[0m  [4/42], [94mLoss[0m : 1.93434
[1mStep[0m  [8/42], [94mLoss[0m : 1.94416
[1mStep[0m  [12/42], [94mLoss[0m : 2.07464
[1mStep[0m  [16/42], [94mLoss[0m : 1.98104
[1mStep[0m  [20/42], [94mLoss[0m : 2.02120
[1mStep[0m  [24/42], [94mLoss[0m : 2.11773
[1mStep[0m  [28/42], [94mLoss[0m : 1.96612
[1mStep[0m  [32/42], [94mLoss[0m : 2.07064
[1mStep[0m  [36/42], [94mLoss[0m : 2.00408
[1mStep[0m  [40/42], [94mLoss[0m : 2.06452

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.029, [92mTest[0m: 2.382, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.06543
[1mStep[0m  [4/42], [94mLoss[0m : 1.90246
[1mStep[0m  [8/42], [94mLoss[0m : 2.04568
[1mStep[0m  [12/42], [94mLoss[0m : 2.05067
[1mStep[0m  [16/42], [94mLoss[0m : 2.10822
[1mStep[0m  [20/42], [94mLoss[0m : 1.99407
[1mStep[0m  [24/42], [94mLoss[0m : 2.01882
[1mStep[0m  [28/42], [94mLoss[0m : 1.99341
[1mStep[0m  [32/42], [94mLoss[0m : 1.86496
[1mStep[0m  [36/42], [94mLoss[0m : 1.85096
[1mStep[0m  [40/42], [94mLoss[0m : 2.08255

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.996, [92mTest[0m: 2.381, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.03325
[1mStep[0m  [4/42], [94mLoss[0m : 1.92932
[1mStep[0m  [8/42], [94mLoss[0m : 1.94372
[1mStep[0m  [12/42], [94mLoss[0m : 1.99068
[1mStep[0m  [16/42], [94mLoss[0m : 2.11981
[1mStep[0m  [20/42], [94mLoss[0m : 1.93402
[1mStep[0m  [24/42], [94mLoss[0m : 2.10847
[1mStep[0m  [28/42], [94mLoss[0m : 2.10968
[1mStep[0m  [32/42], [94mLoss[0m : 2.17621
[1mStep[0m  [36/42], [94mLoss[0m : 1.84991
[1mStep[0m  [40/42], [94mLoss[0m : 1.94341

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.991, [92mTest[0m: 2.423, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.94373
[1mStep[0m  [4/42], [94mLoss[0m : 2.10701
[1mStep[0m  [8/42], [94mLoss[0m : 2.13422
[1mStep[0m  [12/42], [94mLoss[0m : 2.00594
[1mStep[0m  [16/42], [94mLoss[0m : 1.87134
[1mStep[0m  [20/42], [94mLoss[0m : 1.99697
[1mStep[0m  [24/42], [94mLoss[0m : 2.05240
[1mStep[0m  [28/42], [94mLoss[0m : 2.03550
[1mStep[0m  [32/42], [94mLoss[0m : 1.81252
[1mStep[0m  [36/42], [94mLoss[0m : 1.90716
[1mStep[0m  [40/42], [94mLoss[0m : 1.97016

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.969, [92mTest[0m: 2.413, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.93835
[1mStep[0m  [4/42], [94mLoss[0m : 2.00824
[1mStep[0m  [8/42], [94mLoss[0m : 1.91732
[1mStep[0m  [12/42], [94mLoss[0m : 1.93173
[1mStep[0m  [16/42], [94mLoss[0m : 1.90741
[1mStep[0m  [20/42], [94mLoss[0m : 1.84002
[1mStep[0m  [24/42], [94mLoss[0m : 1.89358
[1mStep[0m  [28/42], [94mLoss[0m : 1.97498
[1mStep[0m  [32/42], [94mLoss[0m : 1.71890
[1mStep[0m  [36/42], [94mLoss[0m : 1.92729
[1mStep[0m  [40/42], [94mLoss[0m : 1.99716

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.953, [92mTest[0m: 2.429, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.75816
[1mStep[0m  [4/42], [94mLoss[0m : 1.98624
[1mStep[0m  [8/42], [94mLoss[0m : 2.00305
[1mStep[0m  [12/42], [94mLoss[0m : 2.03778
[1mStep[0m  [16/42], [94mLoss[0m : 1.74057
[1mStep[0m  [20/42], [94mLoss[0m : 2.01846
[1mStep[0m  [24/42], [94mLoss[0m : 1.97273
[1mStep[0m  [28/42], [94mLoss[0m : 1.92755
[1mStep[0m  [32/42], [94mLoss[0m : 1.90435
[1mStep[0m  [36/42], [94mLoss[0m : 1.93866
[1mStep[0m  [40/42], [94mLoss[0m : 1.95033

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.922, [92mTest[0m: 2.394, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.390
====================================

Phase 2 - Evaluation MAE:  2.390326142311096
MAE score P1      2.345006
MAE score P2      2.390326
loss              1.921797
learning_rate     0.002575
batch_size             256
hidden_sizes         [250]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.5
weight_decay          0.01
Name: 2, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 10.65937
[1mStep[0m  [4/42], [94mLoss[0m : 10.97523
[1mStep[0m  [8/42], [94mLoss[0m : 10.72761
[1mStep[0m  [12/42], [94mLoss[0m : 10.97655
[1mStep[0m  [16/42], [94mLoss[0m : 11.04254
[1mStep[0m  [20/42], [94mLoss[0m : 10.74115
[1mStep[0m  [24/42], [94mLoss[0m : 10.77935
[1mStep[0m  [28/42], [94mLoss[0m : 10.93168
[1mStep[0m  [32/42], [94mLoss[0m : 10.83646
[1mStep[0m  [36/42], [94mLoss[0m : 10.54047
[1mStep[0m  [40/42], [94mLoss[0m : 11.39068

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.834, [92mTest[0m: 11.004, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.51196
[1mStep[0m  [4/42], [94mLoss[0m : 10.71943
[1mStep[0m  [8/42], [94mLoss[0m : 11.08361
[1mStep[0m  [12/42], [94mLoss[0m : 10.49036
[1mStep[0m  [16/42], [94mLoss[0m : 10.85986
[1mStep[0m  [20/42], [94mLoss[0m : 10.75714
[1mStep[0m  [24/42], [94mLoss[0m : 10.83644
[1mStep[0m  [28/42], [94mLoss[0m : 10.42459
[1mStep[0m  [32/42], [94mLoss[0m : 10.54409
[1mStep[0m  [36/42], [94mLoss[0m : 10.80506
[1mStep[0m  [40/42], [94mLoss[0m : 10.72261

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.680, [92mTest[0m: 10.727, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.52643
[1mStep[0m  [4/42], [94mLoss[0m : 10.76624
[1mStep[0m  [8/42], [94mLoss[0m : 10.62220
[1mStep[0m  [12/42], [94mLoss[0m : 10.61829
[1mStep[0m  [16/42], [94mLoss[0m : 10.08253
[1mStep[0m  [20/42], [94mLoss[0m : 10.52405
[1mStep[0m  [24/42], [94mLoss[0m : 10.68198
[1mStep[0m  [28/42], [94mLoss[0m : 10.42691
[1mStep[0m  [32/42], [94mLoss[0m : 10.26902
[1mStep[0m  [36/42], [94mLoss[0m : 10.18038
[1mStep[0m  [40/42], [94mLoss[0m : 10.59139

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.537, [92mTest[0m: 10.539, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.46832
[1mStep[0m  [4/42], [94mLoss[0m : 10.14031
[1mStep[0m  [8/42], [94mLoss[0m : 10.28800
[1mStep[0m  [12/42], [94mLoss[0m : 10.70937
[1mStep[0m  [16/42], [94mLoss[0m : 10.52025
[1mStep[0m  [20/42], [94mLoss[0m : 10.58592
[1mStep[0m  [24/42], [94mLoss[0m : 10.47082
[1mStep[0m  [28/42], [94mLoss[0m : 10.34895
[1mStep[0m  [32/42], [94mLoss[0m : 9.92769
[1mStep[0m  [36/42], [94mLoss[0m : 10.39618
[1mStep[0m  [40/42], [94mLoss[0m : 9.74117

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.406, [92mTest[0m: 10.380, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.05472
[1mStep[0m  [4/42], [94mLoss[0m : 10.08691
[1mStep[0m  [8/42], [94mLoss[0m : 10.71785
[1mStep[0m  [12/42], [94mLoss[0m : 10.25096
[1mStep[0m  [16/42], [94mLoss[0m : 10.18417
[1mStep[0m  [20/42], [94mLoss[0m : 10.37682
[1mStep[0m  [24/42], [94mLoss[0m : 10.35868
[1mStep[0m  [28/42], [94mLoss[0m : 10.43723
[1mStep[0m  [32/42], [94mLoss[0m : 10.43050
[1mStep[0m  [36/42], [94mLoss[0m : 10.21947
[1mStep[0m  [40/42], [94mLoss[0m : 10.33984

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.249, [92mTest[0m: 10.195, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.44043
[1mStep[0m  [4/42], [94mLoss[0m : 10.12652
[1mStep[0m  [8/42], [94mLoss[0m : 10.16142
[1mStep[0m  [12/42], [94mLoss[0m : 10.17869
[1mStep[0m  [16/42], [94mLoss[0m : 10.12895
[1mStep[0m  [20/42], [94mLoss[0m : 10.24020
[1mStep[0m  [24/42], [94mLoss[0m : 9.90666
[1mStep[0m  [28/42], [94mLoss[0m : 10.70942
[1mStep[0m  [32/42], [94mLoss[0m : 9.83674
[1mStep[0m  [36/42], [94mLoss[0m : 9.90849
[1mStep[0m  [40/42], [94mLoss[0m : 9.93250

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.097, [92mTest[0m: 10.033, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.30087
[1mStep[0m  [4/42], [94mLoss[0m : 9.95630
[1mStep[0m  [8/42], [94mLoss[0m : 9.76754
[1mStep[0m  [12/42], [94mLoss[0m : 9.73410
[1mStep[0m  [16/42], [94mLoss[0m : 9.80151
[1mStep[0m  [20/42], [94mLoss[0m : 9.69136
[1mStep[0m  [24/42], [94mLoss[0m : 9.79219
[1mStep[0m  [28/42], [94mLoss[0m : 10.05851
[1mStep[0m  [32/42], [94mLoss[0m : 10.14773
[1mStep[0m  [36/42], [94mLoss[0m : 9.85078
[1mStep[0m  [40/42], [94mLoss[0m : 9.97549

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 9.950, [92mTest[0m: 9.824, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.70831
[1mStep[0m  [4/42], [94mLoss[0m : 9.87413
[1mStep[0m  [8/42], [94mLoss[0m : 9.85963
[1mStep[0m  [12/42], [94mLoss[0m : 9.82441
[1mStep[0m  [16/42], [94mLoss[0m : 9.66911
[1mStep[0m  [20/42], [94mLoss[0m : 10.16197
[1mStep[0m  [24/42], [94mLoss[0m : 9.89200
[1mStep[0m  [28/42], [94mLoss[0m : 10.24429
[1mStep[0m  [32/42], [94mLoss[0m : 9.86098
[1mStep[0m  [36/42], [94mLoss[0m : 9.87816
[1mStep[0m  [40/42], [94mLoss[0m : 9.56490

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 9.790, [92mTest[0m: 9.629, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.79712
[1mStep[0m  [4/42], [94mLoss[0m : 9.82374
[1mStep[0m  [8/42], [94mLoss[0m : 9.59345
[1mStep[0m  [12/42], [94mLoss[0m : 9.79140
[1mStep[0m  [16/42], [94mLoss[0m : 9.81336
[1mStep[0m  [20/42], [94mLoss[0m : 9.53016
[1mStep[0m  [24/42], [94mLoss[0m : 9.57063
[1mStep[0m  [28/42], [94mLoss[0m : 9.75639
[1mStep[0m  [32/42], [94mLoss[0m : 9.48900
[1mStep[0m  [36/42], [94mLoss[0m : 9.99698
[1mStep[0m  [40/42], [94mLoss[0m : 9.67615

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 9.628, [92mTest[0m: 9.421, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.65887
[1mStep[0m  [4/42], [94mLoss[0m : 9.93486
[1mStep[0m  [8/42], [94mLoss[0m : 9.37959
[1mStep[0m  [12/42], [94mLoss[0m : 9.75911
[1mStep[0m  [16/42], [94mLoss[0m : 9.82308
[1mStep[0m  [20/42], [94mLoss[0m : 9.77432
[1mStep[0m  [24/42], [94mLoss[0m : 9.57304
[1mStep[0m  [28/42], [94mLoss[0m : 9.37619
[1mStep[0m  [32/42], [94mLoss[0m : 9.66998
[1mStep[0m  [36/42], [94mLoss[0m : 9.30630
[1mStep[0m  [40/42], [94mLoss[0m : 9.26418

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 9.452, [92mTest[0m: 9.203, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.11264
[1mStep[0m  [4/42], [94mLoss[0m : 9.45582
[1mStep[0m  [8/42], [94mLoss[0m : 9.24404
[1mStep[0m  [12/42], [94mLoss[0m : 9.38661
[1mStep[0m  [16/42], [94mLoss[0m : 9.04132
[1mStep[0m  [20/42], [94mLoss[0m : 9.38215
[1mStep[0m  [24/42], [94mLoss[0m : 8.83300
[1mStep[0m  [28/42], [94mLoss[0m : 9.26153
[1mStep[0m  [32/42], [94mLoss[0m : 9.46114
[1mStep[0m  [36/42], [94mLoss[0m : 8.85732
[1mStep[0m  [40/42], [94mLoss[0m : 9.79452

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 9.265, [92mTest[0m: 8.983, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.26509
[1mStep[0m  [4/42], [94mLoss[0m : 8.98559
[1mStep[0m  [8/42], [94mLoss[0m : 9.55796
[1mStep[0m  [12/42], [94mLoss[0m : 9.33241
[1mStep[0m  [16/42], [94mLoss[0m : 9.14725
[1mStep[0m  [20/42], [94mLoss[0m : 8.89640
[1mStep[0m  [24/42], [94mLoss[0m : 8.90183
[1mStep[0m  [28/42], [94mLoss[0m : 8.98266
[1mStep[0m  [32/42], [94mLoss[0m : 8.74254
[1mStep[0m  [36/42], [94mLoss[0m : 8.97368
[1mStep[0m  [40/42], [94mLoss[0m : 9.22514

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 9.067, [92mTest[0m: 8.758, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.29183
[1mStep[0m  [4/42], [94mLoss[0m : 9.04724
[1mStep[0m  [8/42], [94mLoss[0m : 8.92762
[1mStep[0m  [12/42], [94mLoss[0m : 9.17189
[1mStep[0m  [16/42], [94mLoss[0m : 9.31259
[1mStep[0m  [20/42], [94mLoss[0m : 8.65750
[1mStep[0m  [24/42], [94mLoss[0m : 8.65557
[1mStep[0m  [28/42], [94mLoss[0m : 8.77411
[1mStep[0m  [32/42], [94mLoss[0m : 8.93360
[1mStep[0m  [36/42], [94mLoss[0m : 8.63170
[1mStep[0m  [40/42], [94mLoss[0m : 8.37086

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 8.857, [92mTest[0m: 8.528, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.52675
[1mStep[0m  [4/42], [94mLoss[0m : 8.70902
[1mStep[0m  [8/42], [94mLoss[0m : 8.63169
[1mStep[0m  [12/42], [94mLoss[0m : 8.84909
[1mStep[0m  [16/42], [94mLoss[0m : 8.62011
[1mStep[0m  [20/42], [94mLoss[0m : 8.97832
[1mStep[0m  [24/42], [94mLoss[0m : 8.61166
[1mStep[0m  [28/42], [94mLoss[0m : 8.71773
[1mStep[0m  [32/42], [94mLoss[0m : 8.50164
[1mStep[0m  [36/42], [94mLoss[0m : 8.14274
[1mStep[0m  [40/42], [94mLoss[0m : 8.24683

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 8.621, [92mTest[0m: 8.267, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.11965
[1mStep[0m  [4/42], [94mLoss[0m : 8.32331
[1mStep[0m  [8/42], [94mLoss[0m : 8.67817
[1mStep[0m  [12/42], [94mLoss[0m : 8.54750
[1mStep[0m  [16/42], [94mLoss[0m : 8.59122
[1mStep[0m  [20/42], [94mLoss[0m : 8.23613
[1mStep[0m  [24/42], [94mLoss[0m : 8.66264
[1mStep[0m  [28/42], [94mLoss[0m : 8.60590
[1mStep[0m  [32/42], [94mLoss[0m : 8.36975
[1mStep[0m  [36/42], [94mLoss[0m : 8.40231
[1mStep[0m  [40/42], [94mLoss[0m : 8.22245

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 8.383, [92mTest[0m: 7.903, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.63565
[1mStep[0m  [4/42], [94mLoss[0m : 8.32578
[1mStep[0m  [8/42], [94mLoss[0m : 8.04858
[1mStep[0m  [12/42], [94mLoss[0m : 8.46810
[1mStep[0m  [16/42], [94mLoss[0m : 8.39350
[1mStep[0m  [20/42], [94mLoss[0m : 8.29411
[1mStep[0m  [24/42], [94mLoss[0m : 7.94018
[1mStep[0m  [28/42], [94mLoss[0m : 7.95708
[1mStep[0m  [32/42], [94mLoss[0m : 8.08108
[1mStep[0m  [36/42], [94mLoss[0m : 8.12923
[1mStep[0m  [40/42], [94mLoss[0m : 8.14948

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 8.121, [92mTest[0m: 7.606, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.36993
[1mStep[0m  [4/42], [94mLoss[0m : 8.31164
[1mStep[0m  [8/42], [94mLoss[0m : 7.98522
[1mStep[0m  [12/42], [94mLoss[0m : 7.91566
[1mStep[0m  [16/42], [94mLoss[0m : 8.22364
[1mStep[0m  [20/42], [94mLoss[0m : 7.73297
[1mStep[0m  [24/42], [94mLoss[0m : 8.09443
[1mStep[0m  [28/42], [94mLoss[0m : 7.58462
[1mStep[0m  [32/42], [94mLoss[0m : 7.63714
[1mStep[0m  [36/42], [94mLoss[0m : 7.73543
[1mStep[0m  [40/42], [94mLoss[0m : 7.54781

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 7.812, [92mTest[0m: 7.325, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.61745
[1mStep[0m  [4/42], [94mLoss[0m : 7.35597
[1mStep[0m  [8/42], [94mLoss[0m : 7.62364
[1mStep[0m  [12/42], [94mLoss[0m : 7.85646
[1mStep[0m  [16/42], [94mLoss[0m : 7.57807
[1mStep[0m  [20/42], [94mLoss[0m : 7.83217
[1mStep[0m  [24/42], [94mLoss[0m : 7.18641
[1mStep[0m  [28/42], [94mLoss[0m : 7.35179
[1mStep[0m  [32/42], [94mLoss[0m : 7.35540
[1mStep[0m  [36/42], [94mLoss[0m : 7.88536
[1mStep[0m  [40/42], [94mLoss[0m : 7.38229

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 7.514, [92mTest[0m: 6.871, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.27644
[1mStep[0m  [4/42], [94mLoss[0m : 7.43137
[1mStep[0m  [8/42], [94mLoss[0m : 7.12347
[1mStep[0m  [12/42], [94mLoss[0m : 7.43871
[1mStep[0m  [16/42], [94mLoss[0m : 7.29313
[1mStep[0m  [20/42], [94mLoss[0m : 7.50063
[1mStep[0m  [24/42], [94mLoss[0m : 7.30856
[1mStep[0m  [28/42], [94mLoss[0m : 7.40110
[1mStep[0m  [32/42], [94mLoss[0m : 6.96757
[1mStep[0m  [36/42], [94mLoss[0m : 6.95009
[1mStep[0m  [40/42], [94mLoss[0m : 7.44094

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 7.207, [92mTest[0m: 6.574, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.41061
[1mStep[0m  [4/42], [94mLoss[0m : 6.90525
[1mStep[0m  [8/42], [94mLoss[0m : 6.93999
[1mStep[0m  [12/42], [94mLoss[0m : 6.94203
[1mStep[0m  [16/42], [94mLoss[0m : 6.73055
[1mStep[0m  [20/42], [94mLoss[0m : 6.63323
[1mStep[0m  [24/42], [94mLoss[0m : 6.79279
[1mStep[0m  [28/42], [94mLoss[0m : 6.93841
[1mStep[0m  [32/42], [94mLoss[0m : 6.79181
[1mStep[0m  [36/42], [94mLoss[0m : 6.66037
[1mStep[0m  [40/42], [94mLoss[0m : 7.28248

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 6.907, [92mTest[0m: 6.289, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.87695
[1mStep[0m  [4/42], [94mLoss[0m : 6.58980
[1mStep[0m  [8/42], [94mLoss[0m : 6.59647
[1mStep[0m  [12/42], [94mLoss[0m : 6.65960
[1mStep[0m  [16/42], [94mLoss[0m : 7.03748
[1mStep[0m  [20/42], [94mLoss[0m : 6.45916
[1mStep[0m  [24/42], [94mLoss[0m : 6.59190
[1mStep[0m  [28/42], [94mLoss[0m : 6.22768
[1mStep[0m  [32/42], [94mLoss[0m : 6.71012
[1mStep[0m  [36/42], [94mLoss[0m : 6.68469
[1mStep[0m  [40/42], [94mLoss[0m : 6.78659

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 6.624, [92mTest[0m: 5.901, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.35092
[1mStep[0m  [4/42], [94mLoss[0m : 6.95496
[1mStep[0m  [8/42], [94mLoss[0m : 6.25691
[1mStep[0m  [12/42], [94mLoss[0m : 6.56930
[1mStep[0m  [16/42], [94mLoss[0m : 6.26127
[1mStep[0m  [20/42], [94mLoss[0m : 6.31219
[1mStep[0m  [24/42], [94mLoss[0m : 6.22163
[1mStep[0m  [28/42], [94mLoss[0m : 6.28663
[1mStep[0m  [32/42], [94mLoss[0m : 6.17329
[1mStep[0m  [36/42], [94mLoss[0m : 6.25639
[1mStep[0m  [40/42], [94mLoss[0m : 5.95848

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 6.380, [92mTest[0m: 5.691, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.41271
[1mStep[0m  [4/42], [94mLoss[0m : 6.30232
[1mStep[0m  [8/42], [94mLoss[0m : 6.32880
[1mStep[0m  [12/42], [94mLoss[0m : 6.13064
[1mStep[0m  [16/42], [94mLoss[0m : 6.19579
[1mStep[0m  [20/42], [94mLoss[0m : 6.08606
[1mStep[0m  [24/42], [94mLoss[0m : 6.03778
[1mStep[0m  [28/42], [94mLoss[0m : 6.11163
[1mStep[0m  [32/42], [94mLoss[0m : 5.87884
[1mStep[0m  [36/42], [94mLoss[0m : 6.07286
[1mStep[0m  [40/42], [94mLoss[0m : 6.23467

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 6.127, [92mTest[0m: 5.430, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.00800
[1mStep[0m  [4/42], [94mLoss[0m : 5.99466
[1mStep[0m  [8/42], [94mLoss[0m : 5.99733
[1mStep[0m  [12/42], [94mLoss[0m : 5.71847
[1mStep[0m  [16/42], [94mLoss[0m : 5.88568
[1mStep[0m  [20/42], [94mLoss[0m : 5.82950
[1mStep[0m  [24/42], [94mLoss[0m : 5.91137
[1mStep[0m  [28/42], [94mLoss[0m : 6.21073
[1mStep[0m  [32/42], [94mLoss[0m : 5.74193
[1mStep[0m  [36/42], [94mLoss[0m : 5.73437
[1mStep[0m  [40/42], [94mLoss[0m : 5.92787

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 5.865, [92mTest[0m: 5.164, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.76195
[1mStep[0m  [4/42], [94mLoss[0m : 5.70163
[1mStep[0m  [8/42], [94mLoss[0m : 5.69082
[1mStep[0m  [12/42], [94mLoss[0m : 5.54917
[1mStep[0m  [16/42], [94mLoss[0m : 5.79282
[1mStep[0m  [20/42], [94mLoss[0m : 5.59565
[1mStep[0m  [24/42], [94mLoss[0m : 5.94457
[1mStep[0m  [28/42], [94mLoss[0m : 5.34456
[1mStep[0m  [32/42], [94mLoss[0m : 5.24710
[1mStep[0m  [36/42], [94mLoss[0m : 5.94486
[1mStep[0m  [40/42], [94mLoss[0m : 5.55020

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 5.611, [92mTest[0m: 4.948, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.43220
[1mStep[0m  [4/42], [94mLoss[0m : 5.52614
[1mStep[0m  [8/42], [94mLoss[0m : 5.50411
[1mStep[0m  [12/42], [94mLoss[0m : 5.28976
[1mStep[0m  [16/42], [94mLoss[0m : 5.35001
[1mStep[0m  [20/42], [94mLoss[0m : 5.18792
[1mStep[0m  [24/42], [94mLoss[0m : 5.59774
[1mStep[0m  [28/42], [94mLoss[0m : 5.21333
[1mStep[0m  [32/42], [94mLoss[0m : 5.35303
[1mStep[0m  [36/42], [94mLoss[0m : 5.02967
[1mStep[0m  [40/42], [94mLoss[0m : 5.25190

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 5.357, [92mTest[0m: 4.662, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.10849
[1mStep[0m  [4/42], [94mLoss[0m : 5.51978
[1mStep[0m  [8/42], [94mLoss[0m : 4.74130
[1mStep[0m  [12/42], [94mLoss[0m : 5.10691
[1mStep[0m  [16/42], [94mLoss[0m : 5.04924
[1mStep[0m  [20/42], [94mLoss[0m : 5.40952
[1mStep[0m  [24/42], [94mLoss[0m : 5.37046
[1mStep[0m  [28/42], [94mLoss[0m : 5.29047
[1mStep[0m  [32/42], [94mLoss[0m : 5.17532
[1mStep[0m  [36/42], [94mLoss[0m : 4.99921
[1mStep[0m  [40/42], [94mLoss[0m : 5.35968

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 5.111, [92mTest[0m: 4.385, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.21441
[1mStep[0m  [4/42], [94mLoss[0m : 4.98739
[1mStep[0m  [8/42], [94mLoss[0m : 5.10335
[1mStep[0m  [12/42], [94mLoss[0m : 4.91301
[1mStep[0m  [16/42], [94mLoss[0m : 4.53959
[1mStep[0m  [20/42], [94mLoss[0m : 4.83540
[1mStep[0m  [24/42], [94mLoss[0m : 5.03312
[1mStep[0m  [28/42], [94mLoss[0m : 4.44142
[1mStep[0m  [32/42], [94mLoss[0m : 4.78749
[1mStep[0m  [36/42], [94mLoss[0m : 5.04259
[1mStep[0m  [40/42], [94mLoss[0m : 4.72344

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 4.854, [92mTest[0m: 4.210, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.50797
[1mStep[0m  [4/42], [94mLoss[0m : 4.59530
[1mStep[0m  [8/42], [94mLoss[0m : 4.72558
[1mStep[0m  [12/42], [94mLoss[0m : 4.91951
[1mStep[0m  [16/42], [94mLoss[0m : 4.72675
[1mStep[0m  [20/42], [94mLoss[0m : 4.36053
[1mStep[0m  [24/42], [94mLoss[0m : 4.54881
[1mStep[0m  [28/42], [94mLoss[0m : 4.35928
[1mStep[0m  [32/42], [94mLoss[0m : 4.49667
[1mStep[0m  [36/42], [94mLoss[0m : 4.70438
[1mStep[0m  [40/42], [94mLoss[0m : 4.69234

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 4.588, [92mTest[0m: 3.975, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.68203
[1mStep[0m  [4/42], [94mLoss[0m : 4.17253
[1mStep[0m  [8/42], [94mLoss[0m : 4.17784
[1mStep[0m  [12/42], [94mLoss[0m : 4.45995
[1mStep[0m  [16/42], [94mLoss[0m : 4.21650
[1mStep[0m  [20/42], [94mLoss[0m : 4.26778
[1mStep[0m  [24/42], [94mLoss[0m : 3.95936
[1mStep[0m  [28/42], [94mLoss[0m : 4.11241
[1mStep[0m  [32/42], [94mLoss[0m : 4.34710
[1mStep[0m  [36/42], [94mLoss[0m : 4.25921
[1mStep[0m  [40/42], [94mLoss[0m : 4.49948

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 4.320, [92mTest[0m: 3.732, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 3.519
====================================

Phase 1 - Evaluation MAE:  3.5190359694617137
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 4.37278
[1mStep[0m  [4/42], [94mLoss[0m : 4.31470
[1mStep[0m  [8/42], [94mLoss[0m : 4.16201
[1mStep[0m  [12/42], [94mLoss[0m : 4.09709
[1mStep[0m  [16/42], [94mLoss[0m : 4.38337
[1mStep[0m  [20/42], [94mLoss[0m : 3.71835
[1mStep[0m  [24/42], [94mLoss[0m : 4.27933
[1mStep[0m  [28/42], [94mLoss[0m : 3.62584
[1mStep[0m  [32/42], [94mLoss[0m : 3.85006
[1mStep[0m  [36/42], [94mLoss[0m : 4.01854
[1mStep[0m  [40/42], [94mLoss[0m : 3.65006

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.086, [92mTest[0m: 3.514, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.86544
[1mStep[0m  [4/42], [94mLoss[0m : 3.64700
[1mStep[0m  [8/42], [94mLoss[0m : 4.01501
[1mStep[0m  [12/42], [94mLoss[0m : 3.73208
[1mStep[0m  [16/42], [94mLoss[0m : 4.02135
[1mStep[0m  [20/42], [94mLoss[0m : 3.91279
[1mStep[0m  [24/42], [94mLoss[0m : 3.65561
[1mStep[0m  [28/42], [94mLoss[0m : 3.55102
[1mStep[0m  [32/42], [94mLoss[0m : 3.66854
[1mStep[0m  [36/42], [94mLoss[0m : 3.74402
[1mStep[0m  [40/42], [94mLoss[0m : 3.80326

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.810, [92mTest[0m: 3.340, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.47165
[1mStep[0m  [4/42], [94mLoss[0m : 3.45029
[1mStep[0m  [8/42], [94mLoss[0m : 3.63576
[1mStep[0m  [12/42], [94mLoss[0m : 3.35923
[1mStep[0m  [16/42], [94mLoss[0m : 3.39914
[1mStep[0m  [20/42], [94mLoss[0m : 3.63079
[1mStep[0m  [24/42], [94mLoss[0m : 3.28532
[1mStep[0m  [28/42], [94mLoss[0m : 3.66910
[1mStep[0m  [32/42], [94mLoss[0m : 3.28042
[1mStep[0m  [36/42], [94mLoss[0m : 3.32872
[1mStep[0m  [40/42], [94mLoss[0m : 3.72041

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.540, [92mTest[0m: 3.132, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.32219
[1mStep[0m  [4/42], [94mLoss[0m : 3.59082
[1mStep[0m  [8/42], [94mLoss[0m : 3.29882
[1mStep[0m  [12/42], [94mLoss[0m : 3.26516
[1mStep[0m  [16/42], [94mLoss[0m : 3.10832
[1mStep[0m  [20/42], [94mLoss[0m : 2.95253
[1mStep[0m  [24/42], [94mLoss[0m : 3.14398
[1mStep[0m  [28/42], [94mLoss[0m : 3.33189
[1mStep[0m  [32/42], [94mLoss[0m : 3.04864
[1mStep[0m  [36/42], [94mLoss[0m : 3.28477
[1mStep[0m  [40/42], [94mLoss[0m : 3.04927

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 3.248, [92mTest[0m: 2.798, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.13085
[1mStep[0m  [4/42], [94mLoss[0m : 3.33440
[1mStep[0m  [8/42], [94mLoss[0m : 3.24947
[1mStep[0m  [12/42], [94mLoss[0m : 3.09149
[1mStep[0m  [16/42], [94mLoss[0m : 2.98217
[1mStep[0m  [20/42], [94mLoss[0m : 2.95083
[1mStep[0m  [24/42], [94mLoss[0m : 3.11200
[1mStep[0m  [28/42], [94mLoss[0m : 2.97799
[1mStep[0m  [32/42], [94mLoss[0m : 3.48988
[1mStep[0m  [36/42], [94mLoss[0m : 3.08144
[1mStep[0m  [40/42], [94mLoss[0m : 3.10239

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 3.104, [92mTest[0m: 2.652, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.67908
[1mStep[0m  [4/42], [94mLoss[0m : 3.11225
[1mStep[0m  [8/42], [94mLoss[0m : 2.88047
[1mStep[0m  [12/42], [94mLoss[0m : 2.98702
[1mStep[0m  [16/42], [94mLoss[0m : 3.09843
[1mStep[0m  [20/42], [94mLoss[0m : 2.64880
[1mStep[0m  [24/42], [94mLoss[0m : 3.06664
[1mStep[0m  [28/42], [94mLoss[0m : 3.02088
[1mStep[0m  [32/42], [94mLoss[0m : 2.76367
[1mStep[0m  [36/42], [94mLoss[0m : 2.78602
[1mStep[0m  [40/42], [94mLoss[0m : 2.89944

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.984, [92mTest[0m: 2.562, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.66741
[1mStep[0m  [4/42], [94mLoss[0m : 3.01260
[1mStep[0m  [8/42], [94mLoss[0m : 3.07265
[1mStep[0m  [12/42], [94mLoss[0m : 3.09702
[1mStep[0m  [16/42], [94mLoss[0m : 2.81145
[1mStep[0m  [20/42], [94mLoss[0m : 2.89019
[1mStep[0m  [24/42], [94mLoss[0m : 2.85915
[1mStep[0m  [28/42], [94mLoss[0m : 2.76619
[1mStep[0m  [32/42], [94mLoss[0m : 2.81761
[1mStep[0m  [36/42], [94mLoss[0m : 2.77529
[1mStep[0m  [40/42], [94mLoss[0m : 2.64997

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.853, [92mTest[0m: 2.485, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.77738
[1mStep[0m  [4/42], [94mLoss[0m : 2.96281
[1mStep[0m  [8/42], [94mLoss[0m : 2.78223
[1mStep[0m  [12/42], [94mLoss[0m : 2.63020
[1mStep[0m  [16/42], [94mLoss[0m : 2.73436
[1mStep[0m  [20/42], [94mLoss[0m : 2.69189
[1mStep[0m  [24/42], [94mLoss[0m : 2.66715
[1mStep[0m  [28/42], [94mLoss[0m : 2.97252
[1mStep[0m  [32/42], [94mLoss[0m : 2.86245
[1mStep[0m  [36/42], [94mLoss[0m : 2.75448
[1mStep[0m  [40/42], [94mLoss[0m : 2.75073

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.800, [92mTest[0m: 2.425, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.87754
[1mStep[0m  [4/42], [94mLoss[0m : 2.96831
[1mStep[0m  [8/42], [94mLoss[0m : 2.77002
[1mStep[0m  [12/42], [94mLoss[0m : 2.71674
[1mStep[0m  [16/42], [94mLoss[0m : 2.65796
[1mStep[0m  [20/42], [94mLoss[0m : 2.85896
[1mStep[0m  [24/42], [94mLoss[0m : 2.81347
[1mStep[0m  [28/42], [94mLoss[0m : 3.02779
[1mStep[0m  [32/42], [94mLoss[0m : 2.53229
[1mStep[0m  [36/42], [94mLoss[0m : 2.80236
[1mStep[0m  [40/42], [94mLoss[0m : 2.89769

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.748, [92mTest[0m: 2.449, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.78050
[1mStep[0m  [4/42], [94mLoss[0m : 2.86024
[1mStep[0m  [8/42], [94mLoss[0m : 2.68485
[1mStep[0m  [12/42], [94mLoss[0m : 2.50861
[1mStep[0m  [16/42], [94mLoss[0m : 2.83838
[1mStep[0m  [20/42], [94mLoss[0m : 2.92983
[1mStep[0m  [24/42], [94mLoss[0m : 2.87042
[1mStep[0m  [28/42], [94mLoss[0m : 2.66967
[1mStep[0m  [32/42], [94mLoss[0m : 2.70035
[1mStep[0m  [36/42], [94mLoss[0m : 2.70938
[1mStep[0m  [40/42], [94mLoss[0m : 2.49594

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.699, [92mTest[0m: 2.469, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60439
[1mStep[0m  [4/42], [94mLoss[0m : 2.33647
[1mStep[0m  [8/42], [94mLoss[0m : 2.53416
[1mStep[0m  [12/42], [94mLoss[0m : 2.76663
[1mStep[0m  [16/42], [94mLoss[0m : 2.93469
[1mStep[0m  [20/42], [94mLoss[0m : 2.64152
[1mStep[0m  [24/42], [94mLoss[0m : 2.56671
[1mStep[0m  [28/42], [94mLoss[0m : 2.75842
[1mStep[0m  [32/42], [94mLoss[0m : 2.42865
[1mStep[0m  [36/42], [94mLoss[0m : 2.79073
[1mStep[0m  [40/42], [94mLoss[0m : 2.81454

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.668, [92mTest[0m: 2.486, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46821
[1mStep[0m  [4/42], [94mLoss[0m : 2.78118
[1mStep[0m  [8/42], [94mLoss[0m : 2.42278
[1mStep[0m  [12/42], [94mLoss[0m : 2.49506
[1mStep[0m  [16/42], [94mLoss[0m : 2.65492
[1mStep[0m  [20/42], [94mLoss[0m : 2.66283
[1mStep[0m  [24/42], [94mLoss[0m : 2.74238
[1mStep[0m  [28/42], [94mLoss[0m : 2.89305
[1mStep[0m  [32/42], [94mLoss[0m : 2.68266
[1mStep[0m  [36/42], [94mLoss[0m : 2.55989
[1mStep[0m  [40/42], [94mLoss[0m : 2.68152

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.639, [92mTest[0m: 2.441, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.73223
[1mStep[0m  [4/42], [94mLoss[0m : 2.40163
[1mStep[0m  [8/42], [94mLoss[0m : 2.73863
[1mStep[0m  [12/42], [94mLoss[0m : 2.70277
[1mStep[0m  [16/42], [94mLoss[0m : 2.54601
[1mStep[0m  [20/42], [94mLoss[0m : 2.47591
[1mStep[0m  [24/42], [94mLoss[0m : 2.86850
[1mStep[0m  [28/42], [94mLoss[0m : 2.44034
[1mStep[0m  [32/42], [94mLoss[0m : 2.50860
[1mStep[0m  [36/42], [94mLoss[0m : 2.68438
[1mStep[0m  [40/42], [94mLoss[0m : 2.63615

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.630, [92mTest[0m: 2.485, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.03662
[1mStep[0m  [4/42], [94mLoss[0m : 2.61279
[1mStep[0m  [8/42], [94mLoss[0m : 2.63825
[1mStep[0m  [12/42], [94mLoss[0m : 2.48363
[1mStep[0m  [16/42], [94mLoss[0m : 2.62929
[1mStep[0m  [20/42], [94mLoss[0m : 2.54065
[1mStep[0m  [24/42], [94mLoss[0m : 2.69105
[1mStep[0m  [28/42], [94mLoss[0m : 2.55078
[1mStep[0m  [32/42], [94mLoss[0m : 2.55288
[1mStep[0m  [36/42], [94mLoss[0m : 2.37604
[1mStep[0m  [40/42], [94mLoss[0m : 2.61019

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.631, [92mTest[0m: 2.465, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61917
[1mStep[0m  [4/42], [94mLoss[0m : 2.64132
[1mStep[0m  [8/42], [94mLoss[0m : 2.51528
[1mStep[0m  [12/42], [94mLoss[0m : 2.55807
[1mStep[0m  [16/42], [94mLoss[0m : 2.73106
[1mStep[0m  [20/42], [94mLoss[0m : 2.74924
[1mStep[0m  [24/42], [94mLoss[0m : 2.56454
[1mStep[0m  [28/42], [94mLoss[0m : 2.43845
[1mStep[0m  [32/42], [94mLoss[0m : 2.53234
[1mStep[0m  [36/42], [94mLoss[0m : 2.63849
[1mStep[0m  [40/42], [94mLoss[0m : 2.57995

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.590, [92mTest[0m: 2.452, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.71483
[1mStep[0m  [4/42], [94mLoss[0m : 2.65189
[1mStep[0m  [8/42], [94mLoss[0m : 2.64876
[1mStep[0m  [12/42], [94mLoss[0m : 2.52798
[1mStep[0m  [16/42], [94mLoss[0m : 2.48127
[1mStep[0m  [20/42], [94mLoss[0m : 2.57564
[1mStep[0m  [24/42], [94mLoss[0m : 2.59880
[1mStep[0m  [28/42], [94mLoss[0m : 2.59588
[1mStep[0m  [32/42], [94mLoss[0m : 2.68167
[1mStep[0m  [36/42], [94mLoss[0m : 2.68164
[1mStep[0m  [40/42], [94mLoss[0m : 2.70248

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.579, [92mTest[0m: 2.510, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.70037
[1mStep[0m  [4/42], [94mLoss[0m : 2.43110
[1mStep[0m  [8/42], [94mLoss[0m : 2.59047
[1mStep[0m  [12/42], [94mLoss[0m : 2.60216
[1mStep[0m  [16/42], [94mLoss[0m : 2.68336
[1mStep[0m  [20/42], [94mLoss[0m : 2.48221
[1mStep[0m  [24/42], [94mLoss[0m : 2.54836
[1mStep[0m  [28/42], [94mLoss[0m : 2.41627
[1mStep[0m  [32/42], [94mLoss[0m : 2.40435
[1mStep[0m  [36/42], [94mLoss[0m : 2.57509
[1mStep[0m  [40/42], [94mLoss[0m : 2.55648

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.569, [92mTest[0m: 2.491, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.71801
[1mStep[0m  [4/42], [94mLoss[0m : 2.70193
[1mStep[0m  [8/42], [94mLoss[0m : 2.47590
[1mStep[0m  [12/42], [94mLoss[0m : 2.73438
[1mStep[0m  [16/42], [94mLoss[0m : 2.53862
[1mStep[0m  [20/42], [94mLoss[0m : 2.53620
[1mStep[0m  [24/42], [94mLoss[0m : 2.71294
[1mStep[0m  [28/42], [94mLoss[0m : 2.44847
[1mStep[0m  [32/42], [94mLoss[0m : 2.51189
[1mStep[0m  [36/42], [94mLoss[0m : 2.38379
[1mStep[0m  [40/42], [94mLoss[0m : 2.25160

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.558, [92mTest[0m: 2.512, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46575
[1mStep[0m  [4/42], [94mLoss[0m : 2.57080
[1mStep[0m  [8/42], [94mLoss[0m : 2.33637
[1mStep[0m  [12/42], [94mLoss[0m : 2.69030
[1mStep[0m  [16/42], [94mLoss[0m : 2.47793
[1mStep[0m  [20/42], [94mLoss[0m : 2.29027
[1mStep[0m  [24/42], [94mLoss[0m : 2.55886
[1mStep[0m  [28/42], [94mLoss[0m : 2.50380
[1mStep[0m  [32/42], [94mLoss[0m : 2.52695
[1mStep[0m  [36/42], [94mLoss[0m : 2.76586
[1mStep[0m  [40/42], [94mLoss[0m : 2.52324

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.542, [92mTest[0m: 2.479, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49991
[1mStep[0m  [4/42], [94mLoss[0m : 2.61511
[1mStep[0m  [8/42], [94mLoss[0m : 2.56712
[1mStep[0m  [12/42], [94mLoss[0m : 2.56086
[1mStep[0m  [16/42], [94mLoss[0m : 2.42634
[1mStep[0m  [20/42], [94mLoss[0m : 2.33190
[1mStep[0m  [24/42], [94mLoss[0m : 2.57882
[1mStep[0m  [28/42], [94mLoss[0m : 2.60117
[1mStep[0m  [32/42], [94mLoss[0m : 2.53572
[1mStep[0m  [36/42], [94mLoss[0m : 2.38277
[1mStep[0m  [40/42], [94mLoss[0m : 2.42061

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.470, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.77362
[1mStep[0m  [4/42], [94mLoss[0m : 2.40184
[1mStep[0m  [8/42], [94mLoss[0m : 2.61629
[1mStep[0m  [12/42], [94mLoss[0m : 2.45296
[1mStep[0m  [16/42], [94mLoss[0m : 2.46623
[1mStep[0m  [20/42], [94mLoss[0m : 2.42623
[1mStep[0m  [24/42], [94mLoss[0m : 2.65470
[1mStep[0m  [28/42], [94mLoss[0m : 2.39732
[1mStep[0m  [32/42], [94mLoss[0m : 2.40392
[1mStep[0m  [36/42], [94mLoss[0m : 2.54307
[1mStep[0m  [40/42], [94mLoss[0m : 2.54566

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.522, [92mTest[0m: 2.479, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59719
[1mStep[0m  [4/42], [94mLoss[0m : 2.49950
[1mStep[0m  [8/42], [94mLoss[0m : 2.49720
[1mStep[0m  [12/42], [94mLoss[0m : 2.48877
[1mStep[0m  [16/42], [94mLoss[0m : 2.54362
[1mStep[0m  [20/42], [94mLoss[0m : 2.34775
[1mStep[0m  [24/42], [94mLoss[0m : 2.49934
[1mStep[0m  [28/42], [94mLoss[0m : 2.69243
[1mStep[0m  [32/42], [94mLoss[0m : 2.50285
[1mStep[0m  [36/42], [94mLoss[0m : 2.43624
[1mStep[0m  [40/42], [94mLoss[0m : 2.69529

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.520, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44315
[1mStep[0m  [4/42], [94mLoss[0m : 2.51381
[1mStep[0m  [8/42], [94mLoss[0m : 2.59988
[1mStep[0m  [12/42], [94mLoss[0m : 2.26774
[1mStep[0m  [16/42], [94mLoss[0m : 2.53322
[1mStep[0m  [20/42], [94mLoss[0m : 2.59609
[1mStep[0m  [24/42], [94mLoss[0m : 2.47617
[1mStep[0m  [28/42], [94mLoss[0m : 2.53775
[1mStep[0m  [32/42], [94mLoss[0m : 2.34420
[1mStep[0m  [36/42], [94mLoss[0m : 2.54118
[1mStep[0m  [40/42], [94mLoss[0m : 2.65518

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.490, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42472
[1mStep[0m  [4/42], [94mLoss[0m : 2.55589
[1mStep[0m  [8/42], [94mLoss[0m : 2.30579
[1mStep[0m  [12/42], [94mLoss[0m : 2.52524
[1mStep[0m  [16/42], [94mLoss[0m : 2.33106
[1mStep[0m  [20/42], [94mLoss[0m : 2.42064
[1mStep[0m  [24/42], [94mLoss[0m : 2.36336
[1mStep[0m  [28/42], [94mLoss[0m : 2.44780
[1mStep[0m  [32/42], [94mLoss[0m : 2.57469
[1mStep[0m  [36/42], [94mLoss[0m : 2.60031
[1mStep[0m  [40/42], [94mLoss[0m : 2.79533

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.499, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.63085
[1mStep[0m  [4/42], [94mLoss[0m : 2.58302
[1mStep[0m  [8/42], [94mLoss[0m : 2.46436
[1mStep[0m  [12/42], [94mLoss[0m : 2.37426
[1mStep[0m  [16/42], [94mLoss[0m : 2.46944
[1mStep[0m  [20/42], [94mLoss[0m : 2.61601
[1mStep[0m  [24/42], [94mLoss[0m : 2.54404
[1mStep[0m  [28/42], [94mLoss[0m : 2.52306
[1mStep[0m  [32/42], [94mLoss[0m : 2.53852
[1mStep[0m  [36/42], [94mLoss[0m : 2.33048
[1mStep[0m  [40/42], [94mLoss[0m : 2.40810

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.512, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53902
[1mStep[0m  [4/42], [94mLoss[0m : 2.55522
[1mStep[0m  [8/42], [94mLoss[0m : 2.33950
[1mStep[0m  [12/42], [94mLoss[0m : 2.48268
[1mStep[0m  [16/42], [94mLoss[0m : 2.38616
[1mStep[0m  [20/42], [94mLoss[0m : 2.72872
[1mStep[0m  [24/42], [94mLoss[0m : 2.68681
[1mStep[0m  [28/42], [94mLoss[0m : 2.51211
[1mStep[0m  [32/42], [94mLoss[0m : 2.46579
[1mStep[0m  [36/42], [94mLoss[0m : 2.56283
[1mStep[0m  [40/42], [94mLoss[0m : 2.70649

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.472, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40420
[1mStep[0m  [4/42], [94mLoss[0m : 2.50898
[1mStep[0m  [8/42], [94mLoss[0m : 2.35098
[1mStep[0m  [12/42], [94mLoss[0m : 2.52910
[1mStep[0m  [16/42], [94mLoss[0m : 2.30397
[1mStep[0m  [20/42], [94mLoss[0m : 2.43163
[1mStep[0m  [24/42], [94mLoss[0m : 2.52040
[1mStep[0m  [28/42], [94mLoss[0m : 2.63179
[1mStep[0m  [32/42], [94mLoss[0m : 2.41499
[1mStep[0m  [36/42], [94mLoss[0m : 2.51471
[1mStep[0m  [40/42], [94mLoss[0m : 2.36412

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.472, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37569
[1mStep[0m  [4/42], [94mLoss[0m : 2.23660
[1mStep[0m  [8/42], [94mLoss[0m : 2.65056
[1mStep[0m  [12/42], [94mLoss[0m : 2.34730
[1mStep[0m  [16/42], [94mLoss[0m : 2.48900
[1mStep[0m  [20/42], [94mLoss[0m : 2.43007
[1mStep[0m  [24/42], [94mLoss[0m : 2.41931
[1mStep[0m  [28/42], [94mLoss[0m : 2.60492
[1mStep[0m  [32/42], [94mLoss[0m : 2.32441
[1mStep[0m  [36/42], [94mLoss[0m : 2.34756
[1mStep[0m  [40/42], [94mLoss[0m : 2.52079

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.477, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27872
[1mStep[0m  [4/42], [94mLoss[0m : 2.49535
[1mStep[0m  [8/42], [94mLoss[0m : 2.58519
[1mStep[0m  [12/42], [94mLoss[0m : 2.22336
[1mStep[0m  [16/42], [94mLoss[0m : 2.46631
[1mStep[0m  [20/42], [94mLoss[0m : 2.46807
[1mStep[0m  [24/42], [94mLoss[0m : 2.54262
[1mStep[0m  [28/42], [94mLoss[0m : 2.35840
[1mStep[0m  [32/42], [94mLoss[0m : 2.29217
[1mStep[0m  [36/42], [94mLoss[0m : 2.29403
[1mStep[0m  [40/42], [94mLoss[0m : 2.30696

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.477, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.16159
[1mStep[0m  [4/42], [94mLoss[0m : 2.33816
[1mStep[0m  [8/42], [94mLoss[0m : 2.56418
[1mStep[0m  [12/42], [94mLoss[0m : 2.38705
[1mStep[0m  [16/42], [94mLoss[0m : 2.34236
[1mStep[0m  [20/42], [94mLoss[0m : 2.48308
[1mStep[0m  [24/42], [94mLoss[0m : 2.25887
[1mStep[0m  [28/42], [94mLoss[0m : 2.39792
[1mStep[0m  [32/42], [94mLoss[0m : 2.46849
[1mStep[0m  [36/42], [94mLoss[0m : 2.39837
[1mStep[0m  [40/42], [94mLoss[0m : 2.46398

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.462, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.484
====================================

Phase 2 - Evaluation MAE:  2.48367817061288
MAE score P1        3.519036
MAE score P2        2.483678
loss                2.397383
learning_rate       0.002575
batch_size               256
hidden_sizes      [300, 100]
epochs                    30
activation              tanh
optimizer                sgd
early stopping         False
dropout                  0.3
momentum                 0.1
weight_decay           0.001
Name: 3, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/21], [94mLoss[0m : 10.94733
[1mStep[0m  [2/21], [94mLoss[0m : 10.69063
[1mStep[0m  [4/21], [94mLoss[0m : 10.90336
[1mStep[0m  [6/21], [94mLoss[0m : 10.70629
[1mStep[0m  [8/21], [94mLoss[0m : 10.75315
[1mStep[0m  [10/21], [94mLoss[0m : 10.88470
[1mStep[0m  [12/21], [94mLoss[0m : 11.00520
[1mStep[0m  [14/21], [94mLoss[0m : 11.03263
[1mStep[0m  [16/21], [94mLoss[0m : 10.75970
[1mStep[0m  [18/21], [94mLoss[0m : 11.16105
[1mStep[0m  [20/21], [94mLoss[0m : 10.96204

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.831, [92mTest[0m: 10.836, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.63097
[1mStep[0m  [2/21], [94mLoss[0m : 10.57804
[1mStep[0m  [4/21], [94mLoss[0m : 10.53866
[1mStep[0m  [6/21], [94mLoss[0m : 10.49250
[1mStep[0m  [8/21], [94mLoss[0m : 10.64785
[1mStep[0m  [10/21], [94mLoss[0m : 10.83013
[1mStep[0m  [12/21], [94mLoss[0m : 10.79384
[1mStep[0m  [14/21], [94mLoss[0m : 10.83433
[1mStep[0m  [16/21], [94mLoss[0m : 10.97972
[1mStep[0m  [18/21], [94mLoss[0m : 10.79523
[1mStep[0m  [20/21], [94mLoss[0m : 10.47111

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.730, [92mTest[0m: 10.787, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.62085
[1mStep[0m  [2/21], [94mLoss[0m : 10.70940
[1mStep[0m  [4/21], [94mLoss[0m : 10.62416
[1mStep[0m  [6/21], [94mLoss[0m : 10.76278
[1mStep[0m  [8/21], [94mLoss[0m : 10.62990
[1mStep[0m  [10/21], [94mLoss[0m : 10.76759
[1mStep[0m  [12/21], [94mLoss[0m : 10.48001
[1mStep[0m  [14/21], [94mLoss[0m : 10.81780
[1mStep[0m  [16/21], [94mLoss[0m : 10.48368
[1mStep[0m  [18/21], [94mLoss[0m : 10.44253
[1mStep[0m  [20/21], [94mLoss[0m : 10.62099

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.619, [92mTest[0m: 10.660, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.40792
[1mStep[0m  [2/21], [94mLoss[0m : 10.60962
[1mStep[0m  [4/21], [94mLoss[0m : 10.54563
[1mStep[0m  [6/21], [94mLoss[0m : 10.78637
[1mStep[0m  [8/21], [94mLoss[0m : 10.57952
[1mStep[0m  [10/21], [94mLoss[0m : 10.56296
[1mStep[0m  [12/21], [94mLoss[0m : 10.36992
[1mStep[0m  [14/21], [94mLoss[0m : 10.40029
[1mStep[0m  [16/21], [94mLoss[0m : 10.59883
[1mStep[0m  [18/21], [94mLoss[0m : 10.33444
[1mStep[0m  [20/21], [94mLoss[0m : 10.50424

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.520, [92mTest[0m: 10.504, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.65962
[1mStep[0m  [2/21], [94mLoss[0m : 10.63063
[1mStep[0m  [4/21], [94mLoss[0m : 10.39226
[1mStep[0m  [6/21], [94mLoss[0m : 10.47598
[1mStep[0m  [8/21], [94mLoss[0m : 10.29351
[1mStep[0m  [10/21], [94mLoss[0m : 10.47931
[1mStep[0m  [12/21], [94mLoss[0m : 10.41299
[1mStep[0m  [14/21], [94mLoss[0m : 10.24102
[1mStep[0m  [16/21], [94mLoss[0m : 10.27842
[1mStep[0m  [18/21], [94mLoss[0m : 10.39108
[1mStep[0m  [20/21], [94mLoss[0m : 10.50716

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.428, [92mTest[0m: 10.390, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.27141
[1mStep[0m  [2/21], [94mLoss[0m : 10.44719
[1mStep[0m  [4/21], [94mLoss[0m : 10.23671
[1mStep[0m  [6/21], [94mLoss[0m : 10.17930
[1mStep[0m  [8/21], [94mLoss[0m : 10.25998
[1mStep[0m  [10/21], [94mLoss[0m : 10.27029
[1mStep[0m  [12/21], [94mLoss[0m : 10.20193
[1mStep[0m  [14/21], [94mLoss[0m : 10.11598
[1mStep[0m  [16/21], [94mLoss[0m : 10.36921
[1mStep[0m  [18/21], [94mLoss[0m : 10.41185
[1mStep[0m  [20/21], [94mLoss[0m : 10.27166

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.319, [92mTest[0m: 10.251, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.89652
[1mStep[0m  [2/21], [94mLoss[0m : 10.20611
[1mStep[0m  [4/21], [94mLoss[0m : 10.28683
[1mStep[0m  [6/21], [94mLoss[0m : 10.44404
[1mStep[0m  [8/21], [94mLoss[0m : 10.39304
[1mStep[0m  [10/21], [94mLoss[0m : 10.32927
[1mStep[0m  [12/21], [94mLoss[0m : 10.05006
[1mStep[0m  [14/21], [94mLoss[0m : 10.27007
[1mStep[0m  [16/21], [94mLoss[0m : 10.34249
[1mStep[0m  [18/21], [94mLoss[0m : 10.38717
[1mStep[0m  [20/21], [94mLoss[0m : 10.09937

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.211, [92mTest[0m: 10.096, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.10760
[1mStep[0m  [2/21], [94mLoss[0m : 10.14320
[1mStep[0m  [4/21], [94mLoss[0m : 10.35037
[1mStep[0m  [6/21], [94mLoss[0m : 10.16799
[1mStep[0m  [8/21], [94mLoss[0m : 10.02666
[1mStep[0m  [10/21], [94mLoss[0m : 10.19399
[1mStep[0m  [12/21], [94mLoss[0m : 10.05830
[1mStep[0m  [14/21], [94mLoss[0m : 9.98009
[1mStep[0m  [16/21], [94mLoss[0m : 10.22237
[1mStep[0m  [18/21], [94mLoss[0m : 9.80518
[1mStep[0m  [20/21], [94mLoss[0m : 10.09983

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.109, [92mTest[0m: 9.954, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.96986
[1mStep[0m  [2/21], [94mLoss[0m : 9.99948
[1mStep[0m  [4/21], [94mLoss[0m : 10.22626
[1mStep[0m  [6/21], [94mLoss[0m : 10.00309
[1mStep[0m  [8/21], [94mLoss[0m : 10.11800
[1mStep[0m  [10/21], [94mLoss[0m : 10.06430
[1mStep[0m  [12/21], [94mLoss[0m : 9.88337
[1mStep[0m  [14/21], [94mLoss[0m : 10.07063
[1mStep[0m  [16/21], [94mLoss[0m : 10.07151
[1mStep[0m  [18/21], [94mLoss[0m : 9.92779
[1mStep[0m  [20/21], [94mLoss[0m : 10.05845

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 9.994, [92mTest[0m: 9.794, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.84040
[1mStep[0m  [2/21], [94mLoss[0m : 10.13797
[1mStep[0m  [4/21], [94mLoss[0m : 9.53984
[1mStep[0m  [6/21], [94mLoss[0m : 10.02822
[1mStep[0m  [8/21], [94mLoss[0m : 9.98067
[1mStep[0m  [10/21], [94mLoss[0m : 10.03482
[1mStep[0m  [12/21], [94mLoss[0m : 9.83275
[1mStep[0m  [14/21], [94mLoss[0m : 9.85889
[1mStep[0m  [16/21], [94mLoss[0m : 9.83070
[1mStep[0m  [18/21], [94mLoss[0m : 9.52309
[1mStep[0m  [20/21], [94mLoss[0m : 9.89364

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 9.878, [92mTest[0m: 9.641, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.90984
[1mStep[0m  [2/21], [94mLoss[0m : 9.69893
[1mStep[0m  [4/21], [94mLoss[0m : 9.50961
[1mStep[0m  [6/21], [94mLoss[0m : 9.62313
[1mStep[0m  [8/21], [94mLoss[0m : 9.73233
[1mStep[0m  [10/21], [94mLoss[0m : 9.68302
[1mStep[0m  [12/21], [94mLoss[0m : 9.90272
[1mStep[0m  [14/21], [94mLoss[0m : 9.76159
[1mStep[0m  [16/21], [94mLoss[0m : 9.83685
[1mStep[0m  [18/21], [94mLoss[0m : 9.79658
[1mStep[0m  [20/21], [94mLoss[0m : 9.54507

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 9.756, [92mTest[0m: 9.471, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.44899
[1mStep[0m  [2/21], [94mLoss[0m : 9.61293
[1mStep[0m  [4/21], [94mLoss[0m : 9.81702
[1mStep[0m  [6/21], [94mLoss[0m : 9.86754
[1mStep[0m  [8/21], [94mLoss[0m : 9.49753
[1mStep[0m  [10/21], [94mLoss[0m : 9.74838
[1mStep[0m  [12/21], [94mLoss[0m : 9.79680
[1mStep[0m  [14/21], [94mLoss[0m : 9.78876
[1mStep[0m  [16/21], [94mLoss[0m : 9.56896
[1mStep[0m  [18/21], [94mLoss[0m : 9.51606
[1mStep[0m  [20/21], [94mLoss[0m : 9.65546

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 9.630, [92mTest[0m: 9.347, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.70603
[1mStep[0m  [2/21], [94mLoss[0m : 9.63680
[1mStep[0m  [4/21], [94mLoss[0m : 9.41653
[1mStep[0m  [6/21], [94mLoss[0m : 9.43746
[1mStep[0m  [8/21], [94mLoss[0m : 9.51530
[1mStep[0m  [10/21], [94mLoss[0m : 9.51431
[1mStep[0m  [12/21], [94mLoss[0m : 9.55455
[1mStep[0m  [14/21], [94mLoss[0m : 9.46346
[1mStep[0m  [16/21], [94mLoss[0m : 9.44982
[1mStep[0m  [18/21], [94mLoss[0m : 9.50781
[1mStep[0m  [20/21], [94mLoss[0m : 9.49211

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 9.506, [92mTest[0m: 9.153, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.70669
[1mStep[0m  [2/21], [94mLoss[0m : 9.38070
[1mStep[0m  [4/21], [94mLoss[0m : 9.50804
[1mStep[0m  [6/21], [94mLoss[0m : 9.56698
[1mStep[0m  [8/21], [94mLoss[0m : 9.36034
[1mStep[0m  [10/21], [94mLoss[0m : 9.35257
[1mStep[0m  [12/21], [94mLoss[0m : 9.33429
[1mStep[0m  [14/21], [94mLoss[0m : 9.24894
[1mStep[0m  [16/21], [94mLoss[0m : 9.29332
[1mStep[0m  [18/21], [94mLoss[0m : 9.45810
[1mStep[0m  [20/21], [94mLoss[0m : 9.40749

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 9.360, [92mTest[0m: 8.968, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.93172
[1mStep[0m  [2/21], [94mLoss[0m : 9.39334
[1mStep[0m  [4/21], [94mLoss[0m : 9.02019
[1mStep[0m  [6/21], [94mLoss[0m : 9.16021
[1mStep[0m  [8/21], [94mLoss[0m : 9.38240
[1mStep[0m  [10/21], [94mLoss[0m : 9.18730
[1mStep[0m  [12/21], [94mLoss[0m : 9.37554
[1mStep[0m  [14/21], [94mLoss[0m : 9.38030
[1mStep[0m  [16/21], [94mLoss[0m : 9.24012
[1mStep[0m  [18/21], [94mLoss[0m : 9.36956
[1mStep[0m  [20/21], [94mLoss[0m : 9.16108

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 9.229, [92mTest[0m: 8.794, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.17076
[1mStep[0m  [2/21], [94mLoss[0m : 9.16060
[1mStep[0m  [4/21], [94mLoss[0m : 9.21515
[1mStep[0m  [6/21], [94mLoss[0m : 9.23111
[1mStep[0m  [8/21], [94mLoss[0m : 9.24900
[1mStep[0m  [10/21], [94mLoss[0m : 8.88428
[1mStep[0m  [12/21], [94mLoss[0m : 9.20661
[1mStep[0m  [14/21], [94mLoss[0m : 8.92890
[1mStep[0m  [16/21], [94mLoss[0m : 9.11710
[1mStep[0m  [18/21], [94mLoss[0m : 8.85274
[1mStep[0m  [20/21], [94mLoss[0m : 9.06639

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 9.083, [92mTest[0m: 8.654, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.14814
[1mStep[0m  [2/21], [94mLoss[0m : 8.87503
[1mStep[0m  [4/21], [94mLoss[0m : 9.18862
[1mStep[0m  [6/21], [94mLoss[0m : 8.88833
[1mStep[0m  [8/21], [94mLoss[0m : 8.97572
[1mStep[0m  [10/21], [94mLoss[0m : 8.94148
[1mStep[0m  [12/21], [94mLoss[0m : 9.35806
[1mStep[0m  [14/21], [94mLoss[0m : 9.06412
[1mStep[0m  [16/21], [94mLoss[0m : 8.94784
[1mStep[0m  [18/21], [94mLoss[0m : 8.83256
[1mStep[0m  [20/21], [94mLoss[0m : 8.67625

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 8.936, [92mTest[0m: 8.483, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.78084
[1mStep[0m  [2/21], [94mLoss[0m : 8.92283
[1mStep[0m  [4/21], [94mLoss[0m : 8.68110
[1mStep[0m  [6/21], [94mLoss[0m : 8.82043
[1mStep[0m  [8/21], [94mLoss[0m : 8.82149
[1mStep[0m  [10/21], [94mLoss[0m : 8.70999
[1mStep[0m  [12/21], [94mLoss[0m : 8.89526
[1mStep[0m  [14/21], [94mLoss[0m : 8.67678
[1mStep[0m  [16/21], [94mLoss[0m : 8.70675
[1mStep[0m  [18/21], [94mLoss[0m : 8.91558
[1mStep[0m  [20/21], [94mLoss[0m : 8.81560

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 8.786, [92mTest[0m: 8.242, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.61932
[1mStep[0m  [2/21], [94mLoss[0m : 8.55384
[1mStep[0m  [4/21], [94mLoss[0m : 8.70266
[1mStep[0m  [6/21], [94mLoss[0m : 8.75112
[1mStep[0m  [8/21], [94mLoss[0m : 9.07699
[1mStep[0m  [10/21], [94mLoss[0m : 8.68037
[1mStep[0m  [12/21], [94mLoss[0m : 8.63267
[1mStep[0m  [14/21], [94mLoss[0m : 8.30397
[1mStep[0m  [16/21], [94mLoss[0m : 8.57972
[1mStep[0m  [18/21], [94mLoss[0m : 8.52594
[1mStep[0m  [20/21], [94mLoss[0m : 8.74613

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 8.633, [92mTest[0m: 8.089, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.62814
[1mStep[0m  [2/21], [94mLoss[0m : 8.47110
[1mStep[0m  [4/21], [94mLoss[0m : 8.41394
[1mStep[0m  [6/21], [94mLoss[0m : 8.60361
[1mStep[0m  [8/21], [94mLoss[0m : 8.49568
[1mStep[0m  [10/21], [94mLoss[0m : 8.56416
[1mStep[0m  [12/21], [94mLoss[0m : 8.39461
[1mStep[0m  [14/21], [94mLoss[0m : 8.56385
[1mStep[0m  [16/21], [94mLoss[0m : 8.68495
[1mStep[0m  [18/21], [94mLoss[0m : 8.24584
[1mStep[0m  [20/21], [94mLoss[0m : 8.58587

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 8.489, [92mTest[0m: 7.862, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.29288
[1mStep[0m  [2/21], [94mLoss[0m : 8.32529
[1mStep[0m  [4/21], [94mLoss[0m : 8.26898
[1mStep[0m  [6/21], [94mLoss[0m : 8.34225
[1mStep[0m  [8/21], [94mLoss[0m : 8.50591
[1mStep[0m  [10/21], [94mLoss[0m : 8.44706
[1mStep[0m  [12/21], [94mLoss[0m : 8.32481
[1mStep[0m  [14/21], [94mLoss[0m : 8.72339
[1mStep[0m  [16/21], [94mLoss[0m : 8.16833
[1mStep[0m  [18/21], [94mLoss[0m : 8.19622
[1mStep[0m  [20/21], [94mLoss[0m : 8.45490

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 8.347, [92mTest[0m: 7.665, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.29123
[1mStep[0m  [2/21], [94mLoss[0m : 8.54446
[1mStep[0m  [4/21], [94mLoss[0m : 8.31092
[1mStep[0m  [6/21], [94mLoss[0m : 8.26460
[1mStep[0m  [8/21], [94mLoss[0m : 8.31411
[1mStep[0m  [10/21], [94mLoss[0m : 8.26871
[1mStep[0m  [12/21], [94mLoss[0m : 8.16819
[1mStep[0m  [14/21], [94mLoss[0m : 8.09580
[1mStep[0m  [16/21], [94mLoss[0m : 8.03524
[1mStep[0m  [18/21], [94mLoss[0m : 8.20161
[1mStep[0m  [20/21], [94mLoss[0m : 8.08220

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 8.219, [92mTest[0m: 7.589, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.07797
[1mStep[0m  [2/21], [94mLoss[0m : 8.18999
[1mStep[0m  [4/21], [94mLoss[0m : 8.07520
[1mStep[0m  [6/21], [94mLoss[0m : 7.96874
[1mStep[0m  [8/21], [94mLoss[0m : 8.10328
[1mStep[0m  [10/21], [94mLoss[0m : 8.33310
[1mStep[0m  [12/21], [94mLoss[0m : 7.76103
[1mStep[0m  [14/21], [94mLoss[0m : 8.16747
[1mStep[0m  [16/21], [94mLoss[0m : 7.91476
[1mStep[0m  [18/21], [94mLoss[0m : 7.91694
[1mStep[0m  [20/21], [94mLoss[0m : 7.85044

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 8.076, [92mTest[0m: 7.366, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.98937
[1mStep[0m  [2/21], [94mLoss[0m : 8.11789
[1mStep[0m  [4/21], [94mLoss[0m : 7.94671
[1mStep[0m  [6/21], [94mLoss[0m : 7.87671
[1mStep[0m  [8/21], [94mLoss[0m : 8.05602
[1mStep[0m  [10/21], [94mLoss[0m : 7.90524
[1mStep[0m  [12/21], [94mLoss[0m : 8.15236
[1mStep[0m  [14/21], [94mLoss[0m : 7.91348
[1mStep[0m  [16/21], [94mLoss[0m : 7.76342
[1mStep[0m  [18/21], [94mLoss[0m : 7.92147
[1mStep[0m  [20/21], [94mLoss[0m : 7.80312

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 7.965, [92mTest[0m: 7.283, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.76504
[1mStep[0m  [2/21], [94mLoss[0m : 7.78816
[1mStep[0m  [4/21], [94mLoss[0m : 7.87100
[1mStep[0m  [6/21], [94mLoss[0m : 8.07205
[1mStep[0m  [8/21], [94mLoss[0m : 7.83381
[1mStep[0m  [10/21], [94mLoss[0m : 7.97110
[1mStep[0m  [12/21], [94mLoss[0m : 7.67912
[1mStep[0m  [14/21], [94mLoss[0m : 7.74300
[1mStep[0m  [16/21], [94mLoss[0m : 7.92361
[1mStep[0m  [18/21], [94mLoss[0m : 7.62225
[1mStep[0m  [20/21], [94mLoss[0m : 7.92397

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 7.820, [92mTest[0m: 7.155, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.66666
[1mStep[0m  [2/21], [94mLoss[0m : 7.67979
[1mStep[0m  [4/21], [94mLoss[0m : 7.73015
[1mStep[0m  [6/21], [94mLoss[0m : 7.89914
[1mStep[0m  [8/21], [94mLoss[0m : 7.64144
[1mStep[0m  [10/21], [94mLoss[0m : 7.60960
[1mStep[0m  [12/21], [94mLoss[0m : 7.77703
[1mStep[0m  [14/21], [94mLoss[0m : 7.63736
[1mStep[0m  [16/21], [94mLoss[0m : 7.60451
[1mStep[0m  [18/21], [94mLoss[0m : 7.73000
[1mStep[0m  [20/21], [94mLoss[0m : 7.53254

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 7.679, [92mTest[0m: 6.911, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.77244
[1mStep[0m  [2/21], [94mLoss[0m : 7.57563
[1mStep[0m  [4/21], [94mLoss[0m : 7.58041
[1mStep[0m  [6/21], [94mLoss[0m : 7.50757
[1mStep[0m  [8/21], [94mLoss[0m : 7.58660
[1mStep[0m  [10/21], [94mLoss[0m : 7.85048
[1mStep[0m  [12/21], [94mLoss[0m : 8.15667
[1mStep[0m  [14/21], [94mLoss[0m : 7.44057
[1mStep[0m  [16/21], [94mLoss[0m : 7.71744
[1mStep[0m  [18/21], [94mLoss[0m : 7.41436
[1mStep[0m  [20/21], [94mLoss[0m : 7.52277

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 7.573, [92mTest[0m: 6.843, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.59368
[1mStep[0m  [2/21], [94mLoss[0m : 7.61286
[1mStep[0m  [4/21], [94mLoss[0m : 7.29630
[1mStep[0m  [6/21], [94mLoss[0m : 7.48901
[1mStep[0m  [8/21], [94mLoss[0m : 7.38279
[1mStep[0m  [10/21], [94mLoss[0m : 7.38128
[1mStep[0m  [12/21], [94mLoss[0m : 7.27247
[1mStep[0m  [14/21], [94mLoss[0m : 7.43628
[1mStep[0m  [16/21], [94mLoss[0m : 7.46220
[1mStep[0m  [18/21], [94mLoss[0m : 7.47599
[1mStep[0m  [20/21], [94mLoss[0m : 7.44827

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 7.456, [92mTest[0m: 6.715, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.43377
[1mStep[0m  [2/21], [94mLoss[0m : 7.54665
[1mStep[0m  [4/21], [94mLoss[0m : 7.67339
[1mStep[0m  [6/21], [94mLoss[0m : 7.19568
[1mStep[0m  [8/21], [94mLoss[0m : 7.31542
[1mStep[0m  [10/21], [94mLoss[0m : 7.43700
[1mStep[0m  [12/21], [94mLoss[0m : 7.09577
[1mStep[0m  [14/21], [94mLoss[0m : 7.12296
[1mStep[0m  [16/21], [94mLoss[0m : 7.04695
[1mStep[0m  [18/21], [94mLoss[0m : 7.51409
[1mStep[0m  [20/21], [94mLoss[0m : 7.21373

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 7.334, [92mTest[0m: 6.604, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.05061
[1mStep[0m  [2/21], [94mLoss[0m : 7.23130
[1mStep[0m  [4/21], [94mLoss[0m : 7.37584
[1mStep[0m  [6/21], [94mLoss[0m : 7.35740
[1mStep[0m  [8/21], [94mLoss[0m : 7.39808
[1mStep[0m  [10/21], [94mLoss[0m : 7.49562
[1mStep[0m  [12/21], [94mLoss[0m : 7.18803
[1mStep[0m  [14/21], [94mLoss[0m : 7.22118
[1mStep[0m  [16/21], [94mLoss[0m : 7.19696
[1mStep[0m  [18/21], [94mLoss[0m : 7.29284
[1mStep[0m  [20/21], [94mLoss[0m : 7.11998

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 7.236, [92mTest[0m: 6.454, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 6.403
====================================

Phase 1 - Evaluation MAE:  6.403254985809326
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/21], [94mLoss[0m : 7.25765
[1mStep[0m  [2/21], [94mLoss[0m : 7.20373
[1mStep[0m  [4/21], [94mLoss[0m : 7.10282
[1mStep[0m  [6/21], [94mLoss[0m : 7.09764
[1mStep[0m  [8/21], [94mLoss[0m : 6.96464
[1mStep[0m  [10/21], [94mLoss[0m : 7.06445
[1mStep[0m  [12/21], [94mLoss[0m : 6.70663
[1mStep[0m  [14/21], [94mLoss[0m : 7.12991
[1mStep[0m  [16/21], [94mLoss[0m : 6.98989
[1mStep[0m  [18/21], [94mLoss[0m : 7.07246
[1mStep[0m  [20/21], [94mLoss[0m : 6.89082

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 7.011, [92mTest[0m: 6.419, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.74936
[1mStep[0m  [2/21], [94mLoss[0m : 7.09093
[1mStep[0m  [4/21], [94mLoss[0m : 6.93440
[1mStep[0m  [6/21], [94mLoss[0m : 7.16864
[1mStep[0m  [8/21], [94mLoss[0m : 7.00179
[1mStep[0m  [10/21], [94mLoss[0m : 6.64260
[1mStep[0m  [12/21], [94mLoss[0m : 6.95461
[1mStep[0m  [14/21], [94mLoss[0m : 6.51156
[1mStep[0m  [16/21], [94mLoss[0m : 6.59264
[1mStep[0m  [18/21], [94mLoss[0m : 6.64739
[1mStep[0m  [20/21], [94mLoss[0m : 6.57110

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 6.807, [92mTest[0m: 5.855, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.84435
[1mStep[0m  [2/21], [94mLoss[0m : 6.84886
[1mStep[0m  [4/21], [94mLoss[0m : 6.73222
[1mStep[0m  [6/21], [94mLoss[0m : 6.87901
[1mStep[0m  [8/21], [94mLoss[0m : 6.66064
[1mStep[0m  [10/21], [94mLoss[0m : 6.36860
[1mStep[0m  [12/21], [94mLoss[0m : 6.53969
[1mStep[0m  [14/21], [94mLoss[0m : 6.51809
[1mStep[0m  [16/21], [94mLoss[0m : 6.62312
[1mStep[0m  [18/21], [94mLoss[0m : 6.56095
[1mStep[0m  [20/21], [94mLoss[0m : 6.38762

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 6.645, [92mTest[0m: 6.129, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.81782
[1mStep[0m  [2/21], [94mLoss[0m : 6.64660
[1mStep[0m  [4/21], [94mLoss[0m : 6.68551
[1mStep[0m  [6/21], [94mLoss[0m : 6.67800
[1mStep[0m  [8/21], [94mLoss[0m : 6.51713
[1mStep[0m  [10/21], [94mLoss[0m : 6.36396
[1mStep[0m  [12/21], [94mLoss[0m : 6.50254
[1mStep[0m  [14/21], [94mLoss[0m : 6.43023
[1mStep[0m  [16/21], [94mLoss[0m : 6.35611
[1mStep[0m  [18/21], [94mLoss[0m : 6.31744
[1mStep[0m  [20/21], [94mLoss[0m : 6.24866

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 6.488, [92mTest[0m: 8.312, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.39761
[1mStep[0m  [2/21], [94mLoss[0m : 6.52369
[1mStep[0m  [4/21], [94mLoss[0m : 6.38569
[1mStep[0m  [6/21], [94mLoss[0m : 6.38271
[1mStep[0m  [8/21], [94mLoss[0m : 6.52257
[1mStep[0m  [10/21], [94mLoss[0m : 6.10392
[1mStep[0m  [12/21], [94mLoss[0m : 6.21110
[1mStep[0m  [14/21], [94mLoss[0m : 6.29909
[1mStep[0m  [16/21], [94mLoss[0m : 6.31559
[1mStep[0m  [18/21], [94mLoss[0m : 6.23627
[1mStep[0m  [20/21], [94mLoss[0m : 6.33016

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 6.321, [92mTest[0m: 6.191, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.13792
[1mStep[0m  [2/21], [94mLoss[0m : 6.41798
[1mStep[0m  [4/21], [94mLoss[0m : 6.20491
[1mStep[0m  [6/21], [94mLoss[0m : 6.10683
[1mStep[0m  [8/21], [94mLoss[0m : 6.34177
[1mStep[0m  [10/21], [94mLoss[0m : 6.01029
[1mStep[0m  [12/21], [94mLoss[0m : 6.15546
[1mStep[0m  [14/21], [94mLoss[0m : 6.14724
[1mStep[0m  [16/21], [94mLoss[0m : 5.90668
[1mStep[0m  [18/21], [94mLoss[0m : 5.81183
[1mStep[0m  [20/21], [94mLoss[0m : 6.27561

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 6.174, [92mTest[0m: 6.052, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.86802
[1mStep[0m  [2/21], [94mLoss[0m : 5.87318
[1mStep[0m  [4/21], [94mLoss[0m : 6.09015
[1mStep[0m  [6/21], [94mLoss[0m : 6.28717
[1mStep[0m  [8/21], [94mLoss[0m : 6.09300
[1mStep[0m  [10/21], [94mLoss[0m : 6.12954
[1mStep[0m  [12/21], [94mLoss[0m : 6.19798
[1mStep[0m  [14/21], [94mLoss[0m : 6.01410
[1mStep[0m  [16/21], [94mLoss[0m : 6.18623
[1mStep[0m  [18/21], [94mLoss[0m : 5.80899
[1mStep[0m  [20/21], [94mLoss[0m : 5.87437

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 6.027, [92mTest[0m: 6.654, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.74961
[1mStep[0m  [2/21], [94mLoss[0m : 5.81046
[1mStep[0m  [4/21], [94mLoss[0m : 6.01612
[1mStep[0m  [6/21], [94mLoss[0m : 5.89317
[1mStep[0m  [8/21], [94mLoss[0m : 5.72671
[1mStep[0m  [10/21], [94mLoss[0m : 5.79006
[1mStep[0m  [12/21], [94mLoss[0m : 5.93412
[1mStep[0m  [14/21], [94mLoss[0m : 5.88330
[1mStep[0m  [16/21], [94mLoss[0m : 5.65236
[1mStep[0m  [18/21], [94mLoss[0m : 6.09067
[1mStep[0m  [20/21], [94mLoss[0m : 5.50870

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 5.851, [92mTest[0m: 5.335, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.80363
[1mStep[0m  [2/21], [94mLoss[0m : 5.84364
[1mStep[0m  [4/21], [94mLoss[0m : 5.65404
[1mStep[0m  [6/21], [94mLoss[0m : 5.77197
[1mStep[0m  [8/21], [94mLoss[0m : 5.72566
[1mStep[0m  [10/21], [94mLoss[0m : 5.56392
[1mStep[0m  [12/21], [94mLoss[0m : 5.65406
[1mStep[0m  [14/21], [94mLoss[0m : 5.55772
[1mStep[0m  [16/21], [94mLoss[0m : 5.38313
[1mStep[0m  [18/21], [94mLoss[0m : 5.62572
[1mStep[0m  [20/21], [94mLoss[0m : 5.73350

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 5.683, [92mTest[0m: 5.195, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.71332
[1mStep[0m  [2/21], [94mLoss[0m : 5.01383
[1mStep[0m  [4/21], [94mLoss[0m : 5.53612
[1mStep[0m  [6/21], [94mLoss[0m : 5.55078
[1mStep[0m  [8/21], [94mLoss[0m : 5.51572
[1mStep[0m  [10/21], [94mLoss[0m : 5.80481
[1mStep[0m  [12/21], [94mLoss[0m : 5.59505
[1mStep[0m  [14/21], [94mLoss[0m : 5.51350
[1mStep[0m  [16/21], [94mLoss[0m : 5.54519
[1mStep[0m  [18/21], [94mLoss[0m : 5.57449
[1mStep[0m  [20/21], [94mLoss[0m : 5.48637

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 5.519, [92mTest[0m: 5.466, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.62102
[1mStep[0m  [2/21], [94mLoss[0m : 5.45336
[1mStep[0m  [4/21], [94mLoss[0m : 5.54699
[1mStep[0m  [6/21], [94mLoss[0m : 5.38437
[1mStep[0m  [8/21], [94mLoss[0m : 5.19370
[1mStep[0m  [10/21], [94mLoss[0m : 5.41968
[1mStep[0m  [12/21], [94mLoss[0m : 5.56050
[1mStep[0m  [14/21], [94mLoss[0m : 5.24716
[1mStep[0m  [16/21], [94mLoss[0m : 5.35094
[1mStep[0m  [18/21], [94mLoss[0m : 5.37447
[1mStep[0m  [20/21], [94mLoss[0m : 5.34002

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 5.328, [92mTest[0m: 5.607, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.27098
[1mStep[0m  [2/21], [94mLoss[0m : 5.30057
[1mStep[0m  [4/21], [94mLoss[0m : 5.55144
[1mStep[0m  [6/21], [94mLoss[0m : 5.22642
[1mStep[0m  [8/21], [94mLoss[0m : 4.89642
[1mStep[0m  [10/21], [94mLoss[0m : 4.98738
[1mStep[0m  [12/21], [94mLoss[0m : 4.98718
[1mStep[0m  [14/21], [94mLoss[0m : 5.35668
[1mStep[0m  [16/21], [94mLoss[0m : 5.20632
[1mStep[0m  [18/21], [94mLoss[0m : 5.32697
[1mStep[0m  [20/21], [94mLoss[0m : 5.16703

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 5.160, [92mTest[0m: 5.780, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.05935
[1mStep[0m  [2/21], [94mLoss[0m : 4.92265
[1mStep[0m  [4/21], [94mLoss[0m : 5.18039
[1mStep[0m  [6/21], [94mLoss[0m : 4.93579
[1mStep[0m  [8/21], [94mLoss[0m : 4.81141
[1mStep[0m  [10/21], [94mLoss[0m : 5.04244
[1mStep[0m  [12/21], [94mLoss[0m : 5.22391
[1mStep[0m  [14/21], [94mLoss[0m : 5.06401
[1mStep[0m  [16/21], [94mLoss[0m : 4.84357
[1mStep[0m  [18/21], [94mLoss[0m : 4.69736
[1mStep[0m  [20/21], [94mLoss[0m : 5.30466

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 4.989, [92mTest[0m: 5.646, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.16271
[1mStep[0m  [2/21], [94mLoss[0m : 4.98183
[1mStep[0m  [4/21], [94mLoss[0m : 4.63168
[1mStep[0m  [6/21], [94mLoss[0m : 4.64496
[1mStep[0m  [8/21], [94mLoss[0m : 4.76025
[1mStep[0m  [10/21], [94mLoss[0m : 4.65922
[1mStep[0m  [12/21], [94mLoss[0m : 4.90180
[1mStep[0m  [14/21], [94mLoss[0m : 4.60232
[1mStep[0m  [16/21], [94mLoss[0m : 4.54953
[1mStep[0m  [18/21], [94mLoss[0m : 4.67242
[1mStep[0m  [20/21], [94mLoss[0m : 4.90142

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 4.800, [92mTest[0m: 6.149, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.75928
[1mStep[0m  [2/21], [94mLoss[0m : 4.63821
[1mStep[0m  [4/21], [94mLoss[0m : 4.50355
[1mStep[0m  [6/21], [94mLoss[0m : 4.94134
[1mStep[0m  [8/21], [94mLoss[0m : 4.64979
[1mStep[0m  [10/21], [94mLoss[0m : 4.68771
[1mStep[0m  [12/21], [94mLoss[0m : 4.67905
[1mStep[0m  [14/21], [94mLoss[0m : 4.63366
[1mStep[0m  [16/21], [94mLoss[0m : 4.85225
[1mStep[0m  [18/21], [94mLoss[0m : 4.77237
[1mStep[0m  [20/21], [94mLoss[0m : 4.88172

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 4.662, [92mTest[0m: 5.179, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.38297
[1mStep[0m  [2/21], [94mLoss[0m : 4.68910
[1mStep[0m  [4/21], [94mLoss[0m : 4.67950
[1mStep[0m  [6/21], [94mLoss[0m : 4.55460
[1mStep[0m  [8/21], [94mLoss[0m : 4.59896
[1mStep[0m  [10/21], [94mLoss[0m : 4.58350
[1mStep[0m  [12/21], [94mLoss[0m : 4.34399
[1mStep[0m  [14/21], [94mLoss[0m : 4.33383
[1mStep[0m  [16/21], [94mLoss[0m : 4.17170
[1mStep[0m  [18/21], [94mLoss[0m : 4.52756
[1mStep[0m  [20/21], [94mLoss[0m : 4.77345

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 4.500, [92mTest[0m: 4.439, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.13791
[1mStep[0m  [2/21], [94mLoss[0m : 4.25235
[1mStep[0m  [4/21], [94mLoss[0m : 4.38759
[1mStep[0m  [6/21], [94mLoss[0m : 4.47279
[1mStep[0m  [8/21], [94mLoss[0m : 4.56774
[1mStep[0m  [10/21], [94mLoss[0m : 4.21906
[1mStep[0m  [12/21], [94mLoss[0m : 4.62811
[1mStep[0m  [14/21], [94mLoss[0m : 4.27232
[1mStep[0m  [16/21], [94mLoss[0m : 4.29555
[1mStep[0m  [18/21], [94mLoss[0m : 4.32393
[1mStep[0m  [20/21], [94mLoss[0m : 4.10613

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 4.317, [92mTest[0m: 4.468, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.17346
[1mStep[0m  [2/21], [94mLoss[0m : 4.41484
[1mStep[0m  [4/21], [94mLoss[0m : 4.21026
[1mStep[0m  [6/21], [94mLoss[0m : 4.31814
[1mStep[0m  [8/21], [94mLoss[0m : 4.39071
[1mStep[0m  [10/21], [94mLoss[0m : 4.08053
[1mStep[0m  [12/21], [94mLoss[0m : 3.88156
[1mStep[0m  [14/21], [94mLoss[0m : 3.90026
[1mStep[0m  [16/21], [94mLoss[0m : 3.83335
[1mStep[0m  [18/21], [94mLoss[0m : 4.02561
[1mStep[0m  [20/21], [94mLoss[0m : 4.04229

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 4.150, [92mTest[0m: 3.971, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.93938
[1mStep[0m  [2/21], [94mLoss[0m : 3.85856
[1mStep[0m  [4/21], [94mLoss[0m : 3.93191
[1mStep[0m  [6/21], [94mLoss[0m : 3.80610
[1mStep[0m  [8/21], [94mLoss[0m : 3.81395
[1mStep[0m  [10/21], [94mLoss[0m : 4.01408
[1mStep[0m  [12/21], [94mLoss[0m : 4.11236
[1mStep[0m  [14/21], [94mLoss[0m : 3.98310
[1mStep[0m  [16/21], [94mLoss[0m : 4.09658
[1mStep[0m  [18/21], [94mLoss[0m : 4.23755
[1mStep[0m  [20/21], [94mLoss[0m : 4.15716

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 4.036, [92mTest[0m: 3.886, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.19691
[1mStep[0m  [2/21], [94mLoss[0m : 3.74928
[1mStep[0m  [4/21], [94mLoss[0m : 4.18563
[1mStep[0m  [6/21], [94mLoss[0m : 4.06669
[1mStep[0m  [8/21], [94mLoss[0m : 3.81993
[1mStep[0m  [10/21], [94mLoss[0m : 3.94261
[1mStep[0m  [12/21], [94mLoss[0m : 3.70258
[1mStep[0m  [14/21], [94mLoss[0m : 3.83394
[1mStep[0m  [16/21], [94mLoss[0m : 3.83297
[1mStep[0m  [18/21], [94mLoss[0m : 3.93745
[1mStep[0m  [20/21], [94mLoss[0m : 3.77618

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 3.890, [92mTest[0m: 6.936, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.55455
[1mStep[0m  [2/21], [94mLoss[0m : 3.79978
[1mStep[0m  [4/21], [94mLoss[0m : 3.82430
[1mStep[0m  [6/21], [94mLoss[0m : 3.85414
[1mStep[0m  [8/21], [94mLoss[0m : 3.64466
[1mStep[0m  [10/21], [94mLoss[0m : 3.67735
[1mStep[0m  [12/21], [94mLoss[0m : 3.81513
[1mStep[0m  [14/21], [94mLoss[0m : 3.76825
[1mStep[0m  [16/21], [94mLoss[0m : 3.63815
[1mStep[0m  [18/21], [94mLoss[0m : 3.83777
[1mStep[0m  [20/21], [94mLoss[0m : 3.72785

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 3.745, [92mTest[0m: 3.561, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.57070
[1mStep[0m  [2/21], [94mLoss[0m : 3.79385
[1mStep[0m  [4/21], [94mLoss[0m : 3.47189
[1mStep[0m  [6/21], [94mLoss[0m : 3.48450
[1mStep[0m  [8/21], [94mLoss[0m : 3.72004
[1mStep[0m  [10/21], [94mLoss[0m : 3.89804
[1mStep[0m  [12/21], [94mLoss[0m : 3.51907
[1mStep[0m  [14/21], [94mLoss[0m : 3.39188
[1mStep[0m  [16/21], [94mLoss[0m : 3.55153
[1mStep[0m  [18/21], [94mLoss[0m : 3.59239
[1mStep[0m  [20/21], [94mLoss[0m : 3.72420

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 3.627, [92mTest[0m: 3.326, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.51655
[1mStep[0m  [2/21], [94mLoss[0m : 3.73069
[1mStep[0m  [4/21], [94mLoss[0m : 3.53972
[1mStep[0m  [6/21], [94mLoss[0m : 3.42271
[1mStep[0m  [8/21], [94mLoss[0m : 3.39636
[1mStep[0m  [10/21], [94mLoss[0m : 3.39740
[1mStep[0m  [12/21], [94mLoss[0m : 3.31214
[1mStep[0m  [14/21], [94mLoss[0m : 3.44966
[1mStep[0m  [16/21], [94mLoss[0m : 3.43588
[1mStep[0m  [18/21], [94mLoss[0m : 3.40541
[1mStep[0m  [20/21], [94mLoss[0m : 3.40849

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 3.489, [92mTest[0m: 3.912, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.47975
[1mStep[0m  [2/21], [94mLoss[0m : 3.41447
[1mStep[0m  [4/21], [94mLoss[0m : 3.30390
[1mStep[0m  [6/21], [94mLoss[0m : 3.31327
[1mStep[0m  [8/21], [94mLoss[0m : 3.35979
[1mStep[0m  [10/21], [94mLoss[0m : 3.28073
[1mStep[0m  [12/21], [94mLoss[0m : 3.38488
[1mStep[0m  [14/21], [94mLoss[0m : 3.54439
[1mStep[0m  [16/21], [94mLoss[0m : 3.27307
[1mStep[0m  [18/21], [94mLoss[0m : 3.18586
[1mStep[0m  [20/21], [94mLoss[0m : 3.26290

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 3.352, [92mTest[0m: 3.169, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.29041
[1mStep[0m  [2/21], [94mLoss[0m : 3.41440
[1mStep[0m  [4/21], [94mLoss[0m : 3.22979
[1mStep[0m  [6/21], [94mLoss[0m : 3.23792
[1mStep[0m  [8/21], [94mLoss[0m : 3.36332
[1mStep[0m  [10/21], [94mLoss[0m : 3.36804
[1mStep[0m  [12/21], [94mLoss[0m : 3.32680
[1mStep[0m  [14/21], [94mLoss[0m : 3.25737
[1mStep[0m  [16/21], [94mLoss[0m : 3.19930
[1mStep[0m  [18/21], [94mLoss[0m : 3.17677
[1mStep[0m  [20/21], [94mLoss[0m : 3.19751

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 3.252, [92mTest[0m: 3.086, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.47136
[1mStep[0m  [2/21], [94mLoss[0m : 3.13120
[1mStep[0m  [4/21], [94mLoss[0m : 3.19432
[1mStep[0m  [6/21], [94mLoss[0m : 3.08998
[1mStep[0m  [8/21], [94mLoss[0m : 3.22340
[1mStep[0m  [10/21], [94mLoss[0m : 2.92602
[1mStep[0m  [12/21], [94mLoss[0m : 3.26883
[1mStep[0m  [14/21], [94mLoss[0m : 3.11666
[1mStep[0m  [16/21], [94mLoss[0m : 3.17655
[1mStep[0m  [18/21], [94mLoss[0m : 3.24594
[1mStep[0m  [20/21], [94mLoss[0m : 3.09689

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 3.182, [92mTest[0m: 3.167, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.13942
[1mStep[0m  [2/21], [94mLoss[0m : 3.36378
[1mStep[0m  [4/21], [94mLoss[0m : 3.00140
[1mStep[0m  [6/21], [94mLoss[0m : 3.24623
[1mStep[0m  [8/21], [94mLoss[0m : 2.90963
[1mStep[0m  [10/21], [94mLoss[0m : 2.90671
[1mStep[0m  [12/21], [94mLoss[0m : 3.11863
[1mStep[0m  [14/21], [94mLoss[0m : 2.99624
[1mStep[0m  [16/21], [94mLoss[0m : 3.16521
[1mStep[0m  [18/21], [94mLoss[0m : 3.04933
[1mStep[0m  [20/21], [94mLoss[0m : 2.97577

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 3.071, [92mTest[0m: 2.889, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.01037
[1mStep[0m  [2/21], [94mLoss[0m : 3.18032
[1mStep[0m  [4/21], [94mLoss[0m : 3.02017
[1mStep[0m  [6/21], [94mLoss[0m : 3.08639
[1mStep[0m  [8/21], [94mLoss[0m : 2.89050
[1mStep[0m  [10/21], [94mLoss[0m : 2.97936
[1mStep[0m  [12/21], [94mLoss[0m : 2.97264
[1mStep[0m  [14/21], [94mLoss[0m : 2.96099
[1mStep[0m  [16/21], [94mLoss[0m : 2.90902
[1mStep[0m  [18/21], [94mLoss[0m : 3.03859
[1mStep[0m  [20/21], [94mLoss[0m : 2.89538

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.972, [92mTest[0m: 2.908, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.93671
[1mStep[0m  [2/21], [94mLoss[0m : 2.99953
[1mStep[0m  [4/21], [94mLoss[0m : 3.01631
[1mStep[0m  [6/21], [94mLoss[0m : 3.00944
[1mStep[0m  [8/21], [94mLoss[0m : 3.15746
[1mStep[0m  [10/21], [94mLoss[0m : 2.64806
[1mStep[0m  [12/21], [94mLoss[0m : 2.83727
[1mStep[0m  [14/21], [94mLoss[0m : 2.97021
[1mStep[0m  [16/21], [94mLoss[0m : 2.75370
[1mStep[0m  [18/21], [94mLoss[0m : 2.86684
[1mStep[0m  [20/21], [94mLoss[0m : 2.95830

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.920, [92mTest[0m: 2.915, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.80370
[1mStep[0m  [2/21], [94mLoss[0m : 2.71999
[1mStep[0m  [4/21], [94mLoss[0m : 2.86644
[1mStep[0m  [6/21], [94mLoss[0m : 2.86779
[1mStep[0m  [8/21], [94mLoss[0m : 2.60944
[1mStep[0m  [10/21], [94mLoss[0m : 2.94182
[1mStep[0m  [12/21], [94mLoss[0m : 2.74094
[1mStep[0m  [14/21], [94mLoss[0m : 2.51992
[1mStep[0m  [16/21], [94mLoss[0m : 2.87196
[1mStep[0m  [18/21], [94mLoss[0m : 2.91427
[1mStep[0m  [20/21], [94mLoss[0m : 2.79349

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.810, [92mTest[0m: 2.758, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.736
====================================

Phase 2 - Evaluation MAE:  2.7355960437229703
MAE score P1      6.403255
MAE score P2      2.735596
loss              2.809649
learning_rate     0.002575
batch_size             512
hidden_sizes         [250]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.1
weight_decay          0.01
Name: 4, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 10.79828
[1mStep[0m  [4/42], [94mLoss[0m : 10.82669
[1mStep[0m  [8/42], [94mLoss[0m : 10.65201
[1mStep[0m  [12/42], [94mLoss[0m : 10.63698
[1mStep[0m  [16/42], [94mLoss[0m : 10.48603
[1mStep[0m  [20/42], [94mLoss[0m : 10.60725
[1mStep[0m  [24/42], [94mLoss[0m : 10.56083
[1mStep[0m  [28/42], [94mLoss[0m : 10.07644
[1mStep[0m  [32/42], [94mLoss[0m : 9.88436
[1mStep[0m  [36/42], [94mLoss[0m : 9.78757
[1mStep[0m  [40/42], [94mLoss[0m : 9.70008

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.351, [92mTest[0m: 10.898, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.52321
[1mStep[0m  [4/42], [94mLoss[0m : 9.16945
[1mStep[0m  [8/42], [94mLoss[0m : 9.04961
[1mStep[0m  [12/42], [94mLoss[0m : 8.85545
[1mStep[0m  [16/42], [94mLoss[0m : 8.59349
[1mStep[0m  [20/42], [94mLoss[0m : 8.24297
[1mStep[0m  [24/42], [94mLoss[0m : 8.17878
[1mStep[0m  [28/42], [94mLoss[0m : 7.78317
[1mStep[0m  [32/42], [94mLoss[0m : 7.80562
[1mStep[0m  [36/42], [94mLoss[0m : 7.42177
[1mStep[0m  [40/42], [94mLoss[0m : 7.00200

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 8.299, [92mTest[0m: 9.318, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.42655
[1mStep[0m  [4/42], [94mLoss[0m : 6.98723
[1mStep[0m  [8/42], [94mLoss[0m : 7.05516
[1mStep[0m  [12/42], [94mLoss[0m : 6.55137
[1mStep[0m  [16/42], [94mLoss[0m : 6.73744
[1mStep[0m  [20/42], [94mLoss[0m : 6.16084
[1mStep[0m  [24/42], [94mLoss[0m : 6.37974
[1mStep[0m  [28/42], [94mLoss[0m : 6.03392
[1mStep[0m  [32/42], [94mLoss[0m : 6.20742
[1mStep[0m  [36/42], [94mLoss[0m : 5.76124
[1mStep[0m  [40/42], [94mLoss[0m : 5.76061

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 6.344, [92mTest[0m: 7.055, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.51263
[1mStep[0m  [4/42], [94mLoss[0m : 5.34293
[1mStep[0m  [8/42], [94mLoss[0m : 5.21265
[1mStep[0m  [12/42], [94mLoss[0m : 4.94462
[1mStep[0m  [16/42], [94mLoss[0m : 4.54790
[1mStep[0m  [20/42], [94mLoss[0m : 3.88653
[1mStep[0m  [24/42], [94mLoss[0m : 4.43563
[1mStep[0m  [28/42], [94mLoss[0m : 3.87390
[1mStep[0m  [32/42], [94mLoss[0m : 3.78472
[1mStep[0m  [36/42], [94mLoss[0m : 3.54577
[1mStep[0m  [40/42], [94mLoss[0m : 3.64062

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 4.417, [92mTest[0m: 4.273, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.26539
[1mStep[0m  [4/42], [94mLoss[0m : 2.79383
[1mStep[0m  [8/42], [94mLoss[0m : 2.98157
[1mStep[0m  [12/42], [94mLoss[0m : 2.92626
[1mStep[0m  [16/42], [94mLoss[0m : 2.85166
[1mStep[0m  [20/42], [94mLoss[0m : 2.64746
[1mStep[0m  [24/42], [94mLoss[0m : 2.52582
[1mStep[0m  [28/42], [94mLoss[0m : 2.63891
[1mStep[0m  [32/42], [94mLoss[0m : 2.41746
[1mStep[0m  [36/42], [94mLoss[0m : 2.55360
[1mStep[0m  [40/42], [94mLoss[0m : 2.86861

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.789, [92mTest[0m: 2.733, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.87539
[1mStep[0m  [4/42], [94mLoss[0m : 2.54700
[1mStep[0m  [8/42], [94mLoss[0m : 2.56592
[1mStep[0m  [12/42], [94mLoss[0m : 2.85133
[1mStep[0m  [16/42], [94mLoss[0m : 2.67049
[1mStep[0m  [20/42], [94mLoss[0m : 2.61721
[1mStep[0m  [24/42], [94mLoss[0m : 2.67524
[1mStep[0m  [28/42], [94mLoss[0m : 2.43156
[1mStep[0m  [32/42], [94mLoss[0m : 2.45482
[1mStep[0m  [36/42], [94mLoss[0m : 2.64717
[1mStep[0m  [40/42], [94mLoss[0m : 2.59329

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.597, [92mTest[0m: 2.561, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43353
[1mStep[0m  [4/42], [94mLoss[0m : 2.57487
[1mStep[0m  [8/42], [94mLoss[0m : 2.56674
[1mStep[0m  [12/42], [94mLoss[0m : 2.66736
[1mStep[0m  [16/42], [94mLoss[0m : 2.58799
[1mStep[0m  [20/42], [94mLoss[0m : 2.84664
[1mStep[0m  [24/42], [94mLoss[0m : 2.53279
[1mStep[0m  [28/42], [94mLoss[0m : 2.56781
[1mStep[0m  [32/42], [94mLoss[0m : 2.39816
[1mStep[0m  [36/42], [94mLoss[0m : 2.45246
[1mStep[0m  [40/42], [94mLoss[0m : 2.55772

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.581, [92mTest[0m: 2.471, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64838
[1mStep[0m  [4/42], [94mLoss[0m : 2.54721
[1mStep[0m  [8/42], [94mLoss[0m : 2.53300
[1mStep[0m  [12/42], [94mLoss[0m : 2.37248
[1mStep[0m  [16/42], [94mLoss[0m : 2.69475
[1mStep[0m  [20/42], [94mLoss[0m : 2.63748
[1mStep[0m  [24/42], [94mLoss[0m : 2.69712
[1mStep[0m  [28/42], [94mLoss[0m : 2.67402
[1mStep[0m  [32/42], [94mLoss[0m : 2.58912
[1mStep[0m  [36/42], [94mLoss[0m : 2.33228
[1mStep[0m  [40/42], [94mLoss[0m : 2.25348

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.542, [92mTest[0m: 2.451, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45705
[1mStep[0m  [4/42], [94mLoss[0m : 2.55054
[1mStep[0m  [8/42], [94mLoss[0m : 2.48779
[1mStep[0m  [12/42], [94mLoss[0m : 2.42422
[1mStep[0m  [16/42], [94mLoss[0m : 2.49217
[1mStep[0m  [20/42], [94mLoss[0m : 2.72030
[1mStep[0m  [24/42], [94mLoss[0m : 2.42641
[1mStep[0m  [28/42], [94mLoss[0m : 2.43326
[1mStep[0m  [32/42], [94mLoss[0m : 2.56798
[1mStep[0m  [36/42], [94mLoss[0m : 2.57712
[1mStep[0m  [40/42], [94mLoss[0m : 2.66239

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.431, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55804
[1mStep[0m  [4/42], [94mLoss[0m : 2.51730
[1mStep[0m  [8/42], [94mLoss[0m : 2.48347
[1mStep[0m  [12/42], [94mLoss[0m : 2.31120
[1mStep[0m  [16/42], [94mLoss[0m : 2.34040
[1mStep[0m  [20/42], [94mLoss[0m : 2.43721
[1mStep[0m  [24/42], [94mLoss[0m : 2.75168
[1mStep[0m  [28/42], [94mLoss[0m : 2.56074
[1mStep[0m  [32/42], [94mLoss[0m : 2.60714
[1mStep[0m  [36/42], [94mLoss[0m : 2.33502
[1mStep[0m  [40/42], [94mLoss[0m : 2.66673

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.351, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56300
[1mStep[0m  [4/42], [94mLoss[0m : 2.30708
[1mStep[0m  [8/42], [94mLoss[0m : 2.60164
[1mStep[0m  [12/42], [94mLoss[0m : 2.48675
[1mStep[0m  [16/42], [94mLoss[0m : 2.56554
[1mStep[0m  [20/42], [94mLoss[0m : 2.27749
[1mStep[0m  [24/42], [94mLoss[0m : 2.45478
[1mStep[0m  [28/42], [94mLoss[0m : 2.53074
[1mStep[0m  [32/42], [94mLoss[0m : 2.63729
[1mStep[0m  [36/42], [94mLoss[0m : 2.59624
[1mStep[0m  [40/42], [94mLoss[0m : 2.88246

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.386, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.19980
[1mStep[0m  [4/42], [94mLoss[0m : 2.54906
[1mStep[0m  [8/42], [94mLoss[0m : 2.43414
[1mStep[0m  [12/42], [94mLoss[0m : 2.40450
[1mStep[0m  [16/42], [94mLoss[0m : 2.38232
[1mStep[0m  [20/42], [94mLoss[0m : 2.49345
[1mStep[0m  [24/42], [94mLoss[0m : 2.42181
[1mStep[0m  [28/42], [94mLoss[0m : 2.53004
[1mStep[0m  [32/42], [94mLoss[0m : 2.41872
[1mStep[0m  [36/42], [94mLoss[0m : 2.50086
[1mStep[0m  [40/42], [94mLoss[0m : 2.61177

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.366, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50447
[1mStep[0m  [4/42], [94mLoss[0m : 2.31708
[1mStep[0m  [8/42], [94mLoss[0m : 2.39490
[1mStep[0m  [12/42], [94mLoss[0m : 2.40754
[1mStep[0m  [16/42], [94mLoss[0m : 2.39826
[1mStep[0m  [20/42], [94mLoss[0m : 2.63415
[1mStep[0m  [24/42], [94mLoss[0m : 2.20836
[1mStep[0m  [28/42], [94mLoss[0m : 2.49743
[1mStep[0m  [32/42], [94mLoss[0m : 2.37073
[1mStep[0m  [36/42], [94mLoss[0m : 2.14365
[1mStep[0m  [40/42], [94mLoss[0m : 2.41489

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.380, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38130
[1mStep[0m  [4/42], [94mLoss[0m : 2.62475
[1mStep[0m  [8/42], [94mLoss[0m : 2.33034
[1mStep[0m  [12/42], [94mLoss[0m : 2.34034
[1mStep[0m  [16/42], [94mLoss[0m : 2.50437
[1mStep[0m  [20/42], [94mLoss[0m : 2.12827
[1mStep[0m  [24/42], [94mLoss[0m : 2.48099
[1mStep[0m  [28/42], [94mLoss[0m : 2.75733
[1mStep[0m  [32/42], [94mLoss[0m : 2.33674
[1mStep[0m  [36/42], [94mLoss[0m : 2.33997
[1mStep[0m  [40/42], [94mLoss[0m : 2.51629

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.400, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42981
[1mStep[0m  [4/42], [94mLoss[0m : 2.50726
[1mStep[0m  [8/42], [94mLoss[0m : 2.33313
[1mStep[0m  [12/42], [94mLoss[0m : 2.39321
[1mStep[0m  [16/42], [94mLoss[0m : 2.52802
[1mStep[0m  [20/42], [94mLoss[0m : 2.39680
[1mStep[0m  [24/42], [94mLoss[0m : 2.55466
[1mStep[0m  [28/42], [94mLoss[0m : 2.40184
[1mStep[0m  [32/42], [94mLoss[0m : 2.60869
[1mStep[0m  [36/42], [94mLoss[0m : 2.50967
[1mStep[0m  [40/42], [94mLoss[0m : 2.26785

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.381, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38261
[1mStep[0m  [4/42], [94mLoss[0m : 2.53480
[1mStep[0m  [8/42], [94mLoss[0m : 2.49603
[1mStep[0m  [12/42], [94mLoss[0m : 2.39764
[1mStep[0m  [16/42], [94mLoss[0m : 2.54159
[1mStep[0m  [20/42], [94mLoss[0m : 2.27107
[1mStep[0m  [24/42], [94mLoss[0m : 2.56608
[1mStep[0m  [28/42], [94mLoss[0m : 2.32393
[1mStep[0m  [32/42], [94mLoss[0m : 2.34917
[1mStep[0m  [36/42], [94mLoss[0m : 2.43116
[1mStep[0m  [40/42], [94mLoss[0m : 2.59633

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.358, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44722
[1mStep[0m  [4/42], [94mLoss[0m : 2.19459
[1mStep[0m  [8/42], [94mLoss[0m : 2.48112
[1mStep[0m  [12/42], [94mLoss[0m : 2.19979
[1mStep[0m  [16/42], [94mLoss[0m : 2.61328
[1mStep[0m  [20/42], [94mLoss[0m : 2.36139
[1mStep[0m  [24/42], [94mLoss[0m : 2.61241
[1mStep[0m  [28/42], [94mLoss[0m : 2.28824
[1mStep[0m  [32/42], [94mLoss[0m : 2.43570
[1mStep[0m  [36/42], [94mLoss[0m : 2.31841
[1mStep[0m  [40/42], [94mLoss[0m : 2.45652

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.387, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46343
[1mStep[0m  [4/42], [94mLoss[0m : 2.41550
[1mStep[0m  [8/42], [94mLoss[0m : 2.39018
[1mStep[0m  [12/42], [94mLoss[0m : 2.41176
[1mStep[0m  [16/42], [94mLoss[0m : 2.43394
[1mStep[0m  [20/42], [94mLoss[0m : 2.49084
[1mStep[0m  [24/42], [94mLoss[0m : 2.23199
[1mStep[0m  [28/42], [94mLoss[0m : 2.61925
[1mStep[0m  [32/42], [94mLoss[0m : 2.31772
[1mStep[0m  [36/42], [94mLoss[0m : 2.61481
[1mStep[0m  [40/42], [94mLoss[0m : 2.52378

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.406, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44266
[1mStep[0m  [4/42], [94mLoss[0m : 2.36500
[1mStep[0m  [8/42], [94mLoss[0m : 2.68248
[1mStep[0m  [12/42], [94mLoss[0m : 2.33464
[1mStep[0m  [16/42], [94mLoss[0m : 2.44799
[1mStep[0m  [20/42], [94mLoss[0m : 2.32013
[1mStep[0m  [24/42], [94mLoss[0m : 2.49003
[1mStep[0m  [28/42], [94mLoss[0m : 2.34862
[1mStep[0m  [32/42], [94mLoss[0m : 2.25459
[1mStep[0m  [36/42], [94mLoss[0m : 2.20778
[1mStep[0m  [40/42], [94mLoss[0m : 2.48599

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.396, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42150
[1mStep[0m  [4/42], [94mLoss[0m : 2.34947
[1mStep[0m  [8/42], [94mLoss[0m : 2.45477
[1mStep[0m  [12/42], [94mLoss[0m : 2.71108
[1mStep[0m  [16/42], [94mLoss[0m : 2.49740
[1mStep[0m  [20/42], [94mLoss[0m : 2.16626
[1mStep[0m  [24/42], [94mLoss[0m : 2.40186
[1mStep[0m  [28/42], [94mLoss[0m : 2.39971
[1mStep[0m  [32/42], [94mLoss[0m : 2.27509
[1mStep[0m  [36/42], [94mLoss[0m : 2.41768
[1mStep[0m  [40/42], [94mLoss[0m : 2.42683

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.359, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37208
[1mStep[0m  [4/42], [94mLoss[0m : 2.40803
[1mStep[0m  [8/42], [94mLoss[0m : 2.40724
[1mStep[0m  [12/42], [94mLoss[0m : 2.50007
[1mStep[0m  [16/42], [94mLoss[0m : 2.46071
[1mStep[0m  [20/42], [94mLoss[0m : 2.25202
[1mStep[0m  [24/42], [94mLoss[0m : 2.59620
[1mStep[0m  [28/42], [94mLoss[0m : 2.36546
[1mStep[0m  [32/42], [94mLoss[0m : 2.45609
[1mStep[0m  [36/42], [94mLoss[0m : 2.52481
[1mStep[0m  [40/42], [94mLoss[0m : 2.60969

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.395, [92mTest[0m: 2.347, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38581
[1mStep[0m  [4/42], [94mLoss[0m : 2.36366
[1mStep[0m  [8/42], [94mLoss[0m : 2.45500
[1mStep[0m  [12/42], [94mLoss[0m : 2.24376
[1mStep[0m  [16/42], [94mLoss[0m : 2.31196
[1mStep[0m  [20/42], [94mLoss[0m : 2.05217
[1mStep[0m  [24/42], [94mLoss[0m : 2.17557
[1mStep[0m  [28/42], [94mLoss[0m : 2.13936
[1mStep[0m  [32/42], [94mLoss[0m : 2.35750
[1mStep[0m  [36/42], [94mLoss[0m : 2.58816
[1mStep[0m  [40/42], [94mLoss[0m : 2.40456

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.387, [92mTest[0m: 2.383, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54533
[1mStep[0m  [4/42], [94mLoss[0m : 2.29500
[1mStep[0m  [8/42], [94mLoss[0m : 2.25184
[1mStep[0m  [12/42], [94mLoss[0m : 2.45419
[1mStep[0m  [16/42], [94mLoss[0m : 2.56776
[1mStep[0m  [20/42], [94mLoss[0m : 2.32835
[1mStep[0m  [24/42], [94mLoss[0m : 2.41669
[1mStep[0m  [28/42], [94mLoss[0m : 2.37706
[1mStep[0m  [32/42], [94mLoss[0m : 2.55892
[1mStep[0m  [36/42], [94mLoss[0m : 2.39572
[1mStep[0m  [40/42], [94mLoss[0m : 2.68327

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.356, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48778
[1mStep[0m  [4/42], [94mLoss[0m : 2.38190
[1mStep[0m  [8/42], [94mLoss[0m : 2.45909
[1mStep[0m  [12/42], [94mLoss[0m : 2.32954
[1mStep[0m  [16/42], [94mLoss[0m : 2.33158
[1mStep[0m  [20/42], [94mLoss[0m : 2.54123
[1mStep[0m  [24/42], [94mLoss[0m : 2.11055
[1mStep[0m  [28/42], [94mLoss[0m : 2.50163
[1mStep[0m  [32/42], [94mLoss[0m : 2.21979
[1mStep[0m  [36/42], [94mLoss[0m : 2.35652
[1mStep[0m  [40/42], [94mLoss[0m : 2.56171

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.374, [92mTest[0m: 2.366, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21134
[1mStep[0m  [4/42], [94mLoss[0m : 2.37031
[1mStep[0m  [8/42], [94mLoss[0m : 2.37192
[1mStep[0m  [12/42], [94mLoss[0m : 2.37513
[1mStep[0m  [16/42], [94mLoss[0m : 2.45982
[1mStep[0m  [20/42], [94mLoss[0m : 2.18789
[1mStep[0m  [24/42], [94mLoss[0m : 2.34600
[1mStep[0m  [28/42], [94mLoss[0m : 2.20442
[1mStep[0m  [32/42], [94mLoss[0m : 2.55526
[1mStep[0m  [36/42], [94mLoss[0m : 2.29783
[1mStep[0m  [40/42], [94mLoss[0m : 2.44526

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.387, [92mTest[0m: 2.344, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27129
[1mStep[0m  [4/42], [94mLoss[0m : 2.29721
[1mStep[0m  [8/42], [94mLoss[0m : 2.49122
[1mStep[0m  [12/42], [94mLoss[0m : 2.41428
[1mStep[0m  [16/42], [94mLoss[0m : 2.41701
[1mStep[0m  [20/42], [94mLoss[0m : 2.19202
[1mStep[0m  [24/42], [94mLoss[0m : 2.33480
[1mStep[0m  [28/42], [94mLoss[0m : 2.65728
[1mStep[0m  [32/42], [94mLoss[0m : 2.65923
[1mStep[0m  [36/42], [94mLoss[0m : 2.46941
[1mStep[0m  [40/42], [94mLoss[0m : 2.32971

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.388, [92mTest[0m: 2.376, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57196
[1mStep[0m  [4/42], [94mLoss[0m : 2.34627
[1mStep[0m  [8/42], [94mLoss[0m : 2.46279
[1mStep[0m  [12/42], [94mLoss[0m : 2.49525
[1mStep[0m  [16/42], [94mLoss[0m : 2.46917
[1mStep[0m  [20/42], [94mLoss[0m : 2.21943
[1mStep[0m  [24/42], [94mLoss[0m : 2.23918
[1mStep[0m  [28/42], [94mLoss[0m : 2.45058
[1mStep[0m  [32/42], [94mLoss[0m : 2.46848
[1mStep[0m  [36/42], [94mLoss[0m : 2.42618
[1mStep[0m  [40/42], [94mLoss[0m : 2.42518

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.387, [92mTest[0m: 2.342, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44721
[1mStep[0m  [4/42], [94mLoss[0m : 2.43404
[1mStep[0m  [8/42], [94mLoss[0m : 2.49008
[1mStep[0m  [12/42], [94mLoss[0m : 2.43082
[1mStep[0m  [16/42], [94mLoss[0m : 2.24109
[1mStep[0m  [20/42], [94mLoss[0m : 2.54663
[1mStep[0m  [24/42], [94mLoss[0m : 2.37215
[1mStep[0m  [28/42], [94mLoss[0m : 2.47947
[1mStep[0m  [32/42], [94mLoss[0m : 2.31494
[1mStep[0m  [36/42], [94mLoss[0m : 2.44178
[1mStep[0m  [40/42], [94mLoss[0m : 2.34939

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.377, [92mTest[0m: 2.360, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.14490
[1mStep[0m  [4/42], [94mLoss[0m : 2.60662
[1mStep[0m  [8/42], [94mLoss[0m : 2.36631
[1mStep[0m  [12/42], [94mLoss[0m : 2.56792
[1mStep[0m  [16/42], [94mLoss[0m : 2.30564
[1mStep[0m  [20/42], [94mLoss[0m : 2.67356
[1mStep[0m  [24/42], [94mLoss[0m : 2.17499
[1mStep[0m  [28/42], [94mLoss[0m : 2.58061
[1mStep[0m  [32/42], [94mLoss[0m : 2.11774
[1mStep[0m  [36/42], [94mLoss[0m : 2.50564
[1mStep[0m  [40/42], [94mLoss[0m : 2.52758

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.367, [92mTest[0m: 2.350, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32108
[1mStep[0m  [4/42], [94mLoss[0m : 2.35979
[1mStep[0m  [8/42], [94mLoss[0m : 2.36695
[1mStep[0m  [12/42], [94mLoss[0m : 2.26664
[1mStep[0m  [16/42], [94mLoss[0m : 2.26199
[1mStep[0m  [20/42], [94mLoss[0m : 2.36035
[1mStep[0m  [24/42], [94mLoss[0m : 2.33541
[1mStep[0m  [28/42], [94mLoss[0m : 2.29534
[1mStep[0m  [32/42], [94mLoss[0m : 2.55152
[1mStep[0m  [36/42], [94mLoss[0m : 2.37529
[1mStep[0m  [40/42], [94mLoss[0m : 2.29475

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.374, [92mTest[0m: 2.356, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.342
====================================

Phase 1 - Evaluation MAE:  2.3417297261101857
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 2.38504
[1mStep[0m  [4/42], [94mLoss[0m : 2.32410
[1mStep[0m  [8/42], [94mLoss[0m : 2.31052
[1mStep[0m  [12/42], [94mLoss[0m : 2.60886
[1mStep[0m  [16/42], [94mLoss[0m : 2.58142
[1mStep[0m  [20/42], [94mLoss[0m : 2.49561
[1mStep[0m  [24/42], [94mLoss[0m : 2.45839
[1mStep[0m  [28/42], [94mLoss[0m : 2.18140
[1mStep[0m  [32/42], [94mLoss[0m : 2.45517
[1mStep[0m  [36/42], [94mLoss[0m : 2.47518
[1mStep[0m  [40/42], [94mLoss[0m : 2.56742

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.346, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44129
[1mStep[0m  [4/42], [94mLoss[0m : 2.25755
[1mStep[0m  [8/42], [94mLoss[0m : 2.52406
[1mStep[0m  [12/42], [94mLoss[0m : 2.45549
[1mStep[0m  [16/42], [94mLoss[0m : 2.47488
[1mStep[0m  [20/42], [94mLoss[0m : 2.38855
[1mStep[0m  [24/42], [94mLoss[0m : 2.28204
[1mStep[0m  [28/42], [94mLoss[0m : 2.22433
[1mStep[0m  [32/42], [94mLoss[0m : 2.25337
[1mStep[0m  [36/42], [94mLoss[0m : 2.32672
[1mStep[0m  [40/42], [94mLoss[0m : 2.30810

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.356, [92mTest[0m: 2.435, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.11993
[1mStep[0m  [4/42], [94mLoss[0m : 2.54200
[1mStep[0m  [8/42], [94mLoss[0m : 2.25404
[1mStep[0m  [12/42], [94mLoss[0m : 2.22076
[1mStep[0m  [16/42], [94mLoss[0m : 2.36779
[1mStep[0m  [20/42], [94mLoss[0m : 2.48558
[1mStep[0m  [24/42], [94mLoss[0m : 2.26139
[1mStep[0m  [28/42], [94mLoss[0m : 2.41299
[1mStep[0m  [32/42], [94mLoss[0m : 2.27031
[1mStep[0m  [36/42], [94mLoss[0m : 2.30035
[1mStep[0m  [40/42], [94mLoss[0m : 2.34336

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.298, [92mTest[0m: 2.463, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.14797
[1mStep[0m  [4/42], [94mLoss[0m : 2.32570
[1mStep[0m  [8/42], [94mLoss[0m : 2.18706
[1mStep[0m  [12/42], [94mLoss[0m : 2.39791
[1mStep[0m  [16/42], [94mLoss[0m : 2.18141
[1mStep[0m  [20/42], [94mLoss[0m : 2.06832
[1mStep[0m  [24/42], [94mLoss[0m : 2.44473
[1mStep[0m  [28/42], [94mLoss[0m : 2.28486
[1mStep[0m  [32/42], [94mLoss[0m : 2.28562
[1mStep[0m  [36/42], [94mLoss[0m : 2.20503
[1mStep[0m  [40/42], [94mLoss[0m : 2.21526

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.236, [92mTest[0m: 2.474, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.22949
[1mStep[0m  [4/42], [94mLoss[0m : 2.12286
[1mStep[0m  [8/42], [94mLoss[0m : 1.99699
[1mStep[0m  [12/42], [94mLoss[0m : 2.16704
[1mStep[0m  [16/42], [94mLoss[0m : 2.35766
[1mStep[0m  [20/42], [94mLoss[0m : 2.04265
[1mStep[0m  [24/42], [94mLoss[0m : 2.38405
[1mStep[0m  [28/42], [94mLoss[0m : 2.10465
[1mStep[0m  [32/42], [94mLoss[0m : 2.29117
[1mStep[0m  [36/42], [94mLoss[0m : 2.21381
[1mStep[0m  [40/42], [94mLoss[0m : 1.93123

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.171, [92mTest[0m: 2.491, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.18815
[1mStep[0m  [4/42], [94mLoss[0m : 2.05993
[1mStep[0m  [8/42], [94mLoss[0m : 2.02152
[1mStep[0m  [12/42], [94mLoss[0m : 2.14371
[1mStep[0m  [16/42], [94mLoss[0m : 2.14382
[1mStep[0m  [20/42], [94mLoss[0m : 2.11934
[1mStep[0m  [24/42], [94mLoss[0m : 1.91803
[1mStep[0m  [28/42], [94mLoss[0m : 2.10576
[1mStep[0m  [32/42], [94mLoss[0m : 2.38715
[1mStep[0m  [36/42], [94mLoss[0m : 2.26080
[1mStep[0m  [40/42], [94mLoss[0m : 2.26392

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.122, [92mTest[0m: 2.474, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.91416
[1mStep[0m  [4/42], [94mLoss[0m : 1.94812
[1mStep[0m  [8/42], [94mLoss[0m : 1.92496
[1mStep[0m  [12/42], [94mLoss[0m : 2.05131
[1mStep[0m  [16/42], [94mLoss[0m : 2.04565
[1mStep[0m  [20/42], [94mLoss[0m : 2.12007
[1mStep[0m  [24/42], [94mLoss[0m : 1.93766
[1mStep[0m  [28/42], [94mLoss[0m : 1.94491
[1mStep[0m  [32/42], [94mLoss[0m : 2.06739
[1mStep[0m  [36/42], [94mLoss[0m : 1.95738
[1mStep[0m  [40/42], [94mLoss[0m : 2.05231

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.038, [92mTest[0m: 2.836, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.15948
[1mStep[0m  [4/42], [94mLoss[0m : 1.82767
[1mStep[0m  [8/42], [94mLoss[0m : 2.21179
[1mStep[0m  [12/42], [94mLoss[0m : 2.00082
[1mStep[0m  [16/42], [94mLoss[0m : 1.72823
[1mStep[0m  [20/42], [94mLoss[0m : 1.90905
[1mStep[0m  [24/42], [94mLoss[0m : 1.93620
[1mStep[0m  [28/42], [94mLoss[0m : 2.05048
[1mStep[0m  [32/42], [94mLoss[0m : 2.20675
[1mStep[0m  [36/42], [94mLoss[0m : 1.97124
[1mStep[0m  [40/42], [94mLoss[0m : 1.90745

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.981, [92mTest[0m: 2.506, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.71722
[1mStep[0m  [4/42], [94mLoss[0m : 1.83425
[1mStep[0m  [8/42], [94mLoss[0m : 1.66435
[1mStep[0m  [12/42], [94mLoss[0m : 1.85464
[1mStep[0m  [16/42], [94mLoss[0m : 2.01753
[1mStep[0m  [20/42], [94mLoss[0m : 1.87587
[1mStep[0m  [24/42], [94mLoss[0m : 1.98664
[1mStep[0m  [28/42], [94mLoss[0m : 1.98699
[1mStep[0m  [32/42], [94mLoss[0m : 2.17630
[1mStep[0m  [36/42], [94mLoss[0m : 2.09987
[1mStep[0m  [40/42], [94mLoss[0m : 1.91116

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.929, [92mTest[0m: 2.467, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.95771
[1mStep[0m  [4/42], [94mLoss[0m : 1.81826
[1mStep[0m  [8/42], [94mLoss[0m : 1.79459
[1mStep[0m  [12/42], [94mLoss[0m : 2.03568
[1mStep[0m  [16/42], [94mLoss[0m : 1.96936
[1mStep[0m  [20/42], [94mLoss[0m : 1.85935
[1mStep[0m  [24/42], [94mLoss[0m : 1.64612
[1mStep[0m  [28/42], [94mLoss[0m : 1.69681
[1mStep[0m  [32/42], [94mLoss[0m : 1.88343
[1mStep[0m  [36/42], [94mLoss[0m : 1.78612
[1mStep[0m  [40/42], [94mLoss[0m : 2.02904

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.876, [92mTest[0m: 2.557, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.67894
[1mStep[0m  [4/42], [94mLoss[0m : 1.75751
[1mStep[0m  [8/42], [94mLoss[0m : 1.82725
[1mStep[0m  [12/42], [94mLoss[0m : 1.88245
[1mStep[0m  [16/42], [94mLoss[0m : 1.79446
[1mStep[0m  [20/42], [94mLoss[0m : 1.91578
[1mStep[0m  [24/42], [94mLoss[0m : 1.76677
[1mStep[0m  [28/42], [94mLoss[0m : 1.84996
[1mStep[0m  [32/42], [94mLoss[0m : 1.77070
[1mStep[0m  [36/42], [94mLoss[0m : 1.75938
[1mStep[0m  [40/42], [94mLoss[0m : 1.68256

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.827, [92mTest[0m: 2.474, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.74007
[1mStep[0m  [4/42], [94mLoss[0m : 1.73860
[1mStep[0m  [8/42], [94mLoss[0m : 1.70021
[1mStep[0m  [12/42], [94mLoss[0m : 1.65613
[1mStep[0m  [16/42], [94mLoss[0m : 1.65913
[1mStep[0m  [20/42], [94mLoss[0m : 1.88836
[1mStep[0m  [24/42], [94mLoss[0m : 1.90749
[1mStep[0m  [28/42], [94mLoss[0m : 1.81373
[1mStep[0m  [32/42], [94mLoss[0m : 1.85948
[1mStep[0m  [36/42], [94mLoss[0m : 1.65584
[1mStep[0m  [40/42], [94mLoss[0m : 1.67777

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.795, [92mTest[0m: 2.497, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.65952
[1mStep[0m  [4/42], [94mLoss[0m : 1.70147
[1mStep[0m  [8/42], [94mLoss[0m : 1.79135
[1mStep[0m  [12/42], [94mLoss[0m : 1.61204
[1mStep[0m  [16/42], [94mLoss[0m : 1.69255
[1mStep[0m  [20/42], [94mLoss[0m : 1.73648
[1mStep[0m  [24/42], [94mLoss[0m : 1.91765
[1mStep[0m  [28/42], [94mLoss[0m : 1.77206
[1mStep[0m  [32/42], [94mLoss[0m : 1.94062
[1mStep[0m  [36/42], [94mLoss[0m : 1.69740
[1mStep[0m  [40/42], [94mLoss[0m : 1.79501

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.740, [92mTest[0m: 2.479, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.67081
[1mStep[0m  [4/42], [94mLoss[0m : 1.45669
[1mStep[0m  [8/42], [94mLoss[0m : 1.56043
[1mStep[0m  [12/42], [94mLoss[0m : 1.63502
[1mStep[0m  [16/42], [94mLoss[0m : 1.64409
[1mStep[0m  [20/42], [94mLoss[0m : 1.74074
[1mStep[0m  [24/42], [94mLoss[0m : 1.55917
[1mStep[0m  [28/42], [94mLoss[0m : 1.62047
[1mStep[0m  [32/42], [94mLoss[0m : 1.72155
[1mStep[0m  [36/42], [94mLoss[0m : 1.67136
[1mStep[0m  [40/42], [94mLoss[0m : 1.60880

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.684, [92mTest[0m: 2.528, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.50928
[1mStep[0m  [4/42], [94mLoss[0m : 1.66030
[1mStep[0m  [8/42], [94mLoss[0m : 1.54974
[1mStep[0m  [12/42], [94mLoss[0m : 1.57419
[1mStep[0m  [16/42], [94mLoss[0m : 1.63051
[1mStep[0m  [20/42], [94mLoss[0m : 1.60865
[1mStep[0m  [24/42], [94mLoss[0m : 1.61944
[1mStep[0m  [28/42], [94mLoss[0m : 1.51178
[1mStep[0m  [32/42], [94mLoss[0m : 1.64023
[1mStep[0m  [36/42], [94mLoss[0m : 1.69663
[1mStep[0m  [40/42], [94mLoss[0m : 1.62154

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.650, [92mTest[0m: 2.486, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.52019
[1mStep[0m  [4/42], [94mLoss[0m : 1.72993
[1mStep[0m  [8/42], [94mLoss[0m : 1.45554
[1mStep[0m  [12/42], [94mLoss[0m : 1.42683
[1mStep[0m  [16/42], [94mLoss[0m : 1.47430
[1mStep[0m  [20/42], [94mLoss[0m : 1.53255
[1mStep[0m  [24/42], [94mLoss[0m : 1.64539
[1mStep[0m  [28/42], [94mLoss[0m : 1.58827
[1mStep[0m  [32/42], [94mLoss[0m : 1.60162
[1mStep[0m  [36/42], [94mLoss[0m : 1.42319
[1mStep[0m  [40/42], [94mLoss[0m : 1.96426

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.611, [92mTest[0m: 2.469, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.41206
[1mStep[0m  [4/42], [94mLoss[0m : 1.61768
[1mStep[0m  [8/42], [94mLoss[0m : 1.73827
[1mStep[0m  [12/42], [94mLoss[0m : 1.54852
[1mStep[0m  [16/42], [94mLoss[0m : 1.52184
[1mStep[0m  [20/42], [94mLoss[0m : 1.65123
[1mStep[0m  [24/42], [94mLoss[0m : 1.49531
[1mStep[0m  [28/42], [94mLoss[0m : 1.60408
[1mStep[0m  [32/42], [94mLoss[0m : 1.50538
[1mStep[0m  [36/42], [94mLoss[0m : 1.51927
[1mStep[0m  [40/42], [94mLoss[0m : 1.62287

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.575, [92mTest[0m: 2.451, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.54588
[1mStep[0m  [4/42], [94mLoss[0m : 1.47361
[1mStep[0m  [8/42], [94mLoss[0m : 1.52001
[1mStep[0m  [12/42], [94mLoss[0m : 1.53159
[1mStep[0m  [16/42], [94mLoss[0m : 1.62247
[1mStep[0m  [20/42], [94mLoss[0m : 1.70631
[1mStep[0m  [24/42], [94mLoss[0m : 1.56640
[1mStep[0m  [28/42], [94mLoss[0m : 1.55083
[1mStep[0m  [32/42], [94mLoss[0m : 1.54086
[1mStep[0m  [36/42], [94mLoss[0m : 1.57506
[1mStep[0m  [40/42], [94mLoss[0m : 1.46991

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.544, [92mTest[0m: 2.577, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.45347
[1mStep[0m  [4/42], [94mLoss[0m : 1.52632
[1mStep[0m  [8/42], [94mLoss[0m : 1.71699
[1mStep[0m  [12/42], [94mLoss[0m : 1.44450
[1mStep[0m  [16/42], [94mLoss[0m : 1.58169
[1mStep[0m  [20/42], [94mLoss[0m : 1.45630
[1mStep[0m  [24/42], [94mLoss[0m : 1.47504
[1mStep[0m  [28/42], [94mLoss[0m : 1.57680
[1mStep[0m  [32/42], [94mLoss[0m : 1.40390
[1mStep[0m  [36/42], [94mLoss[0m : 1.41209
[1mStep[0m  [40/42], [94mLoss[0m : 1.60344

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.530, [92mTest[0m: 2.480, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.42143
[1mStep[0m  [4/42], [94mLoss[0m : 1.59844
[1mStep[0m  [8/42], [94mLoss[0m : 1.55183
[1mStep[0m  [12/42], [94mLoss[0m : 1.50304
[1mStep[0m  [16/42], [94mLoss[0m : 1.59334
[1mStep[0m  [20/42], [94mLoss[0m : 1.40900
[1mStep[0m  [24/42], [94mLoss[0m : 1.46514
[1mStep[0m  [28/42], [94mLoss[0m : 1.53537
[1mStep[0m  [32/42], [94mLoss[0m : 1.41793
[1mStep[0m  [36/42], [94mLoss[0m : 1.45750
[1mStep[0m  [40/42], [94mLoss[0m : 1.38445

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.497, [92mTest[0m: 2.545, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.46027
[1mStep[0m  [4/42], [94mLoss[0m : 1.50436
[1mStep[0m  [8/42], [94mLoss[0m : 1.45590
[1mStep[0m  [12/42], [94mLoss[0m : 1.50502
[1mStep[0m  [16/42], [94mLoss[0m : 1.43388
[1mStep[0m  [20/42], [94mLoss[0m : 1.57626
[1mStep[0m  [24/42], [94mLoss[0m : 1.36843
[1mStep[0m  [28/42], [94mLoss[0m : 1.48002
[1mStep[0m  [32/42], [94mLoss[0m : 1.32590
[1mStep[0m  [36/42], [94mLoss[0m : 1.35472
[1mStep[0m  [40/42], [94mLoss[0m : 1.40647

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.459, [92mTest[0m: 2.527, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.40001
[1mStep[0m  [4/42], [94mLoss[0m : 1.31772
[1mStep[0m  [8/42], [94mLoss[0m : 1.32187
[1mStep[0m  [12/42], [94mLoss[0m : 1.36387
[1mStep[0m  [16/42], [94mLoss[0m : 1.41373
[1mStep[0m  [20/42], [94mLoss[0m : 1.35553
[1mStep[0m  [24/42], [94mLoss[0m : 1.40194
[1mStep[0m  [28/42], [94mLoss[0m : 1.48917
[1mStep[0m  [32/42], [94mLoss[0m : 1.48944
[1mStep[0m  [36/42], [94mLoss[0m : 1.29642
[1mStep[0m  [40/42], [94mLoss[0m : 1.37748

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.404, [92mTest[0m: 2.535, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.40885
[1mStep[0m  [4/42], [94mLoss[0m : 1.25862
[1mStep[0m  [8/42], [94mLoss[0m : 1.23936
[1mStep[0m  [12/42], [94mLoss[0m : 1.24013
[1mStep[0m  [16/42], [94mLoss[0m : 1.35817
[1mStep[0m  [20/42], [94mLoss[0m : 1.45228
[1mStep[0m  [24/42], [94mLoss[0m : 1.24705
[1mStep[0m  [28/42], [94mLoss[0m : 1.39365
[1mStep[0m  [32/42], [94mLoss[0m : 1.35142
[1mStep[0m  [36/42], [94mLoss[0m : 1.32008
[1mStep[0m  [40/42], [94mLoss[0m : 1.31129

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.369, [92mTest[0m: 2.516, [96mlr[0m: 0.0023175
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 22 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.557
====================================

Phase 2 - Evaluation MAE:  2.5569201026644026
MAE score P1       2.34173
MAE score P2       2.55692
loss              1.368948
learning_rate     0.002575
batch_size             256
hidden_sizes         [300]
epochs                  30
activation            tanh
optimizer              sgd
early stopping        True
dropout                0.2
momentum               0.9
weight_decay        0.0001
Name: 5, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/21], [94mLoss[0m : 10.83197
[1mStep[0m  [2/21], [94mLoss[0m : 10.84155
[1mStep[0m  [4/21], [94mLoss[0m : 10.83105
[1mStep[0m  [6/21], [94mLoss[0m : 10.87404
[1mStep[0m  [8/21], [94mLoss[0m : 10.75605
[1mStep[0m  [10/21], [94mLoss[0m : 10.78595
[1mStep[0m  [12/21], [94mLoss[0m : 10.65080
[1mStep[0m  [14/21], [94mLoss[0m : 10.47638
[1mStep[0m  [16/21], [94mLoss[0m : 10.44578
[1mStep[0m  [18/21], [94mLoss[0m : 10.34651
[1mStep[0m  [20/21], [94mLoss[0m : 10.60758

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.719, [92mTest[0m: 10.821, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.20473
[1mStep[0m  [2/21], [94mLoss[0m : 10.09780
[1mStep[0m  [4/21], [94mLoss[0m : 10.36815
[1mStep[0m  [6/21], [94mLoss[0m : 10.17703
[1mStep[0m  [8/21], [94mLoss[0m : 10.15967
[1mStep[0m  [10/21], [94mLoss[0m : 9.93548
[1mStep[0m  [12/21], [94mLoss[0m : 9.77068
[1mStep[0m  [14/21], [94mLoss[0m : 9.63492
[1mStep[0m  [16/21], [94mLoss[0m : 9.57825
[1mStep[0m  [18/21], [94mLoss[0m : 9.73249
[1mStep[0m  [20/21], [94mLoss[0m : 9.60631

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.935, [92mTest[0m: 10.297, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.25613
[1mStep[0m  [2/21], [94mLoss[0m : 9.24763
[1mStep[0m  [4/21], [94mLoss[0m : 8.82639
[1mStep[0m  [6/21], [94mLoss[0m : 8.85291
[1mStep[0m  [8/21], [94mLoss[0m : 8.78204
[1mStep[0m  [10/21], [94mLoss[0m : 8.99990
[1mStep[0m  [12/21], [94mLoss[0m : 8.72564
[1mStep[0m  [14/21], [94mLoss[0m : 8.48279
[1mStep[0m  [16/21], [94mLoss[0m : 8.79325
[1mStep[0m  [18/21], [94mLoss[0m : 8.35873
[1mStep[0m  [20/21], [94mLoss[0m : 8.32238

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 8.852, [92mTest[0m: 9.216, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.04523
[1mStep[0m  [2/21], [94mLoss[0m : 8.37191
[1mStep[0m  [4/21], [94mLoss[0m : 7.97909
[1mStep[0m  [6/21], [94mLoss[0m : 7.75168
[1mStep[0m  [8/21], [94mLoss[0m : 7.87892
[1mStep[0m  [10/21], [94mLoss[0m : 7.75491
[1mStep[0m  [12/21], [94mLoss[0m : 7.71194
[1mStep[0m  [14/21], [94mLoss[0m : 7.69667
[1mStep[0m  [16/21], [94mLoss[0m : 7.42097
[1mStep[0m  [18/21], [94mLoss[0m : 7.31683
[1mStep[0m  [20/21], [94mLoss[0m : 7.20564

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 7.725, [92mTest[0m: 8.155, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.08355
[1mStep[0m  [2/21], [94mLoss[0m : 7.22762
[1mStep[0m  [4/21], [94mLoss[0m : 7.13747
[1mStep[0m  [6/21], [94mLoss[0m : 6.92494
[1mStep[0m  [8/21], [94mLoss[0m : 6.83170
[1mStep[0m  [10/21], [94mLoss[0m : 6.55005
[1mStep[0m  [12/21], [94mLoss[0m : 6.88192
[1mStep[0m  [14/21], [94mLoss[0m : 6.44852
[1mStep[0m  [16/21], [94mLoss[0m : 6.37276
[1mStep[0m  [18/21], [94mLoss[0m : 6.62129
[1mStep[0m  [20/21], [94mLoss[0m : 6.25157

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 6.736, [92mTest[0m: 7.088, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.17732
[1mStep[0m  [2/21], [94mLoss[0m : 6.28005
[1mStep[0m  [4/21], [94mLoss[0m : 6.03114
[1mStep[0m  [6/21], [94mLoss[0m : 5.74616
[1mStep[0m  [8/21], [94mLoss[0m : 6.03292
[1mStep[0m  [10/21], [94mLoss[0m : 5.87672
[1mStep[0m  [12/21], [94mLoss[0m : 5.79786
[1mStep[0m  [14/21], [94mLoss[0m : 5.55797
[1mStep[0m  [16/21], [94mLoss[0m : 5.60128
[1mStep[0m  [18/21], [94mLoss[0m : 5.56106
[1mStep[0m  [20/21], [94mLoss[0m : 5.47932

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 5.816, [92mTest[0m: 5.485, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.37151
[1mStep[0m  [2/21], [94mLoss[0m : 5.11008
[1mStep[0m  [4/21], [94mLoss[0m : 5.20813
[1mStep[0m  [6/21], [94mLoss[0m : 4.89402
[1mStep[0m  [8/21], [94mLoss[0m : 4.95410
[1mStep[0m  [10/21], [94mLoss[0m : 4.88062
[1mStep[0m  [12/21], [94mLoss[0m : 4.88029
[1mStep[0m  [14/21], [94mLoss[0m : 4.80795
[1mStep[0m  [16/21], [94mLoss[0m : 4.19147
[1mStep[0m  [18/21], [94mLoss[0m : 4.40097
[1mStep[0m  [20/21], [94mLoss[0m : 4.23064

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 4.787, [92mTest[0m: 4.122, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.09001
[1mStep[0m  [2/21], [94mLoss[0m : 4.04833
[1mStep[0m  [4/21], [94mLoss[0m : 4.30078
[1mStep[0m  [6/21], [94mLoss[0m : 3.88372
[1mStep[0m  [8/21], [94mLoss[0m : 3.93307
[1mStep[0m  [10/21], [94mLoss[0m : 3.74797
[1mStep[0m  [12/21], [94mLoss[0m : 3.61924
[1mStep[0m  [14/21], [94mLoss[0m : 3.49666
[1mStep[0m  [16/21], [94mLoss[0m : 3.63363
[1mStep[0m  [18/21], [94mLoss[0m : 3.49699
[1mStep[0m  [20/21], [94mLoss[0m : 3.30636

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 3.737, [92mTest[0m: 3.227, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.26205
[1mStep[0m  [2/21], [94mLoss[0m : 3.09938
[1mStep[0m  [4/21], [94mLoss[0m : 3.02636
[1mStep[0m  [6/21], [94mLoss[0m : 3.02129
[1mStep[0m  [8/21], [94mLoss[0m : 2.99426
[1mStep[0m  [10/21], [94mLoss[0m : 2.88035
[1mStep[0m  [12/21], [94mLoss[0m : 2.59428
[1mStep[0m  [14/21], [94mLoss[0m : 2.75499
[1mStep[0m  [16/21], [94mLoss[0m : 2.81507
[1mStep[0m  [18/21], [94mLoss[0m : 2.64569
[1mStep[0m  [20/21], [94mLoss[0m : 2.71567

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.914, [92mTest[0m: 2.621, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.60502
[1mStep[0m  [2/21], [94mLoss[0m : 2.66566
[1mStep[0m  [4/21], [94mLoss[0m : 2.60864
[1mStep[0m  [6/21], [94mLoss[0m : 2.72036
[1mStep[0m  [8/21], [94mLoss[0m : 2.60260
[1mStep[0m  [10/21], [94mLoss[0m : 2.75855
[1mStep[0m  [12/21], [94mLoss[0m : 2.56485
[1mStep[0m  [14/21], [94mLoss[0m : 2.62580
[1mStep[0m  [16/21], [94mLoss[0m : 2.60829
[1mStep[0m  [18/21], [94mLoss[0m : 2.69523
[1mStep[0m  [20/21], [94mLoss[0m : 2.70070

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.652, [92mTest[0m: 2.422, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.59786
[1mStep[0m  [2/21], [94mLoss[0m : 2.62224
[1mStep[0m  [4/21], [94mLoss[0m : 2.61222
[1mStep[0m  [6/21], [94mLoss[0m : 2.61825
[1mStep[0m  [8/21], [94mLoss[0m : 2.58864
[1mStep[0m  [10/21], [94mLoss[0m : 2.48617
[1mStep[0m  [12/21], [94mLoss[0m : 2.65637
[1mStep[0m  [14/21], [94mLoss[0m : 2.58010
[1mStep[0m  [16/21], [94mLoss[0m : 2.55619
[1mStep[0m  [18/21], [94mLoss[0m : 2.51372
[1mStep[0m  [20/21], [94mLoss[0m : 2.70827

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.604, [92mTest[0m: 2.643, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.58142
[1mStep[0m  [2/21], [94mLoss[0m : 2.60399
[1mStep[0m  [4/21], [94mLoss[0m : 2.57543
[1mStep[0m  [6/21], [94mLoss[0m : 2.42679
[1mStep[0m  [8/21], [94mLoss[0m : 2.49923
[1mStep[0m  [10/21], [94mLoss[0m : 2.63862
[1mStep[0m  [12/21], [94mLoss[0m : 2.63329
[1mStep[0m  [14/21], [94mLoss[0m : 2.73394
[1mStep[0m  [16/21], [94mLoss[0m : 2.69410
[1mStep[0m  [18/21], [94mLoss[0m : 2.53734
[1mStep[0m  [20/21], [94mLoss[0m : 2.61169

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.596, [92mTest[0m: 2.431, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44004
[1mStep[0m  [2/21], [94mLoss[0m : 2.75853
[1mStep[0m  [4/21], [94mLoss[0m : 2.52760
[1mStep[0m  [6/21], [94mLoss[0m : 2.67093
[1mStep[0m  [8/21], [94mLoss[0m : 2.49022
[1mStep[0m  [10/21], [94mLoss[0m : 2.65651
[1mStep[0m  [12/21], [94mLoss[0m : 2.51650
[1mStep[0m  [14/21], [94mLoss[0m : 2.53993
[1mStep[0m  [16/21], [94mLoss[0m : 2.58194
[1mStep[0m  [18/21], [94mLoss[0m : 2.54525
[1mStep[0m  [20/21], [94mLoss[0m : 2.55214

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.593, [92mTest[0m: 2.399, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.58348
[1mStep[0m  [2/21], [94mLoss[0m : 2.68973
[1mStep[0m  [4/21], [94mLoss[0m : 2.60239
[1mStep[0m  [6/21], [94mLoss[0m : 2.70762
[1mStep[0m  [8/21], [94mLoss[0m : 2.58363
[1mStep[0m  [10/21], [94mLoss[0m : 2.46709
[1mStep[0m  [12/21], [94mLoss[0m : 2.57113
[1mStep[0m  [14/21], [94mLoss[0m : 2.52177
[1mStep[0m  [16/21], [94mLoss[0m : 2.50005
[1mStep[0m  [18/21], [94mLoss[0m : 2.55180
[1mStep[0m  [20/21], [94mLoss[0m : 2.63926

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.566, [92mTest[0m: 2.388, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.57446
[1mStep[0m  [2/21], [94mLoss[0m : 2.71806
[1mStep[0m  [4/21], [94mLoss[0m : 2.56233
[1mStep[0m  [6/21], [94mLoss[0m : 2.51226
[1mStep[0m  [8/21], [94mLoss[0m : 2.60659
[1mStep[0m  [10/21], [94mLoss[0m : 2.56622
[1mStep[0m  [12/21], [94mLoss[0m : 2.47025
[1mStep[0m  [14/21], [94mLoss[0m : 2.60443
[1mStep[0m  [16/21], [94mLoss[0m : 2.61334
[1mStep[0m  [18/21], [94mLoss[0m : 2.57743
[1mStep[0m  [20/21], [94mLoss[0m : 2.67227

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.557, [92mTest[0m: 2.398, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51885
[1mStep[0m  [2/21], [94mLoss[0m : 2.59830
[1mStep[0m  [4/21], [94mLoss[0m : 2.54229
[1mStep[0m  [6/21], [94mLoss[0m : 2.54306
[1mStep[0m  [8/21], [94mLoss[0m : 2.49254
[1mStep[0m  [10/21], [94mLoss[0m : 2.41361
[1mStep[0m  [12/21], [94mLoss[0m : 2.51425
[1mStep[0m  [14/21], [94mLoss[0m : 2.50822
[1mStep[0m  [16/21], [94mLoss[0m : 2.54027
[1mStep[0m  [18/21], [94mLoss[0m : 2.64326
[1mStep[0m  [20/21], [94mLoss[0m : 2.50365

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.544, [92mTest[0m: 2.408, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.67601
[1mStep[0m  [2/21], [94mLoss[0m : 2.53478
[1mStep[0m  [4/21], [94mLoss[0m : 2.49092
[1mStep[0m  [6/21], [94mLoss[0m : 2.52197
[1mStep[0m  [8/21], [94mLoss[0m : 2.65361
[1mStep[0m  [10/21], [94mLoss[0m : 2.57882
[1mStep[0m  [12/21], [94mLoss[0m : 2.80569
[1mStep[0m  [14/21], [94mLoss[0m : 2.53506
[1mStep[0m  [16/21], [94mLoss[0m : 2.64497
[1mStep[0m  [18/21], [94mLoss[0m : 2.49930
[1mStep[0m  [20/21], [94mLoss[0m : 2.52247

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.562, [92mTest[0m: 2.400, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48050
[1mStep[0m  [2/21], [94mLoss[0m : 2.69192
[1mStep[0m  [4/21], [94mLoss[0m : 2.37991
[1mStep[0m  [6/21], [94mLoss[0m : 2.65283
[1mStep[0m  [8/21], [94mLoss[0m : 2.55965
[1mStep[0m  [10/21], [94mLoss[0m : 2.42741
[1mStep[0m  [12/21], [94mLoss[0m : 2.65538
[1mStep[0m  [14/21], [94mLoss[0m : 2.49821
[1mStep[0m  [16/21], [94mLoss[0m : 2.47497
[1mStep[0m  [18/21], [94mLoss[0m : 2.55155
[1mStep[0m  [20/21], [94mLoss[0m : 2.43858

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.529, [92mTest[0m: 2.366, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.59132
[1mStep[0m  [2/21], [94mLoss[0m : 2.64680
[1mStep[0m  [4/21], [94mLoss[0m : 2.52197
[1mStep[0m  [6/21], [94mLoss[0m : 2.53704
[1mStep[0m  [8/21], [94mLoss[0m : 2.57532
[1mStep[0m  [10/21], [94mLoss[0m : 2.55385
[1mStep[0m  [12/21], [94mLoss[0m : 2.45077
[1mStep[0m  [14/21], [94mLoss[0m : 2.60832
[1mStep[0m  [16/21], [94mLoss[0m : 2.45258
[1mStep[0m  [18/21], [94mLoss[0m : 2.47882
[1mStep[0m  [20/21], [94mLoss[0m : 2.51234

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.363, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52549
[1mStep[0m  [2/21], [94mLoss[0m : 2.43625
[1mStep[0m  [4/21], [94mLoss[0m : 2.38715
[1mStep[0m  [6/21], [94mLoss[0m : 2.52587
[1mStep[0m  [8/21], [94mLoss[0m : 2.42826
[1mStep[0m  [10/21], [94mLoss[0m : 2.48126
[1mStep[0m  [12/21], [94mLoss[0m : 2.58456
[1mStep[0m  [14/21], [94mLoss[0m : 2.53456
[1mStep[0m  [16/21], [94mLoss[0m : 2.56839
[1mStep[0m  [18/21], [94mLoss[0m : 2.63662
[1mStep[0m  [20/21], [94mLoss[0m : 2.60489

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.406, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56272
[1mStep[0m  [2/21], [94mLoss[0m : 2.63976
[1mStep[0m  [4/21], [94mLoss[0m : 2.56401
[1mStep[0m  [6/21], [94mLoss[0m : 2.49724
[1mStep[0m  [8/21], [94mLoss[0m : 2.48804
[1mStep[0m  [10/21], [94mLoss[0m : 2.50954
[1mStep[0m  [12/21], [94mLoss[0m : 2.65401
[1mStep[0m  [14/21], [94mLoss[0m : 2.53018
[1mStep[0m  [16/21], [94mLoss[0m : 2.38944
[1mStep[0m  [18/21], [94mLoss[0m : 2.42717
[1mStep[0m  [20/21], [94mLoss[0m : 2.50479

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.529, [92mTest[0m: 2.370, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.45801
[1mStep[0m  [2/21], [94mLoss[0m : 2.53999
[1mStep[0m  [4/21], [94mLoss[0m : 2.59778
[1mStep[0m  [6/21], [94mLoss[0m : 2.60574
[1mStep[0m  [8/21], [94mLoss[0m : 2.42486
[1mStep[0m  [10/21], [94mLoss[0m : 2.58323
[1mStep[0m  [12/21], [94mLoss[0m : 2.55555
[1mStep[0m  [14/21], [94mLoss[0m : 2.53500
[1mStep[0m  [16/21], [94mLoss[0m : 2.51356
[1mStep[0m  [18/21], [94mLoss[0m : 2.59415
[1mStep[0m  [20/21], [94mLoss[0m : 2.42341

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.515, [92mTest[0m: 2.382, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.59248
[1mStep[0m  [2/21], [94mLoss[0m : 2.56083
[1mStep[0m  [4/21], [94mLoss[0m : 2.55276
[1mStep[0m  [6/21], [94mLoss[0m : 2.47399
[1mStep[0m  [8/21], [94mLoss[0m : 2.59781
[1mStep[0m  [10/21], [94mLoss[0m : 2.26956
[1mStep[0m  [12/21], [94mLoss[0m : 2.57804
[1mStep[0m  [14/21], [94mLoss[0m : 2.45428
[1mStep[0m  [16/21], [94mLoss[0m : 2.61453
[1mStep[0m  [18/21], [94mLoss[0m : 2.52585
[1mStep[0m  [20/21], [94mLoss[0m : 2.42353

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.366, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.41442
[1mStep[0m  [2/21], [94mLoss[0m : 2.52852
[1mStep[0m  [4/21], [94mLoss[0m : 2.51763
[1mStep[0m  [6/21], [94mLoss[0m : 2.54668
[1mStep[0m  [8/21], [94mLoss[0m : 2.47800
[1mStep[0m  [10/21], [94mLoss[0m : 2.45994
[1mStep[0m  [12/21], [94mLoss[0m : 2.61535
[1mStep[0m  [14/21], [94mLoss[0m : 2.41897
[1mStep[0m  [16/21], [94mLoss[0m : 2.48998
[1mStep[0m  [18/21], [94mLoss[0m : 2.64115
[1mStep[0m  [20/21], [94mLoss[0m : 2.33977

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.361, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.55752
[1mStep[0m  [2/21], [94mLoss[0m : 2.45885
[1mStep[0m  [4/21], [94mLoss[0m : 2.41090
[1mStep[0m  [6/21], [94mLoss[0m : 2.29198
[1mStep[0m  [8/21], [94mLoss[0m : 2.57386
[1mStep[0m  [10/21], [94mLoss[0m : 2.53111
[1mStep[0m  [12/21], [94mLoss[0m : 2.60714
[1mStep[0m  [14/21], [94mLoss[0m : 2.52356
[1mStep[0m  [16/21], [94mLoss[0m : 2.35232
[1mStep[0m  [18/21], [94mLoss[0m : 2.45152
[1mStep[0m  [20/21], [94mLoss[0m : 2.55614

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.328, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.60058
[1mStep[0m  [2/21], [94mLoss[0m : 2.37653
[1mStep[0m  [4/21], [94mLoss[0m : 2.36964
[1mStep[0m  [6/21], [94mLoss[0m : 2.40734
[1mStep[0m  [8/21], [94mLoss[0m : 2.49231
[1mStep[0m  [10/21], [94mLoss[0m : 2.57232
[1mStep[0m  [12/21], [94mLoss[0m : 2.43570
[1mStep[0m  [14/21], [94mLoss[0m : 2.56221
[1mStep[0m  [16/21], [94mLoss[0m : 2.34391
[1mStep[0m  [18/21], [94mLoss[0m : 2.40883
[1mStep[0m  [20/21], [94mLoss[0m : 2.54880

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.347, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56770
[1mStep[0m  [2/21], [94mLoss[0m : 2.44914
[1mStep[0m  [4/21], [94mLoss[0m : 2.44446
[1mStep[0m  [6/21], [94mLoss[0m : 2.53974
[1mStep[0m  [8/21], [94mLoss[0m : 2.50365
[1mStep[0m  [10/21], [94mLoss[0m : 2.42386
[1mStep[0m  [12/21], [94mLoss[0m : 2.43484
[1mStep[0m  [14/21], [94mLoss[0m : 2.41741
[1mStep[0m  [16/21], [94mLoss[0m : 2.61140
[1mStep[0m  [18/21], [94mLoss[0m : 2.49841
[1mStep[0m  [20/21], [94mLoss[0m : 2.45584

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.353, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42766
[1mStep[0m  [2/21], [94mLoss[0m : 2.42144
[1mStep[0m  [4/21], [94mLoss[0m : 2.41666
[1mStep[0m  [6/21], [94mLoss[0m : 2.44837
[1mStep[0m  [8/21], [94mLoss[0m : 2.55476
[1mStep[0m  [10/21], [94mLoss[0m : 2.46858
[1mStep[0m  [12/21], [94mLoss[0m : 2.51903
[1mStep[0m  [14/21], [94mLoss[0m : 2.45219
[1mStep[0m  [16/21], [94mLoss[0m : 2.37763
[1mStep[0m  [18/21], [94mLoss[0m : 2.49210
[1mStep[0m  [20/21], [94mLoss[0m : 2.38222

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.330, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.39389
[1mStep[0m  [2/21], [94mLoss[0m : 2.46459
[1mStep[0m  [4/21], [94mLoss[0m : 2.53962
[1mStep[0m  [6/21], [94mLoss[0m : 2.50883
[1mStep[0m  [8/21], [94mLoss[0m : 2.41377
[1mStep[0m  [10/21], [94mLoss[0m : 2.47777
[1mStep[0m  [12/21], [94mLoss[0m : 2.41093
[1mStep[0m  [14/21], [94mLoss[0m : 2.47778
[1mStep[0m  [16/21], [94mLoss[0m : 2.44588
[1mStep[0m  [18/21], [94mLoss[0m : 2.45570
[1mStep[0m  [20/21], [94mLoss[0m : 2.60450

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.351, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53683
[1mStep[0m  [2/21], [94mLoss[0m : 2.63309
[1mStep[0m  [4/21], [94mLoss[0m : 2.59130
[1mStep[0m  [6/21], [94mLoss[0m : 2.62778
[1mStep[0m  [8/21], [94mLoss[0m : 2.48041
[1mStep[0m  [10/21], [94mLoss[0m : 2.42751
[1mStep[0m  [12/21], [94mLoss[0m : 2.28662
[1mStep[0m  [14/21], [94mLoss[0m : 2.52971
[1mStep[0m  [16/21], [94mLoss[0m : 2.48189
[1mStep[0m  [18/21], [94mLoss[0m : 2.21818
[1mStep[0m  [20/21], [94mLoss[0m : 2.51327

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.350, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.348
====================================

Phase 1 - Evaluation MAE:  2.348203590938023
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/21], [94mLoss[0m : 2.39110
[1mStep[0m  [2/21], [94mLoss[0m : 2.43112
[1mStep[0m  [4/21], [94mLoss[0m : 2.55378
[1mStep[0m  [6/21], [94mLoss[0m : 2.61840
[1mStep[0m  [8/21], [94mLoss[0m : 2.43575
[1mStep[0m  [10/21], [94mLoss[0m : 2.48546
[1mStep[0m  [12/21], [94mLoss[0m : 2.50964
[1mStep[0m  [14/21], [94mLoss[0m : 2.66109
[1mStep[0m  [16/21], [94mLoss[0m : 2.55631
[1mStep[0m  [18/21], [94mLoss[0m : 2.49217
[1mStep[0m  [20/21], [94mLoss[0m : 2.49968

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.341, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.45490
[1mStep[0m  [2/21], [94mLoss[0m : 2.42080
[1mStep[0m  [4/21], [94mLoss[0m : 2.39354
[1mStep[0m  [6/21], [94mLoss[0m : 2.47917
[1mStep[0m  [8/21], [94mLoss[0m : 2.42051
[1mStep[0m  [10/21], [94mLoss[0m : 2.63285
[1mStep[0m  [12/21], [94mLoss[0m : 2.39326
[1mStep[0m  [14/21], [94mLoss[0m : 2.45260
[1mStep[0m  [16/21], [94mLoss[0m : 2.40308
[1mStep[0m  [18/21], [94mLoss[0m : 2.55923
[1mStep[0m  [20/21], [94mLoss[0m : 2.37927

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.849, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43907
[1mStep[0m  [2/21], [94mLoss[0m : 2.40810
[1mStep[0m  [4/21], [94mLoss[0m : 2.35605
[1mStep[0m  [6/21], [94mLoss[0m : 2.36517
[1mStep[0m  [8/21], [94mLoss[0m : 2.48888
[1mStep[0m  [10/21], [94mLoss[0m : 2.43395
[1mStep[0m  [12/21], [94mLoss[0m : 2.40225
[1mStep[0m  [14/21], [94mLoss[0m : 2.42904
[1mStep[0m  [16/21], [94mLoss[0m : 2.37827
[1mStep[0m  [18/21], [94mLoss[0m : 2.51768
[1mStep[0m  [20/21], [94mLoss[0m : 2.35785

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.504, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.36198
[1mStep[0m  [2/21], [94mLoss[0m : 2.31777
[1mStep[0m  [4/21], [94mLoss[0m : 2.39526
[1mStep[0m  [6/21], [94mLoss[0m : 2.39241
[1mStep[0m  [8/21], [94mLoss[0m : 2.41079
[1mStep[0m  [10/21], [94mLoss[0m : 2.41856
[1mStep[0m  [12/21], [94mLoss[0m : 2.50789
[1mStep[0m  [14/21], [94mLoss[0m : 2.39470
[1mStep[0m  [16/21], [94mLoss[0m : 2.38249
[1mStep[0m  [18/21], [94mLoss[0m : 2.43486
[1mStep[0m  [20/21], [94mLoss[0m : 2.43551

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.737, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.30517
[1mStep[0m  [2/21], [94mLoss[0m : 2.35087
[1mStep[0m  [4/21], [94mLoss[0m : 2.37881
[1mStep[0m  [6/21], [94mLoss[0m : 2.40567
[1mStep[0m  [8/21], [94mLoss[0m : 2.49178
[1mStep[0m  [10/21], [94mLoss[0m : 2.38021
[1mStep[0m  [12/21], [94mLoss[0m : 2.34470
[1mStep[0m  [14/21], [94mLoss[0m : 2.21153
[1mStep[0m  [16/21], [94mLoss[0m : 2.40491
[1mStep[0m  [18/21], [94mLoss[0m : 2.31958
[1mStep[0m  [20/21], [94mLoss[0m : 2.24988

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.336, [92mTest[0m: 3.092, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.20279
[1mStep[0m  [2/21], [94mLoss[0m : 2.22580
[1mStep[0m  [4/21], [94mLoss[0m : 2.37551
[1mStep[0m  [6/21], [94mLoss[0m : 2.22638
[1mStep[0m  [8/21], [94mLoss[0m : 2.37032
[1mStep[0m  [10/21], [94mLoss[0m : 2.54410
[1mStep[0m  [12/21], [94mLoss[0m : 2.33557
[1mStep[0m  [14/21], [94mLoss[0m : 2.26566
[1mStep[0m  [16/21], [94mLoss[0m : 2.29448
[1mStep[0m  [18/21], [94mLoss[0m : 2.40849
[1mStep[0m  [20/21], [94mLoss[0m : 2.30336

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.317, [92mTest[0m: 2.765, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.33401
[1mStep[0m  [2/21], [94mLoss[0m : 2.38707
[1mStep[0m  [4/21], [94mLoss[0m : 2.20501
[1mStep[0m  [6/21], [94mLoss[0m : 2.24325
[1mStep[0m  [8/21], [94mLoss[0m : 2.31795
[1mStep[0m  [10/21], [94mLoss[0m : 2.29245
[1mStep[0m  [12/21], [94mLoss[0m : 2.25023
[1mStep[0m  [14/21], [94mLoss[0m : 2.21631
[1mStep[0m  [16/21], [94mLoss[0m : 2.12416
[1mStep[0m  [18/21], [94mLoss[0m : 2.15434
[1mStep[0m  [20/21], [94mLoss[0m : 2.25609

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.261, [92mTest[0m: 2.616, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.13690
[1mStep[0m  [2/21], [94mLoss[0m : 2.20220
[1mStep[0m  [4/21], [94mLoss[0m : 2.14149
[1mStep[0m  [6/21], [94mLoss[0m : 2.12929
[1mStep[0m  [8/21], [94mLoss[0m : 2.39439
[1mStep[0m  [10/21], [94mLoss[0m : 2.32451
[1mStep[0m  [12/21], [94mLoss[0m : 2.25251
[1mStep[0m  [14/21], [94mLoss[0m : 2.17917
[1mStep[0m  [16/21], [94mLoss[0m : 2.16601
[1mStep[0m  [18/21], [94mLoss[0m : 2.15199
[1mStep[0m  [20/21], [94mLoss[0m : 2.26876

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.201, [92mTest[0m: 2.659, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.15942
[1mStep[0m  [2/21], [94mLoss[0m : 2.23275
[1mStep[0m  [4/21], [94mLoss[0m : 2.09611
[1mStep[0m  [6/21], [94mLoss[0m : 2.17874
[1mStep[0m  [8/21], [94mLoss[0m : 2.22006
[1mStep[0m  [10/21], [94mLoss[0m : 2.12064
[1mStep[0m  [12/21], [94mLoss[0m : 2.10197
[1mStep[0m  [14/21], [94mLoss[0m : 2.15672
[1mStep[0m  [16/21], [94mLoss[0m : 2.03671
[1mStep[0m  [18/21], [94mLoss[0m : 2.17545
[1mStep[0m  [20/21], [94mLoss[0m : 2.24795

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.154, [92mTest[0m: 2.693, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.14897
[1mStep[0m  [2/21], [94mLoss[0m : 2.22116
[1mStep[0m  [4/21], [94mLoss[0m : 2.14569
[1mStep[0m  [6/21], [94mLoss[0m : 1.96803
[1mStep[0m  [8/21], [94mLoss[0m : 2.04430
[1mStep[0m  [10/21], [94mLoss[0m : 2.10544
[1mStep[0m  [12/21], [94mLoss[0m : 2.13497
[1mStep[0m  [14/21], [94mLoss[0m : 2.12198
[1mStep[0m  [16/21], [94mLoss[0m : 2.01086
[1mStep[0m  [18/21], [94mLoss[0m : 2.22470
[1mStep[0m  [20/21], [94mLoss[0m : 2.13506

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.116, [92mTest[0m: 2.629, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.08089
[1mStep[0m  [2/21], [94mLoss[0m : 1.94971
[1mStep[0m  [4/21], [94mLoss[0m : 2.09191
[1mStep[0m  [6/21], [94mLoss[0m : 2.09749
[1mStep[0m  [8/21], [94mLoss[0m : 1.90580
[1mStep[0m  [10/21], [94mLoss[0m : 1.96977
[1mStep[0m  [12/21], [94mLoss[0m : 2.03781
[1mStep[0m  [14/21], [94mLoss[0m : 2.05318
[1mStep[0m  [16/21], [94mLoss[0m : 2.00867
[1mStep[0m  [18/21], [94mLoss[0m : 2.09402
[1mStep[0m  [20/21], [94mLoss[0m : 2.15080

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.036, [92mTest[0m: 2.708, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.97109
[1mStep[0m  [2/21], [94mLoss[0m : 2.02930
[1mStep[0m  [4/21], [94mLoss[0m : 2.01583
[1mStep[0m  [6/21], [94mLoss[0m : 2.00586
[1mStep[0m  [8/21], [94mLoss[0m : 2.04828
[1mStep[0m  [10/21], [94mLoss[0m : 1.97056
[1mStep[0m  [12/21], [94mLoss[0m : 2.04326
[1mStep[0m  [14/21], [94mLoss[0m : 1.83056
[1mStep[0m  [16/21], [94mLoss[0m : 2.06369
[1mStep[0m  [18/21], [94mLoss[0m : 2.00376
[1mStep[0m  [20/21], [94mLoss[0m : 2.16386

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.016, [92mTest[0m: 2.612, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.99582
[1mStep[0m  [2/21], [94mLoss[0m : 2.04735
[1mStep[0m  [4/21], [94mLoss[0m : 1.86885
[1mStep[0m  [6/21], [94mLoss[0m : 1.99597
[1mStep[0m  [8/21], [94mLoss[0m : 1.92250
[1mStep[0m  [10/21], [94mLoss[0m : 1.96585
[1mStep[0m  [12/21], [94mLoss[0m : 2.11761
[1mStep[0m  [14/21], [94mLoss[0m : 1.90003
[1mStep[0m  [16/21], [94mLoss[0m : 2.06804
[1mStep[0m  [18/21], [94mLoss[0m : 1.92631
[1mStep[0m  [20/21], [94mLoss[0m : 1.96324

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.952, [92mTest[0m: 2.558, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.99640
[1mStep[0m  [2/21], [94mLoss[0m : 1.94229
[1mStep[0m  [4/21], [94mLoss[0m : 1.76676
[1mStep[0m  [6/21], [94mLoss[0m : 1.83126
[1mStep[0m  [8/21], [94mLoss[0m : 1.98534
[1mStep[0m  [10/21], [94mLoss[0m : 1.86668
[1mStep[0m  [12/21], [94mLoss[0m : 2.06351
[1mStep[0m  [14/21], [94mLoss[0m : 1.94031
[1mStep[0m  [16/21], [94mLoss[0m : 2.04654
[1mStep[0m  [18/21], [94mLoss[0m : 1.84319
[1mStep[0m  [20/21], [94mLoss[0m : 1.90450

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.925, [92mTest[0m: 2.595, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.84702
[1mStep[0m  [2/21], [94mLoss[0m : 1.89229
[1mStep[0m  [4/21], [94mLoss[0m : 1.88134
[1mStep[0m  [6/21], [94mLoss[0m : 1.85644
[1mStep[0m  [8/21], [94mLoss[0m : 1.97131
[1mStep[0m  [10/21], [94mLoss[0m : 1.86186
[1mStep[0m  [12/21], [94mLoss[0m : 1.90559
[1mStep[0m  [14/21], [94mLoss[0m : 1.91848
[1mStep[0m  [16/21], [94mLoss[0m : 1.90964
[1mStep[0m  [18/21], [94mLoss[0m : 1.82023
[1mStep[0m  [20/21], [94mLoss[0m : 1.96976

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.919, [92mTest[0m: 2.545, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.88351
[1mStep[0m  [2/21], [94mLoss[0m : 1.95615
[1mStep[0m  [4/21], [94mLoss[0m : 1.86178
[1mStep[0m  [6/21], [94mLoss[0m : 1.82319
[1mStep[0m  [8/21], [94mLoss[0m : 1.89229
[1mStep[0m  [10/21], [94mLoss[0m : 1.79812
[1mStep[0m  [12/21], [94mLoss[0m : 1.87500
[1mStep[0m  [14/21], [94mLoss[0m : 1.69222
[1mStep[0m  [16/21], [94mLoss[0m : 1.79143
[1mStep[0m  [18/21], [94mLoss[0m : 1.91924
[1mStep[0m  [20/21], [94mLoss[0m : 1.81436

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.860, [92mTest[0m: 2.619, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.84429
[1mStep[0m  [2/21], [94mLoss[0m : 1.73541
[1mStep[0m  [4/21], [94mLoss[0m : 1.74135
[1mStep[0m  [6/21], [94mLoss[0m : 1.82599
[1mStep[0m  [8/21], [94mLoss[0m : 1.86666
[1mStep[0m  [10/21], [94mLoss[0m : 1.87242
[1mStep[0m  [12/21], [94mLoss[0m : 1.67963
[1mStep[0m  [14/21], [94mLoss[0m : 1.87029
[1mStep[0m  [16/21], [94mLoss[0m : 1.79662
[1mStep[0m  [18/21], [94mLoss[0m : 1.93280
[1mStep[0m  [20/21], [94mLoss[0m : 1.92808

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.814, [92mTest[0m: 2.560, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.74496
[1mStep[0m  [2/21], [94mLoss[0m : 1.91476
[1mStep[0m  [4/21], [94mLoss[0m : 1.74150
[1mStep[0m  [6/21], [94mLoss[0m : 1.75684
[1mStep[0m  [8/21], [94mLoss[0m : 1.79656
[1mStep[0m  [10/21], [94mLoss[0m : 1.82945
[1mStep[0m  [12/21], [94mLoss[0m : 1.86475
[1mStep[0m  [14/21], [94mLoss[0m : 1.64212
[1mStep[0m  [16/21], [94mLoss[0m : 1.70808
[1mStep[0m  [18/21], [94mLoss[0m : 1.79386
[1mStep[0m  [20/21], [94mLoss[0m : 1.78406

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.792, [92mTest[0m: 2.493, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.73061
[1mStep[0m  [2/21], [94mLoss[0m : 1.80228
[1mStep[0m  [4/21], [94mLoss[0m : 1.72747
[1mStep[0m  [6/21], [94mLoss[0m : 1.69323
[1mStep[0m  [8/21], [94mLoss[0m : 1.75531
[1mStep[0m  [10/21], [94mLoss[0m : 1.71286
[1mStep[0m  [12/21], [94mLoss[0m : 1.79777
[1mStep[0m  [14/21], [94mLoss[0m : 1.81543
[1mStep[0m  [16/21], [94mLoss[0m : 1.67254
[1mStep[0m  [18/21], [94mLoss[0m : 1.72023
[1mStep[0m  [20/21], [94mLoss[0m : 1.86765

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.761, [92mTest[0m: 2.503, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.71137
[1mStep[0m  [2/21], [94mLoss[0m : 1.65985
[1mStep[0m  [4/21], [94mLoss[0m : 1.69923
[1mStep[0m  [6/21], [94mLoss[0m : 1.75440
[1mStep[0m  [8/21], [94mLoss[0m : 1.76630
[1mStep[0m  [10/21], [94mLoss[0m : 1.88432
[1mStep[0m  [12/21], [94mLoss[0m : 1.71786
[1mStep[0m  [14/21], [94mLoss[0m : 1.73038
[1mStep[0m  [16/21], [94mLoss[0m : 1.77746
[1mStep[0m  [18/21], [94mLoss[0m : 1.71836
[1mStep[0m  [20/21], [94mLoss[0m : 1.68999

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.728, [92mTest[0m: 2.498, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.67079
[1mStep[0m  [2/21], [94mLoss[0m : 1.52262
[1mStep[0m  [4/21], [94mLoss[0m : 1.70297
[1mStep[0m  [6/21], [94mLoss[0m : 1.69702
[1mStep[0m  [8/21], [94mLoss[0m : 1.75049
[1mStep[0m  [10/21], [94mLoss[0m : 1.71902
[1mStep[0m  [12/21], [94mLoss[0m : 1.72731
[1mStep[0m  [14/21], [94mLoss[0m : 1.68589
[1mStep[0m  [16/21], [94mLoss[0m : 1.64736
[1mStep[0m  [18/21], [94mLoss[0m : 1.67513
[1mStep[0m  [20/21], [94mLoss[0m : 1.69245

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.678, [92mTest[0m: 2.471, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.57146
[1mStep[0m  [2/21], [94mLoss[0m : 1.62020
[1mStep[0m  [4/21], [94mLoss[0m : 1.79818
[1mStep[0m  [6/21], [94mLoss[0m : 1.65761
[1mStep[0m  [8/21], [94mLoss[0m : 1.64757
[1mStep[0m  [10/21], [94mLoss[0m : 1.66440
[1mStep[0m  [12/21], [94mLoss[0m : 1.70695
[1mStep[0m  [14/21], [94mLoss[0m : 1.58555
[1mStep[0m  [16/21], [94mLoss[0m : 1.61234
[1mStep[0m  [18/21], [94mLoss[0m : 1.60598
[1mStep[0m  [20/21], [94mLoss[0m : 1.67650

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.669, [92mTest[0m: 2.577, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.61603
[1mStep[0m  [2/21], [94mLoss[0m : 1.64816
[1mStep[0m  [4/21], [94mLoss[0m : 1.62899
[1mStep[0m  [6/21], [94mLoss[0m : 1.61474
[1mStep[0m  [8/21], [94mLoss[0m : 1.69282
[1mStep[0m  [10/21], [94mLoss[0m : 1.61939
[1mStep[0m  [12/21], [94mLoss[0m : 1.51249
[1mStep[0m  [14/21], [94mLoss[0m : 1.60226
[1mStep[0m  [16/21], [94mLoss[0m : 1.58226
[1mStep[0m  [18/21], [94mLoss[0m : 1.64537
[1mStep[0m  [20/21], [94mLoss[0m : 1.57164

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.630, [92mTest[0m: 2.507, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.71413
[1mStep[0m  [2/21], [94mLoss[0m : 1.55071
[1mStep[0m  [4/21], [94mLoss[0m : 1.52594
[1mStep[0m  [6/21], [94mLoss[0m : 1.65510
[1mStep[0m  [8/21], [94mLoss[0m : 1.55628
[1mStep[0m  [10/21], [94mLoss[0m : 1.65339
[1mStep[0m  [12/21], [94mLoss[0m : 1.61398
[1mStep[0m  [14/21], [94mLoss[0m : 1.56674
[1mStep[0m  [16/21], [94mLoss[0m : 1.68222
[1mStep[0m  [18/21], [94mLoss[0m : 1.76373
[1mStep[0m  [20/21], [94mLoss[0m : 1.57490

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.604, [92mTest[0m: 2.755, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.59034
[1mStep[0m  [2/21], [94mLoss[0m : 1.64032
[1mStep[0m  [4/21], [94mLoss[0m : 1.54106
[1mStep[0m  [6/21], [94mLoss[0m : 1.56780
[1mStep[0m  [8/21], [94mLoss[0m : 1.47084
[1mStep[0m  [10/21], [94mLoss[0m : 1.57448
[1mStep[0m  [12/21], [94mLoss[0m : 1.41759
[1mStep[0m  [14/21], [94mLoss[0m : 1.53014
[1mStep[0m  [16/21], [94mLoss[0m : 1.46670
[1mStep[0m  [18/21], [94mLoss[0m : 1.54957
[1mStep[0m  [20/21], [94mLoss[0m : 1.66556

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.567, [92mTest[0m: 2.521, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.53102
[1mStep[0m  [2/21], [94mLoss[0m : 1.59841
[1mStep[0m  [4/21], [94mLoss[0m : 1.40779
[1mStep[0m  [6/21], [94mLoss[0m : 1.54477
[1mStep[0m  [8/21], [94mLoss[0m : 1.47464
[1mStep[0m  [10/21], [94mLoss[0m : 1.55246
[1mStep[0m  [12/21], [94mLoss[0m : 1.63254
[1mStep[0m  [14/21], [94mLoss[0m : 1.55429
[1mStep[0m  [16/21], [94mLoss[0m : 1.57039
[1mStep[0m  [18/21], [94mLoss[0m : 1.54968
[1mStep[0m  [20/21], [94mLoss[0m : 1.47864

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.545, [92mTest[0m: 2.503, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.46276
[1mStep[0m  [2/21], [94mLoss[0m : 1.47618
[1mStep[0m  [4/21], [94mLoss[0m : 1.50770
[1mStep[0m  [6/21], [94mLoss[0m : 1.50638
[1mStep[0m  [8/21], [94mLoss[0m : 1.45275
[1mStep[0m  [10/21], [94mLoss[0m : 1.51057
[1mStep[0m  [12/21], [94mLoss[0m : 1.62220
[1mStep[0m  [14/21], [94mLoss[0m : 1.55125
[1mStep[0m  [16/21], [94mLoss[0m : 1.55197
[1mStep[0m  [18/21], [94mLoss[0m : 1.55993
[1mStep[0m  [20/21], [94mLoss[0m : 1.60816

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.531, [92mTest[0m: 2.544, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.46561
[1mStep[0m  [2/21], [94mLoss[0m : 1.45055
[1mStep[0m  [4/21], [94mLoss[0m : 1.49139
[1mStep[0m  [6/21], [94mLoss[0m : 1.53471
[1mStep[0m  [8/21], [94mLoss[0m : 1.42697
[1mStep[0m  [10/21], [94mLoss[0m : 1.47974
[1mStep[0m  [12/21], [94mLoss[0m : 1.50039
[1mStep[0m  [14/21], [94mLoss[0m : 1.34913
[1mStep[0m  [16/21], [94mLoss[0m : 1.64736
[1mStep[0m  [18/21], [94mLoss[0m : 1.48475
[1mStep[0m  [20/21], [94mLoss[0m : 1.56291

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.491, [92mTest[0m: 2.562, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.55871
[1mStep[0m  [2/21], [94mLoss[0m : 1.51387
[1mStep[0m  [4/21], [94mLoss[0m : 1.54885
[1mStep[0m  [6/21], [94mLoss[0m : 1.56396
[1mStep[0m  [8/21], [94mLoss[0m : 1.45194
[1mStep[0m  [10/21], [94mLoss[0m : 1.42378
[1mStep[0m  [12/21], [94mLoss[0m : 1.54523
[1mStep[0m  [14/21], [94mLoss[0m : 1.47531
[1mStep[0m  [16/21], [94mLoss[0m : 1.42109
[1mStep[0m  [18/21], [94mLoss[0m : 1.51816
[1mStep[0m  [20/21], [94mLoss[0m : 1.46905

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.501, [92mTest[0m: 2.518, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.61463
[1mStep[0m  [2/21], [94mLoss[0m : 1.47191
[1mStep[0m  [4/21], [94mLoss[0m : 1.45866
[1mStep[0m  [6/21], [94mLoss[0m : 1.50193
[1mStep[0m  [8/21], [94mLoss[0m : 1.52087
[1mStep[0m  [10/21], [94mLoss[0m : 1.45859
[1mStep[0m  [12/21], [94mLoss[0m : 1.45882
[1mStep[0m  [14/21], [94mLoss[0m : 1.31737
[1mStep[0m  [16/21], [94mLoss[0m : 1.51282
[1mStep[0m  [18/21], [94mLoss[0m : 1.50118
[1mStep[0m  [20/21], [94mLoss[0m : 1.46234

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.473, [92mTest[0m: 2.539, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.547
====================================

Phase 2 - Evaluation MAE:  2.546966927392142
MAE score P1      2.348204
MAE score P2      2.546967
loss              1.472674
learning_rate     0.002575
batch_size             512
hidden_sizes         [300]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.9
weight_decay         0.001
Name: 6, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 10.77903
[1mStep[0m  [4/42], [94mLoss[0m : 10.88415
[1mStep[0m  [8/42], [94mLoss[0m : 11.05861
[1mStep[0m  [12/42], [94mLoss[0m : 10.25653
[1mStep[0m  [16/42], [94mLoss[0m : 10.58731
[1mStep[0m  [20/42], [94mLoss[0m : 10.03889
[1mStep[0m  [24/42], [94mLoss[0m : 10.06914
[1mStep[0m  [28/42], [94mLoss[0m : 10.33127
[1mStep[0m  [32/42], [94mLoss[0m : 10.24322
[1mStep[0m  [36/42], [94mLoss[0m : 9.54595
[1mStep[0m  [40/42], [94mLoss[0m : 9.80961

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.282, [92mTest[0m: 10.837, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.72405
[1mStep[0m  [4/42], [94mLoss[0m : 9.30469
[1mStep[0m  [8/42], [94mLoss[0m : 9.63940
[1mStep[0m  [12/42], [94mLoss[0m : 9.45139
[1mStep[0m  [16/42], [94mLoss[0m : 8.93754
[1mStep[0m  [20/42], [94mLoss[0m : 9.21262
[1mStep[0m  [24/42], [94mLoss[0m : 9.54405
[1mStep[0m  [28/42], [94mLoss[0m : 8.75547
[1mStep[0m  [32/42], [94mLoss[0m : 8.79513
[1mStep[0m  [36/42], [94mLoss[0m : 8.72053
[1mStep[0m  [40/42], [94mLoss[0m : 8.35840

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.251, [92mTest[0m: 10.202, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.05495
[1mStep[0m  [4/42], [94mLoss[0m : 8.48066
[1mStep[0m  [8/42], [94mLoss[0m : 8.39819
[1mStep[0m  [12/42], [94mLoss[0m : 8.46478
[1mStep[0m  [16/42], [94mLoss[0m : 8.27093
[1mStep[0m  [20/42], [94mLoss[0m : 8.13443
[1mStep[0m  [24/42], [94mLoss[0m : 7.75206
[1mStep[0m  [28/42], [94mLoss[0m : 7.90256
[1mStep[0m  [32/42], [94mLoss[0m : 8.10212
[1mStep[0m  [36/42], [94mLoss[0m : 7.97999
[1mStep[0m  [40/42], [94mLoss[0m : 7.61383

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 8.176, [92mTest[0m: 9.508, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.24925
[1mStep[0m  [4/42], [94mLoss[0m : 7.16715
[1mStep[0m  [8/42], [94mLoss[0m : 7.38928
[1mStep[0m  [12/42], [94mLoss[0m : 7.42018
[1mStep[0m  [16/42], [94mLoss[0m : 7.34911
[1mStep[0m  [20/42], [94mLoss[0m : 7.24812
[1mStep[0m  [24/42], [94mLoss[0m : 7.00353
[1mStep[0m  [28/42], [94mLoss[0m : 6.79343
[1mStep[0m  [32/42], [94mLoss[0m : 6.77863
[1mStep[0m  [36/42], [94mLoss[0m : 6.71965
[1mStep[0m  [40/42], [94mLoss[0m : 6.78517

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 7.099, [92mTest[0m: 8.793, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.83242
[1mStep[0m  [4/42], [94mLoss[0m : 6.49026
[1mStep[0m  [8/42], [94mLoss[0m : 6.71693
[1mStep[0m  [12/42], [94mLoss[0m : 6.19447
[1mStep[0m  [16/42], [94mLoss[0m : 6.06421
[1mStep[0m  [20/42], [94mLoss[0m : 6.56630
[1mStep[0m  [24/42], [94mLoss[0m : 5.98494
[1mStep[0m  [28/42], [94mLoss[0m : 5.92306
[1mStep[0m  [32/42], [94mLoss[0m : 5.43126
[1mStep[0m  [36/42], [94mLoss[0m : 5.56948
[1mStep[0m  [40/42], [94mLoss[0m : 5.78293

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 6.038, [92mTest[0m: 8.017, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.60474
[1mStep[0m  [4/42], [94mLoss[0m : 5.29011
[1mStep[0m  [8/42], [94mLoss[0m : 5.15178
[1mStep[0m  [12/42], [94mLoss[0m : 5.10312
[1mStep[0m  [16/42], [94mLoss[0m : 4.89605
[1mStep[0m  [20/42], [94mLoss[0m : 5.12207
[1mStep[0m  [24/42], [94mLoss[0m : 4.89605
[1mStep[0m  [28/42], [94mLoss[0m : 4.95733
[1mStep[0m  [32/42], [94mLoss[0m : 4.79124
[1mStep[0m  [36/42], [94mLoss[0m : 4.91444
[1mStep[0m  [40/42], [94mLoss[0m : 4.61162

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 5.026, [92mTest[0m: 6.999, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.51357
[1mStep[0m  [4/42], [94mLoss[0m : 4.95851
[1mStep[0m  [8/42], [94mLoss[0m : 4.35600
[1mStep[0m  [12/42], [94mLoss[0m : 4.24854
[1mStep[0m  [16/42], [94mLoss[0m : 4.13412
[1mStep[0m  [20/42], [94mLoss[0m : 4.02800
[1mStep[0m  [24/42], [94mLoss[0m : 3.98081
[1mStep[0m  [28/42], [94mLoss[0m : 4.13302
[1mStep[0m  [32/42], [94mLoss[0m : 4.07463
[1mStep[0m  [36/42], [94mLoss[0m : 4.02637
[1mStep[0m  [40/42], [94mLoss[0m : 4.26528

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 4.222, [92mTest[0m: 5.891, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.00254
[1mStep[0m  [4/42], [94mLoss[0m : 3.50470
[1mStep[0m  [8/42], [94mLoss[0m : 3.39822
[1mStep[0m  [12/42], [94mLoss[0m : 3.67179
[1mStep[0m  [16/42], [94mLoss[0m : 3.56808
[1mStep[0m  [20/42], [94mLoss[0m : 3.72339
[1mStep[0m  [24/42], [94mLoss[0m : 3.66287
[1mStep[0m  [28/42], [94mLoss[0m : 3.68758
[1mStep[0m  [32/42], [94mLoss[0m : 3.40159
[1mStep[0m  [36/42], [94mLoss[0m : 3.54824
[1mStep[0m  [40/42], [94mLoss[0m : 3.40714

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 3.656, [92mTest[0m: 5.053, [96mlr[0m: 0.002575
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 7 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 4.400
====================================

Phase 1 - Evaluation MAE:  4.399741172790527
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 3.46939
[1mStep[0m  [4/42], [94mLoss[0m : 3.49475
[1mStep[0m  [8/42], [94mLoss[0m : 3.78920
[1mStep[0m  [12/42], [94mLoss[0m : 3.15427
[1mStep[0m  [16/42], [94mLoss[0m : 3.26872
[1mStep[0m  [20/42], [94mLoss[0m : 3.60865
[1mStep[0m  [24/42], [94mLoss[0m : 3.33388
[1mStep[0m  [28/42], [94mLoss[0m : 3.50045
[1mStep[0m  [32/42], [94mLoss[0m : 3.31758
[1mStep[0m  [36/42], [94mLoss[0m : 3.71188
[1mStep[0m  [40/42], [94mLoss[0m : 3.41410

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.403, [92mTest[0m: 4.395, [96mlr[0m: 0.002575
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 0 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 3.257
====================================

Phase 2 - Evaluation MAE:  3.256973913737706
MAE score P1       4.399741
MAE score P2       3.256974
loss               3.403304
learning_rate      0.002575
batch_size              256
hidden_sizes      [250, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping         True
dropout                 0.4
momentum                0.1
weight_decay           0.01
Name: 7, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 10.77465
[1mStep[0m  [8/84], [94mLoss[0m : 9.72440
[1mStep[0m  [16/84], [94mLoss[0m : 8.39225
[1mStep[0m  [24/84], [94mLoss[0m : 5.21088
[1mStep[0m  [32/84], [94mLoss[0m : 3.28015
[1mStep[0m  [40/84], [94mLoss[0m : 2.84742
[1mStep[0m  [48/84], [94mLoss[0m : 2.54026
[1mStep[0m  [56/84], [94mLoss[0m : 2.58454
[1mStep[0m  [64/84], [94mLoss[0m : 2.48016
[1mStep[0m  [72/84], [94mLoss[0m : 2.52127
[1mStep[0m  [80/84], [94mLoss[0m : 2.58169

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.571, [92mTest[0m: 10.735, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44392
[1mStep[0m  [8/84], [94mLoss[0m : 2.39457
[1mStep[0m  [16/84], [94mLoss[0m : 2.46100
[1mStep[0m  [24/84], [94mLoss[0m : 2.45724
[1mStep[0m  [32/84], [94mLoss[0m : 2.64047
[1mStep[0m  [40/84], [94mLoss[0m : 2.64787
[1mStep[0m  [48/84], [94mLoss[0m : 2.47405
[1mStep[0m  [56/84], [94mLoss[0m : 2.52672
[1mStep[0m  [64/84], [94mLoss[0m : 2.02904
[1mStep[0m  [72/84], [94mLoss[0m : 2.52171
[1mStep[0m  [80/84], [94mLoss[0m : 2.76874

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.401, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.29070
[1mStep[0m  [8/84], [94mLoss[0m : 2.51461
[1mStep[0m  [16/84], [94mLoss[0m : 2.63637
[1mStep[0m  [24/84], [94mLoss[0m : 2.48783
[1mStep[0m  [32/84], [94mLoss[0m : 2.77027
[1mStep[0m  [40/84], [94mLoss[0m : 2.74545
[1mStep[0m  [48/84], [94mLoss[0m : 2.56204
[1mStep[0m  [56/84], [94mLoss[0m : 2.12987
[1mStep[0m  [64/84], [94mLoss[0m : 2.33247
[1mStep[0m  [72/84], [94mLoss[0m : 2.39573
[1mStep[0m  [80/84], [94mLoss[0m : 2.70678

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.352, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47629
[1mStep[0m  [8/84], [94mLoss[0m : 2.59179
[1mStep[0m  [16/84], [94mLoss[0m : 2.25004
[1mStep[0m  [24/84], [94mLoss[0m : 2.46480
[1mStep[0m  [32/84], [94mLoss[0m : 2.26369
[1mStep[0m  [40/84], [94mLoss[0m : 2.51020
[1mStep[0m  [48/84], [94mLoss[0m : 2.21932
[1mStep[0m  [56/84], [94mLoss[0m : 2.44017
[1mStep[0m  [64/84], [94mLoss[0m : 2.68012
[1mStep[0m  [72/84], [94mLoss[0m : 2.29739
[1mStep[0m  [80/84], [94mLoss[0m : 2.67833

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.335, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.05149
[1mStep[0m  [8/84], [94mLoss[0m : 2.59757
[1mStep[0m  [16/84], [94mLoss[0m : 2.31462
[1mStep[0m  [24/84], [94mLoss[0m : 2.15763
[1mStep[0m  [32/84], [94mLoss[0m : 2.20235
[1mStep[0m  [40/84], [94mLoss[0m : 2.32108
[1mStep[0m  [48/84], [94mLoss[0m : 2.70054
[1mStep[0m  [56/84], [94mLoss[0m : 2.57347
[1mStep[0m  [64/84], [94mLoss[0m : 2.41248
[1mStep[0m  [72/84], [94mLoss[0m : 2.37148
[1mStep[0m  [80/84], [94mLoss[0m : 2.58694

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.329, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.64448
[1mStep[0m  [8/84], [94mLoss[0m : 2.50562
[1mStep[0m  [16/84], [94mLoss[0m : 2.28928
[1mStep[0m  [24/84], [94mLoss[0m : 2.25976
[1mStep[0m  [32/84], [94mLoss[0m : 2.85739
[1mStep[0m  [40/84], [94mLoss[0m : 2.37979
[1mStep[0m  [48/84], [94mLoss[0m : 2.54549
[1mStep[0m  [56/84], [94mLoss[0m : 2.29906
[1mStep[0m  [64/84], [94mLoss[0m : 2.18034
[1mStep[0m  [72/84], [94mLoss[0m : 2.50211
[1mStep[0m  [80/84], [94mLoss[0m : 2.76057

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.339, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44843
[1mStep[0m  [8/84], [94mLoss[0m : 2.67392
[1mStep[0m  [16/84], [94mLoss[0m : 2.63898
[1mStep[0m  [24/84], [94mLoss[0m : 2.42604
[1mStep[0m  [32/84], [94mLoss[0m : 2.49460
[1mStep[0m  [40/84], [94mLoss[0m : 2.40701
[1mStep[0m  [48/84], [94mLoss[0m : 2.63594
[1mStep[0m  [56/84], [94mLoss[0m : 2.61950
[1mStep[0m  [64/84], [94mLoss[0m : 2.47076
[1mStep[0m  [72/84], [94mLoss[0m : 2.41157
[1mStep[0m  [80/84], [94mLoss[0m : 2.57908

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.328, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.51058
[1mStep[0m  [8/84], [94mLoss[0m : 2.32376
[1mStep[0m  [16/84], [94mLoss[0m : 2.29134
[1mStep[0m  [24/84], [94mLoss[0m : 2.33859
[1mStep[0m  [32/84], [94mLoss[0m : 2.73930
[1mStep[0m  [40/84], [94mLoss[0m : 2.59525
[1mStep[0m  [48/84], [94mLoss[0m : 2.46113
[1mStep[0m  [56/84], [94mLoss[0m : 2.48859
[1mStep[0m  [64/84], [94mLoss[0m : 2.48029
[1mStep[0m  [72/84], [94mLoss[0m : 2.42475
[1mStep[0m  [80/84], [94mLoss[0m : 2.34636

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.334, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50818
[1mStep[0m  [8/84], [94mLoss[0m : 2.68841
[1mStep[0m  [16/84], [94mLoss[0m : 2.42111
[1mStep[0m  [24/84], [94mLoss[0m : 2.15727
[1mStep[0m  [32/84], [94mLoss[0m : 2.45922
[1mStep[0m  [40/84], [94mLoss[0m : 2.58734
[1mStep[0m  [48/84], [94mLoss[0m : 2.51161
[1mStep[0m  [56/84], [94mLoss[0m : 2.44458
[1mStep[0m  [64/84], [94mLoss[0m : 2.42955
[1mStep[0m  [72/84], [94mLoss[0m : 2.46475
[1mStep[0m  [80/84], [94mLoss[0m : 2.43701

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.331, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52090
[1mStep[0m  [8/84], [94mLoss[0m : 2.21369
[1mStep[0m  [16/84], [94mLoss[0m : 2.33799
[1mStep[0m  [24/84], [94mLoss[0m : 2.26955
[1mStep[0m  [32/84], [94mLoss[0m : 2.29702
[1mStep[0m  [40/84], [94mLoss[0m : 2.53139
[1mStep[0m  [48/84], [94mLoss[0m : 2.39208
[1mStep[0m  [56/84], [94mLoss[0m : 2.27996
[1mStep[0m  [64/84], [94mLoss[0m : 2.82915
[1mStep[0m  [72/84], [94mLoss[0m : 2.47517
[1mStep[0m  [80/84], [94mLoss[0m : 2.30654

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.337, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.69999
[1mStep[0m  [8/84], [94mLoss[0m : 2.52101
[1mStep[0m  [16/84], [94mLoss[0m : 2.53656
[1mStep[0m  [24/84], [94mLoss[0m : 2.58103
[1mStep[0m  [32/84], [94mLoss[0m : 2.36375
[1mStep[0m  [40/84], [94mLoss[0m : 2.58138
[1mStep[0m  [48/84], [94mLoss[0m : 2.59143
[1mStep[0m  [56/84], [94mLoss[0m : 2.68231
[1mStep[0m  [64/84], [94mLoss[0m : 2.62171
[1mStep[0m  [72/84], [94mLoss[0m : 2.21798
[1mStep[0m  [80/84], [94mLoss[0m : 2.53493

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.335, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46005
[1mStep[0m  [8/84], [94mLoss[0m : 2.46016
[1mStep[0m  [16/84], [94mLoss[0m : 2.57334
[1mStep[0m  [24/84], [94mLoss[0m : 2.44681
[1mStep[0m  [32/84], [94mLoss[0m : 2.57997
[1mStep[0m  [40/84], [94mLoss[0m : 2.54209
[1mStep[0m  [48/84], [94mLoss[0m : 2.58641
[1mStep[0m  [56/84], [94mLoss[0m : 2.45633
[1mStep[0m  [64/84], [94mLoss[0m : 2.44804
[1mStep[0m  [72/84], [94mLoss[0m : 2.50937
[1mStep[0m  [80/84], [94mLoss[0m : 2.50921

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.330, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.34524
[1mStep[0m  [8/84], [94mLoss[0m : 2.41133
[1mStep[0m  [16/84], [94mLoss[0m : 2.54562
[1mStep[0m  [24/84], [94mLoss[0m : 2.50345
[1mStep[0m  [32/84], [94mLoss[0m : 2.71833
[1mStep[0m  [40/84], [94mLoss[0m : 2.42261
[1mStep[0m  [48/84], [94mLoss[0m : 2.37919
[1mStep[0m  [56/84], [94mLoss[0m : 2.36931
[1mStep[0m  [64/84], [94mLoss[0m : 2.54792
[1mStep[0m  [72/84], [94mLoss[0m : 2.35646
[1mStep[0m  [80/84], [94mLoss[0m : 2.31021

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.326, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.71386
[1mStep[0m  [8/84], [94mLoss[0m : 2.43785
[1mStep[0m  [16/84], [94mLoss[0m : 2.52801
[1mStep[0m  [24/84], [94mLoss[0m : 2.15322
[1mStep[0m  [32/84], [94mLoss[0m : 2.42534
[1mStep[0m  [40/84], [94mLoss[0m : 2.36595
[1mStep[0m  [48/84], [94mLoss[0m : 2.19039
[1mStep[0m  [56/84], [94mLoss[0m : 2.35416
[1mStep[0m  [64/84], [94mLoss[0m : 2.31728
[1mStep[0m  [72/84], [94mLoss[0m : 2.77719
[1mStep[0m  [80/84], [94mLoss[0m : 2.37803

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.324, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.70355
[1mStep[0m  [8/84], [94mLoss[0m : 2.20190
[1mStep[0m  [16/84], [94mLoss[0m : 2.52989
[1mStep[0m  [24/84], [94mLoss[0m : 2.57314
[1mStep[0m  [32/84], [94mLoss[0m : 2.59101
[1mStep[0m  [40/84], [94mLoss[0m : 2.37532
[1mStep[0m  [48/84], [94mLoss[0m : 2.46259
[1mStep[0m  [56/84], [94mLoss[0m : 2.50675
[1mStep[0m  [64/84], [94mLoss[0m : 2.47346
[1mStep[0m  [72/84], [94mLoss[0m : 2.21608
[1mStep[0m  [80/84], [94mLoss[0m : 2.50589

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.332, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44386
[1mStep[0m  [8/84], [94mLoss[0m : 2.24486
[1mStep[0m  [16/84], [94mLoss[0m : 2.43516
[1mStep[0m  [24/84], [94mLoss[0m : 2.46925
[1mStep[0m  [32/84], [94mLoss[0m : 2.61551
[1mStep[0m  [40/84], [94mLoss[0m : 2.50375
[1mStep[0m  [48/84], [94mLoss[0m : 2.50233
[1mStep[0m  [56/84], [94mLoss[0m : 2.51725
[1mStep[0m  [64/84], [94mLoss[0m : 2.35783
[1mStep[0m  [72/84], [94mLoss[0m : 2.20051
[1mStep[0m  [80/84], [94mLoss[0m : 2.20141

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.331, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40656
[1mStep[0m  [8/84], [94mLoss[0m : 2.38747
[1mStep[0m  [16/84], [94mLoss[0m : 2.24720
[1mStep[0m  [24/84], [94mLoss[0m : 2.37358
[1mStep[0m  [32/84], [94mLoss[0m : 2.44828
[1mStep[0m  [40/84], [94mLoss[0m : 2.38779
[1mStep[0m  [48/84], [94mLoss[0m : 2.16022
[1mStep[0m  [56/84], [94mLoss[0m : 2.46318
[1mStep[0m  [64/84], [94mLoss[0m : 2.33843
[1mStep[0m  [72/84], [94mLoss[0m : 2.29470
[1mStep[0m  [80/84], [94mLoss[0m : 2.48750

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.332, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.07081
[1mStep[0m  [8/84], [94mLoss[0m : 2.41951
[1mStep[0m  [16/84], [94mLoss[0m : 2.38944
[1mStep[0m  [24/84], [94mLoss[0m : 2.88177
[1mStep[0m  [32/84], [94mLoss[0m : 2.36815
[1mStep[0m  [40/84], [94mLoss[0m : 2.15800
[1mStep[0m  [48/84], [94mLoss[0m : 2.65809
[1mStep[0m  [56/84], [94mLoss[0m : 2.41227
[1mStep[0m  [64/84], [94mLoss[0m : 2.33893
[1mStep[0m  [72/84], [94mLoss[0m : 2.59034
[1mStep[0m  [80/84], [94mLoss[0m : 2.21259

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.326, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.63256
[1mStep[0m  [8/84], [94mLoss[0m : 2.78965
[1mStep[0m  [16/84], [94mLoss[0m : 2.40474
[1mStep[0m  [24/84], [94mLoss[0m : 2.48943
[1mStep[0m  [32/84], [94mLoss[0m : 2.47533
[1mStep[0m  [40/84], [94mLoss[0m : 2.54286
[1mStep[0m  [48/84], [94mLoss[0m : 2.48395
[1mStep[0m  [56/84], [94mLoss[0m : 2.54563
[1mStep[0m  [64/84], [94mLoss[0m : 2.43278
[1mStep[0m  [72/84], [94mLoss[0m : 2.39684
[1mStep[0m  [80/84], [94mLoss[0m : 2.49391

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.324, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43008
[1mStep[0m  [8/84], [94mLoss[0m : 2.59541
[1mStep[0m  [16/84], [94mLoss[0m : 2.46313
[1mStep[0m  [24/84], [94mLoss[0m : 2.39407
[1mStep[0m  [32/84], [94mLoss[0m : 2.71935
[1mStep[0m  [40/84], [94mLoss[0m : 2.55297
[1mStep[0m  [48/84], [94mLoss[0m : 2.43359
[1mStep[0m  [56/84], [94mLoss[0m : 2.48490
[1mStep[0m  [64/84], [94mLoss[0m : 2.28722
[1mStep[0m  [72/84], [94mLoss[0m : 2.47532
[1mStep[0m  [80/84], [94mLoss[0m : 2.39126

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.331, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26681
[1mStep[0m  [8/84], [94mLoss[0m : 2.44186
[1mStep[0m  [16/84], [94mLoss[0m : 2.19222
[1mStep[0m  [24/84], [94mLoss[0m : 2.58954
[1mStep[0m  [32/84], [94mLoss[0m : 2.63119
[1mStep[0m  [40/84], [94mLoss[0m : 2.61729
[1mStep[0m  [48/84], [94mLoss[0m : 2.44793
[1mStep[0m  [56/84], [94mLoss[0m : 2.48205
[1mStep[0m  [64/84], [94mLoss[0m : 2.78729
[1mStep[0m  [72/84], [94mLoss[0m : 2.36377
[1mStep[0m  [80/84], [94mLoss[0m : 2.12922

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.329, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.57784
[1mStep[0m  [8/84], [94mLoss[0m : 2.34741
[1mStep[0m  [16/84], [94mLoss[0m : 2.36140
[1mStep[0m  [24/84], [94mLoss[0m : 2.61489
[1mStep[0m  [32/84], [94mLoss[0m : 2.67230
[1mStep[0m  [40/84], [94mLoss[0m : 2.41467
[1mStep[0m  [48/84], [94mLoss[0m : 2.70984
[1mStep[0m  [56/84], [94mLoss[0m : 2.58345
[1mStep[0m  [64/84], [94mLoss[0m : 2.34469
[1mStep[0m  [72/84], [94mLoss[0m : 2.57001
[1mStep[0m  [80/84], [94mLoss[0m : 2.59826

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.332, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.51248
[1mStep[0m  [8/84], [94mLoss[0m : 2.70967
[1mStep[0m  [16/84], [94mLoss[0m : 2.61188
[1mStep[0m  [24/84], [94mLoss[0m : 2.12331
[1mStep[0m  [32/84], [94mLoss[0m : 2.23571
[1mStep[0m  [40/84], [94mLoss[0m : 2.55341
[1mStep[0m  [48/84], [94mLoss[0m : 2.48403
[1mStep[0m  [56/84], [94mLoss[0m : 2.61220
[1mStep[0m  [64/84], [94mLoss[0m : 2.43062
[1mStep[0m  [72/84], [94mLoss[0m : 2.32470
[1mStep[0m  [80/84], [94mLoss[0m : 2.34382

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.330, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.32431
[1mStep[0m  [8/84], [94mLoss[0m : 2.35583
[1mStep[0m  [16/84], [94mLoss[0m : 2.41267
[1mStep[0m  [24/84], [94mLoss[0m : 2.41401
[1mStep[0m  [32/84], [94mLoss[0m : 2.35335
[1mStep[0m  [40/84], [94mLoss[0m : 2.54078
[1mStep[0m  [48/84], [94mLoss[0m : 2.41153
[1mStep[0m  [56/84], [94mLoss[0m : 2.56474
[1mStep[0m  [64/84], [94mLoss[0m : 2.64712
[1mStep[0m  [72/84], [94mLoss[0m : 2.52815
[1mStep[0m  [80/84], [94mLoss[0m : 2.32631

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.327, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50494
[1mStep[0m  [8/84], [94mLoss[0m : 2.39800
[1mStep[0m  [16/84], [94mLoss[0m : 2.38692
[1mStep[0m  [24/84], [94mLoss[0m : 2.52046
[1mStep[0m  [32/84], [94mLoss[0m : 2.27833
[1mStep[0m  [40/84], [94mLoss[0m : 2.51127
[1mStep[0m  [48/84], [94mLoss[0m : 2.60737
[1mStep[0m  [56/84], [94mLoss[0m : 2.46096
[1mStep[0m  [64/84], [94mLoss[0m : 2.33088
[1mStep[0m  [72/84], [94mLoss[0m : 2.61962
[1mStep[0m  [80/84], [94mLoss[0m : 2.53240

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.331, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33818
[1mStep[0m  [8/84], [94mLoss[0m : 2.56088
[1mStep[0m  [16/84], [94mLoss[0m : 2.51356
[1mStep[0m  [24/84], [94mLoss[0m : 2.11405
[1mStep[0m  [32/84], [94mLoss[0m : 2.37047
[1mStep[0m  [40/84], [94mLoss[0m : 2.24013
[1mStep[0m  [48/84], [94mLoss[0m : 2.48230
[1mStep[0m  [56/84], [94mLoss[0m : 2.28089
[1mStep[0m  [64/84], [94mLoss[0m : 2.70126
[1mStep[0m  [72/84], [94mLoss[0m : 2.30535
[1mStep[0m  [80/84], [94mLoss[0m : 2.60956

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.312, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.23696
[1mStep[0m  [8/84], [94mLoss[0m : 2.68729
[1mStep[0m  [16/84], [94mLoss[0m : 2.27109
[1mStep[0m  [24/84], [94mLoss[0m : 2.34783
[1mStep[0m  [32/84], [94mLoss[0m : 2.28614
[1mStep[0m  [40/84], [94mLoss[0m : 2.60988
[1mStep[0m  [48/84], [94mLoss[0m : 2.44763
[1mStep[0m  [56/84], [94mLoss[0m : 2.67545
[1mStep[0m  [64/84], [94mLoss[0m : 2.27924
[1mStep[0m  [72/84], [94mLoss[0m : 2.37303
[1mStep[0m  [80/84], [94mLoss[0m : 2.51990

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.326, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55138
[1mStep[0m  [8/84], [94mLoss[0m : 2.46600
[1mStep[0m  [16/84], [94mLoss[0m : 2.42896
[1mStep[0m  [24/84], [94mLoss[0m : 2.10894
[1mStep[0m  [32/84], [94mLoss[0m : 2.42195
[1mStep[0m  [40/84], [94mLoss[0m : 2.39711
[1mStep[0m  [48/84], [94mLoss[0m : 2.48181
[1mStep[0m  [56/84], [94mLoss[0m : 2.47696
[1mStep[0m  [64/84], [94mLoss[0m : 2.22190
[1mStep[0m  [72/84], [94mLoss[0m : 2.53268
[1mStep[0m  [80/84], [94mLoss[0m : 2.44960

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.332, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41274
[1mStep[0m  [8/84], [94mLoss[0m : 2.20761
[1mStep[0m  [16/84], [94mLoss[0m : 2.79935
[1mStep[0m  [24/84], [94mLoss[0m : 2.52411
[1mStep[0m  [32/84], [94mLoss[0m : 2.37499
[1mStep[0m  [40/84], [94mLoss[0m : 2.53847
[1mStep[0m  [48/84], [94mLoss[0m : 2.38645
[1mStep[0m  [56/84], [94mLoss[0m : 2.57766
[1mStep[0m  [64/84], [94mLoss[0m : 2.06464
[1mStep[0m  [72/84], [94mLoss[0m : 2.43538
[1mStep[0m  [80/84], [94mLoss[0m : 2.56395

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.330, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.73305
[1mStep[0m  [8/84], [94mLoss[0m : 2.46403
[1mStep[0m  [16/84], [94mLoss[0m : 2.52661
[1mStep[0m  [24/84], [94mLoss[0m : 2.36273
[1mStep[0m  [32/84], [94mLoss[0m : 2.39436
[1mStep[0m  [40/84], [94mLoss[0m : 2.35730
[1mStep[0m  [48/84], [94mLoss[0m : 2.32626
[1mStep[0m  [56/84], [94mLoss[0m : 2.34877
[1mStep[0m  [64/84], [94mLoss[0m : 2.75782
[1mStep[0m  [72/84], [94mLoss[0m : 2.36533
[1mStep[0m  [80/84], [94mLoss[0m : 2.29264

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.337, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.326
====================================

Phase 1 - Evaluation MAE:  2.3264488662992204
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 2.42992
[1mStep[0m  [8/84], [94mLoss[0m : 2.45436
[1mStep[0m  [16/84], [94mLoss[0m : 2.34908
[1mStep[0m  [24/84], [94mLoss[0m : 2.42392
[1mStep[0m  [32/84], [94mLoss[0m : 2.62164
[1mStep[0m  [40/84], [94mLoss[0m : 2.66889
[1mStep[0m  [48/84], [94mLoss[0m : 2.53303
[1mStep[0m  [56/84], [94mLoss[0m : 2.44103
[1mStep[0m  [64/84], [94mLoss[0m : 2.48629
[1mStep[0m  [72/84], [94mLoss[0m : 2.56571
[1mStep[0m  [80/84], [94mLoss[0m : 2.56922

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.322, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.10112
[1mStep[0m  [8/84], [94mLoss[0m : 2.36021
[1mStep[0m  [16/84], [94mLoss[0m : 2.26941
[1mStep[0m  [24/84], [94mLoss[0m : 2.23222
[1mStep[0m  [32/84], [94mLoss[0m : 2.44263
[1mStep[0m  [40/84], [94mLoss[0m : 2.60836
[1mStep[0m  [48/84], [94mLoss[0m : 2.34889
[1mStep[0m  [56/84], [94mLoss[0m : 2.30004
[1mStep[0m  [64/84], [94mLoss[0m : 2.24169
[1mStep[0m  [72/84], [94mLoss[0m : 2.25517
[1mStep[0m  [80/84], [94mLoss[0m : 2.68356

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.394, [92mTest[0m: 2.330, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.21573
[1mStep[0m  [8/84], [94mLoss[0m : 2.62013
[1mStep[0m  [16/84], [94mLoss[0m : 2.51170
[1mStep[0m  [24/84], [94mLoss[0m : 2.44832
[1mStep[0m  [32/84], [94mLoss[0m : 2.37743
[1mStep[0m  [40/84], [94mLoss[0m : 2.37584
[1mStep[0m  [48/84], [94mLoss[0m : 2.27499
[1mStep[0m  [56/84], [94mLoss[0m : 2.15133
[1mStep[0m  [64/84], [94mLoss[0m : 2.28652
[1mStep[0m  [72/84], [94mLoss[0m : 2.34780
[1mStep[0m  [80/84], [94mLoss[0m : 2.54232

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.324, [92mTest[0m: 2.338, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.02012
[1mStep[0m  [8/84], [94mLoss[0m : 1.93753
[1mStep[0m  [16/84], [94mLoss[0m : 2.52284
[1mStep[0m  [24/84], [94mLoss[0m : 2.40408
[1mStep[0m  [32/84], [94mLoss[0m : 2.34211
[1mStep[0m  [40/84], [94mLoss[0m : 2.07072
[1mStep[0m  [48/84], [94mLoss[0m : 2.29978
[1mStep[0m  [56/84], [94mLoss[0m : 2.20974
[1mStep[0m  [64/84], [94mLoss[0m : 2.35797
[1mStep[0m  [72/84], [94mLoss[0m : 2.07973
[1mStep[0m  [80/84], [94mLoss[0m : 2.36217

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.267, [92mTest[0m: 2.331, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.16258
[1mStep[0m  [8/84], [94mLoss[0m : 2.28874
[1mStep[0m  [16/84], [94mLoss[0m : 1.95739
[1mStep[0m  [24/84], [94mLoss[0m : 2.34302
[1mStep[0m  [32/84], [94mLoss[0m : 2.33103
[1mStep[0m  [40/84], [94mLoss[0m : 2.20843
[1mStep[0m  [48/84], [94mLoss[0m : 2.26745
[1mStep[0m  [56/84], [94mLoss[0m : 2.02820
[1mStep[0m  [64/84], [94mLoss[0m : 2.21535
[1mStep[0m  [72/84], [94mLoss[0m : 2.07576
[1mStep[0m  [80/84], [94mLoss[0m : 2.12445

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.194, [92mTest[0m: 2.347, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.09596
[1mStep[0m  [8/84], [94mLoss[0m : 1.84825
[1mStep[0m  [16/84], [94mLoss[0m : 1.91266
[1mStep[0m  [24/84], [94mLoss[0m : 2.15011
[1mStep[0m  [32/84], [94mLoss[0m : 2.19553
[1mStep[0m  [40/84], [94mLoss[0m : 2.27269
[1mStep[0m  [48/84], [94mLoss[0m : 2.14965
[1mStep[0m  [56/84], [94mLoss[0m : 2.42511
[1mStep[0m  [64/84], [94mLoss[0m : 2.19272
[1mStep[0m  [72/84], [94mLoss[0m : 2.34744
[1mStep[0m  [80/84], [94mLoss[0m : 2.33172

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.140, [92mTest[0m: 2.365, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.97682
[1mStep[0m  [8/84], [94mLoss[0m : 2.14608
[1mStep[0m  [16/84], [94mLoss[0m : 2.18579
[1mStep[0m  [24/84], [94mLoss[0m : 1.92097
[1mStep[0m  [32/84], [94mLoss[0m : 2.07761
[1mStep[0m  [40/84], [94mLoss[0m : 2.34786
[1mStep[0m  [48/84], [94mLoss[0m : 2.06334
[1mStep[0m  [56/84], [94mLoss[0m : 2.04607
[1mStep[0m  [64/84], [94mLoss[0m : 2.54621
[1mStep[0m  [72/84], [94mLoss[0m : 2.01767
[1mStep[0m  [80/84], [94mLoss[0m : 2.09224

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.096, [92mTest[0m: 2.372, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.82037
[1mStep[0m  [8/84], [94mLoss[0m : 1.89935
[1mStep[0m  [16/84], [94mLoss[0m : 2.05712
[1mStep[0m  [24/84], [94mLoss[0m : 1.93662
[1mStep[0m  [32/84], [94mLoss[0m : 2.31979
[1mStep[0m  [40/84], [94mLoss[0m : 2.23215
[1mStep[0m  [48/84], [94mLoss[0m : 2.02756
[1mStep[0m  [56/84], [94mLoss[0m : 1.98602
[1mStep[0m  [64/84], [94mLoss[0m : 2.13401
[1mStep[0m  [72/84], [94mLoss[0m : 1.98525
[1mStep[0m  [80/84], [94mLoss[0m : 2.07746

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.044, [92mTest[0m: 2.391, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.97111
[1mStep[0m  [8/84], [94mLoss[0m : 1.87624
[1mStep[0m  [16/84], [94mLoss[0m : 1.98324
[1mStep[0m  [24/84], [94mLoss[0m : 1.80881
[1mStep[0m  [32/84], [94mLoss[0m : 1.91280
[1mStep[0m  [40/84], [94mLoss[0m : 2.24029
[1mStep[0m  [48/84], [94mLoss[0m : 2.07750
[1mStep[0m  [56/84], [94mLoss[0m : 2.09728
[1mStep[0m  [64/84], [94mLoss[0m : 1.95653
[1mStep[0m  [72/84], [94mLoss[0m : 2.22192
[1mStep[0m  [80/84], [94mLoss[0m : 2.21450

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.008, [92mTest[0m: 2.462, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.00158
[1mStep[0m  [8/84], [94mLoss[0m : 2.03512
[1mStep[0m  [16/84], [94mLoss[0m : 2.03326
[1mStep[0m  [24/84], [94mLoss[0m : 2.04243
[1mStep[0m  [32/84], [94mLoss[0m : 1.70566
[1mStep[0m  [40/84], [94mLoss[0m : 2.08740
[1mStep[0m  [48/84], [94mLoss[0m : 1.85388
[1mStep[0m  [56/84], [94mLoss[0m : 2.27984
[1mStep[0m  [64/84], [94mLoss[0m : 1.90120
[1mStep[0m  [72/84], [94mLoss[0m : 2.02853
[1mStep[0m  [80/84], [94mLoss[0m : 2.05204

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.972, [92mTest[0m: 2.525, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.81638
[1mStep[0m  [8/84], [94mLoss[0m : 1.94517
[1mStep[0m  [16/84], [94mLoss[0m : 1.76127
[1mStep[0m  [24/84], [94mLoss[0m : 1.83803
[1mStep[0m  [32/84], [94mLoss[0m : 1.71514
[1mStep[0m  [40/84], [94mLoss[0m : 1.93735
[1mStep[0m  [48/84], [94mLoss[0m : 1.65007
[1mStep[0m  [56/84], [94mLoss[0m : 2.13597
[1mStep[0m  [64/84], [94mLoss[0m : 2.09645
[1mStep[0m  [72/84], [94mLoss[0m : 1.87963
[1mStep[0m  [80/84], [94mLoss[0m : 1.92391

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.936, [92mTest[0m: 2.436, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.91548
[1mStep[0m  [8/84], [94mLoss[0m : 1.75885
[1mStep[0m  [16/84], [94mLoss[0m : 1.78996
[1mStep[0m  [24/84], [94mLoss[0m : 1.91750
[1mStep[0m  [32/84], [94mLoss[0m : 1.91427
[1mStep[0m  [40/84], [94mLoss[0m : 1.89483
[1mStep[0m  [48/84], [94mLoss[0m : 1.82083
[1mStep[0m  [56/84], [94mLoss[0m : 1.83771
[1mStep[0m  [64/84], [94mLoss[0m : 1.86069
[1mStep[0m  [72/84], [94mLoss[0m : 1.99321
[1mStep[0m  [80/84], [94mLoss[0m : 1.81920

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.886, [92mTest[0m: 2.393, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.81002
[1mStep[0m  [8/84], [94mLoss[0m : 1.65921
[1mStep[0m  [16/84], [94mLoss[0m : 1.84424
[1mStep[0m  [24/84], [94mLoss[0m : 1.59012
[1mStep[0m  [32/84], [94mLoss[0m : 1.64469
[1mStep[0m  [40/84], [94mLoss[0m : 1.95886
[1mStep[0m  [48/84], [94mLoss[0m : 1.94008
[1mStep[0m  [56/84], [94mLoss[0m : 1.95377
[1mStep[0m  [64/84], [94mLoss[0m : 1.89879
[1mStep[0m  [72/84], [94mLoss[0m : 1.89656
[1mStep[0m  [80/84], [94mLoss[0m : 1.81162

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.882, [92mTest[0m: 2.491, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.70945
[1mStep[0m  [8/84], [94mLoss[0m : 1.81741
[1mStep[0m  [16/84], [94mLoss[0m : 1.72310
[1mStep[0m  [24/84], [94mLoss[0m : 1.88272
[1mStep[0m  [32/84], [94mLoss[0m : 2.15452
[1mStep[0m  [40/84], [94mLoss[0m : 1.94222
[1mStep[0m  [48/84], [94mLoss[0m : 1.87400
[1mStep[0m  [56/84], [94mLoss[0m : 1.98248
[1mStep[0m  [64/84], [94mLoss[0m : 2.03592
[1mStep[0m  [72/84], [94mLoss[0m : 1.80967
[1mStep[0m  [80/84], [94mLoss[0m : 1.91327

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.841, [92mTest[0m: 2.468, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.64353
[1mStep[0m  [8/84], [94mLoss[0m : 1.55958
[1mStep[0m  [16/84], [94mLoss[0m : 1.68141
[1mStep[0m  [24/84], [94mLoss[0m : 1.69738
[1mStep[0m  [32/84], [94mLoss[0m : 1.60283
[1mStep[0m  [40/84], [94mLoss[0m : 2.01489
[1mStep[0m  [48/84], [94mLoss[0m : 1.64697
[1mStep[0m  [56/84], [94mLoss[0m : 1.86059
[1mStep[0m  [64/84], [94mLoss[0m : 1.90235
[1mStep[0m  [72/84], [94mLoss[0m : 1.87796
[1mStep[0m  [80/84], [94mLoss[0m : 1.48404

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.810, [92mTest[0m: 2.425, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.74130
[1mStep[0m  [8/84], [94mLoss[0m : 1.93868
[1mStep[0m  [16/84], [94mLoss[0m : 1.87277
[1mStep[0m  [24/84], [94mLoss[0m : 1.67541
[1mStep[0m  [32/84], [94mLoss[0m : 1.83151
[1mStep[0m  [40/84], [94mLoss[0m : 1.69804
[1mStep[0m  [48/84], [94mLoss[0m : 1.74896
[1mStep[0m  [56/84], [94mLoss[0m : 1.78159
[1mStep[0m  [64/84], [94mLoss[0m : 1.91687
[1mStep[0m  [72/84], [94mLoss[0m : 1.75180
[1mStep[0m  [80/84], [94mLoss[0m : 2.03724

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.785, [92mTest[0m: 2.476, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.92845
[1mStep[0m  [8/84], [94mLoss[0m : 1.75628
[1mStep[0m  [16/84], [94mLoss[0m : 1.92229
[1mStep[0m  [24/84], [94mLoss[0m : 1.89746
[1mStep[0m  [32/84], [94mLoss[0m : 1.80221
[1mStep[0m  [40/84], [94mLoss[0m : 1.36496
[1mStep[0m  [48/84], [94mLoss[0m : 1.88555
[1mStep[0m  [56/84], [94mLoss[0m : 1.58653
[1mStep[0m  [64/84], [94mLoss[0m : 1.70650
[1mStep[0m  [72/84], [94mLoss[0m : 1.98252
[1mStep[0m  [80/84], [94mLoss[0m : 2.00457

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.784, [92mTest[0m: 2.459, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.79448
[1mStep[0m  [8/84], [94mLoss[0m : 1.62690
[1mStep[0m  [16/84], [94mLoss[0m : 1.66397
[1mStep[0m  [24/84], [94mLoss[0m : 1.84588
[1mStep[0m  [32/84], [94mLoss[0m : 1.60230
[1mStep[0m  [40/84], [94mLoss[0m : 1.43297
[1mStep[0m  [48/84], [94mLoss[0m : 1.75603
[1mStep[0m  [56/84], [94mLoss[0m : 1.67697
[1mStep[0m  [64/84], [94mLoss[0m : 1.66834
[1mStep[0m  [72/84], [94mLoss[0m : 1.95721
[1mStep[0m  [80/84], [94mLoss[0m : 1.79945

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.730, [92mTest[0m: 2.502, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.66595
[1mStep[0m  [8/84], [94mLoss[0m : 1.53473
[1mStep[0m  [16/84], [94mLoss[0m : 1.69186
[1mStep[0m  [24/84], [94mLoss[0m : 1.63229
[1mStep[0m  [32/84], [94mLoss[0m : 1.85711
[1mStep[0m  [40/84], [94mLoss[0m : 1.59507
[1mStep[0m  [48/84], [94mLoss[0m : 1.73307
[1mStep[0m  [56/84], [94mLoss[0m : 1.65214
[1mStep[0m  [64/84], [94mLoss[0m : 1.70935
[1mStep[0m  [72/84], [94mLoss[0m : 1.66007
[1mStep[0m  [80/84], [94mLoss[0m : 1.85881

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.727, [92mTest[0m: 2.416, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.74437
[1mStep[0m  [8/84], [94mLoss[0m : 1.66692
[1mStep[0m  [16/84], [94mLoss[0m : 1.56241
[1mStep[0m  [24/84], [94mLoss[0m : 1.89205
[1mStep[0m  [32/84], [94mLoss[0m : 1.88014
[1mStep[0m  [40/84], [94mLoss[0m : 1.70298
[1mStep[0m  [48/84], [94mLoss[0m : 1.60846
[1mStep[0m  [56/84], [94mLoss[0m : 1.74428
[1mStep[0m  [64/84], [94mLoss[0m : 1.69211
[1mStep[0m  [72/84], [94mLoss[0m : 1.89447
[1mStep[0m  [80/84], [94mLoss[0m : 1.76156

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.708, [92mTest[0m: 2.448, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.66727
[1mStep[0m  [8/84], [94mLoss[0m : 1.70326
[1mStep[0m  [16/84], [94mLoss[0m : 1.77443
[1mStep[0m  [24/84], [94mLoss[0m : 1.60151
[1mStep[0m  [32/84], [94mLoss[0m : 1.85272
[1mStep[0m  [40/84], [94mLoss[0m : 1.75417
[1mStep[0m  [48/84], [94mLoss[0m : 1.70974
[1mStep[0m  [56/84], [94mLoss[0m : 1.89555
[1mStep[0m  [64/84], [94mLoss[0m : 1.72434
[1mStep[0m  [72/84], [94mLoss[0m : 1.81692
[1mStep[0m  [80/84], [94mLoss[0m : 1.51349

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.691, [92mTest[0m: 2.463, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.60365
[1mStep[0m  [8/84], [94mLoss[0m : 1.64300
[1mStep[0m  [16/84], [94mLoss[0m : 1.55431
[1mStep[0m  [24/84], [94mLoss[0m : 1.65953
[1mStep[0m  [32/84], [94mLoss[0m : 1.59160
[1mStep[0m  [40/84], [94mLoss[0m : 1.66777
[1mStep[0m  [48/84], [94mLoss[0m : 1.54493
[1mStep[0m  [56/84], [94mLoss[0m : 1.50722
[1mStep[0m  [64/84], [94mLoss[0m : 1.48475
[1mStep[0m  [72/84], [94mLoss[0m : 1.57696
[1mStep[0m  [80/84], [94mLoss[0m : 1.75419

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.643, [92mTest[0m: 2.521, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.57812
[1mStep[0m  [8/84], [94mLoss[0m : 1.43856
[1mStep[0m  [16/84], [94mLoss[0m : 1.41667
[1mStep[0m  [24/84], [94mLoss[0m : 1.60763
[1mStep[0m  [32/84], [94mLoss[0m : 1.53452
[1mStep[0m  [40/84], [94mLoss[0m : 1.70923
[1mStep[0m  [48/84], [94mLoss[0m : 1.77206
[1mStep[0m  [56/84], [94mLoss[0m : 1.62481
[1mStep[0m  [64/84], [94mLoss[0m : 1.68823
[1mStep[0m  [72/84], [94mLoss[0m : 1.56664
[1mStep[0m  [80/84], [94mLoss[0m : 1.84835

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.640, [92mTest[0m: 2.473, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.93537
[1mStep[0m  [8/84], [94mLoss[0m : 1.66053
[1mStep[0m  [16/84], [94mLoss[0m : 1.64283
[1mStep[0m  [24/84], [94mLoss[0m : 1.60986
[1mStep[0m  [32/84], [94mLoss[0m : 1.90310
[1mStep[0m  [40/84], [94mLoss[0m : 1.33644
[1mStep[0m  [48/84], [94mLoss[0m : 1.90711
[1mStep[0m  [56/84], [94mLoss[0m : 1.56635
[1mStep[0m  [64/84], [94mLoss[0m : 1.59362
[1mStep[0m  [72/84], [94mLoss[0m : 1.70344
[1mStep[0m  [80/84], [94mLoss[0m : 1.63152

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.622, [92mTest[0m: 2.559, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.82467
[1mStep[0m  [8/84], [94mLoss[0m : 1.56516
[1mStep[0m  [16/84], [94mLoss[0m : 1.50630
[1mStep[0m  [24/84], [94mLoss[0m : 1.80602
[1mStep[0m  [32/84], [94mLoss[0m : 1.72960
[1mStep[0m  [40/84], [94mLoss[0m : 1.59269
[1mStep[0m  [48/84], [94mLoss[0m : 1.46819
[1mStep[0m  [56/84], [94mLoss[0m : 1.31373
[1mStep[0m  [64/84], [94mLoss[0m : 1.55343
[1mStep[0m  [72/84], [94mLoss[0m : 1.74027
[1mStep[0m  [80/84], [94mLoss[0m : 1.67386

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.619, [92mTest[0m: 2.472, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.69563
[1mStep[0m  [8/84], [94mLoss[0m : 1.60050
[1mStep[0m  [16/84], [94mLoss[0m : 1.63124
[1mStep[0m  [24/84], [94mLoss[0m : 1.74917
[1mStep[0m  [32/84], [94mLoss[0m : 1.65018
[1mStep[0m  [40/84], [94mLoss[0m : 1.58430
[1mStep[0m  [48/84], [94mLoss[0m : 1.49935
[1mStep[0m  [56/84], [94mLoss[0m : 1.69656
[1mStep[0m  [64/84], [94mLoss[0m : 1.35036
[1mStep[0m  [72/84], [94mLoss[0m : 1.81883
[1mStep[0m  [80/84], [94mLoss[0m : 1.59603

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.599, [92mTest[0m: 2.471, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.42946
[1mStep[0m  [8/84], [94mLoss[0m : 1.44361
[1mStep[0m  [16/84], [94mLoss[0m : 1.54697
[1mStep[0m  [24/84], [94mLoss[0m : 1.69674
[1mStep[0m  [32/84], [94mLoss[0m : 1.47126
[1mStep[0m  [40/84], [94mLoss[0m : 1.48637
[1mStep[0m  [48/84], [94mLoss[0m : 1.59293
[1mStep[0m  [56/84], [94mLoss[0m : 1.59754
[1mStep[0m  [64/84], [94mLoss[0m : 1.25610
[1mStep[0m  [72/84], [94mLoss[0m : 1.75106
[1mStep[0m  [80/84], [94mLoss[0m : 1.63792

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.613, [92mTest[0m: 2.497, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.67425
[1mStep[0m  [8/84], [94mLoss[0m : 1.46661
[1mStep[0m  [16/84], [94mLoss[0m : 1.52763
[1mStep[0m  [24/84], [94mLoss[0m : 1.75798
[1mStep[0m  [32/84], [94mLoss[0m : 1.67915
[1mStep[0m  [40/84], [94mLoss[0m : 1.56431
[1mStep[0m  [48/84], [94mLoss[0m : 1.67403
[1mStep[0m  [56/84], [94mLoss[0m : 1.65422
[1mStep[0m  [64/84], [94mLoss[0m : 1.53583
[1mStep[0m  [72/84], [94mLoss[0m : 1.71736
[1mStep[0m  [80/84], [94mLoss[0m : 1.59876

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.601, [92mTest[0m: 2.492, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.67518
[1mStep[0m  [8/84], [94mLoss[0m : 1.63025
[1mStep[0m  [16/84], [94mLoss[0m : 1.48456
[1mStep[0m  [24/84], [94mLoss[0m : 1.53233
[1mStep[0m  [32/84], [94mLoss[0m : 1.47878
[1mStep[0m  [40/84], [94mLoss[0m : 1.71990
[1mStep[0m  [48/84], [94mLoss[0m : 1.50659
[1mStep[0m  [56/84], [94mLoss[0m : 1.68590
[1mStep[0m  [64/84], [94mLoss[0m : 1.60361
[1mStep[0m  [72/84], [94mLoss[0m : 1.71514
[1mStep[0m  [80/84], [94mLoss[0m : 1.44610

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.572, [92mTest[0m: 2.510, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.68387
[1mStep[0m  [8/84], [94mLoss[0m : 1.53498
[1mStep[0m  [16/84], [94mLoss[0m : 1.78774
[1mStep[0m  [24/84], [94mLoss[0m : 1.43161
[1mStep[0m  [32/84], [94mLoss[0m : 1.48735
[1mStep[0m  [40/84], [94mLoss[0m : 1.57462
[1mStep[0m  [48/84], [94mLoss[0m : 1.37962
[1mStep[0m  [56/84], [94mLoss[0m : 1.60438
[1mStep[0m  [64/84], [94mLoss[0m : 1.58981
[1mStep[0m  [72/84], [94mLoss[0m : 1.87537
[1mStep[0m  [80/84], [94mLoss[0m : 1.51252

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.581, [92mTest[0m: 2.517, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.509
====================================

Phase 2 - Evaluation MAE:  2.5087721858705794
MAE score P1       2.326449
MAE score P2       2.508772
loss               1.572434
learning_rate      0.002575
batch_size              128
hidden_sizes      [200, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.9
weight_decay           0.01
Name: 8, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 10.21458
[1mStep[0m  [8/84], [94mLoss[0m : 10.28796
[1mStep[0m  [16/84], [94mLoss[0m : 10.17651
[1mStep[0m  [24/84], [94mLoss[0m : 10.19776
[1mStep[0m  [32/84], [94mLoss[0m : 9.69985
[1mStep[0m  [40/84], [94mLoss[0m : 9.15438
[1mStep[0m  [48/84], [94mLoss[0m : 8.83741
[1mStep[0m  [56/84], [94mLoss[0m : 8.14277
[1mStep[0m  [64/84], [94mLoss[0m : 8.22328
[1mStep[0m  [72/84], [94mLoss[0m : 7.73204
[1mStep[0m  [80/84], [94mLoss[0m : 7.77794

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.198, [92mTest[0m: 11.031, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.85483
[1mStep[0m  [8/84], [94mLoss[0m : 6.73308
[1mStep[0m  [16/84], [94mLoss[0m : 6.07778
[1mStep[0m  [24/84], [94mLoss[0m : 6.41478
[1mStep[0m  [32/84], [94mLoss[0m : 5.88779
[1mStep[0m  [40/84], [94mLoss[0m : 4.90121
[1mStep[0m  [48/84], [94mLoss[0m : 5.01613
[1mStep[0m  [56/84], [94mLoss[0m : 5.01836
[1mStep[0m  [64/84], [94mLoss[0m : 4.26143
[1mStep[0m  [72/84], [94mLoss[0m : 3.92337
[1mStep[0m  [80/84], [94mLoss[0m : 3.38506

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 5.267, [92mTest[0m: 8.229, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.02637
[1mStep[0m  [8/84], [94mLoss[0m : 3.62355
[1mStep[0m  [16/84], [94mLoss[0m : 3.63779
[1mStep[0m  [24/84], [94mLoss[0m : 3.36996
[1mStep[0m  [32/84], [94mLoss[0m : 2.95830
[1mStep[0m  [40/84], [94mLoss[0m : 2.89705
[1mStep[0m  [48/84], [94mLoss[0m : 2.76018
[1mStep[0m  [56/84], [94mLoss[0m : 2.68977
[1mStep[0m  [64/84], [94mLoss[0m : 2.55963
[1mStep[0m  [72/84], [94mLoss[0m : 2.71645
[1mStep[0m  [80/84], [94mLoss[0m : 2.99077

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.051, [92mTest[0m: 4.559, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53886
[1mStep[0m  [8/84], [94mLoss[0m : 2.77072
[1mStep[0m  [16/84], [94mLoss[0m : 2.79752
[1mStep[0m  [24/84], [94mLoss[0m : 2.71474
[1mStep[0m  [32/84], [94mLoss[0m : 2.61389
[1mStep[0m  [40/84], [94mLoss[0m : 2.84093
[1mStep[0m  [48/84], [94mLoss[0m : 2.67706
[1mStep[0m  [56/84], [94mLoss[0m : 2.55179
[1mStep[0m  [64/84], [94mLoss[0m : 2.88273
[1mStep[0m  [72/84], [94mLoss[0m : 2.84650
[1mStep[0m  [80/84], [94mLoss[0m : 2.31143

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.712, [92mTest[0m: 2.821, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.79969
[1mStep[0m  [8/84], [94mLoss[0m : 2.34315
[1mStep[0m  [16/84], [94mLoss[0m : 2.74512
[1mStep[0m  [24/84], [94mLoss[0m : 2.77994
[1mStep[0m  [32/84], [94mLoss[0m : 2.96059
[1mStep[0m  [40/84], [94mLoss[0m : 2.45118
[1mStep[0m  [48/84], [94mLoss[0m : 2.67794
[1mStep[0m  [56/84], [94mLoss[0m : 2.52300
[1mStep[0m  [64/84], [94mLoss[0m : 3.14974
[1mStep[0m  [72/84], [94mLoss[0m : 2.68027
[1mStep[0m  [80/84], [94mLoss[0m : 3.12546

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.683, [92mTest[0m: 2.540, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.68515
[1mStep[0m  [8/84], [94mLoss[0m : 2.46632
[1mStep[0m  [16/84], [94mLoss[0m : 2.74689
[1mStep[0m  [24/84], [94mLoss[0m : 2.84632
[1mStep[0m  [32/84], [94mLoss[0m : 3.01869
[1mStep[0m  [40/84], [94mLoss[0m : 2.50119
[1mStep[0m  [48/84], [94mLoss[0m : 2.36940
[1mStep[0m  [56/84], [94mLoss[0m : 2.60019
[1mStep[0m  [64/84], [94mLoss[0m : 2.27078
[1mStep[0m  [72/84], [94mLoss[0m : 3.05168
[1mStep[0m  [80/84], [94mLoss[0m : 2.49574

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.649, [92mTest[0m: 2.449, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.70777
[1mStep[0m  [8/84], [94mLoss[0m : 2.54172
[1mStep[0m  [16/84], [94mLoss[0m : 2.87990
[1mStep[0m  [24/84], [94mLoss[0m : 2.31572
[1mStep[0m  [32/84], [94mLoss[0m : 3.09517
[1mStep[0m  [40/84], [94mLoss[0m : 2.18748
[1mStep[0m  [48/84], [94mLoss[0m : 2.62967
[1mStep[0m  [56/84], [94mLoss[0m : 2.55091
[1mStep[0m  [64/84], [94mLoss[0m : 2.57983
[1mStep[0m  [72/84], [94mLoss[0m : 2.70144
[1mStep[0m  [80/84], [94mLoss[0m : 2.06519

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.603, [92mTest[0m: 2.432, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31921
[1mStep[0m  [8/84], [94mLoss[0m : 2.44148
[1mStep[0m  [16/84], [94mLoss[0m : 2.98202
[1mStep[0m  [24/84], [94mLoss[0m : 2.68212
[1mStep[0m  [32/84], [94mLoss[0m : 2.58456
[1mStep[0m  [40/84], [94mLoss[0m : 2.61012
[1mStep[0m  [48/84], [94mLoss[0m : 2.65634
[1mStep[0m  [56/84], [94mLoss[0m : 2.56469
[1mStep[0m  [64/84], [94mLoss[0m : 2.82898
[1mStep[0m  [72/84], [94mLoss[0m : 3.03477
[1mStep[0m  [80/84], [94mLoss[0m : 2.42917

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.623, [92mTest[0m: 2.362, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.82919
[1mStep[0m  [8/84], [94mLoss[0m : 2.95395
[1mStep[0m  [16/84], [94mLoss[0m : 2.59021
[1mStep[0m  [24/84], [94mLoss[0m : 2.71759
[1mStep[0m  [32/84], [94mLoss[0m : 2.30084
[1mStep[0m  [40/84], [94mLoss[0m : 2.61995
[1mStep[0m  [48/84], [94mLoss[0m : 2.48332
[1mStep[0m  [56/84], [94mLoss[0m : 2.83651
[1mStep[0m  [64/84], [94mLoss[0m : 2.50896
[1mStep[0m  [72/84], [94mLoss[0m : 2.62384
[1mStep[0m  [80/84], [94mLoss[0m : 2.76718

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.593, [92mTest[0m: 2.366, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.70555
[1mStep[0m  [8/84], [94mLoss[0m : 2.51471
[1mStep[0m  [16/84], [94mLoss[0m : 2.41449
[1mStep[0m  [24/84], [94mLoss[0m : 2.53707
[1mStep[0m  [32/84], [94mLoss[0m : 2.56693
[1mStep[0m  [40/84], [94mLoss[0m : 2.61949
[1mStep[0m  [48/84], [94mLoss[0m : 2.20375
[1mStep[0m  [56/84], [94mLoss[0m : 2.28899
[1mStep[0m  [64/84], [94mLoss[0m : 2.40296
[1mStep[0m  [72/84], [94mLoss[0m : 2.35925
[1mStep[0m  [80/84], [94mLoss[0m : 2.51119

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.557, [92mTest[0m: 2.370, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44182
[1mStep[0m  [8/84], [94mLoss[0m : 2.72604
[1mStep[0m  [16/84], [94mLoss[0m : 2.41768
[1mStep[0m  [24/84], [94mLoss[0m : 2.83231
[1mStep[0m  [32/84], [94mLoss[0m : 2.48445
[1mStep[0m  [40/84], [94mLoss[0m : 2.58745
[1mStep[0m  [48/84], [94mLoss[0m : 2.69470
[1mStep[0m  [56/84], [94mLoss[0m : 2.37516
[1mStep[0m  [64/84], [94mLoss[0m : 2.18868
[1mStep[0m  [72/84], [94mLoss[0m : 2.56491
[1mStep[0m  [80/84], [94mLoss[0m : 2.67499

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.559, [92mTest[0m: 2.366, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40433
[1mStep[0m  [8/84], [94mLoss[0m : 2.54441
[1mStep[0m  [16/84], [94mLoss[0m : 2.73099
[1mStep[0m  [24/84], [94mLoss[0m : 2.46887
[1mStep[0m  [32/84], [94mLoss[0m : 2.72888
[1mStep[0m  [40/84], [94mLoss[0m : 2.66558
[1mStep[0m  [48/84], [94mLoss[0m : 2.71667
[1mStep[0m  [56/84], [94mLoss[0m : 2.83420
[1mStep[0m  [64/84], [94mLoss[0m : 2.40977
[1mStep[0m  [72/84], [94mLoss[0m : 2.77130
[1mStep[0m  [80/84], [94mLoss[0m : 2.62534

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.352, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46966
[1mStep[0m  [8/84], [94mLoss[0m : 2.61972
[1mStep[0m  [16/84], [94mLoss[0m : 2.52205
[1mStep[0m  [24/84], [94mLoss[0m : 2.84983
[1mStep[0m  [32/84], [94mLoss[0m : 2.44472
[1mStep[0m  [40/84], [94mLoss[0m : 2.64792
[1mStep[0m  [48/84], [94mLoss[0m : 2.48873
[1mStep[0m  [56/84], [94mLoss[0m : 2.35195
[1mStep[0m  [64/84], [94mLoss[0m : 2.56136
[1mStep[0m  [72/84], [94mLoss[0m : 2.04996
[1mStep[0m  [80/84], [94mLoss[0m : 2.54201

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.366, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.25555
[1mStep[0m  [8/84], [94mLoss[0m : 2.49267
[1mStep[0m  [16/84], [94mLoss[0m : 2.49607
[1mStep[0m  [24/84], [94mLoss[0m : 2.66456
[1mStep[0m  [32/84], [94mLoss[0m : 2.65537
[1mStep[0m  [40/84], [94mLoss[0m : 2.57764
[1mStep[0m  [48/84], [94mLoss[0m : 2.55506
[1mStep[0m  [56/84], [94mLoss[0m : 2.58727
[1mStep[0m  [64/84], [94mLoss[0m : 2.40326
[1mStep[0m  [72/84], [94mLoss[0m : 2.80774
[1mStep[0m  [80/84], [94mLoss[0m : 2.43801

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.559, [92mTest[0m: 2.352, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.05725
[1mStep[0m  [8/84], [94mLoss[0m : 2.50336
[1mStep[0m  [16/84], [94mLoss[0m : 2.58666
[1mStep[0m  [24/84], [94mLoss[0m : 2.51304
[1mStep[0m  [32/84], [94mLoss[0m : 2.72835
[1mStep[0m  [40/84], [94mLoss[0m : 2.16234
[1mStep[0m  [48/84], [94mLoss[0m : 2.61312
[1mStep[0m  [56/84], [94mLoss[0m : 2.56658
[1mStep[0m  [64/84], [94mLoss[0m : 2.38274
[1mStep[0m  [72/84], [94mLoss[0m : 2.43748
[1mStep[0m  [80/84], [94mLoss[0m : 2.61940

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.346, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.19643
[1mStep[0m  [8/84], [94mLoss[0m : 2.64560
[1mStep[0m  [16/84], [94mLoss[0m : 2.38019
[1mStep[0m  [24/84], [94mLoss[0m : 2.67269
[1mStep[0m  [32/84], [94mLoss[0m : 2.35929
[1mStep[0m  [40/84], [94mLoss[0m : 2.56370
[1mStep[0m  [48/84], [94mLoss[0m : 2.61506
[1mStep[0m  [56/84], [94mLoss[0m : 2.78933
[1mStep[0m  [64/84], [94mLoss[0m : 2.50850
[1mStep[0m  [72/84], [94mLoss[0m : 2.49984
[1mStep[0m  [80/84], [94mLoss[0m : 2.64542

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.345, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37095
[1mStep[0m  [8/84], [94mLoss[0m : 2.50741
[1mStep[0m  [16/84], [94mLoss[0m : 2.42823
[1mStep[0m  [24/84], [94mLoss[0m : 2.27111
[1mStep[0m  [32/84], [94mLoss[0m : 2.58756
[1mStep[0m  [40/84], [94mLoss[0m : 2.62409
[1mStep[0m  [48/84], [94mLoss[0m : 2.68658
[1mStep[0m  [56/84], [94mLoss[0m : 2.31654
[1mStep[0m  [64/84], [94mLoss[0m : 2.27860
[1mStep[0m  [72/84], [94mLoss[0m : 2.37616
[1mStep[0m  [80/84], [94mLoss[0m : 2.50852

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.347, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.51982
[1mStep[0m  [8/84], [94mLoss[0m : 2.68293
[1mStep[0m  [16/84], [94mLoss[0m : 2.72555
[1mStep[0m  [24/84], [94mLoss[0m : 2.56225
[1mStep[0m  [32/84], [94mLoss[0m : 2.27170
[1mStep[0m  [40/84], [94mLoss[0m : 2.11444
[1mStep[0m  [48/84], [94mLoss[0m : 2.40421
[1mStep[0m  [56/84], [94mLoss[0m : 2.67436
[1mStep[0m  [64/84], [94mLoss[0m : 2.58824
[1mStep[0m  [72/84], [94mLoss[0m : 2.58764
[1mStep[0m  [80/84], [94mLoss[0m : 2.29953

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.351, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.80717
[1mStep[0m  [8/84], [94mLoss[0m : 2.47213
[1mStep[0m  [16/84], [94mLoss[0m : 2.54235
[1mStep[0m  [24/84], [94mLoss[0m : 2.22179
[1mStep[0m  [32/84], [94mLoss[0m : 2.69474
[1mStep[0m  [40/84], [94mLoss[0m : 2.32375
[1mStep[0m  [48/84], [94mLoss[0m : 2.36047
[1mStep[0m  [56/84], [94mLoss[0m : 2.46352
[1mStep[0m  [64/84], [94mLoss[0m : 2.51042
[1mStep[0m  [72/84], [94mLoss[0m : 2.65191
[1mStep[0m  [80/84], [94mLoss[0m : 2.34055

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.337, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.38301
[1mStep[0m  [8/84], [94mLoss[0m : 2.20651
[1mStep[0m  [16/84], [94mLoss[0m : 2.36349
[1mStep[0m  [24/84], [94mLoss[0m : 2.49188
[1mStep[0m  [32/84], [94mLoss[0m : 2.60948
[1mStep[0m  [40/84], [94mLoss[0m : 2.50071
[1mStep[0m  [48/84], [94mLoss[0m : 2.30873
[1mStep[0m  [56/84], [94mLoss[0m : 2.27546
[1mStep[0m  [64/84], [94mLoss[0m : 2.15173
[1mStep[0m  [72/84], [94mLoss[0m : 2.73421
[1mStep[0m  [80/84], [94mLoss[0m : 2.51958

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.329, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.63474
[1mStep[0m  [8/84], [94mLoss[0m : 2.41220
[1mStep[0m  [16/84], [94mLoss[0m : 2.43495
[1mStep[0m  [24/84], [94mLoss[0m : 2.32591
[1mStep[0m  [32/84], [94mLoss[0m : 2.50729
[1mStep[0m  [40/84], [94mLoss[0m : 2.08292
[1mStep[0m  [48/84], [94mLoss[0m : 2.42680
[1mStep[0m  [56/84], [94mLoss[0m : 2.26702
[1mStep[0m  [64/84], [94mLoss[0m : 2.42157
[1mStep[0m  [72/84], [94mLoss[0m : 2.42222
[1mStep[0m  [80/84], [94mLoss[0m : 2.44359

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.322, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37015
[1mStep[0m  [8/84], [94mLoss[0m : 2.12907
[1mStep[0m  [16/84], [94mLoss[0m : 2.30571
[1mStep[0m  [24/84], [94mLoss[0m : 2.30664
[1mStep[0m  [32/84], [94mLoss[0m : 2.37592
[1mStep[0m  [40/84], [94mLoss[0m : 2.65415
[1mStep[0m  [48/84], [94mLoss[0m : 3.30364
[1mStep[0m  [56/84], [94mLoss[0m : 2.31578
[1mStep[0m  [64/84], [94mLoss[0m : 2.45369
[1mStep[0m  [72/84], [94mLoss[0m : 2.47716
[1mStep[0m  [80/84], [94mLoss[0m : 2.03138

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.321, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.89168
[1mStep[0m  [8/84], [94mLoss[0m : 2.51742
[1mStep[0m  [16/84], [94mLoss[0m : 2.32237
[1mStep[0m  [24/84], [94mLoss[0m : 2.54524
[1mStep[0m  [32/84], [94mLoss[0m : 2.44923
[1mStep[0m  [40/84], [94mLoss[0m : 2.43008
[1mStep[0m  [48/84], [94mLoss[0m : 2.24703
[1mStep[0m  [56/84], [94mLoss[0m : 2.35719
[1mStep[0m  [64/84], [94mLoss[0m : 2.55255
[1mStep[0m  [72/84], [94mLoss[0m : 2.44406
[1mStep[0m  [80/84], [94mLoss[0m : 2.75388

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.324, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36468
[1mStep[0m  [8/84], [94mLoss[0m : 2.53939
[1mStep[0m  [16/84], [94mLoss[0m : 2.37994
[1mStep[0m  [24/84], [94mLoss[0m : 2.25861
[1mStep[0m  [32/84], [94mLoss[0m : 2.33042
[1mStep[0m  [40/84], [94mLoss[0m : 2.53502
[1mStep[0m  [48/84], [94mLoss[0m : 2.73848
[1mStep[0m  [56/84], [94mLoss[0m : 2.63702
[1mStep[0m  [64/84], [94mLoss[0m : 2.53506
[1mStep[0m  [72/84], [94mLoss[0m : 2.72647
[1mStep[0m  [80/84], [94mLoss[0m : 2.67909

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.313, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46490
[1mStep[0m  [8/84], [94mLoss[0m : 2.17814
[1mStep[0m  [16/84], [94mLoss[0m : 2.26004
[1mStep[0m  [24/84], [94mLoss[0m : 2.59189
[1mStep[0m  [32/84], [94mLoss[0m : 2.44202
[1mStep[0m  [40/84], [94mLoss[0m : 2.67458
[1mStep[0m  [48/84], [94mLoss[0m : 2.49789
[1mStep[0m  [56/84], [94mLoss[0m : 2.11491
[1mStep[0m  [64/84], [94mLoss[0m : 2.59952
[1mStep[0m  [72/84], [94mLoss[0m : 2.51575
[1mStep[0m  [80/84], [94mLoss[0m : 2.50291

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.312, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.38697
[1mStep[0m  [8/84], [94mLoss[0m : 2.45645
[1mStep[0m  [16/84], [94mLoss[0m : 2.35467
[1mStep[0m  [24/84], [94mLoss[0m : 2.53492
[1mStep[0m  [32/84], [94mLoss[0m : 2.25991
[1mStep[0m  [40/84], [94mLoss[0m : 2.60900
[1mStep[0m  [48/84], [94mLoss[0m : 2.62820
[1mStep[0m  [56/84], [94mLoss[0m : 2.50266
[1mStep[0m  [64/84], [94mLoss[0m : 2.45471
[1mStep[0m  [72/84], [94mLoss[0m : 2.35290
[1mStep[0m  [80/84], [94mLoss[0m : 2.32933

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.313, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33811
[1mStep[0m  [8/84], [94mLoss[0m : 2.59925
[1mStep[0m  [16/84], [94mLoss[0m : 2.44937
[1mStep[0m  [24/84], [94mLoss[0m : 2.54077
[1mStep[0m  [32/84], [94mLoss[0m : 2.61060
[1mStep[0m  [40/84], [94mLoss[0m : 2.63957
[1mStep[0m  [48/84], [94mLoss[0m : 2.42339
[1mStep[0m  [56/84], [94mLoss[0m : 2.65936
[1mStep[0m  [64/84], [94mLoss[0m : 2.31282
[1mStep[0m  [72/84], [94mLoss[0m : 2.19930
[1mStep[0m  [80/84], [94mLoss[0m : 2.19151

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.316, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26279
[1mStep[0m  [8/84], [94mLoss[0m : 2.56839
[1mStep[0m  [16/84], [94mLoss[0m : 2.50053
[1mStep[0m  [24/84], [94mLoss[0m : 2.36615
[1mStep[0m  [32/84], [94mLoss[0m : 2.70802
[1mStep[0m  [40/84], [94mLoss[0m : 2.40110
[1mStep[0m  [48/84], [94mLoss[0m : 2.64392
[1mStep[0m  [56/84], [94mLoss[0m : 2.56891
[1mStep[0m  [64/84], [94mLoss[0m : 2.33555
[1mStep[0m  [72/84], [94mLoss[0m : 2.39028
[1mStep[0m  [80/84], [94mLoss[0m : 2.38324

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.327, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37038
[1mStep[0m  [8/84], [94mLoss[0m : 2.35080
[1mStep[0m  [16/84], [94mLoss[0m : 2.80552
[1mStep[0m  [24/84], [94mLoss[0m : 2.38098
[1mStep[0m  [32/84], [94mLoss[0m : 2.56867
[1mStep[0m  [40/84], [94mLoss[0m : 2.28485
[1mStep[0m  [48/84], [94mLoss[0m : 2.67010
[1mStep[0m  [56/84], [94mLoss[0m : 2.69163
[1mStep[0m  [64/84], [94mLoss[0m : 2.85916
[1mStep[0m  [72/84], [94mLoss[0m : 2.40304
[1mStep[0m  [80/84], [94mLoss[0m : 2.50841

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.322, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41658
[1mStep[0m  [8/84], [94mLoss[0m : 2.20523
[1mStep[0m  [16/84], [94mLoss[0m : 2.33396
[1mStep[0m  [24/84], [94mLoss[0m : 2.41496
[1mStep[0m  [32/84], [94mLoss[0m : 2.35726
[1mStep[0m  [40/84], [94mLoss[0m : 2.45329
[1mStep[0m  [48/84], [94mLoss[0m : 2.37574
[1mStep[0m  [56/84], [94mLoss[0m : 2.23979
[1mStep[0m  [64/84], [94mLoss[0m : 2.42860
[1mStep[0m  [72/84], [94mLoss[0m : 2.58219
[1mStep[0m  [80/84], [94mLoss[0m : 2.57656

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.318, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.311
====================================

Phase 1 - Evaluation MAE:  2.3113764439310347
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 2.30432
[1mStep[0m  [8/84], [94mLoss[0m : 2.84636
[1mStep[0m  [16/84], [94mLoss[0m : 2.39921
[1mStep[0m  [24/84], [94mLoss[0m : 2.96017
[1mStep[0m  [32/84], [94mLoss[0m : 2.36473
[1mStep[0m  [40/84], [94mLoss[0m : 2.57842
[1mStep[0m  [48/84], [94mLoss[0m : 2.65103
[1mStep[0m  [56/84], [94mLoss[0m : 2.59234
[1mStep[0m  [64/84], [94mLoss[0m : 2.49566
[1mStep[0m  [72/84], [94mLoss[0m : 2.16411
[1mStep[0m  [80/84], [94mLoss[0m : 2.26037

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.531, [92mTest[0m: 2.314, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.69888
[1mStep[0m  [8/84], [94mLoss[0m : 2.60127
[1mStep[0m  [16/84], [94mLoss[0m : 2.48861
[1mStep[0m  [24/84], [94mLoss[0m : 2.61871
[1mStep[0m  [32/84], [94mLoss[0m : 2.31556
[1mStep[0m  [40/84], [94mLoss[0m : 2.42815
[1mStep[0m  [48/84], [94mLoss[0m : 2.43047
[1mStep[0m  [56/84], [94mLoss[0m : 2.65399
[1mStep[0m  [64/84], [94mLoss[0m : 2.60552
[1mStep[0m  [72/84], [94mLoss[0m : 2.41571
[1mStep[0m  [80/84], [94mLoss[0m : 2.41227

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.688, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52043
[1mStep[0m  [8/84], [94mLoss[0m : 2.30624
[1mStep[0m  [16/84], [94mLoss[0m : 2.49907
[1mStep[0m  [24/84], [94mLoss[0m : 2.26566
[1mStep[0m  [32/84], [94mLoss[0m : 2.33140
[1mStep[0m  [40/84], [94mLoss[0m : 2.51898
[1mStep[0m  [48/84], [94mLoss[0m : 2.31013
[1mStep[0m  [56/84], [94mLoss[0m : 2.39696
[1mStep[0m  [64/84], [94mLoss[0m : 2.61621
[1mStep[0m  [72/84], [94mLoss[0m : 2.28048
[1mStep[0m  [80/84], [94mLoss[0m : 2.34485

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.559, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26774
[1mStep[0m  [8/84], [94mLoss[0m : 2.28940
[1mStep[0m  [16/84], [94mLoss[0m : 2.39075
[1mStep[0m  [24/84], [94mLoss[0m : 2.27779
[1mStep[0m  [32/84], [94mLoss[0m : 2.44858
[1mStep[0m  [40/84], [94mLoss[0m : 2.21146
[1mStep[0m  [48/84], [94mLoss[0m : 2.30874
[1mStep[0m  [56/84], [94mLoss[0m : 2.33728
[1mStep[0m  [64/84], [94mLoss[0m : 2.39930
[1mStep[0m  [72/84], [94mLoss[0m : 2.44876
[1mStep[0m  [80/84], [94mLoss[0m : 2.39189

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.550, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.61365
[1mStep[0m  [8/84], [94mLoss[0m : 2.40633
[1mStep[0m  [16/84], [94mLoss[0m : 2.33805
[1mStep[0m  [24/84], [94mLoss[0m : 2.53174
[1mStep[0m  [32/84], [94mLoss[0m : 2.30577
[1mStep[0m  [40/84], [94mLoss[0m : 2.23502
[1mStep[0m  [48/84], [94mLoss[0m : 2.46195
[1mStep[0m  [56/84], [94mLoss[0m : 2.33406
[1mStep[0m  [64/84], [94mLoss[0m : 2.38312
[1mStep[0m  [72/84], [94mLoss[0m : 2.28776
[1mStep[0m  [80/84], [94mLoss[0m : 2.47972

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.634, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.24496
[1mStep[0m  [8/84], [94mLoss[0m : 2.22758
[1mStep[0m  [16/84], [94mLoss[0m : 2.76577
[1mStep[0m  [24/84], [94mLoss[0m : 2.46479
[1mStep[0m  [32/84], [94mLoss[0m : 2.23129
[1mStep[0m  [40/84], [94mLoss[0m : 2.56375
[1mStep[0m  [48/84], [94mLoss[0m : 2.18916
[1mStep[0m  [56/84], [94mLoss[0m : 2.45642
[1mStep[0m  [64/84], [94mLoss[0m : 2.53164
[1mStep[0m  [72/84], [94mLoss[0m : 2.58067
[1mStep[0m  [80/84], [94mLoss[0m : 2.36191

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.393, [92mTest[0m: 2.561, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.11235
[1mStep[0m  [8/84], [94mLoss[0m : 2.49419
[1mStep[0m  [16/84], [94mLoss[0m : 2.24410
[1mStep[0m  [24/84], [94mLoss[0m : 2.44667
[1mStep[0m  [32/84], [94mLoss[0m : 2.72935
[1mStep[0m  [40/84], [94mLoss[0m : 2.42323
[1mStep[0m  [48/84], [94mLoss[0m : 2.62500
[1mStep[0m  [56/84], [94mLoss[0m : 2.56897
[1mStep[0m  [64/84], [94mLoss[0m : 2.32400
[1mStep[0m  [72/84], [94mLoss[0m : 2.66377
[1mStep[0m  [80/84], [94mLoss[0m : 2.25754

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.365, [92mTest[0m: 2.543, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45887
[1mStep[0m  [8/84], [94mLoss[0m : 2.49027
[1mStep[0m  [16/84], [94mLoss[0m : 2.33297
[1mStep[0m  [24/84], [94mLoss[0m : 2.38877
[1mStep[0m  [32/84], [94mLoss[0m : 2.28357
[1mStep[0m  [40/84], [94mLoss[0m : 2.54673
[1mStep[0m  [48/84], [94mLoss[0m : 2.36088
[1mStep[0m  [56/84], [94mLoss[0m : 2.32868
[1mStep[0m  [64/84], [94mLoss[0m : 2.31160
[1mStep[0m  [72/84], [94mLoss[0m : 2.23095
[1mStep[0m  [80/84], [94mLoss[0m : 2.75088

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.317, [92mTest[0m: 2.523, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36130
[1mStep[0m  [8/84], [94mLoss[0m : 2.36995
[1mStep[0m  [16/84], [94mLoss[0m : 2.10807
[1mStep[0m  [24/84], [94mLoss[0m : 2.36162
[1mStep[0m  [32/84], [94mLoss[0m : 2.00229
[1mStep[0m  [40/84], [94mLoss[0m : 2.36255
[1mStep[0m  [48/84], [94mLoss[0m : 2.44716
[1mStep[0m  [56/84], [94mLoss[0m : 2.49872
[1mStep[0m  [64/84], [94mLoss[0m : 2.17382
[1mStep[0m  [72/84], [94mLoss[0m : 2.23690
[1mStep[0m  [80/84], [94mLoss[0m : 2.28129

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.297, [92mTest[0m: 2.548, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35051
[1mStep[0m  [8/84], [94mLoss[0m : 2.32251
[1mStep[0m  [16/84], [94mLoss[0m : 2.11981
[1mStep[0m  [24/84], [94mLoss[0m : 2.30652
[1mStep[0m  [32/84], [94mLoss[0m : 2.49098
[1mStep[0m  [40/84], [94mLoss[0m : 2.47252
[1mStep[0m  [48/84], [94mLoss[0m : 2.19349
[1mStep[0m  [56/84], [94mLoss[0m : 1.86645
[1mStep[0m  [64/84], [94mLoss[0m : 2.13753
[1mStep[0m  [72/84], [94mLoss[0m : 2.36706
[1mStep[0m  [80/84], [94mLoss[0m : 2.41271

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.281, [92mTest[0m: 2.495, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31940
[1mStep[0m  [8/84], [94mLoss[0m : 2.34693
[1mStep[0m  [16/84], [94mLoss[0m : 2.10023
[1mStep[0m  [24/84], [94mLoss[0m : 2.29043
[1mStep[0m  [32/84], [94mLoss[0m : 2.31972
[1mStep[0m  [40/84], [94mLoss[0m : 2.28344
[1mStep[0m  [48/84], [94mLoss[0m : 2.50306
[1mStep[0m  [56/84], [94mLoss[0m : 2.31215
[1mStep[0m  [64/84], [94mLoss[0m : 2.08178
[1mStep[0m  [72/84], [94mLoss[0m : 2.30645
[1mStep[0m  [80/84], [94mLoss[0m : 2.25032

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.233, [92mTest[0m: 2.445, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.21657
[1mStep[0m  [8/84], [94mLoss[0m : 2.31734
[1mStep[0m  [16/84], [94mLoss[0m : 2.16414
[1mStep[0m  [24/84], [94mLoss[0m : 2.28208
[1mStep[0m  [32/84], [94mLoss[0m : 2.34604
[1mStep[0m  [40/84], [94mLoss[0m : 2.34687
[1mStep[0m  [48/84], [94mLoss[0m : 2.53545
[1mStep[0m  [56/84], [94mLoss[0m : 2.29884
[1mStep[0m  [64/84], [94mLoss[0m : 2.21968
[1mStep[0m  [72/84], [94mLoss[0m : 2.23444
[1mStep[0m  [80/84], [94mLoss[0m : 2.11770

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.196, [92mTest[0m: 2.471, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.20824
[1mStep[0m  [8/84], [94mLoss[0m : 2.05633
[1mStep[0m  [16/84], [94mLoss[0m : 2.16758
[1mStep[0m  [24/84], [94mLoss[0m : 2.02338
[1mStep[0m  [32/84], [94mLoss[0m : 2.14342
[1mStep[0m  [40/84], [94mLoss[0m : 1.96720
[1mStep[0m  [48/84], [94mLoss[0m : 2.33248
[1mStep[0m  [56/84], [94mLoss[0m : 2.24408
[1mStep[0m  [64/84], [94mLoss[0m : 2.45015
[1mStep[0m  [72/84], [94mLoss[0m : 2.33340
[1mStep[0m  [80/84], [94mLoss[0m : 1.95299

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.176, [92mTest[0m: 2.583, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.05645
[1mStep[0m  [8/84], [94mLoss[0m : 2.47935
[1mStep[0m  [16/84], [94mLoss[0m : 2.11281
[1mStep[0m  [24/84], [94mLoss[0m : 1.94528
[1mStep[0m  [32/84], [94mLoss[0m : 2.19473
[1mStep[0m  [40/84], [94mLoss[0m : 2.09568
[1mStep[0m  [48/84], [94mLoss[0m : 2.12646
[1mStep[0m  [56/84], [94mLoss[0m : 2.11270
[1mStep[0m  [64/84], [94mLoss[0m : 1.96343
[1mStep[0m  [72/84], [94mLoss[0m : 2.09006
[1mStep[0m  [80/84], [94mLoss[0m : 2.12286

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.136, [92mTest[0m: 2.444, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.07723
[1mStep[0m  [8/84], [94mLoss[0m : 2.01645
[1mStep[0m  [16/84], [94mLoss[0m : 2.15402
[1mStep[0m  [24/84], [94mLoss[0m : 1.98551
[1mStep[0m  [32/84], [94mLoss[0m : 2.40593
[1mStep[0m  [40/84], [94mLoss[0m : 2.49440
[1mStep[0m  [48/84], [94mLoss[0m : 1.68704
[1mStep[0m  [56/84], [94mLoss[0m : 2.13697
[1mStep[0m  [64/84], [94mLoss[0m : 1.96177
[1mStep[0m  [72/84], [94mLoss[0m : 2.04180
[1mStep[0m  [80/84], [94mLoss[0m : 2.18502

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.117, [92mTest[0m: 2.538, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.93378
[1mStep[0m  [8/84], [94mLoss[0m : 2.15756
[1mStep[0m  [16/84], [94mLoss[0m : 2.22776
[1mStep[0m  [24/84], [94mLoss[0m : 1.99428
[1mStep[0m  [32/84], [94mLoss[0m : 2.10392
[1mStep[0m  [40/84], [94mLoss[0m : 1.98194
[1mStep[0m  [48/84], [94mLoss[0m : 2.04434
[1mStep[0m  [56/84], [94mLoss[0m : 2.03586
[1mStep[0m  [64/84], [94mLoss[0m : 2.03916
[1mStep[0m  [72/84], [94mLoss[0m : 2.17645
[1mStep[0m  [80/84], [94mLoss[0m : 1.92911

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.093, [92mTest[0m: 2.426, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.99085
[1mStep[0m  [8/84], [94mLoss[0m : 2.02869
[1mStep[0m  [16/84], [94mLoss[0m : 2.14370
[1mStep[0m  [24/84], [94mLoss[0m : 2.03546
[1mStep[0m  [32/84], [94mLoss[0m : 2.22604
[1mStep[0m  [40/84], [94mLoss[0m : 2.19127
[1mStep[0m  [48/84], [94mLoss[0m : 1.94543
[1mStep[0m  [56/84], [94mLoss[0m : 2.06439
[1mStep[0m  [64/84], [94mLoss[0m : 2.13158
[1mStep[0m  [72/84], [94mLoss[0m : 2.06237
[1mStep[0m  [80/84], [94mLoss[0m : 2.19989

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.080, [92mTest[0m: 2.445, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.23517
[1mStep[0m  [8/84], [94mLoss[0m : 2.19583
[1mStep[0m  [16/84], [94mLoss[0m : 1.77276
[1mStep[0m  [24/84], [94mLoss[0m : 1.76832
[1mStep[0m  [32/84], [94mLoss[0m : 2.05232
[1mStep[0m  [40/84], [94mLoss[0m : 2.01314
[1mStep[0m  [48/84], [94mLoss[0m : 2.07157
[1mStep[0m  [56/84], [94mLoss[0m : 1.88499
[1mStep[0m  [64/84], [94mLoss[0m : 2.24942
[1mStep[0m  [72/84], [94mLoss[0m : 2.28433
[1mStep[0m  [80/84], [94mLoss[0m : 1.75721

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.004, [92mTest[0m: 2.428, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.81998
[1mStep[0m  [8/84], [94mLoss[0m : 2.03835
[1mStep[0m  [16/84], [94mLoss[0m : 1.94874
[1mStep[0m  [24/84], [94mLoss[0m : 2.01494
[1mStep[0m  [32/84], [94mLoss[0m : 1.90678
[1mStep[0m  [40/84], [94mLoss[0m : 1.91834
[1mStep[0m  [48/84], [94mLoss[0m : 1.77631
[1mStep[0m  [56/84], [94mLoss[0m : 1.98998
[1mStep[0m  [64/84], [94mLoss[0m : 1.90608
[1mStep[0m  [72/84], [94mLoss[0m : 2.02615
[1mStep[0m  [80/84], [94mLoss[0m : 1.78920

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.000, [92mTest[0m: 2.461, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.89487
[1mStep[0m  [8/84], [94mLoss[0m : 1.75117
[1mStep[0m  [16/84], [94mLoss[0m : 1.74829
[1mStep[0m  [24/84], [94mLoss[0m : 2.20328
[1mStep[0m  [32/84], [94mLoss[0m : 1.91733
[1mStep[0m  [40/84], [94mLoss[0m : 2.05821
[1mStep[0m  [48/84], [94mLoss[0m : 2.02446
[1mStep[0m  [56/84], [94mLoss[0m : 1.92989
[1mStep[0m  [64/84], [94mLoss[0m : 1.97160
[1mStep[0m  [72/84], [94mLoss[0m : 2.09926
[1mStep[0m  [80/84], [94mLoss[0m : 2.03219

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.968, [92mTest[0m: 2.481, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.88816
[1mStep[0m  [8/84], [94mLoss[0m : 1.83499
[1mStep[0m  [16/84], [94mLoss[0m : 1.81155
[1mStep[0m  [24/84], [94mLoss[0m : 1.78876
[1mStep[0m  [32/84], [94mLoss[0m : 1.88102
[1mStep[0m  [40/84], [94mLoss[0m : 2.19215
[1mStep[0m  [48/84], [94mLoss[0m : 1.83311
[1mStep[0m  [56/84], [94mLoss[0m : 2.01376
[1mStep[0m  [64/84], [94mLoss[0m : 1.82286
[1mStep[0m  [72/84], [94mLoss[0m : 1.83057
[1mStep[0m  [80/84], [94mLoss[0m : 1.74009

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.935, [92mTest[0m: 2.436, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.91564
[1mStep[0m  [8/84], [94mLoss[0m : 1.98236
[1mStep[0m  [16/84], [94mLoss[0m : 1.70941
[1mStep[0m  [24/84], [94mLoss[0m : 2.15483
[1mStep[0m  [32/84], [94mLoss[0m : 2.02818
[1mStep[0m  [40/84], [94mLoss[0m : 1.76555
[1mStep[0m  [48/84], [94mLoss[0m : 1.62711
[1mStep[0m  [56/84], [94mLoss[0m : 2.04902
[1mStep[0m  [64/84], [94mLoss[0m : 1.72226
[1mStep[0m  [72/84], [94mLoss[0m : 1.89997
[1mStep[0m  [80/84], [94mLoss[0m : 2.12891

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.900, [92mTest[0m: 2.435, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.77826
[1mStep[0m  [8/84], [94mLoss[0m : 1.94843
[1mStep[0m  [16/84], [94mLoss[0m : 1.87348
[1mStep[0m  [24/84], [94mLoss[0m : 1.78506
[1mStep[0m  [32/84], [94mLoss[0m : 1.73569
[1mStep[0m  [40/84], [94mLoss[0m : 1.95600
[1mStep[0m  [48/84], [94mLoss[0m : 1.52580
[1mStep[0m  [56/84], [94mLoss[0m : 1.84860
[1mStep[0m  [64/84], [94mLoss[0m : 2.07302
[1mStep[0m  [72/84], [94mLoss[0m : 1.93492
[1mStep[0m  [80/84], [94mLoss[0m : 1.99310

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.863, [92mTest[0m: 2.483, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.04049
[1mStep[0m  [8/84], [94mLoss[0m : 1.77297
[1mStep[0m  [16/84], [94mLoss[0m : 1.76627
[1mStep[0m  [24/84], [94mLoss[0m : 1.87975
[1mStep[0m  [32/84], [94mLoss[0m : 2.04007
[1mStep[0m  [40/84], [94mLoss[0m : 1.71064
[1mStep[0m  [48/84], [94mLoss[0m : 1.64103
[1mStep[0m  [56/84], [94mLoss[0m : 1.60182
[1mStep[0m  [64/84], [94mLoss[0m : 1.78039
[1mStep[0m  [72/84], [94mLoss[0m : 2.02090
[1mStep[0m  [80/84], [94mLoss[0m : 1.83321

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.824, [92mTest[0m: 2.462, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.61984
[1mStep[0m  [8/84], [94mLoss[0m : 1.75852
[1mStep[0m  [16/84], [94mLoss[0m : 1.75926
[1mStep[0m  [24/84], [94mLoss[0m : 1.89714
[1mStep[0m  [32/84], [94mLoss[0m : 1.91878
[1mStep[0m  [40/84], [94mLoss[0m : 1.82919
[1mStep[0m  [48/84], [94mLoss[0m : 1.81272
[1mStep[0m  [56/84], [94mLoss[0m : 1.88989
[1mStep[0m  [64/84], [94mLoss[0m : 1.86777
[1mStep[0m  [72/84], [94mLoss[0m : 1.72854
[1mStep[0m  [80/84], [94mLoss[0m : 1.71657

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.821, [92mTest[0m: 2.457, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.82456
[1mStep[0m  [8/84], [94mLoss[0m : 1.72046
[1mStep[0m  [16/84], [94mLoss[0m : 1.88719
[1mStep[0m  [24/84], [94mLoss[0m : 1.75561
[1mStep[0m  [32/84], [94mLoss[0m : 1.91002
[1mStep[0m  [40/84], [94mLoss[0m : 1.83028
[1mStep[0m  [48/84], [94mLoss[0m : 1.87115
[1mStep[0m  [56/84], [94mLoss[0m : 1.76592
[1mStep[0m  [64/84], [94mLoss[0m : 1.67060
[1mStep[0m  [72/84], [94mLoss[0m : 1.86561
[1mStep[0m  [80/84], [94mLoss[0m : 1.75293

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.824, [92mTest[0m: 2.426, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.96222
[1mStep[0m  [8/84], [94mLoss[0m : 1.81227
[1mStep[0m  [16/84], [94mLoss[0m : 1.89094
[1mStep[0m  [24/84], [94mLoss[0m : 1.67910
[1mStep[0m  [32/84], [94mLoss[0m : 1.85100
[1mStep[0m  [40/84], [94mLoss[0m : 1.68294
[1mStep[0m  [48/84], [94mLoss[0m : 1.71371
[1mStep[0m  [56/84], [94mLoss[0m : 1.69783
[1mStep[0m  [64/84], [94mLoss[0m : 1.71789
[1mStep[0m  [72/84], [94mLoss[0m : 1.78423
[1mStep[0m  [80/84], [94mLoss[0m : 1.86686

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.805, [92mTest[0m: 2.446, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.63240
[1mStep[0m  [8/84], [94mLoss[0m : 1.64422
[1mStep[0m  [16/84], [94mLoss[0m : 1.88536
[1mStep[0m  [24/84], [94mLoss[0m : 1.84765
[1mStep[0m  [32/84], [94mLoss[0m : 1.80098
[1mStep[0m  [40/84], [94mLoss[0m : 1.53808
[1mStep[0m  [48/84], [94mLoss[0m : 1.78723
[1mStep[0m  [56/84], [94mLoss[0m : 1.82451
[1mStep[0m  [64/84], [94mLoss[0m : 1.77313
[1mStep[0m  [72/84], [94mLoss[0m : 2.05370
[1mStep[0m  [80/84], [94mLoss[0m : 1.70185

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.773, [92mTest[0m: 2.498, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.72245
[1mStep[0m  [8/84], [94mLoss[0m : 1.72243
[1mStep[0m  [16/84], [94mLoss[0m : 1.73929
[1mStep[0m  [24/84], [94mLoss[0m : 1.77747
[1mStep[0m  [32/84], [94mLoss[0m : 1.73445
[1mStep[0m  [40/84], [94mLoss[0m : 1.80045
[1mStep[0m  [48/84], [94mLoss[0m : 1.67406
[1mStep[0m  [56/84], [94mLoss[0m : 1.64625
[1mStep[0m  [64/84], [94mLoss[0m : 1.77988
[1mStep[0m  [72/84], [94mLoss[0m : 1.72050
[1mStep[0m  [80/84], [94mLoss[0m : 1.58032

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.737, [92mTest[0m: 2.453, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.51548
[1mStep[0m  [8/84], [94mLoss[0m : 1.80610
[1mStep[0m  [16/84], [94mLoss[0m : 1.75119
[1mStep[0m  [24/84], [94mLoss[0m : 1.67624
[1mStep[0m  [32/84], [94mLoss[0m : 1.79697
[1mStep[0m  [40/84], [94mLoss[0m : 1.85425
[1mStep[0m  [48/84], [94mLoss[0m : 1.55046
[1mStep[0m  [56/84], [94mLoss[0m : 2.01722
[1mStep[0m  [64/84], [94mLoss[0m : 1.90599
[1mStep[0m  [72/84], [94mLoss[0m : 1.60454
[1mStep[0m  [80/84], [94mLoss[0m : 1.68087

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.744, [92mTest[0m: 2.559, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.516
====================================

Phase 2 - Evaluation MAE:  2.5155982971191406
MAE score P1       2.311376
MAE score P2       2.515598
loss                1.73709
learning_rate      0.002575
batch_size              128
hidden_sizes      [200, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.5
weight_decay          0.001
Name: 9, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/21], [94mLoss[0m : 11.11208
[1mStep[0m  [2/21], [94mLoss[0m : 10.60493
[1mStep[0m  [4/21], [94mLoss[0m : 10.19710
[1mStep[0m  [6/21], [94mLoss[0m : 10.05342
[1mStep[0m  [8/21], [94mLoss[0m : 9.58314
[1mStep[0m  [10/21], [94mLoss[0m : 9.34285
[1mStep[0m  [12/21], [94mLoss[0m : 8.68619
[1mStep[0m  [14/21], [94mLoss[0m : 8.24335
[1mStep[0m  [16/21], [94mLoss[0m : 7.80200
[1mStep[0m  [18/21], [94mLoss[0m : 7.80239
[1mStep[0m  [20/21], [94mLoss[0m : 7.40919

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.165, [92mTest[0m: 10.876, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.85656
[1mStep[0m  [2/21], [94mLoss[0m : 6.56695
[1mStep[0m  [4/21], [94mLoss[0m : 6.07386
[1mStep[0m  [6/21], [94mLoss[0m : 6.13552
[1mStep[0m  [8/21], [94mLoss[0m : 5.67074
[1mStep[0m  [10/21], [94mLoss[0m : 5.60644
[1mStep[0m  [12/21], [94mLoss[0m : 5.37018
[1mStep[0m  [14/21], [94mLoss[0m : 5.29984
[1mStep[0m  [16/21], [94mLoss[0m : 4.82590
[1mStep[0m  [18/21], [94mLoss[0m : 4.65943
[1mStep[0m  [20/21], [94mLoss[0m : 4.18406

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 5.568, [92mTest[0m: 8.666, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.33786
[1mStep[0m  [2/21], [94mLoss[0m : 4.05440
[1mStep[0m  [4/21], [94mLoss[0m : 3.75405
[1mStep[0m  [6/21], [94mLoss[0m : 3.63990
[1mStep[0m  [8/21], [94mLoss[0m : 3.78853
[1mStep[0m  [10/21], [94mLoss[0m : 3.47959
[1mStep[0m  [12/21], [94mLoss[0m : 3.30842
[1mStep[0m  [14/21], [94mLoss[0m : 3.24663
[1mStep[0m  [16/21], [94mLoss[0m : 2.96472
[1mStep[0m  [18/21], [94mLoss[0m : 2.98035
[1mStep[0m  [20/21], [94mLoss[0m : 2.80309

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.469, [92mTest[0m: 5.382, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.97839
[1mStep[0m  [2/21], [94mLoss[0m : 2.70222
[1mStep[0m  [4/21], [94mLoss[0m : 2.99473
[1mStep[0m  [6/21], [94mLoss[0m : 3.09449
[1mStep[0m  [8/21], [94mLoss[0m : 2.81456
[1mStep[0m  [10/21], [94mLoss[0m : 2.70384
[1mStep[0m  [12/21], [94mLoss[0m : 2.89736
[1mStep[0m  [14/21], [94mLoss[0m : 2.64648
[1mStep[0m  [16/21], [94mLoss[0m : 2.88944
[1mStep[0m  [18/21], [94mLoss[0m : 2.75922
[1mStep[0m  [20/21], [94mLoss[0m : 2.80354

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.831, [92mTest[0m: 3.415, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.55230
[1mStep[0m  [2/21], [94mLoss[0m : 2.71799
[1mStep[0m  [4/21], [94mLoss[0m : 2.58660
[1mStep[0m  [6/21], [94mLoss[0m : 2.77141
[1mStep[0m  [8/21], [94mLoss[0m : 2.73997
[1mStep[0m  [10/21], [94mLoss[0m : 2.59944
[1mStep[0m  [12/21], [94mLoss[0m : 2.63435
[1mStep[0m  [14/21], [94mLoss[0m : 2.65099
[1mStep[0m  [16/21], [94mLoss[0m : 2.73526
[1mStep[0m  [18/21], [94mLoss[0m : 2.69668
[1mStep[0m  [20/21], [94mLoss[0m : 2.79710

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.703, [92mTest[0m: 2.953, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.61312
[1mStep[0m  [2/21], [94mLoss[0m : 2.69344
[1mStep[0m  [4/21], [94mLoss[0m : 2.64006
[1mStep[0m  [6/21], [94mLoss[0m : 2.83531
[1mStep[0m  [8/21], [94mLoss[0m : 2.77095
[1mStep[0m  [10/21], [94mLoss[0m : 2.44469
[1mStep[0m  [12/21], [94mLoss[0m : 2.54632
[1mStep[0m  [14/21], [94mLoss[0m : 2.92325
[1mStep[0m  [16/21], [94mLoss[0m : 2.50360
[1mStep[0m  [18/21], [94mLoss[0m : 2.64237
[1mStep[0m  [20/21], [94mLoss[0m : 2.67171

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.649, [92mTest[0m: 2.808, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.76088
[1mStep[0m  [2/21], [94mLoss[0m : 2.65034
[1mStep[0m  [4/21], [94mLoss[0m : 2.63081
[1mStep[0m  [6/21], [94mLoss[0m : 2.57385
[1mStep[0m  [8/21], [94mLoss[0m : 2.53804
[1mStep[0m  [10/21], [94mLoss[0m : 2.73918
[1mStep[0m  [12/21], [94mLoss[0m : 2.49569
[1mStep[0m  [14/21], [94mLoss[0m : 2.72020
[1mStep[0m  [16/21], [94mLoss[0m : 2.72987
[1mStep[0m  [18/21], [94mLoss[0m : 2.63652
[1mStep[0m  [20/21], [94mLoss[0m : 2.70378

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.641, [92mTest[0m: 2.713, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52087
[1mStep[0m  [2/21], [94mLoss[0m : 2.71920
[1mStep[0m  [4/21], [94mLoss[0m : 2.55960
[1mStep[0m  [6/21], [94mLoss[0m : 2.71749
[1mStep[0m  [8/21], [94mLoss[0m : 2.63252
[1mStep[0m  [10/21], [94mLoss[0m : 2.56033
[1mStep[0m  [12/21], [94mLoss[0m : 2.57225
[1mStep[0m  [14/21], [94mLoss[0m : 2.43499
[1mStep[0m  [16/21], [94mLoss[0m : 2.68015
[1mStep[0m  [18/21], [94mLoss[0m : 2.64154
[1mStep[0m  [20/21], [94mLoss[0m : 2.72863

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.606, [92mTest[0m: 2.689, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56979
[1mStep[0m  [2/21], [94mLoss[0m : 2.67817
[1mStep[0m  [4/21], [94mLoss[0m : 2.60110
[1mStep[0m  [6/21], [94mLoss[0m : 2.60044
[1mStep[0m  [8/21], [94mLoss[0m : 2.60522
[1mStep[0m  [10/21], [94mLoss[0m : 2.49935
[1mStep[0m  [12/21], [94mLoss[0m : 2.69237
[1mStep[0m  [14/21], [94mLoss[0m : 2.53809
[1mStep[0m  [16/21], [94mLoss[0m : 2.40349
[1mStep[0m  [18/21], [94mLoss[0m : 2.53312
[1mStep[0m  [20/21], [94mLoss[0m : 2.54585

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.605, [92mTest[0m: 2.653, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.74611
[1mStep[0m  [2/21], [94mLoss[0m : 2.49611
[1mStep[0m  [4/21], [94mLoss[0m : 2.61794
[1mStep[0m  [6/21], [94mLoss[0m : 2.67125
[1mStep[0m  [8/21], [94mLoss[0m : 2.45638
[1mStep[0m  [10/21], [94mLoss[0m : 2.50458
[1mStep[0m  [12/21], [94mLoss[0m : 2.52936
[1mStep[0m  [14/21], [94mLoss[0m : 2.59541
[1mStep[0m  [16/21], [94mLoss[0m : 2.64856
[1mStep[0m  [18/21], [94mLoss[0m : 2.72494
[1mStep[0m  [20/21], [94mLoss[0m : 2.57833

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.598, [92mTest[0m: 2.642, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52929
[1mStep[0m  [2/21], [94mLoss[0m : 2.66593
[1mStep[0m  [4/21], [94mLoss[0m : 2.78127
[1mStep[0m  [6/21], [94mLoss[0m : 2.58731
[1mStep[0m  [8/21], [94mLoss[0m : 2.69533
[1mStep[0m  [10/21], [94mLoss[0m : 2.61321
[1mStep[0m  [12/21], [94mLoss[0m : 2.69926
[1mStep[0m  [14/21], [94mLoss[0m : 2.56087
[1mStep[0m  [16/21], [94mLoss[0m : 2.56231
[1mStep[0m  [18/21], [94mLoss[0m : 2.50501
[1mStep[0m  [20/21], [94mLoss[0m : 2.48897

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.577, [92mTest[0m: 2.650, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42844
[1mStep[0m  [2/21], [94mLoss[0m : 2.46143
[1mStep[0m  [4/21], [94mLoss[0m : 2.55796
[1mStep[0m  [6/21], [94mLoss[0m : 2.58467
[1mStep[0m  [8/21], [94mLoss[0m : 2.67730
[1mStep[0m  [10/21], [94mLoss[0m : 2.39796
[1mStep[0m  [12/21], [94mLoss[0m : 2.65501
[1mStep[0m  [14/21], [94mLoss[0m : 2.51672
[1mStep[0m  [16/21], [94mLoss[0m : 2.42577
[1mStep[0m  [18/21], [94mLoss[0m : 2.57901
[1mStep[0m  [20/21], [94mLoss[0m : 2.50916

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.544, [92mTest[0m: 2.617, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56318
[1mStep[0m  [2/21], [94mLoss[0m : 2.70808
[1mStep[0m  [4/21], [94mLoss[0m : 2.56442
[1mStep[0m  [6/21], [94mLoss[0m : 2.53230
[1mStep[0m  [8/21], [94mLoss[0m : 2.44379
[1mStep[0m  [10/21], [94mLoss[0m : 2.49803
[1mStep[0m  [12/21], [94mLoss[0m : 2.57779
[1mStep[0m  [14/21], [94mLoss[0m : 2.60106
[1mStep[0m  [16/21], [94mLoss[0m : 2.33099
[1mStep[0m  [18/21], [94mLoss[0m : 2.53895
[1mStep[0m  [20/21], [94mLoss[0m : 2.57304

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.561, [92mTest[0m: 2.620, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.57133
[1mStep[0m  [2/21], [94mLoss[0m : 2.56524
[1mStep[0m  [4/21], [94mLoss[0m : 2.46939
[1mStep[0m  [6/21], [94mLoss[0m : 2.49772
[1mStep[0m  [8/21], [94mLoss[0m : 2.35046
[1mStep[0m  [10/21], [94mLoss[0m : 2.55088
[1mStep[0m  [12/21], [94mLoss[0m : 2.26992
[1mStep[0m  [14/21], [94mLoss[0m : 2.63342
[1mStep[0m  [16/21], [94mLoss[0m : 2.59430
[1mStep[0m  [18/21], [94mLoss[0m : 2.69352
[1mStep[0m  [20/21], [94mLoss[0m : 2.51079

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.534, [92mTest[0m: 2.617, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48162
[1mStep[0m  [2/21], [94mLoss[0m : 2.35973
[1mStep[0m  [4/21], [94mLoss[0m : 2.60687
[1mStep[0m  [6/21], [94mLoss[0m : 2.50721
[1mStep[0m  [8/21], [94mLoss[0m : 2.46789
[1mStep[0m  [10/21], [94mLoss[0m : 2.62172
[1mStep[0m  [12/21], [94mLoss[0m : 2.51150
[1mStep[0m  [14/21], [94mLoss[0m : 2.48142
[1mStep[0m  [16/21], [94mLoss[0m : 2.58100
[1mStep[0m  [18/21], [94mLoss[0m : 2.57372
[1mStep[0m  [20/21], [94mLoss[0m : 2.47311

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.531, [92mTest[0m: 2.582, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56774
[1mStep[0m  [2/21], [94mLoss[0m : 2.62736
[1mStep[0m  [4/21], [94mLoss[0m : 2.51416
[1mStep[0m  [6/21], [94mLoss[0m : 2.46118
[1mStep[0m  [8/21], [94mLoss[0m : 2.52662
[1mStep[0m  [10/21], [94mLoss[0m : 2.42201
[1mStep[0m  [12/21], [94mLoss[0m : 2.59296
[1mStep[0m  [14/21], [94mLoss[0m : 2.50223
[1mStep[0m  [16/21], [94mLoss[0m : 2.49416
[1mStep[0m  [18/21], [94mLoss[0m : 2.49628
[1mStep[0m  [20/21], [94mLoss[0m : 2.47508

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.538, [92mTest[0m: 2.596, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56070
[1mStep[0m  [2/21], [94mLoss[0m : 2.52990
[1mStep[0m  [4/21], [94mLoss[0m : 2.51398
[1mStep[0m  [6/21], [94mLoss[0m : 2.53657
[1mStep[0m  [8/21], [94mLoss[0m : 2.70320
[1mStep[0m  [10/21], [94mLoss[0m : 2.50014
[1mStep[0m  [12/21], [94mLoss[0m : 2.59808
[1mStep[0m  [14/21], [94mLoss[0m : 2.54968
[1mStep[0m  [16/21], [94mLoss[0m : 2.56831
[1mStep[0m  [18/21], [94mLoss[0m : 2.51031
[1mStep[0m  [20/21], [94mLoss[0m : 2.61162

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.550, [92mTest[0m: 2.610, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.62200
[1mStep[0m  [2/21], [94mLoss[0m : 2.43485
[1mStep[0m  [4/21], [94mLoss[0m : 2.54178
[1mStep[0m  [6/21], [94mLoss[0m : 2.45835
[1mStep[0m  [8/21], [94mLoss[0m : 2.54624
[1mStep[0m  [10/21], [94mLoss[0m : 2.57413
[1mStep[0m  [12/21], [94mLoss[0m : 2.52597
[1mStep[0m  [14/21], [94mLoss[0m : 2.53776
[1mStep[0m  [16/21], [94mLoss[0m : 2.52241
[1mStep[0m  [18/21], [94mLoss[0m : 2.40029
[1mStep[0m  [20/21], [94mLoss[0m : 2.45388

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.620, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.37468
[1mStep[0m  [2/21], [94mLoss[0m : 2.57524
[1mStep[0m  [4/21], [94mLoss[0m : 2.50948
[1mStep[0m  [6/21], [94mLoss[0m : 2.51047
[1mStep[0m  [8/21], [94mLoss[0m : 2.48054
[1mStep[0m  [10/21], [94mLoss[0m : 2.55709
[1mStep[0m  [12/21], [94mLoss[0m : 2.48046
[1mStep[0m  [14/21], [94mLoss[0m : 2.61933
[1mStep[0m  [16/21], [94mLoss[0m : 2.45946
[1mStep[0m  [18/21], [94mLoss[0m : 2.57733
[1mStep[0m  [20/21], [94mLoss[0m : 2.47849

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.575, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56766
[1mStep[0m  [2/21], [94mLoss[0m : 2.41611
[1mStep[0m  [4/21], [94mLoss[0m : 2.60754
[1mStep[0m  [6/21], [94mLoss[0m : 2.65118
[1mStep[0m  [8/21], [94mLoss[0m : 2.55707
[1mStep[0m  [10/21], [94mLoss[0m : 2.43528
[1mStep[0m  [12/21], [94mLoss[0m : 2.47612
[1mStep[0m  [14/21], [94mLoss[0m : 2.44648
[1mStep[0m  [16/21], [94mLoss[0m : 2.47775
[1mStep[0m  [18/21], [94mLoss[0m : 2.51465
[1mStep[0m  [20/21], [94mLoss[0m : 2.64291

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.526, [92mTest[0m: 2.590, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.45780
[1mStep[0m  [2/21], [94mLoss[0m : 2.61843
[1mStep[0m  [4/21], [94mLoss[0m : 2.50195
[1mStep[0m  [6/21], [94mLoss[0m : 2.62129
[1mStep[0m  [8/21], [94mLoss[0m : 2.36598
[1mStep[0m  [10/21], [94mLoss[0m : 2.47819
[1mStep[0m  [12/21], [94mLoss[0m : 2.37330
[1mStep[0m  [14/21], [94mLoss[0m : 2.36671
[1mStep[0m  [16/21], [94mLoss[0m : 2.50889
[1mStep[0m  [18/21], [94mLoss[0m : 2.65425
[1mStep[0m  [20/21], [94mLoss[0m : 2.76951

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.529, [92mTest[0m: 2.559, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43205
[1mStep[0m  [2/21], [94mLoss[0m : 2.34402
[1mStep[0m  [4/21], [94mLoss[0m : 2.58415
[1mStep[0m  [6/21], [94mLoss[0m : 2.54166
[1mStep[0m  [8/21], [94mLoss[0m : 2.58574
[1mStep[0m  [10/21], [94mLoss[0m : 2.58115
[1mStep[0m  [12/21], [94mLoss[0m : 2.52709
[1mStep[0m  [14/21], [94mLoss[0m : 2.57047
[1mStep[0m  [16/21], [94mLoss[0m : 2.51858
[1mStep[0m  [18/21], [94mLoss[0m : 2.60937
[1mStep[0m  [20/21], [94mLoss[0m : 2.39113

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.575, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.49213
[1mStep[0m  [2/21], [94mLoss[0m : 2.39323
[1mStep[0m  [4/21], [94mLoss[0m : 2.38137
[1mStep[0m  [6/21], [94mLoss[0m : 2.35794
[1mStep[0m  [8/21], [94mLoss[0m : 2.43803
[1mStep[0m  [10/21], [94mLoss[0m : 2.49731
[1mStep[0m  [12/21], [94mLoss[0m : 2.52857
[1mStep[0m  [14/21], [94mLoss[0m : 2.48612
[1mStep[0m  [16/21], [94mLoss[0m : 2.50084
[1mStep[0m  [18/21], [94mLoss[0m : 2.39783
[1mStep[0m  [20/21], [94mLoss[0m : 2.57706

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.590, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.59684
[1mStep[0m  [2/21], [94mLoss[0m : 2.53930
[1mStep[0m  [4/21], [94mLoss[0m : 2.53522
[1mStep[0m  [6/21], [94mLoss[0m : 2.35756
[1mStep[0m  [8/21], [94mLoss[0m : 2.52966
[1mStep[0m  [10/21], [94mLoss[0m : 2.46695
[1mStep[0m  [12/21], [94mLoss[0m : 2.37823
[1mStep[0m  [14/21], [94mLoss[0m : 2.44933
[1mStep[0m  [16/21], [94mLoss[0m : 2.54036
[1mStep[0m  [18/21], [94mLoss[0m : 2.42293
[1mStep[0m  [20/21], [94mLoss[0m : 2.66689

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.570, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53122
[1mStep[0m  [2/21], [94mLoss[0m : 2.46335
[1mStep[0m  [4/21], [94mLoss[0m : 2.31765
[1mStep[0m  [6/21], [94mLoss[0m : 2.68976
[1mStep[0m  [8/21], [94mLoss[0m : 2.49581
[1mStep[0m  [10/21], [94mLoss[0m : 2.51368
[1mStep[0m  [12/21], [94mLoss[0m : 2.65619
[1mStep[0m  [14/21], [94mLoss[0m : 2.54018
[1mStep[0m  [16/21], [94mLoss[0m : 2.49076
[1mStep[0m  [18/21], [94mLoss[0m : 2.44089
[1mStep[0m  [20/21], [94mLoss[0m : 2.38071

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.573, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51578
[1mStep[0m  [2/21], [94mLoss[0m : 2.51982
[1mStep[0m  [4/21], [94mLoss[0m : 2.45352
[1mStep[0m  [6/21], [94mLoss[0m : 2.49595
[1mStep[0m  [8/21], [94mLoss[0m : 2.59347
[1mStep[0m  [10/21], [94mLoss[0m : 2.59887
[1mStep[0m  [12/21], [94mLoss[0m : 2.45902
[1mStep[0m  [14/21], [94mLoss[0m : 2.61416
[1mStep[0m  [16/21], [94mLoss[0m : 2.44861
[1mStep[0m  [18/21], [94mLoss[0m : 2.57343
[1mStep[0m  [20/21], [94mLoss[0m : 2.55952

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.510, [92mTest[0m: 2.568, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54355
[1mStep[0m  [2/21], [94mLoss[0m : 2.54564
[1mStep[0m  [4/21], [94mLoss[0m : 2.54836
[1mStep[0m  [6/21], [94mLoss[0m : 2.42688
[1mStep[0m  [8/21], [94mLoss[0m : 2.38791
[1mStep[0m  [10/21], [94mLoss[0m : 2.54160
[1mStep[0m  [12/21], [94mLoss[0m : 2.54069
[1mStep[0m  [14/21], [94mLoss[0m : 2.38786
[1mStep[0m  [16/21], [94mLoss[0m : 2.74025
[1mStep[0m  [18/21], [94mLoss[0m : 2.54418
[1mStep[0m  [20/21], [94mLoss[0m : 2.46813

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.561, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44190
[1mStep[0m  [2/21], [94mLoss[0m : 2.37743
[1mStep[0m  [4/21], [94mLoss[0m : 2.45617
[1mStep[0m  [6/21], [94mLoss[0m : 2.50337
[1mStep[0m  [8/21], [94mLoss[0m : 2.47735
[1mStep[0m  [10/21], [94mLoss[0m : 2.50523
[1mStep[0m  [12/21], [94mLoss[0m : 2.46891
[1mStep[0m  [14/21], [94mLoss[0m : 2.53597
[1mStep[0m  [16/21], [94mLoss[0m : 2.65359
[1mStep[0m  [18/21], [94mLoss[0m : 2.68353
[1mStep[0m  [20/21], [94mLoss[0m : 2.53900

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.554, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.34973
[1mStep[0m  [2/21], [94mLoss[0m : 2.62242
[1mStep[0m  [4/21], [94mLoss[0m : 2.52314
[1mStep[0m  [6/21], [94mLoss[0m : 2.47762
[1mStep[0m  [8/21], [94mLoss[0m : 2.52412
[1mStep[0m  [10/21], [94mLoss[0m : 2.57613
[1mStep[0m  [12/21], [94mLoss[0m : 2.50356
[1mStep[0m  [14/21], [94mLoss[0m : 2.52403
[1mStep[0m  [16/21], [94mLoss[0m : 2.71030
[1mStep[0m  [18/21], [94mLoss[0m : 2.45859
[1mStep[0m  [20/21], [94mLoss[0m : 2.34735

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.561, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.59514
[1mStep[0m  [2/21], [94mLoss[0m : 2.41197
[1mStep[0m  [4/21], [94mLoss[0m : 2.48002
[1mStep[0m  [6/21], [94mLoss[0m : 2.43863
[1mStep[0m  [8/21], [94mLoss[0m : 2.46174
[1mStep[0m  [10/21], [94mLoss[0m : 2.34678
[1mStep[0m  [12/21], [94mLoss[0m : 2.56204
[1mStep[0m  [14/21], [94mLoss[0m : 2.43292
[1mStep[0m  [16/21], [94mLoss[0m : 2.41704
[1mStep[0m  [18/21], [94mLoss[0m : 2.54480
[1mStep[0m  [20/21], [94mLoss[0m : 2.54344

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.544, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.565
====================================

Phase 1 - Evaluation MAE:  2.565485511507307
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/21], [94mLoss[0m : 2.54505
[1mStep[0m  [2/21], [94mLoss[0m : 2.43774
[1mStep[0m  [4/21], [94mLoss[0m : 2.55854
[1mStep[0m  [6/21], [94mLoss[0m : 2.47550
[1mStep[0m  [8/21], [94mLoss[0m : 2.54841
[1mStep[0m  [10/21], [94mLoss[0m : 2.50513
[1mStep[0m  [12/21], [94mLoss[0m : 2.61013
[1mStep[0m  [14/21], [94mLoss[0m : 2.28916
[1mStep[0m  [16/21], [94mLoss[0m : 2.44763
[1mStep[0m  [18/21], [94mLoss[0m : 2.51019
[1mStep[0m  [20/21], [94mLoss[0m : 2.50650

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.566, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.39941
[1mStep[0m  [2/21], [94mLoss[0m : 2.59117
[1mStep[0m  [4/21], [94mLoss[0m : 2.36796
[1mStep[0m  [6/21], [94mLoss[0m : 2.66999
[1mStep[0m  [8/21], [94mLoss[0m : 2.51176
[1mStep[0m  [10/21], [94mLoss[0m : 2.50083
[1mStep[0m  [12/21], [94mLoss[0m : 2.57987
[1mStep[0m  [14/21], [94mLoss[0m : 2.49704
[1mStep[0m  [16/21], [94mLoss[0m : 2.53644
[1mStep[0m  [18/21], [94mLoss[0m : 2.51607
[1mStep[0m  [20/21], [94mLoss[0m : 2.29822

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.522, [92mTest[0m: 3.234, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46170
[1mStep[0m  [2/21], [94mLoss[0m : 2.38218
[1mStep[0m  [4/21], [94mLoss[0m : 2.61085
[1mStep[0m  [6/21], [94mLoss[0m : 2.50433
[1mStep[0m  [8/21], [94mLoss[0m : 2.63831
[1mStep[0m  [10/21], [94mLoss[0m : 2.55786
[1mStep[0m  [12/21], [94mLoss[0m : 2.40813
[1mStep[0m  [14/21], [94mLoss[0m : 2.25978
[1mStep[0m  [16/21], [94mLoss[0m : 2.54219
[1mStep[0m  [18/21], [94mLoss[0m : 2.36224
[1mStep[0m  [20/21], [94mLoss[0m : 2.50950

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.651, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.76519
[1mStep[0m  [2/21], [94mLoss[0m : 2.60038
[1mStep[0m  [4/21], [94mLoss[0m : 2.49472
[1mStep[0m  [6/21], [94mLoss[0m : 2.51343
[1mStep[0m  [8/21], [94mLoss[0m : 2.38346
[1mStep[0m  [10/21], [94mLoss[0m : 2.46956
[1mStep[0m  [12/21], [94mLoss[0m : 2.54206
[1mStep[0m  [14/21], [94mLoss[0m : 2.37074
[1mStep[0m  [16/21], [94mLoss[0m : 2.49624
[1mStep[0m  [18/21], [94mLoss[0m : 2.53088
[1mStep[0m  [20/21], [94mLoss[0m : 2.50078

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.530, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.36047
[1mStep[0m  [2/21], [94mLoss[0m : 2.31295
[1mStep[0m  [4/21], [94mLoss[0m : 2.46439
[1mStep[0m  [6/21], [94mLoss[0m : 2.41781
[1mStep[0m  [8/21], [94mLoss[0m : 2.46284
[1mStep[0m  [10/21], [94mLoss[0m : 2.55887
[1mStep[0m  [12/21], [94mLoss[0m : 2.42714
[1mStep[0m  [14/21], [94mLoss[0m : 2.50171
[1mStep[0m  [16/21], [94mLoss[0m : 2.41390
[1mStep[0m  [18/21], [94mLoss[0m : 2.51068
[1mStep[0m  [20/21], [94mLoss[0m : 2.45872

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.474, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42498
[1mStep[0m  [2/21], [94mLoss[0m : 2.47809
[1mStep[0m  [4/21], [94mLoss[0m : 2.46499
[1mStep[0m  [6/21], [94mLoss[0m : 2.31672
[1mStep[0m  [8/21], [94mLoss[0m : 2.48597
[1mStep[0m  [10/21], [94mLoss[0m : 2.27836
[1mStep[0m  [12/21], [94mLoss[0m : 2.41796
[1mStep[0m  [14/21], [94mLoss[0m : 2.45423
[1mStep[0m  [16/21], [94mLoss[0m : 2.28468
[1mStep[0m  [18/21], [94mLoss[0m : 2.51232
[1mStep[0m  [20/21], [94mLoss[0m : 2.31722

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.431, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51267
[1mStep[0m  [2/21], [94mLoss[0m : 2.40683
[1mStep[0m  [4/21], [94mLoss[0m : 2.40084
[1mStep[0m  [6/21], [94mLoss[0m : 2.48030
[1mStep[0m  [8/21], [94mLoss[0m : 2.58973
[1mStep[0m  [10/21], [94mLoss[0m : 2.57617
[1mStep[0m  [12/21], [94mLoss[0m : 2.40991
[1mStep[0m  [14/21], [94mLoss[0m : 2.41327
[1mStep[0m  [16/21], [94mLoss[0m : 2.34290
[1mStep[0m  [18/21], [94mLoss[0m : 2.33136
[1mStep[0m  [20/21], [94mLoss[0m : 2.39523

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.417, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48673
[1mStep[0m  [2/21], [94mLoss[0m : 2.53737
[1mStep[0m  [4/21], [94mLoss[0m : 2.42599
[1mStep[0m  [6/21], [94mLoss[0m : 2.45680
[1mStep[0m  [8/21], [94mLoss[0m : 2.37581
[1mStep[0m  [10/21], [94mLoss[0m : 2.28600
[1mStep[0m  [12/21], [94mLoss[0m : 2.46583
[1mStep[0m  [14/21], [94mLoss[0m : 2.40450
[1mStep[0m  [16/21], [94mLoss[0m : 2.41489
[1mStep[0m  [18/21], [94mLoss[0m : 2.43084
[1mStep[0m  [20/21], [94mLoss[0m : 2.50302

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.428, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52841
[1mStep[0m  [2/21], [94mLoss[0m : 2.49283
[1mStep[0m  [4/21], [94mLoss[0m : 2.48479
[1mStep[0m  [6/21], [94mLoss[0m : 2.47509
[1mStep[0m  [8/21], [94mLoss[0m : 2.54540
[1mStep[0m  [10/21], [94mLoss[0m : 2.34187
[1mStep[0m  [12/21], [94mLoss[0m : 2.35866
[1mStep[0m  [14/21], [94mLoss[0m : 2.48917
[1mStep[0m  [16/21], [94mLoss[0m : 2.44487
[1mStep[0m  [18/21], [94mLoss[0m : 2.54747
[1mStep[0m  [20/21], [94mLoss[0m : 2.38816

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.416, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.45327
[1mStep[0m  [2/21], [94mLoss[0m : 2.42770
[1mStep[0m  [4/21], [94mLoss[0m : 2.49287
[1mStep[0m  [6/21], [94mLoss[0m : 2.31212
[1mStep[0m  [8/21], [94mLoss[0m : 2.45553
[1mStep[0m  [10/21], [94mLoss[0m : 2.26925
[1mStep[0m  [12/21], [94mLoss[0m : 2.43778
[1mStep[0m  [14/21], [94mLoss[0m : 2.40559
[1mStep[0m  [16/21], [94mLoss[0m : 2.43029
[1mStep[0m  [18/21], [94mLoss[0m : 2.41400
[1mStep[0m  [20/21], [94mLoss[0m : 2.39239

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.394, [92mTest[0m: 2.435, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44813
[1mStep[0m  [2/21], [94mLoss[0m : 2.17866
[1mStep[0m  [4/21], [94mLoss[0m : 2.40353
[1mStep[0m  [6/21], [94mLoss[0m : 2.45456
[1mStep[0m  [8/21], [94mLoss[0m : 2.37553
[1mStep[0m  [10/21], [94mLoss[0m : 2.46033
[1mStep[0m  [12/21], [94mLoss[0m : 2.25107
[1mStep[0m  [14/21], [94mLoss[0m : 2.31885
[1mStep[0m  [16/21], [94mLoss[0m : 2.32869
[1mStep[0m  [18/21], [94mLoss[0m : 2.36338
[1mStep[0m  [20/21], [94mLoss[0m : 2.31461

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.369, [92mTest[0m: 2.440, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.22237
[1mStep[0m  [2/21], [94mLoss[0m : 2.40359
[1mStep[0m  [4/21], [94mLoss[0m : 2.27872
[1mStep[0m  [6/21], [94mLoss[0m : 2.38174
[1mStep[0m  [8/21], [94mLoss[0m : 2.33496
[1mStep[0m  [10/21], [94mLoss[0m : 2.36240
[1mStep[0m  [12/21], [94mLoss[0m : 2.36200
[1mStep[0m  [14/21], [94mLoss[0m : 2.33779
[1mStep[0m  [16/21], [94mLoss[0m : 2.41179
[1mStep[0m  [18/21], [94mLoss[0m : 2.38598
[1mStep[0m  [20/21], [94mLoss[0m : 2.38420

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.368, [92mTest[0m: 2.456, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.34595
[1mStep[0m  [2/21], [94mLoss[0m : 2.41792
[1mStep[0m  [4/21], [94mLoss[0m : 2.52398
[1mStep[0m  [6/21], [94mLoss[0m : 2.19358
[1mStep[0m  [8/21], [94mLoss[0m : 2.30630
[1mStep[0m  [10/21], [94mLoss[0m : 2.39265
[1mStep[0m  [12/21], [94mLoss[0m : 2.41243
[1mStep[0m  [14/21], [94mLoss[0m : 2.23689
[1mStep[0m  [16/21], [94mLoss[0m : 2.26707
[1mStep[0m  [18/21], [94mLoss[0m : 2.29596
[1mStep[0m  [20/21], [94mLoss[0m : 2.49040

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.362, [92mTest[0m: 2.502, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42378
[1mStep[0m  [2/21], [94mLoss[0m : 2.25340
[1mStep[0m  [4/21], [94mLoss[0m : 2.21454
[1mStep[0m  [6/21], [94mLoss[0m : 2.24484
[1mStep[0m  [8/21], [94mLoss[0m : 2.25200
[1mStep[0m  [10/21], [94mLoss[0m : 2.30286
[1mStep[0m  [12/21], [94mLoss[0m : 2.50569
[1mStep[0m  [14/21], [94mLoss[0m : 2.33967
[1mStep[0m  [16/21], [94mLoss[0m : 2.45358
[1mStep[0m  [18/21], [94mLoss[0m : 2.28418
[1mStep[0m  [20/21], [94mLoss[0m : 2.44535

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.328, [92mTest[0m: 2.467, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.29323
[1mStep[0m  [2/21], [94mLoss[0m : 2.33081
[1mStep[0m  [4/21], [94mLoss[0m : 2.14829
[1mStep[0m  [6/21], [94mLoss[0m : 2.47596
[1mStep[0m  [8/21], [94mLoss[0m : 2.28196
[1mStep[0m  [10/21], [94mLoss[0m : 2.39754
[1mStep[0m  [12/21], [94mLoss[0m : 2.26631
[1mStep[0m  [14/21], [94mLoss[0m : 2.33369
[1mStep[0m  [16/21], [94mLoss[0m : 2.35234
[1mStep[0m  [18/21], [94mLoss[0m : 2.43652
[1mStep[0m  [20/21], [94mLoss[0m : 2.32110

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.323, [92mTest[0m: 2.466, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.12230
[1mStep[0m  [2/21], [94mLoss[0m : 2.30127
[1mStep[0m  [4/21], [94mLoss[0m : 2.29745
[1mStep[0m  [6/21], [94mLoss[0m : 2.32003
[1mStep[0m  [8/21], [94mLoss[0m : 2.27069
[1mStep[0m  [10/21], [94mLoss[0m : 2.40029
[1mStep[0m  [12/21], [94mLoss[0m : 2.22414
[1mStep[0m  [14/21], [94mLoss[0m : 2.38673
[1mStep[0m  [16/21], [94mLoss[0m : 2.23576
[1mStep[0m  [18/21], [94mLoss[0m : 2.26985
[1mStep[0m  [20/21], [94mLoss[0m : 2.35818

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.291, [92mTest[0m: 2.474, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.31745
[1mStep[0m  [2/21], [94mLoss[0m : 2.29326
[1mStep[0m  [4/21], [94mLoss[0m : 2.27843
[1mStep[0m  [6/21], [94mLoss[0m : 2.29364
[1mStep[0m  [8/21], [94mLoss[0m : 2.31459
[1mStep[0m  [10/21], [94mLoss[0m : 2.29845
[1mStep[0m  [12/21], [94mLoss[0m : 2.12451
[1mStep[0m  [14/21], [94mLoss[0m : 2.32984
[1mStep[0m  [16/21], [94mLoss[0m : 2.22389
[1mStep[0m  [18/21], [94mLoss[0m : 2.10512
[1mStep[0m  [20/21], [94mLoss[0m : 2.33297

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.283, [92mTest[0m: 2.420, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.29456
[1mStep[0m  [2/21], [94mLoss[0m : 2.09493
[1mStep[0m  [4/21], [94mLoss[0m : 2.23442
[1mStep[0m  [6/21], [94mLoss[0m : 2.33385
[1mStep[0m  [8/21], [94mLoss[0m : 2.25091
[1mStep[0m  [10/21], [94mLoss[0m : 2.24319
[1mStep[0m  [12/21], [94mLoss[0m : 2.27923
[1mStep[0m  [14/21], [94mLoss[0m : 2.36654
[1mStep[0m  [16/21], [94mLoss[0m : 2.19115
[1mStep[0m  [18/21], [94mLoss[0m : 2.24761
[1mStep[0m  [20/21], [94mLoss[0m : 2.35482

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.274, [92mTest[0m: 2.411, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.19844
[1mStep[0m  [2/21], [94mLoss[0m : 2.13734
[1mStep[0m  [4/21], [94mLoss[0m : 2.13785
[1mStep[0m  [6/21], [94mLoss[0m : 2.29877
[1mStep[0m  [8/21], [94mLoss[0m : 2.26364
[1mStep[0m  [10/21], [94mLoss[0m : 2.26320
[1mStep[0m  [12/21], [94mLoss[0m : 2.24088
[1mStep[0m  [14/21], [94mLoss[0m : 2.39206
[1mStep[0m  [16/21], [94mLoss[0m : 2.38972
[1mStep[0m  [18/21], [94mLoss[0m : 2.32758
[1mStep[0m  [20/21], [94mLoss[0m : 2.16631

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.249, [92mTest[0m: 2.402, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.33290
[1mStep[0m  [2/21], [94mLoss[0m : 2.33795
[1mStep[0m  [4/21], [94mLoss[0m : 2.34531
[1mStep[0m  [6/21], [94mLoss[0m : 2.25363
[1mStep[0m  [8/21], [94mLoss[0m : 2.07773
[1mStep[0m  [10/21], [94mLoss[0m : 2.12339
[1mStep[0m  [12/21], [94mLoss[0m : 2.27021
[1mStep[0m  [14/21], [94mLoss[0m : 2.11874
[1mStep[0m  [16/21], [94mLoss[0m : 2.44352
[1mStep[0m  [18/21], [94mLoss[0m : 2.38917
[1mStep[0m  [20/21], [94mLoss[0m : 2.20324

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.258, [92mTest[0m: 2.401, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.28408
[1mStep[0m  [2/21], [94mLoss[0m : 2.26567
[1mStep[0m  [4/21], [94mLoss[0m : 2.22423
[1mStep[0m  [6/21], [94mLoss[0m : 2.27868
[1mStep[0m  [8/21], [94mLoss[0m : 2.24597
[1mStep[0m  [10/21], [94mLoss[0m : 2.22771
[1mStep[0m  [12/21], [94mLoss[0m : 2.22977
[1mStep[0m  [14/21], [94mLoss[0m : 2.19900
[1mStep[0m  [16/21], [94mLoss[0m : 2.22667
[1mStep[0m  [18/21], [94mLoss[0m : 2.33040
[1mStep[0m  [20/21], [94mLoss[0m : 2.23612

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.242, [92mTest[0m: 2.411, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.17085
[1mStep[0m  [2/21], [94mLoss[0m : 2.19614
[1mStep[0m  [4/21], [94mLoss[0m : 2.21077
[1mStep[0m  [6/21], [94mLoss[0m : 2.37986
[1mStep[0m  [8/21], [94mLoss[0m : 2.05719
[1mStep[0m  [10/21], [94mLoss[0m : 2.14549
[1mStep[0m  [12/21], [94mLoss[0m : 2.18127
[1mStep[0m  [14/21], [94mLoss[0m : 2.33083
[1mStep[0m  [16/21], [94mLoss[0m : 2.31346
[1mStep[0m  [18/21], [94mLoss[0m : 2.28637
[1mStep[0m  [20/21], [94mLoss[0m : 2.29259

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.216, [92mTest[0m: 2.421, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.25681
[1mStep[0m  [2/21], [94mLoss[0m : 2.08184
[1mStep[0m  [4/21], [94mLoss[0m : 2.19903
[1mStep[0m  [6/21], [94mLoss[0m : 2.07575
[1mStep[0m  [8/21], [94mLoss[0m : 2.20715
[1mStep[0m  [10/21], [94mLoss[0m : 2.14046
[1mStep[0m  [12/21], [94mLoss[0m : 2.15237
[1mStep[0m  [14/21], [94mLoss[0m : 2.13509
[1mStep[0m  [16/21], [94mLoss[0m : 2.32231
[1mStep[0m  [18/21], [94mLoss[0m : 2.32233
[1mStep[0m  [20/21], [94mLoss[0m : 2.26704

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.190, [92mTest[0m: 2.411, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.19172
[1mStep[0m  [2/21], [94mLoss[0m : 2.25858
[1mStep[0m  [4/21], [94mLoss[0m : 2.13368
[1mStep[0m  [6/21], [94mLoss[0m : 2.19037
[1mStep[0m  [8/21], [94mLoss[0m : 2.05385
[1mStep[0m  [10/21], [94mLoss[0m : 2.13314
[1mStep[0m  [12/21], [94mLoss[0m : 2.26902
[1mStep[0m  [14/21], [94mLoss[0m : 2.13761
[1mStep[0m  [16/21], [94mLoss[0m : 2.22039
[1mStep[0m  [18/21], [94mLoss[0m : 2.20629
[1mStep[0m  [20/21], [94mLoss[0m : 2.14122

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.179, [92mTest[0m: 2.426, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.10575
[1mStep[0m  [2/21], [94mLoss[0m : 2.20545
[1mStep[0m  [4/21], [94mLoss[0m : 2.09984
[1mStep[0m  [6/21], [94mLoss[0m : 2.18961
[1mStep[0m  [8/21], [94mLoss[0m : 2.00054
[1mStep[0m  [10/21], [94mLoss[0m : 2.06401
[1mStep[0m  [12/21], [94mLoss[0m : 2.04619
[1mStep[0m  [14/21], [94mLoss[0m : 2.20745
[1mStep[0m  [16/21], [94mLoss[0m : 2.26805
[1mStep[0m  [18/21], [94mLoss[0m : 2.16181
[1mStep[0m  [20/21], [94mLoss[0m : 2.05747

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.164, [92mTest[0m: 2.403, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.29349
[1mStep[0m  [2/21], [94mLoss[0m : 2.03869
[1mStep[0m  [4/21], [94mLoss[0m : 2.02750
[1mStep[0m  [6/21], [94mLoss[0m : 2.03151
[1mStep[0m  [8/21], [94mLoss[0m : 2.19232
[1mStep[0m  [10/21], [94mLoss[0m : 2.11263
[1mStep[0m  [12/21], [94mLoss[0m : 2.16625
[1mStep[0m  [14/21], [94mLoss[0m : 2.01348
[1mStep[0m  [16/21], [94mLoss[0m : 2.15554
[1mStep[0m  [18/21], [94mLoss[0m : 2.28866
[1mStep[0m  [20/21], [94mLoss[0m : 2.19020

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.163, [92mTest[0m: 2.417, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.11263
[1mStep[0m  [2/21], [94mLoss[0m : 2.24641
[1mStep[0m  [4/21], [94mLoss[0m : 2.11159
[1mStep[0m  [6/21], [94mLoss[0m : 2.20413
[1mStep[0m  [8/21], [94mLoss[0m : 2.19498
[1mStep[0m  [10/21], [94mLoss[0m : 2.20808
[1mStep[0m  [12/21], [94mLoss[0m : 2.07785
[1mStep[0m  [14/21], [94mLoss[0m : 2.18871
[1mStep[0m  [16/21], [94mLoss[0m : 2.07782
[1mStep[0m  [18/21], [94mLoss[0m : 2.07948
[1mStep[0m  [20/21], [94mLoss[0m : 2.06955

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.148, [92mTest[0m: 2.417, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.14695
[1mStep[0m  [2/21], [94mLoss[0m : 2.15622
[1mStep[0m  [4/21], [94mLoss[0m : 2.18353
[1mStep[0m  [6/21], [94mLoss[0m : 2.10677
[1mStep[0m  [8/21], [94mLoss[0m : 2.10085
[1mStep[0m  [10/21], [94mLoss[0m : 2.19058
[1mStep[0m  [12/21], [94mLoss[0m : 2.07967
[1mStep[0m  [14/21], [94mLoss[0m : 2.10126
[1mStep[0m  [16/21], [94mLoss[0m : 2.10848
[1mStep[0m  [18/21], [94mLoss[0m : 2.14474
[1mStep[0m  [20/21], [94mLoss[0m : 2.14590

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.135, [92mTest[0m: 2.418, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.12017
[1mStep[0m  [2/21], [94mLoss[0m : 2.01196
[1mStep[0m  [4/21], [94mLoss[0m : 2.14031
[1mStep[0m  [6/21], [94mLoss[0m : 2.19948
[1mStep[0m  [8/21], [94mLoss[0m : 2.03003
[1mStep[0m  [10/21], [94mLoss[0m : 2.13451
[1mStep[0m  [12/21], [94mLoss[0m : 2.10485
[1mStep[0m  [14/21], [94mLoss[0m : 2.05671
[1mStep[0m  [16/21], [94mLoss[0m : 2.08956
[1mStep[0m  [18/21], [94mLoss[0m : 2.14103
[1mStep[0m  [20/21], [94mLoss[0m : 2.01450

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.103, [92mTest[0m: 2.433, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.17262
[1mStep[0m  [2/21], [94mLoss[0m : 2.02577
[1mStep[0m  [4/21], [94mLoss[0m : 2.13272
[1mStep[0m  [6/21], [94mLoss[0m : 2.06326
[1mStep[0m  [8/21], [94mLoss[0m : 2.06427
[1mStep[0m  [10/21], [94mLoss[0m : 2.08622
[1mStep[0m  [12/21], [94mLoss[0m : 2.04329
[1mStep[0m  [14/21], [94mLoss[0m : 2.09410
[1mStep[0m  [16/21], [94mLoss[0m : 2.16934
[1mStep[0m  [18/21], [94mLoss[0m : 2.18768
[1mStep[0m  [20/21], [94mLoss[0m : 2.07737

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.100, [92mTest[0m: 2.412, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.429
====================================

Phase 2 - Evaluation MAE:  2.4292622974940707
MAE score P1      2.565486
MAE score P2      2.429262
loss               2.09992
learning_rate     0.002575
batch_size             512
hidden_sizes         [250]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.4
momentum               0.5
weight_decay          0.01
Name: 10, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 11.15201
[1mStep[0m  [2/21], [94mLoss[0m : 10.45656
[1mStep[0m  [4/21], [94mLoss[0m : 9.65147
[1mStep[0m  [6/21], [94mLoss[0m : 8.89382
[1mStep[0m  [8/21], [94mLoss[0m : 7.30263
[1mStep[0m  [10/21], [94mLoss[0m : 6.00142
[1mStep[0m  [12/21], [94mLoss[0m : 5.06005
[1mStep[0m  [14/21], [94mLoss[0m : 4.18322
[1mStep[0m  [16/21], [94mLoss[0m : 3.66308
[1mStep[0m  [18/21], [94mLoss[0m : 3.01896
[1mStep[0m  [20/21], [94mLoss[0m : 2.94184

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.589, [92mTest[0m: 10.987, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.78451
[1mStep[0m  [2/21], [94mLoss[0m : 3.18543
[1mStep[0m  [4/21], [94mLoss[0m : 3.28618
[1mStep[0m  [6/21], [94mLoss[0m : 3.36665
[1mStep[0m  [8/21], [94mLoss[0m : 3.07263
[1mStep[0m  [10/21], [94mLoss[0m : 2.96404
[1mStep[0m  [12/21], [94mLoss[0m : 2.77680
[1mStep[0m  [14/21], [94mLoss[0m : 2.73703
[1mStep[0m  [16/21], [94mLoss[0m : 2.57834
[1mStep[0m  [18/21], [94mLoss[0m : 2.78139
[1mStep[0m  [20/21], [94mLoss[0m : 2.60400

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.928, [92mTest[0m: 4.085, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56613
[1mStep[0m  [2/21], [94mLoss[0m : 2.74417
[1mStep[0m  [4/21], [94mLoss[0m : 2.59398
[1mStep[0m  [6/21], [94mLoss[0m : 2.51726
[1mStep[0m  [8/21], [94mLoss[0m : 2.46564
[1mStep[0m  [10/21], [94mLoss[0m : 2.54134
[1mStep[0m  [12/21], [94mLoss[0m : 2.45202
[1mStep[0m  [14/21], [94mLoss[0m : 2.57199
[1mStep[0m  [16/21], [94mLoss[0m : 2.55267
[1mStep[0m  [18/21], [94mLoss[0m : 2.67122
[1mStep[0m  [20/21], [94mLoss[0m : 2.34303

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.553, [92mTest[0m: 3.488, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.50909
[1mStep[0m  [2/21], [94mLoss[0m : 2.55971
[1mStep[0m  [4/21], [94mLoss[0m : 2.62310
[1mStep[0m  [6/21], [94mLoss[0m : 2.66080
[1mStep[0m  [8/21], [94mLoss[0m : 2.56395
[1mStep[0m  [10/21], [94mLoss[0m : 2.42585
[1mStep[0m  [12/21], [94mLoss[0m : 2.44332
[1mStep[0m  [14/21], [94mLoss[0m : 2.55685
[1mStep[0m  [16/21], [94mLoss[0m : 2.51895
[1mStep[0m  [18/21], [94mLoss[0m : 2.51284
[1mStep[0m  [20/21], [94mLoss[0m : 2.44692

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.529, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54379
[1mStep[0m  [2/21], [94mLoss[0m : 2.54506
[1mStep[0m  [4/21], [94mLoss[0m : 2.47728
[1mStep[0m  [6/21], [94mLoss[0m : 2.40521
[1mStep[0m  [8/21], [94mLoss[0m : 2.68482
[1mStep[0m  [10/21], [94mLoss[0m : 2.64817
[1mStep[0m  [12/21], [94mLoss[0m : 2.45596
[1mStep[0m  [14/21], [94mLoss[0m : 2.47319
[1mStep[0m  [16/21], [94mLoss[0m : 2.45083
[1mStep[0m  [18/21], [94mLoss[0m : 2.42110
[1mStep[0m  [20/21], [94mLoss[0m : 2.45657

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.636, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.57118
[1mStep[0m  [2/21], [94mLoss[0m : 2.38448
[1mStep[0m  [4/21], [94mLoss[0m : 2.44668
[1mStep[0m  [6/21], [94mLoss[0m : 2.49042
[1mStep[0m  [8/21], [94mLoss[0m : 2.42276
[1mStep[0m  [10/21], [94mLoss[0m : 2.55758
[1mStep[0m  [12/21], [94mLoss[0m : 2.42695
[1mStep[0m  [14/21], [94mLoss[0m : 2.41904
[1mStep[0m  [16/21], [94mLoss[0m : 2.45842
[1mStep[0m  [18/21], [94mLoss[0m : 2.29815
[1mStep[0m  [20/21], [94mLoss[0m : 2.41689

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.531, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56900
[1mStep[0m  [2/21], [94mLoss[0m : 2.41263
[1mStep[0m  [4/21], [94mLoss[0m : 2.32929
[1mStep[0m  [6/21], [94mLoss[0m : 2.42248
[1mStep[0m  [8/21], [94mLoss[0m : 2.44541
[1mStep[0m  [10/21], [94mLoss[0m : 2.39841
[1mStep[0m  [12/21], [94mLoss[0m : 2.46893
[1mStep[0m  [14/21], [94mLoss[0m : 2.31962
[1mStep[0m  [16/21], [94mLoss[0m : 2.42628
[1mStep[0m  [18/21], [94mLoss[0m : 2.48685
[1mStep[0m  [20/21], [94mLoss[0m : 2.35447

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.469, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.47879
[1mStep[0m  [2/21], [94mLoss[0m : 2.35646
[1mStep[0m  [4/21], [94mLoss[0m : 2.35938
[1mStep[0m  [6/21], [94mLoss[0m : 2.42653
[1mStep[0m  [8/21], [94mLoss[0m : 2.50993
[1mStep[0m  [10/21], [94mLoss[0m : 2.42212
[1mStep[0m  [12/21], [94mLoss[0m : 2.49806
[1mStep[0m  [14/21], [94mLoss[0m : 2.24271
[1mStep[0m  [16/21], [94mLoss[0m : 2.44795
[1mStep[0m  [18/21], [94mLoss[0m : 2.45176
[1mStep[0m  [20/21], [94mLoss[0m : 2.39219

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.598, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.38799
[1mStep[0m  [2/21], [94mLoss[0m : 2.36841
[1mStep[0m  [4/21], [94mLoss[0m : 2.43894
[1mStep[0m  [6/21], [94mLoss[0m : 2.43630
[1mStep[0m  [8/21], [94mLoss[0m : 2.34734
[1mStep[0m  [10/21], [94mLoss[0m : 2.46759
[1mStep[0m  [12/21], [94mLoss[0m : 2.40760
[1mStep[0m  [14/21], [94mLoss[0m : 2.38913
[1mStep[0m  [16/21], [94mLoss[0m : 2.46020
[1mStep[0m  [18/21], [94mLoss[0m : 2.35536
[1mStep[0m  [20/21], [94mLoss[0m : 2.32089

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.518, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42165
[1mStep[0m  [2/21], [94mLoss[0m : 2.44665
[1mStep[0m  [4/21], [94mLoss[0m : 2.40941
[1mStep[0m  [6/21], [94mLoss[0m : 2.42225
[1mStep[0m  [8/21], [94mLoss[0m : 2.33690
[1mStep[0m  [10/21], [94mLoss[0m : 2.29884
[1mStep[0m  [12/21], [94mLoss[0m : 2.48144
[1mStep[0m  [14/21], [94mLoss[0m : 2.51677
[1mStep[0m  [16/21], [94mLoss[0m : 2.49859
[1mStep[0m  [18/21], [94mLoss[0m : 2.40338
[1mStep[0m  [20/21], [94mLoss[0m : 2.42940

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.496, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.33327
[1mStep[0m  [2/21], [94mLoss[0m : 2.31196
[1mStep[0m  [4/21], [94mLoss[0m : 2.42835
[1mStep[0m  [6/21], [94mLoss[0m : 2.35980
[1mStep[0m  [8/21], [94mLoss[0m : 2.51104
[1mStep[0m  [10/21], [94mLoss[0m : 2.44442
[1mStep[0m  [12/21], [94mLoss[0m : 2.39373
[1mStep[0m  [14/21], [94mLoss[0m : 2.40186
[1mStep[0m  [16/21], [94mLoss[0m : 2.42754
[1mStep[0m  [18/21], [94mLoss[0m : 2.51685
[1mStep[0m  [20/21], [94mLoss[0m : 2.34317

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.390, [92mTest[0m: 2.472, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52841
[1mStep[0m  [2/21], [94mLoss[0m : 2.38569
[1mStep[0m  [4/21], [94mLoss[0m : 2.51050
[1mStep[0m  [6/21], [94mLoss[0m : 2.37494
[1mStep[0m  [8/21], [94mLoss[0m : 2.48408
[1mStep[0m  [10/21], [94mLoss[0m : 2.51976
[1mStep[0m  [12/21], [94mLoss[0m : 2.44707
[1mStep[0m  [14/21], [94mLoss[0m : 2.33188
[1mStep[0m  [16/21], [94mLoss[0m : 2.37292
[1mStep[0m  [18/21], [94mLoss[0m : 2.45044
[1mStep[0m  [20/21], [94mLoss[0m : 2.35980

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.511, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.35212
[1mStep[0m  [2/21], [94mLoss[0m : 2.34305
[1mStep[0m  [4/21], [94mLoss[0m : 2.31541
[1mStep[0m  [6/21], [94mLoss[0m : 2.42726
[1mStep[0m  [8/21], [94mLoss[0m : 2.39459
[1mStep[0m  [10/21], [94mLoss[0m : 2.41349
[1mStep[0m  [12/21], [94mLoss[0m : 2.45091
[1mStep[0m  [14/21], [94mLoss[0m : 2.45003
[1mStep[0m  [16/21], [94mLoss[0m : 2.19962
[1mStep[0m  [18/21], [94mLoss[0m : 2.47566
[1mStep[0m  [20/21], [94mLoss[0m : 2.47089

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.392, [92mTest[0m: 2.539, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.36315
[1mStep[0m  [2/21], [94mLoss[0m : 2.37069
[1mStep[0m  [4/21], [94mLoss[0m : 2.37473
[1mStep[0m  [6/21], [94mLoss[0m : 2.42786
[1mStep[0m  [8/21], [94mLoss[0m : 2.36103
[1mStep[0m  [10/21], [94mLoss[0m : 2.32193
[1mStep[0m  [12/21], [94mLoss[0m : 2.41333
[1mStep[0m  [14/21], [94mLoss[0m : 2.22227
[1mStep[0m  [16/21], [94mLoss[0m : 2.43619
[1mStep[0m  [18/21], [94mLoss[0m : 2.31307
[1mStep[0m  [20/21], [94mLoss[0m : 2.32382

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.373, [92mTest[0m: 2.513, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.32458
[1mStep[0m  [2/21], [94mLoss[0m : 2.33924
[1mStep[0m  [4/21], [94mLoss[0m : 2.33598
[1mStep[0m  [6/21], [94mLoss[0m : 2.46542
[1mStep[0m  [8/21], [94mLoss[0m : 2.39377
[1mStep[0m  [10/21], [94mLoss[0m : 2.46829
[1mStep[0m  [12/21], [94mLoss[0m : 2.25628
[1mStep[0m  [14/21], [94mLoss[0m : 2.42095
[1mStep[0m  [16/21], [94mLoss[0m : 2.39982
[1mStep[0m  [18/21], [94mLoss[0m : 2.46523
[1mStep[0m  [20/21], [94mLoss[0m : 2.35072

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.378, [92mTest[0m: 2.458, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.19386
[1mStep[0m  [2/21], [94mLoss[0m : 2.40688
[1mStep[0m  [4/21], [94mLoss[0m : 2.45657
[1mStep[0m  [6/21], [94mLoss[0m : 2.35920
[1mStep[0m  [8/21], [94mLoss[0m : 2.36411
[1mStep[0m  [10/21], [94mLoss[0m : 2.33430
[1mStep[0m  [12/21], [94mLoss[0m : 2.24064
[1mStep[0m  [14/21], [94mLoss[0m : 2.39099
[1mStep[0m  [16/21], [94mLoss[0m : 2.30266
[1mStep[0m  [18/21], [94mLoss[0m : 2.52512
[1mStep[0m  [20/21], [94mLoss[0m : 2.37346

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.365, [92mTest[0m: 2.454, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43178
[1mStep[0m  [2/21], [94mLoss[0m : 2.30416
[1mStep[0m  [4/21], [94mLoss[0m : 2.27427
[1mStep[0m  [6/21], [94mLoss[0m : 2.48592
[1mStep[0m  [8/21], [94mLoss[0m : 2.26725
[1mStep[0m  [10/21], [94mLoss[0m : 2.44007
[1mStep[0m  [12/21], [94mLoss[0m : 2.35547
[1mStep[0m  [14/21], [94mLoss[0m : 2.30879
[1mStep[0m  [16/21], [94mLoss[0m : 2.37031
[1mStep[0m  [18/21], [94mLoss[0m : 2.30199
[1mStep[0m  [20/21], [94mLoss[0m : 2.46480

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.368, [92mTest[0m: 2.466, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.27633
[1mStep[0m  [2/21], [94mLoss[0m : 2.41772
[1mStep[0m  [4/21], [94mLoss[0m : 2.27301
[1mStep[0m  [6/21], [94mLoss[0m : 2.36374
[1mStep[0m  [8/21], [94mLoss[0m : 2.34326
[1mStep[0m  [10/21], [94mLoss[0m : 2.21334
[1mStep[0m  [12/21], [94mLoss[0m : 2.39432
[1mStep[0m  [14/21], [94mLoss[0m : 2.42840
[1mStep[0m  [16/21], [94mLoss[0m : 2.30276
[1mStep[0m  [18/21], [94mLoss[0m : 2.46423
[1mStep[0m  [20/21], [94mLoss[0m : 2.42256

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.375, [92mTest[0m: 2.468, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.23107
[1mStep[0m  [2/21], [94mLoss[0m : 2.42851
[1mStep[0m  [4/21], [94mLoss[0m : 2.39935
[1mStep[0m  [6/21], [94mLoss[0m : 2.38033
[1mStep[0m  [8/21], [94mLoss[0m : 2.30534
[1mStep[0m  [10/21], [94mLoss[0m : 2.37192
[1mStep[0m  [12/21], [94mLoss[0m : 2.30709
[1mStep[0m  [14/21], [94mLoss[0m : 2.24873
[1mStep[0m  [16/21], [94mLoss[0m : 2.36713
[1mStep[0m  [18/21], [94mLoss[0m : 2.30004
[1mStep[0m  [20/21], [94mLoss[0m : 2.43911

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.369, [92mTest[0m: 2.445, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43259
[1mStep[0m  [2/21], [94mLoss[0m : 2.44014
[1mStep[0m  [4/21], [94mLoss[0m : 2.30888
[1mStep[0m  [6/21], [94mLoss[0m : 2.43637
[1mStep[0m  [8/21], [94mLoss[0m : 2.20367
[1mStep[0m  [10/21], [94mLoss[0m : 2.27762
[1mStep[0m  [12/21], [94mLoss[0m : 2.30392
[1mStep[0m  [14/21], [94mLoss[0m : 2.46373
[1mStep[0m  [16/21], [94mLoss[0m : 2.21878
[1mStep[0m  [18/21], [94mLoss[0m : 2.50604
[1mStep[0m  [20/21], [94mLoss[0m : 2.48028

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.348, [92mTest[0m: 2.448, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.29386
[1mStep[0m  [2/21], [94mLoss[0m : 2.49351
[1mStep[0m  [4/21], [94mLoss[0m : 2.33835
[1mStep[0m  [6/21], [94mLoss[0m : 2.29219
[1mStep[0m  [8/21], [94mLoss[0m : 2.32404
[1mStep[0m  [10/21], [94mLoss[0m : 2.22335
[1mStep[0m  [12/21], [94mLoss[0m : 2.26446
[1mStep[0m  [14/21], [94mLoss[0m : 2.37739
[1mStep[0m  [16/21], [94mLoss[0m : 2.35471
[1mStep[0m  [18/21], [94mLoss[0m : 2.55781
[1mStep[0m  [20/21], [94mLoss[0m : 2.44210

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.356, [92mTest[0m: 2.484, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.39424
[1mStep[0m  [2/21], [94mLoss[0m : 2.31833
[1mStep[0m  [4/21], [94mLoss[0m : 2.42038
[1mStep[0m  [6/21], [94mLoss[0m : 2.36255
[1mStep[0m  [8/21], [94mLoss[0m : 2.18772
[1mStep[0m  [10/21], [94mLoss[0m : 2.32964
[1mStep[0m  [12/21], [94mLoss[0m : 2.22686
[1mStep[0m  [14/21], [94mLoss[0m : 2.14590
[1mStep[0m  [16/21], [94mLoss[0m : 2.45439
[1mStep[0m  [18/21], [94mLoss[0m : 2.40066
[1mStep[0m  [20/21], [94mLoss[0m : 2.29614

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.356, [92mTest[0m: 2.435, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40194
[1mStep[0m  [2/21], [94mLoss[0m : 2.51103
[1mStep[0m  [4/21], [94mLoss[0m : 2.41213
[1mStep[0m  [6/21], [94mLoss[0m : 2.35650
[1mStep[0m  [8/21], [94mLoss[0m : 2.27925
[1mStep[0m  [10/21], [94mLoss[0m : 2.25627
[1mStep[0m  [12/21], [94mLoss[0m : 2.36097
[1mStep[0m  [14/21], [94mLoss[0m : 2.25658
[1mStep[0m  [16/21], [94mLoss[0m : 2.45801
[1mStep[0m  [18/21], [94mLoss[0m : 2.38670
[1mStep[0m  [20/21], [94mLoss[0m : 2.30262

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.332, [92mTest[0m: 2.459, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.32386
[1mStep[0m  [2/21], [94mLoss[0m : 2.33864
[1mStep[0m  [4/21], [94mLoss[0m : 2.41185
[1mStep[0m  [6/21], [94mLoss[0m : 2.37450
[1mStep[0m  [8/21], [94mLoss[0m : 2.28333
[1mStep[0m  [10/21], [94mLoss[0m : 2.32377
[1mStep[0m  [12/21], [94mLoss[0m : 2.39873
[1mStep[0m  [14/21], [94mLoss[0m : 2.30910
[1mStep[0m  [16/21], [94mLoss[0m : 2.57921
[1mStep[0m  [18/21], [94mLoss[0m : 2.26904
[1mStep[0m  [20/21], [94mLoss[0m : 2.39173

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.346, [92mTest[0m: 2.426, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.25385
[1mStep[0m  [2/21], [94mLoss[0m : 2.35611
[1mStep[0m  [4/21], [94mLoss[0m : 2.27638
[1mStep[0m  [6/21], [94mLoss[0m : 2.36918
[1mStep[0m  [8/21], [94mLoss[0m : 2.34087
[1mStep[0m  [10/21], [94mLoss[0m : 2.34274
[1mStep[0m  [12/21], [94mLoss[0m : 2.23890
[1mStep[0m  [14/21], [94mLoss[0m : 2.22236
[1mStep[0m  [16/21], [94mLoss[0m : 2.35608
[1mStep[0m  [18/21], [94mLoss[0m : 2.46942
[1mStep[0m  [20/21], [94mLoss[0m : 2.24549

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.332, [92mTest[0m: 2.433, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.37185
[1mStep[0m  [2/21], [94mLoss[0m : 2.37975
[1mStep[0m  [4/21], [94mLoss[0m : 2.45017
[1mStep[0m  [6/21], [94mLoss[0m : 2.26388
[1mStep[0m  [8/21], [94mLoss[0m : 2.40594
[1mStep[0m  [10/21], [94mLoss[0m : 2.40723
[1mStep[0m  [12/21], [94mLoss[0m : 2.31395
[1mStep[0m  [14/21], [94mLoss[0m : 2.21450
[1mStep[0m  [16/21], [94mLoss[0m : 2.42895
[1mStep[0m  [18/21], [94mLoss[0m : 2.39608
[1mStep[0m  [20/21], [94mLoss[0m : 2.39386

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.331, [92mTest[0m: 2.442, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.24795
[1mStep[0m  [2/21], [94mLoss[0m : 2.45970
[1mStep[0m  [4/21], [94mLoss[0m : 2.31993
[1mStep[0m  [6/21], [94mLoss[0m : 2.32758
[1mStep[0m  [8/21], [94mLoss[0m : 2.35082
[1mStep[0m  [10/21], [94mLoss[0m : 2.24199
[1mStep[0m  [12/21], [94mLoss[0m : 2.22370
[1mStep[0m  [14/21], [94mLoss[0m : 2.22370
[1mStep[0m  [16/21], [94mLoss[0m : 2.27147
[1mStep[0m  [18/21], [94mLoss[0m : 2.34754
[1mStep[0m  [20/21], [94mLoss[0m : 2.48536

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.327, [92mTest[0m: 2.440, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.37996
[1mStep[0m  [2/21], [94mLoss[0m : 2.18529
[1mStep[0m  [4/21], [94mLoss[0m : 2.28309
[1mStep[0m  [6/21], [94mLoss[0m : 2.35285
[1mStep[0m  [8/21], [94mLoss[0m : 2.35329
[1mStep[0m  [10/21], [94mLoss[0m : 2.20970
[1mStep[0m  [12/21], [94mLoss[0m : 2.30806
[1mStep[0m  [14/21], [94mLoss[0m : 2.44984
[1mStep[0m  [16/21], [94mLoss[0m : 2.45599
[1mStep[0m  [18/21], [94mLoss[0m : 2.30023
[1mStep[0m  [20/21], [94mLoss[0m : 2.28779

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.333, [92mTest[0m: 2.414, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.28293
[1mStep[0m  [2/21], [94mLoss[0m : 2.27728
[1mStep[0m  [4/21], [94mLoss[0m : 2.35191
[1mStep[0m  [6/21], [94mLoss[0m : 2.26916
[1mStep[0m  [8/21], [94mLoss[0m : 2.28494
[1mStep[0m  [10/21], [94mLoss[0m : 2.27308
[1mStep[0m  [12/21], [94mLoss[0m : 2.29472
[1mStep[0m  [14/21], [94mLoss[0m : 2.27880
[1mStep[0m  [16/21], [94mLoss[0m : 2.27167
[1mStep[0m  [18/21], [94mLoss[0m : 2.35490
[1mStep[0m  [20/21], [94mLoss[0m : 2.31958

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.316, [92mTest[0m: 2.443, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.23247
[1mStep[0m  [2/21], [94mLoss[0m : 2.35244
[1mStep[0m  [4/21], [94mLoss[0m : 2.12147
[1mStep[0m  [6/21], [94mLoss[0m : 2.44763
[1mStep[0m  [8/21], [94mLoss[0m : 2.47008
[1mStep[0m  [10/21], [94mLoss[0m : 2.34283
[1mStep[0m  [12/21], [94mLoss[0m : 2.39492
[1mStep[0m  [14/21], [94mLoss[0m : 2.37680
[1mStep[0m  [16/21], [94mLoss[0m : 2.44060
[1mStep[0m  [18/21], [94mLoss[0m : 2.36592
[1mStep[0m  [20/21], [94mLoss[0m : 2.29336

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.326, [92mTest[0m: 2.445, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.419
====================================

Phase 1 - Evaluation MAE:  2.419005904878889
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 2.30855
[1mStep[0m  [2/21], [94mLoss[0m : 2.29341
[1mStep[0m  [4/21], [94mLoss[0m : 2.25122
[1mStep[0m  [6/21], [94mLoss[0m : 2.47084
[1mStep[0m  [8/21], [94mLoss[0m : 2.41214
[1mStep[0m  [10/21], [94mLoss[0m : 2.33323
[1mStep[0m  [12/21], [94mLoss[0m : 2.37982
[1mStep[0m  [14/21], [94mLoss[0m : 2.52974
[1mStep[0m  [16/21], [94mLoss[0m : 2.47249
[1mStep[0m  [18/21], [94mLoss[0m : 2.43539
[1mStep[0m  [20/21], [94mLoss[0m : 2.26072

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.389, [92mTest[0m: 2.417, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.30673
[1mStep[0m  [2/21], [94mLoss[0m : 2.45534
[1mStep[0m  [4/21], [94mLoss[0m : 2.41985
[1mStep[0m  [6/21], [94mLoss[0m : 2.39106
[1mStep[0m  [8/21], [94mLoss[0m : 2.28262
[1mStep[0m  [10/21], [94mLoss[0m : 2.49068
[1mStep[0m  [12/21], [94mLoss[0m : 2.24932
[1mStep[0m  [14/21], [94mLoss[0m : 2.51298
[1mStep[0m  [16/21], [94mLoss[0m : 2.56138
[1mStep[0m  [18/21], [94mLoss[0m : 2.33846
[1mStep[0m  [20/21], [94mLoss[0m : 2.34946

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.356, [92mTest[0m: 3.155, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.18069
[1mStep[0m  [2/21], [94mLoss[0m : 2.37547
[1mStep[0m  [4/21], [94mLoss[0m : 2.31758
[1mStep[0m  [6/21], [94mLoss[0m : 2.19459
[1mStep[0m  [8/21], [94mLoss[0m : 2.18486
[1mStep[0m  [10/21], [94mLoss[0m : 2.34762
[1mStep[0m  [12/21], [94mLoss[0m : 2.36989
[1mStep[0m  [14/21], [94mLoss[0m : 2.25003
[1mStep[0m  [16/21], [94mLoss[0m : 2.25457
[1mStep[0m  [18/21], [94mLoss[0m : 2.23731
[1mStep[0m  [20/21], [94mLoss[0m : 2.28930

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.280, [92mTest[0m: 2.380, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.29331
[1mStep[0m  [2/21], [94mLoss[0m : 2.35215
[1mStep[0m  [4/21], [94mLoss[0m : 2.04578
[1mStep[0m  [6/21], [94mLoss[0m : 2.19944
[1mStep[0m  [8/21], [94mLoss[0m : 2.08696
[1mStep[0m  [10/21], [94mLoss[0m : 2.20831
[1mStep[0m  [12/21], [94mLoss[0m : 2.16266
[1mStep[0m  [14/21], [94mLoss[0m : 2.30458
[1mStep[0m  [16/21], [94mLoss[0m : 2.12422
[1mStep[0m  [18/21], [94mLoss[0m : 2.26824
[1mStep[0m  [20/21], [94mLoss[0m : 2.16579

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.222, [92mTest[0m: 2.784, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.15427
[1mStep[0m  [2/21], [94mLoss[0m : 2.10292
[1mStep[0m  [4/21], [94mLoss[0m : 2.21648
[1mStep[0m  [6/21], [94mLoss[0m : 2.18737
[1mStep[0m  [8/21], [94mLoss[0m : 2.21597
[1mStep[0m  [10/21], [94mLoss[0m : 2.13217
[1mStep[0m  [12/21], [94mLoss[0m : 2.22682
[1mStep[0m  [14/21], [94mLoss[0m : 2.14597
[1mStep[0m  [16/21], [94mLoss[0m : 2.29739
[1mStep[0m  [18/21], [94mLoss[0m : 2.15116
[1mStep[0m  [20/21], [94mLoss[0m : 2.24389

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.162, [92mTest[0m: 2.559, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.07373
[1mStep[0m  [2/21], [94mLoss[0m : 2.00285
[1mStep[0m  [4/21], [94mLoss[0m : 1.97415
[1mStep[0m  [6/21], [94mLoss[0m : 2.11372
[1mStep[0m  [8/21], [94mLoss[0m : 2.10132
[1mStep[0m  [10/21], [94mLoss[0m : 2.11260
[1mStep[0m  [12/21], [94mLoss[0m : 2.05919
[1mStep[0m  [14/21], [94mLoss[0m : 2.05338
[1mStep[0m  [16/21], [94mLoss[0m : 2.23969
[1mStep[0m  [18/21], [94mLoss[0m : 2.17982
[1mStep[0m  [20/21], [94mLoss[0m : 2.14443

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.101, [92mTest[0m: 2.537, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.96817
[1mStep[0m  [2/21], [94mLoss[0m : 2.09071
[1mStep[0m  [4/21], [94mLoss[0m : 2.02128
[1mStep[0m  [6/21], [94mLoss[0m : 2.09607
[1mStep[0m  [8/21], [94mLoss[0m : 2.12257
[1mStep[0m  [10/21], [94mLoss[0m : 2.11200
[1mStep[0m  [12/21], [94mLoss[0m : 2.03250
[1mStep[0m  [14/21], [94mLoss[0m : 2.09533
[1mStep[0m  [16/21], [94mLoss[0m : 2.01368
[1mStep[0m  [18/21], [94mLoss[0m : 2.10942
[1mStep[0m  [20/21], [94mLoss[0m : 2.13605

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.048, [92mTest[0m: 2.438, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.98789
[1mStep[0m  [2/21], [94mLoss[0m : 2.09751
[1mStep[0m  [4/21], [94mLoss[0m : 1.97699
[1mStep[0m  [6/21], [94mLoss[0m : 1.93325
[1mStep[0m  [8/21], [94mLoss[0m : 1.99935
[1mStep[0m  [10/21], [94mLoss[0m : 2.03027
[1mStep[0m  [12/21], [94mLoss[0m : 1.95996
[1mStep[0m  [14/21], [94mLoss[0m : 2.14458
[1mStep[0m  [16/21], [94mLoss[0m : 1.89846
[1mStep[0m  [18/21], [94mLoss[0m : 2.04836
[1mStep[0m  [20/21], [94mLoss[0m : 2.07814

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.999, [92mTest[0m: 2.413, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.92222
[1mStep[0m  [2/21], [94mLoss[0m : 1.84697
[1mStep[0m  [4/21], [94mLoss[0m : 2.09713
[1mStep[0m  [6/21], [94mLoss[0m : 2.00712
[1mStep[0m  [8/21], [94mLoss[0m : 1.99865
[1mStep[0m  [10/21], [94mLoss[0m : 1.84493
[1mStep[0m  [12/21], [94mLoss[0m : 1.85533
[1mStep[0m  [14/21], [94mLoss[0m : 1.88704
[1mStep[0m  [16/21], [94mLoss[0m : 1.94401
[1mStep[0m  [18/21], [94mLoss[0m : 1.84392
[1mStep[0m  [20/21], [94mLoss[0m : 1.93551

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.939, [92mTest[0m: 2.437, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.93137
[1mStep[0m  [2/21], [94mLoss[0m : 1.78864
[1mStep[0m  [4/21], [94mLoss[0m : 1.89330
[1mStep[0m  [6/21], [94mLoss[0m : 2.01568
[1mStep[0m  [8/21], [94mLoss[0m : 1.78221
[1mStep[0m  [10/21], [94mLoss[0m : 1.83832
[1mStep[0m  [12/21], [94mLoss[0m : 1.85253
[1mStep[0m  [14/21], [94mLoss[0m : 1.98903
[1mStep[0m  [16/21], [94mLoss[0m : 1.83424
[1mStep[0m  [18/21], [94mLoss[0m : 1.93358
[1mStep[0m  [20/21], [94mLoss[0m : 2.02240

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.893, [92mTest[0m: 2.450, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.94777
[1mStep[0m  [2/21], [94mLoss[0m : 1.90818
[1mStep[0m  [4/21], [94mLoss[0m : 1.78297
[1mStep[0m  [6/21], [94mLoss[0m : 1.74549
[1mStep[0m  [8/21], [94mLoss[0m : 1.87300
[1mStep[0m  [10/21], [94mLoss[0m : 1.82043
[1mStep[0m  [12/21], [94mLoss[0m : 1.85267
[1mStep[0m  [14/21], [94mLoss[0m : 1.88023
[1mStep[0m  [16/21], [94mLoss[0m : 1.90850
[1mStep[0m  [18/21], [94mLoss[0m : 1.76711
[1mStep[0m  [20/21], [94mLoss[0m : 1.82633

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.834, [92mTest[0m: 2.529, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.75858
[1mStep[0m  [2/21], [94mLoss[0m : 1.79934
[1mStep[0m  [4/21], [94mLoss[0m : 1.71727
[1mStep[0m  [6/21], [94mLoss[0m : 1.60389
[1mStep[0m  [8/21], [94mLoss[0m : 1.85386
[1mStep[0m  [10/21], [94mLoss[0m : 1.77061
[1mStep[0m  [12/21], [94mLoss[0m : 1.82736
[1mStep[0m  [14/21], [94mLoss[0m : 1.77316
[1mStep[0m  [16/21], [94mLoss[0m : 1.82607
[1mStep[0m  [18/21], [94mLoss[0m : 1.68248
[1mStep[0m  [20/21], [94mLoss[0m : 1.76283

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.791, [92mTest[0m: 2.440, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.57594
[1mStep[0m  [2/21], [94mLoss[0m : 1.75035
[1mStep[0m  [4/21], [94mLoss[0m : 1.83603
[1mStep[0m  [6/21], [94mLoss[0m : 1.75139
[1mStep[0m  [8/21], [94mLoss[0m : 1.68370
[1mStep[0m  [10/21], [94mLoss[0m : 1.76242
[1mStep[0m  [12/21], [94mLoss[0m : 1.91909
[1mStep[0m  [14/21], [94mLoss[0m : 1.73178
[1mStep[0m  [16/21], [94mLoss[0m : 1.75013
[1mStep[0m  [18/21], [94mLoss[0m : 1.90158
[1mStep[0m  [20/21], [94mLoss[0m : 1.73479

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.747, [92mTest[0m: 2.437, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.69504
[1mStep[0m  [2/21], [94mLoss[0m : 1.60744
[1mStep[0m  [4/21], [94mLoss[0m : 1.70286
[1mStep[0m  [6/21], [94mLoss[0m : 1.62044
[1mStep[0m  [8/21], [94mLoss[0m : 1.69605
[1mStep[0m  [10/21], [94mLoss[0m : 1.63788
[1mStep[0m  [12/21], [94mLoss[0m : 1.74513
[1mStep[0m  [14/21], [94mLoss[0m : 1.65344
[1mStep[0m  [16/21], [94mLoss[0m : 1.72048
[1mStep[0m  [18/21], [94mLoss[0m : 1.77046
[1mStep[0m  [20/21], [94mLoss[0m : 1.68677

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.702, [92mTest[0m: 2.502, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.78478
[1mStep[0m  [2/21], [94mLoss[0m : 1.64593
[1mStep[0m  [4/21], [94mLoss[0m : 1.63275
[1mStep[0m  [6/21], [94mLoss[0m : 1.60169
[1mStep[0m  [8/21], [94mLoss[0m : 1.68905
[1mStep[0m  [10/21], [94mLoss[0m : 1.52921
[1mStep[0m  [12/21], [94mLoss[0m : 1.76696
[1mStep[0m  [14/21], [94mLoss[0m : 1.67607
[1mStep[0m  [16/21], [94mLoss[0m : 1.66639
[1mStep[0m  [18/21], [94mLoss[0m : 1.72081
[1mStep[0m  [20/21], [94mLoss[0m : 1.56725

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.660, [92mTest[0m: 2.470, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.59797
[1mStep[0m  [2/21], [94mLoss[0m : 1.53782
[1mStep[0m  [4/21], [94mLoss[0m : 1.50118
[1mStep[0m  [6/21], [94mLoss[0m : 1.61870
[1mStep[0m  [8/21], [94mLoss[0m : 1.67917
[1mStep[0m  [10/21], [94mLoss[0m : 1.84517
[1mStep[0m  [12/21], [94mLoss[0m : 1.59427
[1mStep[0m  [14/21], [94mLoss[0m : 1.49145
[1mStep[0m  [16/21], [94mLoss[0m : 1.68545
[1mStep[0m  [18/21], [94mLoss[0m : 1.62501
[1mStep[0m  [20/21], [94mLoss[0m : 1.57241

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.624, [92mTest[0m: 2.504, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.52471
[1mStep[0m  [2/21], [94mLoss[0m : 1.52555
[1mStep[0m  [4/21], [94mLoss[0m : 1.54446
[1mStep[0m  [6/21], [94mLoss[0m : 1.68634
[1mStep[0m  [8/21], [94mLoss[0m : 1.56994
[1mStep[0m  [10/21], [94mLoss[0m : 1.57977
[1mStep[0m  [12/21], [94mLoss[0m : 1.73554
[1mStep[0m  [14/21], [94mLoss[0m : 1.56915
[1mStep[0m  [16/21], [94mLoss[0m : 1.54278
[1mStep[0m  [18/21], [94mLoss[0m : 1.60369
[1mStep[0m  [20/21], [94mLoss[0m : 1.63091

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.579, [92mTest[0m: 2.603, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.49806
[1mStep[0m  [2/21], [94mLoss[0m : 1.42846
[1mStep[0m  [4/21], [94mLoss[0m : 1.54780
[1mStep[0m  [6/21], [94mLoss[0m : 1.51695
[1mStep[0m  [8/21], [94mLoss[0m : 1.48662
[1mStep[0m  [10/21], [94mLoss[0m : 1.73491
[1mStep[0m  [12/21], [94mLoss[0m : 1.52560
[1mStep[0m  [14/21], [94mLoss[0m : 1.63351
[1mStep[0m  [16/21], [94mLoss[0m : 1.50173
[1mStep[0m  [18/21], [94mLoss[0m : 1.58385
[1mStep[0m  [20/21], [94mLoss[0m : 1.60073

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.526, [92mTest[0m: 2.538, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.46957
[1mStep[0m  [2/21], [94mLoss[0m : 1.48197
[1mStep[0m  [4/21], [94mLoss[0m : 1.50134
[1mStep[0m  [6/21], [94mLoss[0m : 1.57721
[1mStep[0m  [8/21], [94mLoss[0m : 1.44941
[1mStep[0m  [10/21], [94mLoss[0m : 1.62205
[1mStep[0m  [12/21], [94mLoss[0m : 1.50142
[1mStep[0m  [14/21], [94mLoss[0m : 1.51876
[1mStep[0m  [16/21], [94mLoss[0m : 1.50669
[1mStep[0m  [18/21], [94mLoss[0m : 1.44926
[1mStep[0m  [20/21], [94mLoss[0m : 1.40144

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.508, [92mTest[0m: 2.502, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.43099
[1mStep[0m  [2/21], [94mLoss[0m : 1.54459
[1mStep[0m  [4/21], [94mLoss[0m : 1.64489
[1mStep[0m  [6/21], [94mLoss[0m : 1.47012
[1mStep[0m  [8/21], [94mLoss[0m : 1.47800
[1mStep[0m  [10/21], [94mLoss[0m : 1.38669
[1mStep[0m  [12/21], [94mLoss[0m : 1.45967
[1mStep[0m  [14/21], [94mLoss[0m : 1.55833
[1mStep[0m  [16/21], [94mLoss[0m : 1.57295
[1mStep[0m  [18/21], [94mLoss[0m : 1.48211
[1mStep[0m  [20/21], [94mLoss[0m : 1.59032

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.496, [92mTest[0m: 2.520, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.39940
[1mStep[0m  [2/21], [94mLoss[0m : 1.40577
[1mStep[0m  [4/21], [94mLoss[0m : 1.52172
[1mStep[0m  [6/21], [94mLoss[0m : 1.41123
[1mStep[0m  [8/21], [94mLoss[0m : 1.31682
[1mStep[0m  [10/21], [94mLoss[0m : 1.52465
[1mStep[0m  [12/21], [94mLoss[0m : 1.52326
[1mStep[0m  [14/21], [94mLoss[0m : 1.47388
[1mStep[0m  [16/21], [94mLoss[0m : 1.46804
[1mStep[0m  [18/21], [94mLoss[0m : 1.39026
[1mStep[0m  [20/21], [94mLoss[0m : 1.43369

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.463, [92mTest[0m: 2.525, [96mlr[0m: 0.0023175
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 20 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.578
====================================

Phase 2 - Evaluation MAE:  2.5779317787715366
MAE score P1      2.419006
MAE score P2      2.577932
loss              1.463078
learning_rate     0.002575
batch_size             512
hidden_sizes         [300]
epochs                  30
activation            relu
optimizer              sgd
early stopping        True
dropout                0.2
momentum               0.9
weight_decay        0.0001
Name: 11, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 11.09701
[1mStep[0m  [4/42], [94mLoss[0m : 10.69456
[1mStep[0m  [8/42], [94mLoss[0m : 10.87298
[1mStep[0m  [12/42], [94mLoss[0m : 11.11567
[1mStep[0m  [16/42], [94mLoss[0m : 11.23005
[1mStep[0m  [20/42], [94mLoss[0m : 10.95242
[1mStep[0m  [24/42], [94mLoss[0m : 10.95306
[1mStep[0m  [28/42], [94mLoss[0m : 10.59007
[1mStep[0m  [32/42], [94mLoss[0m : 10.51703
[1mStep[0m  [36/42], [94mLoss[0m : 10.58872
[1mStep[0m  [40/42], [94mLoss[0m : 10.85741

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.844, [92mTest[0m: 11.026, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.63335
[1mStep[0m  [4/42], [94mLoss[0m : 10.62002
[1mStep[0m  [8/42], [94mLoss[0m : 10.69105
[1mStep[0m  [12/42], [94mLoss[0m : 10.54993
[1mStep[0m  [16/42], [94mLoss[0m : 10.66032
[1mStep[0m  [20/42], [94mLoss[0m : 11.06962
[1mStep[0m  [24/42], [94mLoss[0m : 10.59397
[1mStep[0m  [28/42], [94mLoss[0m : 10.65262
[1mStep[0m  [32/42], [94mLoss[0m : 10.63761
[1mStep[0m  [36/42], [94mLoss[0m : 10.66128
[1mStep[0m  [40/42], [94mLoss[0m : 10.24313

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.628, [92mTest[0m: 10.697, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.49109
[1mStep[0m  [4/42], [94mLoss[0m : 10.71266
[1mStep[0m  [8/42], [94mLoss[0m : 10.22780
[1mStep[0m  [12/42], [94mLoss[0m : 10.60724
[1mStep[0m  [16/42], [94mLoss[0m : 10.70287
[1mStep[0m  [20/42], [94mLoss[0m : 10.51635
[1mStep[0m  [24/42], [94mLoss[0m : 10.50244
[1mStep[0m  [28/42], [94mLoss[0m : 10.67510
[1mStep[0m  [32/42], [94mLoss[0m : 10.72045
[1mStep[0m  [36/42], [94mLoss[0m : 10.46247
[1mStep[0m  [40/42], [94mLoss[0m : 10.08098

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.424, [92mTest[0m: 10.389, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.12426
[1mStep[0m  [4/42], [94mLoss[0m : 10.47835
[1mStep[0m  [8/42], [94mLoss[0m : 10.06127
[1mStep[0m  [12/42], [94mLoss[0m : 10.02286
[1mStep[0m  [16/42], [94mLoss[0m : 10.13190
[1mStep[0m  [20/42], [94mLoss[0m : 10.28329
[1mStep[0m  [24/42], [94mLoss[0m : 10.07300
[1mStep[0m  [28/42], [94mLoss[0m : 9.79279
[1mStep[0m  [32/42], [94mLoss[0m : 10.29441
[1mStep[0m  [36/42], [94mLoss[0m : 9.89521
[1mStep[0m  [40/42], [94mLoss[0m : 9.86333

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.214, [92mTest[0m: 10.128, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.37160
[1mStep[0m  [4/42], [94mLoss[0m : 9.80232
[1mStep[0m  [8/42], [94mLoss[0m : 10.07066
[1mStep[0m  [12/42], [94mLoss[0m : 10.34561
[1mStep[0m  [16/42], [94mLoss[0m : 10.43488
[1mStep[0m  [20/42], [94mLoss[0m : 10.01475
[1mStep[0m  [24/42], [94mLoss[0m : 9.73144
[1mStep[0m  [28/42], [94mLoss[0m : 9.68236
[1mStep[0m  [32/42], [94mLoss[0m : 9.78664
[1mStep[0m  [36/42], [94mLoss[0m : 9.44904
[1mStep[0m  [40/42], [94mLoss[0m : 10.15133

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.010, [92mTest[0m: 9.843, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.92532
[1mStep[0m  [4/42], [94mLoss[0m : 9.54439
[1mStep[0m  [8/42], [94mLoss[0m : 9.60015
[1mStep[0m  [12/42], [94mLoss[0m : 9.75587
[1mStep[0m  [16/42], [94mLoss[0m : 9.94122
[1mStep[0m  [20/42], [94mLoss[0m : 9.80200
[1mStep[0m  [24/42], [94mLoss[0m : 9.54048
[1mStep[0m  [28/42], [94mLoss[0m : 10.14217
[1mStep[0m  [32/42], [94mLoss[0m : 10.14331
[1mStep[0m  [36/42], [94mLoss[0m : 9.90602
[1mStep[0m  [40/42], [94mLoss[0m : 9.41325

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 9.768, [92mTest[0m: 9.515, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.81505
[1mStep[0m  [4/42], [94mLoss[0m : 9.36656
[1mStep[0m  [8/42], [94mLoss[0m : 9.39196
[1mStep[0m  [12/42], [94mLoss[0m : 9.55799
[1mStep[0m  [16/42], [94mLoss[0m : 9.44978
[1mStep[0m  [20/42], [94mLoss[0m : 9.79921
[1mStep[0m  [24/42], [94mLoss[0m : 9.17224
[1mStep[0m  [28/42], [94mLoss[0m : 9.26567
[1mStep[0m  [32/42], [94mLoss[0m : 9.53654
[1mStep[0m  [36/42], [94mLoss[0m : 9.31084
[1mStep[0m  [40/42], [94mLoss[0m : 8.93460

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 9.496, [92mTest[0m: 9.161, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.44019
[1mStep[0m  [4/42], [94mLoss[0m : 9.42448
[1mStep[0m  [8/42], [94mLoss[0m : 9.57402
[1mStep[0m  [12/42], [94mLoss[0m : 9.37530
[1mStep[0m  [16/42], [94mLoss[0m : 8.96791
[1mStep[0m  [20/42], [94mLoss[0m : 9.24517
[1mStep[0m  [24/42], [94mLoss[0m : 8.94996
[1mStep[0m  [28/42], [94mLoss[0m : 8.98993
[1mStep[0m  [32/42], [94mLoss[0m : 8.93643
[1mStep[0m  [36/42], [94mLoss[0m : 8.93163
[1mStep[0m  [40/42], [94mLoss[0m : 9.41140

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 9.215, [92mTest[0m: 8.840, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.52896
[1mStep[0m  [4/42], [94mLoss[0m : 8.47078
[1mStep[0m  [8/42], [94mLoss[0m : 8.95919
[1mStep[0m  [12/42], [94mLoss[0m : 9.10150
[1mStep[0m  [16/42], [94mLoss[0m : 9.17431
[1mStep[0m  [20/42], [94mLoss[0m : 8.92996
[1mStep[0m  [24/42], [94mLoss[0m : 8.93133
[1mStep[0m  [28/42], [94mLoss[0m : 8.73317
[1mStep[0m  [32/42], [94mLoss[0m : 8.72548
[1mStep[0m  [36/42], [94mLoss[0m : 9.05267
[1mStep[0m  [40/42], [94mLoss[0m : 8.97357

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 8.918, [92mTest[0m: 8.365, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.35843
[1mStep[0m  [4/42], [94mLoss[0m : 9.16159
[1mStep[0m  [8/42], [94mLoss[0m : 8.78807
[1mStep[0m  [12/42], [94mLoss[0m : 8.53951
[1mStep[0m  [16/42], [94mLoss[0m : 8.20732
[1mStep[0m  [20/42], [94mLoss[0m : 8.50149
[1mStep[0m  [24/42], [94mLoss[0m : 8.57179
[1mStep[0m  [28/42], [94mLoss[0m : 8.68039
[1mStep[0m  [32/42], [94mLoss[0m : 8.35772
[1mStep[0m  [36/42], [94mLoss[0m : 8.21230
[1mStep[0m  [40/42], [94mLoss[0m : 8.61007

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 8.611, [92mTest[0m: 8.097, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.56554
[1mStep[0m  [4/42], [94mLoss[0m : 8.94007
[1mStep[0m  [8/42], [94mLoss[0m : 8.42555
[1mStep[0m  [12/42], [94mLoss[0m : 8.87245
[1mStep[0m  [16/42], [94mLoss[0m : 8.69747
[1mStep[0m  [20/42], [94mLoss[0m : 8.37996
[1mStep[0m  [24/42], [94mLoss[0m : 8.28269
[1mStep[0m  [28/42], [94mLoss[0m : 7.88734
[1mStep[0m  [32/42], [94mLoss[0m : 7.91380
[1mStep[0m  [36/42], [94mLoss[0m : 8.07195
[1mStep[0m  [40/42], [94mLoss[0m : 8.46294

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 8.310, [92mTest[0m: 7.645, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.35431
[1mStep[0m  [4/42], [94mLoss[0m : 7.98145
[1mStep[0m  [8/42], [94mLoss[0m : 8.38402
[1mStep[0m  [12/42], [94mLoss[0m : 8.36076
[1mStep[0m  [16/42], [94mLoss[0m : 8.13537
[1mStep[0m  [20/42], [94mLoss[0m : 8.30541
[1mStep[0m  [24/42], [94mLoss[0m : 7.93329
[1mStep[0m  [28/42], [94mLoss[0m : 7.97811
[1mStep[0m  [32/42], [94mLoss[0m : 8.20119
[1mStep[0m  [36/42], [94mLoss[0m : 7.69278
[1mStep[0m  [40/42], [94mLoss[0m : 7.90137

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 8.030, [92mTest[0m: 7.296, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.98731
[1mStep[0m  [4/42], [94mLoss[0m : 7.77649
[1mStep[0m  [8/42], [94mLoss[0m : 7.77575
[1mStep[0m  [12/42], [94mLoss[0m : 7.76559
[1mStep[0m  [16/42], [94mLoss[0m : 8.26350
[1mStep[0m  [20/42], [94mLoss[0m : 7.64228
[1mStep[0m  [24/42], [94mLoss[0m : 7.66326
[1mStep[0m  [28/42], [94mLoss[0m : 7.71415
[1mStep[0m  [32/42], [94mLoss[0m : 7.69485
[1mStep[0m  [36/42], [94mLoss[0m : 7.75915
[1mStep[0m  [40/42], [94mLoss[0m : 7.57199

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 7.754, [92mTest[0m: 6.887, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.86964
[1mStep[0m  [4/42], [94mLoss[0m : 7.92598
[1mStep[0m  [8/42], [94mLoss[0m : 7.42365
[1mStep[0m  [12/42], [94mLoss[0m : 7.72282
[1mStep[0m  [16/42], [94mLoss[0m : 7.51689
[1mStep[0m  [20/42], [94mLoss[0m : 7.81160
[1mStep[0m  [24/42], [94mLoss[0m : 7.38805
[1mStep[0m  [28/42], [94mLoss[0m : 7.74129
[1mStep[0m  [32/42], [94mLoss[0m : 7.25560
[1mStep[0m  [36/42], [94mLoss[0m : 7.58836
[1mStep[0m  [40/42], [94mLoss[0m : 7.22178

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 7.493, [92mTest[0m: 6.725, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.26105
[1mStep[0m  [4/42], [94mLoss[0m : 7.26714
[1mStep[0m  [8/42], [94mLoss[0m : 7.46498
[1mStep[0m  [12/42], [94mLoss[0m : 7.56683
[1mStep[0m  [16/42], [94mLoss[0m : 7.29297
[1mStep[0m  [20/42], [94mLoss[0m : 7.14821
[1mStep[0m  [24/42], [94mLoss[0m : 7.18393
[1mStep[0m  [28/42], [94mLoss[0m : 7.26682
[1mStep[0m  [32/42], [94mLoss[0m : 7.29601
[1mStep[0m  [36/42], [94mLoss[0m : 7.27436
[1mStep[0m  [40/42], [94mLoss[0m : 7.02613

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 7.244, [92mTest[0m: 6.528, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.17520
[1mStep[0m  [4/42], [94mLoss[0m : 7.32105
[1mStep[0m  [8/42], [94mLoss[0m : 7.25233
[1mStep[0m  [12/42], [94mLoss[0m : 7.08938
[1mStep[0m  [16/42], [94mLoss[0m : 6.99161
[1mStep[0m  [20/42], [94mLoss[0m : 6.75239
[1mStep[0m  [24/42], [94mLoss[0m : 6.82745
[1mStep[0m  [28/42], [94mLoss[0m : 6.73601
[1mStep[0m  [32/42], [94mLoss[0m : 6.91839
[1mStep[0m  [36/42], [94mLoss[0m : 7.02606
[1mStep[0m  [40/42], [94mLoss[0m : 6.83567

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 7.030, [92mTest[0m: 6.349, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.28491
[1mStep[0m  [4/42], [94mLoss[0m : 6.70541
[1mStep[0m  [8/42], [94mLoss[0m : 6.82187
[1mStep[0m  [12/42], [94mLoss[0m : 6.51646
[1mStep[0m  [16/42], [94mLoss[0m : 7.18013
[1mStep[0m  [20/42], [94mLoss[0m : 6.92427
[1mStep[0m  [24/42], [94mLoss[0m : 6.91339
[1mStep[0m  [28/42], [94mLoss[0m : 6.85703
[1mStep[0m  [32/42], [94mLoss[0m : 6.43194
[1mStep[0m  [36/42], [94mLoss[0m : 6.78502
[1mStep[0m  [40/42], [94mLoss[0m : 6.73202

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 6.796, [92mTest[0m: 6.074, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.69957
[1mStep[0m  [4/42], [94mLoss[0m : 6.56070
[1mStep[0m  [8/42], [94mLoss[0m : 6.61215
[1mStep[0m  [12/42], [94mLoss[0m : 6.72467
[1mStep[0m  [16/42], [94mLoss[0m : 6.77408
[1mStep[0m  [20/42], [94mLoss[0m : 6.20350
[1mStep[0m  [24/42], [94mLoss[0m : 6.33007
[1mStep[0m  [28/42], [94mLoss[0m : 6.68499
[1mStep[0m  [32/42], [94mLoss[0m : 6.56662
[1mStep[0m  [36/42], [94mLoss[0m : 6.32506
[1mStep[0m  [40/42], [94mLoss[0m : 6.87989

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 6.585, [92mTest[0m: 5.915, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.44112
[1mStep[0m  [4/42], [94mLoss[0m : 6.60139
[1mStep[0m  [8/42], [94mLoss[0m : 6.76211
[1mStep[0m  [12/42], [94mLoss[0m : 6.31366
[1mStep[0m  [16/42], [94mLoss[0m : 6.26997
[1mStep[0m  [20/42], [94mLoss[0m : 6.27393
[1mStep[0m  [24/42], [94mLoss[0m : 6.07314
[1mStep[0m  [28/42], [94mLoss[0m : 6.26868
[1mStep[0m  [32/42], [94mLoss[0m : 5.99876
[1mStep[0m  [36/42], [94mLoss[0m : 6.13398
[1mStep[0m  [40/42], [94mLoss[0m : 6.31419

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 6.371, [92mTest[0m: 5.641, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.73100
[1mStep[0m  [4/42], [94mLoss[0m : 6.33844
[1mStep[0m  [8/42], [94mLoss[0m : 6.46588
[1mStep[0m  [12/42], [94mLoss[0m : 6.16757
[1mStep[0m  [16/42], [94mLoss[0m : 6.42496
[1mStep[0m  [20/42], [94mLoss[0m : 6.17939
[1mStep[0m  [24/42], [94mLoss[0m : 5.72545
[1mStep[0m  [28/42], [94mLoss[0m : 6.01383
[1mStep[0m  [32/42], [94mLoss[0m : 6.06845
[1mStep[0m  [36/42], [94mLoss[0m : 6.09299
[1mStep[0m  [40/42], [94mLoss[0m : 6.05776

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 6.147, [92mTest[0m: 5.352, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.82107
[1mStep[0m  [4/42], [94mLoss[0m : 6.09295
[1mStep[0m  [8/42], [94mLoss[0m : 5.80124
[1mStep[0m  [12/42], [94mLoss[0m : 6.23837
[1mStep[0m  [16/42], [94mLoss[0m : 6.45290
[1mStep[0m  [20/42], [94mLoss[0m : 5.91548
[1mStep[0m  [24/42], [94mLoss[0m : 5.84557
[1mStep[0m  [28/42], [94mLoss[0m : 6.00616
[1mStep[0m  [32/42], [94mLoss[0m : 6.18341
[1mStep[0m  [36/42], [94mLoss[0m : 5.92770
[1mStep[0m  [40/42], [94mLoss[0m : 5.62255

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 5.916, [92mTest[0m: 5.084, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.50353
[1mStep[0m  [4/42], [94mLoss[0m : 6.07728
[1mStep[0m  [8/42], [94mLoss[0m : 5.89089
[1mStep[0m  [12/42], [94mLoss[0m : 5.52999
[1mStep[0m  [16/42], [94mLoss[0m : 5.84842
[1mStep[0m  [20/42], [94mLoss[0m : 5.46933
[1mStep[0m  [24/42], [94mLoss[0m : 5.61994
[1mStep[0m  [28/42], [94mLoss[0m : 5.85518
[1mStep[0m  [32/42], [94mLoss[0m : 5.63519
[1mStep[0m  [36/42], [94mLoss[0m : 5.49663
[1mStep[0m  [40/42], [94mLoss[0m : 5.54751

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 5.676, [92mTest[0m: 4.923, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.15878
[1mStep[0m  [4/42], [94mLoss[0m : 5.56414
[1mStep[0m  [8/42], [94mLoss[0m : 5.61940
[1mStep[0m  [12/42], [94mLoss[0m : 5.62612
[1mStep[0m  [16/42], [94mLoss[0m : 5.51597
[1mStep[0m  [20/42], [94mLoss[0m : 5.30081
[1mStep[0m  [24/42], [94mLoss[0m : 5.22866
[1mStep[0m  [28/42], [94mLoss[0m : 5.51095
[1mStep[0m  [32/42], [94mLoss[0m : 5.50555
[1mStep[0m  [36/42], [94mLoss[0m : 5.30893
[1mStep[0m  [40/42], [94mLoss[0m : 5.68032

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 5.501, [92mTest[0m: 4.734, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.55022
[1mStep[0m  [4/42], [94mLoss[0m : 5.46798
[1mStep[0m  [8/42], [94mLoss[0m : 5.35851
[1mStep[0m  [12/42], [94mLoss[0m : 5.08555
[1mStep[0m  [16/42], [94mLoss[0m : 5.30535
[1mStep[0m  [20/42], [94mLoss[0m : 4.96257
[1mStep[0m  [24/42], [94mLoss[0m : 5.06604
[1mStep[0m  [28/42], [94mLoss[0m : 5.09992
[1mStep[0m  [32/42], [94mLoss[0m : 5.24101
[1mStep[0m  [36/42], [94mLoss[0m : 4.80390
[1mStep[0m  [40/42], [94mLoss[0m : 5.36565

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 5.261, [92mTest[0m: 4.498, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.02135
[1mStep[0m  [4/42], [94mLoss[0m : 5.21236
[1mStep[0m  [8/42], [94mLoss[0m : 5.10737
[1mStep[0m  [12/42], [94mLoss[0m : 4.91586
[1mStep[0m  [16/42], [94mLoss[0m : 5.03398
[1mStep[0m  [20/42], [94mLoss[0m : 5.30468
[1mStep[0m  [24/42], [94mLoss[0m : 4.74876
[1mStep[0m  [28/42], [94mLoss[0m : 5.12052
[1mStep[0m  [32/42], [94mLoss[0m : 4.94647
[1mStep[0m  [36/42], [94mLoss[0m : 4.82624
[1mStep[0m  [40/42], [94mLoss[0m : 4.74881

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 5.027, [92mTest[0m: 4.306, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.29021
[1mStep[0m  [4/42], [94mLoss[0m : 5.04069
[1mStep[0m  [8/42], [94mLoss[0m : 4.31031
[1mStep[0m  [12/42], [94mLoss[0m : 4.66667
[1mStep[0m  [16/42], [94mLoss[0m : 4.89165
[1mStep[0m  [20/42], [94mLoss[0m : 4.81802
[1mStep[0m  [24/42], [94mLoss[0m : 4.85825
[1mStep[0m  [28/42], [94mLoss[0m : 5.10164
[1mStep[0m  [32/42], [94mLoss[0m : 4.79169
[1mStep[0m  [36/42], [94mLoss[0m : 4.71791
[1mStep[0m  [40/42], [94mLoss[0m : 4.70880

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 4.776, [92mTest[0m: 4.057, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.57460
[1mStep[0m  [4/42], [94mLoss[0m : 4.44950
[1mStep[0m  [8/42], [94mLoss[0m : 4.49686
[1mStep[0m  [12/42], [94mLoss[0m : 4.30112
[1mStep[0m  [16/42], [94mLoss[0m : 4.63457
[1mStep[0m  [20/42], [94mLoss[0m : 4.26109
[1mStep[0m  [24/42], [94mLoss[0m : 4.69507
[1mStep[0m  [28/42], [94mLoss[0m : 4.41230
[1mStep[0m  [32/42], [94mLoss[0m : 4.63331
[1mStep[0m  [36/42], [94mLoss[0m : 4.56045
[1mStep[0m  [40/42], [94mLoss[0m : 4.36753

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 4.527, [92mTest[0m: 3.850, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.13655
[1mStep[0m  [4/42], [94mLoss[0m : 4.24584
[1mStep[0m  [8/42], [94mLoss[0m : 4.52566
[1mStep[0m  [12/42], [94mLoss[0m : 4.29716
[1mStep[0m  [16/42], [94mLoss[0m : 3.95022
[1mStep[0m  [20/42], [94mLoss[0m : 4.78094
[1mStep[0m  [24/42], [94mLoss[0m : 4.01752
[1mStep[0m  [28/42], [94mLoss[0m : 4.01612
[1mStep[0m  [32/42], [94mLoss[0m : 4.01182
[1mStep[0m  [36/42], [94mLoss[0m : 4.47356
[1mStep[0m  [40/42], [94mLoss[0m : 4.39629

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 4.293, [92mTest[0m: 3.702, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.96989
[1mStep[0m  [4/42], [94mLoss[0m : 4.11318
[1mStep[0m  [8/42], [94mLoss[0m : 4.14421
[1mStep[0m  [12/42], [94mLoss[0m : 3.84725
[1mStep[0m  [16/42], [94mLoss[0m : 4.02895
[1mStep[0m  [20/42], [94mLoss[0m : 4.07295
[1mStep[0m  [24/42], [94mLoss[0m : 4.00220
[1mStep[0m  [28/42], [94mLoss[0m : 3.95226
[1mStep[0m  [32/42], [94mLoss[0m : 3.99541
[1mStep[0m  [36/42], [94mLoss[0m : 3.93573
[1mStep[0m  [40/42], [94mLoss[0m : 3.81780

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 4.025, [92mTest[0m: 3.409, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.98468
[1mStep[0m  [4/42], [94mLoss[0m : 3.65748
[1mStep[0m  [8/42], [94mLoss[0m : 3.76725
[1mStep[0m  [12/42], [94mLoss[0m : 3.60243
[1mStep[0m  [16/42], [94mLoss[0m : 3.86786
[1mStep[0m  [20/42], [94mLoss[0m : 3.67335
[1mStep[0m  [24/42], [94mLoss[0m : 3.67181
[1mStep[0m  [28/42], [94mLoss[0m : 3.44451
[1mStep[0m  [32/42], [94mLoss[0m : 3.53049
[1mStep[0m  [36/42], [94mLoss[0m : 3.68046
[1mStep[0m  [40/42], [94mLoss[0m : 3.45854

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 3.795, [92mTest[0m: 3.170, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.989
====================================

Phase 1 - Evaluation MAE:  2.9885820831571306
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 3.77206
[1mStep[0m  [4/42], [94mLoss[0m : 3.62175
[1mStep[0m  [8/42], [94mLoss[0m : 3.48898
[1mStep[0m  [12/42], [94mLoss[0m : 3.69553
[1mStep[0m  [16/42], [94mLoss[0m : 3.54644
[1mStep[0m  [20/42], [94mLoss[0m : 3.60293
[1mStep[0m  [24/42], [94mLoss[0m : 3.48425
[1mStep[0m  [28/42], [94mLoss[0m : 3.59044
[1mStep[0m  [32/42], [94mLoss[0m : 3.60480
[1mStep[0m  [36/42], [94mLoss[0m : 3.59509
[1mStep[0m  [40/42], [94mLoss[0m : 3.53558

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.579, [92mTest[0m: 2.981, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.65571
[1mStep[0m  [4/42], [94mLoss[0m : 3.61336
[1mStep[0m  [8/42], [94mLoss[0m : 3.34391
[1mStep[0m  [12/42], [94mLoss[0m : 3.20217
[1mStep[0m  [16/42], [94mLoss[0m : 3.46740
[1mStep[0m  [20/42], [94mLoss[0m : 3.15212
[1mStep[0m  [24/42], [94mLoss[0m : 3.59258
[1mStep[0m  [28/42], [94mLoss[0m : 3.37184
[1mStep[0m  [32/42], [94mLoss[0m : 2.98975
[1mStep[0m  [36/42], [94mLoss[0m : 3.21350
[1mStep[0m  [40/42], [94mLoss[0m : 3.09180

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.245, [92mTest[0m: 3.065, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.32445
[1mStep[0m  [4/42], [94mLoss[0m : 2.88056
[1mStep[0m  [8/42], [94mLoss[0m : 3.03573
[1mStep[0m  [12/42], [94mLoss[0m : 2.99723
[1mStep[0m  [16/42], [94mLoss[0m : 3.01863
[1mStep[0m  [20/42], [94mLoss[0m : 3.01478
[1mStep[0m  [24/42], [94mLoss[0m : 2.96914
[1mStep[0m  [28/42], [94mLoss[0m : 2.90351
[1mStep[0m  [32/42], [94mLoss[0m : 3.18759
[1mStep[0m  [36/42], [94mLoss[0m : 2.67838
[1mStep[0m  [40/42], [94mLoss[0m : 3.01418

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.022, [92mTest[0m: 2.700, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.16604
[1mStep[0m  [4/42], [94mLoss[0m : 3.18038
[1mStep[0m  [8/42], [94mLoss[0m : 2.82687
[1mStep[0m  [12/42], [94mLoss[0m : 2.89433
[1mStep[0m  [16/42], [94mLoss[0m : 2.96117
[1mStep[0m  [20/42], [94mLoss[0m : 2.90312
[1mStep[0m  [24/42], [94mLoss[0m : 2.66271
[1mStep[0m  [28/42], [94mLoss[0m : 3.16493
[1mStep[0m  [32/42], [94mLoss[0m : 2.93773
[1mStep[0m  [36/42], [94mLoss[0m : 2.79475
[1mStep[0m  [40/42], [94mLoss[0m : 2.88608

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.861, [92mTest[0m: 2.521, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.82007
[1mStep[0m  [4/42], [94mLoss[0m : 2.60637
[1mStep[0m  [8/42], [94mLoss[0m : 2.59427
[1mStep[0m  [12/42], [94mLoss[0m : 2.64401
[1mStep[0m  [16/42], [94mLoss[0m : 2.79908
[1mStep[0m  [20/42], [94mLoss[0m : 2.66255
[1mStep[0m  [24/42], [94mLoss[0m : 2.56276
[1mStep[0m  [28/42], [94mLoss[0m : 2.76119
[1mStep[0m  [32/42], [94mLoss[0m : 2.70566
[1mStep[0m  [36/42], [94mLoss[0m : 2.70656
[1mStep[0m  [40/42], [94mLoss[0m : 2.59189

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.772, [92mTest[0m: 2.473, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.89006
[1mStep[0m  [4/42], [94mLoss[0m : 2.54762
[1mStep[0m  [8/42], [94mLoss[0m : 2.79402
[1mStep[0m  [12/42], [94mLoss[0m : 2.76864
[1mStep[0m  [16/42], [94mLoss[0m : 2.72543
[1mStep[0m  [20/42], [94mLoss[0m : 2.63002
[1mStep[0m  [24/42], [94mLoss[0m : 2.49433
[1mStep[0m  [28/42], [94mLoss[0m : 2.65019
[1mStep[0m  [32/42], [94mLoss[0m : 2.94221
[1mStep[0m  [36/42], [94mLoss[0m : 2.54787
[1mStep[0m  [40/42], [94mLoss[0m : 2.63317

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.669, [92mTest[0m: 2.440, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44260
[1mStep[0m  [4/42], [94mLoss[0m : 2.64549
[1mStep[0m  [8/42], [94mLoss[0m : 2.66963
[1mStep[0m  [12/42], [94mLoss[0m : 2.42496
[1mStep[0m  [16/42], [94mLoss[0m : 2.71982
[1mStep[0m  [20/42], [94mLoss[0m : 2.67897
[1mStep[0m  [24/42], [94mLoss[0m : 2.79248
[1mStep[0m  [28/42], [94mLoss[0m : 2.49950
[1mStep[0m  [32/42], [94mLoss[0m : 2.58473
[1mStep[0m  [36/42], [94mLoss[0m : 2.67976
[1mStep[0m  [40/42], [94mLoss[0m : 2.77637

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.657, [92mTest[0m: 2.413, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61577
[1mStep[0m  [4/42], [94mLoss[0m : 2.58445
[1mStep[0m  [8/42], [94mLoss[0m : 2.55770
[1mStep[0m  [12/42], [94mLoss[0m : 2.65982
[1mStep[0m  [16/42], [94mLoss[0m : 2.84124
[1mStep[0m  [20/42], [94mLoss[0m : 2.56531
[1mStep[0m  [24/42], [94mLoss[0m : 2.65173
[1mStep[0m  [28/42], [94mLoss[0m : 2.48492
[1mStep[0m  [32/42], [94mLoss[0m : 2.30851
[1mStep[0m  [36/42], [94mLoss[0m : 2.69506
[1mStep[0m  [40/42], [94mLoss[0m : 2.70370

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.613, [92mTest[0m: 2.424, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44324
[1mStep[0m  [4/42], [94mLoss[0m : 2.70482
[1mStep[0m  [8/42], [94mLoss[0m : 2.57268
[1mStep[0m  [12/42], [94mLoss[0m : 2.52817
[1mStep[0m  [16/42], [94mLoss[0m : 2.56115
[1mStep[0m  [20/42], [94mLoss[0m : 2.50689
[1mStep[0m  [24/42], [94mLoss[0m : 2.77801
[1mStep[0m  [28/42], [94mLoss[0m : 2.56370
[1mStep[0m  [32/42], [94mLoss[0m : 2.35127
[1mStep[0m  [36/42], [94mLoss[0m : 2.56777
[1mStep[0m  [40/42], [94mLoss[0m : 2.54784

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.587, [92mTest[0m: 2.417, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.71155
[1mStep[0m  [4/42], [94mLoss[0m : 2.56068
[1mStep[0m  [8/42], [94mLoss[0m : 2.44259
[1mStep[0m  [12/42], [94mLoss[0m : 2.25565
[1mStep[0m  [16/42], [94mLoss[0m : 2.48345
[1mStep[0m  [20/42], [94mLoss[0m : 2.59388
[1mStep[0m  [24/42], [94mLoss[0m : 2.68569
[1mStep[0m  [28/42], [94mLoss[0m : 2.70569
[1mStep[0m  [32/42], [94mLoss[0m : 2.71753
[1mStep[0m  [36/42], [94mLoss[0m : 2.57562
[1mStep[0m  [40/42], [94mLoss[0m : 2.62920

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.557, [92mTest[0m: 2.417, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49350
[1mStep[0m  [4/42], [94mLoss[0m : 2.67652
[1mStep[0m  [8/42], [94mLoss[0m : 2.43855
[1mStep[0m  [12/42], [94mLoss[0m : 2.48318
[1mStep[0m  [16/42], [94mLoss[0m : 2.61606
[1mStep[0m  [20/42], [94mLoss[0m : 2.65690
[1mStep[0m  [24/42], [94mLoss[0m : 2.43999
[1mStep[0m  [28/42], [94mLoss[0m : 2.59991
[1mStep[0m  [32/42], [94mLoss[0m : 2.52716
[1mStep[0m  [36/42], [94mLoss[0m : 2.44805
[1mStep[0m  [40/42], [94mLoss[0m : 2.74471

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.549, [92mTest[0m: 2.431, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50382
[1mStep[0m  [4/42], [94mLoss[0m : 2.38301
[1mStep[0m  [8/42], [94mLoss[0m : 2.41414
[1mStep[0m  [12/42], [94mLoss[0m : 2.50223
[1mStep[0m  [16/42], [94mLoss[0m : 2.65911
[1mStep[0m  [20/42], [94mLoss[0m : 2.50830
[1mStep[0m  [24/42], [94mLoss[0m : 2.69597
[1mStep[0m  [28/42], [94mLoss[0m : 2.50164
[1mStep[0m  [32/42], [94mLoss[0m : 2.43029
[1mStep[0m  [36/42], [94mLoss[0m : 2.31613
[1mStep[0m  [40/42], [94mLoss[0m : 2.42662

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.447, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61855
[1mStep[0m  [4/42], [94mLoss[0m : 2.67047
[1mStep[0m  [8/42], [94mLoss[0m : 2.26843
[1mStep[0m  [12/42], [94mLoss[0m : 2.47824
[1mStep[0m  [16/42], [94mLoss[0m : 2.30609
[1mStep[0m  [20/42], [94mLoss[0m : 2.58641
[1mStep[0m  [24/42], [94mLoss[0m : 2.42497
[1mStep[0m  [28/42], [94mLoss[0m : 2.39404
[1mStep[0m  [32/42], [94mLoss[0m : 2.33002
[1mStep[0m  [36/42], [94mLoss[0m : 2.50257
[1mStep[0m  [40/42], [94mLoss[0m : 2.67107

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.417, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55519
[1mStep[0m  [4/42], [94mLoss[0m : 2.43800
[1mStep[0m  [8/42], [94mLoss[0m : 2.75712
[1mStep[0m  [12/42], [94mLoss[0m : 2.46021
[1mStep[0m  [16/42], [94mLoss[0m : 2.47648
[1mStep[0m  [20/42], [94mLoss[0m : 2.39554
[1mStep[0m  [24/42], [94mLoss[0m : 2.43825
[1mStep[0m  [28/42], [94mLoss[0m : 2.43481
[1mStep[0m  [32/42], [94mLoss[0m : 2.68714
[1mStep[0m  [36/42], [94mLoss[0m : 2.36664
[1mStep[0m  [40/42], [94mLoss[0m : 2.59154

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.454, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50413
[1mStep[0m  [4/42], [94mLoss[0m : 2.48674
[1mStep[0m  [8/42], [94mLoss[0m : 2.21874
[1mStep[0m  [12/42], [94mLoss[0m : 2.39670
[1mStep[0m  [16/42], [94mLoss[0m : 2.48511
[1mStep[0m  [20/42], [94mLoss[0m : 2.55139
[1mStep[0m  [24/42], [94mLoss[0m : 2.69191
[1mStep[0m  [28/42], [94mLoss[0m : 2.50452
[1mStep[0m  [32/42], [94mLoss[0m : 2.48991
[1mStep[0m  [36/42], [94mLoss[0m : 2.54199
[1mStep[0m  [40/42], [94mLoss[0m : 2.48407

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.437, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35806
[1mStep[0m  [4/42], [94mLoss[0m : 2.43463
[1mStep[0m  [8/42], [94mLoss[0m : 2.25227
[1mStep[0m  [12/42], [94mLoss[0m : 2.39432
[1mStep[0m  [16/42], [94mLoss[0m : 2.54590
[1mStep[0m  [20/42], [94mLoss[0m : 2.58156
[1mStep[0m  [24/42], [94mLoss[0m : 2.38950
[1mStep[0m  [28/42], [94mLoss[0m : 2.32521
[1mStep[0m  [32/42], [94mLoss[0m : 2.49881
[1mStep[0m  [36/42], [94mLoss[0m : 2.36406
[1mStep[0m  [40/42], [94mLoss[0m : 2.52160

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.469, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37252
[1mStep[0m  [4/42], [94mLoss[0m : 2.36697
[1mStep[0m  [8/42], [94mLoss[0m : 2.26098
[1mStep[0m  [12/42], [94mLoss[0m : 2.25487
[1mStep[0m  [16/42], [94mLoss[0m : 2.39688
[1mStep[0m  [20/42], [94mLoss[0m : 2.17866
[1mStep[0m  [24/42], [94mLoss[0m : 2.30607
[1mStep[0m  [28/42], [94mLoss[0m : 2.39361
[1mStep[0m  [32/42], [94mLoss[0m : 2.40057
[1mStep[0m  [36/42], [94mLoss[0m : 2.14636
[1mStep[0m  [40/42], [94mLoss[0m : 2.52732

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.410, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46072
[1mStep[0m  [4/42], [94mLoss[0m : 2.48489
[1mStep[0m  [8/42], [94mLoss[0m : 2.20960
[1mStep[0m  [12/42], [94mLoss[0m : 2.34075
[1mStep[0m  [16/42], [94mLoss[0m : 2.50960
[1mStep[0m  [20/42], [94mLoss[0m : 2.39695
[1mStep[0m  [24/42], [94mLoss[0m : 2.61189
[1mStep[0m  [28/42], [94mLoss[0m : 2.36588
[1mStep[0m  [32/42], [94mLoss[0m : 2.39996
[1mStep[0m  [36/42], [94mLoss[0m : 2.40259
[1mStep[0m  [40/42], [94mLoss[0m : 2.21039

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.398, [92mTest[0m: 2.450, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26198
[1mStep[0m  [4/42], [94mLoss[0m : 2.52521
[1mStep[0m  [8/42], [94mLoss[0m : 2.23260
[1mStep[0m  [12/42], [94mLoss[0m : 2.43780
[1mStep[0m  [16/42], [94mLoss[0m : 2.23144
[1mStep[0m  [20/42], [94mLoss[0m : 2.21575
[1mStep[0m  [24/42], [94mLoss[0m : 2.52280
[1mStep[0m  [28/42], [94mLoss[0m : 2.39853
[1mStep[0m  [32/42], [94mLoss[0m : 2.30084
[1mStep[0m  [36/42], [94mLoss[0m : 2.44559
[1mStep[0m  [40/42], [94mLoss[0m : 2.28483

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.395, [92mTest[0m: 2.404, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39462
[1mStep[0m  [4/42], [94mLoss[0m : 2.58537
[1mStep[0m  [8/42], [94mLoss[0m : 2.38908
[1mStep[0m  [12/42], [94mLoss[0m : 2.44364
[1mStep[0m  [16/42], [94mLoss[0m : 2.60025
[1mStep[0m  [20/42], [94mLoss[0m : 2.37376
[1mStep[0m  [24/42], [94mLoss[0m : 2.47339
[1mStep[0m  [28/42], [94mLoss[0m : 2.38812
[1mStep[0m  [32/42], [94mLoss[0m : 2.35442
[1mStep[0m  [36/42], [94mLoss[0m : 2.32736
[1mStep[0m  [40/42], [94mLoss[0m : 2.58757

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.366, [92mTest[0m: 2.421, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42496
[1mStep[0m  [4/42], [94mLoss[0m : 2.22064
[1mStep[0m  [8/42], [94mLoss[0m : 2.29863
[1mStep[0m  [12/42], [94mLoss[0m : 2.54800
[1mStep[0m  [16/42], [94mLoss[0m : 2.30780
[1mStep[0m  [20/42], [94mLoss[0m : 2.41534
[1mStep[0m  [24/42], [94mLoss[0m : 2.31482
[1mStep[0m  [28/42], [94mLoss[0m : 2.43838
[1mStep[0m  [32/42], [94mLoss[0m : 2.44235
[1mStep[0m  [36/42], [94mLoss[0m : 2.55609
[1mStep[0m  [40/42], [94mLoss[0m : 2.25195

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.355, [92mTest[0m: 2.432, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.09386
[1mStep[0m  [4/42], [94mLoss[0m : 2.17856
[1mStep[0m  [8/42], [94mLoss[0m : 2.31172
[1mStep[0m  [12/42], [94mLoss[0m : 2.28568
[1mStep[0m  [16/42], [94mLoss[0m : 2.57799
[1mStep[0m  [20/42], [94mLoss[0m : 2.25672
[1mStep[0m  [24/42], [94mLoss[0m : 2.17070
[1mStep[0m  [28/42], [94mLoss[0m : 2.42631
[1mStep[0m  [32/42], [94mLoss[0m : 2.37588
[1mStep[0m  [36/42], [94mLoss[0m : 2.39096
[1mStep[0m  [40/42], [94mLoss[0m : 2.36009

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.328, [92mTest[0m: 2.430, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.07749
[1mStep[0m  [4/42], [94mLoss[0m : 2.46331
[1mStep[0m  [8/42], [94mLoss[0m : 2.34269
[1mStep[0m  [12/42], [94mLoss[0m : 2.41838
[1mStep[0m  [16/42], [94mLoss[0m : 2.17486
[1mStep[0m  [20/42], [94mLoss[0m : 2.37576
[1mStep[0m  [24/42], [94mLoss[0m : 2.43037
[1mStep[0m  [28/42], [94mLoss[0m : 2.41548
[1mStep[0m  [32/42], [94mLoss[0m : 2.19421
[1mStep[0m  [36/42], [94mLoss[0m : 2.18416
[1mStep[0m  [40/42], [94mLoss[0m : 2.26131

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.319, [92mTest[0m: 2.431, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45708
[1mStep[0m  [4/42], [94mLoss[0m : 2.06907
[1mStep[0m  [8/42], [94mLoss[0m : 2.11603
[1mStep[0m  [12/42], [94mLoss[0m : 2.25745
[1mStep[0m  [16/42], [94mLoss[0m : 2.17462
[1mStep[0m  [20/42], [94mLoss[0m : 2.21663
[1mStep[0m  [24/42], [94mLoss[0m : 2.19981
[1mStep[0m  [28/42], [94mLoss[0m : 2.28064
[1mStep[0m  [32/42], [94mLoss[0m : 2.42780
[1mStep[0m  [36/42], [94mLoss[0m : 2.45692
[1mStep[0m  [40/42], [94mLoss[0m : 2.32780

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.295, [92mTest[0m: 2.448, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34519
[1mStep[0m  [4/42], [94mLoss[0m : 2.36553
[1mStep[0m  [8/42], [94mLoss[0m : 2.27010
[1mStep[0m  [12/42], [94mLoss[0m : 2.16094
[1mStep[0m  [16/42], [94mLoss[0m : 2.19671
[1mStep[0m  [20/42], [94mLoss[0m : 2.32817
[1mStep[0m  [24/42], [94mLoss[0m : 2.57558
[1mStep[0m  [28/42], [94mLoss[0m : 2.32412
[1mStep[0m  [32/42], [94mLoss[0m : 2.29975
[1mStep[0m  [36/42], [94mLoss[0m : 2.37237
[1mStep[0m  [40/42], [94mLoss[0m : 2.44226

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.284, [92mTest[0m: 2.416, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.16795
[1mStep[0m  [4/42], [94mLoss[0m : 2.12084
[1mStep[0m  [8/42], [94mLoss[0m : 2.28916
[1mStep[0m  [12/42], [94mLoss[0m : 2.25376
[1mStep[0m  [16/42], [94mLoss[0m : 2.26824
[1mStep[0m  [20/42], [94mLoss[0m : 2.18928
[1mStep[0m  [24/42], [94mLoss[0m : 2.35743
[1mStep[0m  [28/42], [94mLoss[0m : 2.21305
[1mStep[0m  [32/42], [94mLoss[0m : 2.18053
[1mStep[0m  [36/42], [94mLoss[0m : 2.07737
[1mStep[0m  [40/42], [94mLoss[0m : 2.16592

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.245, [92mTest[0m: 2.420, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.20701
[1mStep[0m  [4/42], [94mLoss[0m : 2.23866
[1mStep[0m  [8/42], [94mLoss[0m : 2.16985
[1mStep[0m  [12/42], [94mLoss[0m : 2.29456
[1mStep[0m  [16/42], [94mLoss[0m : 2.17582
[1mStep[0m  [20/42], [94mLoss[0m : 2.25438
[1mStep[0m  [24/42], [94mLoss[0m : 2.17734
[1mStep[0m  [28/42], [94mLoss[0m : 2.15003
[1mStep[0m  [32/42], [94mLoss[0m : 2.36862
[1mStep[0m  [36/42], [94mLoss[0m : 2.30683
[1mStep[0m  [40/42], [94mLoss[0m : 2.19658

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.251, [92mTest[0m: 2.436, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27330
[1mStep[0m  [4/42], [94mLoss[0m : 2.30946
[1mStep[0m  [8/42], [94mLoss[0m : 2.00944
[1mStep[0m  [12/42], [94mLoss[0m : 2.23307
[1mStep[0m  [16/42], [94mLoss[0m : 2.18783
[1mStep[0m  [20/42], [94mLoss[0m : 2.26299
[1mStep[0m  [24/42], [94mLoss[0m : 2.18523
[1mStep[0m  [28/42], [94mLoss[0m : 2.25080
[1mStep[0m  [32/42], [94mLoss[0m : 2.22646
[1mStep[0m  [36/42], [94mLoss[0m : 2.22162
[1mStep[0m  [40/42], [94mLoss[0m : 2.27281

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.220, [92mTest[0m: 2.436, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.12350
[1mStep[0m  [4/42], [94mLoss[0m : 2.23795
[1mStep[0m  [8/42], [94mLoss[0m : 2.43342
[1mStep[0m  [12/42], [94mLoss[0m : 2.24156
[1mStep[0m  [16/42], [94mLoss[0m : 2.21894
[1mStep[0m  [20/42], [94mLoss[0m : 2.10002
[1mStep[0m  [24/42], [94mLoss[0m : 2.14433
[1mStep[0m  [28/42], [94mLoss[0m : 2.16575
[1mStep[0m  [32/42], [94mLoss[0m : 2.07695
[1mStep[0m  [36/42], [94mLoss[0m : 2.31766
[1mStep[0m  [40/42], [94mLoss[0m : 2.35349

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.229, [92mTest[0m: 2.475, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.97976
[1mStep[0m  [4/42], [94mLoss[0m : 2.33937
[1mStep[0m  [8/42], [94mLoss[0m : 2.15658
[1mStep[0m  [12/42], [94mLoss[0m : 2.44055
[1mStep[0m  [16/42], [94mLoss[0m : 2.26012
[1mStep[0m  [20/42], [94mLoss[0m : 2.25430
[1mStep[0m  [24/42], [94mLoss[0m : 2.38387
[1mStep[0m  [28/42], [94mLoss[0m : 2.03175
[1mStep[0m  [32/42], [94mLoss[0m : 2.22878
[1mStep[0m  [36/42], [94mLoss[0m : 2.26408
[1mStep[0m  [40/42], [94mLoss[0m : 2.41985

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.216, [92mTest[0m: 2.438, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.440
====================================

Phase 2 - Evaluation MAE:  2.4403516394751414
MAE score P1      2.988582
MAE score P2      2.440352
loss              2.216325
learning_rate     0.002575
batch_size             256
hidden_sizes         [250]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.1
weight_decay          0.01
Name: 12, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 11.00357
[1mStep[0m  [4/42], [94mLoss[0m : 10.64396
[1mStep[0m  [8/42], [94mLoss[0m : 10.68785
[1mStep[0m  [12/42], [94mLoss[0m : 10.76207
[1mStep[0m  [16/42], [94mLoss[0m : 10.79309
[1mStep[0m  [20/42], [94mLoss[0m : 11.05954
[1mStep[0m  [24/42], [94mLoss[0m : 10.97666
[1mStep[0m  [28/42], [94mLoss[0m : 11.05447
[1mStep[0m  [32/42], [94mLoss[0m : 10.92304
[1mStep[0m  [36/42], [94mLoss[0m : 10.56294
[1mStep[0m  [40/42], [94mLoss[0m : 10.51764

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.827, [92mTest[0m: 11.043, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.44184
[1mStep[0m  [4/42], [94mLoss[0m : 10.76435
[1mStep[0m  [8/42], [94mLoss[0m : 10.39909
[1mStep[0m  [12/42], [94mLoss[0m : 10.32535
[1mStep[0m  [16/42], [94mLoss[0m : 11.02236
[1mStep[0m  [20/42], [94mLoss[0m : 10.72155
[1mStep[0m  [24/42], [94mLoss[0m : 10.26542
[1mStep[0m  [28/42], [94mLoss[0m : 10.29262
[1mStep[0m  [32/42], [94mLoss[0m : 10.03662
[1mStep[0m  [36/42], [94mLoss[0m : 10.15310
[1mStep[0m  [40/42], [94mLoss[0m : 10.34520

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.493, [92mTest[0m: 10.621, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.97414
[1mStep[0m  [4/42], [94mLoss[0m : 10.24751
[1mStep[0m  [8/42], [94mLoss[0m : 10.42586
[1mStep[0m  [12/42], [94mLoss[0m : 9.72409
[1mStep[0m  [16/42], [94mLoss[0m : 9.90159
[1mStep[0m  [20/42], [94mLoss[0m : 9.90686
[1mStep[0m  [24/42], [94mLoss[0m : 10.41050
[1mStep[0m  [28/42], [94mLoss[0m : 10.39210
[1mStep[0m  [32/42], [94mLoss[0m : 9.75883
[1mStep[0m  [36/42], [94mLoss[0m : 10.11525
[1mStep[0m  [40/42], [94mLoss[0m : 9.50764

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.138, [92mTest[0m: 10.167, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.09810
[1mStep[0m  [4/42], [94mLoss[0m : 9.87509
[1mStep[0m  [8/42], [94mLoss[0m : 9.81316
[1mStep[0m  [12/42], [94mLoss[0m : 9.85167
[1mStep[0m  [16/42], [94mLoss[0m : 9.71210
[1mStep[0m  [20/42], [94mLoss[0m : 9.88597
[1mStep[0m  [24/42], [94mLoss[0m : 9.54728
[1mStep[0m  [28/42], [94mLoss[0m : 9.62325
[1mStep[0m  [32/42], [94mLoss[0m : 9.99218
[1mStep[0m  [36/42], [94mLoss[0m : 9.82107
[1mStep[0m  [40/42], [94mLoss[0m : 9.49128

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 9.748, [92mTest[0m: 9.686, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.66303
[1mStep[0m  [4/42], [94mLoss[0m : 9.57311
[1mStep[0m  [8/42], [94mLoss[0m : 9.69255
[1mStep[0m  [12/42], [94mLoss[0m : 9.26079
[1mStep[0m  [16/42], [94mLoss[0m : 9.38327
[1mStep[0m  [20/42], [94mLoss[0m : 9.22812
[1mStep[0m  [24/42], [94mLoss[0m : 9.65391
[1mStep[0m  [28/42], [94mLoss[0m : 9.45311
[1mStep[0m  [32/42], [94mLoss[0m : 9.33757
[1mStep[0m  [36/42], [94mLoss[0m : 9.19441
[1mStep[0m  [40/42], [94mLoss[0m : 8.77409

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 9.301, [92mTest[0m: 9.171, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.95274
[1mStep[0m  [4/42], [94mLoss[0m : 9.01158
[1mStep[0m  [8/42], [94mLoss[0m : 8.96392
[1mStep[0m  [12/42], [94mLoss[0m : 9.22754
[1mStep[0m  [16/42], [94mLoss[0m : 8.89515
[1mStep[0m  [20/42], [94mLoss[0m : 8.80429
[1mStep[0m  [24/42], [94mLoss[0m : 8.67285
[1mStep[0m  [28/42], [94mLoss[0m : 8.64227
[1mStep[0m  [32/42], [94mLoss[0m : 8.50041
[1mStep[0m  [36/42], [94mLoss[0m : 8.63423
[1mStep[0m  [40/42], [94mLoss[0m : 8.31264

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 8.777, [92mTest[0m: 8.609, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.25591
[1mStep[0m  [4/42], [94mLoss[0m : 7.91754
[1mStep[0m  [8/42], [94mLoss[0m : 8.09948
[1mStep[0m  [12/42], [94mLoss[0m : 8.16842
[1mStep[0m  [16/42], [94mLoss[0m : 8.27909
[1mStep[0m  [20/42], [94mLoss[0m : 8.21673
[1mStep[0m  [24/42], [94mLoss[0m : 8.02080
[1mStep[0m  [28/42], [94mLoss[0m : 7.79381
[1mStep[0m  [32/42], [94mLoss[0m : 8.58337
[1mStep[0m  [36/42], [94mLoss[0m : 8.13114
[1mStep[0m  [40/42], [94mLoss[0m : 7.78694

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 8.179, [92mTest[0m: 7.896, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.71702
[1mStep[0m  [4/42], [94mLoss[0m : 7.99070
[1mStep[0m  [8/42], [94mLoss[0m : 7.87237
[1mStep[0m  [12/42], [94mLoss[0m : 7.75321
[1mStep[0m  [16/42], [94mLoss[0m : 7.96461
[1mStep[0m  [20/42], [94mLoss[0m : 7.36382
[1mStep[0m  [24/42], [94mLoss[0m : 7.65448
[1mStep[0m  [28/42], [94mLoss[0m : 7.47390
[1mStep[0m  [32/42], [94mLoss[0m : 7.33100
[1mStep[0m  [36/42], [94mLoss[0m : 7.49121
[1mStep[0m  [40/42], [94mLoss[0m : 7.08503

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 7.552, [92mTest[0m: 7.257, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.21873
[1mStep[0m  [4/42], [94mLoss[0m : 7.20887
[1mStep[0m  [8/42], [94mLoss[0m : 7.14378
[1mStep[0m  [12/42], [94mLoss[0m : 7.05688
[1mStep[0m  [16/42], [94mLoss[0m : 7.03537
[1mStep[0m  [20/42], [94mLoss[0m : 6.81450
[1mStep[0m  [24/42], [94mLoss[0m : 6.87443
[1mStep[0m  [28/42], [94mLoss[0m : 7.51837
[1mStep[0m  [32/42], [94mLoss[0m : 6.98960
[1mStep[0m  [36/42], [94mLoss[0m : 7.12438
[1mStep[0m  [40/42], [94mLoss[0m : 6.79541

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 6.982, [92mTest[0m: 6.516, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.10030
[1mStep[0m  [4/42], [94mLoss[0m : 6.76973
[1mStep[0m  [8/42], [94mLoss[0m : 7.16539
[1mStep[0m  [12/42], [94mLoss[0m : 6.64462
[1mStep[0m  [16/42], [94mLoss[0m : 6.68980
[1mStep[0m  [20/42], [94mLoss[0m : 6.59349
[1mStep[0m  [24/42], [94mLoss[0m : 6.13659
[1mStep[0m  [28/42], [94mLoss[0m : 6.67126
[1mStep[0m  [32/42], [94mLoss[0m : 6.33998
[1mStep[0m  [36/42], [94mLoss[0m : 6.59878
[1mStep[0m  [40/42], [94mLoss[0m : 6.18224

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 6.527, [92mTest[0m: 5.923, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.26693
[1mStep[0m  [4/42], [94mLoss[0m : 6.26122
[1mStep[0m  [8/42], [94mLoss[0m : 6.47573
[1mStep[0m  [12/42], [94mLoss[0m : 6.09685
[1mStep[0m  [16/42], [94mLoss[0m : 6.08320
[1mStep[0m  [20/42], [94mLoss[0m : 6.16527
[1mStep[0m  [24/42], [94mLoss[0m : 6.27282
[1mStep[0m  [28/42], [94mLoss[0m : 6.10937
[1mStep[0m  [32/42], [94mLoss[0m : 5.68175
[1mStep[0m  [36/42], [94mLoss[0m : 6.20999
[1mStep[0m  [40/42], [94mLoss[0m : 6.01956

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 6.101, [92mTest[0m: 5.509, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.88749
[1mStep[0m  [4/42], [94mLoss[0m : 5.74911
[1mStep[0m  [8/42], [94mLoss[0m : 5.89766
[1mStep[0m  [12/42], [94mLoss[0m : 5.54226
[1mStep[0m  [16/42], [94mLoss[0m : 5.79737
[1mStep[0m  [20/42], [94mLoss[0m : 5.31302
[1mStep[0m  [24/42], [94mLoss[0m : 5.46148
[1mStep[0m  [28/42], [94mLoss[0m : 5.88661
[1mStep[0m  [32/42], [94mLoss[0m : 5.70319
[1mStep[0m  [36/42], [94mLoss[0m : 5.74420
[1mStep[0m  [40/42], [94mLoss[0m : 5.51110

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 5.697, [92mTest[0m: 5.031, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.53671
[1mStep[0m  [4/42], [94mLoss[0m : 5.67495
[1mStep[0m  [8/42], [94mLoss[0m : 5.30335
[1mStep[0m  [12/42], [94mLoss[0m : 5.64018
[1mStep[0m  [16/42], [94mLoss[0m : 5.56393
[1mStep[0m  [20/42], [94mLoss[0m : 5.24122
[1mStep[0m  [24/42], [94mLoss[0m : 5.37401
[1mStep[0m  [28/42], [94mLoss[0m : 5.18903
[1mStep[0m  [32/42], [94mLoss[0m : 4.54030
[1mStep[0m  [36/42], [94mLoss[0m : 4.93405
[1mStep[0m  [40/42], [94mLoss[0m : 4.80302

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 5.280, [92mTest[0m: 4.562, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.15451
[1mStep[0m  [4/42], [94mLoss[0m : 5.13955
[1mStep[0m  [8/42], [94mLoss[0m : 5.00297
[1mStep[0m  [12/42], [94mLoss[0m : 4.79876
[1mStep[0m  [16/42], [94mLoss[0m : 4.69575
[1mStep[0m  [20/42], [94mLoss[0m : 5.07793
[1mStep[0m  [24/42], [94mLoss[0m : 4.77453
[1mStep[0m  [28/42], [94mLoss[0m : 4.54686
[1mStep[0m  [32/42], [94mLoss[0m : 4.79473
[1mStep[0m  [36/42], [94mLoss[0m : 4.68485
[1mStep[0m  [40/42], [94mLoss[0m : 4.62641

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 4.872, [92mTest[0m: 4.281, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.64804
[1mStep[0m  [4/42], [94mLoss[0m : 4.35880
[1mStep[0m  [8/42], [94mLoss[0m : 4.40908
[1mStep[0m  [12/42], [94mLoss[0m : 4.21732
[1mStep[0m  [16/42], [94mLoss[0m : 4.29986
[1mStep[0m  [20/42], [94mLoss[0m : 4.12531
[1mStep[0m  [24/42], [94mLoss[0m : 4.29771
[1mStep[0m  [28/42], [94mLoss[0m : 4.30072
[1mStep[0m  [32/42], [94mLoss[0m : 4.48594
[1mStep[0m  [36/42], [94mLoss[0m : 4.74811
[1mStep[0m  [40/42], [94mLoss[0m : 4.25474

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 4.438, [92mTest[0m: 3.829, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.93382
[1mStep[0m  [4/42], [94mLoss[0m : 4.17155
[1mStep[0m  [8/42], [94mLoss[0m : 3.72806
[1mStep[0m  [12/42], [94mLoss[0m : 4.04800
[1mStep[0m  [16/42], [94mLoss[0m : 4.11032
[1mStep[0m  [20/42], [94mLoss[0m : 4.13737
[1mStep[0m  [24/42], [94mLoss[0m : 4.09802
[1mStep[0m  [28/42], [94mLoss[0m : 3.64815
[1mStep[0m  [32/42], [94mLoss[0m : 3.80723
[1mStep[0m  [36/42], [94mLoss[0m : 3.72556
[1mStep[0m  [40/42], [94mLoss[0m : 3.68834

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 4.022, [92mTest[0m: 3.465, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.61655
[1mStep[0m  [4/42], [94mLoss[0m : 3.78240
[1mStep[0m  [8/42], [94mLoss[0m : 3.63077
[1mStep[0m  [12/42], [94mLoss[0m : 3.36341
[1mStep[0m  [16/42], [94mLoss[0m : 3.71288
[1mStep[0m  [20/42], [94mLoss[0m : 3.65719
[1mStep[0m  [24/42], [94mLoss[0m : 3.92243
[1mStep[0m  [28/42], [94mLoss[0m : 3.32551
[1mStep[0m  [32/42], [94mLoss[0m : 3.77593
[1mStep[0m  [36/42], [94mLoss[0m : 3.42786
[1mStep[0m  [40/42], [94mLoss[0m : 3.41289

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 3.595, [92mTest[0m: 3.038, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.29576
[1mStep[0m  [4/42], [94mLoss[0m : 3.49781
[1mStep[0m  [8/42], [94mLoss[0m : 3.52305
[1mStep[0m  [12/42], [94mLoss[0m : 3.17446
[1mStep[0m  [16/42], [94mLoss[0m : 3.40308
[1mStep[0m  [20/42], [94mLoss[0m : 3.19515
[1mStep[0m  [24/42], [94mLoss[0m : 3.22969
[1mStep[0m  [28/42], [94mLoss[0m : 3.00907
[1mStep[0m  [32/42], [94mLoss[0m : 3.04458
[1mStep[0m  [36/42], [94mLoss[0m : 3.01615
[1mStep[0m  [40/42], [94mLoss[0m : 3.11084

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 3.227, [92mTest[0m: 2.791, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.06063
[1mStep[0m  [4/42], [94mLoss[0m : 3.04201
[1mStep[0m  [8/42], [94mLoss[0m : 3.39053
[1mStep[0m  [12/42], [94mLoss[0m : 3.00784
[1mStep[0m  [16/42], [94mLoss[0m : 3.00098
[1mStep[0m  [20/42], [94mLoss[0m : 3.06140
[1mStep[0m  [24/42], [94mLoss[0m : 2.93973
[1mStep[0m  [28/42], [94mLoss[0m : 2.95780
[1mStep[0m  [32/42], [94mLoss[0m : 3.01229
[1mStep[0m  [36/42], [94mLoss[0m : 2.84142
[1mStep[0m  [40/42], [94mLoss[0m : 2.92376

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.994, [92mTest[0m: 2.579, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.68647
[1mStep[0m  [4/42], [94mLoss[0m : 2.84281
[1mStep[0m  [8/42], [94mLoss[0m : 2.96236
[1mStep[0m  [12/42], [94mLoss[0m : 2.88294
[1mStep[0m  [16/42], [94mLoss[0m : 3.10862
[1mStep[0m  [20/42], [94mLoss[0m : 2.71067
[1mStep[0m  [24/42], [94mLoss[0m : 2.71112
[1mStep[0m  [28/42], [94mLoss[0m : 2.91930
[1mStep[0m  [32/42], [94mLoss[0m : 2.89672
[1mStep[0m  [36/42], [94mLoss[0m : 2.74389
[1mStep[0m  [40/42], [94mLoss[0m : 2.76604

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.824, [92mTest[0m: 2.441, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.79459
[1mStep[0m  [4/42], [94mLoss[0m : 2.67284
[1mStep[0m  [8/42], [94mLoss[0m : 2.82420
[1mStep[0m  [12/42], [94mLoss[0m : 2.67609
[1mStep[0m  [16/42], [94mLoss[0m : 2.96026
[1mStep[0m  [20/42], [94mLoss[0m : 2.69331
[1mStep[0m  [24/42], [94mLoss[0m : 2.77267
[1mStep[0m  [28/42], [94mLoss[0m : 2.60604
[1mStep[0m  [32/42], [94mLoss[0m : 2.73017
[1mStep[0m  [36/42], [94mLoss[0m : 2.80934
[1mStep[0m  [40/42], [94mLoss[0m : 2.87700

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.724, [92mTest[0m: 2.372, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.73874
[1mStep[0m  [4/42], [94mLoss[0m : 2.65113
[1mStep[0m  [8/42], [94mLoss[0m : 2.78276
[1mStep[0m  [12/42], [94mLoss[0m : 2.56891
[1mStep[0m  [16/42], [94mLoss[0m : 2.84880
[1mStep[0m  [20/42], [94mLoss[0m : 2.68341
[1mStep[0m  [24/42], [94mLoss[0m : 2.72091
[1mStep[0m  [28/42], [94mLoss[0m : 2.70177
[1mStep[0m  [32/42], [94mLoss[0m : 2.81672
[1mStep[0m  [36/42], [94mLoss[0m : 2.58714
[1mStep[0m  [40/42], [94mLoss[0m : 2.68017

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.712, [92mTest[0m: 2.361, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.74985
[1mStep[0m  [4/42], [94mLoss[0m : 2.66980
[1mStep[0m  [8/42], [94mLoss[0m : 2.62315
[1mStep[0m  [12/42], [94mLoss[0m : 2.65383
[1mStep[0m  [16/42], [94mLoss[0m : 2.77885
[1mStep[0m  [20/42], [94mLoss[0m : 2.71003
[1mStep[0m  [24/42], [94mLoss[0m : 2.60326
[1mStep[0m  [28/42], [94mLoss[0m : 2.58094
[1mStep[0m  [32/42], [94mLoss[0m : 2.72947
[1mStep[0m  [36/42], [94mLoss[0m : 2.74838
[1mStep[0m  [40/42], [94mLoss[0m : 2.70880

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.693, [92mTest[0m: 2.346, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.69477
[1mStep[0m  [4/42], [94mLoss[0m : 2.65380
[1mStep[0m  [8/42], [94mLoss[0m : 2.69075
[1mStep[0m  [12/42], [94mLoss[0m : 2.62371
[1mStep[0m  [16/42], [94mLoss[0m : 2.71794
[1mStep[0m  [20/42], [94mLoss[0m : 2.49757
[1mStep[0m  [24/42], [94mLoss[0m : 2.60354
[1mStep[0m  [28/42], [94mLoss[0m : 2.63471
[1mStep[0m  [32/42], [94mLoss[0m : 2.71999
[1mStep[0m  [36/42], [94mLoss[0m : 2.66032
[1mStep[0m  [40/42], [94mLoss[0m : 2.50996

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.669, [92mTest[0m: 2.351, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.67178
[1mStep[0m  [4/42], [94mLoss[0m : 2.58713
[1mStep[0m  [8/42], [94mLoss[0m : 2.53076
[1mStep[0m  [12/42], [94mLoss[0m : 2.90585
[1mStep[0m  [16/42], [94mLoss[0m : 2.71514
[1mStep[0m  [20/42], [94mLoss[0m : 2.57180
[1mStep[0m  [24/42], [94mLoss[0m : 2.66627
[1mStep[0m  [28/42], [94mLoss[0m : 2.53936
[1mStep[0m  [32/42], [94mLoss[0m : 2.84687
[1mStep[0m  [36/42], [94mLoss[0m : 2.45740
[1mStep[0m  [40/42], [94mLoss[0m : 2.52776

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.660, [92mTest[0m: 2.346, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.68930
[1mStep[0m  [4/42], [94mLoss[0m : 2.52909
[1mStep[0m  [8/42], [94mLoss[0m : 2.73072
[1mStep[0m  [12/42], [94mLoss[0m : 2.59948
[1mStep[0m  [16/42], [94mLoss[0m : 2.72893
[1mStep[0m  [20/42], [94mLoss[0m : 2.56255
[1mStep[0m  [24/42], [94mLoss[0m : 2.54829
[1mStep[0m  [28/42], [94mLoss[0m : 2.63460
[1mStep[0m  [32/42], [94mLoss[0m : 2.42262
[1mStep[0m  [36/42], [94mLoss[0m : 2.40892
[1mStep[0m  [40/42], [94mLoss[0m : 2.78610

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.640, [92mTest[0m: 2.346, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.75918
[1mStep[0m  [4/42], [94mLoss[0m : 2.61171
[1mStep[0m  [8/42], [94mLoss[0m : 2.48755
[1mStep[0m  [12/42], [94mLoss[0m : 2.68993
[1mStep[0m  [16/42], [94mLoss[0m : 2.53867
[1mStep[0m  [20/42], [94mLoss[0m : 2.64706
[1mStep[0m  [24/42], [94mLoss[0m : 2.80880
[1mStep[0m  [28/42], [94mLoss[0m : 2.58160
[1mStep[0m  [32/42], [94mLoss[0m : 2.62361
[1mStep[0m  [36/42], [94mLoss[0m : 2.61942
[1mStep[0m  [40/42], [94mLoss[0m : 2.57345

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.647, [92mTest[0m: 2.374, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.75133
[1mStep[0m  [4/42], [94mLoss[0m : 2.61347
[1mStep[0m  [8/42], [94mLoss[0m : 2.38684
[1mStep[0m  [12/42], [94mLoss[0m : 2.71805
[1mStep[0m  [16/42], [94mLoss[0m : 2.47775
[1mStep[0m  [20/42], [94mLoss[0m : 2.73193
[1mStep[0m  [24/42], [94mLoss[0m : 2.54953
[1mStep[0m  [28/42], [94mLoss[0m : 2.67068
[1mStep[0m  [32/42], [94mLoss[0m : 2.79197
[1mStep[0m  [36/42], [94mLoss[0m : 2.79915
[1mStep[0m  [40/42], [94mLoss[0m : 2.53011

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.637, [92mTest[0m: 2.371, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49646
[1mStep[0m  [4/42], [94mLoss[0m : 2.39299
[1mStep[0m  [8/42], [94mLoss[0m : 2.71471
[1mStep[0m  [12/42], [94mLoss[0m : 2.74967
[1mStep[0m  [16/42], [94mLoss[0m : 2.66453
[1mStep[0m  [20/42], [94mLoss[0m : 2.50893
[1mStep[0m  [24/42], [94mLoss[0m : 2.44776
[1mStep[0m  [28/42], [94mLoss[0m : 2.63842
[1mStep[0m  [32/42], [94mLoss[0m : 2.66658
[1mStep[0m  [36/42], [94mLoss[0m : 2.54200
[1mStep[0m  [40/42], [94mLoss[0m : 2.24730

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.610, [92mTest[0m: 2.365, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.66613
[1mStep[0m  [4/42], [94mLoss[0m : 2.46405
[1mStep[0m  [8/42], [94mLoss[0m : 2.52433
[1mStep[0m  [12/42], [94mLoss[0m : 2.56630
[1mStep[0m  [16/42], [94mLoss[0m : 2.68278
[1mStep[0m  [20/42], [94mLoss[0m : 2.75043
[1mStep[0m  [24/42], [94mLoss[0m : 2.53280
[1mStep[0m  [28/42], [94mLoss[0m : 2.56857
[1mStep[0m  [32/42], [94mLoss[0m : 2.55503
[1mStep[0m  [36/42], [94mLoss[0m : 2.71769
[1mStep[0m  [40/42], [94mLoss[0m : 2.65689

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.653, [92mTest[0m: 2.366, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.366
====================================

Phase 1 - Evaluation MAE:  2.3660717521395003
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 2.67718
[1mStep[0m  [4/42], [94mLoss[0m : 2.56208
[1mStep[0m  [8/42], [94mLoss[0m : 2.76256
[1mStep[0m  [12/42], [94mLoss[0m : 2.67456
[1mStep[0m  [16/42], [94mLoss[0m : 2.61701
[1mStep[0m  [20/42], [94mLoss[0m : 2.46316
[1mStep[0m  [24/42], [94mLoss[0m : 2.72281
[1mStep[0m  [28/42], [94mLoss[0m : 2.98611
[1mStep[0m  [32/42], [94mLoss[0m : 2.40225
[1mStep[0m  [36/42], [94mLoss[0m : 2.78152
[1mStep[0m  [40/42], [94mLoss[0m : 2.66119

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.689, [92mTest[0m: 2.363, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.83262
[1mStep[0m  [4/42], [94mLoss[0m : 2.76257
[1mStep[0m  [8/42], [94mLoss[0m : 2.62528
[1mStep[0m  [12/42], [94mLoss[0m : 2.80080
[1mStep[0m  [16/42], [94mLoss[0m : 2.56490
[1mStep[0m  [20/42], [94mLoss[0m : 2.62296
[1mStep[0m  [24/42], [94mLoss[0m : 2.71597
[1mStep[0m  [28/42], [94mLoss[0m : 2.67728
[1mStep[0m  [32/42], [94mLoss[0m : 2.73748
[1mStep[0m  [36/42], [94mLoss[0m : 2.60997
[1mStep[0m  [40/42], [94mLoss[0m : 2.72388

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.650, [92mTest[0m: 2.361, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53724
[1mStep[0m  [4/42], [94mLoss[0m : 2.44964
[1mStep[0m  [8/42], [94mLoss[0m : 2.81961
[1mStep[0m  [12/42], [94mLoss[0m : 2.38170
[1mStep[0m  [16/42], [94mLoss[0m : 2.49251
[1mStep[0m  [20/42], [94mLoss[0m : 2.46350
[1mStep[0m  [24/42], [94mLoss[0m : 2.66282
[1mStep[0m  [28/42], [94mLoss[0m : 2.73722
[1mStep[0m  [32/42], [94mLoss[0m : 2.72723
[1mStep[0m  [36/42], [94mLoss[0m : 2.52565
[1mStep[0m  [40/42], [94mLoss[0m : 2.37409

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.607, [92mTest[0m: 2.403, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40917
[1mStep[0m  [4/42], [94mLoss[0m : 2.50133
[1mStep[0m  [8/42], [94mLoss[0m : 2.62672
[1mStep[0m  [12/42], [94mLoss[0m : 2.65514
[1mStep[0m  [16/42], [94mLoss[0m : 2.87330
[1mStep[0m  [20/42], [94mLoss[0m : 2.62618
[1mStep[0m  [24/42], [94mLoss[0m : 2.51897
[1mStep[0m  [28/42], [94mLoss[0m : 2.57325
[1mStep[0m  [32/42], [94mLoss[0m : 2.46929
[1mStep[0m  [36/42], [94mLoss[0m : 2.48510
[1mStep[0m  [40/42], [94mLoss[0m : 2.37903

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.587, [92mTest[0m: 2.538, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61722
[1mStep[0m  [4/42], [94mLoss[0m : 2.86548
[1mStep[0m  [8/42], [94mLoss[0m : 2.63156
[1mStep[0m  [12/42], [94mLoss[0m : 2.27338
[1mStep[0m  [16/42], [94mLoss[0m : 2.62154
[1mStep[0m  [20/42], [94mLoss[0m : 2.41588
[1mStep[0m  [24/42], [94mLoss[0m : 2.70844
[1mStep[0m  [28/42], [94mLoss[0m : 2.44019
[1mStep[0m  [32/42], [94mLoss[0m : 2.45947
[1mStep[0m  [36/42], [94mLoss[0m : 2.50140
[1mStep[0m  [40/42], [94mLoss[0m : 2.64526

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.562, [92mTest[0m: 2.585, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37543
[1mStep[0m  [4/42], [94mLoss[0m : 2.63407
[1mStep[0m  [8/42], [94mLoss[0m : 2.56026
[1mStep[0m  [12/42], [94mLoss[0m : 2.67769
[1mStep[0m  [16/42], [94mLoss[0m : 2.29328
[1mStep[0m  [20/42], [94mLoss[0m : 2.60216
[1mStep[0m  [24/42], [94mLoss[0m : 2.33545
[1mStep[0m  [28/42], [94mLoss[0m : 2.62379
[1mStep[0m  [32/42], [94mLoss[0m : 2.75018
[1mStep[0m  [36/42], [94mLoss[0m : 2.33903
[1mStep[0m  [40/42], [94mLoss[0m : 2.74852

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.530, [92mTest[0m: 2.497, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48732
[1mStep[0m  [4/42], [94mLoss[0m : 2.53772
[1mStep[0m  [8/42], [94mLoss[0m : 2.46486
[1mStep[0m  [12/42], [94mLoss[0m : 2.41104
[1mStep[0m  [16/42], [94mLoss[0m : 2.22602
[1mStep[0m  [20/42], [94mLoss[0m : 2.54676
[1mStep[0m  [24/42], [94mLoss[0m : 2.42939
[1mStep[0m  [28/42], [94mLoss[0m : 2.65430
[1mStep[0m  [32/42], [94mLoss[0m : 2.43994
[1mStep[0m  [36/42], [94mLoss[0m : 2.43287
[1mStep[0m  [40/42], [94mLoss[0m : 2.54266

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.514, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.76734
[1mStep[0m  [4/42], [94mLoss[0m : 2.35308
[1mStep[0m  [8/42], [94mLoss[0m : 2.53641
[1mStep[0m  [12/42], [94mLoss[0m : 2.37003
[1mStep[0m  [16/42], [94mLoss[0m : 2.54891
[1mStep[0m  [20/42], [94mLoss[0m : 2.52981
[1mStep[0m  [24/42], [94mLoss[0m : 2.45226
[1mStep[0m  [28/42], [94mLoss[0m : 2.28132
[1mStep[0m  [32/42], [94mLoss[0m : 2.39690
[1mStep[0m  [36/42], [94mLoss[0m : 2.52845
[1mStep[0m  [40/42], [94mLoss[0m : 2.51108

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.497, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43370
[1mStep[0m  [4/42], [94mLoss[0m : 2.63738
[1mStep[0m  [8/42], [94mLoss[0m : 2.53913
[1mStep[0m  [12/42], [94mLoss[0m : 2.49696
[1mStep[0m  [16/42], [94mLoss[0m : 2.51325
[1mStep[0m  [20/42], [94mLoss[0m : 2.43461
[1mStep[0m  [24/42], [94mLoss[0m : 2.30267
[1mStep[0m  [28/42], [94mLoss[0m : 2.47275
[1mStep[0m  [32/42], [94mLoss[0m : 2.47197
[1mStep[0m  [36/42], [94mLoss[0m : 2.64550
[1mStep[0m  [40/42], [94mLoss[0m : 2.61724

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.616, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53455
[1mStep[0m  [4/42], [94mLoss[0m : 2.36556
[1mStep[0m  [8/42], [94mLoss[0m : 2.38520
[1mStep[0m  [12/42], [94mLoss[0m : 2.43605
[1mStep[0m  [16/42], [94mLoss[0m : 2.11632
[1mStep[0m  [20/42], [94mLoss[0m : 2.52892
[1mStep[0m  [24/42], [94mLoss[0m : 2.62941
[1mStep[0m  [28/42], [94mLoss[0m : 2.47653
[1mStep[0m  [32/42], [94mLoss[0m : 2.64429
[1mStep[0m  [36/42], [94mLoss[0m : 2.45852
[1mStep[0m  [40/42], [94mLoss[0m : 2.44534

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.664, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52156
[1mStep[0m  [4/42], [94mLoss[0m : 2.51619
[1mStep[0m  [8/42], [94mLoss[0m : 2.32630
[1mStep[0m  [12/42], [94mLoss[0m : 2.38100
[1mStep[0m  [16/42], [94mLoss[0m : 2.25106
[1mStep[0m  [20/42], [94mLoss[0m : 2.31317
[1mStep[0m  [24/42], [94mLoss[0m : 2.35389
[1mStep[0m  [28/42], [94mLoss[0m : 2.46815
[1mStep[0m  [32/42], [94mLoss[0m : 2.33515
[1mStep[0m  [36/42], [94mLoss[0m : 2.44003
[1mStep[0m  [40/42], [94mLoss[0m : 2.51036

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.618, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29362
[1mStep[0m  [4/42], [94mLoss[0m : 2.41574
[1mStep[0m  [8/42], [94mLoss[0m : 2.44659
[1mStep[0m  [12/42], [94mLoss[0m : 2.32172
[1mStep[0m  [16/42], [94mLoss[0m : 2.40797
[1mStep[0m  [20/42], [94mLoss[0m : 2.40217
[1mStep[0m  [24/42], [94mLoss[0m : 2.51762
[1mStep[0m  [28/42], [94mLoss[0m : 2.49698
[1mStep[0m  [32/42], [94mLoss[0m : 2.17207
[1mStep[0m  [36/42], [94mLoss[0m : 2.37276
[1mStep[0m  [40/42], [94mLoss[0m : 2.33277

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.384, [92mTest[0m: 2.592, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41772
[1mStep[0m  [4/42], [94mLoss[0m : 2.28919
[1mStep[0m  [8/42], [94mLoss[0m : 2.32261
[1mStep[0m  [12/42], [94mLoss[0m : 2.23108
[1mStep[0m  [16/42], [94mLoss[0m : 2.40251
[1mStep[0m  [20/42], [94mLoss[0m : 2.59357
[1mStep[0m  [24/42], [94mLoss[0m : 2.43560
[1mStep[0m  [28/42], [94mLoss[0m : 2.41515
[1mStep[0m  [32/42], [94mLoss[0m : 2.36803
[1mStep[0m  [36/42], [94mLoss[0m : 2.34688
[1mStep[0m  [40/42], [94mLoss[0m : 2.38653

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.362, [92mTest[0m: 2.602, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33223
[1mStep[0m  [4/42], [94mLoss[0m : 2.29791
[1mStep[0m  [8/42], [94mLoss[0m : 2.37397
[1mStep[0m  [12/42], [94mLoss[0m : 2.36005
[1mStep[0m  [16/42], [94mLoss[0m : 2.50581
[1mStep[0m  [20/42], [94mLoss[0m : 2.47716
[1mStep[0m  [24/42], [94mLoss[0m : 2.35024
[1mStep[0m  [28/42], [94mLoss[0m : 2.30175
[1mStep[0m  [32/42], [94mLoss[0m : 1.93839
[1mStep[0m  [36/42], [94mLoss[0m : 2.44300
[1mStep[0m  [40/42], [94mLoss[0m : 2.28978

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.334, [92mTest[0m: 2.646, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31950
[1mStep[0m  [4/42], [94mLoss[0m : 2.34868
[1mStep[0m  [8/42], [94mLoss[0m : 2.40416
[1mStep[0m  [12/42], [94mLoss[0m : 2.36865
[1mStep[0m  [16/42], [94mLoss[0m : 2.28719
[1mStep[0m  [20/42], [94mLoss[0m : 2.42336
[1mStep[0m  [24/42], [94mLoss[0m : 2.40975
[1mStep[0m  [28/42], [94mLoss[0m : 2.21689
[1mStep[0m  [32/42], [94mLoss[0m : 2.01921
[1mStep[0m  [36/42], [94mLoss[0m : 2.45719
[1mStep[0m  [40/42], [94mLoss[0m : 2.20926

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.301, [92mTest[0m: 2.623, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26894
[1mStep[0m  [4/42], [94mLoss[0m : 2.56097
[1mStep[0m  [8/42], [94mLoss[0m : 2.16792
[1mStep[0m  [12/42], [94mLoss[0m : 2.36169
[1mStep[0m  [16/42], [94mLoss[0m : 2.25582
[1mStep[0m  [20/42], [94mLoss[0m : 2.00121
[1mStep[0m  [24/42], [94mLoss[0m : 2.19351
[1mStep[0m  [28/42], [94mLoss[0m : 2.25002
[1mStep[0m  [32/42], [94mLoss[0m : 2.44445
[1mStep[0m  [36/42], [94mLoss[0m : 2.27641
[1mStep[0m  [40/42], [94mLoss[0m : 2.28612

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.288, [92mTest[0m: 2.650, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41817
[1mStep[0m  [4/42], [94mLoss[0m : 2.09075
[1mStep[0m  [8/42], [94mLoss[0m : 1.96046
[1mStep[0m  [12/42], [94mLoss[0m : 2.54285
[1mStep[0m  [16/42], [94mLoss[0m : 2.27613
[1mStep[0m  [20/42], [94mLoss[0m : 2.16310
[1mStep[0m  [24/42], [94mLoss[0m : 2.03491
[1mStep[0m  [28/42], [94mLoss[0m : 2.45657
[1mStep[0m  [32/42], [94mLoss[0m : 2.39578
[1mStep[0m  [36/42], [94mLoss[0m : 2.22138
[1mStep[0m  [40/42], [94mLoss[0m : 2.27554

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.252, [92mTest[0m: 2.636, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.06526
[1mStep[0m  [4/42], [94mLoss[0m : 2.29519
[1mStep[0m  [8/42], [94mLoss[0m : 2.19175
[1mStep[0m  [12/42], [94mLoss[0m : 2.33781
[1mStep[0m  [16/42], [94mLoss[0m : 2.30506
[1mStep[0m  [20/42], [94mLoss[0m : 2.34277
[1mStep[0m  [24/42], [94mLoss[0m : 2.25157
[1mStep[0m  [28/42], [94mLoss[0m : 2.08676
[1mStep[0m  [32/42], [94mLoss[0m : 2.29121
[1mStep[0m  [36/42], [94mLoss[0m : 2.16559
[1mStep[0m  [40/42], [94mLoss[0m : 2.37332

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.227, [92mTest[0m: 2.536, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.17717
[1mStep[0m  [4/42], [94mLoss[0m : 2.09720
[1mStep[0m  [8/42], [94mLoss[0m : 2.27548
[1mStep[0m  [12/42], [94mLoss[0m : 2.13808
[1mStep[0m  [16/42], [94mLoss[0m : 2.19447
[1mStep[0m  [20/42], [94mLoss[0m : 2.21625
[1mStep[0m  [24/42], [94mLoss[0m : 2.11105
[1mStep[0m  [28/42], [94mLoss[0m : 2.08528
[1mStep[0m  [32/42], [94mLoss[0m : 2.43348
[1mStep[0m  [36/42], [94mLoss[0m : 2.32867
[1mStep[0m  [40/42], [94mLoss[0m : 2.23714

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.213, [92mTest[0m: 2.564, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.19138
[1mStep[0m  [4/42], [94mLoss[0m : 1.94964
[1mStep[0m  [8/42], [94mLoss[0m : 2.22006
[1mStep[0m  [12/42], [94mLoss[0m : 2.23496
[1mStep[0m  [16/42], [94mLoss[0m : 2.00333
[1mStep[0m  [20/42], [94mLoss[0m : 2.00648
[1mStep[0m  [24/42], [94mLoss[0m : 2.15860
[1mStep[0m  [28/42], [94mLoss[0m : 2.37257
[1mStep[0m  [32/42], [94mLoss[0m : 2.46337
[1mStep[0m  [36/42], [94mLoss[0m : 2.18109
[1mStep[0m  [40/42], [94mLoss[0m : 2.31168

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.181, [92mTest[0m: 2.617, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.11706
[1mStep[0m  [4/42], [94mLoss[0m : 2.27722
[1mStep[0m  [8/42], [94mLoss[0m : 2.10607
[1mStep[0m  [12/42], [94mLoss[0m : 2.32519
[1mStep[0m  [16/42], [94mLoss[0m : 2.13852
[1mStep[0m  [20/42], [94mLoss[0m : 2.21237
[1mStep[0m  [24/42], [94mLoss[0m : 2.39081
[1mStep[0m  [28/42], [94mLoss[0m : 2.39978
[1mStep[0m  [32/42], [94mLoss[0m : 2.22051
[1mStep[0m  [36/42], [94mLoss[0m : 2.26819
[1mStep[0m  [40/42], [94mLoss[0m : 2.11706

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.166, [92mTest[0m: 2.568, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.20619
[1mStep[0m  [4/42], [94mLoss[0m : 2.25722
[1mStep[0m  [8/42], [94mLoss[0m : 2.11987
[1mStep[0m  [12/42], [94mLoss[0m : 2.13681
[1mStep[0m  [16/42], [94mLoss[0m : 2.22486
[1mStep[0m  [20/42], [94mLoss[0m : 2.16465
[1mStep[0m  [24/42], [94mLoss[0m : 2.15639
[1mStep[0m  [28/42], [94mLoss[0m : 1.99312
[1mStep[0m  [32/42], [94mLoss[0m : 2.11084
[1mStep[0m  [36/42], [94mLoss[0m : 2.08875
[1mStep[0m  [40/42], [94mLoss[0m : 2.09925

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.140, [92mTest[0m: 2.541, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.18609
[1mStep[0m  [4/42], [94mLoss[0m : 1.91218
[1mStep[0m  [8/42], [94mLoss[0m : 1.90631
[1mStep[0m  [12/42], [94mLoss[0m : 2.08051
[1mStep[0m  [16/42], [94mLoss[0m : 2.28610
[1mStep[0m  [20/42], [94mLoss[0m : 2.06622
[1mStep[0m  [24/42], [94mLoss[0m : 2.17174
[1mStep[0m  [28/42], [94mLoss[0m : 2.18777
[1mStep[0m  [32/42], [94mLoss[0m : 2.14117
[1mStep[0m  [36/42], [94mLoss[0m : 2.11064
[1mStep[0m  [40/42], [94mLoss[0m : 2.09135

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.121, [92mTest[0m: 2.557, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.09367
[1mStep[0m  [4/42], [94mLoss[0m : 2.13619
[1mStep[0m  [8/42], [94mLoss[0m : 1.83869
[1mStep[0m  [12/42], [94mLoss[0m : 2.10333
[1mStep[0m  [16/42], [94mLoss[0m : 1.97246
[1mStep[0m  [20/42], [94mLoss[0m : 1.86894
[1mStep[0m  [24/42], [94mLoss[0m : 2.05518
[1mStep[0m  [28/42], [94mLoss[0m : 2.24114
[1mStep[0m  [32/42], [94mLoss[0m : 2.25602
[1mStep[0m  [36/42], [94mLoss[0m : 2.19182
[1mStep[0m  [40/42], [94mLoss[0m : 2.15343

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.090, [92mTest[0m: 2.536, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.12833
[1mStep[0m  [4/42], [94mLoss[0m : 2.04044
[1mStep[0m  [8/42], [94mLoss[0m : 2.01982
[1mStep[0m  [12/42], [94mLoss[0m : 1.94657
[1mStep[0m  [16/42], [94mLoss[0m : 2.16633
[1mStep[0m  [20/42], [94mLoss[0m : 2.06168
[1mStep[0m  [24/42], [94mLoss[0m : 2.10015
[1mStep[0m  [28/42], [94mLoss[0m : 1.79308
[1mStep[0m  [32/42], [94mLoss[0m : 2.15264
[1mStep[0m  [36/42], [94mLoss[0m : 2.26349
[1mStep[0m  [40/42], [94mLoss[0m : 1.95760

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.069, [92mTest[0m: 2.573, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.19977
[1mStep[0m  [4/42], [94mLoss[0m : 1.86232
[1mStep[0m  [8/42], [94mLoss[0m : 2.11124
[1mStep[0m  [12/42], [94mLoss[0m : 1.99156
[1mStep[0m  [16/42], [94mLoss[0m : 2.01124
[1mStep[0m  [20/42], [94mLoss[0m : 2.21754
[1mStep[0m  [24/42], [94mLoss[0m : 2.15994
[1mStep[0m  [28/42], [94mLoss[0m : 2.15539
[1mStep[0m  [32/42], [94mLoss[0m : 2.04944
[1mStep[0m  [36/42], [94mLoss[0m : 1.97876
[1mStep[0m  [40/42], [94mLoss[0m : 2.13396

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.061, [92mTest[0m: 2.546, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.02412
[1mStep[0m  [4/42], [94mLoss[0m : 2.05582
[1mStep[0m  [8/42], [94mLoss[0m : 1.93015
[1mStep[0m  [12/42], [94mLoss[0m : 2.01662
[1mStep[0m  [16/42], [94mLoss[0m : 2.00143
[1mStep[0m  [20/42], [94mLoss[0m : 1.87789
[1mStep[0m  [24/42], [94mLoss[0m : 2.08512
[1mStep[0m  [28/42], [94mLoss[0m : 1.85966
[1mStep[0m  [32/42], [94mLoss[0m : 2.03448
[1mStep[0m  [36/42], [94mLoss[0m : 2.07204
[1mStep[0m  [40/42], [94mLoss[0m : 1.93863

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.023, [92mTest[0m: 2.482, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.88413
[1mStep[0m  [4/42], [94mLoss[0m : 1.98639
[1mStep[0m  [8/42], [94mLoss[0m : 2.06478
[1mStep[0m  [12/42], [94mLoss[0m : 2.08584
[1mStep[0m  [16/42], [94mLoss[0m : 1.92067
[1mStep[0m  [20/42], [94mLoss[0m : 2.18024
[1mStep[0m  [24/42], [94mLoss[0m : 2.11770
[1mStep[0m  [28/42], [94mLoss[0m : 1.97446
[1mStep[0m  [32/42], [94mLoss[0m : 1.92883
[1mStep[0m  [36/42], [94mLoss[0m : 2.00884
[1mStep[0m  [40/42], [94mLoss[0m : 1.88346

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.013, [92mTest[0m: 2.596, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.97322
[1mStep[0m  [4/42], [94mLoss[0m : 1.99843
[1mStep[0m  [8/42], [94mLoss[0m : 1.85119
[1mStep[0m  [12/42], [94mLoss[0m : 2.13385
[1mStep[0m  [16/42], [94mLoss[0m : 1.89187
[1mStep[0m  [20/42], [94mLoss[0m : 2.06120
[1mStep[0m  [24/42], [94mLoss[0m : 1.91507
[1mStep[0m  [28/42], [94mLoss[0m : 1.90582
[1mStep[0m  [32/42], [94mLoss[0m : 1.85967
[1mStep[0m  [36/42], [94mLoss[0m : 1.88882
[1mStep[0m  [40/42], [94mLoss[0m : 1.93659

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.992, [92mTest[0m: 2.596, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.98717
[1mStep[0m  [4/42], [94mLoss[0m : 2.00160
[1mStep[0m  [8/42], [94mLoss[0m : 1.99647
[1mStep[0m  [12/42], [94mLoss[0m : 1.99765
[1mStep[0m  [16/42], [94mLoss[0m : 1.93128
[1mStep[0m  [20/42], [94mLoss[0m : 2.09780
[1mStep[0m  [24/42], [94mLoss[0m : 2.09659
[1mStep[0m  [28/42], [94mLoss[0m : 2.01618
[1mStep[0m  [32/42], [94mLoss[0m : 1.96323
[1mStep[0m  [36/42], [94mLoss[0m : 1.94903
[1mStep[0m  [40/42], [94mLoss[0m : 2.08021

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.987, [92mTest[0m: 2.650, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.567
====================================

Phase 2 - Evaluation MAE:  2.566973567008972
MAE score P1      2.366072
MAE score P2      2.566974
loss              1.986906
learning_rate     0.002575
batch_size             256
hidden_sizes         [100]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.5
weight_decay        0.0001
Name: 13, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/21], [94mLoss[0m : 11.33784
[1mStep[0m  [2/21], [94mLoss[0m : 10.74163
[1mStep[0m  [4/21], [94mLoss[0m : 10.29436
[1mStep[0m  [6/21], [94mLoss[0m : 10.02374
[1mStep[0m  [8/21], [94mLoss[0m : 9.70461
[1mStep[0m  [10/21], [94mLoss[0m : 8.95068
[1mStep[0m  [12/21], [94mLoss[0m : 8.46384
[1mStep[0m  [14/21], [94mLoss[0m : 8.26333
[1mStep[0m  [16/21], [94mLoss[0m : 7.58633
[1mStep[0m  [18/21], [94mLoss[0m : 7.60427
[1mStep[0m  [20/21], [94mLoss[0m : 7.00421

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.132, [92mTest[0m: 11.305, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.58068
[1mStep[0m  [2/21], [94mLoss[0m : 6.25833
[1mStep[0m  [4/21], [94mLoss[0m : 5.78270
[1mStep[0m  [6/21], [94mLoss[0m : 5.36967
[1mStep[0m  [8/21], [94mLoss[0m : 5.10208
[1mStep[0m  [10/21], [94mLoss[0m : 4.80042
[1mStep[0m  [12/21], [94mLoss[0m : 4.31967
[1mStep[0m  [14/21], [94mLoss[0m : 4.08429
[1mStep[0m  [16/21], [94mLoss[0m : 3.94846
[1mStep[0m  [18/21], [94mLoss[0m : 3.68059
[1mStep[0m  [20/21], [94mLoss[0m : 3.63654

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 4.875, [92mTest[0m: 6.710, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.31750
[1mStep[0m  [2/21], [94mLoss[0m : 3.32608
[1mStep[0m  [4/21], [94mLoss[0m : 3.22416
[1mStep[0m  [6/21], [94mLoss[0m : 3.31049
[1mStep[0m  [8/21], [94mLoss[0m : 3.11247
[1mStep[0m  [10/21], [94mLoss[0m : 2.97982
[1mStep[0m  [12/21], [94mLoss[0m : 2.81617
[1mStep[0m  [14/21], [94mLoss[0m : 3.19027
[1mStep[0m  [16/21], [94mLoss[0m : 2.96153
[1mStep[0m  [18/21], [94mLoss[0m : 2.99586
[1mStep[0m  [20/21], [94mLoss[0m : 2.87793

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.118, [92mTest[0m: 3.453, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.83919
[1mStep[0m  [2/21], [94mLoss[0m : 2.82886
[1mStep[0m  [4/21], [94mLoss[0m : 2.87962
[1mStep[0m  [6/21], [94mLoss[0m : 2.86873
[1mStep[0m  [8/21], [94mLoss[0m : 2.69899
[1mStep[0m  [10/21], [94mLoss[0m : 2.79501
[1mStep[0m  [12/21], [94mLoss[0m : 2.76735
[1mStep[0m  [14/21], [94mLoss[0m : 2.74744
[1mStep[0m  [16/21], [94mLoss[0m : 2.89080
[1mStep[0m  [18/21], [94mLoss[0m : 2.80734
[1mStep[0m  [20/21], [94mLoss[0m : 2.71953

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.817, [92mTest[0m: 2.871, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.74671
[1mStep[0m  [2/21], [94mLoss[0m : 2.73441
[1mStep[0m  [4/21], [94mLoss[0m : 2.86995
[1mStep[0m  [6/21], [94mLoss[0m : 2.66519
[1mStep[0m  [8/21], [94mLoss[0m : 2.66366
[1mStep[0m  [10/21], [94mLoss[0m : 2.60968
[1mStep[0m  [12/21], [94mLoss[0m : 2.65106
[1mStep[0m  [14/21], [94mLoss[0m : 2.66470
[1mStep[0m  [16/21], [94mLoss[0m : 2.64549
[1mStep[0m  [18/21], [94mLoss[0m : 2.80218
[1mStep[0m  [20/21], [94mLoss[0m : 2.62119

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.723, [92mTest[0m: 2.705, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.66802
[1mStep[0m  [2/21], [94mLoss[0m : 2.59739
[1mStep[0m  [4/21], [94mLoss[0m : 2.72274
[1mStep[0m  [6/21], [94mLoss[0m : 2.51527
[1mStep[0m  [8/21], [94mLoss[0m : 2.58141
[1mStep[0m  [10/21], [94mLoss[0m : 2.59068
[1mStep[0m  [12/21], [94mLoss[0m : 2.78036
[1mStep[0m  [14/21], [94mLoss[0m : 2.71489
[1mStep[0m  [16/21], [94mLoss[0m : 2.65512
[1mStep[0m  [18/21], [94mLoss[0m : 2.83803
[1mStep[0m  [20/21], [94mLoss[0m : 2.64255

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.657, [92mTest[0m: 2.642, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.76328
[1mStep[0m  [2/21], [94mLoss[0m : 2.50621
[1mStep[0m  [4/21], [94mLoss[0m : 2.57624
[1mStep[0m  [6/21], [94mLoss[0m : 2.69398
[1mStep[0m  [8/21], [94mLoss[0m : 2.53357
[1mStep[0m  [10/21], [94mLoss[0m : 2.53992
[1mStep[0m  [12/21], [94mLoss[0m : 2.63521
[1mStep[0m  [14/21], [94mLoss[0m : 2.62306
[1mStep[0m  [16/21], [94mLoss[0m : 2.67839
[1mStep[0m  [18/21], [94mLoss[0m : 2.68967
[1mStep[0m  [20/21], [94mLoss[0m : 2.61194

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.628, [92mTest[0m: 2.601, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.55901
[1mStep[0m  [2/21], [94mLoss[0m : 2.62099
[1mStep[0m  [4/21], [94mLoss[0m : 2.57103
[1mStep[0m  [6/21], [94mLoss[0m : 2.49107
[1mStep[0m  [8/21], [94mLoss[0m : 2.82439
[1mStep[0m  [10/21], [94mLoss[0m : 2.52674
[1mStep[0m  [12/21], [94mLoss[0m : 2.57190
[1mStep[0m  [14/21], [94mLoss[0m : 2.66317
[1mStep[0m  [16/21], [94mLoss[0m : 2.55552
[1mStep[0m  [18/21], [94mLoss[0m : 2.70383
[1mStep[0m  [20/21], [94mLoss[0m : 2.63471

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.595, [92mTest[0m: 2.563, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.58656
[1mStep[0m  [2/21], [94mLoss[0m : 2.49731
[1mStep[0m  [4/21], [94mLoss[0m : 2.60124
[1mStep[0m  [6/21], [94mLoss[0m : 2.53743
[1mStep[0m  [8/21], [94mLoss[0m : 2.60655
[1mStep[0m  [10/21], [94mLoss[0m : 2.53345
[1mStep[0m  [12/21], [94mLoss[0m : 2.48122
[1mStep[0m  [14/21], [94mLoss[0m : 2.43022
[1mStep[0m  [16/21], [94mLoss[0m : 2.65698
[1mStep[0m  [18/21], [94mLoss[0m : 2.43839
[1mStep[0m  [20/21], [94mLoss[0m : 2.46525

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.583, [92mTest[0m: 2.534, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.55071
[1mStep[0m  [2/21], [94mLoss[0m : 2.72765
[1mStep[0m  [4/21], [94mLoss[0m : 2.63645
[1mStep[0m  [6/21], [94mLoss[0m : 2.52291
[1mStep[0m  [8/21], [94mLoss[0m : 2.35810
[1mStep[0m  [10/21], [94mLoss[0m : 2.70107
[1mStep[0m  [12/21], [94mLoss[0m : 2.54802
[1mStep[0m  [14/21], [94mLoss[0m : 2.56637
[1mStep[0m  [16/21], [94mLoss[0m : 2.56842
[1mStep[0m  [18/21], [94mLoss[0m : 2.46344
[1mStep[0m  [20/21], [94mLoss[0m : 2.67312

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.562, [92mTest[0m: 2.522, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51661
[1mStep[0m  [2/21], [94mLoss[0m : 2.54641
[1mStep[0m  [4/21], [94mLoss[0m : 2.55012
[1mStep[0m  [6/21], [94mLoss[0m : 2.44857
[1mStep[0m  [8/21], [94mLoss[0m : 2.60695
[1mStep[0m  [10/21], [94mLoss[0m : 2.51541
[1mStep[0m  [12/21], [94mLoss[0m : 2.52645
[1mStep[0m  [14/21], [94mLoss[0m : 2.70380
[1mStep[0m  [16/21], [94mLoss[0m : 2.46283
[1mStep[0m  [18/21], [94mLoss[0m : 2.57306
[1mStep[0m  [20/21], [94mLoss[0m : 2.55474

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.553, [92mTest[0m: 2.504, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.55175
[1mStep[0m  [2/21], [94mLoss[0m : 2.52436
[1mStep[0m  [4/21], [94mLoss[0m : 2.66208
[1mStep[0m  [6/21], [94mLoss[0m : 2.59627
[1mStep[0m  [8/21], [94mLoss[0m : 2.49606
[1mStep[0m  [10/21], [94mLoss[0m : 2.57929
[1mStep[0m  [12/21], [94mLoss[0m : 2.52265
[1mStep[0m  [14/21], [94mLoss[0m : 2.55767
[1mStep[0m  [16/21], [94mLoss[0m : 2.45753
[1mStep[0m  [18/21], [94mLoss[0m : 2.35401
[1mStep[0m  [20/21], [94mLoss[0m : 2.34522

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.499, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.62572
[1mStep[0m  [2/21], [94mLoss[0m : 2.53608
[1mStep[0m  [4/21], [94mLoss[0m : 2.51012
[1mStep[0m  [6/21], [94mLoss[0m : 2.41174
[1mStep[0m  [8/21], [94mLoss[0m : 2.49384
[1mStep[0m  [10/21], [94mLoss[0m : 2.66183
[1mStep[0m  [12/21], [94mLoss[0m : 2.45454
[1mStep[0m  [14/21], [94mLoss[0m : 2.46395
[1mStep[0m  [16/21], [94mLoss[0m : 2.58749
[1mStep[0m  [18/21], [94mLoss[0m : 2.69255
[1mStep[0m  [20/21], [94mLoss[0m : 2.49684

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.491, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53261
[1mStep[0m  [2/21], [94mLoss[0m : 2.48361
[1mStep[0m  [4/21], [94mLoss[0m : 2.51276
[1mStep[0m  [6/21], [94mLoss[0m : 2.44697
[1mStep[0m  [8/21], [94mLoss[0m : 2.38454
[1mStep[0m  [10/21], [94mLoss[0m : 2.49884
[1mStep[0m  [12/21], [94mLoss[0m : 2.54944
[1mStep[0m  [14/21], [94mLoss[0m : 2.47236
[1mStep[0m  [16/21], [94mLoss[0m : 2.46731
[1mStep[0m  [18/21], [94mLoss[0m : 2.66541
[1mStep[0m  [20/21], [94mLoss[0m : 2.47821

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.479, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.39920
[1mStep[0m  [2/21], [94mLoss[0m : 2.50154
[1mStep[0m  [4/21], [94mLoss[0m : 2.53391
[1mStep[0m  [6/21], [94mLoss[0m : 2.53787
[1mStep[0m  [8/21], [94mLoss[0m : 2.57090
[1mStep[0m  [10/21], [94mLoss[0m : 2.48535
[1mStep[0m  [12/21], [94mLoss[0m : 2.56184
[1mStep[0m  [14/21], [94mLoss[0m : 2.75872
[1mStep[0m  [16/21], [94mLoss[0m : 2.42521
[1mStep[0m  [18/21], [94mLoss[0m : 2.38391
[1mStep[0m  [20/21], [94mLoss[0m : 2.55246

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.472, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.47889
[1mStep[0m  [2/21], [94mLoss[0m : 2.29171
[1mStep[0m  [4/21], [94mLoss[0m : 2.46327
[1mStep[0m  [6/21], [94mLoss[0m : 2.59209
[1mStep[0m  [8/21], [94mLoss[0m : 2.45446
[1mStep[0m  [10/21], [94mLoss[0m : 2.42227
[1mStep[0m  [12/21], [94mLoss[0m : 2.54106
[1mStep[0m  [14/21], [94mLoss[0m : 2.42828
[1mStep[0m  [16/21], [94mLoss[0m : 2.42074
[1mStep[0m  [18/21], [94mLoss[0m : 2.67961
[1mStep[0m  [20/21], [94mLoss[0m : 2.58656

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.462, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53596
[1mStep[0m  [2/21], [94mLoss[0m : 2.54244
[1mStep[0m  [4/21], [94mLoss[0m : 2.44251
[1mStep[0m  [6/21], [94mLoss[0m : 2.49972
[1mStep[0m  [8/21], [94mLoss[0m : 2.52769
[1mStep[0m  [10/21], [94mLoss[0m : 2.52915
[1mStep[0m  [12/21], [94mLoss[0m : 2.57863
[1mStep[0m  [14/21], [94mLoss[0m : 2.58223
[1mStep[0m  [16/21], [94mLoss[0m : 2.52008
[1mStep[0m  [18/21], [94mLoss[0m : 2.49791
[1mStep[0m  [20/21], [94mLoss[0m : 2.45110

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.519, [92mTest[0m: 2.452, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.55825
[1mStep[0m  [2/21], [94mLoss[0m : 2.66919
[1mStep[0m  [4/21], [94mLoss[0m : 2.42827
[1mStep[0m  [6/21], [94mLoss[0m : 2.44630
[1mStep[0m  [8/21], [94mLoss[0m : 2.40851
[1mStep[0m  [10/21], [94mLoss[0m : 2.41869
[1mStep[0m  [12/21], [94mLoss[0m : 2.44271
[1mStep[0m  [14/21], [94mLoss[0m : 2.59025
[1mStep[0m  [16/21], [94mLoss[0m : 2.58915
[1mStep[0m  [18/21], [94mLoss[0m : 2.57680
[1mStep[0m  [20/21], [94mLoss[0m : 2.43104

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.455, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51903
[1mStep[0m  [2/21], [94mLoss[0m : 2.58595
[1mStep[0m  [4/21], [94mLoss[0m : 2.46869
[1mStep[0m  [6/21], [94mLoss[0m : 2.65659
[1mStep[0m  [8/21], [94mLoss[0m : 2.49746
[1mStep[0m  [10/21], [94mLoss[0m : 2.45860
[1mStep[0m  [12/21], [94mLoss[0m : 2.49807
[1mStep[0m  [14/21], [94mLoss[0m : 2.38818
[1mStep[0m  [16/21], [94mLoss[0m : 2.41452
[1mStep[0m  [18/21], [94mLoss[0m : 2.42485
[1mStep[0m  [20/21], [94mLoss[0m : 2.46757

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.452, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56650
[1mStep[0m  [2/21], [94mLoss[0m : 2.48732
[1mStep[0m  [4/21], [94mLoss[0m : 2.66275
[1mStep[0m  [6/21], [94mLoss[0m : 2.50229
[1mStep[0m  [8/21], [94mLoss[0m : 2.42368
[1mStep[0m  [10/21], [94mLoss[0m : 2.36260
[1mStep[0m  [12/21], [94mLoss[0m : 2.59551
[1mStep[0m  [14/21], [94mLoss[0m : 2.40464
[1mStep[0m  [16/21], [94mLoss[0m : 2.52810
[1mStep[0m  [18/21], [94mLoss[0m : 2.39273
[1mStep[0m  [20/21], [94mLoss[0m : 2.41975

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.443, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46824
[1mStep[0m  [2/21], [94mLoss[0m : 2.38800
[1mStep[0m  [4/21], [94mLoss[0m : 2.39558
[1mStep[0m  [6/21], [94mLoss[0m : 2.51974
[1mStep[0m  [8/21], [94mLoss[0m : 2.66122
[1mStep[0m  [10/21], [94mLoss[0m : 2.55653
[1mStep[0m  [12/21], [94mLoss[0m : 2.45119
[1mStep[0m  [14/21], [94mLoss[0m : 2.59466
[1mStep[0m  [16/21], [94mLoss[0m : 2.48563
[1mStep[0m  [18/21], [94mLoss[0m : 2.64226
[1mStep[0m  [20/21], [94mLoss[0m : 2.39111

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.450, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.35929
[1mStep[0m  [2/21], [94mLoss[0m : 2.54643
[1mStep[0m  [4/21], [94mLoss[0m : 2.30806
[1mStep[0m  [6/21], [94mLoss[0m : 2.60671
[1mStep[0m  [8/21], [94mLoss[0m : 2.49884
[1mStep[0m  [10/21], [94mLoss[0m : 2.53104
[1mStep[0m  [12/21], [94mLoss[0m : 2.59443
[1mStep[0m  [14/21], [94mLoss[0m : 2.44547
[1mStep[0m  [16/21], [94mLoss[0m : 2.46897
[1mStep[0m  [18/21], [94mLoss[0m : 2.39127
[1mStep[0m  [20/21], [94mLoss[0m : 2.47849

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.440, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56056
[1mStep[0m  [2/21], [94mLoss[0m : 2.46982
[1mStep[0m  [4/21], [94mLoss[0m : 2.42088
[1mStep[0m  [6/21], [94mLoss[0m : 2.51761
[1mStep[0m  [8/21], [94mLoss[0m : 2.36235
[1mStep[0m  [10/21], [94mLoss[0m : 2.39596
[1mStep[0m  [12/21], [94mLoss[0m : 2.49532
[1mStep[0m  [14/21], [94mLoss[0m : 2.41237
[1mStep[0m  [16/21], [94mLoss[0m : 2.47395
[1mStep[0m  [18/21], [94mLoss[0m : 2.49199
[1mStep[0m  [20/21], [94mLoss[0m : 2.58861

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.437, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52305
[1mStep[0m  [2/21], [94mLoss[0m : 2.46144
[1mStep[0m  [4/21], [94mLoss[0m : 2.44846
[1mStep[0m  [6/21], [94mLoss[0m : 2.55069
[1mStep[0m  [8/21], [94mLoss[0m : 2.47647
[1mStep[0m  [10/21], [94mLoss[0m : 2.48178
[1mStep[0m  [12/21], [94mLoss[0m : 2.50345
[1mStep[0m  [14/21], [94mLoss[0m : 2.46564
[1mStep[0m  [16/21], [94mLoss[0m : 2.47406
[1mStep[0m  [18/21], [94mLoss[0m : 2.51144
[1mStep[0m  [20/21], [94mLoss[0m : 2.48550

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.428, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.50479
[1mStep[0m  [2/21], [94mLoss[0m : 2.40727
[1mStep[0m  [4/21], [94mLoss[0m : 2.53138
[1mStep[0m  [6/21], [94mLoss[0m : 2.50327
[1mStep[0m  [8/21], [94mLoss[0m : 2.39125
[1mStep[0m  [10/21], [94mLoss[0m : 2.48378
[1mStep[0m  [12/21], [94mLoss[0m : 2.46275
[1mStep[0m  [14/21], [94mLoss[0m : 2.59039
[1mStep[0m  [16/21], [94mLoss[0m : 2.41602
[1mStep[0m  [18/21], [94mLoss[0m : 2.40062
[1mStep[0m  [20/21], [94mLoss[0m : 2.29332

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.430, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.65738
[1mStep[0m  [2/21], [94mLoss[0m : 2.37720
[1mStep[0m  [4/21], [94mLoss[0m : 2.45245
[1mStep[0m  [6/21], [94mLoss[0m : 2.50952
[1mStep[0m  [8/21], [94mLoss[0m : 2.49839
[1mStep[0m  [10/21], [94mLoss[0m : 2.45471
[1mStep[0m  [12/21], [94mLoss[0m : 2.34782
[1mStep[0m  [14/21], [94mLoss[0m : 2.55924
[1mStep[0m  [16/21], [94mLoss[0m : 2.56179
[1mStep[0m  [18/21], [94mLoss[0m : 2.40570
[1mStep[0m  [20/21], [94mLoss[0m : 2.62737

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.433, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44134
[1mStep[0m  [2/21], [94mLoss[0m : 2.53812
[1mStep[0m  [4/21], [94mLoss[0m : 2.49890
[1mStep[0m  [6/21], [94mLoss[0m : 2.37393
[1mStep[0m  [8/21], [94mLoss[0m : 2.35398
[1mStep[0m  [10/21], [94mLoss[0m : 2.62386
[1mStep[0m  [12/21], [94mLoss[0m : 2.46970
[1mStep[0m  [14/21], [94mLoss[0m : 2.44367
[1mStep[0m  [16/21], [94mLoss[0m : 2.39841
[1mStep[0m  [18/21], [94mLoss[0m : 2.45324
[1mStep[0m  [20/21], [94mLoss[0m : 2.33318

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.427, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52184
[1mStep[0m  [2/21], [94mLoss[0m : 2.61015
[1mStep[0m  [4/21], [94mLoss[0m : 2.50484
[1mStep[0m  [6/21], [94mLoss[0m : 2.40081
[1mStep[0m  [8/21], [94mLoss[0m : 2.40770
[1mStep[0m  [10/21], [94mLoss[0m : 2.62303
[1mStep[0m  [12/21], [94mLoss[0m : 2.45684
[1mStep[0m  [14/21], [94mLoss[0m : 2.49747
[1mStep[0m  [16/21], [94mLoss[0m : 2.57277
[1mStep[0m  [18/21], [94mLoss[0m : 2.38599
[1mStep[0m  [20/21], [94mLoss[0m : 2.52819

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.420, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.41122
[1mStep[0m  [2/21], [94mLoss[0m : 2.37850
[1mStep[0m  [4/21], [94mLoss[0m : 2.44842
[1mStep[0m  [6/21], [94mLoss[0m : 2.55038
[1mStep[0m  [8/21], [94mLoss[0m : 2.52540
[1mStep[0m  [10/21], [94mLoss[0m : 2.44353
[1mStep[0m  [12/21], [94mLoss[0m : 2.60155
[1mStep[0m  [14/21], [94mLoss[0m : 2.49610
[1mStep[0m  [16/21], [94mLoss[0m : 2.32106
[1mStep[0m  [18/21], [94mLoss[0m : 2.63314
[1mStep[0m  [20/21], [94mLoss[0m : 2.39384

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.421, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.29568
[1mStep[0m  [2/21], [94mLoss[0m : 2.27380
[1mStep[0m  [4/21], [94mLoss[0m : 2.66265
[1mStep[0m  [6/21], [94mLoss[0m : 2.62481
[1mStep[0m  [8/21], [94mLoss[0m : 2.45553
[1mStep[0m  [10/21], [94mLoss[0m : 2.42671
[1mStep[0m  [12/21], [94mLoss[0m : 2.38442
[1mStep[0m  [14/21], [94mLoss[0m : 2.59524
[1mStep[0m  [16/21], [94mLoss[0m : 2.46589
[1mStep[0m  [18/21], [94mLoss[0m : 2.47888
[1mStep[0m  [20/21], [94mLoss[0m : 2.64886

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.412, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.413
====================================

Phase 1 - Evaluation MAE:  2.412783179964338
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/21], [94mLoss[0m : 2.33301
[1mStep[0m  [2/21], [94mLoss[0m : 2.55212
[1mStep[0m  [4/21], [94mLoss[0m : 2.55221
[1mStep[0m  [6/21], [94mLoss[0m : 2.51647
[1mStep[0m  [8/21], [94mLoss[0m : 2.31268
[1mStep[0m  [10/21], [94mLoss[0m : 2.43775
[1mStep[0m  [12/21], [94mLoss[0m : 2.63297
[1mStep[0m  [14/21], [94mLoss[0m : 2.63044
[1mStep[0m  [16/21], [94mLoss[0m : 2.61361
[1mStep[0m  [18/21], [94mLoss[0m : 2.57416
[1mStep[0m  [20/21], [94mLoss[0m : 2.48856

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.413, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.60134
[1mStep[0m  [2/21], [94mLoss[0m : 2.56170
[1mStep[0m  [4/21], [94mLoss[0m : 2.44618
[1mStep[0m  [6/21], [94mLoss[0m : 2.31243
[1mStep[0m  [8/21], [94mLoss[0m : 2.42655
[1mStep[0m  [10/21], [94mLoss[0m : 2.47756
[1mStep[0m  [12/21], [94mLoss[0m : 2.37888
[1mStep[0m  [14/21], [94mLoss[0m : 2.57296
[1mStep[0m  [16/21], [94mLoss[0m : 2.47662
[1mStep[0m  [18/21], [94mLoss[0m : 2.48792
[1mStep[0m  [20/21], [94mLoss[0m : 2.63582

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.405, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.39011
[1mStep[0m  [2/21], [94mLoss[0m : 2.45406
[1mStep[0m  [4/21], [94mLoss[0m : 2.58397
[1mStep[0m  [6/21], [94mLoss[0m : 2.42897
[1mStep[0m  [8/21], [94mLoss[0m : 2.45725
[1mStep[0m  [10/21], [94mLoss[0m : 2.44094
[1mStep[0m  [12/21], [94mLoss[0m : 2.48062
[1mStep[0m  [14/21], [94mLoss[0m : 2.54041
[1mStep[0m  [16/21], [94mLoss[0m : 2.54449
[1mStep[0m  [18/21], [94mLoss[0m : 2.34037
[1mStep[0m  [20/21], [94mLoss[0m : 2.45261

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.394, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.47470
[1mStep[0m  [2/21], [94mLoss[0m : 2.48656
[1mStep[0m  [4/21], [94mLoss[0m : 2.37953
[1mStep[0m  [6/21], [94mLoss[0m : 2.42422
[1mStep[0m  [8/21], [94mLoss[0m : 2.49119
[1mStep[0m  [10/21], [94mLoss[0m : 2.49117
[1mStep[0m  [12/21], [94mLoss[0m : 2.42523
[1mStep[0m  [14/21], [94mLoss[0m : 2.33978
[1mStep[0m  [16/21], [94mLoss[0m : 2.51470
[1mStep[0m  [18/21], [94mLoss[0m : 2.53104
[1mStep[0m  [20/21], [94mLoss[0m : 2.62939

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.396, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44412
[1mStep[0m  [2/21], [94mLoss[0m : 2.45027
[1mStep[0m  [4/21], [94mLoss[0m : 2.19264
[1mStep[0m  [6/21], [94mLoss[0m : 2.42734
[1mStep[0m  [8/21], [94mLoss[0m : 2.49253
[1mStep[0m  [10/21], [94mLoss[0m : 2.54937
[1mStep[0m  [12/21], [94mLoss[0m : 2.41553
[1mStep[0m  [14/21], [94mLoss[0m : 2.40976
[1mStep[0m  [16/21], [94mLoss[0m : 2.51256
[1mStep[0m  [18/21], [94mLoss[0m : 2.47743
[1mStep[0m  [20/21], [94mLoss[0m : 2.70068

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.389, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52411
[1mStep[0m  [2/21], [94mLoss[0m : 2.41998
[1mStep[0m  [4/21], [94mLoss[0m : 2.39960
[1mStep[0m  [6/21], [94mLoss[0m : 2.56838
[1mStep[0m  [8/21], [94mLoss[0m : 2.44003
[1mStep[0m  [10/21], [94mLoss[0m : 2.40521
[1mStep[0m  [12/21], [94mLoss[0m : 2.39196
[1mStep[0m  [14/21], [94mLoss[0m : 2.42728
[1mStep[0m  [16/21], [94mLoss[0m : 2.53300
[1mStep[0m  [18/21], [94mLoss[0m : 2.35574
[1mStep[0m  [20/21], [94mLoss[0m : 2.46271

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.386, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.23662
[1mStep[0m  [2/21], [94mLoss[0m : 2.49482
[1mStep[0m  [4/21], [94mLoss[0m : 2.51275
[1mStep[0m  [6/21], [94mLoss[0m : 2.40314
[1mStep[0m  [8/21], [94mLoss[0m : 2.33210
[1mStep[0m  [10/21], [94mLoss[0m : 2.38825
[1mStep[0m  [12/21], [94mLoss[0m : 2.62987
[1mStep[0m  [14/21], [94mLoss[0m : 2.27749
[1mStep[0m  [16/21], [94mLoss[0m : 2.61263
[1mStep[0m  [18/21], [94mLoss[0m : 2.40356
[1mStep[0m  [20/21], [94mLoss[0m : 2.31042

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.393, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40273
[1mStep[0m  [2/21], [94mLoss[0m : 2.26725
[1mStep[0m  [4/21], [94mLoss[0m : 2.51053
[1mStep[0m  [6/21], [94mLoss[0m : 2.35539
[1mStep[0m  [8/21], [94mLoss[0m : 2.41457
[1mStep[0m  [10/21], [94mLoss[0m : 2.44394
[1mStep[0m  [12/21], [94mLoss[0m : 2.48592
[1mStep[0m  [14/21], [94mLoss[0m : 2.36183
[1mStep[0m  [16/21], [94mLoss[0m : 2.53531
[1mStep[0m  [18/21], [94mLoss[0m : 2.43612
[1mStep[0m  [20/21], [94mLoss[0m : 2.43908

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.396, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53175
[1mStep[0m  [2/21], [94mLoss[0m : 2.36949
[1mStep[0m  [4/21], [94mLoss[0m : 2.40753
[1mStep[0m  [6/21], [94mLoss[0m : 2.41683
[1mStep[0m  [8/21], [94mLoss[0m : 2.44885
[1mStep[0m  [10/21], [94mLoss[0m : 2.49393
[1mStep[0m  [12/21], [94mLoss[0m : 2.42060
[1mStep[0m  [14/21], [94mLoss[0m : 2.33117
[1mStep[0m  [16/21], [94mLoss[0m : 2.51363
[1mStep[0m  [18/21], [94mLoss[0m : 2.41644
[1mStep[0m  [20/21], [94mLoss[0m : 2.48652

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.394, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.30191
[1mStep[0m  [2/21], [94mLoss[0m : 2.63550
[1mStep[0m  [4/21], [94mLoss[0m : 2.45572
[1mStep[0m  [6/21], [94mLoss[0m : 2.28486
[1mStep[0m  [8/21], [94mLoss[0m : 2.51409
[1mStep[0m  [10/21], [94mLoss[0m : 2.52643
[1mStep[0m  [12/21], [94mLoss[0m : 2.47394
[1mStep[0m  [14/21], [94mLoss[0m : 2.29288
[1mStep[0m  [16/21], [94mLoss[0m : 2.38614
[1mStep[0m  [18/21], [94mLoss[0m : 2.26774
[1mStep[0m  [20/21], [94mLoss[0m : 2.42161

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.368, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.34883
[1mStep[0m  [2/21], [94mLoss[0m : 2.56062
[1mStep[0m  [4/21], [94mLoss[0m : 2.40111
[1mStep[0m  [6/21], [94mLoss[0m : 2.37872
[1mStep[0m  [8/21], [94mLoss[0m : 2.38703
[1mStep[0m  [10/21], [94mLoss[0m : 2.39065
[1mStep[0m  [12/21], [94mLoss[0m : 2.29073
[1mStep[0m  [14/21], [94mLoss[0m : 2.49492
[1mStep[0m  [16/21], [94mLoss[0m : 2.38037
[1mStep[0m  [18/21], [94mLoss[0m : 2.48521
[1mStep[0m  [20/21], [94mLoss[0m : 2.57113

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.382, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.33822
[1mStep[0m  [2/21], [94mLoss[0m : 2.23531
[1mStep[0m  [4/21], [94mLoss[0m : 2.46275
[1mStep[0m  [6/21], [94mLoss[0m : 2.43944
[1mStep[0m  [8/21], [94mLoss[0m : 2.43861
[1mStep[0m  [10/21], [94mLoss[0m : 2.49832
[1mStep[0m  [12/21], [94mLoss[0m : 2.51343
[1mStep[0m  [14/21], [94mLoss[0m : 2.31198
[1mStep[0m  [16/21], [94mLoss[0m : 2.21546
[1mStep[0m  [18/21], [94mLoss[0m : 2.45588
[1mStep[0m  [20/21], [94mLoss[0m : 2.48989

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.371, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46323
[1mStep[0m  [2/21], [94mLoss[0m : 2.32235
[1mStep[0m  [4/21], [94mLoss[0m : 2.28789
[1mStep[0m  [6/21], [94mLoss[0m : 2.32308
[1mStep[0m  [8/21], [94mLoss[0m : 2.51498
[1mStep[0m  [10/21], [94mLoss[0m : 2.34567
[1mStep[0m  [12/21], [94mLoss[0m : 2.51849
[1mStep[0m  [14/21], [94mLoss[0m : 2.36935
[1mStep[0m  [16/21], [94mLoss[0m : 2.49269
[1mStep[0m  [18/21], [94mLoss[0m : 2.51053
[1mStep[0m  [20/21], [94mLoss[0m : 2.51579

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.380, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51377
[1mStep[0m  [2/21], [94mLoss[0m : 2.75146
[1mStep[0m  [4/21], [94mLoss[0m : 2.32483
[1mStep[0m  [6/21], [94mLoss[0m : 2.22839
[1mStep[0m  [8/21], [94mLoss[0m : 2.62246
[1mStep[0m  [10/21], [94mLoss[0m : 2.45015
[1mStep[0m  [12/21], [94mLoss[0m : 2.54010
[1mStep[0m  [14/21], [94mLoss[0m : 2.24684
[1mStep[0m  [16/21], [94mLoss[0m : 2.36916
[1mStep[0m  [18/21], [94mLoss[0m : 2.43168
[1mStep[0m  [20/21], [94mLoss[0m : 2.35698

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.367, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.36458
[1mStep[0m  [2/21], [94mLoss[0m : 2.24793
[1mStep[0m  [4/21], [94mLoss[0m : 2.52039
[1mStep[0m  [6/21], [94mLoss[0m : 2.50110
[1mStep[0m  [8/21], [94mLoss[0m : 2.40337
[1mStep[0m  [10/21], [94mLoss[0m : 2.26101
[1mStep[0m  [12/21], [94mLoss[0m : 2.35851
[1mStep[0m  [14/21], [94mLoss[0m : 2.41737
[1mStep[0m  [16/21], [94mLoss[0m : 2.37084
[1mStep[0m  [18/21], [94mLoss[0m : 2.37856
[1mStep[0m  [20/21], [94mLoss[0m : 2.36098

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.364, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43265
[1mStep[0m  [2/21], [94mLoss[0m : 2.32885
[1mStep[0m  [4/21], [94mLoss[0m : 2.35182
[1mStep[0m  [6/21], [94mLoss[0m : 2.45271
[1mStep[0m  [8/21], [94mLoss[0m : 2.34632
[1mStep[0m  [10/21], [94mLoss[0m : 2.40038
[1mStep[0m  [12/21], [94mLoss[0m : 2.35009
[1mStep[0m  [14/21], [94mLoss[0m : 2.41456
[1mStep[0m  [16/21], [94mLoss[0m : 2.55796
[1mStep[0m  [18/21], [94mLoss[0m : 2.37713
[1mStep[0m  [20/21], [94mLoss[0m : 2.35021

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.395, [92mTest[0m: 2.363, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51869
[1mStep[0m  [2/21], [94mLoss[0m : 2.32158
[1mStep[0m  [4/21], [94mLoss[0m : 2.33889
[1mStep[0m  [6/21], [94mLoss[0m : 2.43185
[1mStep[0m  [8/21], [94mLoss[0m : 2.40407
[1mStep[0m  [10/21], [94mLoss[0m : 2.21325
[1mStep[0m  [12/21], [94mLoss[0m : 2.25378
[1mStep[0m  [14/21], [94mLoss[0m : 2.54588
[1mStep[0m  [16/21], [94mLoss[0m : 2.42283
[1mStep[0m  [18/21], [94mLoss[0m : 2.33232
[1mStep[0m  [20/21], [94mLoss[0m : 2.42940

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.369, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40007
[1mStep[0m  [2/21], [94mLoss[0m : 2.22875
[1mStep[0m  [4/21], [94mLoss[0m : 2.46289
[1mStep[0m  [6/21], [94mLoss[0m : 2.42500
[1mStep[0m  [8/21], [94mLoss[0m : 2.37022
[1mStep[0m  [10/21], [94mLoss[0m : 2.45836
[1mStep[0m  [12/21], [94mLoss[0m : 2.27145
[1mStep[0m  [14/21], [94mLoss[0m : 2.49898
[1mStep[0m  [16/21], [94mLoss[0m : 2.58208
[1mStep[0m  [18/21], [94mLoss[0m : 2.24579
[1mStep[0m  [20/21], [94mLoss[0m : 2.42071

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.353, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.39549
[1mStep[0m  [2/21], [94mLoss[0m : 2.34516
[1mStep[0m  [4/21], [94mLoss[0m : 2.34255
[1mStep[0m  [6/21], [94mLoss[0m : 2.36436
[1mStep[0m  [8/21], [94mLoss[0m : 2.36577
[1mStep[0m  [10/21], [94mLoss[0m : 2.43079
[1mStep[0m  [12/21], [94mLoss[0m : 2.35892
[1mStep[0m  [14/21], [94mLoss[0m : 2.26727
[1mStep[0m  [16/21], [94mLoss[0m : 2.39796
[1mStep[0m  [18/21], [94mLoss[0m : 2.34725
[1mStep[0m  [20/21], [94mLoss[0m : 2.33811

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.388, [92mTest[0m: 2.360, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.24826
[1mStep[0m  [2/21], [94mLoss[0m : 2.33699
[1mStep[0m  [4/21], [94mLoss[0m : 2.33966
[1mStep[0m  [6/21], [94mLoss[0m : 2.54737
[1mStep[0m  [8/21], [94mLoss[0m : 2.35231
[1mStep[0m  [10/21], [94mLoss[0m : 2.49541
[1mStep[0m  [12/21], [94mLoss[0m : 2.44412
[1mStep[0m  [14/21], [94mLoss[0m : 2.39409
[1mStep[0m  [16/21], [94mLoss[0m : 2.51433
[1mStep[0m  [18/21], [94mLoss[0m : 2.36594
[1mStep[0m  [20/21], [94mLoss[0m : 2.25305

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.388, [92mTest[0m: 2.349, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48980
[1mStep[0m  [2/21], [94mLoss[0m : 2.39462
[1mStep[0m  [4/21], [94mLoss[0m : 2.37036
[1mStep[0m  [6/21], [94mLoss[0m : 2.40223
[1mStep[0m  [8/21], [94mLoss[0m : 2.36694
[1mStep[0m  [10/21], [94mLoss[0m : 2.39789
[1mStep[0m  [12/21], [94mLoss[0m : 2.44369
[1mStep[0m  [14/21], [94mLoss[0m : 2.36511
[1mStep[0m  [16/21], [94mLoss[0m : 2.34353
[1mStep[0m  [18/21], [94mLoss[0m : 2.41865
[1mStep[0m  [20/21], [94mLoss[0m : 2.40597

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.385, [92mTest[0m: 2.348, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46575
[1mStep[0m  [2/21], [94mLoss[0m : 2.45528
[1mStep[0m  [4/21], [94mLoss[0m : 2.30912
[1mStep[0m  [6/21], [94mLoss[0m : 2.34155
[1mStep[0m  [8/21], [94mLoss[0m : 2.37367
[1mStep[0m  [10/21], [94mLoss[0m : 2.41002
[1mStep[0m  [12/21], [94mLoss[0m : 2.34043
[1mStep[0m  [14/21], [94mLoss[0m : 2.38924
[1mStep[0m  [16/21], [94mLoss[0m : 2.38620
[1mStep[0m  [18/21], [94mLoss[0m : 2.34680
[1mStep[0m  [20/21], [94mLoss[0m : 2.29423

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.385, [92mTest[0m: 2.350, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.38309
[1mStep[0m  [2/21], [94mLoss[0m : 2.21367
[1mStep[0m  [4/21], [94mLoss[0m : 2.34406
[1mStep[0m  [6/21], [94mLoss[0m : 2.30654
[1mStep[0m  [8/21], [94mLoss[0m : 2.44833
[1mStep[0m  [10/21], [94mLoss[0m : 2.64735
[1mStep[0m  [12/21], [94mLoss[0m : 2.21760
[1mStep[0m  [14/21], [94mLoss[0m : 2.27494
[1mStep[0m  [16/21], [94mLoss[0m : 2.37126
[1mStep[0m  [18/21], [94mLoss[0m : 2.45766
[1mStep[0m  [20/21], [94mLoss[0m : 2.35808

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.377, [92mTest[0m: 2.352, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.34036
[1mStep[0m  [2/21], [94mLoss[0m : 2.28814
[1mStep[0m  [4/21], [94mLoss[0m : 2.31050
[1mStep[0m  [6/21], [94mLoss[0m : 2.44421
[1mStep[0m  [8/21], [94mLoss[0m : 2.39005
[1mStep[0m  [10/21], [94mLoss[0m : 2.30191
[1mStep[0m  [12/21], [94mLoss[0m : 2.21102
[1mStep[0m  [14/21], [94mLoss[0m : 2.34638
[1mStep[0m  [16/21], [94mLoss[0m : 2.56317
[1mStep[0m  [18/21], [94mLoss[0m : 2.23267
[1mStep[0m  [20/21], [94mLoss[0m : 2.40747

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.372, [92mTest[0m: 2.347, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.30163
[1mStep[0m  [2/21], [94mLoss[0m : 2.46248
[1mStep[0m  [4/21], [94mLoss[0m : 2.32546
[1mStep[0m  [6/21], [94mLoss[0m : 2.42216
[1mStep[0m  [8/21], [94mLoss[0m : 2.42985
[1mStep[0m  [10/21], [94mLoss[0m : 2.28836
[1mStep[0m  [12/21], [94mLoss[0m : 2.37647
[1mStep[0m  [14/21], [94mLoss[0m : 2.50127
[1mStep[0m  [16/21], [94mLoss[0m : 2.37228
[1mStep[0m  [18/21], [94mLoss[0m : 2.16920
[1mStep[0m  [20/21], [94mLoss[0m : 2.34147

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.379, [92mTest[0m: 2.366, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.37294
[1mStep[0m  [2/21], [94mLoss[0m : 2.27088
[1mStep[0m  [4/21], [94mLoss[0m : 2.35294
[1mStep[0m  [6/21], [94mLoss[0m : 2.44114
[1mStep[0m  [8/21], [94mLoss[0m : 2.30759
[1mStep[0m  [10/21], [94mLoss[0m : 2.34049
[1mStep[0m  [12/21], [94mLoss[0m : 2.39162
[1mStep[0m  [14/21], [94mLoss[0m : 2.27258
[1mStep[0m  [16/21], [94mLoss[0m : 2.42475
[1mStep[0m  [18/21], [94mLoss[0m : 2.42491
[1mStep[0m  [20/21], [94mLoss[0m : 2.31973

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.355, [92mTest[0m: 2.337, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46558
[1mStep[0m  [2/21], [94mLoss[0m : 2.38628
[1mStep[0m  [4/21], [94mLoss[0m : 2.43618
[1mStep[0m  [6/21], [94mLoss[0m : 2.29910
[1mStep[0m  [8/21], [94mLoss[0m : 2.41555
[1mStep[0m  [10/21], [94mLoss[0m : 2.34169
[1mStep[0m  [12/21], [94mLoss[0m : 2.30710
[1mStep[0m  [14/21], [94mLoss[0m : 2.32672
[1mStep[0m  [16/21], [94mLoss[0m : 2.25647
[1mStep[0m  [18/21], [94mLoss[0m : 2.35493
[1mStep[0m  [20/21], [94mLoss[0m : 2.26321

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.360, [92mTest[0m: 2.369, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54248
[1mStep[0m  [2/21], [94mLoss[0m : 2.52630
[1mStep[0m  [4/21], [94mLoss[0m : 2.22520
[1mStep[0m  [6/21], [94mLoss[0m : 2.45769
[1mStep[0m  [8/21], [94mLoss[0m : 2.34145
[1mStep[0m  [10/21], [94mLoss[0m : 2.37744
[1mStep[0m  [12/21], [94mLoss[0m : 2.26871
[1mStep[0m  [14/21], [94mLoss[0m : 2.22039
[1mStep[0m  [16/21], [94mLoss[0m : 2.40818
[1mStep[0m  [18/21], [94mLoss[0m : 2.32999
[1mStep[0m  [20/21], [94mLoss[0m : 2.23279

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.359, [92mTest[0m: 2.340, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.30990
[1mStep[0m  [2/21], [94mLoss[0m : 2.27513
[1mStep[0m  [4/21], [94mLoss[0m : 2.29594
[1mStep[0m  [6/21], [94mLoss[0m : 2.37084
[1mStep[0m  [8/21], [94mLoss[0m : 2.56240
[1mStep[0m  [10/21], [94mLoss[0m : 2.37704
[1mStep[0m  [12/21], [94mLoss[0m : 2.41369
[1mStep[0m  [14/21], [94mLoss[0m : 2.37003
[1mStep[0m  [16/21], [94mLoss[0m : 2.33342
[1mStep[0m  [18/21], [94mLoss[0m : 2.32274
[1mStep[0m  [20/21], [94mLoss[0m : 2.34943

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.362, [92mTest[0m: 2.341, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.38646
[1mStep[0m  [2/21], [94mLoss[0m : 2.34275
[1mStep[0m  [4/21], [94mLoss[0m : 2.17206
[1mStep[0m  [6/21], [94mLoss[0m : 2.28321
[1mStep[0m  [8/21], [94mLoss[0m : 2.23755
[1mStep[0m  [10/21], [94mLoss[0m : 2.50898
[1mStep[0m  [12/21], [94mLoss[0m : 2.40970
[1mStep[0m  [14/21], [94mLoss[0m : 2.46103
[1mStep[0m  [16/21], [94mLoss[0m : 2.33103
[1mStep[0m  [18/21], [94mLoss[0m : 2.26471
[1mStep[0m  [20/21], [94mLoss[0m : 2.60893

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.360, [92mTest[0m: 2.340, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.338
====================================

Phase 2 - Evaluation MAE:  2.337771075112479
MAE score P1      2.412783
MAE score P2      2.337771
loss              2.355133
learning_rate     0.002575
batch_size             512
hidden_sizes         [300]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.1
weight_decay          0.01
Name: 14, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 11.30300
[1mStep[0m  [4/42], [94mLoss[0m : 10.75263
[1mStep[0m  [8/42], [94mLoss[0m : 9.80978
[1mStep[0m  [12/42], [94mLoss[0m : 9.74837
[1mStep[0m  [16/42], [94mLoss[0m : 9.19011
[1mStep[0m  [20/42], [94mLoss[0m : 8.62570
[1mStep[0m  [24/42], [94mLoss[0m : 8.03841
[1mStep[0m  [28/42], [94mLoss[0m : 7.80572
[1mStep[0m  [32/42], [94mLoss[0m : 6.89191
[1mStep[0m  [36/42], [94mLoss[0m : 6.34203
[1mStep[0m  [40/42], [94mLoss[0m : 6.45644

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 8.563, [92mTest[0m: 10.972, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.42799
[1mStep[0m  [4/42], [94mLoss[0m : 5.67545
[1mStep[0m  [8/42], [94mLoss[0m : 5.11208
[1mStep[0m  [12/42], [94mLoss[0m : 5.17760
[1mStep[0m  [16/42], [94mLoss[0m : 4.69660
[1mStep[0m  [20/42], [94mLoss[0m : 4.57236
[1mStep[0m  [24/42], [94mLoss[0m : 4.27060
[1mStep[0m  [28/42], [94mLoss[0m : 3.66204
[1mStep[0m  [32/42], [94mLoss[0m : 3.55346
[1mStep[0m  [36/42], [94mLoss[0m : 3.38589
[1mStep[0m  [40/42], [94mLoss[0m : 3.74811

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 4.506, [92mTest[0m: 7.054, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.22741
[1mStep[0m  [4/42], [94mLoss[0m : 3.15986
[1mStep[0m  [8/42], [94mLoss[0m : 3.07685
[1mStep[0m  [12/42], [94mLoss[0m : 2.99216
[1mStep[0m  [16/42], [94mLoss[0m : 3.02981
[1mStep[0m  [20/42], [94mLoss[0m : 2.91562
[1mStep[0m  [24/42], [94mLoss[0m : 2.69923
[1mStep[0m  [28/42], [94mLoss[0m : 2.59222
[1mStep[0m  [32/42], [94mLoss[0m : 2.71715
[1mStep[0m  [36/42], [94mLoss[0m : 2.50563
[1mStep[0m  [40/42], [94mLoss[0m : 2.81185

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.917, [92mTest[0m: 4.014, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.69019
[1mStep[0m  [4/42], [94mLoss[0m : 2.64685
[1mStep[0m  [8/42], [94mLoss[0m : 2.70307
[1mStep[0m  [12/42], [94mLoss[0m : 2.80325
[1mStep[0m  [16/42], [94mLoss[0m : 2.47417
[1mStep[0m  [20/42], [94mLoss[0m : 2.59905
[1mStep[0m  [24/42], [94mLoss[0m : 2.58978
[1mStep[0m  [28/42], [94mLoss[0m : 2.52946
[1mStep[0m  [32/42], [94mLoss[0m : 2.55360
[1mStep[0m  [36/42], [94mLoss[0m : 2.72672
[1mStep[0m  [40/42], [94mLoss[0m : 2.59577

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.652, [92mTest[0m: 2.911, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.69197
[1mStep[0m  [4/42], [94mLoss[0m : 2.81050
[1mStep[0m  [8/42], [94mLoss[0m : 2.17893
[1mStep[0m  [12/42], [94mLoss[0m : 2.64452
[1mStep[0m  [16/42], [94mLoss[0m : 2.77169
[1mStep[0m  [20/42], [94mLoss[0m : 2.54272
[1mStep[0m  [24/42], [94mLoss[0m : 2.60267
[1mStep[0m  [28/42], [94mLoss[0m : 2.85877
[1mStep[0m  [32/42], [94mLoss[0m : 2.83947
[1mStep[0m  [36/42], [94mLoss[0m : 2.54918
[1mStep[0m  [40/42], [94mLoss[0m : 2.57884

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.625, [92mTest[0m: 2.712, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.81753
[1mStep[0m  [4/42], [94mLoss[0m : 2.62720
[1mStep[0m  [8/42], [94mLoss[0m : 2.40591
[1mStep[0m  [12/42], [94mLoss[0m : 2.56457
[1mStep[0m  [16/42], [94mLoss[0m : 2.64891
[1mStep[0m  [20/42], [94mLoss[0m : 2.63755
[1mStep[0m  [24/42], [94mLoss[0m : 2.61710
[1mStep[0m  [28/42], [94mLoss[0m : 2.49985
[1mStep[0m  [32/42], [94mLoss[0m : 2.84671
[1mStep[0m  [36/42], [94mLoss[0m : 2.55217
[1mStep[0m  [40/42], [94mLoss[0m : 2.49649

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.584, [92mTest[0m: 2.659, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62487
[1mStep[0m  [4/42], [94mLoss[0m : 2.44869
[1mStep[0m  [8/42], [94mLoss[0m : 2.62752
[1mStep[0m  [12/42], [94mLoss[0m : 2.69638
[1mStep[0m  [16/42], [94mLoss[0m : 2.57721
[1mStep[0m  [20/42], [94mLoss[0m : 2.86347
[1mStep[0m  [24/42], [94mLoss[0m : 2.30541
[1mStep[0m  [28/42], [94mLoss[0m : 2.50019
[1mStep[0m  [32/42], [94mLoss[0m : 2.63306
[1mStep[0m  [36/42], [94mLoss[0m : 2.40636
[1mStep[0m  [40/42], [94mLoss[0m : 2.51205

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.568, [92mTest[0m: 2.655, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.87419
[1mStep[0m  [4/42], [94mLoss[0m : 2.50415
[1mStep[0m  [8/42], [94mLoss[0m : 2.47072
[1mStep[0m  [12/42], [94mLoss[0m : 2.56930
[1mStep[0m  [16/42], [94mLoss[0m : 2.38519
[1mStep[0m  [20/42], [94mLoss[0m : 2.55354
[1mStep[0m  [24/42], [94mLoss[0m : 2.40703
[1mStep[0m  [28/42], [94mLoss[0m : 2.60398
[1mStep[0m  [32/42], [94mLoss[0m : 2.64244
[1mStep[0m  [36/42], [94mLoss[0m : 2.36629
[1mStep[0m  [40/42], [94mLoss[0m : 2.62233

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.610, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56446
[1mStep[0m  [4/42], [94mLoss[0m : 2.55236
[1mStep[0m  [8/42], [94mLoss[0m : 2.53300
[1mStep[0m  [12/42], [94mLoss[0m : 2.53101
[1mStep[0m  [16/42], [94mLoss[0m : 2.41890
[1mStep[0m  [20/42], [94mLoss[0m : 2.56563
[1mStep[0m  [24/42], [94mLoss[0m : 2.46921
[1mStep[0m  [28/42], [94mLoss[0m : 2.71879
[1mStep[0m  [32/42], [94mLoss[0m : 2.49761
[1mStep[0m  [36/42], [94mLoss[0m : 2.87359
[1mStep[0m  [40/42], [94mLoss[0m : 2.53691

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.548, [92mTest[0m: 2.620, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49946
[1mStep[0m  [4/42], [94mLoss[0m : 2.46135
[1mStep[0m  [8/42], [94mLoss[0m : 2.66046
[1mStep[0m  [12/42], [94mLoss[0m : 2.25196
[1mStep[0m  [16/42], [94mLoss[0m : 2.38753
[1mStep[0m  [20/42], [94mLoss[0m : 2.60174
[1mStep[0m  [24/42], [94mLoss[0m : 2.66818
[1mStep[0m  [28/42], [94mLoss[0m : 2.57677
[1mStep[0m  [32/42], [94mLoss[0m : 2.31386
[1mStep[0m  [36/42], [94mLoss[0m : 2.36055
[1mStep[0m  [40/42], [94mLoss[0m : 2.51175

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.597, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.72700
[1mStep[0m  [4/42], [94mLoss[0m : 2.58179
[1mStep[0m  [8/42], [94mLoss[0m : 2.45936
[1mStep[0m  [12/42], [94mLoss[0m : 2.41026
[1mStep[0m  [16/42], [94mLoss[0m : 2.47982
[1mStep[0m  [20/42], [94mLoss[0m : 2.63680
[1mStep[0m  [24/42], [94mLoss[0m : 2.44406
[1mStep[0m  [28/42], [94mLoss[0m : 2.80346
[1mStep[0m  [32/42], [94mLoss[0m : 2.42063
[1mStep[0m  [36/42], [94mLoss[0m : 2.70959
[1mStep[0m  [40/42], [94mLoss[0m : 2.77730

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.609, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44197
[1mStep[0m  [4/42], [94mLoss[0m : 2.27187
[1mStep[0m  [8/42], [94mLoss[0m : 2.61249
[1mStep[0m  [12/42], [94mLoss[0m : 2.57893
[1mStep[0m  [16/42], [94mLoss[0m : 2.38691
[1mStep[0m  [20/42], [94mLoss[0m : 2.67448
[1mStep[0m  [24/42], [94mLoss[0m : 2.56368
[1mStep[0m  [28/42], [94mLoss[0m : 2.59729
[1mStep[0m  [32/42], [94mLoss[0m : 2.72320
[1mStep[0m  [36/42], [94mLoss[0m : 2.48831
[1mStep[0m  [40/42], [94mLoss[0m : 2.59656

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.510, [92mTest[0m: 2.574, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.78325
[1mStep[0m  [4/42], [94mLoss[0m : 2.56574
[1mStep[0m  [8/42], [94mLoss[0m : 2.71625
[1mStep[0m  [12/42], [94mLoss[0m : 2.71680
[1mStep[0m  [16/42], [94mLoss[0m : 2.37116
[1mStep[0m  [20/42], [94mLoss[0m : 2.65165
[1mStep[0m  [24/42], [94mLoss[0m : 2.41052
[1mStep[0m  [28/42], [94mLoss[0m : 2.45924
[1mStep[0m  [32/42], [94mLoss[0m : 2.44029
[1mStep[0m  [36/42], [94mLoss[0m : 2.45685
[1mStep[0m  [40/42], [94mLoss[0m : 2.33535

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.559, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52943
[1mStep[0m  [4/42], [94mLoss[0m : 2.64427
[1mStep[0m  [8/42], [94mLoss[0m : 2.53035
[1mStep[0m  [12/42], [94mLoss[0m : 2.52911
[1mStep[0m  [16/42], [94mLoss[0m : 2.51757
[1mStep[0m  [20/42], [94mLoss[0m : 2.44231
[1mStep[0m  [24/42], [94mLoss[0m : 2.33836
[1mStep[0m  [28/42], [94mLoss[0m : 2.46917
[1mStep[0m  [32/42], [94mLoss[0m : 2.64423
[1mStep[0m  [36/42], [94mLoss[0m : 2.33674
[1mStep[0m  [40/42], [94mLoss[0m : 2.44574

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.551, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52124
[1mStep[0m  [4/42], [94mLoss[0m : 2.30105
[1mStep[0m  [8/42], [94mLoss[0m : 2.40878
[1mStep[0m  [12/42], [94mLoss[0m : 2.61762
[1mStep[0m  [16/42], [94mLoss[0m : 2.56458
[1mStep[0m  [20/42], [94mLoss[0m : 2.41805
[1mStep[0m  [24/42], [94mLoss[0m : 2.27262
[1mStep[0m  [28/42], [94mLoss[0m : 2.64312
[1mStep[0m  [32/42], [94mLoss[0m : 2.86461
[1mStep[0m  [36/42], [94mLoss[0m : 2.58686
[1mStep[0m  [40/42], [94mLoss[0m : 2.66530

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.561, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27692
[1mStep[0m  [4/42], [94mLoss[0m : 2.31140
[1mStep[0m  [8/42], [94mLoss[0m : 2.59224
[1mStep[0m  [12/42], [94mLoss[0m : 2.54378
[1mStep[0m  [16/42], [94mLoss[0m : 2.65538
[1mStep[0m  [20/42], [94mLoss[0m : 2.58472
[1mStep[0m  [24/42], [94mLoss[0m : 2.48036
[1mStep[0m  [28/42], [94mLoss[0m : 2.66937
[1mStep[0m  [32/42], [94mLoss[0m : 2.57806
[1mStep[0m  [36/42], [94mLoss[0m : 2.49068
[1mStep[0m  [40/42], [94mLoss[0m : 2.41134

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.569, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42152
[1mStep[0m  [4/42], [94mLoss[0m : 2.58690
[1mStep[0m  [8/42], [94mLoss[0m : 2.52932
[1mStep[0m  [12/42], [94mLoss[0m : 2.50408
[1mStep[0m  [16/42], [94mLoss[0m : 2.60911
[1mStep[0m  [20/42], [94mLoss[0m : 2.28070
[1mStep[0m  [24/42], [94mLoss[0m : 2.58808
[1mStep[0m  [28/42], [94mLoss[0m : 2.33100
[1mStep[0m  [32/42], [94mLoss[0m : 2.64271
[1mStep[0m  [36/42], [94mLoss[0m : 2.52073
[1mStep[0m  [40/42], [94mLoss[0m : 2.33240

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.543, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44537
[1mStep[0m  [4/42], [94mLoss[0m : 2.16084
[1mStep[0m  [8/42], [94mLoss[0m : 2.49060
[1mStep[0m  [12/42], [94mLoss[0m : 2.52310
[1mStep[0m  [16/42], [94mLoss[0m : 2.39230
[1mStep[0m  [20/42], [94mLoss[0m : 2.47527
[1mStep[0m  [24/42], [94mLoss[0m : 2.49318
[1mStep[0m  [28/42], [94mLoss[0m : 2.40506
[1mStep[0m  [32/42], [94mLoss[0m : 2.53622
[1mStep[0m  [36/42], [94mLoss[0m : 2.47800
[1mStep[0m  [40/42], [94mLoss[0m : 2.66223

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.555, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40400
[1mStep[0m  [4/42], [94mLoss[0m : 2.36674
[1mStep[0m  [8/42], [94mLoss[0m : 2.67343
[1mStep[0m  [12/42], [94mLoss[0m : 2.24201
[1mStep[0m  [16/42], [94mLoss[0m : 2.47177
[1mStep[0m  [20/42], [94mLoss[0m : 2.50764
[1mStep[0m  [24/42], [94mLoss[0m : 2.36898
[1mStep[0m  [28/42], [94mLoss[0m : 2.46716
[1mStep[0m  [32/42], [94mLoss[0m : 2.42525
[1mStep[0m  [36/42], [94mLoss[0m : 2.46990
[1mStep[0m  [40/42], [94mLoss[0m : 3.06613

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.563, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37136
[1mStep[0m  [4/42], [94mLoss[0m : 2.48961
[1mStep[0m  [8/42], [94mLoss[0m : 2.34880
[1mStep[0m  [12/42], [94mLoss[0m : 2.54358
[1mStep[0m  [16/42], [94mLoss[0m : 2.76939
[1mStep[0m  [20/42], [94mLoss[0m : 2.47242
[1mStep[0m  [24/42], [94mLoss[0m : 2.43346
[1mStep[0m  [28/42], [94mLoss[0m : 2.43388
[1mStep[0m  [32/42], [94mLoss[0m : 2.31692
[1mStep[0m  [36/42], [94mLoss[0m : 2.34513
[1mStep[0m  [40/42], [94mLoss[0m : 2.44405

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.546, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54007
[1mStep[0m  [4/42], [94mLoss[0m : 2.64495
[1mStep[0m  [8/42], [94mLoss[0m : 2.39177
[1mStep[0m  [12/42], [94mLoss[0m : 2.45470
[1mStep[0m  [16/42], [94mLoss[0m : 2.20997
[1mStep[0m  [20/42], [94mLoss[0m : 2.51333
[1mStep[0m  [24/42], [94mLoss[0m : 2.35897
[1mStep[0m  [28/42], [94mLoss[0m : 2.49755
[1mStep[0m  [32/42], [94mLoss[0m : 2.54245
[1mStep[0m  [36/42], [94mLoss[0m : 2.30820
[1mStep[0m  [40/42], [94mLoss[0m : 2.38544

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.533, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34665
[1mStep[0m  [4/42], [94mLoss[0m : 2.34619
[1mStep[0m  [8/42], [94mLoss[0m : 2.32339
[1mStep[0m  [12/42], [94mLoss[0m : 2.31903
[1mStep[0m  [16/42], [94mLoss[0m : 2.46238
[1mStep[0m  [20/42], [94mLoss[0m : 2.51396
[1mStep[0m  [24/42], [94mLoss[0m : 2.39253
[1mStep[0m  [28/42], [94mLoss[0m : 2.60767
[1mStep[0m  [32/42], [94mLoss[0m : 2.76149
[1mStep[0m  [36/42], [94mLoss[0m : 2.35187
[1mStep[0m  [40/42], [94mLoss[0m : 2.39768

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.539, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28561
[1mStep[0m  [4/42], [94mLoss[0m : 2.42125
[1mStep[0m  [8/42], [94mLoss[0m : 2.46900
[1mStep[0m  [12/42], [94mLoss[0m : 2.22654
[1mStep[0m  [16/42], [94mLoss[0m : 2.26868
[1mStep[0m  [20/42], [94mLoss[0m : 2.63928
[1mStep[0m  [24/42], [94mLoss[0m : 2.40927
[1mStep[0m  [28/42], [94mLoss[0m : 2.48794
[1mStep[0m  [32/42], [94mLoss[0m : 2.42480
[1mStep[0m  [36/42], [94mLoss[0m : 2.34618
[1mStep[0m  [40/42], [94mLoss[0m : 2.40279

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.551, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34462
[1mStep[0m  [4/42], [94mLoss[0m : 2.39528
[1mStep[0m  [8/42], [94mLoss[0m : 2.56138
[1mStep[0m  [12/42], [94mLoss[0m : 2.48579
[1mStep[0m  [16/42], [94mLoss[0m : 2.49965
[1mStep[0m  [20/42], [94mLoss[0m : 2.42562
[1mStep[0m  [24/42], [94mLoss[0m : 2.47395
[1mStep[0m  [28/42], [94mLoss[0m : 2.41357
[1mStep[0m  [32/42], [94mLoss[0m : 2.51491
[1mStep[0m  [36/42], [94mLoss[0m : 2.64697
[1mStep[0m  [40/42], [94mLoss[0m : 2.44256

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.539, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40987
[1mStep[0m  [4/42], [94mLoss[0m : 2.63451
[1mStep[0m  [8/42], [94mLoss[0m : 2.57324
[1mStep[0m  [12/42], [94mLoss[0m : 2.54431
[1mStep[0m  [16/42], [94mLoss[0m : 2.67442
[1mStep[0m  [20/42], [94mLoss[0m : 2.49525
[1mStep[0m  [24/42], [94mLoss[0m : 2.37630
[1mStep[0m  [28/42], [94mLoss[0m : 2.34430
[1mStep[0m  [32/42], [94mLoss[0m : 2.25012
[1mStep[0m  [36/42], [94mLoss[0m : 2.52523
[1mStep[0m  [40/42], [94mLoss[0m : 2.47194

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.561, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40617
[1mStep[0m  [4/42], [94mLoss[0m : 2.64501
[1mStep[0m  [8/42], [94mLoss[0m : 2.52527
[1mStep[0m  [12/42], [94mLoss[0m : 2.32119
[1mStep[0m  [16/42], [94mLoss[0m : 2.19233
[1mStep[0m  [20/42], [94mLoss[0m : 2.18121
[1mStep[0m  [24/42], [94mLoss[0m : 2.68025
[1mStep[0m  [28/42], [94mLoss[0m : 2.47373
[1mStep[0m  [32/42], [94mLoss[0m : 2.47893
[1mStep[0m  [36/42], [94mLoss[0m : 2.45258
[1mStep[0m  [40/42], [94mLoss[0m : 2.58923

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.544, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47443
[1mStep[0m  [4/42], [94mLoss[0m : 2.48028
[1mStep[0m  [8/42], [94mLoss[0m : 2.38897
[1mStep[0m  [12/42], [94mLoss[0m : 2.51758
[1mStep[0m  [16/42], [94mLoss[0m : 2.42611
[1mStep[0m  [20/42], [94mLoss[0m : 2.47469
[1mStep[0m  [24/42], [94mLoss[0m : 2.53157
[1mStep[0m  [28/42], [94mLoss[0m : 2.56453
[1mStep[0m  [32/42], [94mLoss[0m : 2.52166
[1mStep[0m  [36/42], [94mLoss[0m : 2.28335
[1mStep[0m  [40/42], [94mLoss[0m : 2.43118

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.538, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28914
[1mStep[0m  [4/42], [94mLoss[0m : 2.36782
[1mStep[0m  [8/42], [94mLoss[0m : 2.35662
[1mStep[0m  [12/42], [94mLoss[0m : 2.51278
[1mStep[0m  [16/42], [94mLoss[0m : 2.55132
[1mStep[0m  [20/42], [94mLoss[0m : 2.38038
[1mStep[0m  [24/42], [94mLoss[0m : 2.48991
[1mStep[0m  [28/42], [94mLoss[0m : 2.43105
[1mStep[0m  [32/42], [94mLoss[0m : 2.51361
[1mStep[0m  [36/42], [94mLoss[0m : 2.55291
[1mStep[0m  [40/42], [94mLoss[0m : 2.47508

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.535, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32703
[1mStep[0m  [4/42], [94mLoss[0m : 2.43391
[1mStep[0m  [8/42], [94mLoss[0m : 2.43327
[1mStep[0m  [12/42], [94mLoss[0m : 2.39083
[1mStep[0m  [16/42], [94mLoss[0m : 2.52713
[1mStep[0m  [20/42], [94mLoss[0m : 2.49934
[1mStep[0m  [24/42], [94mLoss[0m : 2.46422
[1mStep[0m  [28/42], [94mLoss[0m : 2.57063
[1mStep[0m  [32/42], [94mLoss[0m : 2.26285
[1mStep[0m  [36/42], [94mLoss[0m : 2.54842
[1mStep[0m  [40/42], [94mLoss[0m : 2.48670

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.540, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38267
[1mStep[0m  [4/42], [94mLoss[0m : 2.55423
[1mStep[0m  [8/42], [94mLoss[0m : 2.43480
[1mStep[0m  [12/42], [94mLoss[0m : 2.36164
[1mStep[0m  [16/42], [94mLoss[0m : 2.46689
[1mStep[0m  [20/42], [94mLoss[0m : 2.28851
[1mStep[0m  [24/42], [94mLoss[0m : 2.48658
[1mStep[0m  [28/42], [94mLoss[0m : 2.45218
[1mStep[0m  [32/42], [94mLoss[0m : 2.34301
[1mStep[0m  [36/42], [94mLoss[0m : 2.48807
[1mStep[0m  [40/42], [94mLoss[0m : 2.52707

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.554, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.551
====================================

Phase 1 - Evaluation MAE:  2.5506223099572316
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 2.62172
[1mStep[0m  [4/42], [94mLoss[0m : 2.38505
[1mStep[0m  [8/42], [94mLoss[0m : 2.47922
[1mStep[0m  [12/42], [94mLoss[0m : 2.32626
[1mStep[0m  [16/42], [94mLoss[0m : 2.23121
[1mStep[0m  [20/42], [94mLoss[0m : 2.44854
[1mStep[0m  [24/42], [94mLoss[0m : 2.56431
[1mStep[0m  [28/42], [94mLoss[0m : 2.66232
[1mStep[0m  [32/42], [94mLoss[0m : 2.49083
[1mStep[0m  [36/42], [94mLoss[0m : 2.40051
[1mStep[0m  [40/42], [94mLoss[0m : 2.41466

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.544, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35530
[1mStep[0m  [4/42], [94mLoss[0m : 2.60405
[1mStep[0m  [8/42], [94mLoss[0m : 2.60420
[1mStep[0m  [12/42], [94mLoss[0m : 2.67895
[1mStep[0m  [16/42], [94mLoss[0m : 2.31344
[1mStep[0m  [20/42], [94mLoss[0m : 2.66954
[1mStep[0m  [24/42], [94mLoss[0m : 2.65711
[1mStep[0m  [28/42], [94mLoss[0m : 2.13432
[1mStep[0m  [32/42], [94mLoss[0m : 2.36502
[1mStep[0m  [36/42], [94mLoss[0m : 2.33089
[1mStep[0m  [40/42], [94mLoss[0m : 2.38668

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.604, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49277
[1mStep[0m  [4/42], [94mLoss[0m : 2.42525
[1mStep[0m  [8/42], [94mLoss[0m : 2.47983
[1mStep[0m  [12/42], [94mLoss[0m : 2.61793
[1mStep[0m  [16/42], [94mLoss[0m : 2.49453
[1mStep[0m  [20/42], [94mLoss[0m : 2.35762
[1mStep[0m  [24/42], [94mLoss[0m : 2.67962
[1mStep[0m  [28/42], [94mLoss[0m : 2.39315
[1mStep[0m  [32/42], [94mLoss[0m : 2.19513
[1mStep[0m  [36/42], [94mLoss[0m : 2.40605
[1mStep[0m  [40/42], [94mLoss[0m : 2.52971

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.428, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47212
[1mStep[0m  [4/42], [94mLoss[0m : 2.59767
[1mStep[0m  [8/42], [94mLoss[0m : 2.41116
[1mStep[0m  [12/42], [94mLoss[0m : 2.46120
[1mStep[0m  [16/42], [94mLoss[0m : 2.30291
[1mStep[0m  [20/42], [94mLoss[0m : 2.16733
[1mStep[0m  [24/42], [94mLoss[0m : 2.49488
[1mStep[0m  [28/42], [94mLoss[0m : 2.57093
[1mStep[0m  [32/42], [94mLoss[0m : 2.39403
[1mStep[0m  [36/42], [94mLoss[0m : 2.51122
[1mStep[0m  [40/42], [94mLoss[0m : 2.52292

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.438, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.71684
[1mStep[0m  [4/42], [94mLoss[0m : 2.39597
[1mStep[0m  [8/42], [94mLoss[0m : 2.55561
[1mStep[0m  [12/42], [94mLoss[0m : 2.40250
[1mStep[0m  [16/42], [94mLoss[0m : 2.22665
[1mStep[0m  [20/42], [94mLoss[0m : 2.60076
[1mStep[0m  [24/42], [94mLoss[0m : 2.23326
[1mStep[0m  [28/42], [94mLoss[0m : 2.40015
[1mStep[0m  [32/42], [94mLoss[0m : 2.43557
[1mStep[0m  [36/42], [94mLoss[0m : 2.40660
[1mStep[0m  [40/42], [94mLoss[0m : 2.41227

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.504, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61236
[1mStep[0m  [4/42], [94mLoss[0m : 2.38238
[1mStep[0m  [8/42], [94mLoss[0m : 2.12027
[1mStep[0m  [12/42], [94mLoss[0m : 2.45021
[1mStep[0m  [16/42], [94mLoss[0m : 2.22531
[1mStep[0m  [20/42], [94mLoss[0m : 2.25998
[1mStep[0m  [24/42], [94mLoss[0m : 2.60396
[1mStep[0m  [28/42], [94mLoss[0m : 2.34293
[1mStep[0m  [32/42], [94mLoss[0m : 2.46733
[1mStep[0m  [36/42], [94mLoss[0m : 2.36964
[1mStep[0m  [40/42], [94mLoss[0m : 2.67482

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.391, [92mTest[0m: 2.638, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28384
[1mStep[0m  [4/42], [94mLoss[0m : 2.45598
[1mStep[0m  [8/42], [94mLoss[0m : 2.40231
[1mStep[0m  [12/42], [94mLoss[0m : 2.37682
[1mStep[0m  [16/42], [94mLoss[0m : 2.67537
[1mStep[0m  [20/42], [94mLoss[0m : 2.13142
[1mStep[0m  [24/42], [94mLoss[0m : 2.47086
[1mStep[0m  [28/42], [94mLoss[0m : 2.38010
[1mStep[0m  [32/42], [94mLoss[0m : 2.41948
[1mStep[0m  [36/42], [94mLoss[0m : 2.29175
[1mStep[0m  [40/42], [94mLoss[0m : 2.28767

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.392, [92mTest[0m: 2.630, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32780
[1mStep[0m  [4/42], [94mLoss[0m : 2.50004
[1mStep[0m  [8/42], [94mLoss[0m : 2.21293
[1mStep[0m  [12/42], [94mLoss[0m : 2.44447
[1mStep[0m  [16/42], [94mLoss[0m : 2.27380
[1mStep[0m  [20/42], [94mLoss[0m : 2.26410
[1mStep[0m  [24/42], [94mLoss[0m : 2.61526
[1mStep[0m  [28/42], [94mLoss[0m : 2.27929
[1mStep[0m  [32/42], [94mLoss[0m : 2.50361
[1mStep[0m  [36/42], [94mLoss[0m : 2.04837
[1mStep[0m  [40/42], [94mLoss[0m : 2.30625

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.365, [92mTest[0m: 2.694, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46515
[1mStep[0m  [4/42], [94mLoss[0m : 2.28572
[1mStep[0m  [8/42], [94mLoss[0m : 2.18025
[1mStep[0m  [12/42], [94mLoss[0m : 2.35748
[1mStep[0m  [16/42], [94mLoss[0m : 2.18780
[1mStep[0m  [20/42], [94mLoss[0m : 2.48433
[1mStep[0m  [24/42], [94mLoss[0m : 2.36244
[1mStep[0m  [28/42], [94mLoss[0m : 2.31852
[1mStep[0m  [32/42], [94mLoss[0m : 2.40451
[1mStep[0m  [36/42], [94mLoss[0m : 2.34902
[1mStep[0m  [40/42], [94mLoss[0m : 2.26912

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.344, [92mTest[0m: 2.594, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.25472
[1mStep[0m  [4/42], [94mLoss[0m : 2.27372
[1mStep[0m  [8/42], [94mLoss[0m : 2.34790
[1mStep[0m  [12/42], [94mLoss[0m : 2.18514
[1mStep[0m  [16/42], [94mLoss[0m : 2.34801
[1mStep[0m  [20/42], [94mLoss[0m : 2.43742
[1mStep[0m  [24/42], [94mLoss[0m : 2.16232
[1mStep[0m  [28/42], [94mLoss[0m : 2.51099
[1mStep[0m  [32/42], [94mLoss[0m : 2.27209
[1mStep[0m  [36/42], [94mLoss[0m : 2.67290
[1mStep[0m  [40/42], [94mLoss[0m : 2.32360

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.342, [92mTest[0m: 2.629, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.16705
[1mStep[0m  [4/42], [94mLoss[0m : 2.58380
[1mStep[0m  [8/42], [94mLoss[0m : 2.45129
[1mStep[0m  [12/42], [94mLoss[0m : 2.36069
[1mStep[0m  [16/42], [94mLoss[0m : 2.19048
[1mStep[0m  [20/42], [94mLoss[0m : 2.35202
[1mStep[0m  [24/42], [94mLoss[0m : 2.27159
[1mStep[0m  [28/42], [94mLoss[0m : 2.51914
[1mStep[0m  [32/42], [94mLoss[0m : 2.29727
[1mStep[0m  [36/42], [94mLoss[0m : 2.25481
[1mStep[0m  [40/42], [94mLoss[0m : 2.38287

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.324, [92mTest[0m: 2.632, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.20616
[1mStep[0m  [4/42], [94mLoss[0m : 2.23428
[1mStep[0m  [8/42], [94mLoss[0m : 2.36414
[1mStep[0m  [12/42], [94mLoss[0m : 2.40445
[1mStep[0m  [16/42], [94mLoss[0m : 2.43108
[1mStep[0m  [20/42], [94mLoss[0m : 2.45237
[1mStep[0m  [24/42], [94mLoss[0m : 2.24764
[1mStep[0m  [28/42], [94mLoss[0m : 2.29943
[1mStep[0m  [32/42], [94mLoss[0m : 2.37949
[1mStep[0m  [36/42], [94mLoss[0m : 2.28342
[1mStep[0m  [40/42], [94mLoss[0m : 2.20986

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.308, [92mTest[0m: 2.586, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.01790
[1mStep[0m  [4/42], [94mLoss[0m : 2.36227
[1mStep[0m  [8/42], [94mLoss[0m : 2.44895
[1mStep[0m  [12/42], [94mLoss[0m : 2.29320
[1mStep[0m  [16/42], [94mLoss[0m : 2.25753
[1mStep[0m  [20/42], [94mLoss[0m : 2.18811
[1mStep[0m  [24/42], [94mLoss[0m : 2.47268
[1mStep[0m  [28/42], [94mLoss[0m : 2.21139
[1mStep[0m  [32/42], [94mLoss[0m : 2.37335
[1mStep[0m  [36/42], [94mLoss[0m : 2.19841
[1mStep[0m  [40/42], [94mLoss[0m : 2.44027

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.277, [92mTest[0m: 2.529, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.22879
[1mStep[0m  [4/42], [94mLoss[0m : 2.33908
[1mStep[0m  [8/42], [94mLoss[0m : 2.07619
[1mStep[0m  [12/42], [94mLoss[0m : 2.38582
[1mStep[0m  [16/42], [94mLoss[0m : 2.28641
[1mStep[0m  [20/42], [94mLoss[0m : 2.02938
[1mStep[0m  [24/42], [94mLoss[0m : 2.17199
[1mStep[0m  [28/42], [94mLoss[0m : 2.40707
[1mStep[0m  [32/42], [94mLoss[0m : 2.19514
[1mStep[0m  [36/42], [94mLoss[0m : 2.50492
[1mStep[0m  [40/42], [94mLoss[0m : 2.16614

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.262, [92mTest[0m: 2.478, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38305
[1mStep[0m  [4/42], [94mLoss[0m : 2.04002
[1mStep[0m  [8/42], [94mLoss[0m : 2.05916
[1mStep[0m  [12/42], [94mLoss[0m : 2.12781
[1mStep[0m  [16/42], [94mLoss[0m : 2.11319
[1mStep[0m  [20/42], [94mLoss[0m : 2.17596
[1mStep[0m  [24/42], [94mLoss[0m : 2.12976
[1mStep[0m  [28/42], [94mLoss[0m : 2.15492
[1mStep[0m  [32/42], [94mLoss[0m : 2.31479
[1mStep[0m  [36/42], [94mLoss[0m : 2.25748
[1mStep[0m  [40/42], [94mLoss[0m : 2.31225

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.261, [92mTest[0m: 2.550, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.22713
[1mStep[0m  [4/42], [94mLoss[0m : 2.32639
[1mStep[0m  [8/42], [94mLoss[0m : 2.18673
[1mStep[0m  [12/42], [94mLoss[0m : 2.32594
[1mStep[0m  [16/42], [94mLoss[0m : 2.33791
[1mStep[0m  [20/42], [94mLoss[0m : 2.25758
[1mStep[0m  [24/42], [94mLoss[0m : 2.19441
[1mStep[0m  [28/42], [94mLoss[0m : 2.35732
[1mStep[0m  [32/42], [94mLoss[0m : 2.06737
[1mStep[0m  [36/42], [94mLoss[0m : 2.38469
[1mStep[0m  [40/42], [94mLoss[0m : 2.40220

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.251, [92mTest[0m: 2.523, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.19432
[1mStep[0m  [4/42], [94mLoss[0m : 2.29408
[1mStep[0m  [8/42], [94mLoss[0m : 2.16619
[1mStep[0m  [12/42], [94mLoss[0m : 2.30176
[1mStep[0m  [16/42], [94mLoss[0m : 2.15453
[1mStep[0m  [20/42], [94mLoss[0m : 2.12223
[1mStep[0m  [24/42], [94mLoss[0m : 2.28388
[1mStep[0m  [28/42], [94mLoss[0m : 2.15768
[1mStep[0m  [32/42], [94mLoss[0m : 2.26690
[1mStep[0m  [36/42], [94mLoss[0m : 2.06779
[1mStep[0m  [40/42], [94mLoss[0m : 2.24341

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.213, [92mTest[0m: 2.559, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.23147
[1mStep[0m  [4/42], [94mLoss[0m : 2.00789
[1mStep[0m  [8/42], [94mLoss[0m : 2.14650
[1mStep[0m  [12/42], [94mLoss[0m : 2.37040
[1mStep[0m  [16/42], [94mLoss[0m : 2.32434
[1mStep[0m  [20/42], [94mLoss[0m : 2.16829
[1mStep[0m  [24/42], [94mLoss[0m : 2.25205
[1mStep[0m  [28/42], [94mLoss[0m : 2.10647
[1mStep[0m  [32/42], [94mLoss[0m : 1.97652
[1mStep[0m  [36/42], [94mLoss[0m : 2.14457
[1mStep[0m  [40/42], [94mLoss[0m : 2.24992

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.199, [92mTest[0m: 2.550, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.03036
[1mStep[0m  [4/42], [94mLoss[0m : 2.27653
[1mStep[0m  [8/42], [94mLoss[0m : 2.25927
[1mStep[0m  [12/42], [94mLoss[0m : 2.18525
[1mStep[0m  [16/42], [94mLoss[0m : 2.25842
[1mStep[0m  [20/42], [94mLoss[0m : 2.42197
[1mStep[0m  [24/42], [94mLoss[0m : 2.11746
[1mStep[0m  [28/42], [94mLoss[0m : 2.14177
[1mStep[0m  [32/42], [94mLoss[0m : 2.20559
[1mStep[0m  [36/42], [94mLoss[0m : 2.27080
[1mStep[0m  [40/42], [94mLoss[0m : 2.21160

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.199, [92mTest[0m: 2.511, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.17822
[1mStep[0m  [4/42], [94mLoss[0m : 2.32330
[1mStep[0m  [8/42], [94mLoss[0m : 2.12707
[1mStep[0m  [12/42], [94mLoss[0m : 2.17796
[1mStep[0m  [16/42], [94mLoss[0m : 2.13549
[1mStep[0m  [20/42], [94mLoss[0m : 2.19023
[1mStep[0m  [24/42], [94mLoss[0m : 2.11924
[1mStep[0m  [28/42], [94mLoss[0m : 2.18856
[1mStep[0m  [32/42], [94mLoss[0m : 2.27015
[1mStep[0m  [36/42], [94mLoss[0m : 2.08320
[1mStep[0m  [40/42], [94mLoss[0m : 2.32531

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.180, [92mTest[0m: 2.499, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.96846
[1mStep[0m  [4/42], [94mLoss[0m : 2.02391
[1mStep[0m  [8/42], [94mLoss[0m : 2.05638
[1mStep[0m  [12/42], [94mLoss[0m : 2.01183
[1mStep[0m  [16/42], [94mLoss[0m : 2.16076
[1mStep[0m  [20/42], [94mLoss[0m : 2.23126
[1mStep[0m  [24/42], [94mLoss[0m : 2.10005
[1mStep[0m  [28/42], [94mLoss[0m : 2.08210
[1mStep[0m  [32/42], [94mLoss[0m : 2.39273
[1mStep[0m  [36/42], [94mLoss[0m : 2.13304
[1mStep[0m  [40/42], [94mLoss[0m : 1.98469

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.153, [92mTest[0m: 2.496, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.16692
[1mStep[0m  [4/42], [94mLoss[0m : 1.99343
[1mStep[0m  [8/42], [94mLoss[0m : 2.15858
[1mStep[0m  [12/42], [94mLoss[0m : 2.16752
[1mStep[0m  [16/42], [94mLoss[0m : 2.32860
[1mStep[0m  [20/42], [94mLoss[0m : 2.08502
[1mStep[0m  [24/42], [94mLoss[0m : 2.06470
[1mStep[0m  [28/42], [94mLoss[0m : 2.19930
[1mStep[0m  [32/42], [94mLoss[0m : 2.37765
[1mStep[0m  [36/42], [94mLoss[0m : 2.28912
[1mStep[0m  [40/42], [94mLoss[0m : 1.92173

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.149, [92mTest[0m: 2.499, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.06538
[1mStep[0m  [4/42], [94mLoss[0m : 2.17114
[1mStep[0m  [8/42], [94mLoss[0m : 2.03579
[1mStep[0m  [12/42], [94mLoss[0m : 2.06832
[1mStep[0m  [16/42], [94mLoss[0m : 2.09964
[1mStep[0m  [20/42], [94mLoss[0m : 2.19282
[1mStep[0m  [24/42], [94mLoss[0m : 2.05157
[1mStep[0m  [28/42], [94mLoss[0m : 2.10729
[1mStep[0m  [32/42], [94mLoss[0m : 2.14956
[1mStep[0m  [36/42], [94mLoss[0m : 2.12317
[1mStep[0m  [40/42], [94mLoss[0m : 2.15814

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.124, [92mTest[0m: 2.492, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.11647
[1mStep[0m  [4/42], [94mLoss[0m : 2.03000
[1mStep[0m  [8/42], [94mLoss[0m : 2.12808
[1mStep[0m  [12/42], [94mLoss[0m : 1.92930
[1mStep[0m  [16/42], [94mLoss[0m : 2.11462
[1mStep[0m  [20/42], [94mLoss[0m : 2.03247
[1mStep[0m  [24/42], [94mLoss[0m : 2.15998
[1mStep[0m  [28/42], [94mLoss[0m : 1.98997
[1mStep[0m  [32/42], [94mLoss[0m : 2.06769
[1mStep[0m  [36/42], [94mLoss[0m : 2.24348
[1mStep[0m  [40/42], [94mLoss[0m : 2.21622

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.108, [92mTest[0m: 2.518, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31176
[1mStep[0m  [4/42], [94mLoss[0m : 2.11429
[1mStep[0m  [8/42], [94mLoss[0m : 1.92438
[1mStep[0m  [12/42], [94mLoss[0m : 2.02280
[1mStep[0m  [16/42], [94mLoss[0m : 2.15406
[1mStep[0m  [20/42], [94mLoss[0m : 2.14463
[1mStep[0m  [24/42], [94mLoss[0m : 2.26436
[1mStep[0m  [28/42], [94mLoss[0m : 2.05171
[1mStep[0m  [32/42], [94mLoss[0m : 2.10890
[1mStep[0m  [36/42], [94mLoss[0m : 2.12478
[1mStep[0m  [40/42], [94mLoss[0m : 2.02893

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.094, [92mTest[0m: 2.511, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.19423
[1mStep[0m  [4/42], [94mLoss[0m : 2.08774
[1mStep[0m  [8/42], [94mLoss[0m : 2.07643
[1mStep[0m  [12/42], [94mLoss[0m : 2.04404
[1mStep[0m  [16/42], [94mLoss[0m : 1.99957
[1mStep[0m  [20/42], [94mLoss[0m : 2.02951
[1mStep[0m  [24/42], [94mLoss[0m : 1.92194
[1mStep[0m  [28/42], [94mLoss[0m : 2.32048
[1mStep[0m  [32/42], [94mLoss[0m : 2.19955
[1mStep[0m  [36/42], [94mLoss[0m : 2.02120
[1mStep[0m  [40/42], [94mLoss[0m : 2.05558

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.068, [92mTest[0m: 2.473, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.90217
[1mStep[0m  [4/42], [94mLoss[0m : 1.90493
[1mStep[0m  [8/42], [94mLoss[0m : 2.04569
[1mStep[0m  [12/42], [94mLoss[0m : 1.91292
[1mStep[0m  [16/42], [94mLoss[0m : 2.27564
[1mStep[0m  [20/42], [94mLoss[0m : 1.95929
[1mStep[0m  [24/42], [94mLoss[0m : 2.09748
[1mStep[0m  [28/42], [94mLoss[0m : 1.94234
[1mStep[0m  [32/42], [94mLoss[0m : 2.21370
[1mStep[0m  [36/42], [94mLoss[0m : 2.10569
[1mStep[0m  [40/42], [94mLoss[0m : 2.21263

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.069, [92mTest[0m: 2.477, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.86134
[1mStep[0m  [4/42], [94mLoss[0m : 1.97600
[1mStep[0m  [8/42], [94mLoss[0m : 2.10397
[1mStep[0m  [12/42], [94mLoss[0m : 2.00287
[1mStep[0m  [16/42], [94mLoss[0m : 2.14015
[1mStep[0m  [20/42], [94mLoss[0m : 2.16304
[1mStep[0m  [24/42], [94mLoss[0m : 1.92628
[1mStep[0m  [28/42], [94mLoss[0m : 2.15862
[1mStep[0m  [32/42], [94mLoss[0m : 2.01057
[1mStep[0m  [36/42], [94mLoss[0m : 1.90702
[1mStep[0m  [40/42], [94mLoss[0m : 1.98498

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.057, [92mTest[0m: 2.465, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.91506
[1mStep[0m  [4/42], [94mLoss[0m : 2.09169
[1mStep[0m  [8/42], [94mLoss[0m : 1.98711
[1mStep[0m  [12/42], [94mLoss[0m : 2.03517
[1mStep[0m  [16/42], [94mLoss[0m : 1.78519
[1mStep[0m  [20/42], [94mLoss[0m : 1.93326
[1mStep[0m  [24/42], [94mLoss[0m : 2.10744
[1mStep[0m  [28/42], [94mLoss[0m : 2.10605
[1mStep[0m  [32/42], [94mLoss[0m : 2.03306
[1mStep[0m  [36/42], [94mLoss[0m : 2.21701
[1mStep[0m  [40/42], [94mLoss[0m : 2.18579

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.021, [92mTest[0m: 2.446, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.89643
[1mStep[0m  [4/42], [94mLoss[0m : 2.04239
[1mStep[0m  [8/42], [94mLoss[0m : 2.02901
[1mStep[0m  [12/42], [94mLoss[0m : 1.84382
[1mStep[0m  [16/42], [94mLoss[0m : 2.12316
[1mStep[0m  [20/42], [94mLoss[0m : 2.05847
[1mStep[0m  [24/42], [94mLoss[0m : 2.06148
[1mStep[0m  [28/42], [94mLoss[0m : 1.91486
[1mStep[0m  [32/42], [94mLoss[0m : 1.98685
[1mStep[0m  [36/42], [94mLoss[0m : 1.94540
[1mStep[0m  [40/42], [94mLoss[0m : 2.09648

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.022, [92mTest[0m: 2.423, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.425
====================================

Phase 2 - Evaluation MAE:  2.4249459164483205
MAE score P1      2.550622
MAE score P2      2.424946
loss              2.020724
learning_rate     0.002575
batch_size             256
hidden_sizes         [300]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.1
weight_decay        0.0001
Name: 15, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 10.46256
[1mStep[0m  [4/42], [94mLoss[0m : 9.67326
[1mStep[0m  [8/42], [94mLoss[0m : 10.09256
[1mStep[0m  [12/42], [94mLoss[0m : 10.16156
[1mStep[0m  [16/42], [94mLoss[0m : 9.87916
[1mStep[0m  [20/42], [94mLoss[0m : 9.75790
[1mStep[0m  [24/42], [94mLoss[0m : 9.66658
[1mStep[0m  [28/42], [94mLoss[0m : 9.22625
[1mStep[0m  [32/42], [94mLoss[0m : 9.12485
[1mStep[0m  [36/42], [94mLoss[0m : 9.38218
[1mStep[0m  [40/42], [94mLoss[0m : 8.73595

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.660, [92mTest[0m: 10.917, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.51021
[1mStep[0m  [4/42], [94mLoss[0m : 8.54469
[1mStep[0m  [8/42], [94mLoss[0m : 8.36693
[1mStep[0m  [12/42], [94mLoss[0m : 8.27284
[1mStep[0m  [16/42], [94mLoss[0m : 8.18260
[1mStep[0m  [20/42], [94mLoss[0m : 7.39508
[1mStep[0m  [24/42], [94mLoss[0m : 7.27921
[1mStep[0m  [28/42], [94mLoss[0m : 7.34566
[1mStep[0m  [32/42], [94mLoss[0m : 7.36017
[1mStep[0m  [36/42], [94mLoss[0m : 6.96226
[1mStep[0m  [40/42], [94mLoss[0m : 6.56128

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 7.722, [92mTest[0m: 9.534, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.79644
[1mStep[0m  [4/42], [94mLoss[0m : 6.32713
[1mStep[0m  [8/42], [94mLoss[0m : 6.06564
[1mStep[0m  [12/42], [94mLoss[0m : 6.43534
[1mStep[0m  [16/42], [94mLoss[0m : 6.43643
[1mStep[0m  [20/42], [94mLoss[0m : 6.20640
[1mStep[0m  [24/42], [94mLoss[0m : 5.48304
[1mStep[0m  [28/42], [94mLoss[0m : 5.52456
[1mStep[0m  [32/42], [94mLoss[0m : 5.17248
[1mStep[0m  [36/42], [94mLoss[0m : 5.29196
[1mStep[0m  [40/42], [94mLoss[0m : 4.79618

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 5.917, [92mTest[0m: 8.213, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.15725
[1mStep[0m  [4/42], [94mLoss[0m : 4.89471
[1mStep[0m  [8/42], [94mLoss[0m : 4.69713
[1mStep[0m  [12/42], [94mLoss[0m : 4.54423
[1mStep[0m  [16/42], [94mLoss[0m : 4.32432
[1mStep[0m  [20/42], [94mLoss[0m : 4.75179
[1mStep[0m  [24/42], [94mLoss[0m : 4.30838
[1mStep[0m  [28/42], [94mLoss[0m : 3.92000
[1mStep[0m  [32/42], [94mLoss[0m : 3.97395
[1mStep[0m  [36/42], [94mLoss[0m : 3.79731
[1mStep[0m  [40/42], [94mLoss[0m : 3.88900

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 4.411, [92mTest[0m: 6.862, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.89409
[1mStep[0m  [4/42], [94mLoss[0m : 3.93742
[1mStep[0m  [8/42], [94mLoss[0m : 3.44303
[1mStep[0m  [12/42], [94mLoss[0m : 3.14481
[1mStep[0m  [16/42], [94mLoss[0m : 3.28452
[1mStep[0m  [20/42], [94mLoss[0m : 3.41763
[1mStep[0m  [24/42], [94mLoss[0m : 3.40555
[1mStep[0m  [28/42], [94mLoss[0m : 3.57702
[1mStep[0m  [32/42], [94mLoss[0m : 3.32767
[1mStep[0m  [36/42], [94mLoss[0m : 3.06332
[1mStep[0m  [40/42], [94mLoss[0m : 3.12819

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 3.475, [92mTest[0m: 5.427, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.90272
[1mStep[0m  [4/42], [94mLoss[0m : 3.10229
[1mStep[0m  [8/42], [94mLoss[0m : 3.19811
[1mStep[0m  [12/42], [94mLoss[0m : 3.38979
[1mStep[0m  [16/42], [94mLoss[0m : 3.27344
[1mStep[0m  [20/42], [94mLoss[0m : 2.85428
[1mStep[0m  [24/42], [94mLoss[0m : 3.05673
[1mStep[0m  [28/42], [94mLoss[0m : 3.29037
[1mStep[0m  [32/42], [94mLoss[0m : 3.17668
[1mStep[0m  [36/42], [94mLoss[0m : 2.83644
[1mStep[0m  [40/42], [94mLoss[0m : 2.86259

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 3.067, [92mTest[0m: 4.485, [96mlr[0m: 0.002575
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 5 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 3.978
====================================

Phase 1 - Evaluation MAE:  3.9780325378690446
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 3.07862
[1mStep[0m  [4/42], [94mLoss[0m : 2.90351
[1mStep[0m  [8/42], [94mLoss[0m : 2.98725
[1mStep[0m  [12/42], [94mLoss[0m : 3.01950
[1mStep[0m  [16/42], [94mLoss[0m : 2.78778
[1mStep[0m  [20/42], [94mLoss[0m : 2.85252
[1mStep[0m  [24/42], [94mLoss[0m : 3.09292
[1mStep[0m  [28/42], [94mLoss[0m : 3.21737
[1mStep[0m  [32/42], [94mLoss[0m : 2.79784
[1mStep[0m  [36/42], [94mLoss[0m : 2.68411
[1mStep[0m  [40/42], [94mLoss[0m : 3.03503

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.934, [92mTest[0m: 3.975, [96mlr[0m: 0.002575
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 0 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.983
====================================

Phase 2 - Evaluation MAE:  2.9826203414372037
MAE score P1        3.978033
MAE score P2         2.98262
loss                2.933651
learning_rate       0.002575
batch_size               256
hidden_sizes      [300, 100]
epochs                    30
activation              relu
optimizer                sgd
early stopping          True
dropout                  0.4
momentum                 0.1
weight_decay           0.001
Name: 16, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 10.63935
[1mStep[0m  [4/42], [94mLoss[0m : 10.45822
[1mStep[0m  [8/42], [94mLoss[0m : 10.18394
[1mStep[0m  [12/42], [94mLoss[0m : 10.13543
[1mStep[0m  [16/42], [94mLoss[0m : 9.87015
[1mStep[0m  [20/42], [94mLoss[0m : 9.52944
[1mStep[0m  [24/42], [94mLoss[0m : 9.14368
[1mStep[0m  [28/42], [94mLoss[0m : 8.88871
[1mStep[0m  [32/42], [94mLoss[0m : 8.72975
[1mStep[0m  [36/42], [94mLoss[0m : 8.12523
[1mStep[0m  [40/42], [94mLoss[0m : 8.64564

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.330, [92mTest[0m: 10.625, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.86913
[1mStep[0m  [4/42], [94mLoss[0m : 7.48520
[1mStep[0m  [8/42], [94mLoss[0m : 7.41345
[1mStep[0m  [12/42], [94mLoss[0m : 6.81164
[1mStep[0m  [16/42], [94mLoss[0m : 6.92934
[1mStep[0m  [20/42], [94mLoss[0m : 6.48348
[1mStep[0m  [24/42], [94mLoss[0m : 6.29964
[1mStep[0m  [28/42], [94mLoss[0m : 5.99289
[1mStep[0m  [32/42], [94mLoss[0m : 5.98513
[1mStep[0m  [36/42], [94mLoss[0m : 5.48313
[1mStep[0m  [40/42], [94mLoss[0m : 5.11363

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 6.422, [92mTest[0m: 7.820, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.71951
[1mStep[0m  [4/42], [94mLoss[0m : 4.91002
[1mStep[0m  [8/42], [94mLoss[0m : 4.70811
[1mStep[0m  [12/42], [94mLoss[0m : 4.11176
[1mStep[0m  [16/42], [94mLoss[0m : 4.42021
[1mStep[0m  [20/42], [94mLoss[0m : 4.10668
[1mStep[0m  [24/42], [94mLoss[0m : 3.84013
[1mStep[0m  [28/42], [94mLoss[0m : 3.55447
[1mStep[0m  [32/42], [94mLoss[0m : 3.80552
[1mStep[0m  [36/42], [94mLoss[0m : 3.48017
[1mStep[0m  [40/42], [94mLoss[0m : 3.26995

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 4.068, [92mTest[0m: 4.990, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.38198
[1mStep[0m  [4/42], [94mLoss[0m : 3.46820
[1mStep[0m  [8/42], [94mLoss[0m : 3.38878
[1mStep[0m  [12/42], [94mLoss[0m : 2.85902
[1mStep[0m  [16/42], [94mLoss[0m : 3.47514
[1mStep[0m  [20/42], [94mLoss[0m : 3.24887
[1mStep[0m  [24/42], [94mLoss[0m : 2.79932
[1mStep[0m  [28/42], [94mLoss[0m : 2.81311
[1mStep[0m  [32/42], [94mLoss[0m : 3.12153
[1mStep[0m  [36/42], [94mLoss[0m : 2.80438
[1mStep[0m  [40/42], [94mLoss[0m : 2.88340

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 3.057, [92mTest[0m: 3.297, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.03991
[1mStep[0m  [4/42], [94mLoss[0m : 2.86961
[1mStep[0m  [8/42], [94mLoss[0m : 2.61337
[1mStep[0m  [12/42], [94mLoss[0m : 2.80581
[1mStep[0m  [16/42], [94mLoss[0m : 2.57340
[1mStep[0m  [20/42], [94mLoss[0m : 2.73903
[1mStep[0m  [24/42], [94mLoss[0m : 2.68150
[1mStep[0m  [28/42], [94mLoss[0m : 2.67631
[1mStep[0m  [32/42], [94mLoss[0m : 2.49713
[1mStep[0m  [36/42], [94mLoss[0m : 2.57538
[1mStep[0m  [40/42], [94mLoss[0m : 2.70121

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.717, [92mTest[0m: 2.659, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62400
[1mStep[0m  [4/42], [94mLoss[0m : 2.82900
[1mStep[0m  [8/42], [94mLoss[0m : 2.73229
[1mStep[0m  [12/42], [94mLoss[0m : 2.63123
[1mStep[0m  [16/42], [94mLoss[0m : 2.78630
[1mStep[0m  [20/42], [94mLoss[0m : 2.70549
[1mStep[0m  [24/42], [94mLoss[0m : 2.56857
[1mStep[0m  [28/42], [94mLoss[0m : 2.41348
[1mStep[0m  [32/42], [94mLoss[0m : 2.70042
[1mStep[0m  [36/42], [94mLoss[0m : 2.59049
[1mStep[0m  [40/42], [94mLoss[0m : 2.34840

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.621, [92mTest[0m: 2.470, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.73371
[1mStep[0m  [4/42], [94mLoss[0m : 2.72900
[1mStep[0m  [8/42], [94mLoss[0m : 2.55407
[1mStep[0m  [12/42], [94mLoss[0m : 2.61501
[1mStep[0m  [16/42], [94mLoss[0m : 2.59780
[1mStep[0m  [20/42], [94mLoss[0m : 2.53846
[1mStep[0m  [24/42], [94mLoss[0m : 2.56570
[1mStep[0m  [28/42], [94mLoss[0m : 2.46871
[1mStep[0m  [32/42], [94mLoss[0m : 2.76723
[1mStep[0m  [36/42], [94mLoss[0m : 2.65190
[1mStep[0m  [40/42], [94mLoss[0m : 2.60728

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.579, [92mTest[0m: 2.429, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52924
[1mStep[0m  [4/42], [94mLoss[0m : 2.64460
[1mStep[0m  [8/42], [94mLoss[0m : 2.65782
[1mStep[0m  [12/42], [94mLoss[0m : 2.33672
[1mStep[0m  [16/42], [94mLoss[0m : 2.63724
[1mStep[0m  [20/42], [94mLoss[0m : 2.34222
[1mStep[0m  [24/42], [94mLoss[0m : 2.40301
[1mStep[0m  [28/42], [94mLoss[0m : 2.56300
[1mStep[0m  [32/42], [94mLoss[0m : 2.74583
[1mStep[0m  [36/42], [94mLoss[0m : 2.38850
[1mStep[0m  [40/42], [94mLoss[0m : 2.66901

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.558, [92mTest[0m: 2.409, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.74243
[1mStep[0m  [4/42], [94mLoss[0m : 2.56691
[1mStep[0m  [8/42], [94mLoss[0m : 2.58518
[1mStep[0m  [12/42], [94mLoss[0m : 2.67121
[1mStep[0m  [16/42], [94mLoss[0m : 2.51995
[1mStep[0m  [20/42], [94mLoss[0m : 2.54171
[1mStep[0m  [24/42], [94mLoss[0m : 2.73750
[1mStep[0m  [28/42], [94mLoss[0m : 2.66680
[1mStep[0m  [32/42], [94mLoss[0m : 2.45583
[1mStep[0m  [36/42], [94mLoss[0m : 2.55366
[1mStep[0m  [40/42], [94mLoss[0m : 2.74055

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.555, [92mTest[0m: 2.403, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62220
[1mStep[0m  [4/42], [94mLoss[0m : 2.29063
[1mStep[0m  [8/42], [94mLoss[0m : 2.71406
[1mStep[0m  [12/42], [94mLoss[0m : 2.44815
[1mStep[0m  [16/42], [94mLoss[0m : 2.55893
[1mStep[0m  [20/42], [94mLoss[0m : 2.65057
[1mStep[0m  [24/42], [94mLoss[0m : 2.68766
[1mStep[0m  [28/42], [94mLoss[0m : 2.29213
[1mStep[0m  [32/42], [94mLoss[0m : 2.55723
[1mStep[0m  [36/42], [94mLoss[0m : 2.59848
[1mStep[0m  [40/42], [94mLoss[0m : 2.55043

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.538, [92mTest[0m: 2.388, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.76093
[1mStep[0m  [4/42], [94mLoss[0m : 2.58328
[1mStep[0m  [8/42], [94mLoss[0m : 2.67059
[1mStep[0m  [12/42], [94mLoss[0m : 2.54702
[1mStep[0m  [16/42], [94mLoss[0m : 2.54422
[1mStep[0m  [20/42], [94mLoss[0m : 2.78692
[1mStep[0m  [24/42], [94mLoss[0m : 2.49848
[1mStep[0m  [28/42], [94mLoss[0m : 2.50885
[1mStep[0m  [32/42], [94mLoss[0m : 2.36920
[1mStep[0m  [36/42], [94mLoss[0m : 2.53318
[1mStep[0m  [40/42], [94mLoss[0m : 2.56820

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.541, [92mTest[0m: 2.378, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.67291
[1mStep[0m  [4/42], [94mLoss[0m : 2.63552
[1mStep[0m  [8/42], [94mLoss[0m : 2.45878
[1mStep[0m  [12/42], [94mLoss[0m : 2.49926
[1mStep[0m  [16/42], [94mLoss[0m : 2.46032
[1mStep[0m  [20/42], [94mLoss[0m : 2.57481
[1mStep[0m  [24/42], [94mLoss[0m : 2.45490
[1mStep[0m  [28/42], [94mLoss[0m : 2.56205
[1mStep[0m  [32/42], [94mLoss[0m : 2.38652
[1mStep[0m  [36/42], [94mLoss[0m : 2.39364
[1mStep[0m  [40/42], [94mLoss[0m : 2.44215

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.537, [92mTest[0m: 2.377, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54324
[1mStep[0m  [4/42], [94mLoss[0m : 2.59206
[1mStep[0m  [8/42], [94mLoss[0m : 2.68431
[1mStep[0m  [12/42], [94mLoss[0m : 2.49084
[1mStep[0m  [16/42], [94mLoss[0m : 2.46156
[1mStep[0m  [20/42], [94mLoss[0m : 2.38932
[1mStep[0m  [24/42], [94mLoss[0m : 2.58068
[1mStep[0m  [28/42], [94mLoss[0m : 2.59924
[1mStep[0m  [32/42], [94mLoss[0m : 2.40101
[1mStep[0m  [36/42], [94mLoss[0m : 2.66730
[1mStep[0m  [40/42], [94mLoss[0m : 2.60711

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.515, [92mTest[0m: 2.364, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.63079
[1mStep[0m  [4/42], [94mLoss[0m : 2.67991
[1mStep[0m  [8/42], [94mLoss[0m : 2.49760
[1mStep[0m  [12/42], [94mLoss[0m : 2.53443
[1mStep[0m  [16/42], [94mLoss[0m : 2.56684
[1mStep[0m  [20/42], [94mLoss[0m : 2.56644
[1mStep[0m  [24/42], [94mLoss[0m : 2.31435
[1mStep[0m  [28/42], [94mLoss[0m : 2.71623
[1mStep[0m  [32/42], [94mLoss[0m : 2.52696
[1mStep[0m  [36/42], [94mLoss[0m : 2.54890
[1mStep[0m  [40/42], [94mLoss[0m : 2.67384

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.524, [92mTest[0m: 2.370, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41625
[1mStep[0m  [4/42], [94mLoss[0m : 2.19987
[1mStep[0m  [8/42], [94mLoss[0m : 2.54762
[1mStep[0m  [12/42], [94mLoss[0m : 2.44707
[1mStep[0m  [16/42], [94mLoss[0m : 2.61650
[1mStep[0m  [20/42], [94mLoss[0m : 2.53503
[1mStep[0m  [24/42], [94mLoss[0m : 2.46798
[1mStep[0m  [28/42], [94mLoss[0m : 2.38626
[1mStep[0m  [32/42], [94mLoss[0m : 2.52437
[1mStep[0m  [36/42], [94mLoss[0m : 2.64097
[1mStep[0m  [40/42], [94mLoss[0m : 2.49877

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.529, [92mTest[0m: 2.359, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53878
[1mStep[0m  [4/42], [94mLoss[0m : 2.54792
[1mStep[0m  [8/42], [94mLoss[0m : 2.57663
[1mStep[0m  [12/42], [94mLoss[0m : 2.53431
[1mStep[0m  [16/42], [94mLoss[0m : 2.55726
[1mStep[0m  [20/42], [94mLoss[0m : 2.58595
[1mStep[0m  [24/42], [94mLoss[0m : 2.52294
[1mStep[0m  [28/42], [94mLoss[0m : 2.47666
[1mStep[0m  [32/42], [94mLoss[0m : 2.53359
[1mStep[0m  [36/42], [94mLoss[0m : 2.41642
[1mStep[0m  [40/42], [94mLoss[0m : 2.38266

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.359, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.67166
[1mStep[0m  [4/42], [94mLoss[0m : 2.51452
[1mStep[0m  [8/42], [94mLoss[0m : 2.72365
[1mStep[0m  [12/42], [94mLoss[0m : 2.43756
[1mStep[0m  [16/42], [94mLoss[0m : 2.52858
[1mStep[0m  [20/42], [94mLoss[0m : 2.44732
[1mStep[0m  [24/42], [94mLoss[0m : 2.27241
[1mStep[0m  [28/42], [94mLoss[0m : 2.47539
[1mStep[0m  [32/42], [94mLoss[0m : 2.37522
[1mStep[0m  [36/42], [94mLoss[0m : 2.81995
[1mStep[0m  [40/42], [94mLoss[0m : 2.57566

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.353, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53004
[1mStep[0m  [4/42], [94mLoss[0m : 2.64520
[1mStep[0m  [8/42], [94mLoss[0m : 2.58177
[1mStep[0m  [12/42], [94mLoss[0m : 2.29468
[1mStep[0m  [16/42], [94mLoss[0m : 2.37085
[1mStep[0m  [20/42], [94mLoss[0m : 2.33437
[1mStep[0m  [24/42], [94mLoss[0m : 2.59283
[1mStep[0m  [28/42], [94mLoss[0m : 2.69333
[1mStep[0m  [32/42], [94mLoss[0m : 2.55806
[1mStep[0m  [36/42], [94mLoss[0m : 2.44834
[1mStep[0m  [40/42], [94mLoss[0m : 2.61198

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.359, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62146
[1mStep[0m  [4/42], [94mLoss[0m : 2.37695
[1mStep[0m  [8/42], [94mLoss[0m : 2.49908
[1mStep[0m  [12/42], [94mLoss[0m : 2.35623
[1mStep[0m  [16/42], [94mLoss[0m : 2.43929
[1mStep[0m  [20/42], [94mLoss[0m : 2.72790
[1mStep[0m  [24/42], [94mLoss[0m : 2.34560
[1mStep[0m  [28/42], [94mLoss[0m : 2.67596
[1mStep[0m  [32/42], [94mLoss[0m : 2.51280
[1mStep[0m  [36/42], [94mLoss[0m : 2.27058
[1mStep[0m  [40/42], [94mLoss[0m : 2.55151

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.351, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.63691
[1mStep[0m  [4/42], [94mLoss[0m : 2.31244
[1mStep[0m  [8/42], [94mLoss[0m : 2.39808
[1mStep[0m  [12/42], [94mLoss[0m : 2.41799
[1mStep[0m  [16/42], [94mLoss[0m : 2.59358
[1mStep[0m  [20/42], [94mLoss[0m : 2.42448
[1mStep[0m  [24/42], [94mLoss[0m : 2.49738
[1mStep[0m  [28/42], [94mLoss[0m : 2.50062
[1mStep[0m  [32/42], [94mLoss[0m : 2.69789
[1mStep[0m  [36/42], [94mLoss[0m : 2.51469
[1mStep[0m  [40/42], [94mLoss[0m : 2.68834

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.352, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61763
[1mStep[0m  [4/42], [94mLoss[0m : 2.47667
[1mStep[0m  [8/42], [94mLoss[0m : 2.78567
[1mStep[0m  [12/42], [94mLoss[0m : 2.24892
[1mStep[0m  [16/42], [94mLoss[0m : 2.38774
[1mStep[0m  [20/42], [94mLoss[0m : 2.48609
[1mStep[0m  [24/42], [94mLoss[0m : 2.68158
[1mStep[0m  [28/42], [94mLoss[0m : 2.50558
[1mStep[0m  [32/42], [94mLoss[0m : 2.28343
[1mStep[0m  [36/42], [94mLoss[0m : 2.43419
[1mStep[0m  [40/42], [94mLoss[0m : 2.34312

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.352, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59895
[1mStep[0m  [4/42], [94mLoss[0m : 2.75757
[1mStep[0m  [8/42], [94mLoss[0m : 2.33956
[1mStep[0m  [12/42], [94mLoss[0m : 2.69288
[1mStep[0m  [16/42], [94mLoss[0m : 2.54420
[1mStep[0m  [20/42], [94mLoss[0m : 2.48132
[1mStep[0m  [24/42], [94mLoss[0m : 2.47569
[1mStep[0m  [28/42], [94mLoss[0m : 2.51829
[1mStep[0m  [32/42], [94mLoss[0m : 2.61339
[1mStep[0m  [36/42], [94mLoss[0m : 2.52452
[1mStep[0m  [40/42], [94mLoss[0m : 2.65294

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.514, [92mTest[0m: 2.347, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.66027
[1mStep[0m  [4/42], [94mLoss[0m : 2.56080
[1mStep[0m  [8/42], [94mLoss[0m : 2.25044
[1mStep[0m  [12/42], [94mLoss[0m : 2.62102
[1mStep[0m  [16/42], [94mLoss[0m : 2.35067
[1mStep[0m  [20/42], [94mLoss[0m : 2.52467
[1mStep[0m  [24/42], [94mLoss[0m : 2.55278
[1mStep[0m  [28/42], [94mLoss[0m : 2.39700
[1mStep[0m  [32/42], [94mLoss[0m : 2.54146
[1mStep[0m  [36/42], [94mLoss[0m : 2.50686
[1mStep[0m  [40/42], [94mLoss[0m : 2.52287

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.517, [92mTest[0m: 2.347, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41539
[1mStep[0m  [4/42], [94mLoss[0m : 2.46782
[1mStep[0m  [8/42], [94mLoss[0m : 2.45905
[1mStep[0m  [12/42], [94mLoss[0m : 2.47288
[1mStep[0m  [16/42], [94mLoss[0m : 2.62540
[1mStep[0m  [20/42], [94mLoss[0m : 2.38119
[1mStep[0m  [24/42], [94mLoss[0m : 2.61335
[1mStep[0m  [28/42], [94mLoss[0m : 2.60273
[1mStep[0m  [32/42], [94mLoss[0m : 2.50312
[1mStep[0m  [36/42], [94mLoss[0m : 2.25458
[1mStep[0m  [40/42], [94mLoss[0m : 2.60679

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.339, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48723
[1mStep[0m  [4/42], [94mLoss[0m : 2.58450
[1mStep[0m  [8/42], [94mLoss[0m : 2.57357
[1mStep[0m  [12/42], [94mLoss[0m : 2.38761
[1mStep[0m  [16/42], [94mLoss[0m : 2.56967
[1mStep[0m  [20/42], [94mLoss[0m : 2.46824
[1mStep[0m  [24/42], [94mLoss[0m : 2.52580
[1mStep[0m  [28/42], [94mLoss[0m : 2.59770
[1mStep[0m  [32/42], [94mLoss[0m : 2.63961
[1mStep[0m  [36/42], [94mLoss[0m : 2.34344
[1mStep[0m  [40/42], [94mLoss[0m : 2.55586

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.338, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.25090
[1mStep[0m  [4/42], [94mLoss[0m : 2.64016
[1mStep[0m  [8/42], [94mLoss[0m : 2.72636
[1mStep[0m  [12/42], [94mLoss[0m : 2.70628
[1mStep[0m  [16/42], [94mLoss[0m : 2.50730
[1mStep[0m  [20/42], [94mLoss[0m : 2.35723
[1mStep[0m  [24/42], [94mLoss[0m : 2.74831
[1mStep[0m  [28/42], [94mLoss[0m : 2.43371
[1mStep[0m  [32/42], [94mLoss[0m : 2.43643
[1mStep[0m  [36/42], [94mLoss[0m : 2.54513
[1mStep[0m  [40/42], [94mLoss[0m : 2.46963

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.342, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53302
[1mStep[0m  [4/42], [94mLoss[0m : 2.33288
[1mStep[0m  [8/42], [94mLoss[0m : 2.32099
[1mStep[0m  [12/42], [94mLoss[0m : 2.69876
[1mStep[0m  [16/42], [94mLoss[0m : 2.51511
[1mStep[0m  [20/42], [94mLoss[0m : 2.51200
[1mStep[0m  [24/42], [94mLoss[0m : 2.40853
[1mStep[0m  [28/42], [94mLoss[0m : 2.39290
[1mStep[0m  [32/42], [94mLoss[0m : 2.68339
[1mStep[0m  [36/42], [94mLoss[0m : 2.43748
[1mStep[0m  [40/42], [94mLoss[0m : 2.26613

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.340, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62336
[1mStep[0m  [4/42], [94mLoss[0m : 2.41638
[1mStep[0m  [8/42], [94mLoss[0m : 2.53040
[1mStep[0m  [12/42], [94mLoss[0m : 2.53115
[1mStep[0m  [16/42], [94mLoss[0m : 2.32336
[1mStep[0m  [20/42], [94mLoss[0m : 2.49803
[1mStep[0m  [24/42], [94mLoss[0m : 2.51198
[1mStep[0m  [28/42], [94mLoss[0m : 2.60817
[1mStep[0m  [32/42], [94mLoss[0m : 2.49806
[1mStep[0m  [36/42], [94mLoss[0m : 2.50275
[1mStep[0m  [40/42], [94mLoss[0m : 2.56315

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.340, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29197
[1mStep[0m  [4/42], [94mLoss[0m : 2.31678
[1mStep[0m  [8/42], [94mLoss[0m : 2.51405
[1mStep[0m  [12/42], [94mLoss[0m : 2.32877
[1mStep[0m  [16/42], [94mLoss[0m : 2.19357
[1mStep[0m  [20/42], [94mLoss[0m : 2.33373
[1mStep[0m  [24/42], [94mLoss[0m : 2.53104
[1mStep[0m  [28/42], [94mLoss[0m : 2.61436
[1mStep[0m  [32/42], [94mLoss[0m : 2.30959
[1mStep[0m  [36/42], [94mLoss[0m : 2.22063
[1mStep[0m  [40/42], [94mLoss[0m : 2.51739

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.332, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50287
[1mStep[0m  [4/42], [94mLoss[0m : 2.60953
[1mStep[0m  [8/42], [94mLoss[0m : 2.41086
[1mStep[0m  [12/42], [94mLoss[0m : 2.57776
[1mStep[0m  [16/42], [94mLoss[0m : 2.53565
[1mStep[0m  [20/42], [94mLoss[0m : 2.57558
[1mStep[0m  [24/42], [94mLoss[0m : 2.47632
[1mStep[0m  [28/42], [94mLoss[0m : 2.37422
[1mStep[0m  [32/42], [94mLoss[0m : 2.41515
[1mStep[0m  [36/42], [94mLoss[0m : 2.58245
[1mStep[0m  [40/42], [94mLoss[0m : 2.41506

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.333, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.334
====================================

Phase 1 - Evaluation MAE:  2.3343679223741804
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 2.33639
[1mStep[0m  [4/42], [94mLoss[0m : 2.56765
[1mStep[0m  [8/42], [94mLoss[0m : 2.40054
[1mStep[0m  [12/42], [94mLoss[0m : 2.52950
[1mStep[0m  [16/42], [94mLoss[0m : 2.40283
[1mStep[0m  [20/42], [94mLoss[0m : 2.53407
[1mStep[0m  [24/42], [94mLoss[0m : 2.53422
[1mStep[0m  [28/42], [94mLoss[0m : 2.42965
[1mStep[0m  [32/42], [94mLoss[0m : 2.43272
[1mStep[0m  [36/42], [94mLoss[0m : 2.54884
[1mStep[0m  [40/42], [94mLoss[0m : 2.60850

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.331, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39054
[1mStep[0m  [4/42], [94mLoss[0m : 2.36856
[1mStep[0m  [8/42], [94mLoss[0m : 2.37425
[1mStep[0m  [12/42], [94mLoss[0m : 2.51130
[1mStep[0m  [16/42], [94mLoss[0m : 2.43569
[1mStep[0m  [20/42], [94mLoss[0m : 2.34425
[1mStep[0m  [24/42], [94mLoss[0m : 2.39142
[1mStep[0m  [28/42], [94mLoss[0m : 2.33602
[1mStep[0m  [32/42], [94mLoss[0m : 2.43224
[1mStep[0m  [36/42], [94mLoss[0m : 2.58586
[1mStep[0m  [40/42], [94mLoss[0m : 2.47124

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.370, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49003
[1mStep[0m  [4/42], [94mLoss[0m : 2.53110
[1mStep[0m  [8/42], [94mLoss[0m : 2.70493
[1mStep[0m  [12/42], [94mLoss[0m : 2.40677
[1mStep[0m  [16/42], [94mLoss[0m : 2.44576
[1mStep[0m  [20/42], [94mLoss[0m : 2.36282
[1mStep[0m  [24/42], [94mLoss[0m : 2.53494
[1mStep[0m  [28/42], [94mLoss[0m : 2.56899
[1mStep[0m  [32/42], [94mLoss[0m : 2.58888
[1mStep[0m  [36/42], [94mLoss[0m : 2.57629
[1mStep[0m  [40/42], [94mLoss[0m : 2.53030

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.327, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45324
[1mStep[0m  [4/42], [94mLoss[0m : 2.57654
[1mStep[0m  [8/42], [94mLoss[0m : 2.53253
[1mStep[0m  [12/42], [94mLoss[0m : 2.63996
[1mStep[0m  [16/42], [94mLoss[0m : 2.58277
[1mStep[0m  [20/42], [94mLoss[0m : 2.56993
[1mStep[0m  [24/42], [94mLoss[0m : 2.37103
[1mStep[0m  [28/42], [94mLoss[0m : 2.38415
[1mStep[0m  [32/42], [94mLoss[0m : 2.44463
[1mStep[0m  [36/42], [94mLoss[0m : 2.45136
[1mStep[0m  [40/42], [94mLoss[0m : 2.51528

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.333, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43492
[1mStep[0m  [4/42], [94mLoss[0m : 2.44070
[1mStep[0m  [8/42], [94mLoss[0m : 2.31846
[1mStep[0m  [12/42], [94mLoss[0m : 2.62398
[1mStep[0m  [16/42], [94mLoss[0m : 2.65750
[1mStep[0m  [20/42], [94mLoss[0m : 2.31202
[1mStep[0m  [24/42], [94mLoss[0m : 2.44763
[1mStep[0m  [28/42], [94mLoss[0m : 2.49643
[1mStep[0m  [32/42], [94mLoss[0m : 2.43174
[1mStep[0m  [36/42], [94mLoss[0m : 2.68879
[1mStep[0m  [40/42], [94mLoss[0m : 2.18392

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.318, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41930
[1mStep[0m  [4/42], [94mLoss[0m : 2.38310
[1mStep[0m  [8/42], [94mLoss[0m : 2.61461
[1mStep[0m  [12/42], [94mLoss[0m : 2.36015
[1mStep[0m  [16/42], [94mLoss[0m : 2.32635
[1mStep[0m  [20/42], [94mLoss[0m : 2.30189
[1mStep[0m  [24/42], [94mLoss[0m : 2.54724
[1mStep[0m  [28/42], [94mLoss[0m : 2.46182
[1mStep[0m  [32/42], [94mLoss[0m : 2.45231
[1mStep[0m  [36/42], [94mLoss[0m : 2.47159
[1mStep[0m  [40/42], [94mLoss[0m : 2.20559

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.344, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35020
[1mStep[0m  [4/42], [94mLoss[0m : 2.50957
[1mStep[0m  [8/42], [94mLoss[0m : 2.44375
[1mStep[0m  [12/42], [94mLoss[0m : 2.67677
[1mStep[0m  [16/42], [94mLoss[0m : 2.62784
[1mStep[0m  [20/42], [94mLoss[0m : 2.46003
[1mStep[0m  [24/42], [94mLoss[0m : 2.42240
[1mStep[0m  [28/42], [94mLoss[0m : 2.57116
[1mStep[0m  [32/42], [94mLoss[0m : 2.38175
[1mStep[0m  [36/42], [94mLoss[0m : 2.47589
[1mStep[0m  [40/42], [94mLoss[0m : 2.50597

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.366, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58132
[1mStep[0m  [4/42], [94mLoss[0m : 2.33413
[1mStep[0m  [8/42], [94mLoss[0m : 2.38841
[1mStep[0m  [12/42], [94mLoss[0m : 2.37896
[1mStep[0m  [16/42], [94mLoss[0m : 2.66860
[1mStep[0m  [20/42], [94mLoss[0m : 2.39488
[1mStep[0m  [24/42], [94mLoss[0m : 2.25625
[1mStep[0m  [28/42], [94mLoss[0m : 2.44328
[1mStep[0m  [32/42], [94mLoss[0m : 2.53018
[1mStep[0m  [36/42], [94mLoss[0m : 2.40638
[1mStep[0m  [40/42], [94mLoss[0m : 2.23633

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.400, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28460
[1mStep[0m  [4/42], [94mLoss[0m : 2.37626
[1mStep[0m  [8/42], [94mLoss[0m : 2.46768
[1mStep[0m  [12/42], [94mLoss[0m : 2.27944
[1mStep[0m  [16/42], [94mLoss[0m : 2.40377
[1mStep[0m  [20/42], [94mLoss[0m : 2.28432
[1mStep[0m  [24/42], [94mLoss[0m : 2.44804
[1mStep[0m  [28/42], [94mLoss[0m : 2.49893
[1mStep[0m  [32/42], [94mLoss[0m : 2.39770
[1mStep[0m  [36/42], [94mLoss[0m : 2.50219
[1mStep[0m  [40/42], [94mLoss[0m : 2.30695

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.393, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49197
[1mStep[0m  [4/42], [94mLoss[0m : 2.37159
[1mStep[0m  [8/42], [94mLoss[0m : 2.50917
[1mStep[0m  [12/42], [94mLoss[0m : 2.45980
[1mStep[0m  [16/42], [94mLoss[0m : 2.39218
[1mStep[0m  [20/42], [94mLoss[0m : 2.35146
[1mStep[0m  [24/42], [94mLoss[0m : 2.41674
[1mStep[0m  [28/42], [94mLoss[0m : 2.37045
[1mStep[0m  [32/42], [94mLoss[0m : 2.39033
[1mStep[0m  [36/42], [94mLoss[0m : 2.39550
[1mStep[0m  [40/42], [94mLoss[0m : 2.41187

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.393, [92mTest[0m: 2.369, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46980
[1mStep[0m  [4/42], [94mLoss[0m : 2.34278
[1mStep[0m  [8/42], [94mLoss[0m : 2.33882
[1mStep[0m  [12/42], [94mLoss[0m : 2.24281
[1mStep[0m  [16/42], [94mLoss[0m : 2.18691
[1mStep[0m  [20/42], [94mLoss[0m : 2.56108
[1mStep[0m  [24/42], [94mLoss[0m : 2.66863
[1mStep[0m  [28/42], [94mLoss[0m : 2.13990
[1mStep[0m  [32/42], [94mLoss[0m : 2.39068
[1mStep[0m  [36/42], [94mLoss[0m : 2.25788
[1mStep[0m  [40/42], [94mLoss[0m : 2.54799

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.371, [92mTest[0m: 2.402, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.65844
[1mStep[0m  [4/42], [94mLoss[0m : 2.57564
[1mStep[0m  [8/42], [94mLoss[0m : 2.43027
[1mStep[0m  [12/42], [94mLoss[0m : 2.23192
[1mStep[0m  [16/42], [94mLoss[0m : 2.27881
[1mStep[0m  [20/42], [94mLoss[0m : 2.39889
[1mStep[0m  [24/42], [94mLoss[0m : 2.28555
[1mStep[0m  [28/42], [94mLoss[0m : 2.38987
[1mStep[0m  [32/42], [94mLoss[0m : 2.30967
[1mStep[0m  [36/42], [94mLoss[0m : 2.27693
[1mStep[0m  [40/42], [94mLoss[0m : 2.48628

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.343, [92mTest[0m: 2.447, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27224
[1mStep[0m  [4/42], [94mLoss[0m : 2.20424
[1mStep[0m  [8/42], [94mLoss[0m : 2.25364
[1mStep[0m  [12/42], [94mLoss[0m : 2.46151
[1mStep[0m  [16/42], [94mLoss[0m : 2.39569
[1mStep[0m  [20/42], [94mLoss[0m : 2.16860
[1mStep[0m  [24/42], [94mLoss[0m : 2.55392
[1mStep[0m  [28/42], [94mLoss[0m : 2.45932
[1mStep[0m  [32/42], [94mLoss[0m : 2.65488
[1mStep[0m  [36/42], [94mLoss[0m : 2.25382
[1mStep[0m  [40/42], [94mLoss[0m : 2.22042

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.348, [92mTest[0m: 2.434, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27791
[1mStep[0m  [4/42], [94mLoss[0m : 2.35977
[1mStep[0m  [8/42], [94mLoss[0m : 2.13886
[1mStep[0m  [12/42], [94mLoss[0m : 2.31909
[1mStep[0m  [16/42], [94mLoss[0m : 2.37403
[1mStep[0m  [20/42], [94mLoss[0m : 2.52068
[1mStep[0m  [24/42], [94mLoss[0m : 2.35187
[1mStep[0m  [28/42], [94mLoss[0m : 2.17276
[1mStep[0m  [32/42], [94mLoss[0m : 2.23093
[1mStep[0m  [36/42], [94mLoss[0m : 2.46544
[1mStep[0m  [40/42], [94mLoss[0m : 2.32878

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.314, [92mTest[0m: 2.434, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40392
[1mStep[0m  [4/42], [94mLoss[0m : 2.32225
[1mStep[0m  [8/42], [94mLoss[0m : 2.28024
[1mStep[0m  [12/42], [94mLoss[0m : 2.39338
[1mStep[0m  [16/42], [94mLoss[0m : 2.18319
[1mStep[0m  [20/42], [94mLoss[0m : 2.39451
[1mStep[0m  [24/42], [94mLoss[0m : 2.53191
[1mStep[0m  [28/42], [94mLoss[0m : 2.13747
[1mStep[0m  [32/42], [94mLoss[0m : 2.42472
[1mStep[0m  [36/42], [94mLoss[0m : 2.20297
[1mStep[0m  [40/42], [94mLoss[0m : 2.38474

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.306, [92mTest[0m: 2.463, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39015
[1mStep[0m  [4/42], [94mLoss[0m : 2.32547
[1mStep[0m  [8/42], [94mLoss[0m : 2.36134
[1mStep[0m  [12/42], [94mLoss[0m : 2.35365
[1mStep[0m  [16/42], [94mLoss[0m : 2.16368
[1mStep[0m  [20/42], [94mLoss[0m : 2.07611
[1mStep[0m  [24/42], [94mLoss[0m : 2.37721
[1mStep[0m  [28/42], [94mLoss[0m : 2.27581
[1mStep[0m  [32/42], [94mLoss[0m : 2.41274
[1mStep[0m  [36/42], [94mLoss[0m : 2.32612
[1mStep[0m  [40/42], [94mLoss[0m : 2.47106

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.294, [92mTest[0m: 2.453, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.10598
[1mStep[0m  [4/42], [94mLoss[0m : 2.37667
[1mStep[0m  [8/42], [94mLoss[0m : 2.24578
[1mStep[0m  [12/42], [94mLoss[0m : 2.25605
[1mStep[0m  [16/42], [94mLoss[0m : 2.31282
[1mStep[0m  [20/42], [94mLoss[0m : 2.32851
[1mStep[0m  [24/42], [94mLoss[0m : 2.19144
[1mStep[0m  [28/42], [94mLoss[0m : 2.49965
[1mStep[0m  [32/42], [94mLoss[0m : 2.26397
[1mStep[0m  [36/42], [94mLoss[0m : 2.24197
[1mStep[0m  [40/42], [94mLoss[0m : 2.36970

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.256, [92mTest[0m: 2.451, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40243
[1mStep[0m  [4/42], [94mLoss[0m : 2.53567
[1mStep[0m  [8/42], [94mLoss[0m : 2.11218
[1mStep[0m  [12/42], [94mLoss[0m : 2.38637
[1mStep[0m  [16/42], [94mLoss[0m : 2.17737
[1mStep[0m  [20/42], [94mLoss[0m : 2.06275
[1mStep[0m  [24/42], [94mLoss[0m : 2.30763
[1mStep[0m  [28/42], [94mLoss[0m : 2.11639
[1mStep[0m  [32/42], [94mLoss[0m : 2.10979
[1mStep[0m  [36/42], [94mLoss[0m : 2.17260
[1mStep[0m  [40/42], [94mLoss[0m : 2.25514

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.252, [92mTest[0m: 2.463, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.09097
[1mStep[0m  [4/42], [94mLoss[0m : 2.05317
[1mStep[0m  [8/42], [94mLoss[0m : 2.46679
[1mStep[0m  [12/42], [94mLoss[0m : 2.29814
[1mStep[0m  [16/42], [94mLoss[0m : 2.41479
[1mStep[0m  [20/42], [94mLoss[0m : 2.02160
[1mStep[0m  [24/42], [94mLoss[0m : 2.14186
[1mStep[0m  [28/42], [94mLoss[0m : 2.25581
[1mStep[0m  [32/42], [94mLoss[0m : 2.12092
[1mStep[0m  [36/42], [94mLoss[0m : 2.29636
[1mStep[0m  [40/42], [94mLoss[0m : 2.12899

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.222, [92mTest[0m: 2.500, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21198
[1mStep[0m  [4/42], [94mLoss[0m : 2.35899
[1mStep[0m  [8/42], [94mLoss[0m : 2.06275
[1mStep[0m  [12/42], [94mLoss[0m : 2.11876
[1mStep[0m  [16/42], [94mLoss[0m : 2.29508
[1mStep[0m  [20/42], [94mLoss[0m : 2.05611
[1mStep[0m  [24/42], [94mLoss[0m : 2.23381
[1mStep[0m  [28/42], [94mLoss[0m : 2.41168
[1mStep[0m  [32/42], [94mLoss[0m : 2.29726
[1mStep[0m  [36/42], [94mLoss[0m : 2.10954
[1mStep[0m  [40/42], [94mLoss[0m : 2.28629

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.193, [92mTest[0m: 2.490, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26172
[1mStep[0m  [4/42], [94mLoss[0m : 2.18405
[1mStep[0m  [8/42], [94mLoss[0m : 2.05846
[1mStep[0m  [12/42], [94mLoss[0m : 2.10624
[1mStep[0m  [16/42], [94mLoss[0m : 2.18978
[1mStep[0m  [20/42], [94mLoss[0m : 2.29891
[1mStep[0m  [24/42], [94mLoss[0m : 2.35255
[1mStep[0m  [28/42], [94mLoss[0m : 2.05074
[1mStep[0m  [32/42], [94mLoss[0m : 2.00747
[1mStep[0m  [36/42], [94mLoss[0m : 2.15625
[1mStep[0m  [40/42], [94mLoss[0m : 2.35611

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.191, [92mTest[0m: 2.495, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27554
[1mStep[0m  [4/42], [94mLoss[0m : 2.11489
[1mStep[0m  [8/42], [94mLoss[0m : 2.10682
[1mStep[0m  [12/42], [94mLoss[0m : 1.96320
[1mStep[0m  [16/42], [94mLoss[0m : 2.27238
[1mStep[0m  [20/42], [94mLoss[0m : 2.05978
[1mStep[0m  [24/42], [94mLoss[0m : 2.11893
[1mStep[0m  [28/42], [94mLoss[0m : 2.05078
[1mStep[0m  [32/42], [94mLoss[0m : 2.12891
[1mStep[0m  [36/42], [94mLoss[0m : 2.13116
[1mStep[0m  [40/42], [94mLoss[0m : 2.06909

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.154, [92mTest[0m: 2.482, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.19214
[1mStep[0m  [4/42], [94mLoss[0m : 2.09993
[1mStep[0m  [8/42], [94mLoss[0m : 1.87217
[1mStep[0m  [12/42], [94mLoss[0m : 2.20806
[1mStep[0m  [16/42], [94mLoss[0m : 2.23466
[1mStep[0m  [20/42], [94mLoss[0m : 2.14898
[1mStep[0m  [24/42], [94mLoss[0m : 2.08409
[1mStep[0m  [28/42], [94mLoss[0m : 2.11448
[1mStep[0m  [32/42], [94mLoss[0m : 2.11542
[1mStep[0m  [36/42], [94mLoss[0m : 2.17278
[1mStep[0m  [40/42], [94mLoss[0m : 2.14523

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.117, [92mTest[0m: 2.464, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.06846
[1mStep[0m  [4/42], [94mLoss[0m : 2.36983
[1mStep[0m  [8/42], [94mLoss[0m : 2.01748
[1mStep[0m  [12/42], [94mLoss[0m : 1.83746
[1mStep[0m  [16/42], [94mLoss[0m : 2.23232
[1mStep[0m  [20/42], [94mLoss[0m : 2.03555
[1mStep[0m  [24/42], [94mLoss[0m : 2.11504
[1mStep[0m  [28/42], [94mLoss[0m : 2.09115
[1mStep[0m  [32/42], [94mLoss[0m : 2.12242
[1mStep[0m  [36/42], [94mLoss[0m : 2.16100
[1mStep[0m  [40/42], [94mLoss[0m : 2.07152

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.092, [92mTest[0m: 2.591, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.95049
[1mStep[0m  [4/42], [94mLoss[0m : 2.16307
[1mStep[0m  [8/42], [94mLoss[0m : 2.03807
[1mStep[0m  [12/42], [94mLoss[0m : 2.11386
[1mStep[0m  [16/42], [94mLoss[0m : 2.14008
[1mStep[0m  [20/42], [94mLoss[0m : 2.11592
[1mStep[0m  [24/42], [94mLoss[0m : 2.07345
[1mStep[0m  [28/42], [94mLoss[0m : 2.11169
[1mStep[0m  [32/42], [94mLoss[0m : 2.22245
[1mStep[0m  [36/42], [94mLoss[0m : 2.13253
[1mStep[0m  [40/42], [94mLoss[0m : 2.16334

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.083, [92mTest[0m: 2.483, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.94778
[1mStep[0m  [4/42], [94mLoss[0m : 1.97172
[1mStep[0m  [8/42], [94mLoss[0m : 2.08150
[1mStep[0m  [12/42], [94mLoss[0m : 2.18660
[1mStep[0m  [16/42], [94mLoss[0m : 2.08268
[1mStep[0m  [20/42], [94mLoss[0m : 2.12478
[1mStep[0m  [24/42], [94mLoss[0m : 1.97360
[1mStep[0m  [28/42], [94mLoss[0m : 1.99689
[1mStep[0m  [32/42], [94mLoss[0m : 2.05700
[1mStep[0m  [36/42], [94mLoss[0m : 2.03934
[1mStep[0m  [40/42], [94mLoss[0m : 2.05400

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.048, [92mTest[0m: 2.524, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.87368
[1mStep[0m  [4/42], [94mLoss[0m : 2.03839
[1mStep[0m  [8/42], [94mLoss[0m : 2.27432
[1mStep[0m  [12/42], [94mLoss[0m : 2.08047
[1mStep[0m  [16/42], [94mLoss[0m : 1.93221
[1mStep[0m  [20/42], [94mLoss[0m : 2.03565
[1mStep[0m  [24/42], [94mLoss[0m : 2.09767
[1mStep[0m  [28/42], [94mLoss[0m : 2.07977
[1mStep[0m  [32/42], [94mLoss[0m : 1.99463
[1mStep[0m  [36/42], [94mLoss[0m : 2.16955
[1mStep[0m  [40/42], [94mLoss[0m : 1.93482

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.040, [92mTest[0m: 2.506, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.79522
[1mStep[0m  [4/42], [94mLoss[0m : 1.95777
[1mStep[0m  [8/42], [94mLoss[0m : 1.98925
[1mStep[0m  [12/42], [94mLoss[0m : 1.83705
[1mStep[0m  [16/42], [94mLoss[0m : 2.06894
[1mStep[0m  [20/42], [94mLoss[0m : 2.05538
[1mStep[0m  [24/42], [94mLoss[0m : 2.06671
[1mStep[0m  [28/42], [94mLoss[0m : 1.82158
[1mStep[0m  [32/42], [94mLoss[0m : 2.05368
[1mStep[0m  [36/42], [94mLoss[0m : 2.00570
[1mStep[0m  [40/42], [94mLoss[0m : 1.94770

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.004, [92mTest[0m: 2.463, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.98799
[1mStep[0m  [4/42], [94mLoss[0m : 2.10113
[1mStep[0m  [8/42], [94mLoss[0m : 2.13545
[1mStep[0m  [12/42], [94mLoss[0m : 2.02824
[1mStep[0m  [16/42], [94mLoss[0m : 1.87439
[1mStep[0m  [20/42], [94mLoss[0m : 1.80456
[1mStep[0m  [24/42], [94mLoss[0m : 1.97454
[1mStep[0m  [28/42], [94mLoss[0m : 1.78415
[1mStep[0m  [32/42], [94mLoss[0m : 2.04558
[1mStep[0m  [36/42], [94mLoss[0m : 2.02018
[1mStep[0m  [40/42], [94mLoss[0m : 2.05911

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.991, [92mTest[0m: 2.474, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.98087
[1mStep[0m  [4/42], [94mLoss[0m : 2.08758
[1mStep[0m  [8/42], [94mLoss[0m : 1.99231
[1mStep[0m  [12/42], [94mLoss[0m : 1.99356
[1mStep[0m  [16/42], [94mLoss[0m : 1.80878
[1mStep[0m  [20/42], [94mLoss[0m : 1.97109
[1mStep[0m  [24/42], [94mLoss[0m : 1.89585
[1mStep[0m  [28/42], [94mLoss[0m : 2.00756
[1mStep[0m  [32/42], [94mLoss[0m : 1.96399
[1mStep[0m  [36/42], [94mLoss[0m : 2.25804
[1mStep[0m  [40/42], [94mLoss[0m : 2.06880

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.971, [92mTest[0m: 2.556, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.480
====================================

Phase 2 - Evaluation MAE:  2.4799546684537614
MAE score P1       2.334368
MAE score P2       2.479955
loss               1.970998
learning_rate      0.002575
batch_size              256
hidden_sizes      [200, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.5
weight_decay           0.01
Name: 17, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 11.71140
[1mStep[0m  [4/42], [94mLoss[0m : 10.85802
[1mStep[0m  [8/42], [94mLoss[0m : 8.98244
[1mStep[0m  [12/42], [94mLoss[0m : 7.59231
[1mStep[0m  [16/42], [94mLoss[0m : 5.79846
[1mStep[0m  [20/42], [94mLoss[0m : 4.05016
[1mStep[0m  [24/42], [94mLoss[0m : 2.78919
[1mStep[0m  [28/42], [94mLoss[0m : 2.71127
[1mStep[0m  [32/42], [94mLoss[0m : 3.05568
[1mStep[0m  [36/42], [94mLoss[0m : 2.92540
[1mStep[0m  [40/42], [94mLoss[0m : 3.09012

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 5.560, [92mTest[0m: 11.332, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.86626
[1mStep[0m  [4/42], [94mLoss[0m : 2.62373
[1mStep[0m  [8/42], [94mLoss[0m : 2.46379
[1mStep[0m  [12/42], [94mLoss[0m : 2.39502
[1mStep[0m  [16/42], [94mLoss[0m : 2.62399
[1mStep[0m  [20/42], [94mLoss[0m : 2.48310
[1mStep[0m  [24/42], [94mLoss[0m : 2.28281
[1mStep[0m  [28/42], [94mLoss[0m : 2.55453
[1mStep[0m  [32/42], [94mLoss[0m : 2.56214
[1mStep[0m  [36/42], [94mLoss[0m : 2.39891
[1mStep[0m  [40/42], [94mLoss[0m : 2.54809

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.545, [92mTest[0m: 2.746, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38475
[1mStep[0m  [4/42], [94mLoss[0m : 2.74300
[1mStep[0m  [8/42], [94mLoss[0m : 2.28025
[1mStep[0m  [12/42], [94mLoss[0m : 2.48268
[1mStep[0m  [16/42], [94mLoss[0m : 2.50366
[1mStep[0m  [20/42], [94mLoss[0m : 2.40229
[1mStep[0m  [24/42], [94mLoss[0m : 2.44319
[1mStep[0m  [28/42], [94mLoss[0m : 2.53513
[1mStep[0m  [32/42], [94mLoss[0m : 2.50374
[1mStep[0m  [36/42], [94mLoss[0m : 2.54854
[1mStep[0m  [40/42], [94mLoss[0m : 2.42748

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.388, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56269
[1mStep[0m  [4/42], [94mLoss[0m : 2.38143
[1mStep[0m  [8/42], [94mLoss[0m : 2.18647
[1mStep[0m  [12/42], [94mLoss[0m : 2.57007
[1mStep[0m  [16/42], [94mLoss[0m : 2.54140
[1mStep[0m  [20/42], [94mLoss[0m : 2.34495
[1mStep[0m  [24/42], [94mLoss[0m : 2.65139
[1mStep[0m  [28/42], [94mLoss[0m : 2.72698
[1mStep[0m  [32/42], [94mLoss[0m : 2.41460
[1mStep[0m  [36/42], [94mLoss[0m : 2.57185
[1mStep[0m  [40/42], [94mLoss[0m : 2.42946

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.366, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44261
[1mStep[0m  [4/42], [94mLoss[0m : 2.15688
[1mStep[0m  [8/42], [94mLoss[0m : 2.51507
[1mStep[0m  [12/42], [94mLoss[0m : 2.52233
[1mStep[0m  [16/42], [94mLoss[0m : 2.63532
[1mStep[0m  [20/42], [94mLoss[0m : 2.64131
[1mStep[0m  [24/42], [94mLoss[0m : 2.24877
[1mStep[0m  [28/42], [94mLoss[0m : 2.42929
[1mStep[0m  [32/42], [94mLoss[0m : 2.51744
[1mStep[0m  [36/42], [94mLoss[0m : 2.44753
[1mStep[0m  [40/42], [94mLoss[0m : 2.60983

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.349, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48649
[1mStep[0m  [4/42], [94mLoss[0m : 2.72332
[1mStep[0m  [8/42], [94mLoss[0m : 2.51012
[1mStep[0m  [12/42], [94mLoss[0m : 2.45236
[1mStep[0m  [16/42], [94mLoss[0m : 2.38570
[1mStep[0m  [20/42], [94mLoss[0m : 2.33034
[1mStep[0m  [24/42], [94mLoss[0m : 2.57865
[1mStep[0m  [28/42], [94mLoss[0m : 2.48659
[1mStep[0m  [32/42], [94mLoss[0m : 2.45126
[1mStep[0m  [36/42], [94mLoss[0m : 2.50881
[1mStep[0m  [40/42], [94mLoss[0m : 2.65471

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.346, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44820
[1mStep[0m  [4/42], [94mLoss[0m : 2.30019
[1mStep[0m  [8/42], [94mLoss[0m : 2.46958
[1mStep[0m  [12/42], [94mLoss[0m : 2.60502
[1mStep[0m  [16/42], [94mLoss[0m : 2.62148
[1mStep[0m  [20/42], [94mLoss[0m : 2.49223
[1mStep[0m  [24/42], [94mLoss[0m : 2.36971
[1mStep[0m  [28/42], [94mLoss[0m : 2.30154
[1mStep[0m  [32/42], [94mLoss[0m : 2.52213
[1mStep[0m  [36/42], [94mLoss[0m : 2.51572
[1mStep[0m  [40/42], [94mLoss[0m : 2.26808

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.347, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48086
[1mStep[0m  [4/42], [94mLoss[0m : 2.37354
[1mStep[0m  [8/42], [94mLoss[0m : 2.31312
[1mStep[0m  [12/42], [94mLoss[0m : 2.33956
[1mStep[0m  [16/42], [94mLoss[0m : 2.53100
[1mStep[0m  [20/42], [94mLoss[0m : 2.49654
[1mStep[0m  [24/42], [94mLoss[0m : 2.61849
[1mStep[0m  [28/42], [94mLoss[0m : 2.64662
[1mStep[0m  [32/42], [94mLoss[0m : 2.38356
[1mStep[0m  [36/42], [94mLoss[0m : 2.63275
[1mStep[0m  [40/42], [94mLoss[0m : 2.54527

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.338, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43050
[1mStep[0m  [4/42], [94mLoss[0m : 2.70582
[1mStep[0m  [8/42], [94mLoss[0m : 2.40271
[1mStep[0m  [12/42], [94mLoss[0m : 2.43957
[1mStep[0m  [16/42], [94mLoss[0m : 2.37718
[1mStep[0m  [20/42], [94mLoss[0m : 2.37691
[1mStep[0m  [24/42], [94mLoss[0m : 2.36504
[1mStep[0m  [28/42], [94mLoss[0m : 2.56587
[1mStep[0m  [32/42], [94mLoss[0m : 2.36386
[1mStep[0m  [36/42], [94mLoss[0m : 2.35891
[1mStep[0m  [40/42], [94mLoss[0m : 2.42734

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.336, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58246
[1mStep[0m  [4/42], [94mLoss[0m : 2.44498
[1mStep[0m  [8/42], [94mLoss[0m : 2.18351
[1mStep[0m  [12/42], [94mLoss[0m : 2.39866
[1mStep[0m  [16/42], [94mLoss[0m : 2.37350
[1mStep[0m  [20/42], [94mLoss[0m : 2.36885
[1mStep[0m  [24/42], [94mLoss[0m : 2.45871
[1mStep[0m  [28/42], [94mLoss[0m : 2.56743
[1mStep[0m  [32/42], [94mLoss[0m : 2.47974
[1mStep[0m  [36/42], [94mLoss[0m : 2.53216
[1mStep[0m  [40/42], [94mLoss[0m : 2.35669

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.341, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48488
[1mStep[0m  [4/42], [94mLoss[0m : 2.56803
[1mStep[0m  [8/42], [94mLoss[0m : 2.56505
[1mStep[0m  [12/42], [94mLoss[0m : 2.42923
[1mStep[0m  [16/42], [94mLoss[0m : 2.59066
[1mStep[0m  [20/42], [94mLoss[0m : 2.73527
[1mStep[0m  [24/42], [94mLoss[0m : 2.33799
[1mStep[0m  [28/42], [94mLoss[0m : 2.24167
[1mStep[0m  [32/42], [94mLoss[0m : 2.44826
[1mStep[0m  [36/42], [94mLoss[0m : 2.44292
[1mStep[0m  [40/42], [94mLoss[0m : 2.32242

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.339, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44458
[1mStep[0m  [4/42], [94mLoss[0m : 2.52413
[1mStep[0m  [8/42], [94mLoss[0m : 2.23155
[1mStep[0m  [12/42], [94mLoss[0m : 2.43455
[1mStep[0m  [16/42], [94mLoss[0m : 2.45114
[1mStep[0m  [20/42], [94mLoss[0m : 2.47898
[1mStep[0m  [24/42], [94mLoss[0m : 2.50143
[1mStep[0m  [28/42], [94mLoss[0m : 2.72084
[1mStep[0m  [32/42], [94mLoss[0m : 2.56084
[1mStep[0m  [36/42], [94mLoss[0m : 2.39934
[1mStep[0m  [40/42], [94mLoss[0m : 2.23806

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.333, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52851
[1mStep[0m  [4/42], [94mLoss[0m : 2.48979
[1mStep[0m  [8/42], [94mLoss[0m : 2.34105
[1mStep[0m  [12/42], [94mLoss[0m : 2.47678
[1mStep[0m  [16/42], [94mLoss[0m : 2.35320
[1mStep[0m  [20/42], [94mLoss[0m : 2.35393
[1mStep[0m  [24/42], [94mLoss[0m : 2.50165
[1mStep[0m  [28/42], [94mLoss[0m : 2.45818
[1mStep[0m  [32/42], [94mLoss[0m : 2.45284
[1mStep[0m  [36/42], [94mLoss[0m : 2.47655
[1mStep[0m  [40/42], [94mLoss[0m : 2.61590

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.333, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28429
[1mStep[0m  [4/42], [94mLoss[0m : 2.58982
[1mStep[0m  [8/42], [94mLoss[0m : 2.55336
[1mStep[0m  [12/42], [94mLoss[0m : 2.41414
[1mStep[0m  [16/42], [94mLoss[0m : 2.44030
[1mStep[0m  [20/42], [94mLoss[0m : 2.39870
[1mStep[0m  [24/42], [94mLoss[0m : 2.71811
[1mStep[0m  [28/42], [94mLoss[0m : 2.24351
[1mStep[0m  [32/42], [94mLoss[0m : 2.51161
[1mStep[0m  [36/42], [94mLoss[0m : 2.34226
[1mStep[0m  [40/42], [94mLoss[0m : 2.13730

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.329, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52532
[1mStep[0m  [4/42], [94mLoss[0m : 2.57525
[1mStep[0m  [8/42], [94mLoss[0m : 2.35796
[1mStep[0m  [12/42], [94mLoss[0m : 2.57593
[1mStep[0m  [16/42], [94mLoss[0m : 2.20352
[1mStep[0m  [20/42], [94mLoss[0m : 2.55891
[1mStep[0m  [24/42], [94mLoss[0m : 2.40399
[1mStep[0m  [28/42], [94mLoss[0m : 2.41605
[1mStep[0m  [32/42], [94mLoss[0m : 2.48012
[1mStep[0m  [36/42], [94mLoss[0m : 2.39964
[1mStep[0m  [40/42], [94mLoss[0m : 2.33457

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.328, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.66430
[1mStep[0m  [4/42], [94mLoss[0m : 2.43058
[1mStep[0m  [8/42], [94mLoss[0m : 2.42537
[1mStep[0m  [12/42], [94mLoss[0m : 2.48239
[1mStep[0m  [16/42], [94mLoss[0m : 2.19253
[1mStep[0m  [20/42], [94mLoss[0m : 2.63277
[1mStep[0m  [24/42], [94mLoss[0m : 2.72044
[1mStep[0m  [28/42], [94mLoss[0m : 2.49282
[1mStep[0m  [32/42], [94mLoss[0m : 2.20865
[1mStep[0m  [36/42], [94mLoss[0m : 2.42240
[1mStep[0m  [40/42], [94mLoss[0m : 2.36622

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.335, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50357
[1mStep[0m  [4/42], [94mLoss[0m : 2.38907
[1mStep[0m  [8/42], [94mLoss[0m : 2.46861
[1mStep[0m  [12/42], [94mLoss[0m : 2.35623
[1mStep[0m  [16/42], [94mLoss[0m : 2.38102
[1mStep[0m  [20/42], [94mLoss[0m : 2.51205
[1mStep[0m  [24/42], [94mLoss[0m : 2.46433
[1mStep[0m  [28/42], [94mLoss[0m : 2.45324
[1mStep[0m  [32/42], [94mLoss[0m : 2.50154
[1mStep[0m  [36/42], [94mLoss[0m : 2.26689
[1mStep[0m  [40/42], [94mLoss[0m : 2.29789

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.324, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54059
[1mStep[0m  [4/42], [94mLoss[0m : 2.38574
[1mStep[0m  [8/42], [94mLoss[0m : 2.52574
[1mStep[0m  [12/42], [94mLoss[0m : 2.49026
[1mStep[0m  [16/42], [94mLoss[0m : 2.62515
[1mStep[0m  [20/42], [94mLoss[0m : 2.20415
[1mStep[0m  [24/42], [94mLoss[0m : 2.34227
[1mStep[0m  [28/42], [94mLoss[0m : 2.36724
[1mStep[0m  [32/42], [94mLoss[0m : 2.48216
[1mStep[0m  [36/42], [94mLoss[0m : 2.38947
[1mStep[0m  [40/42], [94mLoss[0m : 2.34591

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.321, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54540
[1mStep[0m  [4/42], [94mLoss[0m : 2.37429
[1mStep[0m  [8/42], [94mLoss[0m : 2.34418
[1mStep[0m  [12/42], [94mLoss[0m : 2.58232
[1mStep[0m  [16/42], [94mLoss[0m : 2.41906
[1mStep[0m  [20/42], [94mLoss[0m : 2.52355
[1mStep[0m  [24/42], [94mLoss[0m : 2.50091
[1mStep[0m  [28/42], [94mLoss[0m : 2.35384
[1mStep[0m  [32/42], [94mLoss[0m : 2.44274
[1mStep[0m  [36/42], [94mLoss[0m : 2.34424
[1mStep[0m  [40/42], [94mLoss[0m : 2.44874

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.327, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38746
[1mStep[0m  [4/42], [94mLoss[0m : 2.37444
[1mStep[0m  [8/42], [94mLoss[0m : 2.38896
[1mStep[0m  [12/42], [94mLoss[0m : 2.49918
[1mStep[0m  [16/42], [94mLoss[0m : 2.59156
[1mStep[0m  [20/42], [94mLoss[0m : 2.50232
[1mStep[0m  [24/42], [94mLoss[0m : 2.46180
[1mStep[0m  [28/42], [94mLoss[0m : 2.39645
[1mStep[0m  [32/42], [94mLoss[0m : 2.51046
[1mStep[0m  [36/42], [94mLoss[0m : 2.44998
[1mStep[0m  [40/42], [94mLoss[0m : 2.23393

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.324, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42887
[1mStep[0m  [4/42], [94mLoss[0m : 2.30866
[1mStep[0m  [8/42], [94mLoss[0m : 2.41497
[1mStep[0m  [12/42], [94mLoss[0m : 2.51633
[1mStep[0m  [16/42], [94mLoss[0m : 2.39713
[1mStep[0m  [20/42], [94mLoss[0m : 2.24999
[1mStep[0m  [24/42], [94mLoss[0m : 2.52560
[1mStep[0m  [28/42], [94mLoss[0m : 2.46243
[1mStep[0m  [32/42], [94mLoss[0m : 2.61594
[1mStep[0m  [36/42], [94mLoss[0m : 2.53226
[1mStep[0m  [40/42], [94mLoss[0m : 2.40867

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.324, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48851
[1mStep[0m  [4/42], [94mLoss[0m : 2.32437
[1mStep[0m  [8/42], [94mLoss[0m : 2.35123
[1mStep[0m  [12/42], [94mLoss[0m : 2.37612
[1mStep[0m  [16/42], [94mLoss[0m : 2.50884
[1mStep[0m  [20/42], [94mLoss[0m : 2.36471
[1mStep[0m  [24/42], [94mLoss[0m : 2.30880
[1mStep[0m  [28/42], [94mLoss[0m : 2.31206
[1mStep[0m  [32/42], [94mLoss[0m : 2.44711
[1mStep[0m  [36/42], [94mLoss[0m : 2.73976
[1mStep[0m  [40/42], [94mLoss[0m : 2.29617

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.418, [92mTest[0m: 2.328, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.12022
[1mStep[0m  [4/42], [94mLoss[0m : 2.17531
[1mStep[0m  [8/42], [94mLoss[0m : 2.51197
[1mStep[0m  [12/42], [94mLoss[0m : 2.31153
[1mStep[0m  [16/42], [94mLoss[0m : 2.64531
[1mStep[0m  [20/42], [94mLoss[0m : 2.35770
[1mStep[0m  [24/42], [94mLoss[0m : 2.51668
[1mStep[0m  [28/42], [94mLoss[0m : 2.33417
[1mStep[0m  [32/42], [94mLoss[0m : 2.65935
[1mStep[0m  [36/42], [94mLoss[0m : 2.28925
[1mStep[0m  [40/42], [94mLoss[0m : 2.44019

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.327, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59980
[1mStep[0m  [4/42], [94mLoss[0m : 2.29560
[1mStep[0m  [8/42], [94mLoss[0m : 2.31405
[1mStep[0m  [12/42], [94mLoss[0m : 2.44579
[1mStep[0m  [16/42], [94mLoss[0m : 2.46033
[1mStep[0m  [20/42], [94mLoss[0m : 2.57389
[1mStep[0m  [24/42], [94mLoss[0m : 2.38739
[1mStep[0m  [28/42], [94mLoss[0m : 2.43063
[1mStep[0m  [32/42], [94mLoss[0m : 2.49125
[1mStep[0m  [36/42], [94mLoss[0m : 2.40715
[1mStep[0m  [40/42], [94mLoss[0m : 2.32073

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.321, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.65980
[1mStep[0m  [4/42], [94mLoss[0m : 2.61787
[1mStep[0m  [8/42], [94mLoss[0m : 2.42043
[1mStep[0m  [12/42], [94mLoss[0m : 2.53233
[1mStep[0m  [16/42], [94mLoss[0m : 2.55744
[1mStep[0m  [20/42], [94mLoss[0m : 2.47854
[1mStep[0m  [24/42], [94mLoss[0m : 2.38043
[1mStep[0m  [28/42], [94mLoss[0m : 2.39733
[1mStep[0m  [32/42], [94mLoss[0m : 2.53168
[1mStep[0m  [36/42], [94mLoss[0m : 2.12025
[1mStep[0m  [40/42], [94mLoss[0m : 2.33119

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.331, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27498
[1mStep[0m  [4/42], [94mLoss[0m : 2.54154
[1mStep[0m  [8/42], [94mLoss[0m : 2.27613
[1mStep[0m  [12/42], [94mLoss[0m : 2.51358
[1mStep[0m  [16/42], [94mLoss[0m : 2.28431
[1mStep[0m  [20/42], [94mLoss[0m : 2.39941
[1mStep[0m  [24/42], [94mLoss[0m : 2.27152
[1mStep[0m  [28/42], [94mLoss[0m : 2.53839
[1mStep[0m  [32/42], [94mLoss[0m : 2.32453
[1mStep[0m  [36/42], [94mLoss[0m : 2.46115
[1mStep[0m  [40/42], [94mLoss[0m : 2.44361

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.331, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33943
[1mStep[0m  [4/42], [94mLoss[0m : 2.44373
[1mStep[0m  [8/42], [94mLoss[0m : 2.55609
[1mStep[0m  [12/42], [94mLoss[0m : 2.19134
[1mStep[0m  [16/42], [94mLoss[0m : 2.34553
[1mStep[0m  [20/42], [94mLoss[0m : 2.48323
[1mStep[0m  [24/42], [94mLoss[0m : 2.34459
[1mStep[0m  [28/42], [94mLoss[0m : 2.24938
[1mStep[0m  [32/42], [94mLoss[0m : 2.33127
[1mStep[0m  [36/42], [94mLoss[0m : 2.64788
[1mStep[0m  [40/42], [94mLoss[0m : 2.30692

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.325, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56751
[1mStep[0m  [4/42], [94mLoss[0m : 2.44618
[1mStep[0m  [8/42], [94mLoss[0m : 2.35966
[1mStep[0m  [12/42], [94mLoss[0m : 2.38550
[1mStep[0m  [16/42], [94mLoss[0m : 2.24873
[1mStep[0m  [20/42], [94mLoss[0m : 2.37588
[1mStep[0m  [24/42], [94mLoss[0m : 2.42517
[1mStep[0m  [28/42], [94mLoss[0m : 2.36451
[1mStep[0m  [32/42], [94mLoss[0m : 2.55098
[1mStep[0m  [36/42], [94mLoss[0m : 2.28248
[1mStep[0m  [40/42], [94mLoss[0m : 2.57970

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.321, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35028
[1mStep[0m  [4/42], [94mLoss[0m : 2.47749
[1mStep[0m  [8/42], [94mLoss[0m : 2.53612
[1mStep[0m  [12/42], [94mLoss[0m : 2.34041
[1mStep[0m  [16/42], [94mLoss[0m : 2.36763
[1mStep[0m  [20/42], [94mLoss[0m : 2.31627
[1mStep[0m  [24/42], [94mLoss[0m : 2.16562
[1mStep[0m  [28/42], [94mLoss[0m : 2.45418
[1mStep[0m  [32/42], [94mLoss[0m : 2.50090
[1mStep[0m  [36/42], [94mLoss[0m : 2.42939
[1mStep[0m  [40/42], [94mLoss[0m : 2.35733

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.324, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.18493
[1mStep[0m  [4/42], [94mLoss[0m : 2.35572
[1mStep[0m  [8/42], [94mLoss[0m : 2.25965
[1mStep[0m  [12/42], [94mLoss[0m : 2.39464
[1mStep[0m  [16/42], [94mLoss[0m : 2.52157
[1mStep[0m  [20/42], [94mLoss[0m : 2.30672
[1mStep[0m  [24/42], [94mLoss[0m : 2.27275
[1mStep[0m  [28/42], [94mLoss[0m : 2.53042
[1mStep[0m  [32/42], [94mLoss[0m : 2.57117
[1mStep[0m  [36/42], [94mLoss[0m : 2.46843
[1mStep[0m  [40/42], [94mLoss[0m : 2.34214

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.330, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.324
====================================

Phase 1 - Evaluation MAE:  2.3240236214229038
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 2.30368
[1mStep[0m  [4/42], [94mLoss[0m : 2.36563
[1mStep[0m  [8/42], [94mLoss[0m : 2.57336
[1mStep[0m  [12/42], [94mLoss[0m : 2.54489
[1mStep[0m  [16/42], [94mLoss[0m : 2.45797
[1mStep[0m  [20/42], [94mLoss[0m : 2.32543
[1mStep[0m  [24/42], [94mLoss[0m : 2.52019
[1mStep[0m  [28/42], [94mLoss[0m : 2.64735
[1mStep[0m  [32/42], [94mLoss[0m : 2.36304
[1mStep[0m  [36/42], [94mLoss[0m : 2.47838
[1mStep[0m  [40/42], [94mLoss[0m : 2.53131

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.322, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34299
[1mStep[0m  [4/42], [94mLoss[0m : 2.35983
[1mStep[0m  [8/42], [94mLoss[0m : 2.34662
[1mStep[0m  [12/42], [94mLoss[0m : 2.33938
[1mStep[0m  [16/42], [94mLoss[0m : 2.31574
[1mStep[0m  [20/42], [94mLoss[0m : 2.35182
[1mStep[0m  [24/42], [94mLoss[0m : 2.45526
[1mStep[0m  [28/42], [94mLoss[0m : 2.24242
[1mStep[0m  [32/42], [94mLoss[0m : 2.22852
[1mStep[0m  [36/42], [94mLoss[0m : 2.24756
[1mStep[0m  [40/42], [94mLoss[0m : 2.56823

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.388, [92mTest[0m: 2.348, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33456
[1mStep[0m  [4/42], [94mLoss[0m : 2.41293
[1mStep[0m  [8/42], [94mLoss[0m : 2.32729
[1mStep[0m  [12/42], [94mLoss[0m : 2.36140
[1mStep[0m  [16/42], [94mLoss[0m : 2.34740
[1mStep[0m  [20/42], [94mLoss[0m : 2.24475
[1mStep[0m  [24/42], [94mLoss[0m : 2.40711
[1mStep[0m  [28/42], [94mLoss[0m : 2.43766
[1mStep[0m  [32/42], [94mLoss[0m : 2.37337
[1mStep[0m  [36/42], [94mLoss[0m : 2.42397
[1mStep[0m  [40/42], [94mLoss[0m : 2.32647

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.335, [92mTest[0m: 2.323, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32714
[1mStep[0m  [4/42], [94mLoss[0m : 2.17188
[1mStep[0m  [8/42], [94mLoss[0m : 2.46287
[1mStep[0m  [12/42], [94mLoss[0m : 2.03270
[1mStep[0m  [16/42], [94mLoss[0m : 2.45024
[1mStep[0m  [20/42], [94mLoss[0m : 2.18593
[1mStep[0m  [24/42], [94mLoss[0m : 2.19623
[1mStep[0m  [28/42], [94mLoss[0m : 2.24511
[1mStep[0m  [32/42], [94mLoss[0m : 2.52959
[1mStep[0m  [36/42], [94mLoss[0m : 2.61188
[1mStep[0m  [40/42], [94mLoss[0m : 2.45394

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.279, [92mTest[0m: 2.322, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.18615
[1mStep[0m  [4/42], [94mLoss[0m : 2.19145
[1mStep[0m  [8/42], [94mLoss[0m : 2.31939
[1mStep[0m  [12/42], [94mLoss[0m : 2.21873
[1mStep[0m  [16/42], [94mLoss[0m : 2.19800
[1mStep[0m  [20/42], [94mLoss[0m : 2.05650
[1mStep[0m  [24/42], [94mLoss[0m : 2.22319
[1mStep[0m  [28/42], [94mLoss[0m : 2.27119
[1mStep[0m  [32/42], [94mLoss[0m : 1.98349
[1mStep[0m  [36/42], [94mLoss[0m : 2.17084
[1mStep[0m  [40/42], [94mLoss[0m : 2.25533

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.213, [92mTest[0m: 2.325, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.15638
[1mStep[0m  [4/42], [94mLoss[0m : 2.02556
[1mStep[0m  [8/42], [94mLoss[0m : 2.12614
[1mStep[0m  [12/42], [94mLoss[0m : 2.03780
[1mStep[0m  [16/42], [94mLoss[0m : 2.18458
[1mStep[0m  [20/42], [94mLoss[0m : 2.23658
[1mStep[0m  [24/42], [94mLoss[0m : 2.14045
[1mStep[0m  [28/42], [94mLoss[0m : 2.06895
[1mStep[0m  [32/42], [94mLoss[0m : 2.08331
[1mStep[0m  [36/42], [94mLoss[0m : 2.19492
[1mStep[0m  [40/42], [94mLoss[0m : 2.16181

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.146, [92mTest[0m: 2.344, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.88488
[1mStep[0m  [4/42], [94mLoss[0m : 2.09166
[1mStep[0m  [8/42], [94mLoss[0m : 2.00558
[1mStep[0m  [12/42], [94mLoss[0m : 2.17509
[1mStep[0m  [16/42], [94mLoss[0m : 2.10317
[1mStep[0m  [20/42], [94mLoss[0m : 2.18918
[1mStep[0m  [24/42], [94mLoss[0m : 1.96726
[1mStep[0m  [28/42], [94mLoss[0m : 2.13302
[1mStep[0m  [32/42], [94mLoss[0m : 2.23556
[1mStep[0m  [36/42], [94mLoss[0m : 2.23653
[1mStep[0m  [40/42], [94mLoss[0m : 2.25611

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.082, [92mTest[0m: 2.339, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.94761
[1mStep[0m  [4/42], [94mLoss[0m : 2.03479
[1mStep[0m  [8/42], [94mLoss[0m : 2.23884
[1mStep[0m  [12/42], [94mLoss[0m : 1.99254
[1mStep[0m  [16/42], [94mLoss[0m : 2.07525
[1mStep[0m  [20/42], [94mLoss[0m : 1.88591
[1mStep[0m  [24/42], [94mLoss[0m : 2.02207
[1mStep[0m  [28/42], [94mLoss[0m : 1.96750
[1mStep[0m  [32/42], [94mLoss[0m : 2.01071
[1mStep[0m  [36/42], [94mLoss[0m : 2.01234
[1mStep[0m  [40/42], [94mLoss[0m : 2.04545

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.013, [92mTest[0m: 2.413, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.82707
[1mStep[0m  [4/42], [94mLoss[0m : 1.95000
[1mStep[0m  [8/42], [94mLoss[0m : 1.81582
[1mStep[0m  [12/42], [94mLoss[0m : 2.02105
[1mStep[0m  [16/42], [94mLoss[0m : 1.98897
[1mStep[0m  [20/42], [94mLoss[0m : 1.96088
[1mStep[0m  [24/42], [94mLoss[0m : 1.93598
[1mStep[0m  [28/42], [94mLoss[0m : 1.98544
[1mStep[0m  [32/42], [94mLoss[0m : 2.24128
[1mStep[0m  [36/42], [94mLoss[0m : 1.89217
[1mStep[0m  [40/42], [94mLoss[0m : 1.89866

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.965, [92mTest[0m: 2.382, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.99483
[1mStep[0m  [4/42], [94mLoss[0m : 1.95283
[1mStep[0m  [8/42], [94mLoss[0m : 1.74686
[1mStep[0m  [12/42], [94mLoss[0m : 1.77764
[1mStep[0m  [16/42], [94mLoss[0m : 1.91992
[1mStep[0m  [20/42], [94mLoss[0m : 2.05299
[1mStep[0m  [24/42], [94mLoss[0m : 1.95238
[1mStep[0m  [28/42], [94mLoss[0m : 1.91053
[1mStep[0m  [32/42], [94mLoss[0m : 1.96020
[1mStep[0m  [36/42], [94mLoss[0m : 1.92667
[1mStep[0m  [40/42], [94mLoss[0m : 2.05534

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.912, [92mTest[0m: 2.407, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.91927
[1mStep[0m  [4/42], [94mLoss[0m : 1.86681
[1mStep[0m  [8/42], [94mLoss[0m : 1.92891
[1mStep[0m  [12/42], [94mLoss[0m : 1.79066
[1mStep[0m  [16/42], [94mLoss[0m : 1.74693
[1mStep[0m  [20/42], [94mLoss[0m : 1.88658
[1mStep[0m  [24/42], [94mLoss[0m : 1.85627
[1mStep[0m  [28/42], [94mLoss[0m : 1.81128
[1mStep[0m  [32/42], [94mLoss[0m : 1.97714
[1mStep[0m  [36/42], [94mLoss[0m : 1.93358
[1mStep[0m  [40/42], [94mLoss[0m : 1.94172

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.867, [92mTest[0m: 2.449, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.82624
[1mStep[0m  [4/42], [94mLoss[0m : 2.02513
[1mStep[0m  [8/42], [94mLoss[0m : 1.78559
[1mStep[0m  [12/42], [94mLoss[0m : 1.54340
[1mStep[0m  [16/42], [94mLoss[0m : 1.91702
[1mStep[0m  [20/42], [94mLoss[0m : 1.80665
[1mStep[0m  [24/42], [94mLoss[0m : 1.83875
[1mStep[0m  [28/42], [94mLoss[0m : 1.80775
[1mStep[0m  [32/42], [94mLoss[0m : 1.84055
[1mStep[0m  [36/42], [94mLoss[0m : 1.96204
[1mStep[0m  [40/42], [94mLoss[0m : 1.79133

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.812, [92mTest[0m: 2.469, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.73340
[1mStep[0m  [4/42], [94mLoss[0m : 1.72555
[1mStep[0m  [8/42], [94mLoss[0m : 1.62646
[1mStep[0m  [12/42], [94mLoss[0m : 1.79135
[1mStep[0m  [16/42], [94mLoss[0m : 1.77348
[1mStep[0m  [20/42], [94mLoss[0m : 1.56833
[1mStep[0m  [24/42], [94mLoss[0m : 1.87184
[1mStep[0m  [28/42], [94mLoss[0m : 1.82670
[1mStep[0m  [32/42], [94mLoss[0m : 1.65656
[1mStep[0m  [36/42], [94mLoss[0m : 1.67754
[1mStep[0m  [40/42], [94mLoss[0m : 1.71043

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.778, [92mTest[0m: 2.464, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.68996
[1mStep[0m  [4/42], [94mLoss[0m : 1.62888
[1mStep[0m  [8/42], [94mLoss[0m : 1.73825
[1mStep[0m  [12/42], [94mLoss[0m : 1.75966
[1mStep[0m  [16/42], [94mLoss[0m : 1.66084
[1mStep[0m  [20/42], [94mLoss[0m : 1.50088
[1mStep[0m  [24/42], [94mLoss[0m : 1.66879
[1mStep[0m  [28/42], [94mLoss[0m : 1.76521
[1mStep[0m  [32/42], [94mLoss[0m : 1.86258
[1mStep[0m  [36/42], [94mLoss[0m : 1.57111
[1mStep[0m  [40/42], [94mLoss[0m : 1.75165

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.726, [92mTest[0m: 2.429, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.57862
[1mStep[0m  [4/42], [94mLoss[0m : 1.61221
[1mStep[0m  [8/42], [94mLoss[0m : 1.63678
[1mStep[0m  [12/42], [94mLoss[0m : 1.60066
[1mStep[0m  [16/42], [94mLoss[0m : 1.75065
[1mStep[0m  [20/42], [94mLoss[0m : 1.70747
[1mStep[0m  [24/42], [94mLoss[0m : 1.58400
[1mStep[0m  [28/42], [94mLoss[0m : 1.80292
[1mStep[0m  [32/42], [94mLoss[0m : 1.55440
[1mStep[0m  [36/42], [94mLoss[0m : 1.78401
[1mStep[0m  [40/42], [94mLoss[0m : 1.76403

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.680, [92mTest[0m: 2.464, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.55690
[1mStep[0m  [4/42], [94mLoss[0m : 1.82107
[1mStep[0m  [8/42], [94mLoss[0m : 1.65385
[1mStep[0m  [12/42], [94mLoss[0m : 1.75060
[1mStep[0m  [16/42], [94mLoss[0m : 1.61043
[1mStep[0m  [20/42], [94mLoss[0m : 1.70791
[1mStep[0m  [24/42], [94mLoss[0m : 1.57840
[1mStep[0m  [28/42], [94mLoss[0m : 1.80846
[1mStep[0m  [32/42], [94mLoss[0m : 1.83756
[1mStep[0m  [36/42], [94mLoss[0m : 1.80217
[1mStep[0m  [40/42], [94mLoss[0m : 1.78861

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.668, [92mTest[0m: 2.479, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.69323
[1mStep[0m  [4/42], [94mLoss[0m : 1.65635
[1mStep[0m  [8/42], [94mLoss[0m : 1.48646
[1mStep[0m  [12/42], [94mLoss[0m : 1.58972
[1mStep[0m  [16/42], [94mLoss[0m : 1.56465
[1mStep[0m  [20/42], [94mLoss[0m : 1.65589
[1mStep[0m  [24/42], [94mLoss[0m : 1.65576
[1mStep[0m  [28/42], [94mLoss[0m : 1.76742
[1mStep[0m  [32/42], [94mLoss[0m : 1.53431
[1mStep[0m  [36/42], [94mLoss[0m : 1.73202
[1mStep[0m  [40/42], [94mLoss[0m : 1.80290

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.620, [92mTest[0m: 2.536, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.53580
[1mStep[0m  [4/42], [94mLoss[0m : 1.72698
[1mStep[0m  [8/42], [94mLoss[0m : 1.45620
[1mStep[0m  [12/42], [94mLoss[0m : 1.75187
[1mStep[0m  [16/42], [94mLoss[0m : 1.40153
[1mStep[0m  [20/42], [94mLoss[0m : 1.66628
[1mStep[0m  [24/42], [94mLoss[0m : 1.49645
[1mStep[0m  [28/42], [94mLoss[0m : 1.48265
[1mStep[0m  [32/42], [94mLoss[0m : 1.52473
[1mStep[0m  [36/42], [94mLoss[0m : 1.73490
[1mStep[0m  [40/42], [94mLoss[0m : 1.51727

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.598, [92mTest[0m: 2.481, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.57424
[1mStep[0m  [4/42], [94mLoss[0m : 1.39043
[1mStep[0m  [8/42], [94mLoss[0m : 1.46529
[1mStep[0m  [12/42], [94mLoss[0m : 1.51649
[1mStep[0m  [16/42], [94mLoss[0m : 1.64489
[1mStep[0m  [20/42], [94mLoss[0m : 1.60695
[1mStep[0m  [24/42], [94mLoss[0m : 1.53212
[1mStep[0m  [28/42], [94mLoss[0m : 1.57999
[1mStep[0m  [32/42], [94mLoss[0m : 1.50203
[1mStep[0m  [36/42], [94mLoss[0m : 1.44717
[1mStep[0m  [40/42], [94mLoss[0m : 1.63243

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.582, [92mTest[0m: 2.462, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.45187
[1mStep[0m  [4/42], [94mLoss[0m : 1.42302
[1mStep[0m  [8/42], [94mLoss[0m : 1.67927
[1mStep[0m  [12/42], [94mLoss[0m : 1.46823
[1mStep[0m  [16/42], [94mLoss[0m : 1.54578
[1mStep[0m  [20/42], [94mLoss[0m : 1.54345
[1mStep[0m  [24/42], [94mLoss[0m : 1.47283
[1mStep[0m  [28/42], [94mLoss[0m : 1.52111
[1mStep[0m  [32/42], [94mLoss[0m : 1.64161
[1mStep[0m  [36/42], [94mLoss[0m : 1.61115
[1mStep[0m  [40/42], [94mLoss[0m : 1.50970

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.548, [92mTest[0m: 2.647, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.68647
[1mStep[0m  [4/42], [94mLoss[0m : 1.54567
[1mStep[0m  [8/42], [94mLoss[0m : 1.59096
[1mStep[0m  [12/42], [94mLoss[0m : 1.42792
[1mStep[0m  [16/42], [94mLoss[0m : 1.48176
[1mStep[0m  [20/42], [94mLoss[0m : 1.50834
[1mStep[0m  [24/42], [94mLoss[0m : 1.64576
[1mStep[0m  [28/42], [94mLoss[0m : 1.58688
[1mStep[0m  [32/42], [94mLoss[0m : 1.50474
[1mStep[0m  [36/42], [94mLoss[0m : 1.68205
[1mStep[0m  [40/42], [94mLoss[0m : 1.52865

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.531, [92mTest[0m: 2.481, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.43617
[1mStep[0m  [4/42], [94mLoss[0m : 1.42550
[1mStep[0m  [8/42], [94mLoss[0m : 1.42169
[1mStep[0m  [12/42], [94mLoss[0m : 1.58333
[1mStep[0m  [16/42], [94mLoss[0m : 1.49218
[1mStep[0m  [20/42], [94mLoss[0m : 1.59875
[1mStep[0m  [24/42], [94mLoss[0m : 1.59930
[1mStep[0m  [28/42], [94mLoss[0m : 1.56386
[1mStep[0m  [32/42], [94mLoss[0m : 1.48514
[1mStep[0m  [36/42], [94mLoss[0m : 1.43781
[1mStep[0m  [40/42], [94mLoss[0m : 1.48150

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.491, [92mTest[0m: 2.480, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.55568
[1mStep[0m  [4/42], [94mLoss[0m : 1.39790
[1mStep[0m  [8/42], [94mLoss[0m : 1.43717
[1mStep[0m  [12/42], [94mLoss[0m : 1.55480
[1mStep[0m  [16/42], [94mLoss[0m : 1.42624
[1mStep[0m  [20/42], [94mLoss[0m : 1.52513
[1mStep[0m  [24/42], [94mLoss[0m : 1.57488
[1mStep[0m  [28/42], [94mLoss[0m : 1.56228
[1mStep[0m  [32/42], [94mLoss[0m : 1.38272
[1mStep[0m  [36/42], [94mLoss[0m : 1.50347
[1mStep[0m  [40/42], [94mLoss[0m : 1.46039

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.461, [92mTest[0m: 2.467, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.37113
[1mStep[0m  [4/42], [94mLoss[0m : 1.54682
[1mStep[0m  [8/42], [94mLoss[0m : 1.37570
[1mStep[0m  [12/42], [94mLoss[0m : 1.41552
[1mStep[0m  [16/42], [94mLoss[0m : 1.52467
[1mStep[0m  [20/42], [94mLoss[0m : 1.36613
[1mStep[0m  [24/42], [94mLoss[0m : 1.51478
[1mStep[0m  [28/42], [94mLoss[0m : 1.44116
[1mStep[0m  [32/42], [94mLoss[0m : 1.55581
[1mStep[0m  [36/42], [94mLoss[0m : 1.61431
[1mStep[0m  [40/42], [94mLoss[0m : 1.38898

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.455, [92mTest[0m: 2.446, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.52742
[1mStep[0m  [4/42], [94mLoss[0m : 1.49622
[1mStep[0m  [8/42], [94mLoss[0m : 1.41536
[1mStep[0m  [12/42], [94mLoss[0m : 1.46345
[1mStep[0m  [16/42], [94mLoss[0m : 1.41256
[1mStep[0m  [20/42], [94mLoss[0m : 1.49901
[1mStep[0m  [24/42], [94mLoss[0m : 1.61075
[1mStep[0m  [28/42], [94mLoss[0m : 1.41023
[1mStep[0m  [32/42], [94mLoss[0m : 1.46820
[1mStep[0m  [36/42], [94mLoss[0m : 1.51584
[1mStep[0m  [40/42], [94mLoss[0m : 1.41263

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.431, [92mTest[0m: 2.446, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.29520
[1mStep[0m  [4/42], [94mLoss[0m : 1.47113
[1mStep[0m  [8/42], [94mLoss[0m : 1.34901
[1mStep[0m  [12/42], [94mLoss[0m : 1.51870
[1mStep[0m  [16/42], [94mLoss[0m : 1.40524
[1mStep[0m  [20/42], [94mLoss[0m : 1.42157
[1mStep[0m  [24/42], [94mLoss[0m : 1.43524
[1mStep[0m  [28/42], [94mLoss[0m : 1.36716
[1mStep[0m  [32/42], [94mLoss[0m : 1.49466
[1mStep[0m  [36/42], [94mLoss[0m : 1.34454
[1mStep[0m  [40/42], [94mLoss[0m : 1.50304

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.414, [92mTest[0m: 2.469, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.36912
[1mStep[0m  [4/42], [94mLoss[0m : 1.32717
[1mStep[0m  [8/42], [94mLoss[0m : 1.34915
[1mStep[0m  [12/42], [94mLoss[0m : 1.28144
[1mStep[0m  [16/42], [94mLoss[0m : 1.32423
[1mStep[0m  [20/42], [94mLoss[0m : 1.50473
[1mStep[0m  [24/42], [94mLoss[0m : 1.40640
[1mStep[0m  [28/42], [94mLoss[0m : 1.25299
[1mStep[0m  [32/42], [94mLoss[0m : 1.27115
[1mStep[0m  [36/42], [94mLoss[0m : 1.58532
[1mStep[0m  [40/42], [94mLoss[0m : 1.38018

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.374, [92mTest[0m: 2.449, [96mlr[0m: 0.0023175
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 26 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.540
====================================

Phase 2 - Evaluation MAE:  2.540173513548715
MAE score P1      2.324024
MAE score P2      2.540174
loss              1.374155
learning_rate     0.002575
batch_size             256
hidden_sizes         [100]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping        True
dropout                0.3
momentum               0.9
weight_decay          0.01
Name: 18, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 10.82360
[1mStep[0m  [4/42], [94mLoss[0m : 10.83950
[1mStep[0m  [8/42], [94mLoss[0m : 10.46591
[1mStep[0m  [12/42], [94mLoss[0m : 10.23476
[1mStep[0m  [16/42], [94mLoss[0m : 10.25047
[1mStep[0m  [20/42], [94mLoss[0m : 10.10544
[1mStep[0m  [24/42], [94mLoss[0m : 9.42803
[1mStep[0m  [28/42], [94mLoss[0m : 9.71343
[1mStep[0m  [32/42], [94mLoss[0m : 9.65096
[1mStep[0m  [36/42], [94mLoss[0m : 9.38965
[1mStep[0m  [40/42], [94mLoss[0m : 9.45564

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.035, [92mTest[0m: 10.832, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.12468
[1mStep[0m  [4/42], [94mLoss[0m : 9.20979
[1mStep[0m  [8/42], [94mLoss[0m : 9.05430
[1mStep[0m  [12/42], [94mLoss[0m : 8.95531
[1mStep[0m  [16/42], [94mLoss[0m : 8.68693
[1mStep[0m  [20/42], [94mLoss[0m : 8.83383
[1mStep[0m  [24/42], [94mLoss[0m : 8.16695
[1mStep[0m  [28/42], [94mLoss[0m : 7.82362
[1mStep[0m  [32/42], [94mLoss[0m : 8.03644
[1mStep[0m  [36/42], [94mLoss[0m : 7.96933
[1mStep[0m  [40/42], [94mLoss[0m : 7.67541

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 8.409, [92mTest[0m: 9.216, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.42037
[1mStep[0m  [4/42], [94mLoss[0m : 7.43362
[1mStep[0m  [8/42], [94mLoss[0m : 7.05679
[1mStep[0m  [12/42], [94mLoss[0m : 6.91049
[1mStep[0m  [16/42], [94mLoss[0m : 7.12589
[1mStep[0m  [20/42], [94mLoss[0m : 6.88354
[1mStep[0m  [24/42], [94mLoss[0m : 6.46637
[1mStep[0m  [28/42], [94mLoss[0m : 6.62972
[1mStep[0m  [32/42], [94mLoss[0m : 6.36648
[1mStep[0m  [36/42], [94mLoss[0m : 5.95412
[1mStep[0m  [40/42], [94mLoss[0m : 6.24628

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 6.788, [92mTest[0m: 7.565, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.18941
[1mStep[0m  [4/42], [94mLoss[0m : 5.55564
[1mStep[0m  [8/42], [94mLoss[0m : 5.91950
[1mStep[0m  [12/42], [94mLoss[0m : 5.68906
[1mStep[0m  [16/42], [94mLoss[0m : 5.49977
[1mStep[0m  [20/42], [94mLoss[0m : 5.15517
[1mStep[0m  [24/42], [94mLoss[0m : 4.81670
[1mStep[0m  [28/42], [94mLoss[0m : 4.78458
[1mStep[0m  [32/42], [94mLoss[0m : 4.72373
[1mStep[0m  [36/42], [94mLoss[0m : 4.99281
[1mStep[0m  [40/42], [94mLoss[0m : 4.57896

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 5.242, [92mTest[0m: 5.937, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.23321
[1mStep[0m  [4/42], [94mLoss[0m : 4.68234
[1mStep[0m  [8/42], [94mLoss[0m : 4.14224
[1mStep[0m  [12/42], [94mLoss[0m : 3.95783
[1mStep[0m  [16/42], [94mLoss[0m : 4.29412
[1mStep[0m  [20/42], [94mLoss[0m : 4.12474
[1mStep[0m  [24/42], [94mLoss[0m : 4.03528
[1mStep[0m  [28/42], [94mLoss[0m : 3.95828
[1mStep[0m  [32/42], [94mLoss[0m : 3.89031
[1mStep[0m  [36/42], [94mLoss[0m : 3.85643
[1mStep[0m  [40/42], [94mLoss[0m : 4.09954

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 4.109, [92mTest[0m: 4.544, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.36966
[1mStep[0m  [4/42], [94mLoss[0m : 3.86256
[1mStep[0m  [8/42], [94mLoss[0m : 3.51590
[1mStep[0m  [12/42], [94mLoss[0m : 3.53550
[1mStep[0m  [16/42], [94mLoss[0m : 3.24658
[1mStep[0m  [20/42], [94mLoss[0m : 3.72340
[1mStep[0m  [24/42], [94mLoss[0m : 3.50561
[1mStep[0m  [28/42], [94mLoss[0m : 3.24853
[1mStep[0m  [32/42], [94mLoss[0m : 3.58000
[1mStep[0m  [36/42], [94mLoss[0m : 3.58143
[1mStep[0m  [40/42], [94mLoss[0m : 3.38599

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 3.463, [92mTest[0m: 3.635, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.13901
[1mStep[0m  [4/42], [94mLoss[0m : 2.91557
[1mStep[0m  [8/42], [94mLoss[0m : 3.29803
[1mStep[0m  [12/42], [94mLoss[0m : 3.32479
[1mStep[0m  [16/42], [94mLoss[0m : 3.14716
[1mStep[0m  [20/42], [94mLoss[0m : 2.72121
[1mStep[0m  [24/42], [94mLoss[0m : 3.08479
[1mStep[0m  [28/42], [94mLoss[0m : 3.04486
[1mStep[0m  [32/42], [94mLoss[0m : 3.28742
[1mStep[0m  [36/42], [94mLoss[0m : 3.26497
[1mStep[0m  [40/42], [94mLoss[0m : 2.96645

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 3.075, [92mTest[0m: 3.093, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.15727
[1mStep[0m  [4/42], [94mLoss[0m : 2.77774
[1mStep[0m  [8/42], [94mLoss[0m : 2.94901
[1mStep[0m  [12/42], [94mLoss[0m : 2.83805
[1mStep[0m  [16/42], [94mLoss[0m : 2.91235
[1mStep[0m  [20/42], [94mLoss[0m : 2.87250
[1mStep[0m  [24/42], [94mLoss[0m : 2.89533
[1mStep[0m  [28/42], [94mLoss[0m : 2.83007
[1mStep[0m  [32/42], [94mLoss[0m : 2.89613
[1mStep[0m  [36/42], [94mLoss[0m : 2.59835
[1mStep[0m  [40/42], [94mLoss[0m : 2.73253

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.868, [92mTest[0m: 2.776, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.67126
[1mStep[0m  [4/42], [94mLoss[0m : 2.60218
[1mStep[0m  [8/42], [94mLoss[0m : 2.81468
[1mStep[0m  [12/42], [94mLoss[0m : 2.70489
[1mStep[0m  [16/42], [94mLoss[0m : 2.83728
[1mStep[0m  [20/42], [94mLoss[0m : 2.69180
[1mStep[0m  [24/42], [94mLoss[0m : 2.53872
[1mStep[0m  [28/42], [94mLoss[0m : 2.79077
[1mStep[0m  [32/42], [94mLoss[0m : 2.62497
[1mStep[0m  [36/42], [94mLoss[0m : 2.88269
[1mStep[0m  [40/42], [94mLoss[0m : 2.61750

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.760, [92mTest[0m: 2.598, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.84770
[1mStep[0m  [4/42], [94mLoss[0m : 2.79992
[1mStep[0m  [8/42], [94mLoss[0m : 2.88723
[1mStep[0m  [12/42], [94mLoss[0m : 2.70252
[1mStep[0m  [16/42], [94mLoss[0m : 2.93989
[1mStep[0m  [20/42], [94mLoss[0m : 2.46724
[1mStep[0m  [24/42], [94mLoss[0m : 2.84340
[1mStep[0m  [28/42], [94mLoss[0m : 2.52202
[1mStep[0m  [32/42], [94mLoss[0m : 2.82816
[1mStep[0m  [36/42], [94mLoss[0m : 2.49277
[1mStep[0m  [40/42], [94mLoss[0m : 2.65233

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.698, [92mTest[0m: 2.506, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.75574
[1mStep[0m  [4/42], [94mLoss[0m : 2.60157
[1mStep[0m  [8/42], [94mLoss[0m : 2.66133
[1mStep[0m  [12/42], [94mLoss[0m : 2.65478
[1mStep[0m  [16/42], [94mLoss[0m : 2.52399
[1mStep[0m  [20/42], [94mLoss[0m : 2.60098
[1mStep[0m  [24/42], [94mLoss[0m : 2.64070
[1mStep[0m  [28/42], [94mLoss[0m : 2.58038
[1mStep[0m  [32/42], [94mLoss[0m : 2.62412
[1mStep[0m  [36/42], [94mLoss[0m : 2.73512
[1mStep[0m  [40/42], [94mLoss[0m : 2.53413

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.663, [92mTest[0m: 2.469, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37376
[1mStep[0m  [4/42], [94mLoss[0m : 2.53189
[1mStep[0m  [8/42], [94mLoss[0m : 2.51016
[1mStep[0m  [12/42], [94mLoss[0m : 2.79311
[1mStep[0m  [16/42], [94mLoss[0m : 2.74027
[1mStep[0m  [20/42], [94mLoss[0m : 2.81751
[1mStep[0m  [24/42], [94mLoss[0m : 2.43815
[1mStep[0m  [28/42], [94mLoss[0m : 2.58259
[1mStep[0m  [32/42], [94mLoss[0m : 2.67139
[1mStep[0m  [36/42], [94mLoss[0m : 2.78928
[1mStep[0m  [40/42], [94mLoss[0m : 2.83889

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.630, [92mTest[0m: 2.444, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64544
[1mStep[0m  [4/42], [94mLoss[0m : 2.81923
[1mStep[0m  [8/42], [94mLoss[0m : 2.98416
[1mStep[0m  [12/42], [94mLoss[0m : 2.84010
[1mStep[0m  [16/42], [94mLoss[0m : 2.60532
[1mStep[0m  [20/42], [94mLoss[0m : 2.59807
[1mStep[0m  [24/42], [94mLoss[0m : 2.77024
[1mStep[0m  [28/42], [94mLoss[0m : 2.42906
[1mStep[0m  [32/42], [94mLoss[0m : 2.68736
[1mStep[0m  [36/42], [94mLoss[0m : 2.57914
[1mStep[0m  [40/42], [94mLoss[0m : 2.33447

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.640, [92mTest[0m: 2.436, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51709
[1mStep[0m  [4/42], [94mLoss[0m : 2.58803
[1mStep[0m  [8/42], [94mLoss[0m : 2.57328
[1mStep[0m  [12/42], [94mLoss[0m : 2.65441
[1mStep[0m  [16/42], [94mLoss[0m : 2.76421
[1mStep[0m  [20/42], [94mLoss[0m : 2.78047
[1mStep[0m  [24/42], [94mLoss[0m : 2.56331
[1mStep[0m  [28/42], [94mLoss[0m : 2.55093
[1mStep[0m  [32/42], [94mLoss[0m : 2.85186
[1mStep[0m  [36/42], [94mLoss[0m : 2.72596
[1mStep[0m  [40/42], [94mLoss[0m : 2.33353

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.646, [92mTest[0m: 2.424, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.66269
[1mStep[0m  [4/42], [94mLoss[0m : 2.46405
[1mStep[0m  [8/42], [94mLoss[0m : 2.61333
[1mStep[0m  [12/42], [94mLoss[0m : 2.63818
[1mStep[0m  [16/42], [94mLoss[0m : 2.61454
[1mStep[0m  [20/42], [94mLoss[0m : 2.68543
[1mStep[0m  [24/42], [94mLoss[0m : 2.78460
[1mStep[0m  [28/42], [94mLoss[0m : 2.65745
[1mStep[0m  [32/42], [94mLoss[0m : 2.62160
[1mStep[0m  [36/42], [94mLoss[0m : 2.56511
[1mStep[0m  [40/42], [94mLoss[0m : 2.54751

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.622, [92mTest[0m: 2.422, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.68225
[1mStep[0m  [4/42], [94mLoss[0m : 2.72177
[1mStep[0m  [8/42], [94mLoss[0m : 2.53096
[1mStep[0m  [12/42], [94mLoss[0m : 2.45173
[1mStep[0m  [16/42], [94mLoss[0m : 2.48733
[1mStep[0m  [20/42], [94mLoss[0m : 2.48577
[1mStep[0m  [24/42], [94mLoss[0m : 2.56223
[1mStep[0m  [28/42], [94mLoss[0m : 2.67607
[1mStep[0m  [32/42], [94mLoss[0m : 2.70539
[1mStep[0m  [36/42], [94mLoss[0m : 2.44607
[1mStep[0m  [40/42], [94mLoss[0m : 2.85835

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.603, [92mTest[0m: 2.422, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46119
[1mStep[0m  [4/42], [94mLoss[0m : 2.70807
[1mStep[0m  [8/42], [94mLoss[0m : 2.34402
[1mStep[0m  [12/42], [94mLoss[0m : 2.82268
[1mStep[0m  [16/42], [94mLoss[0m : 2.54989
[1mStep[0m  [20/42], [94mLoss[0m : 2.61676
[1mStep[0m  [24/42], [94mLoss[0m : 2.61299
[1mStep[0m  [28/42], [94mLoss[0m : 2.46939
[1mStep[0m  [32/42], [94mLoss[0m : 2.92071
[1mStep[0m  [36/42], [94mLoss[0m : 2.64538
[1mStep[0m  [40/42], [94mLoss[0m : 2.84888

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.600, [92mTest[0m: 2.414, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.94145
[1mStep[0m  [4/42], [94mLoss[0m : 2.72416
[1mStep[0m  [8/42], [94mLoss[0m : 2.48704
[1mStep[0m  [12/42], [94mLoss[0m : 2.46464
[1mStep[0m  [16/42], [94mLoss[0m : 2.48458
[1mStep[0m  [20/42], [94mLoss[0m : 2.49593
[1mStep[0m  [24/42], [94mLoss[0m : 2.48271
[1mStep[0m  [28/42], [94mLoss[0m : 2.69170
[1mStep[0m  [32/42], [94mLoss[0m : 2.50467
[1mStep[0m  [36/42], [94mLoss[0m : 2.67449
[1mStep[0m  [40/42], [94mLoss[0m : 2.60548

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.605, [92mTest[0m: 2.399, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61436
[1mStep[0m  [4/42], [94mLoss[0m : 2.66431
[1mStep[0m  [8/42], [94mLoss[0m : 2.62200
[1mStep[0m  [12/42], [94mLoss[0m : 2.70866
[1mStep[0m  [16/42], [94mLoss[0m : 2.60175
[1mStep[0m  [20/42], [94mLoss[0m : 2.73493
[1mStep[0m  [24/42], [94mLoss[0m : 2.56783
[1mStep[0m  [28/42], [94mLoss[0m : 2.61968
[1mStep[0m  [32/42], [94mLoss[0m : 2.64109
[1mStep[0m  [36/42], [94mLoss[0m : 2.64255
[1mStep[0m  [40/42], [94mLoss[0m : 2.52343

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.614, [92mTest[0m: 2.395, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.74291
[1mStep[0m  [4/42], [94mLoss[0m : 2.61867
[1mStep[0m  [8/42], [94mLoss[0m : 2.58964
[1mStep[0m  [12/42], [94mLoss[0m : 2.73884
[1mStep[0m  [16/42], [94mLoss[0m : 2.78013
[1mStep[0m  [20/42], [94mLoss[0m : 2.56305
[1mStep[0m  [24/42], [94mLoss[0m : 2.31920
[1mStep[0m  [28/42], [94mLoss[0m : 2.59929
[1mStep[0m  [32/42], [94mLoss[0m : 2.73947
[1mStep[0m  [36/42], [94mLoss[0m : 2.58977
[1mStep[0m  [40/42], [94mLoss[0m : 2.42664

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.596, [92mTest[0m: 2.391, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.63245
[1mStep[0m  [4/42], [94mLoss[0m : 2.53029
[1mStep[0m  [8/42], [94mLoss[0m : 2.65047
[1mStep[0m  [12/42], [94mLoss[0m : 2.87142
[1mStep[0m  [16/42], [94mLoss[0m : 2.58859
[1mStep[0m  [20/42], [94mLoss[0m : 2.53418
[1mStep[0m  [24/42], [94mLoss[0m : 2.77481
[1mStep[0m  [28/42], [94mLoss[0m : 2.65130
[1mStep[0m  [32/42], [94mLoss[0m : 2.64352
[1mStep[0m  [36/42], [94mLoss[0m : 2.64828
[1mStep[0m  [40/42], [94mLoss[0m : 2.64803

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.592, [92mTest[0m: 2.393, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52838
[1mStep[0m  [4/42], [94mLoss[0m : 2.59668
[1mStep[0m  [8/42], [94mLoss[0m : 2.42335
[1mStep[0m  [12/42], [94mLoss[0m : 2.88863
[1mStep[0m  [16/42], [94mLoss[0m : 2.69550
[1mStep[0m  [20/42], [94mLoss[0m : 2.48373
[1mStep[0m  [24/42], [94mLoss[0m : 2.59963
[1mStep[0m  [28/42], [94mLoss[0m : 2.56019
[1mStep[0m  [32/42], [94mLoss[0m : 2.61626
[1mStep[0m  [36/42], [94mLoss[0m : 2.92769
[1mStep[0m  [40/42], [94mLoss[0m : 2.53297

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.600, [92mTest[0m: 2.386, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61422
[1mStep[0m  [4/42], [94mLoss[0m : 2.63937
[1mStep[0m  [8/42], [94mLoss[0m : 2.68946
[1mStep[0m  [12/42], [94mLoss[0m : 2.48357
[1mStep[0m  [16/42], [94mLoss[0m : 2.69909
[1mStep[0m  [20/42], [94mLoss[0m : 2.69360
[1mStep[0m  [24/42], [94mLoss[0m : 2.26865
[1mStep[0m  [28/42], [94mLoss[0m : 2.74918
[1mStep[0m  [32/42], [94mLoss[0m : 2.61279
[1mStep[0m  [36/42], [94mLoss[0m : 2.44928
[1mStep[0m  [40/42], [94mLoss[0m : 2.82548

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.588, [92mTest[0m: 2.390, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57381
[1mStep[0m  [4/42], [94mLoss[0m : 2.57366
[1mStep[0m  [8/42], [94mLoss[0m : 2.72025
[1mStep[0m  [12/42], [94mLoss[0m : 2.53092
[1mStep[0m  [16/42], [94mLoss[0m : 2.49333
[1mStep[0m  [20/42], [94mLoss[0m : 2.73883
[1mStep[0m  [24/42], [94mLoss[0m : 2.66053
[1mStep[0m  [28/42], [94mLoss[0m : 2.76370
[1mStep[0m  [32/42], [94mLoss[0m : 2.63592
[1mStep[0m  [36/42], [94mLoss[0m : 2.81566
[1mStep[0m  [40/42], [94mLoss[0m : 2.53165

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.611, [92mTest[0m: 2.384, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37854
[1mStep[0m  [4/42], [94mLoss[0m : 2.44934
[1mStep[0m  [8/42], [94mLoss[0m : 2.61482
[1mStep[0m  [12/42], [94mLoss[0m : 2.50112
[1mStep[0m  [16/42], [94mLoss[0m : 2.73517
[1mStep[0m  [20/42], [94mLoss[0m : 2.57331
[1mStep[0m  [24/42], [94mLoss[0m : 2.78549
[1mStep[0m  [28/42], [94mLoss[0m : 2.56260
[1mStep[0m  [32/42], [94mLoss[0m : 2.36822
[1mStep[0m  [36/42], [94mLoss[0m : 2.63180
[1mStep[0m  [40/42], [94mLoss[0m : 2.73369

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.585, [92mTest[0m: 2.379, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50951
[1mStep[0m  [4/42], [94mLoss[0m : 2.54513
[1mStep[0m  [8/42], [94mLoss[0m : 2.53055
[1mStep[0m  [12/42], [94mLoss[0m : 2.79879
[1mStep[0m  [16/42], [94mLoss[0m : 2.78840
[1mStep[0m  [20/42], [94mLoss[0m : 2.62207
[1mStep[0m  [24/42], [94mLoss[0m : 2.36066
[1mStep[0m  [28/42], [94mLoss[0m : 2.36170
[1mStep[0m  [32/42], [94mLoss[0m : 2.49518
[1mStep[0m  [36/42], [94mLoss[0m : 2.65765
[1mStep[0m  [40/42], [94mLoss[0m : 2.77111

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.611, [92mTest[0m: 2.378, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.73138
[1mStep[0m  [4/42], [94mLoss[0m : 2.60542
[1mStep[0m  [8/42], [94mLoss[0m : 2.73180
[1mStep[0m  [12/42], [94mLoss[0m : 2.67083
[1mStep[0m  [16/42], [94mLoss[0m : 2.24750
[1mStep[0m  [20/42], [94mLoss[0m : 2.66177
[1mStep[0m  [24/42], [94mLoss[0m : 2.54440
[1mStep[0m  [28/42], [94mLoss[0m : 2.60690
[1mStep[0m  [32/42], [94mLoss[0m : 2.88569
[1mStep[0m  [36/42], [94mLoss[0m : 2.33926
[1mStep[0m  [40/42], [94mLoss[0m : 2.31053

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.594, [92mTest[0m: 2.375, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49231
[1mStep[0m  [4/42], [94mLoss[0m : 2.51222
[1mStep[0m  [8/42], [94mLoss[0m : 2.65948
[1mStep[0m  [12/42], [94mLoss[0m : 2.44854
[1mStep[0m  [16/42], [94mLoss[0m : 2.37307
[1mStep[0m  [20/42], [94mLoss[0m : 2.44554
[1mStep[0m  [24/42], [94mLoss[0m : 2.41511
[1mStep[0m  [28/42], [94mLoss[0m : 2.47597
[1mStep[0m  [32/42], [94mLoss[0m : 2.62139
[1mStep[0m  [36/42], [94mLoss[0m : 2.52685
[1mStep[0m  [40/42], [94mLoss[0m : 2.79860

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.562, [92mTest[0m: 2.369, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52100
[1mStep[0m  [4/42], [94mLoss[0m : 2.73483
[1mStep[0m  [8/42], [94mLoss[0m : 2.46689
[1mStep[0m  [12/42], [94mLoss[0m : 2.51863
[1mStep[0m  [16/42], [94mLoss[0m : 2.50767
[1mStep[0m  [20/42], [94mLoss[0m : 2.44707
[1mStep[0m  [24/42], [94mLoss[0m : 2.55846
[1mStep[0m  [28/42], [94mLoss[0m : 2.70958
[1mStep[0m  [32/42], [94mLoss[0m : 2.68559
[1mStep[0m  [36/42], [94mLoss[0m : 2.53683
[1mStep[0m  [40/42], [94mLoss[0m : 2.66836

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.571, [92mTest[0m: 2.369, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.78174
[1mStep[0m  [4/42], [94mLoss[0m : 2.54182
[1mStep[0m  [8/42], [94mLoss[0m : 2.31833
[1mStep[0m  [12/42], [94mLoss[0m : 2.59164
[1mStep[0m  [16/42], [94mLoss[0m : 2.57306
[1mStep[0m  [20/42], [94mLoss[0m : 2.54603
[1mStep[0m  [24/42], [94mLoss[0m : 2.72585
[1mStep[0m  [28/42], [94mLoss[0m : 2.73196
[1mStep[0m  [32/42], [94mLoss[0m : 2.32306
[1mStep[0m  [36/42], [94mLoss[0m : 2.66646
[1mStep[0m  [40/42], [94mLoss[0m : 2.45850

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.585, [92mTest[0m: 2.370, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.360
====================================

Phase 1 - Evaluation MAE:  2.360234652246748
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 2.58808
[1mStep[0m  [4/42], [94mLoss[0m : 2.44587
[1mStep[0m  [8/42], [94mLoss[0m : 2.45152
[1mStep[0m  [12/42], [94mLoss[0m : 2.59673
[1mStep[0m  [16/42], [94mLoss[0m : 2.60856
[1mStep[0m  [20/42], [94mLoss[0m : 2.55763
[1mStep[0m  [24/42], [94mLoss[0m : 2.46251
[1mStep[0m  [28/42], [94mLoss[0m : 2.52286
[1mStep[0m  [32/42], [94mLoss[0m : 2.57078
[1mStep[0m  [36/42], [94mLoss[0m : 2.74207
[1mStep[0m  [40/42], [94mLoss[0m : 2.51802

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.569, [92mTest[0m: 2.361, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61746
[1mStep[0m  [4/42], [94mLoss[0m : 2.59269
[1mStep[0m  [8/42], [94mLoss[0m : 2.67336
[1mStep[0m  [12/42], [94mLoss[0m : 2.47197
[1mStep[0m  [16/42], [94mLoss[0m : 2.65553
[1mStep[0m  [20/42], [94mLoss[0m : 2.48112
[1mStep[0m  [24/42], [94mLoss[0m : 2.56870
[1mStep[0m  [28/42], [94mLoss[0m : 2.58963
[1mStep[0m  [32/42], [94mLoss[0m : 2.61403
[1mStep[0m  [36/42], [94mLoss[0m : 2.54843
[1mStep[0m  [40/42], [94mLoss[0m : 2.69575

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.577, [92mTest[0m: 2.351, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62641
[1mStep[0m  [4/42], [94mLoss[0m : 2.68355
[1mStep[0m  [8/42], [94mLoss[0m : 2.78631
[1mStep[0m  [12/42], [94mLoss[0m : 2.67698
[1mStep[0m  [16/42], [94mLoss[0m : 2.49903
[1mStep[0m  [20/42], [94mLoss[0m : 2.68480
[1mStep[0m  [24/42], [94mLoss[0m : 2.59817
[1mStep[0m  [28/42], [94mLoss[0m : 2.45553
[1mStep[0m  [32/42], [94mLoss[0m : 2.62395
[1mStep[0m  [36/42], [94mLoss[0m : 2.46113
[1mStep[0m  [40/42], [94mLoss[0m : 2.32930

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.571, [92mTest[0m: 2.373, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61508
[1mStep[0m  [4/42], [94mLoss[0m : 2.55445
[1mStep[0m  [8/42], [94mLoss[0m : 2.50521
[1mStep[0m  [12/42], [94mLoss[0m : 2.68656
[1mStep[0m  [16/42], [94mLoss[0m : 2.49428
[1mStep[0m  [20/42], [94mLoss[0m : 2.60859
[1mStep[0m  [24/42], [94mLoss[0m : 2.80316
[1mStep[0m  [28/42], [94mLoss[0m : 2.55256
[1mStep[0m  [32/42], [94mLoss[0m : 2.54805
[1mStep[0m  [36/42], [94mLoss[0m : 2.48603
[1mStep[0m  [40/42], [94mLoss[0m : 2.40191

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.561, [92mTest[0m: 2.387, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.65638
[1mStep[0m  [4/42], [94mLoss[0m : 2.45788
[1mStep[0m  [8/42], [94mLoss[0m : 2.58019
[1mStep[0m  [12/42], [94mLoss[0m : 2.36372
[1mStep[0m  [16/42], [94mLoss[0m : 2.55800
[1mStep[0m  [20/42], [94mLoss[0m : 2.39481
[1mStep[0m  [24/42], [94mLoss[0m : 2.56824
[1mStep[0m  [28/42], [94mLoss[0m : 2.64589
[1mStep[0m  [32/42], [94mLoss[0m : 2.56097
[1mStep[0m  [36/42], [94mLoss[0m : 2.54734
[1mStep[0m  [40/42], [94mLoss[0m : 2.54696

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.528, [92mTest[0m: 2.372, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58520
[1mStep[0m  [4/42], [94mLoss[0m : 2.35073
[1mStep[0m  [8/42], [94mLoss[0m : 2.63366
[1mStep[0m  [12/42], [94mLoss[0m : 2.35219
[1mStep[0m  [16/42], [94mLoss[0m : 2.45492
[1mStep[0m  [20/42], [94mLoss[0m : 2.77815
[1mStep[0m  [24/42], [94mLoss[0m : 2.50466
[1mStep[0m  [28/42], [94mLoss[0m : 2.52334
[1mStep[0m  [32/42], [94mLoss[0m : 2.54711
[1mStep[0m  [36/42], [94mLoss[0m : 2.48096
[1mStep[0m  [40/42], [94mLoss[0m : 2.47527

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.383, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44917
[1mStep[0m  [4/42], [94mLoss[0m : 2.52165
[1mStep[0m  [8/42], [94mLoss[0m : 2.63132
[1mStep[0m  [12/42], [94mLoss[0m : 2.68293
[1mStep[0m  [16/42], [94mLoss[0m : 2.67484
[1mStep[0m  [20/42], [94mLoss[0m : 2.56014
[1mStep[0m  [24/42], [94mLoss[0m : 2.54205
[1mStep[0m  [28/42], [94mLoss[0m : 2.66604
[1mStep[0m  [32/42], [94mLoss[0m : 2.40842
[1mStep[0m  [36/42], [94mLoss[0m : 2.54350
[1mStep[0m  [40/42], [94mLoss[0m : 2.42923

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.550, [92mTest[0m: 2.415, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49974
[1mStep[0m  [4/42], [94mLoss[0m : 2.70587
[1mStep[0m  [8/42], [94mLoss[0m : 2.29815
[1mStep[0m  [12/42], [94mLoss[0m : 2.37632
[1mStep[0m  [16/42], [94mLoss[0m : 2.55926
[1mStep[0m  [20/42], [94mLoss[0m : 2.44877
[1mStep[0m  [24/42], [94mLoss[0m : 2.65658
[1mStep[0m  [28/42], [94mLoss[0m : 2.72365
[1mStep[0m  [32/42], [94mLoss[0m : 2.50439
[1mStep[0m  [36/42], [94mLoss[0m : 2.53772
[1mStep[0m  [40/42], [94mLoss[0m : 2.61418

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.526, [92mTest[0m: 2.383, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55390
[1mStep[0m  [4/42], [94mLoss[0m : 2.52095
[1mStep[0m  [8/42], [94mLoss[0m : 2.76559
[1mStep[0m  [12/42], [94mLoss[0m : 2.51348
[1mStep[0m  [16/42], [94mLoss[0m : 2.62623
[1mStep[0m  [20/42], [94mLoss[0m : 2.37740
[1mStep[0m  [24/42], [94mLoss[0m : 2.17017
[1mStep[0m  [28/42], [94mLoss[0m : 2.58813
[1mStep[0m  [32/42], [94mLoss[0m : 2.90654
[1mStep[0m  [36/42], [94mLoss[0m : 2.53237
[1mStep[0m  [40/42], [94mLoss[0m : 2.63903

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.517, [92mTest[0m: 2.411, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58493
[1mStep[0m  [4/42], [94mLoss[0m : 2.61017
[1mStep[0m  [8/42], [94mLoss[0m : 2.32723
[1mStep[0m  [12/42], [94mLoss[0m : 2.56788
[1mStep[0m  [16/42], [94mLoss[0m : 2.40162
[1mStep[0m  [20/42], [94mLoss[0m : 2.57527
[1mStep[0m  [24/42], [94mLoss[0m : 2.60738
[1mStep[0m  [28/42], [94mLoss[0m : 2.43767
[1mStep[0m  [32/42], [94mLoss[0m : 2.51169
[1mStep[0m  [36/42], [94mLoss[0m : 2.68310
[1mStep[0m  [40/42], [94mLoss[0m : 2.47352

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.410, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57884
[1mStep[0m  [4/42], [94mLoss[0m : 2.55311
[1mStep[0m  [8/42], [94mLoss[0m : 2.41887
[1mStep[0m  [12/42], [94mLoss[0m : 2.67297
[1mStep[0m  [16/42], [94mLoss[0m : 2.44114
[1mStep[0m  [20/42], [94mLoss[0m : 2.50173
[1mStep[0m  [24/42], [94mLoss[0m : 2.61586
[1mStep[0m  [28/42], [94mLoss[0m : 2.57827
[1mStep[0m  [32/42], [94mLoss[0m : 2.32947
[1mStep[0m  [36/42], [94mLoss[0m : 2.39677
[1mStep[0m  [40/42], [94mLoss[0m : 2.47146

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.514, [92mTest[0m: 2.423, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56153
[1mStep[0m  [4/42], [94mLoss[0m : 2.37985
[1mStep[0m  [8/42], [94mLoss[0m : 2.33297
[1mStep[0m  [12/42], [94mLoss[0m : 2.60292
[1mStep[0m  [16/42], [94mLoss[0m : 2.40935
[1mStep[0m  [20/42], [94mLoss[0m : 2.35679
[1mStep[0m  [24/42], [94mLoss[0m : 2.53377
[1mStep[0m  [28/42], [94mLoss[0m : 2.55111
[1mStep[0m  [32/42], [94mLoss[0m : 2.60992
[1mStep[0m  [36/42], [94mLoss[0m : 2.71472
[1mStep[0m  [40/42], [94mLoss[0m : 2.58746

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.434, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54501
[1mStep[0m  [4/42], [94mLoss[0m : 2.56150
[1mStep[0m  [8/42], [94mLoss[0m : 2.36714
[1mStep[0m  [12/42], [94mLoss[0m : 2.59846
[1mStep[0m  [16/42], [94mLoss[0m : 2.42934
[1mStep[0m  [20/42], [94mLoss[0m : 2.31654
[1mStep[0m  [24/42], [94mLoss[0m : 2.58269
[1mStep[0m  [28/42], [94mLoss[0m : 2.51537
[1mStep[0m  [32/42], [94mLoss[0m : 2.30894
[1mStep[0m  [36/42], [94mLoss[0m : 2.52509
[1mStep[0m  [40/42], [94mLoss[0m : 2.25264

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.423, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.75453
[1mStep[0m  [4/42], [94mLoss[0m : 2.33965
[1mStep[0m  [8/42], [94mLoss[0m : 2.49003
[1mStep[0m  [12/42], [94mLoss[0m : 2.47993
[1mStep[0m  [16/42], [94mLoss[0m : 2.42757
[1mStep[0m  [20/42], [94mLoss[0m : 2.52991
[1mStep[0m  [24/42], [94mLoss[0m : 2.46504
[1mStep[0m  [28/42], [94mLoss[0m : 2.39393
[1mStep[0m  [32/42], [94mLoss[0m : 2.34599
[1mStep[0m  [36/42], [94mLoss[0m : 2.27070
[1mStep[0m  [40/42], [94mLoss[0m : 2.53188

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.470, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27559
[1mStep[0m  [4/42], [94mLoss[0m : 2.43775
[1mStep[0m  [8/42], [94mLoss[0m : 2.42721
[1mStep[0m  [12/42], [94mLoss[0m : 2.49195
[1mStep[0m  [16/42], [94mLoss[0m : 2.44004
[1mStep[0m  [20/42], [94mLoss[0m : 2.54544
[1mStep[0m  [24/42], [94mLoss[0m : 2.44096
[1mStep[0m  [28/42], [94mLoss[0m : 2.49798
[1mStep[0m  [32/42], [94mLoss[0m : 2.49739
[1mStep[0m  [36/42], [94mLoss[0m : 2.48171
[1mStep[0m  [40/42], [94mLoss[0m : 2.45658

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.451, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46916
[1mStep[0m  [4/42], [94mLoss[0m : 2.43608
[1mStep[0m  [8/42], [94mLoss[0m : 2.35104
[1mStep[0m  [12/42], [94mLoss[0m : 2.44421
[1mStep[0m  [16/42], [94mLoss[0m : 2.66695
[1mStep[0m  [20/42], [94mLoss[0m : 2.35134
[1mStep[0m  [24/42], [94mLoss[0m : 2.51538
[1mStep[0m  [28/42], [94mLoss[0m : 2.22692
[1mStep[0m  [32/42], [94mLoss[0m : 2.59056
[1mStep[0m  [36/42], [94mLoss[0m : 2.51726
[1mStep[0m  [40/42], [94mLoss[0m : 2.28340

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.401, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31082
[1mStep[0m  [4/42], [94mLoss[0m : 2.65853
[1mStep[0m  [8/42], [94mLoss[0m : 2.37041
[1mStep[0m  [12/42], [94mLoss[0m : 2.36218
[1mStep[0m  [16/42], [94mLoss[0m : 2.51178
[1mStep[0m  [20/42], [94mLoss[0m : 2.70109
[1mStep[0m  [24/42], [94mLoss[0m : 2.46003
[1mStep[0m  [28/42], [94mLoss[0m : 2.31865
[1mStep[0m  [32/42], [94mLoss[0m : 2.37510
[1mStep[0m  [36/42], [94mLoss[0m : 2.30013
[1mStep[0m  [40/42], [94mLoss[0m : 2.38343

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.473, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48295
[1mStep[0m  [4/42], [94mLoss[0m : 2.42843
[1mStep[0m  [8/42], [94mLoss[0m : 2.43077
[1mStep[0m  [12/42], [94mLoss[0m : 2.36962
[1mStep[0m  [16/42], [94mLoss[0m : 2.50027
[1mStep[0m  [20/42], [94mLoss[0m : 2.81650
[1mStep[0m  [24/42], [94mLoss[0m : 2.39025
[1mStep[0m  [28/42], [94mLoss[0m : 2.20348
[1mStep[0m  [32/42], [94mLoss[0m : 2.36651
[1mStep[0m  [36/42], [94mLoss[0m : 2.33144
[1mStep[0m  [40/42], [94mLoss[0m : 2.26041

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.438, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56492
[1mStep[0m  [4/42], [94mLoss[0m : 2.52857
[1mStep[0m  [8/42], [94mLoss[0m : 2.32641
[1mStep[0m  [12/42], [94mLoss[0m : 2.67502
[1mStep[0m  [16/42], [94mLoss[0m : 2.53710
[1mStep[0m  [20/42], [94mLoss[0m : 2.49039
[1mStep[0m  [24/42], [94mLoss[0m : 2.29238
[1mStep[0m  [28/42], [94mLoss[0m : 2.33115
[1mStep[0m  [32/42], [94mLoss[0m : 2.27041
[1mStep[0m  [36/42], [94mLoss[0m : 2.72866
[1mStep[0m  [40/42], [94mLoss[0m : 2.54413

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.478, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56204
[1mStep[0m  [4/42], [94mLoss[0m : 2.69672
[1mStep[0m  [8/42], [94mLoss[0m : 2.30218
[1mStep[0m  [12/42], [94mLoss[0m : 2.48476
[1mStep[0m  [16/42], [94mLoss[0m : 2.53800
[1mStep[0m  [20/42], [94mLoss[0m : 2.48877
[1mStep[0m  [24/42], [94mLoss[0m : 2.41402
[1mStep[0m  [28/42], [94mLoss[0m : 2.44050
[1mStep[0m  [32/42], [94mLoss[0m : 2.51277
[1mStep[0m  [36/42], [94mLoss[0m : 2.73050
[1mStep[0m  [40/42], [94mLoss[0m : 2.47265

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.458, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49780
[1mStep[0m  [4/42], [94mLoss[0m : 2.47079
[1mStep[0m  [8/42], [94mLoss[0m : 2.33266
[1mStep[0m  [12/42], [94mLoss[0m : 2.37324
[1mStep[0m  [16/42], [94mLoss[0m : 2.36086
[1mStep[0m  [20/42], [94mLoss[0m : 2.44023
[1mStep[0m  [24/42], [94mLoss[0m : 2.50696
[1mStep[0m  [28/42], [94mLoss[0m : 2.41430
[1mStep[0m  [32/42], [94mLoss[0m : 2.31361
[1mStep[0m  [36/42], [94mLoss[0m : 2.47899
[1mStep[0m  [40/42], [94mLoss[0m : 2.38020

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.459, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.68517
[1mStep[0m  [4/42], [94mLoss[0m : 2.47550
[1mStep[0m  [8/42], [94mLoss[0m : 2.29171
[1mStep[0m  [12/42], [94mLoss[0m : 2.42871
[1mStep[0m  [16/42], [94mLoss[0m : 2.64812
[1mStep[0m  [20/42], [94mLoss[0m : 2.52271
[1mStep[0m  [24/42], [94mLoss[0m : 2.68456
[1mStep[0m  [28/42], [94mLoss[0m : 2.41603
[1mStep[0m  [32/42], [94mLoss[0m : 2.34015
[1mStep[0m  [36/42], [94mLoss[0m : 2.62824
[1mStep[0m  [40/42], [94mLoss[0m : 2.23846

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.474, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39747
[1mStep[0m  [4/42], [94mLoss[0m : 2.40636
[1mStep[0m  [8/42], [94mLoss[0m : 2.44116
[1mStep[0m  [12/42], [94mLoss[0m : 2.53156
[1mStep[0m  [16/42], [94mLoss[0m : 2.59535
[1mStep[0m  [20/42], [94mLoss[0m : 2.33233
[1mStep[0m  [24/42], [94mLoss[0m : 2.33345
[1mStep[0m  [28/42], [94mLoss[0m : 2.22916
[1mStep[0m  [32/42], [94mLoss[0m : 2.69368
[1mStep[0m  [36/42], [94mLoss[0m : 2.65594
[1mStep[0m  [40/42], [94mLoss[0m : 2.40738

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.507, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28992
[1mStep[0m  [4/42], [94mLoss[0m : 2.45435
[1mStep[0m  [8/42], [94mLoss[0m : 2.41007
[1mStep[0m  [12/42], [94mLoss[0m : 2.40333
[1mStep[0m  [16/42], [94mLoss[0m : 2.59537
[1mStep[0m  [20/42], [94mLoss[0m : 2.46962
[1mStep[0m  [24/42], [94mLoss[0m : 2.39572
[1mStep[0m  [28/42], [94mLoss[0m : 2.23363
[1mStep[0m  [32/42], [94mLoss[0m : 2.34693
[1mStep[0m  [36/42], [94mLoss[0m : 2.29962
[1mStep[0m  [40/42], [94mLoss[0m : 2.59671

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.445, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35930
[1mStep[0m  [4/42], [94mLoss[0m : 2.43358
[1mStep[0m  [8/42], [94mLoss[0m : 2.22827
[1mStep[0m  [12/42], [94mLoss[0m : 2.64725
[1mStep[0m  [16/42], [94mLoss[0m : 2.53753
[1mStep[0m  [20/42], [94mLoss[0m : 2.34257
[1mStep[0m  [24/42], [94mLoss[0m : 2.54758
[1mStep[0m  [28/42], [94mLoss[0m : 2.66579
[1mStep[0m  [32/42], [94mLoss[0m : 2.63864
[1mStep[0m  [36/42], [94mLoss[0m : 2.45118
[1mStep[0m  [40/42], [94mLoss[0m : 2.40264

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.476, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55904
[1mStep[0m  [4/42], [94mLoss[0m : 2.48315
[1mStep[0m  [8/42], [94mLoss[0m : 2.46290
[1mStep[0m  [12/42], [94mLoss[0m : 2.39304
[1mStep[0m  [16/42], [94mLoss[0m : 2.35387
[1mStep[0m  [20/42], [94mLoss[0m : 2.52033
[1mStep[0m  [24/42], [94mLoss[0m : 2.40963
[1mStep[0m  [28/42], [94mLoss[0m : 2.24502
[1mStep[0m  [32/42], [94mLoss[0m : 2.28285
[1mStep[0m  [36/42], [94mLoss[0m : 2.29379
[1mStep[0m  [40/42], [94mLoss[0m : 2.34146

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.499, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60085
[1mStep[0m  [4/42], [94mLoss[0m : 2.24313
[1mStep[0m  [8/42], [94mLoss[0m : 2.40979
[1mStep[0m  [12/42], [94mLoss[0m : 2.55193
[1mStep[0m  [16/42], [94mLoss[0m : 2.59368
[1mStep[0m  [20/42], [94mLoss[0m : 2.43880
[1mStep[0m  [24/42], [94mLoss[0m : 2.57256
[1mStep[0m  [28/42], [94mLoss[0m : 2.28230
[1mStep[0m  [32/42], [94mLoss[0m : 2.25322
[1mStep[0m  [36/42], [94mLoss[0m : 2.47921
[1mStep[0m  [40/42], [94mLoss[0m : 2.46076

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.492, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42025
[1mStep[0m  [4/42], [94mLoss[0m : 2.29826
[1mStep[0m  [8/42], [94mLoss[0m : 2.39040
[1mStep[0m  [12/42], [94mLoss[0m : 2.30609
[1mStep[0m  [16/42], [94mLoss[0m : 2.50847
[1mStep[0m  [20/42], [94mLoss[0m : 2.41783
[1mStep[0m  [24/42], [94mLoss[0m : 2.35032
[1mStep[0m  [28/42], [94mLoss[0m : 2.24414
[1mStep[0m  [32/42], [94mLoss[0m : 2.49574
[1mStep[0m  [36/42], [94mLoss[0m : 2.49218
[1mStep[0m  [40/42], [94mLoss[0m : 2.41806

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.494, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33210
[1mStep[0m  [4/42], [94mLoss[0m : 2.50716
[1mStep[0m  [8/42], [94mLoss[0m : 2.40975
[1mStep[0m  [12/42], [94mLoss[0m : 2.21070
[1mStep[0m  [16/42], [94mLoss[0m : 2.26286
[1mStep[0m  [20/42], [94mLoss[0m : 2.50785
[1mStep[0m  [24/42], [94mLoss[0m : 2.40390
[1mStep[0m  [28/42], [94mLoss[0m : 2.41103
[1mStep[0m  [32/42], [94mLoss[0m : 2.27396
[1mStep[0m  [36/42], [94mLoss[0m : 2.41225
[1mStep[0m  [40/42], [94mLoss[0m : 2.37183

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.501, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.18276
[1mStep[0m  [4/42], [94mLoss[0m : 2.29186
[1mStep[0m  [8/42], [94mLoss[0m : 2.40166
[1mStep[0m  [12/42], [94mLoss[0m : 2.57838
[1mStep[0m  [16/42], [94mLoss[0m : 2.51752
[1mStep[0m  [20/42], [94mLoss[0m : 2.29423
[1mStep[0m  [24/42], [94mLoss[0m : 2.32215
[1mStep[0m  [28/42], [94mLoss[0m : 2.42682
[1mStep[0m  [32/42], [94mLoss[0m : 2.51285
[1mStep[0m  [36/42], [94mLoss[0m : 2.30634
[1mStep[0m  [40/42], [94mLoss[0m : 2.34813

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.398, [92mTest[0m: 2.473, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.487
====================================

Phase 2 - Evaluation MAE:  2.4865650960377286
MAE score P1       2.360235
MAE score P2       2.486565
loss               2.398254
learning_rate      0.002575
batch_size              256
hidden_sizes      [250, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.1
weight_decay         0.0001
Name: 19, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 11.04932
[1mStep[0m  [8/84], [94mLoss[0m : 10.80628
[1mStep[0m  [16/84], [94mLoss[0m : 10.47108
[1mStep[0m  [24/84], [94mLoss[0m : 10.61696
[1mStep[0m  [32/84], [94mLoss[0m : 11.01611
[1mStep[0m  [40/84], [94mLoss[0m : 9.57818
[1mStep[0m  [48/84], [94mLoss[0m : 10.13694
[1mStep[0m  [56/84], [94mLoss[0m : 9.38160
[1mStep[0m  [64/84], [94mLoss[0m : 9.59573
[1mStep[0m  [72/84], [94mLoss[0m : 8.77052
[1mStep[0m  [80/84], [94mLoss[0m : 8.70384

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.905, [92mTest[0m: 11.035, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.70431
[1mStep[0m  [8/84], [94mLoss[0m : 8.03824
[1mStep[0m  [16/84], [94mLoss[0m : 8.04332
[1mStep[0m  [24/84], [94mLoss[0m : 7.14935
[1mStep[0m  [32/84], [94mLoss[0m : 6.74173
[1mStep[0m  [40/84], [94mLoss[0m : 6.32140
[1mStep[0m  [48/84], [94mLoss[0m : 5.46413
[1mStep[0m  [56/84], [94mLoss[0m : 5.05704
[1mStep[0m  [64/84], [94mLoss[0m : 4.38551
[1mStep[0m  [72/84], [94mLoss[0m : 4.11199
[1mStep[0m  [80/84], [94mLoss[0m : 3.62157

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 6.011, [92mTest[0m: 8.104, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.06258
[1mStep[0m  [8/84], [94mLoss[0m : 3.20945
[1mStep[0m  [16/84], [94mLoss[0m : 3.42945
[1mStep[0m  [24/84], [94mLoss[0m : 2.47728
[1mStep[0m  [32/84], [94mLoss[0m : 2.92046
[1mStep[0m  [40/84], [94mLoss[0m : 2.73698
[1mStep[0m  [48/84], [94mLoss[0m : 2.64059
[1mStep[0m  [56/84], [94mLoss[0m : 2.58680
[1mStep[0m  [64/84], [94mLoss[0m : 2.43401
[1mStep[0m  [72/84], [94mLoss[0m : 2.97629
[1mStep[0m  [80/84], [94mLoss[0m : 2.56918

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.887, [92mTest[0m: 3.190, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.20673
[1mStep[0m  [8/84], [94mLoss[0m : 2.34408
[1mStep[0m  [16/84], [94mLoss[0m : 2.27589
[1mStep[0m  [24/84], [94mLoss[0m : 2.69431
[1mStep[0m  [32/84], [94mLoss[0m : 2.50255
[1mStep[0m  [40/84], [94mLoss[0m : 2.52635
[1mStep[0m  [48/84], [94mLoss[0m : 2.24859
[1mStep[0m  [56/84], [94mLoss[0m : 2.98060
[1mStep[0m  [64/84], [94mLoss[0m : 2.90035
[1mStep[0m  [72/84], [94mLoss[0m : 2.65608
[1mStep[0m  [80/84], [94mLoss[0m : 2.64780

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.593, [92mTest[0m: 2.385, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52069
[1mStep[0m  [8/84], [94mLoss[0m : 2.73712
[1mStep[0m  [16/84], [94mLoss[0m : 2.20229
[1mStep[0m  [24/84], [94mLoss[0m : 2.53570
[1mStep[0m  [32/84], [94mLoss[0m : 2.73191
[1mStep[0m  [40/84], [94mLoss[0m : 2.55597
[1mStep[0m  [48/84], [94mLoss[0m : 2.41822
[1mStep[0m  [56/84], [94mLoss[0m : 2.62679
[1mStep[0m  [64/84], [94mLoss[0m : 2.48438
[1mStep[0m  [72/84], [94mLoss[0m : 2.48050
[1mStep[0m  [80/84], [94mLoss[0m : 2.78811

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.566, [92mTest[0m: 2.368, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.39972
[1mStep[0m  [8/84], [94mLoss[0m : 2.84277
[1mStep[0m  [16/84], [94mLoss[0m : 2.51893
[1mStep[0m  [24/84], [94mLoss[0m : 2.57808
[1mStep[0m  [32/84], [94mLoss[0m : 2.33103
[1mStep[0m  [40/84], [94mLoss[0m : 2.58626
[1mStep[0m  [48/84], [94mLoss[0m : 2.78201
[1mStep[0m  [56/84], [94mLoss[0m : 2.23763
[1mStep[0m  [64/84], [94mLoss[0m : 2.55917
[1mStep[0m  [72/84], [94mLoss[0m : 2.56973
[1mStep[0m  [80/84], [94mLoss[0m : 2.62297

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.540, [92mTest[0m: 2.369, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33605
[1mStep[0m  [8/84], [94mLoss[0m : 2.48931
[1mStep[0m  [16/84], [94mLoss[0m : 2.51832
[1mStep[0m  [24/84], [94mLoss[0m : 2.52552
[1mStep[0m  [32/84], [94mLoss[0m : 2.46722
[1mStep[0m  [40/84], [94mLoss[0m : 2.88640
[1mStep[0m  [48/84], [94mLoss[0m : 2.58676
[1mStep[0m  [56/84], [94mLoss[0m : 2.47896
[1mStep[0m  [64/84], [94mLoss[0m : 2.58860
[1mStep[0m  [72/84], [94mLoss[0m : 2.58122
[1mStep[0m  [80/84], [94mLoss[0m : 2.49132

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.335, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54818
[1mStep[0m  [8/84], [94mLoss[0m : 2.40564
[1mStep[0m  [16/84], [94mLoss[0m : 2.42231
[1mStep[0m  [24/84], [94mLoss[0m : 2.27710
[1mStep[0m  [32/84], [94mLoss[0m : 2.83744
[1mStep[0m  [40/84], [94mLoss[0m : 2.42046
[1mStep[0m  [48/84], [94mLoss[0m : 2.46988
[1mStep[0m  [56/84], [94mLoss[0m : 2.40342
[1mStep[0m  [64/84], [94mLoss[0m : 2.72605
[1mStep[0m  [72/84], [94mLoss[0m : 2.58424
[1mStep[0m  [80/84], [94mLoss[0m : 2.33026

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.343, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60284
[1mStep[0m  [8/84], [94mLoss[0m : 2.36980
[1mStep[0m  [16/84], [94mLoss[0m : 2.31944
[1mStep[0m  [24/84], [94mLoss[0m : 2.30350
[1mStep[0m  [32/84], [94mLoss[0m : 2.44393
[1mStep[0m  [40/84], [94mLoss[0m : 2.42832
[1mStep[0m  [48/84], [94mLoss[0m : 2.49307
[1mStep[0m  [56/84], [94mLoss[0m : 2.38596
[1mStep[0m  [64/84], [94mLoss[0m : 2.65964
[1mStep[0m  [72/84], [94mLoss[0m : 2.52566
[1mStep[0m  [80/84], [94mLoss[0m : 2.42237

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.353, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.57754
[1mStep[0m  [8/84], [94mLoss[0m : 2.41110
[1mStep[0m  [16/84], [94mLoss[0m : 2.89991
[1mStep[0m  [24/84], [94mLoss[0m : 2.76619
[1mStep[0m  [32/84], [94mLoss[0m : 2.42087
[1mStep[0m  [40/84], [94mLoss[0m : 2.68531
[1mStep[0m  [48/84], [94mLoss[0m : 2.61696
[1mStep[0m  [56/84], [94mLoss[0m : 2.45842
[1mStep[0m  [64/84], [94mLoss[0m : 2.75366
[1mStep[0m  [72/84], [94mLoss[0m : 2.31936
[1mStep[0m  [80/84], [94mLoss[0m : 2.40760

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.329, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.23100
[1mStep[0m  [8/84], [94mLoss[0m : 2.49065
[1mStep[0m  [16/84], [94mLoss[0m : 2.64198
[1mStep[0m  [24/84], [94mLoss[0m : 2.49204
[1mStep[0m  [32/84], [94mLoss[0m : 2.51016
[1mStep[0m  [40/84], [94mLoss[0m : 2.21359
[1mStep[0m  [48/84], [94mLoss[0m : 2.50224
[1mStep[0m  [56/84], [94mLoss[0m : 2.35566
[1mStep[0m  [64/84], [94mLoss[0m : 2.23057
[1mStep[0m  [72/84], [94mLoss[0m : 2.53842
[1mStep[0m  [80/84], [94mLoss[0m : 2.94738

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.319, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48875
[1mStep[0m  [8/84], [94mLoss[0m : 2.38046
[1mStep[0m  [16/84], [94mLoss[0m : 2.53127
[1mStep[0m  [24/84], [94mLoss[0m : 2.24381
[1mStep[0m  [32/84], [94mLoss[0m : 2.21736
[1mStep[0m  [40/84], [94mLoss[0m : 2.14431
[1mStep[0m  [48/84], [94mLoss[0m : 2.44965
[1mStep[0m  [56/84], [94mLoss[0m : 2.32582
[1mStep[0m  [64/84], [94mLoss[0m : 2.36028
[1mStep[0m  [72/84], [94mLoss[0m : 2.48951
[1mStep[0m  [80/84], [94mLoss[0m : 2.55220

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.325, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45379
[1mStep[0m  [8/84], [94mLoss[0m : 2.60370
[1mStep[0m  [16/84], [94mLoss[0m : 2.46255
[1mStep[0m  [24/84], [94mLoss[0m : 2.83557
[1mStep[0m  [32/84], [94mLoss[0m : 2.29141
[1mStep[0m  [40/84], [94mLoss[0m : 2.32793
[1mStep[0m  [48/84], [94mLoss[0m : 2.47738
[1mStep[0m  [56/84], [94mLoss[0m : 2.36276
[1mStep[0m  [64/84], [94mLoss[0m : 2.22217
[1mStep[0m  [72/84], [94mLoss[0m : 2.39953
[1mStep[0m  [80/84], [94mLoss[0m : 2.31955

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.328, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45885
[1mStep[0m  [8/84], [94mLoss[0m : 2.31103
[1mStep[0m  [16/84], [94mLoss[0m : 2.39796
[1mStep[0m  [24/84], [94mLoss[0m : 2.35502
[1mStep[0m  [32/84], [94mLoss[0m : 2.46203
[1mStep[0m  [40/84], [94mLoss[0m : 2.51783
[1mStep[0m  [48/84], [94mLoss[0m : 2.56588
[1mStep[0m  [56/84], [94mLoss[0m : 2.29411
[1mStep[0m  [64/84], [94mLoss[0m : 2.45016
[1mStep[0m  [72/84], [94mLoss[0m : 2.26864
[1mStep[0m  [80/84], [94mLoss[0m : 2.55040

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.324, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26469
[1mStep[0m  [8/84], [94mLoss[0m : 2.32947
[1mStep[0m  [16/84], [94mLoss[0m : 2.42324
[1mStep[0m  [24/84], [94mLoss[0m : 2.38524
[1mStep[0m  [32/84], [94mLoss[0m : 2.29991
[1mStep[0m  [40/84], [94mLoss[0m : 2.54861
[1mStep[0m  [48/84], [94mLoss[0m : 2.59893
[1mStep[0m  [56/84], [94mLoss[0m : 2.36752
[1mStep[0m  [64/84], [94mLoss[0m : 2.58370
[1mStep[0m  [72/84], [94mLoss[0m : 2.43197
[1mStep[0m  [80/84], [94mLoss[0m : 2.41232

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.331, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53743
[1mStep[0m  [8/84], [94mLoss[0m : 2.16468
[1mStep[0m  [16/84], [94mLoss[0m : 2.42890
[1mStep[0m  [24/84], [94mLoss[0m : 2.51554
[1mStep[0m  [32/84], [94mLoss[0m : 2.45483
[1mStep[0m  [40/84], [94mLoss[0m : 2.25378
[1mStep[0m  [48/84], [94mLoss[0m : 2.45894
[1mStep[0m  [56/84], [94mLoss[0m : 2.30606
[1mStep[0m  [64/84], [94mLoss[0m : 2.46012
[1mStep[0m  [72/84], [94mLoss[0m : 2.37445
[1mStep[0m  [80/84], [94mLoss[0m : 2.41907

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.332, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.56411
[1mStep[0m  [8/84], [94mLoss[0m : 2.42330
[1mStep[0m  [16/84], [94mLoss[0m : 2.37622
[1mStep[0m  [24/84], [94mLoss[0m : 2.06629
[1mStep[0m  [32/84], [94mLoss[0m : 2.47249
[1mStep[0m  [40/84], [94mLoss[0m : 2.68561
[1mStep[0m  [48/84], [94mLoss[0m : 2.32975
[1mStep[0m  [56/84], [94mLoss[0m : 2.36777
[1mStep[0m  [64/84], [94mLoss[0m : 2.06898
[1mStep[0m  [72/84], [94mLoss[0m : 2.08284
[1mStep[0m  [80/84], [94mLoss[0m : 2.27244

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.333, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.05939
[1mStep[0m  [8/84], [94mLoss[0m : 2.20375
[1mStep[0m  [16/84], [94mLoss[0m : 2.21805
[1mStep[0m  [24/84], [94mLoss[0m : 2.17665
[1mStep[0m  [32/84], [94mLoss[0m : 2.11528
[1mStep[0m  [40/84], [94mLoss[0m : 2.59277
[1mStep[0m  [48/84], [94mLoss[0m : 2.34675
[1mStep[0m  [56/84], [94mLoss[0m : 2.49733
[1mStep[0m  [64/84], [94mLoss[0m : 2.11481
[1mStep[0m  [72/84], [94mLoss[0m : 2.21418
[1mStep[0m  [80/84], [94mLoss[0m : 2.45941

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.317, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67311
[1mStep[0m  [8/84], [94mLoss[0m : 2.22346
[1mStep[0m  [16/84], [94mLoss[0m : 2.50558
[1mStep[0m  [24/84], [94mLoss[0m : 2.51623
[1mStep[0m  [32/84], [94mLoss[0m : 2.43597
[1mStep[0m  [40/84], [94mLoss[0m : 2.33351
[1mStep[0m  [48/84], [94mLoss[0m : 2.47732
[1mStep[0m  [56/84], [94mLoss[0m : 2.42908
[1mStep[0m  [64/84], [94mLoss[0m : 2.32453
[1mStep[0m  [72/84], [94mLoss[0m : 2.48392
[1mStep[0m  [80/84], [94mLoss[0m : 2.60010

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.316, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33587
[1mStep[0m  [8/84], [94mLoss[0m : 2.47255
[1mStep[0m  [16/84], [94mLoss[0m : 2.24430
[1mStep[0m  [24/84], [94mLoss[0m : 2.20728
[1mStep[0m  [32/84], [94mLoss[0m : 2.23763
[1mStep[0m  [40/84], [94mLoss[0m : 2.35297
[1mStep[0m  [48/84], [94mLoss[0m : 2.70347
[1mStep[0m  [56/84], [94mLoss[0m : 2.63279
[1mStep[0m  [64/84], [94mLoss[0m : 2.51415
[1mStep[0m  [72/84], [94mLoss[0m : 2.31389
[1mStep[0m  [80/84], [94mLoss[0m : 2.23392

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.321, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33903
[1mStep[0m  [8/84], [94mLoss[0m : 2.88697
[1mStep[0m  [16/84], [94mLoss[0m : 2.56791
[1mStep[0m  [24/84], [94mLoss[0m : 2.35983
[1mStep[0m  [32/84], [94mLoss[0m : 2.41580
[1mStep[0m  [40/84], [94mLoss[0m : 2.12262
[1mStep[0m  [48/84], [94mLoss[0m : 2.33467
[1mStep[0m  [56/84], [94mLoss[0m : 2.51422
[1mStep[0m  [64/84], [94mLoss[0m : 2.03666
[1mStep[0m  [72/84], [94mLoss[0m : 2.46376
[1mStep[0m  [80/84], [94mLoss[0m : 2.33794

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.389, [92mTest[0m: 2.317, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.25040
[1mStep[0m  [8/84], [94mLoss[0m : 2.38766
[1mStep[0m  [16/84], [94mLoss[0m : 2.42989
[1mStep[0m  [24/84], [94mLoss[0m : 2.40895
[1mStep[0m  [32/84], [94mLoss[0m : 2.26649
[1mStep[0m  [40/84], [94mLoss[0m : 2.52189
[1mStep[0m  [48/84], [94mLoss[0m : 2.60407
[1mStep[0m  [56/84], [94mLoss[0m : 2.37350
[1mStep[0m  [64/84], [94mLoss[0m : 2.03670
[1mStep[0m  [72/84], [94mLoss[0m : 2.69182
[1mStep[0m  [80/84], [94mLoss[0m : 2.46636

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.387, [92mTest[0m: 2.319, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.12643
[1mStep[0m  [8/84], [94mLoss[0m : 2.22552
[1mStep[0m  [16/84], [94mLoss[0m : 2.38757
[1mStep[0m  [24/84], [94mLoss[0m : 2.35376
[1mStep[0m  [32/84], [94mLoss[0m : 2.39691
[1mStep[0m  [40/84], [94mLoss[0m : 2.29061
[1mStep[0m  [48/84], [94mLoss[0m : 2.54425
[1mStep[0m  [56/84], [94mLoss[0m : 2.34896
[1mStep[0m  [64/84], [94mLoss[0m : 1.99300
[1mStep[0m  [72/84], [94mLoss[0m : 2.36467
[1mStep[0m  [80/84], [94mLoss[0m : 2.07939

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.376, [92mTest[0m: 2.323, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30334
[1mStep[0m  [8/84], [94mLoss[0m : 2.42383
[1mStep[0m  [16/84], [94mLoss[0m : 2.35148
[1mStep[0m  [24/84], [94mLoss[0m : 2.28019
[1mStep[0m  [32/84], [94mLoss[0m : 2.59268
[1mStep[0m  [40/84], [94mLoss[0m : 2.52447
[1mStep[0m  [48/84], [94mLoss[0m : 2.22899
[1mStep[0m  [56/84], [94mLoss[0m : 2.25259
[1mStep[0m  [64/84], [94mLoss[0m : 2.41050
[1mStep[0m  [72/84], [94mLoss[0m : 2.45893
[1mStep[0m  [80/84], [94mLoss[0m : 2.26478

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.387, [92mTest[0m: 2.314, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50178
[1mStep[0m  [8/84], [94mLoss[0m : 2.19500
[1mStep[0m  [16/84], [94mLoss[0m : 2.07934
[1mStep[0m  [24/84], [94mLoss[0m : 2.43993
[1mStep[0m  [32/84], [94mLoss[0m : 2.16575
[1mStep[0m  [40/84], [94mLoss[0m : 2.13668
[1mStep[0m  [48/84], [94mLoss[0m : 2.58291
[1mStep[0m  [56/84], [94mLoss[0m : 2.20969
[1mStep[0m  [64/84], [94mLoss[0m : 2.19194
[1mStep[0m  [72/84], [94mLoss[0m : 2.81833
[1mStep[0m  [80/84], [94mLoss[0m : 2.15048

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.382, [92mTest[0m: 2.325, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42372
[1mStep[0m  [8/84], [94mLoss[0m : 2.27380
[1mStep[0m  [16/84], [94mLoss[0m : 2.17273
[1mStep[0m  [24/84], [94mLoss[0m : 2.43427
[1mStep[0m  [32/84], [94mLoss[0m : 2.24475
[1mStep[0m  [40/84], [94mLoss[0m : 2.23673
[1mStep[0m  [48/84], [94mLoss[0m : 2.14753
[1mStep[0m  [56/84], [94mLoss[0m : 2.21685
[1mStep[0m  [64/84], [94mLoss[0m : 2.63354
[1mStep[0m  [72/84], [94mLoss[0m : 2.49592
[1mStep[0m  [80/84], [94mLoss[0m : 2.38623

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.374, [92mTest[0m: 2.327, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.32491
[1mStep[0m  [8/84], [94mLoss[0m : 2.26763
[1mStep[0m  [16/84], [94mLoss[0m : 2.35143
[1mStep[0m  [24/84], [94mLoss[0m : 2.42346
[1mStep[0m  [32/84], [94mLoss[0m : 2.42141
[1mStep[0m  [40/84], [94mLoss[0m : 2.43550
[1mStep[0m  [48/84], [94mLoss[0m : 2.48177
[1mStep[0m  [56/84], [94mLoss[0m : 1.98358
[1mStep[0m  [64/84], [94mLoss[0m : 2.39026
[1mStep[0m  [72/84], [94mLoss[0m : 2.18924
[1mStep[0m  [80/84], [94mLoss[0m : 2.64577

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.372, [92mTest[0m: 2.308, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.22625
[1mStep[0m  [8/84], [94mLoss[0m : 2.53344
[1mStep[0m  [16/84], [94mLoss[0m : 2.24649
[1mStep[0m  [24/84], [94mLoss[0m : 2.24235
[1mStep[0m  [32/84], [94mLoss[0m : 2.39780
[1mStep[0m  [40/84], [94mLoss[0m : 2.58110
[1mStep[0m  [48/84], [94mLoss[0m : 2.34971
[1mStep[0m  [56/84], [94mLoss[0m : 2.38699
[1mStep[0m  [64/84], [94mLoss[0m : 2.66995
[1mStep[0m  [72/84], [94mLoss[0m : 2.55009
[1mStep[0m  [80/84], [94mLoss[0m : 2.45586

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.378, [92mTest[0m: 2.330, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.34953
[1mStep[0m  [8/84], [94mLoss[0m : 2.65236
[1mStep[0m  [16/84], [94mLoss[0m : 2.21872
[1mStep[0m  [24/84], [94mLoss[0m : 2.29741
[1mStep[0m  [32/84], [94mLoss[0m : 2.71245
[1mStep[0m  [40/84], [94mLoss[0m : 2.36780
[1mStep[0m  [48/84], [94mLoss[0m : 2.33157
[1mStep[0m  [56/84], [94mLoss[0m : 2.62536
[1mStep[0m  [64/84], [94mLoss[0m : 2.59631
[1mStep[0m  [72/84], [94mLoss[0m : 2.24708
[1mStep[0m  [80/84], [94mLoss[0m : 2.18511

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.376, [92mTest[0m: 2.316, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44407
[1mStep[0m  [8/84], [94mLoss[0m : 2.30138
[1mStep[0m  [16/84], [94mLoss[0m : 2.42824
[1mStep[0m  [24/84], [94mLoss[0m : 2.26842
[1mStep[0m  [32/84], [94mLoss[0m : 2.28701
[1mStep[0m  [40/84], [94mLoss[0m : 2.13694
[1mStep[0m  [48/84], [94mLoss[0m : 2.20070
[1mStep[0m  [56/84], [94mLoss[0m : 2.50462
[1mStep[0m  [64/84], [94mLoss[0m : 2.58909
[1mStep[0m  [72/84], [94mLoss[0m : 2.20684
[1mStep[0m  [80/84], [94mLoss[0m : 2.47137

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.368, [92mTest[0m: 2.312, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.318
====================================

Phase 1 - Evaluation MAE:  2.3182755538395474
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 2.63128
[1mStep[0m  [8/84], [94mLoss[0m : 2.50237
[1mStep[0m  [16/84], [94mLoss[0m : 2.20367
[1mStep[0m  [24/84], [94mLoss[0m : 2.37647
[1mStep[0m  [32/84], [94mLoss[0m : 2.59287
[1mStep[0m  [40/84], [94mLoss[0m : 2.72255
[1mStep[0m  [48/84], [94mLoss[0m : 2.41993
[1mStep[0m  [56/84], [94mLoss[0m : 2.38485
[1mStep[0m  [64/84], [94mLoss[0m : 2.52134
[1mStep[0m  [72/84], [94mLoss[0m : 2.46808
[1mStep[0m  [80/84], [94mLoss[0m : 2.44666

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.319, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.28604
[1mStep[0m  [8/84], [94mLoss[0m : 2.65531
[1mStep[0m  [16/84], [94mLoss[0m : 2.12765
[1mStep[0m  [24/84], [94mLoss[0m : 2.07564
[1mStep[0m  [32/84], [94mLoss[0m : 2.40370
[1mStep[0m  [40/84], [94mLoss[0m : 2.14070
[1mStep[0m  [48/84], [94mLoss[0m : 2.52610
[1mStep[0m  [56/84], [94mLoss[0m : 2.37583
[1mStep[0m  [64/84], [94mLoss[0m : 2.22709
[1mStep[0m  [72/84], [94mLoss[0m : 2.26425
[1mStep[0m  [80/84], [94mLoss[0m : 2.39653

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.351, [92mTest[0m: 2.377, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.09836
[1mStep[0m  [8/84], [94mLoss[0m : 2.29451
[1mStep[0m  [16/84], [94mLoss[0m : 2.33316
[1mStep[0m  [24/84], [94mLoss[0m : 2.29159
[1mStep[0m  [32/84], [94mLoss[0m : 2.39606
[1mStep[0m  [40/84], [94mLoss[0m : 2.40807
[1mStep[0m  [48/84], [94mLoss[0m : 2.34517
[1mStep[0m  [56/84], [94mLoss[0m : 2.09999
[1mStep[0m  [64/84], [94mLoss[0m : 2.23715
[1mStep[0m  [72/84], [94mLoss[0m : 2.51411
[1mStep[0m  [80/84], [94mLoss[0m : 2.45473

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.279, [92mTest[0m: 2.347, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.05581
[1mStep[0m  [8/84], [94mLoss[0m : 2.26131
[1mStep[0m  [16/84], [94mLoss[0m : 2.34372
[1mStep[0m  [24/84], [94mLoss[0m : 2.06681
[1mStep[0m  [32/84], [94mLoss[0m : 1.99846
[1mStep[0m  [40/84], [94mLoss[0m : 2.01669
[1mStep[0m  [48/84], [94mLoss[0m : 2.31097
[1mStep[0m  [56/84], [94mLoss[0m : 2.06118
[1mStep[0m  [64/84], [94mLoss[0m : 1.82639
[1mStep[0m  [72/84], [94mLoss[0m : 1.77960
[1mStep[0m  [80/84], [94mLoss[0m : 1.83331

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.183, [92mTest[0m: 2.323, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31773
[1mStep[0m  [8/84], [94mLoss[0m : 2.10554
[1mStep[0m  [16/84], [94mLoss[0m : 1.89742
[1mStep[0m  [24/84], [94mLoss[0m : 2.10641
[1mStep[0m  [32/84], [94mLoss[0m : 2.29742
[1mStep[0m  [40/84], [94mLoss[0m : 2.00520
[1mStep[0m  [48/84], [94mLoss[0m : 1.91359
[1mStep[0m  [56/84], [94mLoss[0m : 2.08350
[1mStep[0m  [64/84], [94mLoss[0m : 2.08560
[1mStep[0m  [72/84], [94mLoss[0m : 2.31599
[1mStep[0m  [80/84], [94mLoss[0m : 1.78336

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.126, [92mTest[0m: 2.366, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.97274
[1mStep[0m  [8/84], [94mLoss[0m : 1.84999
[1mStep[0m  [16/84], [94mLoss[0m : 2.12495
[1mStep[0m  [24/84], [94mLoss[0m : 2.08549
[1mStep[0m  [32/84], [94mLoss[0m : 1.96270
[1mStep[0m  [40/84], [94mLoss[0m : 2.05045
[1mStep[0m  [48/84], [94mLoss[0m : 2.45025
[1mStep[0m  [56/84], [94mLoss[0m : 2.58632
[1mStep[0m  [64/84], [94mLoss[0m : 2.04831
[1mStep[0m  [72/84], [94mLoss[0m : 2.12122
[1mStep[0m  [80/84], [94mLoss[0m : 2.25189

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.053, [92mTest[0m: 2.384, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.05702
[1mStep[0m  [8/84], [94mLoss[0m : 2.02958
[1mStep[0m  [16/84], [94mLoss[0m : 2.02524
[1mStep[0m  [24/84], [94mLoss[0m : 2.09008
[1mStep[0m  [32/84], [94mLoss[0m : 2.00089
[1mStep[0m  [40/84], [94mLoss[0m : 2.02441
[1mStep[0m  [48/84], [94mLoss[0m : 1.98644
[1mStep[0m  [56/84], [94mLoss[0m : 1.91233
[1mStep[0m  [64/84], [94mLoss[0m : 1.80068
[1mStep[0m  [72/84], [94mLoss[0m : 2.13442
[1mStep[0m  [80/84], [94mLoss[0m : 2.04880

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.988, [92mTest[0m: 2.437, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.97371
[1mStep[0m  [8/84], [94mLoss[0m : 1.70582
[1mStep[0m  [16/84], [94mLoss[0m : 2.07980
[1mStep[0m  [24/84], [94mLoss[0m : 1.82827
[1mStep[0m  [32/84], [94mLoss[0m : 1.92343
[1mStep[0m  [40/84], [94mLoss[0m : 1.68223
[1mStep[0m  [48/84], [94mLoss[0m : 1.77528
[1mStep[0m  [56/84], [94mLoss[0m : 1.88425
[1mStep[0m  [64/84], [94mLoss[0m : 2.30751
[1mStep[0m  [72/84], [94mLoss[0m : 2.21804
[1mStep[0m  [80/84], [94mLoss[0m : 2.05165

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.917, [92mTest[0m: 2.391, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.90262
[1mStep[0m  [8/84], [94mLoss[0m : 1.93835
[1mStep[0m  [16/84], [94mLoss[0m : 1.87708
[1mStep[0m  [24/84], [94mLoss[0m : 1.76314
[1mStep[0m  [32/84], [94mLoss[0m : 1.90697
[1mStep[0m  [40/84], [94mLoss[0m : 1.88210
[1mStep[0m  [48/84], [94mLoss[0m : 1.90259
[1mStep[0m  [56/84], [94mLoss[0m : 1.95009
[1mStep[0m  [64/84], [94mLoss[0m : 2.06086
[1mStep[0m  [72/84], [94mLoss[0m : 1.75140
[1mStep[0m  [80/84], [94mLoss[0m : 2.09454

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.882, [92mTest[0m: 2.385, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.70392
[1mStep[0m  [8/84], [94mLoss[0m : 1.70502
[1mStep[0m  [16/84], [94mLoss[0m : 1.77518
[1mStep[0m  [24/84], [94mLoss[0m : 1.94386
[1mStep[0m  [32/84], [94mLoss[0m : 1.90442
[1mStep[0m  [40/84], [94mLoss[0m : 1.80588
[1mStep[0m  [48/84], [94mLoss[0m : 1.87282
[1mStep[0m  [56/84], [94mLoss[0m : 1.78033
[1mStep[0m  [64/84], [94mLoss[0m : 1.87025
[1mStep[0m  [72/84], [94mLoss[0m : 1.86189
[1mStep[0m  [80/84], [94mLoss[0m : 1.74131

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.810, [92mTest[0m: 2.472, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.82397
[1mStep[0m  [8/84], [94mLoss[0m : 1.75648
[1mStep[0m  [16/84], [94mLoss[0m : 1.66772
[1mStep[0m  [24/84], [94mLoss[0m : 1.66898
[1mStep[0m  [32/84], [94mLoss[0m : 1.76497
[1mStep[0m  [40/84], [94mLoss[0m : 1.73694
[1mStep[0m  [48/84], [94mLoss[0m : 1.76954
[1mStep[0m  [56/84], [94mLoss[0m : 1.82380
[1mStep[0m  [64/84], [94mLoss[0m : 1.88962
[1mStep[0m  [72/84], [94mLoss[0m : 1.75244
[1mStep[0m  [80/84], [94mLoss[0m : 1.79098

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.775, [92mTest[0m: 2.422, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.78628
[1mStep[0m  [8/84], [94mLoss[0m : 1.72338
[1mStep[0m  [16/84], [94mLoss[0m : 1.65836
[1mStep[0m  [24/84], [94mLoss[0m : 1.57365
[1mStep[0m  [32/84], [94mLoss[0m : 1.52738
[1mStep[0m  [40/84], [94mLoss[0m : 1.75409
[1mStep[0m  [48/84], [94mLoss[0m : 1.62461
[1mStep[0m  [56/84], [94mLoss[0m : 1.93201
[1mStep[0m  [64/84], [94mLoss[0m : 1.68692
[1mStep[0m  [72/84], [94mLoss[0m : 1.70035
[1mStep[0m  [80/84], [94mLoss[0m : 1.94338

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.725, [92mTest[0m: 2.431, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.67684
[1mStep[0m  [8/84], [94mLoss[0m : 1.81794
[1mStep[0m  [16/84], [94mLoss[0m : 1.71046
[1mStep[0m  [24/84], [94mLoss[0m : 1.78228
[1mStep[0m  [32/84], [94mLoss[0m : 1.60298
[1mStep[0m  [40/84], [94mLoss[0m : 1.80866
[1mStep[0m  [48/84], [94mLoss[0m : 1.90623
[1mStep[0m  [56/84], [94mLoss[0m : 1.71355
[1mStep[0m  [64/84], [94mLoss[0m : 1.67606
[1mStep[0m  [72/84], [94mLoss[0m : 1.56872
[1mStep[0m  [80/84], [94mLoss[0m : 1.75265

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.692, [92mTest[0m: 2.481, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.65565
[1mStep[0m  [8/84], [94mLoss[0m : 1.78341
[1mStep[0m  [16/84], [94mLoss[0m : 1.51449
[1mStep[0m  [24/84], [94mLoss[0m : 1.41736
[1mStep[0m  [32/84], [94mLoss[0m : 1.57550
[1mStep[0m  [40/84], [94mLoss[0m : 1.67260
[1mStep[0m  [48/84], [94mLoss[0m : 1.82873
[1mStep[0m  [56/84], [94mLoss[0m : 1.46778
[1mStep[0m  [64/84], [94mLoss[0m : 1.41138
[1mStep[0m  [72/84], [94mLoss[0m : 1.57918
[1mStep[0m  [80/84], [94mLoss[0m : 1.50567

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.652, [92mTest[0m: 2.458, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.52711
[1mStep[0m  [8/84], [94mLoss[0m : 1.34722
[1mStep[0m  [16/84], [94mLoss[0m : 1.43490
[1mStep[0m  [24/84], [94mLoss[0m : 1.80653
[1mStep[0m  [32/84], [94mLoss[0m : 1.56984
[1mStep[0m  [40/84], [94mLoss[0m : 1.66793
[1mStep[0m  [48/84], [94mLoss[0m : 1.63046
[1mStep[0m  [56/84], [94mLoss[0m : 1.59005
[1mStep[0m  [64/84], [94mLoss[0m : 1.93431
[1mStep[0m  [72/84], [94mLoss[0m : 1.54859
[1mStep[0m  [80/84], [94mLoss[0m : 1.77373

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.628, [92mTest[0m: 2.411, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.42790
[1mStep[0m  [8/84], [94mLoss[0m : 1.37292
[1mStep[0m  [16/84], [94mLoss[0m : 1.63152
[1mStep[0m  [24/84], [94mLoss[0m : 1.36114
[1mStep[0m  [32/84], [94mLoss[0m : 1.59076
[1mStep[0m  [40/84], [94mLoss[0m : 1.40903
[1mStep[0m  [48/84], [94mLoss[0m : 1.77146
[1mStep[0m  [56/84], [94mLoss[0m : 1.74172
[1mStep[0m  [64/84], [94mLoss[0m : 1.71659
[1mStep[0m  [72/84], [94mLoss[0m : 1.38687
[1mStep[0m  [80/84], [94mLoss[0m : 1.57160

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.578, [92mTest[0m: 2.503, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.44709
[1mStep[0m  [8/84], [94mLoss[0m : 1.58689
[1mStep[0m  [16/84], [94mLoss[0m : 1.77805
[1mStep[0m  [24/84], [94mLoss[0m : 1.46148
[1mStep[0m  [32/84], [94mLoss[0m : 1.52170
[1mStep[0m  [40/84], [94mLoss[0m : 1.41579
[1mStep[0m  [48/84], [94mLoss[0m : 1.59318
[1mStep[0m  [56/84], [94mLoss[0m : 1.77049
[1mStep[0m  [64/84], [94mLoss[0m : 1.49099
[1mStep[0m  [72/84], [94mLoss[0m : 1.61001
[1mStep[0m  [80/84], [94mLoss[0m : 1.56422

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.556, [92mTest[0m: 2.480, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.28935
[1mStep[0m  [8/84], [94mLoss[0m : 1.60630
[1mStep[0m  [16/84], [94mLoss[0m : 1.49936
[1mStep[0m  [24/84], [94mLoss[0m : 1.41235
[1mStep[0m  [32/84], [94mLoss[0m : 1.51442
[1mStep[0m  [40/84], [94mLoss[0m : 1.51130
[1mStep[0m  [48/84], [94mLoss[0m : 1.48307
[1mStep[0m  [56/84], [94mLoss[0m : 1.56118
[1mStep[0m  [64/84], [94mLoss[0m : 1.63492
[1mStep[0m  [72/84], [94mLoss[0m : 1.50505
[1mStep[0m  [80/84], [94mLoss[0m : 1.49817

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.514, [92mTest[0m: 2.470, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.29476
[1mStep[0m  [8/84], [94mLoss[0m : 1.49903
[1mStep[0m  [16/84], [94mLoss[0m : 1.32560
[1mStep[0m  [24/84], [94mLoss[0m : 1.27525
[1mStep[0m  [32/84], [94mLoss[0m : 1.47060
[1mStep[0m  [40/84], [94mLoss[0m : 1.39298
[1mStep[0m  [48/84], [94mLoss[0m : 1.62629
[1mStep[0m  [56/84], [94mLoss[0m : 1.64756
[1mStep[0m  [64/84], [94mLoss[0m : 1.44770
[1mStep[0m  [72/84], [94mLoss[0m : 1.34994
[1mStep[0m  [80/84], [94mLoss[0m : 1.58935

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.484, [92mTest[0m: 2.483, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.54928
[1mStep[0m  [8/84], [94mLoss[0m : 1.45978
[1mStep[0m  [16/84], [94mLoss[0m : 1.33284
[1mStep[0m  [24/84], [94mLoss[0m : 1.49213
[1mStep[0m  [32/84], [94mLoss[0m : 1.39335
[1mStep[0m  [40/84], [94mLoss[0m : 1.40100
[1mStep[0m  [48/84], [94mLoss[0m : 1.37720
[1mStep[0m  [56/84], [94mLoss[0m : 1.43601
[1mStep[0m  [64/84], [94mLoss[0m : 1.30914
[1mStep[0m  [72/84], [94mLoss[0m : 1.44671
[1mStep[0m  [80/84], [94mLoss[0m : 1.37245

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.459, [92mTest[0m: 2.482, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.26714
[1mStep[0m  [8/84], [94mLoss[0m : 1.38020
[1mStep[0m  [16/84], [94mLoss[0m : 1.54473
[1mStep[0m  [24/84], [94mLoss[0m : 1.40803
[1mStep[0m  [32/84], [94mLoss[0m : 1.44304
[1mStep[0m  [40/84], [94mLoss[0m : 1.50260
[1mStep[0m  [48/84], [94mLoss[0m : 1.37421
[1mStep[0m  [56/84], [94mLoss[0m : 1.35285
[1mStep[0m  [64/84], [94mLoss[0m : 1.44932
[1mStep[0m  [72/84], [94mLoss[0m : 1.51664
[1mStep[0m  [80/84], [94mLoss[0m : 1.35273

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.416, [92mTest[0m: 2.505, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.31124
[1mStep[0m  [8/84], [94mLoss[0m : 1.29254
[1mStep[0m  [16/84], [94mLoss[0m : 1.37294
[1mStep[0m  [24/84], [94mLoss[0m : 1.34984
[1mStep[0m  [32/84], [94mLoss[0m : 1.68813
[1mStep[0m  [40/84], [94mLoss[0m : 1.22224
[1mStep[0m  [48/84], [94mLoss[0m : 1.48480
[1mStep[0m  [56/84], [94mLoss[0m : 1.42257
[1mStep[0m  [64/84], [94mLoss[0m : 1.17278
[1mStep[0m  [72/84], [94mLoss[0m : 1.48042
[1mStep[0m  [80/84], [94mLoss[0m : 1.31571

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.400, [92mTest[0m: 2.454, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.33067
[1mStep[0m  [8/84], [94mLoss[0m : 1.21762
[1mStep[0m  [16/84], [94mLoss[0m : 1.41863
[1mStep[0m  [24/84], [94mLoss[0m : 1.25945
[1mStep[0m  [32/84], [94mLoss[0m : 1.34139
[1mStep[0m  [40/84], [94mLoss[0m : 1.21287
[1mStep[0m  [48/84], [94mLoss[0m : 1.25957
[1mStep[0m  [56/84], [94mLoss[0m : 1.31819
[1mStep[0m  [64/84], [94mLoss[0m : 1.19506
[1mStep[0m  [72/84], [94mLoss[0m : 1.36073
[1mStep[0m  [80/84], [94mLoss[0m : 1.35277

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.362, [92mTest[0m: 2.470, [96mlr[0m: 0.0023175
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 22 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.445
====================================

Phase 2 - Evaluation MAE:  2.4452515074184964
MAE score P1       2.318276
MAE score P2       2.445252
loss               1.362464
learning_rate      0.002575
batch_size              128
hidden_sizes      [200, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping         True
dropout                 0.2
momentum                0.9
weight_decay         0.0001
Name: 20, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 10.36676
[1mStep[0m  [4/42], [94mLoss[0m : 10.34777
[1mStep[0m  [8/42], [94mLoss[0m : 9.58819
[1mStep[0m  [12/42], [94mLoss[0m : 8.54838
[1mStep[0m  [16/42], [94mLoss[0m : 7.36069
[1mStep[0m  [20/42], [94mLoss[0m : 6.76701
[1mStep[0m  [24/42], [94mLoss[0m : 6.43239
[1mStep[0m  [28/42], [94mLoss[0m : 5.93562
[1mStep[0m  [32/42], [94mLoss[0m : 5.84257
[1mStep[0m  [36/42], [94mLoss[0m : 4.80620
[1mStep[0m  [40/42], [94mLoss[0m : 4.05092

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 7.201, [92mTest[0m: 10.950, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.93664
[1mStep[0m  [4/42], [94mLoss[0m : 3.68250
[1mStep[0m  [8/42], [94mLoss[0m : 3.40680
[1mStep[0m  [12/42], [94mLoss[0m : 2.96577
[1mStep[0m  [16/42], [94mLoss[0m : 2.98343
[1mStep[0m  [20/42], [94mLoss[0m : 2.87299
[1mStep[0m  [24/42], [94mLoss[0m : 2.97646
[1mStep[0m  [28/42], [94mLoss[0m : 2.40729
[1mStep[0m  [32/42], [94mLoss[0m : 2.85590
[1mStep[0m  [36/42], [94mLoss[0m : 2.45078
[1mStep[0m  [40/42], [94mLoss[0m : 2.67184

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.064, [92mTest[0m: 5.275, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.72354
[1mStep[0m  [4/42], [94mLoss[0m : 2.65615
[1mStep[0m  [8/42], [94mLoss[0m : 2.64441
[1mStep[0m  [12/42], [94mLoss[0m : 2.76380
[1mStep[0m  [16/42], [94mLoss[0m : 2.76911
[1mStep[0m  [20/42], [94mLoss[0m : 2.66611
[1mStep[0m  [24/42], [94mLoss[0m : 3.14335
[1mStep[0m  [28/42], [94mLoss[0m : 2.33875
[1mStep[0m  [32/42], [94mLoss[0m : 2.52680
[1mStep[0m  [36/42], [94mLoss[0m : 2.48392
[1mStep[0m  [40/42], [94mLoss[0m : 2.44641

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.646, [92mTest[0m: 2.941, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41962
[1mStep[0m  [4/42], [94mLoss[0m : 2.57529
[1mStep[0m  [8/42], [94mLoss[0m : 2.68463
[1mStep[0m  [12/42], [94mLoss[0m : 2.61049
[1mStep[0m  [16/42], [94mLoss[0m : 2.43117
[1mStep[0m  [20/42], [94mLoss[0m : 2.51293
[1mStep[0m  [24/42], [94mLoss[0m : 2.38540
[1mStep[0m  [28/42], [94mLoss[0m : 2.47736
[1mStep[0m  [32/42], [94mLoss[0m : 2.68150
[1mStep[0m  [36/42], [94mLoss[0m : 2.94835
[1mStep[0m  [40/42], [94mLoss[0m : 2.38608

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.599, [92mTest[0m: 2.688, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46616
[1mStep[0m  [4/42], [94mLoss[0m : 2.37377
[1mStep[0m  [8/42], [94mLoss[0m : 2.43368
[1mStep[0m  [12/42], [94mLoss[0m : 2.93313
[1mStep[0m  [16/42], [94mLoss[0m : 2.65741
[1mStep[0m  [20/42], [94mLoss[0m : 2.44024
[1mStep[0m  [24/42], [94mLoss[0m : 2.62240
[1mStep[0m  [28/42], [94mLoss[0m : 2.47831
[1mStep[0m  [32/42], [94mLoss[0m : 2.24223
[1mStep[0m  [36/42], [94mLoss[0m : 2.57244
[1mStep[0m  [40/42], [94mLoss[0m : 2.45296

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.553, [92mTest[0m: 2.633, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52762
[1mStep[0m  [4/42], [94mLoss[0m : 2.67599
[1mStep[0m  [8/42], [94mLoss[0m : 2.43308
[1mStep[0m  [12/42], [94mLoss[0m : 2.57506
[1mStep[0m  [16/42], [94mLoss[0m : 2.66338
[1mStep[0m  [20/42], [94mLoss[0m : 2.53446
[1mStep[0m  [24/42], [94mLoss[0m : 2.42563
[1mStep[0m  [28/42], [94mLoss[0m : 2.48151
[1mStep[0m  [32/42], [94mLoss[0m : 2.52883
[1mStep[0m  [36/42], [94mLoss[0m : 2.51821
[1mStep[0m  [40/42], [94mLoss[0m : 2.54948

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.589, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58148
[1mStep[0m  [4/42], [94mLoss[0m : 2.35722
[1mStep[0m  [8/42], [94mLoss[0m : 2.49045
[1mStep[0m  [12/42], [94mLoss[0m : 2.48101
[1mStep[0m  [16/42], [94mLoss[0m : 2.55882
[1mStep[0m  [20/42], [94mLoss[0m : 2.59783
[1mStep[0m  [24/42], [94mLoss[0m : 2.56456
[1mStep[0m  [28/42], [94mLoss[0m : 2.69686
[1mStep[0m  [32/42], [94mLoss[0m : 2.50594
[1mStep[0m  [36/42], [94mLoss[0m : 2.58782
[1mStep[0m  [40/42], [94mLoss[0m : 2.58431

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.573, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56009
[1mStep[0m  [4/42], [94mLoss[0m : 2.48581
[1mStep[0m  [8/42], [94mLoss[0m : 2.61746
[1mStep[0m  [12/42], [94mLoss[0m : 2.65096
[1mStep[0m  [16/42], [94mLoss[0m : 2.53395
[1mStep[0m  [20/42], [94mLoss[0m : 2.40690
[1mStep[0m  [24/42], [94mLoss[0m : 2.44036
[1mStep[0m  [28/42], [94mLoss[0m : 2.44164
[1mStep[0m  [32/42], [94mLoss[0m : 2.50353
[1mStep[0m  [36/42], [94mLoss[0m : 2.51120
[1mStep[0m  [40/42], [94mLoss[0m : 2.39565

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.534, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61231
[1mStep[0m  [4/42], [94mLoss[0m : 2.48697
[1mStep[0m  [8/42], [94mLoss[0m : 2.74903
[1mStep[0m  [12/42], [94mLoss[0m : 2.53113
[1mStep[0m  [16/42], [94mLoss[0m : 2.60444
[1mStep[0m  [20/42], [94mLoss[0m : 2.37733
[1mStep[0m  [24/42], [94mLoss[0m : 2.43526
[1mStep[0m  [28/42], [94mLoss[0m : 2.47732
[1mStep[0m  [32/42], [94mLoss[0m : 2.46004
[1mStep[0m  [36/42], [94mLoss[0m : 2.27400
[1mStep[0m  [40/42], [94mLoss[0m : 2.50101

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.544, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44233
[1mStep[0m  [4/42], [94mLoss[0m : 2.43761
[1mStep[0m  [8/42], [94mLoss[0m : 2.45489
[1mStep[0m  [12/42], [94mLoss[0m : 2.40573
[1mStep[0m  [16/42], [94mLoss[0m : 2.53911
[1mStep[0m  [20/42], [94mLoss[0m : 2.37011
[1mStep[0m  [24/42], [94mLoss[0m : 2.42498
[1mStep[0m  [28/42], [94mLoss[0m : 2.48991
[1mStep[0m  [32/42], [94mLoss[0m : 2.54962
[1mStep[0m  [36/42], [94mLoss[0m : 2.49776
[1mStep[0m  [40/42], [94mLoss[0m : 2.40144

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.587, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58081
[1mStep[0m  [4/42], [94mLoss[0m : 2.52095
[1mStep[0m  [8/42], [94mLoss[0m : 2.60145
[1mStep[0m  [12/42], [94mLoss[0m : 2.33848
[1mStep[0m  [16/42], [94mLoss[0m : 2.68968
[1mStep[0m  [20/42], [94mLoss[0m : 2.50077
[1mStep[0m  [24/42], [94mLoss[0m : 2.50698
[1mStep[0m  [28/42], [94mLoss[0m : 2.52369
[1mStep[0m  [32/42], [94mLoss[0m : 2.31215
[1mStep[0m  [36/42], [94mLoss[0m : 2.49263
[1mStep[0m  [40/42], [94mLoss[0m : 2.38657

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.551, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45766
[1mStep[0m  [4/42], [94mLoss[0m : 2.49458
[1mStep[0m  [8/42], [94mLoss[0m : 2.43832
[1mStep[0m  [12/42], [94mLoss[0m : 2.53305
[1mStep[0m  [16/42], [94mLoss[0m : 2.61249
[1mStep[0m  [20/42], [94mLoss[0m : 2.41507
[1mStep[0m  [24/42], [94mLoss[0m : 2.48426
[1mStep[0m  [28/42], [94mLoss[0m : 2.15015
[1mStep[0m  [32/42], [94mLoss[0m : 2.43249
[1mStep[0m  [36/42], [94mLoss[0m : 2.36017
[1mStep[0m  [40/42], [94mLoss[0m : 2.34851

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.568, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47072
[1mStep[0m  [4/42], [94mLoss[0m : 2.43207
[1mStep[0m  [8/42], [94mLoss[0m : 2.44650
[1mStep[0m  [12/42], [94mLoss[0m : 2.42770
[1mStep[0m  [16/42], [94mLoss[0m : 2.49484
[1mStep[0m  [20/42], [94mLoss[0m : 2.55118
[1mStep[0m  [24/42], [94mLoss[0m : 2.62852
[1mStep[0m  [28/42], [94mLoss[0m : 2.41114
[1mStep[0m  [32/42], [94mLoss[0m : 2.56922
[1mStep[0m  [36/42], [94mLoss[0m : 2.35196
[1mStep[0m  [40/42], [94mLoss[0m : 2.50943

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.566, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27516
[1mStep[0m  [4/42], [94mLoss[0m : 2.46389
[1mStep[0m  [8/42], [94mLoss[0m : 2.43005
[1mStep[0m  [12/42], [94mLoss[0m : 2.41231
[1mStep[0m  [16/42], [94mLoss[0m : 2.47769
[1mStep[0m  [20/42], [94mLoss[0m : 2.38842
[1mStep[0m  [24/42], [94mLoss[0m : 2.31275
[1mStep[0m  [28/42], [94mLoss[0m : 2.50707
[1mStep[0m  [32/42], [94mLoss[0m : 2.59411
[1mStep[0m  [36/42], [94mLoss[0m : 2.43464
[1mStep[0m  [40/42], [94mLoss[0m : 2.40806

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.536, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37681
[1mStep[0m  [4/42], [94mLoss[0m : 2.29095
[1mStep[0m  [8/42], [94mLoss[0m : 2.55097
[1mStep[0m  [12/42], [94mLoss[0m : 2.48278
[1mStep[0m  [16/42], [94mLoss[0m : 2.37711
[1mStep[0m  [20/42], [94mLoss[0m : 2.45412
[1mStep[0m  [24/42], [94mLoss[0m : 2.49551
[1mStep[0m  [28/42], [94mLoss[0m : 2.47676
[1mStep[0m  [32/42], [94mLoss[0m : 2.46667
[1mStep[0m  [36/42], [94mLoss[0m : 2.33907
[1mStep[0m  [40/42], [94mLoss[0m : 2.39083

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.536, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40496
[1mStep[0m  [4/42], [94mLoss[0m : 2.47736
[1mStep[0m  [8/42], [94mLoss[0m : 2.57337
[1mStep[0m  [12/42], [94mLoss[0m : 2.66647
[1mStep[0m  [16/42], [94mLoss[0m : 2.41529
[1mStep[0m  [20/42], [94mLoss[0m : 2.52079
[1mStep[0m  [24/42], [94mLoss[0m : 2.44677
[1mStep[0m  [28/42], [94mLoss[0m : 2.57740
[1mStep[0m  [32/42], [94mLoss[0m : 2.34041
[1mStep[0m  [36/42], [94mLoss[0m : 2.53422
[1mStep[0m  [40/42], [94mLoss[0m : 2.34084

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.526, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53142
[1mStep[0m  [4/42], [94mLoss[0m : 2.32028
[1mStep[0m  [8/42], [94mLoss[0m : 2.34808
[1mStep[0m  [12/42], [94mLoss[0m : 2.47497
[1mStep[0m  [16/42], [94mLoss[0m : 2.36388
[1mStep[0m  [20/42], [94mLoss[0m : 2.34985
[1mStep[0m  [24/42], [94mLoss[0m : 2.45597
[1mStep[0m  [28/42], [94mLoss[0m : 2.51360
[1mStep[0m  [32/42], [94mLoss[0m : 2.60826
[1mStep[0m  [36/42], [94mLoss[0m : 2.60133
[1mStep[0m  [40/42], [94mLoss[0m : 2.39066

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.528, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56370
[1mStep[0m  [4/42], [94mLoss[0m : 2.44318
[1mStep[0m  [8/42], [94mLoss[0m : 2.43561
[1mStep[0m  [12/42], [94mLoss[0m : 2.49182
[1mStep[0m  [16/42], [94mLoss[0m : 2.19448
[1mStep[0m  [20/42], [94mLoss[0m : 2.64142
[1mStep[0m  [24/42], [94mLoss[0m : 2.59937
[1mStep[0m  [28/42], [94mLoss[0m : 2.48121
[1mStep[0m  [32/42], [94mLoss[0m : 2.65456
[1mStep[0m  [36/42], [94mLoss[0m : 2.42680
[1mStep[0m  [40/42], [94mLoss[0m : 2.53263

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.536, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39041
[1mStep[0m  [4/42], [94mLoss[0m : 2.33573
[1mStep[0m  [8/42], [94mLoss[0m : 2.32075
[1mStep[0m  [12/42], [94mLoss[0m : 2.57097
[1mStep[0m  [16/42], [94mLoss[0m : 2.63792
[1mStep[0m  [20/42], [94mLoss[0m : 2.29564
[1mStep[0m  [24/42], [94mLoss[0m : 2.33988
[1mStep[0m  [28/42], [94mLoss[0m : 2.44195
[1mStep[0m  [32/42], [94mLoss[0m : 2.34584
[1mStep[0m  [36/42], [94mLoss[0m : 2.36075
[1mStep[0m  [40/42], [94mLoss[0m : 2.33702

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.512, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.19919
[1mStep[0m  [4/42], [94mLoss[0m : 2.32163
[1mStep[0m  [8/42], [94mLoss[0m : 2.47819
[1mStep[0m  [12/42], [94mLoss[0m : 2.66715
[1mStep[0m  [16/42], [94mLoss[0m : 2.71689
[1mStep[0m  [20/42], [94mLoss[0m : 2.21586
[1mStep[0m  [24/42], [94mLoss[0m : 2.57094
[1mStep[0m  [28/42], [94mLoss[0m : 2.16655
[1mStep[0m  [32/42], [94mLoss[0m : 2.45837
[1mStep[0m  [36/42], [94mLoss[0m : 2.37086
[1mStep[0m  [40/42], [94mLoss[0m : 2.43906

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.528, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41832
[1mStep[0m  [4/42], [94mLoss[0m : 2.58782
[1mStep[0m  [8/42], [94mLoss[0m : 2.39189
[1mStep[0m  [12/42], [94mLoss[0m : 2.18977
[1mStep[0m  [16/42], [94mLoss[0m : 2.66718
[1mStep[0m  [20/42], [94mLoss[0m : 2.47029
[1mStep[0m  [24/42], [94mLoss[0m : 2.49139
[1mStep[0m  [28/42], [94mLoss[0m : 2.51393
[1mStep[0m  [32/42], [94mLoss[0m : 2.62734
[1mStep[0m  [36/42], [94mLoss[0m : 2.43381
[1mStep[0m  [40/42], [94mLoss[0m : 2.51628

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.507, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49032
[1mStep[0m  [4/42], [94mLoss[0m : 2.52343
[1mStep[0m  [8/42], [94mLoss[0m : 2.25852
[1mStep[0m  [12/42], [94mLoss[0m : 2.32996
[1mStep[0m  [16/42], [94mLoss[0m : 2.41874
[1mStep[0m  [20/42], [94mLoss[0m : 2.38629
[1mStep[0m  [24/42], [94mLoss[0m : 2.54281
[1mStep[0m  [28/42], [94mLoss[0m : 2.44515
[1mStep[0m  [32/42], [94mLoss[0m : 2.51175
[1mStep[0m  [36/42], [94mLoss[0m : 2.47979
[1mStep[0m  [40/42], [94mLoss[0m : 2.37810

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.506, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33991
[1mStep[0m  [4/42], [94mLoss[0m : 2.14192
[1mStep[0m  [8/42], [94mLoss[0m : 2.44871
[1mStep[0m  [12/42], [94mLoss[0m : 2.62930
[1mStep[0m  [16/42], [94mLoss[0m : 2.51602
[1mStep[0m  [20/42], [94mLoss[0m : 2.25409
[1mStep[0m  [24/42], [94mLoss[0m : 2.51085
[1mStep[0m  [28/42], [94mLoss[0m : 2.40536
[1mStep[0m  [32/42], [94mLoss[0m : 2.51159
[1mStep[0m  [36/42], [94mLoss[0m : 2.53454
[1mStep[0m  [40/42], [94mLoss[0m : 2.46720

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.523, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52975
[1mStep[0m  [4/42], [94mLoss[0m : 2.63194
[1mStep[0m  [8/42], [94mLoss[0m : 2.45718
[1mStep[0m  [12/42], [94mLoss[0m : 2.47249
[1mStep[0m  [16/42], [94mLoss[0m : 2.37774
[1mStep[0m  [20/42], [94mLoss[0m : 2.45901
[1mStep[0m  [24/42], [94mLoss[0m : 2.57256
[1mStep[0m  [28/42], [94mLoss[0m : 2.50732
[1mStep[0m  [32/42], [94mLoss[0m : 2.48710
[1mStep[0m  [36/42], [94mLoss[0m : 2.33869
[1mStep[0m  [40/42], [94mLoss[0m : 2.35127

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.518, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55211
[1mStep[0m  [4/42], [94mLoss[0m : 2.35530
[1mStep[0m  [8/42], [94mLoss[0m : 2.29065
[1mStep[0m  [12/42], [94mLoss[0m : 2.50902
[1mStep[0m  [16/42], [94mLoss[0m : 2.30210
[1mStep[0m  [20/42], [94mLoss[0m : 2.15159
[1mStep[0m  [24/42], [94mLoss[0m : 2.51077
[1mStep[0m  [28/42], [94mLoss[0m : 2.42670
[1mStep[0m  [32/42], [94mLoss[0m : 2.63422
[1mStep[0m  [36/42], [94mLoss[0m : 2.45715
[1mStep[0m  [40/42], [94mLoss[0m : 2.32753

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.499, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53124
[1mStep[0m  [4/42], [94mLoss[0m : 2.47209
[1mStep[0m  [8/42], [94mLoss[0m : 2.22084
[1mStep[0m  [12/42], [94mLoss[0m : 2.45942
[1mStep[0m  [16/42], [94mLoss[0m : 2.31211
[1mStep[0m  [20/42], [94mLoss[0m : 2.48478
[1mStep[0m  [24/42], [94mLoss[0m : 2.36420
[1mStep[0m  [28/42], [94mLoss[0m : 2.53693
[1mStep[0m  [32/42], [94mLoss[0m : 2.44875
[1mStep[0m  [36/42], [94mLoss[0m : 2.37126
[1mStep[0m  [40/42], [94mLoss[0m : 2.30834

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.508, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48792
[1mStep[0m  [4/42], [94mLoss[0m : 2.35659
[1mStep[0m  [8/42], [94mLoss[0m : 2.76580
[1mStep[0m  [12/42], [94mLoss[0m : 2.36901
[1mStep[0m  [16/42], [94mLoss[0m : 2.56123
[1mStep[0m  [20/42], [94mLoss[0m : 2.55544
[1mStep[0m  [24/42], [94mLoss[0m : 2.54871
[1mStep[0m  [28/42], [94mLoss[0m : 2.50888
[1mStep[0m  [32/42], [94mLoss[0m : 2.15494
[1mStep[0m  [36/42], [94mLoss[0m : 2.57442
[1mStep[0m  [40/42], [94mLoss[0m : 2.17973

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.508, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34349
[1mStep[0m  [4/42], [94mLoss[0m : 2.41158
[1mStep[0m  [8/42], [94mLoss[0m : 2.44399
[1mStep[0m  [12/42], [94mLoss[0m : 2.43608
[1mStep[0m  [16/42], [94mLoss[0m : 2.62481
[1mStep[0m  [20/42], [94mLoss[0m : 2.45105
[1mStep[0m  [24/42], [94mLoss[0m : 2.30144
[1mStep[0m  [28/42], [94mLoss[0m : 2.38046
[1mStep[0m  [32/42], [94mLoss[0m : 2.43393
[1mStep[0m  [36/42], [94mLoss[0m : 2.33799
[1mStep[0m  [40/42], [94mLoss[0m : 2.47073

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.501, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40753
[1mStep[0m  [4/42], [94mLoss[0m : 2.27108
[1mStep[0m  [8/42], [94mLoss[0m : 2.40229
[1mStep[0m  [12/42], [94mLoss[0m : 2.49533
[1mStep[0m  [16/42], [94mLoss[0m : 2.30712
[1mStep[0m  [20/42], [94mLoss[0m : 2.36304
[1mStep[0m  [24/42], [94mLoss[0m : 2.50954
[1mStep[0m  [28/42], [94mLoss[0m : 2.31376
[1mStep[0m  [32/42], [94mLoss[0m : 2.44798
[1mStep[0m  [36/42], [94mLoss[0m : 2.45994
[1mStep[0m  [40/42], [94mLoss[0m : 2.63566

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.486, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38207
[1mStep[0m  [4/42], [94mLoss[0m : 2.31218
[1mStep[0m  [8/42], [94mLoss[0m : 2.45289
[1mStep[0m  [12/42], [94mLoss[0m : 2.56924
[1mStep[0m  [16/42], [94mLoss[0m : 2.35868
[1mStep[0m  [20/42], [94mLoss[0m : 2.47332
[1mStep[0m  [24/42], [94mLoss[0m : 2.37501
[1mStep[0m  [28/42], [94mLoss[0m : 2.56849
[1mStep[0m  [32/42], [94mLoss[0m : 2.37350
[1mStep[0m  [36/42], [94mLoss[0m : 2.55778
[1mStep[0m  [40/42], [94mLoss[0m : 2.34248

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.486, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.496
====================================

Phase 1 - Evaluation MAE:  2.495590771947588
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 2.38283
[1mStep[0m  [4/42], [94mLoss[0m : 2.61327
[1mStep[0m  [8/42], [94mLoss[0m : 2.30151
[1mStep[0m  [12/42], [94mLoss[0m : 2.44302
[1mStep[0m  [16/42], [94mLoss[0m : 2.51936
[1mStep[0m  [20/42], [94mLoss[0m : 2.70536
[1mStep[0m  [24/42], [94mLoss[0m : 2.43771
[1mStep[0m  [28/42], [94mLoss[0m : 2.58866
[1mStep[0m  [32/42], [94mLoss[0m : 2.35447
[1mStep[0m  [36/42], [94mLoss[0m : 2.43867
[1mStep[0m  [40/42], [94mLoss[0m : 2.46440

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.500, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52740
[1mStep[0m  [4/42], [94mLoss[0m : 2.34762
[1mStep[0m  [8/42], [94mLoss[0m : 2.21582
[1mStep[0m  [12/42], [94mLoss[0m : 2.56119
[1mStep[0m  [16/42], [94mLoss[0m : 2.44644
[1mStep[0m  [20/42], [94mLoss[0m : 2.42351
[1mStep[0m  [24/42], [94mLoss[0m : 2.51956
[1mStep[0m  [28/42], [94mLoss[0m : 2.57937
[1mStep[0m  [32/42], [94mLoss[0m : 2.38904
[1mStep[0m  [36/42], [94mLoss[0m : 2.44570
[1mStep[0m  [40/42], [94mLoss[0m : 2.36844

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.615, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57250
[1mStep[0m  [4/42], [94mLoss[0m : 2.34189
[1mStep[0m  [8/42], [94mLoss[0m : 2.48675
[1mStep[0m  [12/42], [94mLoss[0m : 2.32479
[1mStep[0m  [16/42], [94mLoss[0m : 2.62440
[1mStep[0m  [20/42], [94mLoss[0m : 2.26750
[1mStep[0m  [24/42], [94mLoss[0m : 2.20142
[1mStep[0m  [28/42], [94mLoss[0m : 2.52155
[1mStep[0m  [32/42], [94mLoss[0m : 2.25442
[1mStep[0m  [36/42], [94mLoss[0m : 2.23967
[1mStep[0m  [40/42], [94mLoss[0m : 2.42921

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.396, [92mTest[0m: 2.405, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24984
[1mStep[0m  [4/42], [94mLoss[0m : 2.41410
[1mStep[0m  [8/42], [94mLoss[0m : 2.30420
[1mStep[0m  [12/42], [94mLoss[0m : 2.36175
[1mStep[0m  [16/42], [94mLoss[0m : 2.37039
[1mStep[0m  [20/42], [94mLoss[0m : 2.55908
[1mStep[0m  [24/42], [94mLoss[0m : 2.16991
[1mStep[0m  [28/42], [94mLoss[0m : 2.48250
[1mStep[0m  [32/42], [94mLoss[0m : 2.28271
[1mStep[0m  [36/42], [94mLoss[0m : 2.26164
[1mStep[0m  [40/42], [94mLoss[0m : 2.58901

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.381, [92mTest[0m: 2.372, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47431
[1mStep[0m  [4/42], [94mLoss[0m : 2.38028
[1mStep[0m  [8/42], [94mLoss[0m : 2.24507
[1mStep[0m  [12/42], [94mLoss[0m : 2.22006
[1mStep[0m  [16/42], [94mLoss[0m : 2.20591
[1mStep[0m  [20/42], [94mLoss[0m : 2.31845
[1mStep[0m  [24/42], [94mLoss[0m : 2.38307
[1mStep[0m  [28/42], [94mLoss[0m : 2.23152
[1mStep[0m  [32/42], [94mLoss[0m : 2.44318
[1mStep[0m  [36/42], [94mLoss[0m : 2.29595
[1mStep[0m  [40/42], [94mLoss[0m : 2.40748

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.359, [92mTest[0m: 2.373, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31203
[1mStep[0m  [4/42], [94mLoss[0m : 2.20382
[1mStep[0m  [8/42], [94mLoss[0m : 2.15940
[1mStep[0m  [12/42], [94mLoss[0m : 2.37142
[1mStep[0m  [16/42], [94mLoss[0m : 2.22330
[1mStep[0m  [20/42], [94mLoss[0m : 2.25730
[1mStep[0m  [24/42], [94mLoss[0m : 2.24311
[1mStep[0m  [28/42], [94mLoss[0m : 2.19694
[1mStep[0m  [32/42], [94mLoss[0m : 2.45019
[1mStep[0m  [36/42], [94mLoss[0m : 2.53882
[1mStep[0m  [40/42], [94mLoss[0m : 2.17528

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.327, [92mTest[0m: 2.394, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.17264
[1mStep[0m  [4/42], [94mLoss[0m : 2.26530
[1mStep[0m  [8/42], [94mLoss[0m : 2.61099
[1mStep[0m  [12/42], [94mLoss[0m : 2.13864
[1mStep[0m  [16/42], [94mLoss[0m : 2.43100
[1mStep[0m  [20/42], [94mLoss[0m : 2.34484
[1mStep[0m  [24/42], [94mLoss[0m : 2.18618
[1mStep[0m  [28/42], [94mLoss[0m : 2.38695
[1mStep[0m  [32/42], [94mLoss[0m : 2.54129
[1mStep[0m  [36/42], [94mLoss[0m : 2.16212
[1mStep[0m  [40/42], [94mLoss[0m : 2.15251

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.308, [92mTest[0m: 2.398, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.22824
[1mStep[0m  [4/42], [94mLoss[0m : 2.26900
[1mStep[0m  [8/42], [94mLoss[0m : 2.27418
[1mStep[0m  [12/42], [94mLoss[0m : 2.10484
[1mStep[0m  [16/42], [94mLoss[0m : 2.43657
[1mStep[0m  [20/42], [94mLoss[0m : 2.25202
[1mStep[0m  [24/42], [94mLoss[0m : 2.19946
[1mStep[0m  [28/42], [94mLoss[0m : 2.21747
[1mStep[0m  [32/42], [94mLoss[0m : 2.22652
[1mStep[0m  [36/42], [94mLoss[0m : 2.21139
[1mStep[0m  [40/42], [94mLoss[0m : 2.26099

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.276, [92mTest[0m: 2.374, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.30160
[1mStep[0m  [4/42], [94mLoss[0m : 2.09379
[1mStep[0m  [8/42], [94mLoss[0m : 2.37990
[1mStep[0m  [12/42], [94mLoss[0m : 2.27431
[1mStep[0m  [16/42], [94mLoss[0m : 2.28928
[1mStep[0m  [20/42], [94mLoss[0m : 2.28094
[1mStep[0m  [24/42], [94mLoss[0m : 2.18071
[1mStep[0m  [28/42], [94mLoss[0m : 2.21855
[1mStep[0m  [32/42], [94mLoss[0m : 2.31482
[1mStep[0m  [36/42], [94mLoss[0m : 2.12750
[1mStep[0m  [40/42], [94mLoss[0m : 2.45082

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.245, [92mTest[0m: 2.400, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.19146
[1mStep[0m  [4/42], [94mLoss[0m : 2.18048
[1mStep[0m  [8/42], [94mLoss[0m : 2.35692
[1mStep[0m  [12/42], [94mLoss[0m : 2.29056
[1mStep[0m  [16/42], [94mLoss[0m : 2.16218
[1mStep[0m  [20/42], [94mLoss[0m : 2.21192
[1mStep[0m  [24/42], [94mLoss[0m : 2.26147
[1mStep[0m  [28/42], [94mLoss[0m : 2.25091
[1mStep[0m  [32/42], [94mLoss[0m : 2.10169
[1mStep[0m  [36/42], [94mLoss[0m : 2.15116
[1mStep[0m  [40/42], [94mLoss[0m : 2.35293

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.199, [92mTest[0m: 2.391, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47115
[1mStep[0m  [4/42], [94mLoss[0m : 2.12772
[1mStep[0m  [8/42], [94mLoss[0m : 2.15739
[1mStep[0m  [12/42], [94mLoss[0m : 2.18829
[1mStep[0m  [16/42], [94mLoss[0m : 2.34704
[1mStep[0m  [20/42], [94mLoss[0m : 2.19920
[1mStep[0m  [24/42], [94mLoss[0m : 2.25788
[1mStep[0m  [28/42], [94mLoss[0m : 2.01254
[1mStep[0m  [32/42], [94mLoss[0m : 2.27883
[1mStep[0m  [36/42], [94mLoss[0m : 2.23051
[1mStep[0m  [40/42], [94mLoss[0m : 2.42365

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.201, [92mTest[0m: 2.413, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.99802
[1mStep[0m  [4/42], [94mLoss[0m : 2.12977
[1mStep[0m  [8/42], [94mLoss[0m : 2.19210
[1mStep[0m  [12/42], [94mLoss[0m : 2.17450
[1mStep[0m  [16/42], [94mLoss[0m : 2.35426
[1mStep[0m  [20/42], [94mLoss[0m : 2.27329
[1mStep[0m  [24/42], [94mLoss[0m : 2.16169
[1mStep[0m  [28/42], [94mLoss[0m : 2.32155
[1mStep[0m  [32/42], [94mLoss[0m : 2.25436
[1mStep[0m  [36/42], [94mLoss[0m : 1.93432
[1mStep[0m  [40/42], [94mLoss[0m : 2.37611

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.169, [92mTest[0m: 2.409, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34822
[1mStep[0m  [4/42], [94mLoss[0m : 1.97656
[1mStep[0m  [8/42], [94mLoss[0m : 2.00559
[1mStep[0m  [12/42], [94mLoss[0m : 2.21849
[1mStep[0m  [16/42], [94mLoss[0m : 2.22394
[1mStep[0m  [20/42], [94mLoss[0m : 1.97451
[1mStep[0m  [24/42], [94mLoss[0m : 2.05383
[1mStep[0m  [28/42], [94mLoss[0m : 2.21265
[1mStep[0m  [32/42], [94mLoss[0m : 2.12431
[1mStep[0m  [36/42], [94mLoss[0m : 2.17033
[1mStep[0m  [40/42], [94mLoss[0m : 2.38798

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.141, [92mTest[0m: 2.415, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.02293
[1mStep[0m  [4/42], [94mLoss[0m : 2.04253
[1mStep[0m  [8/42], [94mLoss[0m : 2.20264
[1mStep[0m  [12/42], [94mLoss[0m : 2.28107
[1mStep[0m  [16/42], [94mLoss[0m : 2.22005
[1mStep[0m  [20/42], [94mLoss[0m : 2.09475
[1mStep[0m  [24/42], [94mLoss[0m : 2.00729
[1mStep[0m  [28/42], [94mLoss[0m : 2.15567
[1mStep[0m  [32/42], [94mLoss[0m : 2.07667
[1mStep[0m  [36/42], [94mLoss[0m : 1.92717
[1mStep[0m  [40/42], [94mLoss[0m : 2.30603

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.125, [92mTest[0m: 2.403, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.04451
[1mStep[0m  [4/42], [94mLoss[0m : 2.01813
[1mStep[0m  [8/42], [94mLoss[0m : 1.99635
[1mStep[0m  [12/42], [94mLoss[0m : 1.92548
[1mStep[0m  [16/42], [94mLoss[0m : 1.94942
[1mStep[0m  [20/42], [94mLoss[0m : 1.89724
[1mStep[0m  [24/42], [94mLoss[0m : 2.01552
[1mStep[0m  [28/42], [94mLoss[0m : 2.12020
[1mStep[0m  [32/42], [94mLoss[0m : 2.16831
[1mStep[0m  [36/42], [94mLoss[0m : 2.06724
[1mStep[0m  [40/42], [94mLoss[0m : 2.00906

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.053, [92mTest[0m: 2.435, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.00446
[1mStep[0m  [4/42], [94mLoss[0m : 1.71955
[1mStep[0m  [8/42], [94mLoss[0m : 2.17955
[1mStep[0m  [12/42], [94mLoss[0m : 1.92873
[1mStep[0m  [16/42], [94mLoss[0m : 1.92623
[1mStep[0m  [20/42], [94mLoss[0m : 2.13274
[1mStep[0m  [24/42], [94mLoss[0m : 2.15730
[1mStep[0m  [28/42], [94mLoss[0m : 2.04357
[1mStep[0m  [32/42], [94mLoss[0m : 2.17945
[1mStep[0m  [36/42], [94mLoss[0m : 2.16936
[1mStep[0m  [40/42], [94mLoss[0m : 2.13278

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.039, [92mTest[0m: 2.402, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.88030
[1mStep[0m  [4/42], [94mLoss[0m : 2.00902
[1mStep[0m  [8/42], [94mLoss[0m : 2.12112
[1mStep[0m  [12/42], [94mLoss[0m : 1.92065
[1mStep[0m  [16/42], [94mLoss[0m : 1.94138
[1mStep[0m  [20/42], [94mLoss[0m : 2.25469
[1mStep[0m  [24/42], [94mLoss[0m : 1.85198
[1mStep[0m  [28/42], [94mLoss[0m : 1.98190
[1mStep[0m  [32/42], [94mLoss[0m : 1.90518
[1mStep[0m  [36/42], [94mLoss[0m : 1.94515
[1mStep[0m  [40/42], [94mLoss[0m : 1.99661

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.022, [92mTest[0m: 2.429, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.91292
[1mStep[0m  [4/42], [94mLoss[0m : 1.97830
[1mStep[0m  [8/42], [94mLoss[0m : 1.84067
[1mStep[0m  [12/42], [94mLoss[0m : 1.87540
[1mStep[0m  [16/42], [94mLoss[0m : 1.79763
[1mStep[0m  [20/42], [94mLoss[0m : 2.12358
[1mStep[0m  [24/42], [94mLoss[0m : 2.05159
[1mStep[0m  [28/42], [94mLoss[0m : 1.94749
[1mStep[0m  [32/42], [94mLoss[0m : 2.00261
[1mStep[0m  [36/42], [94mLoss[0m : 2.07551
[1mStep[0m  [40/42], [94mLoss[0m : 2.25523

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.999, [92mTest[0m: 2.391, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.00835
[1mStep[0m  [4/42], [94mLoss[0m : 2.21353
[1mStep[0m  [8/42], [94mLoss[0m : 1.98822
[1mStep[0m  [12/42], [94mLoss[0m : 1.86196
[1mStep[0m  [16/42], [94mLoss[0m : 1.84090
[1mStep[0m  [20/42], [94mLoss[0m : 1.73306
[1mStep[0m  [24/42], [94mLoss[0m : 1.82783
[1mStep[0m  [28/42], [94mLoss[0m : 1.86837
[1mStep[0m  [32/42], [94mLoss[0m : 2.01205
[1mStep[0m  [36/42], [94mLoss[0m : 1.91792
[1mStep[0m  [40/42], [94mLoss[0m : 2.19308

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.962, [92mTest[0m: 2.457, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.85522
[1mStep[0m  [4/42], [94mLoss[0m : 1.85540
[1mStep[0m  [8/42], [94mLoss[0m : 1.72573
[1mStep[0m  [12/42], [94mLoss[0m : 1.82999
[1mStep[0m  [16/42], [94mLoss[0m : 1.79053
[1mStep[0m  [20/42], [94mLoss[0m : 1.96970
[1mStep[0m  [24/42], [94mLoss[0m : 2.11043
[1mStep[0m  [28/42], [94mLoss[0m : 2.28782
[1mStep[0m  [32/42], [94mLoss[0m : 2.05558
[1mStep[0m  [36/42], [94mLoss[0m : 1.86583
[1mStep[0m  [40/42], [94mLoss[0m : 1.97420

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.937, [92mTest[0m: 2.444, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.78000
[1mStep[0m  [4/42], [94mLoss[0m : 1.92799
[1mStep[0m  [8/42], [94mLoss[0m : 1.89879
[1mStep[0m  [12/42], [94mLoss[0m : 2.02806
[1mStep[0m  [16/42], [94mLoss[0m : 1.86168
[1mStep[0m  [20/42], [94mLoss[0m : 1.80768
[1mStep[0m  [24/42], [94mLoss[0m : 1.80540
[1mStep[0m  [28/42], [94mLoss[0m : 1.91961
[1mStep[0m  [32/42], [94mLoss[0m : 2.03504
[1mStep[0m  [36/42], [94mLoss[0m : 1.89334
[1mStep[0m  [40/42], [94mLoss[0m : 2.08088

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.916, [92mTest[0m: 2.474, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.99309
[1mStep[0m  [4/42], [94mLoss[0m : 1.86751
[1mStep[0m  [8/42], [94mLoss[0m : 1.85276
[1mStep[0m  [12/42], [94mLoss[0m : 1.90844
[1mStep[0m  [16/42], [94mLoss[0m : 1.69231
[1mStep[0m  [20/42], [94mLoss[0m : 1.86297
[1mStep[0m  [24/42], [94mLoss[0m : 1.99779
[1mStep[0m  [28/42], [94mLoss[0m : 1.89972
[1mStep[0m  [32/42], [94mLoss[0m : 1.92974
[1mStep[0m  [36/42], [94mLoss[0m : 1.94073
[1mStep[0m  [40/42], [94mLoss[0m : 1.96684

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.894, [92mTest[0m: 2.517, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.98711
[1mStep[0m  [4/42], [94mLoss[0m : 1.82875
[1mStep[0m  [8/42], [94mLoss[0m : 2.11327
[1mStep[0m  [12/42], [94mLoss[0m : 1.95754
[1mStep[0m  [16/42], [94mLoss[0m : 1.69109
[1mStep[0m  [20/42], [94mLoss[0m : 1.90035
[1mStep[0m  [24/42], [94mLoss[0m : 1.95211
[1mStep[0m  [28/42], [94mLoss[0m : 1.76269
[1mStep[0m  [32/42], [94mLoss[0m : 1.72877
[1mStep[0m  [36/42], [94mLoss[0m : 1.70235
[1mStep[0m  [40/42], [94mLoss[0m : 1.90345

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.840, [92mTest[0m: 2.436, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.85579
[1mStep[0m  [4/42], [94mLoss[0m : 2.04827
[1mStep[0m  [8/42], [94mLoss[0m : 1.82536
[1mStep[0m  [12/42], [94mLoss[0m : 1.90314
[1mStep[0m  [16/42], [94mLoss[0m : 1.82493
[1mStep[0m  [20/42], [94mLoss[0m : 1.77635
[1mStep[0m  [24/42], [94mLoss[0m : 1.84508
[1mStep[0m  [28/42], [94mLoss[0m : 1.87231
[1mStep[0m  [32/42], [94mLoss[0m : 1.76745
[1mStep[0m  [36/42], [94mLoss[0m : 1.93128
[1mStep[0m  [40/42], [94mLoss[0m : 1.88167

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.809, [92mTest[0m: 2.443, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.78281
[1mStep[0m  [4/42], [94mLoss[0m : 1.88109
[1mStep[0m  [8/42], [94mLoss[0m : 1.83870
[1mStep[0m  [12/42], [94mLoss[0m : 1.76870
[1mStep[0m  [16/42], [94mLoss[0m : 2.01042
[1mStep[0m  [20/42], [94mLoss[0m : 1.80440
[1mStep[0m  [24/42], [94mLoss[0m : 1.86747
[1mStep[0m  [28/42], [94mLoss[0m : 1.77990
[1mStep[0m  [32/42], [94mLoss[0m : 1.74068
[1mStep[0m  [36/42], [94mLoss[0m : 1.77536
[1mStep[0m  [40/42], [94mLoss[0m : 1.92764

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.801, [92mTest[0m: 2.470, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.88374
[1mStep[0m  [4/42], [94mLoss[0m : 1.68747
[1mStep[0m  [8/42], [94mLoss[0m : 1.66344
[1mStep[0m  [12/42], [94mLoss[0m : 1.87957
[1mStep[0m  [16/42], [94mLoss[0m : 1.92179
[1mStep[0m  [20/42], [94mLoss[0m : 1.82175
[1mStep[0m  [24/42], [94mLoss[0m : 1.72668
[1mStep[0m  [28/42], [94mLoss[0m : 1.75765
[1mStep[0m  [32/42], [94mLoss[0m : 1.78879
[1mStep[0m  [36/42], [94mLoss[0m : 1.74174
[1mStep[0m  [40/42], [94mLoss[0m : 1.73366

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.771, [92mTest[0m: 2.421, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.75836
[1mStep[0m  [4/42], [94mLoss[0m : 1.67374
[1mStep[0m  [8/42], [94mLoss[0m : 1.75088
[1mStep[0m  [12/42], [94mLoss[0m : 1.83594
[1mStep[0m  [16/42], [94mLoss[0m : 1.71692
[1mStep[0m  [20/42], [94mLoss[0m : 1.88733
[1mStep[0m  [24/42], [94mLoss[0m : 1.77782
[1mStep[0m  [28/42], [94mLoss[0m : 1.58009
[1mStep[0m  [32/42], [94mLoss[0m : 1.80430
[1mStep[0m  [36/42], [94mLoss[0m : 1.60042
[1mStep[0m  [40/42], [94mLoss[0m : 1.73160

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.768, [92mTest[0m: 2.469, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.72915
[1mStep[0m  [4/42], [94mLoss[0m : 1.62059
[1mStep[0m  [8/42], [94mLoss[0m : 1.80430
[1mStep[0m  [12/42], [94mLoss[0m : 1.83931
[1mStep[0m  [16/42], [94mLoss[0m : 1.62102
[1mStep[0m  [20/42], [94mLoss[0m : 1.82510
[1mStep[0m  [24/42], [94mLoss[0m : 1.74343
[1mStep[0m  [28/42], [94mLoss[0m : 1.93883
[1mStep[0m  [32/42], [94mLoss[0m : 1.82051
[1mStep[0m  [36/42], [94mLoss[0m : 1.79669
[1mStep[0m  [40/42], [94mLoss[0m : 1.72716

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.731, [92mTest[0m: 2.453, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.71317
[1mStep[0m  [4/42], [94mLoss[0m : 1.66713
[1mStep[0m  [8/42], [94mLoss[0m : 1.50379
[1mStep[0m  [12/42], [94mLoss[0m : 1.80855
[1mStep[0m  [16/42], [94mLoss[0m : 1.75181
[1mStep[0m  [20/42], [94mLoss[0m : 1.60107
[1mStep[0m  [24/42], [94mLoss[0m : 1.75068
[1mStep[0m  [28/42], [94mLoss[0m : 1.86507
[1mStep[0m  [32/42], [94mLoss[0m : 1.87502
[1mStep[0m  [36/42], [94mLoss[0m : 1.86131
[1mStep[0m  [40/42], [94mLoss[0m : 1.65776

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.700, [92mTest[0m: 2.481, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.69492
[1mStep[0m  [4/42], [94mLoss[0m : 1.67018
[1mStep[0m  [8/42], [94mLoss[0m : 1.57137
[1mStep[0m  [12/42], [94mLoss[0m : 1.64407
[1mStep[0m  [16/42], [94mLoss[0m : 1.54488
[1mStep[0m  [20/42], [94mLoss[0m : 1.81975
[1mStep[0m  [24/42], [94mLoss[0m : 1.51522
[1mStep[0m  [28/42], [94mLoss[0m : 1.82817
[1mStep[0m  [32/42], [94mLoss[0m : 1.60271
[1mStep[0m  [36/42], [94mLoss[0m : 1.75343
[1mStep[0m  [40/42], [94mLoss[0m : 1.88161

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.711, [92mTest[0m: 2.503, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.535
====================================

Phase 2 - Evaluation MAE:  2.5350167751312256
MAE score P1      2.495591
MAE score P2      2.535017
loss              1.700245
learning_rate     0.002575
batch_size             256
hidden_sizes         [250]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.5
weight_decay          0.01
Name: 21, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 10.97498
[1mStep[0m  [4/42], [94mLoss[0m : 11.03935
[1mStep[0m  [8/42], [94mLoss[0m : 10.82578
[1mStep[0m  [12/42], [94mLoss[0m : 11.01492
[1mStep[0m  [16/42], [94mLoss[0m : 10.75852
[1mStep[0m  [20/42], [94mLoss[0m : 10.39272
[1mStep[0m  [24/42], [94mLoss[0m : 10.07394
[1mStep[0m  [28/42], [94mLoss[0m : 9.83621
[1mStep[0m  [32/42], [94mLoss[0m : 9.74822
[1mStep[0m  [36/42], [94mLoss[0m : 9.63639
[1mStep[0m  [40/42], [94mLoss[0m : 9.75573

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.374, [92mTest[0m: 11.004, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.31358
[1mStep[0m  [4/42], [94mLoss[0m : 9.48692
[1mStep[0m  [8/42], [94mLoss[0m : 9.01643
[1mStep[0m  [12/42], [94mLoss[0m : 8.89198
[1mStep[0m  [16/42], [94mLoss[0m : 8.57916
[1mStep[0m  [20/42], [94mLoss[0m : 8.89978
[1mStep[0m  [24/42], [94mLoss[0m : 8.41321
[1mStep[0m  [28/42], [94mLoss[0m : 8.08929
[1mStep[0m  [32/42], [94mLoss[0m : 8.14363
[1mStep[0m  [36/42], [94mLoss[0m : 7.72443
[1mStep[0m  [40/42], [94mLoss[0m : 7.62989

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 8.471, [92mTest[0m: 9.945, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.55875
[1mStep[0m  [4/42], [94mLoss[0m : 7.54520
[1mStep[0m  [8/42], [94mLoss[0m : 7.29006
[1mStep[0m  [12/42], [94mLoss[0m : 6.79395
[1mStep[0m  [16/42], [94mLoss[0m : 6.86163
[1mStep[0m  [20/42], [94mLoss[0m : 6.31120
[1mStep[0m  [24/42], [94mLoss[0m : 6.14346
[1mStep[0m  [28/42], [94mLoss[0m : 6.05254
[1mStep[0m  [32/42], [94mLoss[0m : 5.96672
[1mStep[0m  [36/42], [94mLoss[0m : 5.39497
[1mStep[0m  [40/42], [94mLoss[0m : 5.27859

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 6.416, [92mTest[0m: 8.352, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.65436
[1mStep[0m  [4/42], [94mLoss[0m : 5.09818
[1mStep[0m  [8/42], [94mLoss[0m : 4.81128
[1mStep[0m  [12/42], [94mLoss[0m : 4.42829
[1mStep[0m  [16/42], [94mLoss[0m : 4.56768
[1mStep[0m  [20/42], [94mLoss[0m : 4.33097
[1mStep[0m  [24/42], [94mLoss[0m : 3.88436
[1mStep[0m  [28/42], [94mLoss[0m : 4.62530
[1mStep[0m  [32/42], [94mLoss[0m : 4.22946
[1mStep[0m  [36/42], [94mLoss[0m : 3.89457
[1mStep[0m  [40/42], [94mLoss[0m : 4.02737

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 4.511, [92mTest[0m: 6.510, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.75695
[1mStep[0m  [4/42], [94mLoss[0m : 3.73520
[1mStep[0m  [8/42], [94mLoss[0m : 3.92768
[1mStep[0m  [12/42], [94mLoss[0m : 3.50524
[1mStep[0m  [16/42], [94mLoss[0m : 3.63800
[1mStep[0m  [20/42], [94mLoss[0m : 3.53369
[1mStep[0m  [24/42], [94mLoss[0m : 3.37999
[1mStep[0m  [28/42], [94mLoss[0m : 3.25310
[1mStep[0m  [32/42], [94mLoss[0m : 2.79527
[1mStep[0m  [36/42], [94mLoss[0m : 3.03547
[1mStep[0m  [40/42], [94mLoss[0m : 2.80008

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 3.324, [92mTest[0m: 4.694, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.79177
[1mStep[0m  [4/42], [94mLoss[0m : 3.20409
[1mStep[0m  [8/42], [94mLoss[0m : 3.06676
[1mStep[0m  [12/42], [94mLoss[0m : 2.84818
[1mStep[0m  [16/42], [94mLoss[0m : 3.17103
[1mStep[0m  [20/42], [94mLoss[0m : 2.94132
[1mStep[0m  [24/42], [94mLoss[0m : 2.76771
[1mStep[0m  [28/42], [94mLoss[0m : 2.90000
[1mStep[0m  [32/42], [94mLoss[0m : 2.81173
[1mStep[0m  [36/42], [94mLoss[0m : 2.77683
[1mStep[0m  [40/42], [94mLoss[0m : 2.48241

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.892, [92mTest[0m: 3.454, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47893
[1mStep[0m  [4/42], [94mLoss[0m : 2.72947
[1mStep[0m  [8/42], [94mLoss[0m : 2.58552
[1mStep[0m  [12/42], [94mLoss[0m : 2.91308
[1mStep[0m  [16/42], [94mLoss[0m : 2.69430
[1mStep[0m  [20/42], [94mLoss[0m : 2.79996
[1mStep[0m  [24/42], [94mLoss[0m : 2.66994
[1mStep[0m  [28/42], [94mLoss[0m : 2.69301
[1mStep[0m  [32/42], [94mLoss[0m : 2.97398
[1mStep[0m  [36/42], [94mLoss[0m : 2.72784
[1mStep[0m  [40/42], [94mLoss[0m : 2.83160

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.757, [92mTest[0m: 2.951, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.80492
[1mStep[0m  [4/42], [94mLoss[0m : 2.66650
[1mStep[0m  [8/42], [94mLoss[0m : 2.70267
[1mStep[0m  [12/42], [94mLoss[0m : 2.71130
[1mStep[0m  [16/42], [94mLoss[0m : 2.59982
[1mStep[0m  [20/42], [94mLoss[0m : 2.95793
[1mStep[0m  [24/42], [94mLoss[0m : 2.82703
[1mStep[0m  [28/42], [94mLoss[0m : 2.88838
[1mStep[0m  [32/42], [94mLoss[0m : 2.58175
[1mStep[0m  [36/42], [94mLoss[0m : 2.65975
[1mStep[0m  [40/42], [94mLoss[0m : 2.62547

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.713, [92mTest[0m: 2.695, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.66239
[1mStep[0m  [4/42], [94mLoss[0m : 2.38133
[1mStep[0m  [8/42], [94mLoss[0m : 2.83205
[1mStep[0m  [12/42], [94mLoss[0m : 2.79201
[1mStep[0m  [16/42], [94mLoss[0m : 2.49210
[1mStep[0m  [20/42], [94mLoss[0m : 2.70488
[1mStep[0m  [24/42], [94mLoss[0m : 2.66441
[1mStep[0m  [28/42], [94mLoss[0m : 2.78275
[1mStep[0m  [32/42], [94mLoss[0m : 2.88401
[1mStep[0m  [36/42], [94mLoss[0m : 2.71724
[1mStep[0m  [40/42], [94mLoss[0m : 2.53897

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.674, [92mTest[0m: 2.599, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41419
[1mStep[0m  [4/42], [94mLoss[0m : 2.73111
[1mStep[0m  [8/42], [94mLoss[0m : 2.54548
[1mStep[0m  [12/42], [94mLoss[0m : 2.81618
[1mStep[0m  [16/42], [94mLoss[0m : 2.46969
[1mStep[0m  [20/42], [94mLoss[0m : 2.75486
[1mStep[0m  [24/42], [94mLoss[0m : 2.64035
[1mStep[0m  [28/42], [94mLoss[0m : 2.40972
[1mStep[0m  [32/42], [94mLoss[0m : 2.51120
[1mStep[0m  [36/42], [94mLoss[0m : 2.53667
[1mStep[0m  [40/42], [94mLoss[0m : 2.76062

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.660, [92mTest[0m: 2.561, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61172
[1mStep[0m  [4/42], [94mLoss[0m : 2.57964
[1mStep[0m  [8/42], [94mLoss[0m : 2.56342
[1mStep[0m  [12/42], [94mLoss[0m : 2.64251
[1mStep[0m  [16/42], [94mLoss[0m : 2.77321
[1mStep[0m  [20/42], [94mLoss[0m : 2.72984
[1mStep[0m  [24/42], [94mLoss[0m : 2.59675
[1mStep[0m  [28/42], [94mLoss[0m : 2.57635
[1mStep[0m  [32/42], [94mLoss[0m : 2.52287
[1mStep[0m  [36/42], [94mLoss[0m : 2.42626
[1mStep[0m  [40/42], [94mLoss[0m : 2.67743

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.638, [92mTest[0m: 2.502, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56258
[1mStep[0m  [4/42], [94mLoss[0m : 2.78406
[1mStep[0m  [8/42], [94mLoss[0m : 2.70473
[1mStep[0m  [12/42], [94mLoss[0m : 2.60344
[1mStep[0m  [16/42], [94mLoss[0m : 2.57662
[1mStep[0m  [20/42], [94mLoss[0m : 2.65121
[1mStep[0m  [24/42], [94mLoss[0m : 2.63833
[1mStep[0m  [28/42], [94mLoss[0m : 2.70103
[1mStep[0m  [32/42], [94mLoss[0m : 2.60595
[1mStep[0m  [36/42], [94mLoss[0m : 2.78504
[1mStep[0m  [40/42], [94mLoss[0m : 2.52537

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.631, [92mTest[0m: 2.499, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.74068
[1mStep[0m  [4/42], [94mLoss[0m : 2.56543
[1mStep[0m  [8/42], [94mLoss[0m : 2.52807
[1mStep[0m  [12/42], [94mLoss[0m : 2.58257
[1mStep[0m  [16/42], [94mLoss[0m : 2.52199
[1mStep[0m  [20/42], [94mLoss[0m : 2.67357
[1mStep[0m  [24/42], [94mLoss[0m : 2.72835
[1mStep[0m  [28/42], [94mLoss[0m : 2.74030
[1mStep[0m  [32/42], [94mLoss[0m : 2.44996
[1mStep[0m  [36/42], [94mLoss[0m : 2.80971
[1mStep[0m  [40/42], [94mLoss[0m : 2.72428

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.621, [92mTest[0m: 2.488, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61497
[1mStep[0m  [4/42], [94mLoss[0m : 2.48957
[1mStep[0m  [8/42], [94mLoss[0m : 2.57580
[1mStep[0m  [12/42], [94mLoss[0m : 2.80681
[1mStep[0m  [16/42], [94mLoss[0m : 2.49950
[1mStep[0m  [20/42], [94mLoss[0m : 2.72591
[1mStep[0m  [24/42], [94mLoss[0m : 2.61354
[1mStep[0m  [28/42], [94mLoss[0m : 2.71367
[1mStep[0m  [32/42], [94mLoss[0m : 2.53825
[1mStep[0m  [36/42], [94mLoss[0m : 2.72142
[1mStep[0m  [40/42], [94mLoss[0m : 2.45305

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.625, [92mTest[0m: 2.463, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38815
[1mStep[0m  [4/42], [94mLoss[0m : 2.64308
[1mStep[0m  [8/42], [94mLoss[0m : 2.56252
[1mStep[0m  [12/42], [94mLoss[0m : 2.79391
[1mStep[0m  [16/42], [94mLoss[0m : 2.45771
[1mStep[0m  [20/42], [94mLoss[0m : 2.61454
[1mStep[0m  [24/42], [94mLoss[0m : 2.64383
[1mStep[0m  [28/42], [94mLoss[0m : 2.66911
[1mStep[0m  [32/42], [94mLoss[0m : 2.73424
[1mStep[0m  [36/42], [94mLoss[0m : 2.76383
[1mStep[0m  [40/42], [94mLoss[0m : 2.69974

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.614, [92mTest[0m: 2.448, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57224
[1mStep[0m  [4/42], [94mLoss[0m : 2.45006
[1mStep[0m  [8/42], [94mLoss[0m : 2.52936
[1mStep[0m  [12/42], [94mLoss[0m : 2.59078
[1mStep[0m  [16/42], [94mLoss[0m : 2.69393
[1mStep[0m  [20/42], [94mLoss[0m : 2.60520
[1mStep[0m  [24/42], [94mLoss[0m : 2.75806
[1mStep[0m  [28/42], [94mLoss[0m : 2.68957
[1mStep[0m  [32/42], [94mLoss[0m : 2.56431
[1mStep[0m  [36/42], [94mLoss[0m : 2.73622
[1mStep[0m  [40/42], [94mLoss[0m : 2.62447

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.582, [92mTest[0m: 2.436, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58912
[1mStep[0m  [4/42], [94mLoss[0m : 2.38058
[1mStep[0m  [8/42], [94mLoss[0m : 2.48353
[1mStep[0m  [12/42], [94mLoss[0m : 2.41411
[1mStep[0m  [16/42], [94mLoss[0m : 2.59976
[1mStep[0m  [20/42], [94mLoss[0m : 2.62890
[1mStep[0m  [24/42], [94mLoss[0m : 2.58642
[1mStep[0m  [28/42], [94mLoss[0m : 2.50303
[1mStep[0m  [32/42], [94mLoss[0m : 2.52752
[1mStep[0m  [36/42], [94mLoss[0m : 2.78312
[1mStep[0m  [40/42], [94mLoss[0m : 2.69987

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.563, [92mTest[0m: 2.424, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.72356
[1mStep[0m  [4/42], [94mLoss[0m : 2.72081
[1mStep[0m  [8/42], [94mLoss[0m : 2.55274
[1mStep[0m  [12/42], [94mLoss[0m : 2.47994
[1mStep[0m  [16/42], [94mLoss[0m : 2.94607
[1mStep[0m  [20/42], [94mLoss[0m : 2.40527
[1mStep[0m  [24/42], [94mLoss[0m : 2.63019
[1mStep[0m  [28/42], [94mLoss[0m : 2.42682
[1mStep[0m  [32/42], [94mLoss[0m : 2.62105
[1mStep[0m  [36/42], [94mLoss[0m : 2.40595
[1mStep[0m  [40/42], [94mLoss[0m : 2.31444

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.593, [92mTest[0m: 2.399, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58487
[1mStep[0m  [4/42], [94mLoss[0m : 2.48773
[1mStep[0m  [8/42], [94mLoss[0m : 2.54471
[1mStep[0m  [12/42], [94mLoss[0m : 2.77380
[1mStep[0m  [16/42], [94mLoss[0m : 2.37763
[1mStep[0m  [20/42], [94mLoss[0m : 2.39909
[1mStep[0m  [24/42], [94mLoss[0m : 2.58986
[1mStep[0m  [28/42], [94mLoss[0m : 2.66046
[1mStep[0m  [32/42], [94mLoss[0m : 2.63955
[1mStep[0m  [36/42], [94mLoss[0m : 2.70772
[1mStep[0m  [40/42], [94mLoss[0m : 2.41122

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.565, [92mTest[0m: 2.415, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47534
[1mStep[0m  [4/42], [94mLoss[0m : 2.29946
[1mStep[0m  [8/42], [94mLoss[0m : 2.62062
[1mStep[0m  [12/42], [94mLoss[0m : 2.71471
[1mStep[0m  [16/42], [94mLoss[0m : 2.61088
[1mStep[0m  [20/42], [94mLoss[0m : 2.31891
[1mStep[0m  [24/42], [94mLoss[0m : 2.63825
[1mStep[0m  [28/42], [94mLoss[0m : 2.50080
[1mStep[0m  [32/42], [94mLoss[0m : 2.19297
[1mStep[0m  [36/42], [94mLoss[0m : 2.63749
[1mStep[0m  [40/42], [94mLoss[0m : 2.59269

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.574, [92mTest[0m: 2.391, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45013
[1mStep[0m  [4/42], [94mLoss[0m : 2.67630
[1mStep[0m  [8/42], [94mLoss[0m : 2.58126
[1mStep[0m  [12/42], [94mLoss[0m : 2.52499
[1mStep[0m  [16/42], [94mLoss[0m : 2.62699
[1mStep[0m  [20/42], [94mLoss[0m : 2.45292
[1mStep[0m  [24/42], [94mLoss[0m : 2.57583
[1mStep[0m  [28/42], [94mLoss[0m : 2.66003
[1mStep[0m  [32/42], [94mLoss[0m : 2.44595
[1mStep[0m  [36/42], [94mLoss[0m : 2.54836
[1mStep[0m  [40/42], [94mLoss[0m : 2.57737

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.544, [92mTest[0m: 2.390, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64498
[1mStep[0m  [4/42], [94mLoss[0m : 2.50421
[1mStep[0m  [8/42], [94mLoss[0m : 2.70303
[1mStep[0m  [12/42], [94mLoss[0m : 2.37265
[1mStep[0m  [16/42], [94mLoss[0m : 2.51958
[1mStep[0m  [20/42], [94mLoss[0m : 2.71236
[1mStep[0m  [24/42], [94mLoss[0m : 2.48092
[1mStep[0m  [28/42], [94mLoss[0m : 2.60862
[1mStep[0m  [32/42], [94mLoss[0m : 2.72424
[1mStep[0m  [36/42], [94mLoss[0m : 2.63259
[1mStep[0m  [40/42], [94mLoss[0m : 2.60051

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.588, [92mTest[0m: 2.387, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56518
[1mStep[0m  [4/42], [94mLoss[0m : 2.52285
[1mStep[0m  [8/42], [94mLoss[0m : 2.60792
[1mStep[0m  [12/42], [94mLoss[0m : 2.60367
[1mStep[0m  [16/42], [94mLoss[0m : 2.61079
[1mStep[0m  [20/42], [94mLoss[0m : 2.46472
[1mStep[0m  [24/42], [94mLoss[0m : 2.55055
[1mStep[0m  [28/42], [94mLoss[0m : 2.51951
[1mStep[0m  [32/42], [94mLoss[0m : 2.63036
[1mStep[0m  [36/42], [94mLoss[0m : 2.38721
[1mStep[0m  [40/42], [94mLoss[0m : 2.52789

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.567, [92mTest[0m: 2.381, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54277
[1mStep[0m  [4/42], [94mLoss[0m : 2.46969
[1mStep[0m  [8/42], [94mLoss[0m : 2.42001
[1mStep[0m  [12/42], [94mLoss[0m : 2.65466
[1mStep[0m  [16/42], [94mLoss[0m : 2.52675
[1mStep[0m  [20/42], [94mLoss[0m : 2.33418
[1mStep[0m  [24/42], [94mLoss[0m : 2.38301
[1mStep[0m  [28/42], [94mLoss[0m : 2.46110
[1mStep[0m  [32/42], [94mLoss[0m : 2.56225
[1mStep[0m  [36/42], [94mLoss[0m : 2.37694
[1mStep[0m  [40/42], [94mLoss[0m : 2.49576

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.391, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.63031
[1mStep[0m  [4/42], [94mLoss[0m : 2.54610
[1mStep[0m  [8/42], [94mLoss[0m : 2.19132
[1mStep[0m  [12/42], [94mLoss[0m : 2.62229
[1mStep[0m  [16/42], [94mLoss[0m : 2.52204
[1mStep[0m  [20/42], [94mLoss[0m : 2.28856
[1mStep[0m  [24/42], [94mLoss[0m : 2.52750
[1mStep[0m  [28/42], [94mLoss[0m : 2.49200
[1mStep[0m  [32/42], [94mLoss[0m : 2.66717
[1mStep[0m  [36/42], [94mLoss[0m : 2.62895
[1mStep[0m  [40/42], [94mLoss[0m : 2.40220

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.531, [92mTest[0m: 2.384, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.63461
[1mStep[0m  [4/42], [94mLoss[0m : 2.26793
[1mStep[0m  [8/42], [94mLoss[0m : 2.59089
[1mStep[0m  [12/42], [94mLoss[0m : 2.43336
[1mStep[0m  [16/42], [94mLoss[0m : 2.51153
[1mStep[0m  [20/42], [94mLoss[0m : 2.41208
[1mStep[0m  [24/42], [94mLoss[0m : 2.46798
[1mStep[0m  [28/42], [94mLoss[0m : 2.48694
[1mStep[0m  [32/42], [94mLoss[0m : 2.30191
[1mStep[0m  [36/42], [94mLoss[0m : 2.59304
[1mStep[0m  [40/42], [94mLoss[0m : 2.56421

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.514, [92mTest[0m: 2.372, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45735
[1mStep[0m  [4/42], [94mLoss[0m : 2.48605
[1mStep[0m  [8/42], [94mLoss[0m : 2.59463
[1mStep[0m  [12/42], [94mLoss[0m : 2.42947
[1mStep[0m  [16/42], [94mLoss[0m : 2.46012
[1mStep[0m  [20/42], [94mLoss[0m : 2.67807
[1mStep[0m  [24/42], [94mLoss[0m : 2.32197
[1mStep[0m  [28/42], [94mLoss[0m : 2.38827
[1mStep[0m  [32/42], [94mLoss[0m : 2.37439
[1mStep[0m  [36/42], [94mLoss[0m : 2.55676
[1mStep[0m  [40/42], [94mLoss[0m : 2.55021

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.374, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46932
[1mStep[0m  [4/42], [94mLoss[0m : 2.53651
[1mStep[0m  [8/42], [94mLoss[0m : 2.69497
[1mStep[0m  [12/42], [94mLoss[0m : 2.58934
[1mStep[0m  [16/42], [94mLoss[0m : 2.52804
[1mStep[0m  [20/42], [94mLoss[0m : 2.58691
[1mStep[0m  [24/42], [94mLoss[0m : 2.68940
[1mStep[0m  [28/42], [94mLoss[0m : 2.48388
[1mStep[0m  [32/42], [94mLoss[0m : 2.39712
[1mStep[0m  [36/42], [94mLoss[0m : 2.44183
[1mStep[0m  [40/42], [94mLoss[0m : 2.52329

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.540, [92mTest[0m: 2.384, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35817
[1mStep[0m  [4/42], [94mLoss[0m : 2.66838
[1mStep[0m  [8/42], [94mLoss[0m : 2.52895
[1mStep[0m  [12/42], [94mLoss[0m : 2.41613
[1mStep[0m  [16/42], [94mLoss[0m : 2.44722
[1mStep[0m  [20/42], [94mLoss[0m : 2.32034
[1mStep[0m  [24/42], [94mLoss[0m : 2.69631
[1mStep[0m  [28/42], [94mLoss[0m : 2.39390
[1mStep[0m  [32/42], [94mLoss[0m : 2.74896
[1mStep[0m  [36/42], [94mLoss[0m : 2.49843
[1mStep[0m  [40/42], [94mLoss[0m : 2.53247

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.535, [92mTest[0m: 2.375, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53430
[1mStep[0m  [4/42], [94mLoss[0m : 2.86229
[1mStep[0m  [8/42], [94mLoss[0m : 2.66451
[1mStep[0m  [12/42], [94mLoss[0m : 2.31157
[1mStep[0m  [16/42], [94mLoss[0m : 2.46105
[1mStep[0m  [20/42], [94mLoss[0m : 2.65961
[1mStep[0m  [24/42], [94mLoss[0m : 2.54087
[1mStep[0m  [28/42], [94mLoss[0m : 2.50456
[1mStep[0m  [32/42], [94mLoss[0m : 2.48644
[1mStep[0m  [36/42], [94mLoss[0m : 2.56630
[1mStep[0m  [40/42], [94mLoss[0m : 2.66023

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.377, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.370
====================================

Phase 1 - Evaluation MAE:  2.369855386870248
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 2.50103
[1mStep[0m  [4/42], [94mLoss[0m : 2.51957
[1mStep[0m  [8/42], [94mLoss[0m : 2.45281
[1mStep[0m  [12/42], [94mLoss[0m : 2.66296
[1mStep[0m  [16/42], [94mLoss[0m : 2.71793
[1mStep[0m  [20/42], [94mLoss[0m : 2.54129
[1mStep[0m  [24/42], [94mLoss[0m : 2.62581
[1mStep[0m  [28/42], [94mLoss[0m : 2.45261
[1mStep[0m  [32/42], [94mLoss[0m : 2.69395
[1mStep[0m  [36/42], [94mLoss[0m : 2.57716
[1mStep[0m  [40/42], [94mLoss[0m : 2.57930

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.590, [92mTest[0m: 2.366, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61248
[1mStep[0m  [4/42], [94mLoss[0m : 2.67267
[1mStep[0m  [8/42], [94mLoss[0m : 2.74782
[1mStep[0m  [12/42], [94mLoss[0m : 2.76770
[1mStep[0m  [16/42], [94mLoss[0m : 2.46370
[1mStep[0m  [20/42], [94mLoss[0m : 2.40928
[1mStep[0m  [24/42], [94mLoss[0m : 2.64712
[1mStep[0m  [28/42], [94mLoss[0m : 2.44542
[1mStep[0m  [32/42], [94mLoss[0m : 2.39481
[1mStep[0m  [36/42], [94mLoss[0m : 2.67353
[1mStep[0m  [40/42], [94mLoss[0m : 2.51563

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.577, [92mTest[0m: 2.463, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52844
[1mStep[0m  [4/42], [94mLoss[0m : 2.39007
[1mStep[0m  [8/42], [94mLoss[0m : 2.55693
[1mStep[0m  [12/42], [94mLoss[0m : 2.78952
[1mStep[0m  [16/42], [94mLoss[0m : 2.59629
[1mStep[0m  [20/42], [94mLoss[0m : 2.46251
[1mStep[0m  [24/42], [94mLoss[0m : 2.70083
[1mStep[0m  [28/42], [94mLoss[0m : 2.55460
[1mStep[0m  [32/42], [94mLoss[0m : 2.61121
[1mStep[0m  [36/42], [94mLoss[0m : 2.58976
[1mStep[0m  [40/42], [94mLoss[0m : 2.43222

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.570, [92mTest[0m: 2.886, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48616
[1mStep[0m  [4/42], [94mLoss[0m : 2.60129
[1mStep[0m  [8/42], [94mLoss[0m : 2.43607
[1mStep[0m  [12/42], [94mLoss[0m : 2.30821
[1mStep[0m  [16/42], [94mLoss[0m : 2.76549
[1mStep[0m  [20/42], [94mLoss[0m : 2.49607
[1mStep[0m  [24/42], [94mLoss[0m : 2.57286
[1mStep[0m  [28/42], [94mLoss[0m : 2.44074
[1mStep[0m  [32/42], [94mLoss[0m : 2.50418
[1mStep[0m  [36/42], [94mLoss[0m : 2.82347
[1mStep[0m  [40/42], [94mLoss[0m : 2.50178

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.529, [92mTest[0m: 2.786, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26696
[1mStep[0m  [4/42], [94mLoss[0m : 2.67094
[1mStep[0m  [8/42], [94mLoss[0m : 2.30542
[1mStep[0m  [12/42], [94mLoss[0m : 2.61783
[1mStep[0m  [16/42], [94mLoss[0m : 2.44085
[1mStep[0m  [20/42], [94mLoss[0m : 2.69394
[1mStep[0m  [24/42], [94mLoss[0m : 2.51016
[1mStep[0m  [28/42], [94mLoss[0m : 2.83426
[1mStep[0m  [32/42], [94mLoss[0m : 2.59931
[1mStep[0m  [36/42], [94mLoss[0m : 2.54816
[1mStep[0m  [40/42], [94mLoss[0m : 2.60107

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.530, [92mTest[0m: 2.923, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41614
[1mStep[0m  [4/42], [94mLoss[0m : 2.70648
[1mStep[0m  [8/42], [94mLoss[0m : 2.40916
[1mStep[0m  [12/42], [94mLoss[0m : 2.48084
[1mStep[0m  [16/42], [94mLoss[0m : 2.32193
[1mStep[0m  [20/42], [94mLoss[0m : 2.62648
[1mStep[0m  [24/42], [94mLoss[0m : 2.37228
[1mStep[0m  [28/42], [94mLoss[0m : 2.79593
[1mStep[0m  [32/42], [94mLoss[0m : 2.59830
[1mStep[0m  [36/42], [94mLoss[0m : 2.68443
[1mStep[0m  [40/42], [94mLoss[0m : 2.35998

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.590, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48529
[1mStep[0m  [4/42], [94mLoss[0m : 2.67177
[1mStep[0m  [8/42], [94mLoss[0m : 2.49450
[1mStep[0m  [12/42], [94mLoss[0m : 2.37729
[1mStep[0m  [16/42], [94mLoss[0m : 2.50002
[1mStep[0m  [20/42], [94mLoss[0m : 2.30314
[1mStep[0m  [24/42], [94mLoss[0m : 2.55231
[1mStep[0m  [28/42], [94mLoss[0m : 2.32598
[1mStep[0m  [32/42], [94mLoss[0m : 2.36316
[1mStep[0m  [36/42], [94mLoss[0m : 2.23328
[1mStep[0m  [40/42], [94mLoss[0m : 2.50209

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.724, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61607
[1mStep[0m  [4/42], [94mLoss[0m : 2.51570
[1mStep[0m  [8/42], [94mLoss[0m : 2.28559
[1mStep[0m  [12/42], [94mLoss[0m : 2.45783
[1mStep[0m  [16/42], [94mLoss[0m : 2.35913
[1mStep[0m  [20/42], [94mLoss[0m : 2.51198
[1mStep[0m  [24/42], [94mLoss[0m : 2.31130
[1mStep[0m  [28/42], [94mLoss[0m : 2.46294
[1mStep[0m  [32/42], [94mLoss[0m : 2.33889
[1mStep[0m  [36/42], [94mLoss[0m : 2.49727
[1mStep[0m  [40/42], [94mLoss[0m : 2.31561

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.707, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49673
[1mStep[0m  [4/42], [94mLoss[0m : 2.44589
[1mStep[0m  [8/42], [94mLoss[0m : 2.40797
[1mStep[0m  [12/42], [94mLoss[0m : 2.59028
[1mStep[0m  [16/42], [94mLoss[0m : 2.48590
[1mStep[0m  [20/42], [94mLoss[0m : 2.22553
[1mStep[0m  [24/42], [94mLoss[0m : 2.42439
[1mStep[0m  [28/42], [94mLoss[0m : 2.46586
[1mStep[0m  [32/42], [94mLoss[0m : 2.40276
[1mStep[0m  [36/42], [94mLoss[0m : 2.60979
[1mStep[0m  [40/42], [94mLoss[0m : 2.35249

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.665, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34327
[1mStep[0m  [4/42], [94mLoss[0m : 2.34917
[1mStep[0m  [8/42], [94mLoss[0m : 2.38106
[1mStep[0m  [12/42], [94mLoss[0m : 2.35958
[1mStep[0m  [16/42], [94mLoss[0m : 2.31535
[1mStep[0m  [20/42], [94mLoss[0m : 2.62158
[1mStep[0m  [24/42], [94mLoss[0m : 2.45957
[1mStep[0m  [28/42], [94mLoss[0m : 2.55556
[1mStep[0m  [32/42], [94mLoss[0m : 2.38141
[1mStep[0m  [36/42], [94mLoss[0m : 2.48507
[1mStep[0m  [40/42], [94mLoss[0m : 2.30635

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.812, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.23619
[1mStep[0m  [4/42], [94mLoss[0m : 2.32189
[1mStep[0m  [8/42], [94mLoss[0m : 2.31809
[1mStep[0m  [12/42], [94mLoss[0m : 2.29644
[1mStep[0m  [16/42], [94mLoss[0m : 2.47478
[1mStep[0m  [20/42], [94mLoss[0m : 2.18987
[1mStep[0m  [24/42], [94mLoss[0m : 2.40712
[1mStep[0m  [28/42], [94mLoss[0m : 2.41885
[1mStep[0m  [32/42], [94mLoss[0m : 2.71484
[1mStep[0m  [36/42], [94mLoss[0m : 2.28619
[1mStep[0m  [40/42], [94mLoss[0m : 2.40412

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.373, [92mTest[0m: 2.713, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.18157
[1mStep[0m  [4/42], [94mLoss[0m : 2.41101
[1mStep[0m  [8/42], [94mLoss[0m : 2.40306
[1mStep[0m  [12/42], [94mLoss[0m : 2.25505
[1mStep[0m  [16/42], [94mLoss[0m : 2.21527
[1mStep[0m  [20/42], [94mLoss[0m : 2.39598
[1mStep[0m  [24/42], [94mLoss[0m : 2.52823
[1mStep[0m  [28/42], [94mLoss[0m : 2.41576
[1mStep[0m  [32/42], [94mLoss[0m : 2.14000
[1mStep[0m  [36/42], [94mLoss[0m : 2.32711
[1mStep[0m  [40/42], [94mLoss[0m : 2.32587

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.393, [92mTest[0m: 2.566, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47772
[1mStep[0m  [4/42], [94mLoss[0m : 2.41063
[1mStep[0m  [8/42], [94mLoss[0m : 2.43478
[1mStep[0m  [12/42], [94mLoss[0m : 2.15098
[1mStep[0m  [16/42], [94mLoss[0m : 2.10729
[1mStep[0m  [20/42], [94mLoss[0m : 2.33724
[1mStep[0m  [24/42], [94mLoss[0m : 2.32695
[1mStep[0m  [28/42], [94mLoss[0m : 2.49328
[1mStep[0m  [32/42], [94mLoss[0m : 2.29230
[1mStep[0m  [36/42], [94mLoss[0m : 2.36369
[1mStep[0m  [40/42], [94mLoss[0m : 2.35791

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.362, [92mTest[0m: 2.605, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48734
[1mStep[0m  [4/42], [94mLoss[0m : 2.21405
[1mStep[0m  [8/42], [94mLoss[0m : 2.62192
[1mStep[0m  [12/42], [94mLoss[0m : 2.41619
[1mStep[0m  [16/42], [94mLoss[0m : 2.52002
[1mStep[0m  [20/42], [94mLoss[0m : 2.48278
[1mStep[0m  [24/42], [94mLoss[0m : 2.29608
[1mStep[0m  [28/42], [94mLoss[0m : 2.11393
[1mStep[0m  [32/42], [94mLoss[0m : 2.23631
[1mStep[0m  [36/42], [94mLoss[0m : 2.49941
[1mStep[0m  [40/42], [94mLoss[0m : 2.32275

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.333, [92mTest[0m: 2.683, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.25636
[1mStep[0m  [4/42], [94mLoss[0m : 2.55481
[1mStep[0m  [8/42], [94mLoss[0m : 2.18030
[1mStep[0m  [12/42], [94mLoss[0m : 2.41791
[1mStep[0m  [16/42], [94mLoss[0m : 2.17926
[1mStep[0m  [20/42], [94mLoss[0m : 2.34414
[1mStep[0m  [24/42], [94mLoss[0m : 2.38042
[1mStep[0m  [28/42], [94mLoss[0m : 2.22820
[1mStep[0m  [32/42], [94mLoss[0m : 2.43318
[1mStep[0m  [36/42], [94mLoss[0m : 2.32499
[1mStep[0m  [40/42], [94mLoss[0m : 2.50281

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.326, [92mTest[0m: 2.636, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.10633
[1mStep[0m  [4/42], [94mLoss[0m : 2.20451
[1mStep[0m  [8/42], [94mLoss[0m : 2.08391
[1mStep[0m  [12/42], [94mLoss[0m : 2.45515
[1mStep[0m  [16/42], [94mLoss[0m : 2.43387
[1mStep[0m  [20/42], [94mLoss[0m : 2.60335
[1mStep[0m  [24/42], [94mLoss[0m : 2.34765
[1mStep[0m  [28/42], [94mLoss[0m : 2.21682
[1mStep[0m  [32/42], [94mLoss[0m : 2.36538
[1mStep[0m  [36/42], [94mLoss[0m : 2.83442
[1mStep[0m  [40/42], [94mLoss[0m : 2.24927

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.331, [92mTest[0m: 2.617, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31117
[1mStep[0m  [4/42], [94mLoss[0m : 2.51674
[1mStep[0m  [8/42], [94mLoss[0m : 2.47292
[1mStep[0m  [12/42], [94mLoss[0m : 2.27854
[1mStep[0m  [16/42], [94mLoss[0m : 2.28049
[1mStep[0m  [20/42], [94mLoss[0m : 2.16352
[1mStep[0m  [24/42], [94mLoss[0m : 2.31855
[1mStep[0m  [28/42], [94mLoss[0m : 2.30019
[1mStep[0m  [32/42], [94mLoss[0m : 2.34815
[1mStep[0m  [36/42], [94mLoss[0m : 2.22910
[1mStep[0m  [40/42], [94mLoss[0m : 2.30251

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.288, [92mTest[0m: 2.586, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26048
[1mStep[0m  [4/42], [94mLoss[0m : 2.27916
[1mStep[0m  [8/42], [94mLoss[0m : 2.35238
[1mStep[0m  [12/42], [94mLoss[0m : 2.32866
[1mStep[0m  [16/42], [94mLoss[0m : 2.28942
[1mStep[0m  [20/42], [94mLoss[0m : 2.13766
[1mStep[0m  [24/42], [94mLoss[0m : 2.27551
[1mStep[0m  [28/42], [94mLoss[0m : 2.45101
[1mStep[0m  [32/42], [94mLoss[0m : 2.42185
[1mStep[0m  [36/42], [94mLoss[0m : 2.33268
[1mStep[0m  [40/42], [94mLoss[0m : 2.49706

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.291, [92mTest[0m: 2.616, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.08557
[1mStep[0m  [4/42], [94mLoss[0m : 2.34136
[1mStep[0m  [8/42], [94mLoss[0m : 2.10228
[1mStep[0m  [12/42], [94mLoss[0m : 2.28172
[1mStep[0m  [16/42], [94mLoss[0m : 2.31563
[1mStep[0m  [20/42], [94mLoss[0m : 2.41420
[1mStep[0m  [24/42], [94mLoss[0m : 2.34398
[1mStep[0m  [28/42], [94mLoss[0m : 2.20448
[1mStep[0m  [32/42], [94mLoss[0m : 2.07662
[1mStep[0m  [36/42], [94mLoss[0m : 2.29164
[1mStep[0m  [40/42], [94mLoss[0m : 2.14245

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.228, [92mTest[0m: 2.528, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.14372
[1mStep[0m  [4/42], [94mLoss[0m : 2.48376
[1mStep[0m  [8/42], [94mLoss[0m : 2.30429
[1mStep[0m  [12/42], [94mLoss[0m : 2.32813
[1mStep[0m  [16/42], [94mLoss[0m : 2.37191
[1mStep[0m  [20/42], [94mLoss[0m : 2.19547
[1mStep[0m  [24/42], [94mLoss[0m : 2.13716
[1mStep[0m  [28/42], [94mLoss[0m : 2.29999
[1mStep[0m  [32/42], [94mLoss[0m : 2.28353
[1mStep[0m  [36/42], [94mLoss[0m : 2.17642
[1mStep[0m  [40/42], [94mLoss[0m : 2.10074

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.206, [92mTest[0m: 2.623, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27712
[1mStep[0m  [4/42], [94mLoss[0m : 2.36357
[1mStep[0m  [8/42], [94mLoss[0m : 2.23005
[1mStep[0m  [12/42], [94mLoss[0m : 2.06509
[1mStep[0m  [16/42], [94mLoss[0m : 2.32881
[1mStep[0m  [20/42], [94mLoss[0m : 2.05409
[1mStep[0m  [24/42], [94mLoss[0m : 2.00554
[1mStep[0m  [28/42], [94mLoss[0m : 1.92251
[1mStep[0m  [32/42], [94mLoss[0m : 2.07480
[1mStep[0m  [36/42], [94mLoss[0m : 2.32063
[1mStep[0m  [40/42], [94mLoss[0m : 2.06166

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.191, [92mTest[0m: 2.610, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.23823
[1mStep[0m  [4/42], [94mLoss[0m : 2.25412
[1mStep[0m  [8/42], [94mLoss[0m : 2.31631
[1mStep[0m  [12/42], [94mLoss[0m : 2.16209
[1mStep[0m  [16/42], [94mLoss[0m : 2.20073
[1mStep[0m  [20/42], [94mLoss[0m : 2.46404
[1mStep[0m  [24/42], [94mLoss[0m : 2.13392
[1mStep[0m  [28/42], [94mLoss[0m : 2.14440
[1mStep[0m  [32/42], [94mLoss[0m : 2.18061
[1mStep[0m  [36/42], [94mLoss[0m : 2.04856
[1mStep[0m  [40/42], [94mLoss[0m : 2.25744

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.193, [92mTest[0m: 2.560, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28075
[1mStep[0m  [4/42], [94mLoss[0m : 2.05042
[1mStep[0m  [8/42], [94mLoss[0m : 2.21517
[1mStep[0m  [12/42], [94mLoss[0m : 2.13564
[1mStep[0m  [16/42], [94mLoss[0m : 2.07422
[1mStep[0m  [20/42], [94mLoss[0m : 2.08689
[1mStep[0m  [24/42], [94mLoss[0m : 2.17762
[1mStep[0m  [28/42], [94mLoss[0m : 2.09415
[1mStep[0m  [32/42], [94mLoss[0m : 2.01853
[1mStep[0m  [36/42], [94mLoss[0m : 2.11911
[1mStep[0m  [40/42], [94mLoss[0m : 1.96959

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.169, [92mTest[0m: 2.568, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.07384
[1mStep[0m  [4/42], [94mLoss[0m : 2.13306
[1mStep[0m  [8/42], [94mLoss[0m : 2.21573
[1mStep[0m  [12/42], [94mLoss[0m : 2.27233
[1mStep[0m  [16/42], [94mLoss[0m : 2.20669
[1mStep[0m  [20/42], [94mLoss[0m : 2.11617
[1mStep[0m  [24/42], [94mLoss[0m : 2.09215
[1mStep[0m  [28/42], [94mLoss[0m : 2.09174
[1mStep[0m  [32/42], [94mLoss[0m : 2.18442
[1mStep[0m  [36/42], [94mLoss[0m : 1.83674
[1mStep[0m  [40/42], [94mLoss[0m : 2.11971

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.119, [92mTest[0m: 2.540, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.12985
[1mStep[0m  [4/42], [94mLoss[0m : 2.26367
[1mStep[0m  [8/42], [94mLoss[0m : 2.08610
[1mStep[0m  [12/42], [94mLoss[0m : 2.08419
[1mStep[0m  [16/42], [94mLoss[0m : 2.08027
[1mStep[0m  [20/42], [94mLoss[0m : 2.31410
[1mStep[0m  [24/42], [94mLoss[0m : 1.95270
[1mStep[0m  [28/42], [94mLoss[0m : 2.04291
[1mStep[0m  [32/42], [94mLoss[0m : 2.07206
[1mStep[0m  [36/42], [94mLoss[0m : 2.06324
[1mStep[0m  [40/42], [94mLoss[0m : 2.19311

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.122, [92mTest[0m: 2.546, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21786
[1mStep[0m  [4/42], [94mLoss[0m : 2.03358
[1mStep[0m  [8/42], [94mLoss[0m : 2.11363
[1mStep[0m  [12/42], [94mLoss[0m : 2.01750
[1mStep[0m  [16/42], [94mLoss[0m : 1.96688
[1mStep[0m  [20/42], [94mLoss[0m : 2.08537
[1mStep[0m  [24/42], [94mLoss[0m : 1.94733
[1mStep[0m  [28/42], [94mLoss[0m : 2.05275
[1mStep[0m  [32/42], [94mLoss[0m : 2.11222
[1mStep[0m  [36/42], [94mLoss[0m : 1.91012
[1mStep[0m  [40/42], [94mLoss[0m : 1.95346

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.094, [92mTest[0m: 2.589, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.95117
[1mStep[0m  [4/42], [94mLoss[0m : 2.23660
[1mStep[0m  [8/42], [94mLoss[0m : 1.92690
[1mStep[0m  [12/42], [94mLoss[0m : 1.96976
[1mStep[0m  [16/42], [94mLoss[0m : 2.28829
[1mStep[0m  [20/42], [94mLoss[0m : 2.05903
[1mStep[0m  [24/42], [94mLoss[0m : 1.90148
[1mStep[0m  [28/42], [94mLoss[0m : 2.33840
[1mStep[0m  [32/42], [94mLoss[0m : 2.02843
[1mStep[0m  [36/42], [94mLoss[0m : 2.00863
[1mStep[0m  [40/42], [94mLoss[0m : 2.16539

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.072, [92mTest[0m: 2.511, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.08570
[1mStep[0m  [4/42], [94mLoss[0m : 2.23850
[1mStep[0m  [8/42], [94mLoss[0m : 2.12018
[1mStep[0m  [12/42], [94mLoss[0m : 1.96958
[1mStep[0m  [16/42], [94mLoss[0m : 2.24797
[1mStep[0m  [20/42], [94mLoss[0m : 2.17238
[1mStep[0m  [24/42], [94mLoss[0m : 2.00914
[1mStep[0m  [28/42], [94mLoss[0m : 1.94197
[1mStep[0m  [32/42], [94mLoss[0m : 2.00538
[1mStep[0m  [36/42], [94mLoss[0m : 2.08281
[1mStep[0m  [40/42], [94mLoss[0m : 1.98163

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.077, [92mTest[0m: 2.480, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.04952
[1mStep[0m  [4/42], [94mLoss[0m : 2.11821
[1mStep[0m  [8/42], [94mLoss[0m : 1.99811
[1mStep[0m  [12/42], [94mLoss[0m : 2.00777
[1mStep[0m  [16/42], [94mLoss[0m : 1.91550
[1mStep[0m  [20/42], [94mLoss[0m : 2.06427
[1mStep[0m  [24/42], [94mLoss[0m : 1.89607
[1mStep[0m  [28/42], [94mLoss[0m : 2.03169
[1mStep[0m  [32/42], [94mLoss[0m : 2.31384
[1mStep[0m  [36/42], [94mLoss[0m : 1.96273
[1mStep[0m  [40/42], [94mLoss[0m : 1.96361

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.033, [92mTest[0m: 2.578, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.16315
[1mStep[0m  [4/42], [94mLoss[0m : 2.06436
[1mStep[0m  [8/42], [94mLoss[0m : 1.95925
[1mStep[0m  [12/42], [94mLoss[0m : 2.11191
[1mStep[0m  [16/42], [94mLoss[0m : 2.00254
[1mStep[0m  [20/42], [94mLoss[0m : 2.03207
[1mStep[0m  [24/42], [94mLoss[0m : 2.03743
[1mStep[0m  [28/42], [94mLoss[0m : 1.93925
[1mStep[0m  [32/42], [94mLoss[0m : 2.22082
[1mStep[0m  [36/42], [94mLoss[0m : 2.16009
[1mStep[0m  [40/42], [94mLoss[0m : 2.05499

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.017, [92mTest[0m: 2.599, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.580
====================================

Phase 2 - Evaluation MAE:  2.5795961959021434
MAE score P1       2.369855
MAE score P2       2.579596
loss               2.017327
learning_rate      0.002575
batch_size              256
hidden_sizes      [200, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.5
weight_decay         0.0001
Name: 22, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 11.15104
[1mStep[0m  [2/21], [94mLoss[0m : 11.03560
[1mStep[0m  [4/21], [94mLoss[0m : 10.95848
[1mStep[0m  [6/21], [94mLoss[0m : 11.15165
[1mStep[0m  [8/21], [94mLoss[0m : 10.87147
[1mStep[0m  [10/21], [94mLoss[0m : 10.57190
[1mStep[0m  [12/21], [94mLoss[0m : 10.35369
[1mStep[0m  [14/21], [94mLoss[0m : 10.20099
[1mStep[0m  [16/21], [94mLoss[0m : 10.05002
[1mStep[0m  [18/21], [94mLoss[0m : 10.21489
[1mStep[0m  [20/21], [94mLoss[0m : 10.19742

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.564, [92mTest[0m: 10.910, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.89963
[1mStep[0m  [2/21], [94mLoss[0m : 10.26997
[1mStep[0m  [4/21], [94mLoss[0m : 9.86216
[1mStep[0m  [6/21], [94mLoss[0m : 9.86613
[1mStep[0m  [8/21], [94mLoss[0m : 9.62722
[1mStep[0m  [10/21], [94mLoss[0m : 9.62223
[1mStep[0m  [12/21], [94mLoss[0m : 9.40671
[1mStep[0m  [14/21], [94mLoss[0m : 9.40425
[1mStep[0m  [16/21], [94mLoss[0m : 9.14173
[1mStep[0m  [18/21], [94mLoss[0m : 9.42123
[1mStep[0m  [20/21], [94mLoss[0m : 9.05872

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.600, [92mTest[0m: 10.353, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.46440
[1mStep[0m  [2/21], [94mLoss[0m : 8.94464
[1mStep[0m  [4/21], [94mLoss[0m : 9.12959
[1mStep[0m  [6/21], [94mLoss[0m : 8.62903
[1mStep[0m  [8/21], [94mLoss[0m : 8.80179
[1mStep[0m  [10/21], [94mLoss[0m : 8.59464
[1mStep[0m  [12/21], [94mLoss[0m : 8.74602
[1mStep[0m  [14/21], [94mLoss[0m : 8.46433
[1mStep[0m  [16/21], [94mLoss[0m : 8.29681
[1mStep[0m  [18/21], [94mLoss[0m : 8.09698
[1mStep[0m  [20/21], [94mLoss[0m : 8.27943

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 8.626, [92mTest[0m: 9.419, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.12977
[1mStep[0m  [2/21], [94mLoss[0m : 7.98697
[1mStep[0m  [4/21], [94mLoss[0m : 7.81467
[1mStep[0m  [6/21], [94mLoss[0m : 7.87713
[1mStep[0m  [8/21], [94mLoss[0m : 7.99149
[1mStep[0m  [10/21], [94mLoss[0m : 7.68396
[1mStep[0m  [12/21], [94mLoss[0m : 7.57139
[1mStep[0m  [14/21], [94mLoss[0m : 7.59036
[1mStep[0m  [16/21], [94mLoss[0m : 7.28813
[1mStep[0m  [18/21], [94mLoss[0m : 7.16464
[1mStep[0m  [20/21], [94mLoss[0m : 7.45970

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 7.653, [92mTest[0m: 8.546, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.92037
[1mStep[0m  [2/21], [94mLoss[0m : 6.87837
[1mStep[0m  [4/21], [94mLoss[0m : 7.47964
[1mStep[0m  [6/21], [94mLoss[0m : 6.95461
[1mStep[0m  [8/21], [94mLoss[0m : 6.74729
[1mStep[0m  [10/21], [94mLoss[0m : 6.67670
[1mStep[0m  [12/21], [94mLoss[0m : 6.78967
[1mStep[0m  [14/21], [94mLoss[0m : 6.51715
[1mStep[0m  [16/21], [94mLoss[0m : 6.28704
[1mStep[0m  [18/21], [94mLoss[0m : 6.11906
[1mStep[0m  [20/21], [94mLoss[0m : 6.29831

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 6.697, [92mTest[0m: 7.718, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.25146
[1mStep[0m  [2/21], [94mLoss[0m : 6.29678
[1mStep[0m  [4/21], [94mLoss[0m : 5.61800
[1mStep[0m  [6/21], [94mLoss[0m : 6.04980
[1mStep[0m  [8/21], [94mLoss[0m : 6.29166
[1mStep[0m  [10/21], [94mLoss[0m : 5.81320
[1mStep[0m  [12/21], [94mLoss[0m : 5.87154
[1mStep[0m  [14/21], [94mLoss[0m : 5.66966
[1mStep[0m  [16/21], [94mLoss[0m : 5.50567
[1mStep[0m  [18/21], [94mLoss[0m : 5.43229
[1mStep[0m  [20/21], [94mLoss[0m : 5.10967

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 5.807, [92mTest[0m: 6.890, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.39031
[1mStep[0m  [2/21], [94mLoss[0m : 5.22544
[1mStep[0m  [4/21], [94mLoss[0m : 5.16121
[1mStep[0m  [6/21], [94mLoss[0m : 4.90090
[1mStep[0m  [8/21], [94mLoss[0m : 4.87834
[1mStep[0m  [10/21], [94mLoss[0m : 4.86746
[1mStep[0m  [12/21], [94mLoss[0m : 4.78462
[1mStep[0m  [14/21], [94mLoss[0m : 4.93523
[1mStep[0m  [16/21], [94mLoss[0m : 4.92125
[1mStep[0m  [18/21], [94mLoss[0m : 5.01017
[1mStep[0m  [20/21], [94mLoss[0m : 4.58543

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 4.961, [92mTest[0m: 6.007, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.53934
[1mStep[0m  [2/21], [94mLoss[0m : 4.37951
[1mStep[0m  [4/21], [94mLoss[0m : 4.64608
[1mStep[0m  [6/21], [94mLoss[0m : 4.32493
[1mStep[0m  [8/21], [94mLoss[0m : 4.08782
[1mStep[0m  [10/21], [94mLoss[0m : 4.37411
[1mStep[0m  [12/21], [94mLoss[0m : 4.09338
[1mStep[0m  [14/21], [94mLoss[0m : 4.24399
[1mStep[0m  [16/21], [94mLoss[0m : 3.86702
[1mStep[0m  [18/21], [94mLoss[0m : 3.97517
[1mStep[0m  [20/21], [94mLoss[0m : 3.68320

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 4.238, [92mTest[0m: 5.171, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.91064
[1mStep[0m  [2/21], [94mLoss[0m : 3.91435
[1mStep[0m  [4/21], [94mLoss[0m : 3.70637
[1mStep[0m  [6/21], [94mLoss[0m : 3.65958
[1mStep[0m  [8/21], [94mLoss[0m : 3.51311
[1mStep[0m  [10/21], [94mLoss[0m : 3.77829
[1mStep[0m  [12/21], [94mLoss[0m : 3.42311
[1mStep[0m  [14/21], [94mLoss[0m : 3.69512
[1mStep[0m  [16/21], [94mLoss[0m : 3.63759
[1mStep[0m  [18/21], [94mLoss[0m : 3.74088
[1mStep[0m  [20/21], [94mLoss[0m : 3.41711

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 3.677, [92mTest[0m: 4.430, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.73436
[1mStep[0m  [2/21], [94mLoss[0m : 3.37244
[1mStep[0m  [4/21], [94mLoss[0m : 3.22481
[1mStep[0m  [6/21], [94mLoss[0m : 3.33628
[1mStep[0m  [8/21], [94mLoss[0m : 3.34762
[1mStep[0m  [10/21], [94mLoss[0m : 3.40572
[1mStep[0m  [12/21], [94mLoss[0m : 3.25209
[1mStep[0m  [14/21], [94mLoss[0m : 2.96828
[1mStep[0m  [16/21], [94mLoss[0m : 3.29067
[1mStep[0m  [18/21], [94mLoss[0m : 2.84346
[1mStep[0m  [20/21], [94mLoss[0m : 3.23326

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 3.266, [92mTest[0m: 3.834, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.23982
[1mStep[0m  [2/21], [94mLoss[0m : 3.29353
[1mStep[0m  [4/21], [94mLoss[0m : 3.25931
[1mStep[0m  [6/21], [94mLoss[0m : 2.96221
[1mStep[0m  [8/21], [94mLoss[0m : 3.15448
[1mStep[0m  [10/21], [94mLoss[0m : 2.89346
[1mStep[0m  [12/21], [94mLoss[0m : 2.88819
[1mStep[0m  [14/21], [94mLoss[0m : 2.78564
[1mStep[0m  [16/21], [94mLoss[0m : 2.84158
[1mStep[0m  [18/21], [94mLoss[0m : 3.13959
[1mStep[0m  [20/21], [94mLoss[0m : 2.88851

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 3.014, [92mTest[0m: 3.414, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.94330
[1mStep[0m  [2/21], [94mLoss[0m : 2.64301
[1mStep[0m  [4/21], [94mLoss[0m : 2.92139
[1mStep[0m  [6/21], [94mLoss[0m : 2.91572
[1mStep[0m  [8/21], [94mLoss[0m : 3.04577
[1mStep[0m  [10/21], [94mLoss[0m : 2.77845
[1mStep[0m  [12/21], [94mLoss[0m : 2.82295
[1mStep[0m  [14/21], [94mLoss[0m : 2.72298
[1mStep[0m  [16/21], [94mLoss[0m : 2.75662
[1mStep[0m  [18/21], [94mLoss[0m : 2.79462
[1mStep[0m  [20/21], [94mLoss[0m : 2.84964

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.849, [92mTest[0m: 3.167, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.84300
[1mStep[0m  [2/21], [94mLoss[0m : 2.96341
[1mStep[0m  [4/21], [94mLoss[0m : 2.93383
[1mStep[0m  [6/21], [94mLoss[0m : 2.81058
[1mStep[0m  [8/21], [94mLoss[0m : 2.56409
[1mStep[0m  [10/21], [94mLoss[0m : 2.73052
[1mStep[0m  [12/21], [94mLoss[0m : 2.67630
[1mStep[0m  [14/21], [94mLoss[0m : 2.72569
[1mStep[0m  [16/21], [94mLoss[0m : 2.84055
[1mStep[0m  [18/21], [94mLoss[0m : 2.75859
[1mStep[0m  [20/21], [94mLoss[0m : 2.61061

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.756, [92mTest[0m: 2.927, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.84872
[1mStep[0m  [2/21], [94mLoss[0m : 2.65653
[1mStep[0m  [4/21], [94mLoss[0m : 2.80674
[1mStep[0m  [6/21], [94mLoss[0m : 2.64432
[1mStep[0m  [8/21], [94mLoss[0m : 2.76446
[1mStep[0m  [10/21], [94mLoss[0m : 2.62157
[1mStep[0m  [12/21], [94mLoss[0m : 2.92363
[1mStep[0m  [14/21], [94mLoss[0m : 2.46120
[1mStep[0m  [16/21], [94mLoss[0m : 2.62888
[1mStep[0m  [18/21], [94mLoss[0m : 2.55936
[1mStep[0m  [20/21], [94mLoss[0m : 2.75572

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.684, [92mTest[0m: 2.838, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.93892
[1mStep[0m  [2/21], [94mLoss[0m : 2.41272
[1mStep[0m  [4/21], [94mLoss[0m : 2.51394
[1mStep[0m  [6/21], [94mLoss[0m : 2.81028
[1mStep[0m  [8/21], [94mLoss[0m : 2.69581
[1mStep[0m  [10/21], [94mLoss[0m : 2.58603
[1mStep[0m  [12/21], [94mLoss[0m : 2.51422
[1mStep[0m  [14/21], [94mLoss[0m : 2.72966
[1mStep[0m  [16/21], [94mLoss[0m : 2.64067
[1mStep[0m  [18/21], [94mLoss[0m : 2.54823
[1mStep[0m  [20/21], [94mLoss[0m : 2.73008

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.643, [92mTest[0m: 2.742, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.59912
[1mStep[0m  [2/21], [94mLoss[0m : 2.65261
[1mStep[0m  [4/21], [94mLoss[0m : 2.51230
[1mStep[0m  [6/21], [94mLoss[0m : 2.66894
[1mStep[0m  [8/21], [94mLoss[0m : 2.64448
[1mStep[0m  [10/21], [94mLoss[0m : 2.59965
[1mStep[0m  [12/21], [94mLoss[0m : 2.76085
[1mStep[0m  [14/21], [94mLoss[0m : 2.62926
[1mStep[0m  [16/21], [94mLoss[0m : 2.62888
[1mStep[0m  [18/21], [94mLoss[0m : 2.65124
[1mStep[0m  [20/21], [94mLoss[0m : 2.53864

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.636, [92mTest[0m: 2.708, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.59670
[1mStep[0m  [2/21], [94mLoss[0m : 2.53378
[1mStep[0m  [4/21], [94mLoss[0m : 2.44286
[1mStep[0m  [6/21], [94mLoss[0m : 2.86270
[1mStep[0m  [8/21], [94mLoss[0m : 2.43480
[1mStep[0m  [10/21], [94mLoss[0m : 2.48700
[1mStep[0m  [12/21], [94mLoss[0m : 2.48920
[1mStep[0m  [14/21], [94mLoss[0m : 2.45446
[1mStep[0m  [16/21], [94mLoss[0m : 2.47445
[1mStep[0m  [18/21], [94mLoss[0m : 2.52155
[1mStep[0m  [20/21], [94mLoss[0m : 2.61283

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.579, [92mTest[0m: 2.633, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56365
[1mStep[0m  [2/21], [94mLoss[0m : 2.56390
[1mStep[0m  [4/21], [94mLoss[0m : 2.72996
[1mStep[0m  [6/21], [94mLoss[0m : 2.62952
[1mStep[0m  [8/21], [94mLoss[0m : 2.45974
[1mStep[0m  [10/21], [94mLoss[0m : 2.52762
[1mStep[0m  [12/21], [94mLoss[0m : 2.67707
[1mStep[0m  [14/21], [94mLoss[0m : 2.47805
[1mStep[0m  [16/21], [94mLoss[0m : 2.58677
[1mStep[0m  [18/21], [94mLoss[0m : 2.52806
[1mStep[0m  [20/21], [94mLoss[0m : 2.63171

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.574, [92mTest[0m: 2.603, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.65513
[1mStep[0m  [2/21], [94mLoss[0m : 2.76948
[1mStep[0m  [4/21], [94mLoss[0m : 2.62007
[1mStep[0m  [6/21], [94mLoss[0m : 2.58470
[1mStep[0m  [8/21], [94mLoss[0m : 2.69701
[1mStep[0m  [10/21], [94mLoss[0m : 2.58087
[1mStep[0m  [12/21], [94mLoss[0m : 2.54137
[1mStep[0m  [14/21], [94mLoss[0m : 2.54731
[1mStep[0m  [16/21], [94mLoss[0m : 2.59567
[1mStep[0m  [18/21], [94mLoss[0m : 2.42441
[1mStep[0m  [20/21], [94mLoss[0m : 2.61190

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.576, [92mTest[0m: 2.593, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.61766
[1mStep[0m  [2/21], [94mLoss[0m : 2.48755
[1mStep[0m  [4/21], [94mLoss[0m : 2.75445
[1mStep[0m  [6/21], [94mLoss[0m : 2.65079
[1mStep[0m  [8/21], [94mLoss[0m : 2.49574
[1mStep[0m  [10/21], [94mLoss[0m : 2.60733
[1mStep[0m  [12/21], [94mLoss[0m : 2.48235
[1mStep[0m  [14/21], [94mLoss[0m : 2.73901
[1mStep[0m  [16/21], [94mLoss[0m : 2.49423
[1mStep[0m  [18/21], [94mLoss[0m : 2.61136
[1mStep[0m  [20/21], [94mLoss[0m : 2.67106

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.567, [92mTest[0m: 2.559, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.74261
[1mStep[0m  [2/21], [94mLoss[0m : 2.55344
[1mStep[0m  [4/21], [94mLoss[0m : 2.57145
[1mStep[0m  [6/21], [94mLoss[0m : 2.52563
[1mStep[0m  [8/21], [94mLoss[0m : 2.52762
[1mStep[0m  [10/21], [94mLoss[0m : 2.66484
[1mStep[0m  [12/21], [94mLoss[0m : 2.60739
[1mStep[0m  [14/21], [94mLoss[0m : 2.53045
[1mStep[0m  [16/21], [94mLoss[0m : 2.57882
[1mStep[0m  [18/21], [94mLoss[0m : 2.51667
[1mStep[0m  [20/21], [94mLoss[0m : 2.61459

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.577, [92mTest[0m: 2.540, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.58833
[1mStep[0m  [2/21], [94mLoss[0m : 2.50492
[1mStep[0m  [4/21], [94mLoss[0m : 2.48676
[1mStep[0m  [6/21], [94mLoss[0m : 2.75336
[1mStep[0m  [8/21], [94mLoss[0m : 2.65121
[1mStep[0m  [10/21], [94mLoss[0m : 2.38244
[1mStep[0m  [12/21], [94mLoss[0m : 2.71898
[1mStep[0m  [14/21], [94mLoss[0m : 2.56336
[1mStep[0m  [16/21], [94mLoss[0m : 2.57004
[1mStep[0m  [18/21], [94mLoss[0m : 2.48569
[1mStep[0m  [20/21], [94mLoss[0m : 2.58432

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.564, [92mTest[0m: 2.539, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.57982
[1mStep[0m  [2/21], [94mLoss[0m : 2.34603
[1mStep[0m  [4/21], [94mLoss[0m : 2.55337
[1mStep[0m  [6/21], [94mLoss[0m : 2.72681
[1mStep[0m  [8/21], [94mLoss[0m : 2.36880
[1mStep[0m  [10/21], [94mLoss[0m : 2.72067
[1mStep[0m  [12/21], [94mLoss[0m : 2.55325
[1mStep[0m  [14/21], [94mLoss[0m : 2.46284
[1mStep[0m  [16/21], [94mLoss[0m : 2.70935
[1mStep[0m  [18/21], [94mLoss[0m : 2.52258
[1mStep[0m  [20/21], [94mLoss[0m : 2.49011

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.548, [92mTest[0m: 2.517, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.59617
[1mStep[0m  [2/21], [94mLoss[0m : 2.68041
[1mStep[0m  [4/21], [94mLoss[0m : 2.74251
[1mStep[0m  [6/21], [94mLoss[0m : 2.48148
[1mStep[0m  [8/21], [94mLoss[0m : 2.54699
[1mStep[0m  [10/21], [94mLoss[0m : 2.45649
[1mStep[0m  [12/21], [94mLoss[0m : 2.51956
[1mStep[0m  [14/21], [94mLoss[0m : 2.55096
[1mStep[0m  [16/21], [94mLoss[0m : 2.54345
[1mStep[0m  [18/21], [94mLoss[0m : 2.60755
[1mStep[0m  [20/21], [94mLoss[0m : 2.51898

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.557, [92mTest[0m: 2.506, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.58104
[1mStep[0m  [2/21], [94mLoss[0m : 2.59910
[1mStep[0m  [4/21], [94mLoss[0m : 2.54539
[1mStep[0m  [6/21], [94mLoss[0m : 2.67678
[1mStep[0m  [8/21], [94mLoss[0m : 2.54900
[1mStep[0m  [10/21], [94mLoss[0m : 2.54588
[1mStep[0m  [12/21], [94mLoss[0m : 2.57501
[1mStep[0m  [14/21], [94mLoss[0m : 2.46844
[1mStep[0m  [16/21], [94mLoss[0m : 2.50645
[1mStep[0m  [18/21], [94mLoss[0m : 2.54168
[1mStep[0m  [20/21], [94mLoss[0m : 2.50410

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.557, [92mTest[0m: 2.506, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.61094
[1mStep[0m  [2/21], [94mLoss[0m : 2.54481
[1mStep[0m  [4/21], [94mLoss[0m : 2.58632
[1mStep[0m  [6/21], [94mLoss[0m : 2.54106
[1mStep[0m  [8/21], [94mLoss[0m : 2.41496
[1mStep[0m  [10/21], [94mLoss[0m : 2.48029
[1mStep[0m  [12/21], [94mLoss[0m : 2.53406
[1mStep[0m  [14/21], [94mLoss[0m : 2.66493
[1mStep[0m  [16/21], [94mLoss[0m : 2.61998
[1mStep[0m  [18/21], [94mLoss[0m : 2.66751
[1mStep[0m  [20/21], [94mLoss[0m : 2.44311

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.547, [92mTest[0m: 2.508, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48317
[1mStep[0m  [2/21], [94mLoss[0m : 2.47507
[1mStep[0m  [4/21], [94mLoss[0m : 2.56150
[1mStep[0m  [6/21], [94mLoss[0m : 2.50608
[1mStep[0m  [8/21], [94mLoss[0m : 2.37879
[1mStep[0m  [10/21], [94mLoss[0m : 2.42299
[1mStep[0m  [12/21], [94mLoss[0m : 2.46117
[1mStep[0m  [14/21], [94mLoss[0m : 2.59679
[1mStep[0m  [16/21], [94mLoss[0m : 2.57865
[1mStep[0m  [18/21], [94mLoss[0m : 2.45431
[1mStep[0m  [20/21], [94mLoss[0m : 2.51171

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.497, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.50876
[1mStep[0m  [2/21], [94mLoss[0m : 2.49295
[1mStep[0m  [4/21], [94mLoss[0m : 2.57763
[1mStep[0m  [6/21], [94mLoss[0m : 2.52667
[1mStep[0m  [8/21], [94mLoss[0m : 2.60732
[1mStep[0m  [10/21], [94mLoss[0m : 2.54023
[1mStep[0m  [12/21], [94mLoss[0m : 2.56400
[1mStep[0m  [14/21], [94mLoss[0m : 2.45016
[1mStep[0m  [16/21], [94mLoss[0m : 2.72796
[1mStep[0m  [18/21], [94mLoss[0m : 2.45917
[1mStep[0m  [20/21], [94mLoss[0m : 2.44679

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.503, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.39159
[1mStep[0m  [2/21], [94mLoss[0m : 2.58749
[1mStep[0m  [4/21], [94mLoss[0m : 2.70996
[1mStep[0m  [6/21], [94mLoss[0m : 2.52377
[1mStep[0m  [8/21], [94mLoss[0m : 2.56568
[1mStep[0m  [10/21], [94mLoss[0m : 2.47812
[1mStep[0m  [12/21], [94mLoss[0m : 2.52952
[1mStep[0m  [14/21], [94mLoss[0m : 2.41040
[1mStep[0m  [16/21], [94mLoss[0m : 2.51579
[1mStep[0m  [18/21], [94mLoss[0m : 2.58968
[1mStep[0m  [20/21], [94mLoss[0m : 2.56982

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.534, [92mTest[0m: 2.485, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.68195
[1mStep[0m  [2/21], [94mLoss[0m : 2.41033
[1mStep[0m  [4/21], [94mLoss[0m : 2.49802
[1mStep[0m  [6/21], [94mLoss[0m : 2.62202
[1mStep[0m  [8/21], [94mLoss[0m : 2.49648
[1mStep[0m  [10/21], [94mLoss[0m : 2.48817
[1mStep[0m  [12/21], [94mLoss[0m : 2.50518
[1mStep[0m  [14/21], [94mLoss[0m : 2.57524
[1mStep[0m  [16/21], [94mLoss[0m : 2.48005
[1mStep[0m  [18/21], [94mLoss[0m : 2.61405
[1mStep[0m  [20/21], [94mLoss[0m : 2.57775

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.499, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.492
====================================

Phase 1 - Evaluation MAE:  2.4916588238307407
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 2.50570
[1mStep[0m  [2/21], [94mLoss[0m : 2.52966
[1mStep[0m  [4/21], [94mLoss[0m : 2.31712
[1mStep[0m  [6/21], [94mLoss[0m : 2.68142
[1mStep[0m  [8/21], [94mLoss[0m : 2.59827
[1mStep[0m  [10/21], [94mLoss[0m : 2.53060
[1mStep[0m  [12/21], [94mLoss[0m : 2.74925
[1mStep[0m  [14/21], [94mLoss[0m : 2.48407
[1mStep[0m  [16/21], [94mLoss[0m : 2.46527
[1mStep[0m  [18/21], [94mLoss[0m : 2.69223
[1mStep[0m  [20/21], [94mLoss[0m : 2.54535

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.539, [92mTest[0m: 2.498, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.47087
[1mStep[0m  [2/21], [94mLoss[0m : 2.50439
[1mStep[0m  [4/21], [94mLoss[0m : 2.47506
[1mStep[0m  [6/21], [94mLoss[0m : 2.48069
[1mStep[0m  [8/21], [94mLoss[0m : 2.59097
[1mStep[0m  [10/21], [94mLoss[0m : 2.56009
[1mStep[0m  [12/21], [94mLoss[0m : 2.61296
[1mStep[0m  [14/21], [94mLoss[0m : 2.61966
[1mStep[0m  [16/21], [94mLoss[0m : 2.54365
[1mStep[0m  [18/21], [94mLoss[0m : 2.57690
[1mStep[0m  [20/21], [94mLoss[0m : 2.68589

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.551, [92mTest[0m: 2.898, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54541
[1mStep[0m  [2/21], [94mLoss[0m : 2.73715
[1mStep[0m  [4/21], [94mLoss[0m : 2.50905
[1mStep[0m  [6/21], [94mLoss[0m : 2.28881
[1mStep[0m  [8/21], [94mLoss[0m : 2.73128
[1mStep[0m  [10/21], [94mLoss[0m : 2.53985
[1mStep[0m  [12/21], [94mLoss[0m : 2.56047
[1mStep[0m  [14/21], [94mLoss[0m : 2.54831
[1mStep[0m  [16/21], [94mLoss[0m : 2.43031
[1mStep[0m  [18/21], [94mLoss[0m : 2.41646
[1mStep[0m  [20/21], [94mLoss[0m : 2.42451

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.592, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.61516
[1mStep[0m  [2/21], [94mLoss[0m : 2.51614
[1mStep[0m  [4/21], [94mLoss[0m : 2.69139
[1mStep[0m  [6/21], [94mLoss[0m : 2.70632
[1mStep[0m  [8/21], [94mLoss[0m : 2.47528
[1mStep[0m  [10/21], [94mLoss[0m : 2.59923
[1mStep[0m  [12/21], [94mLoss[0m : 2.49566
[1mStep[0m  [14/21], [94mLoss[0m : 2.45565
[1mStep[0m  [16/21], [94mLoss[0m : 2.58063
[1mStep[0m  [18/21], [94mLoss[0m : 2.55418
[1mStep[0m  [20/21], [94mLoss[0m : 2.65269

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.424, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.50187
[1mStep[0m  [2/21], [94mLoss[0m : 2.56141
[1mStep[0m  [4/21], [94mLoss[0m : 2.54966
[1mStep[0m  [6/21], [94mLoss[0m : 2.60931
[1mStep[0m  [8/21], [94mLoss[0m : 2.63313
[1mStep[0m  [10/21], [94mLoss[0m : 2.48040
[1mStep[0m  [12/21], [94mLoss[0m : 2.50742
[1mStep[0m  [14/21], [94mLoss[0m : 2.61559
[1mStep[0m  [16/21], [94mLoss[0m : 2.49708
[1mStep[0m  [18/21], [94mLoss[0m : 2.48181
[1mStep[0m  [20/21], [94mLoss[0m : 2.45500

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.481, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42953
[1mStep[0m  [2/21], [94mLoss[0m : 2.60418
[1mStep[0m  [4/21], [94mLoss[0m : 2.41306
[1mStep[0m  [6/21], [94mLoss[0m : 2.48532
[1mStep[0m  [8/21], [94mLoss[0m : 2.46185
[1mStep[0m  [10/21], [94mLoss[0m : 2.49042
[1mStep[0m  [12/21], [94mLoss[0m : 2.46456
[1mStep[0m  [14/21], [94mLoss[0m : 2.40542
[1mStep[0m  [16/21], [94mLoss[0m : 2.62322
[1mStep[0m  [18/21], [94mLoss[0m : 2.35609
[1mStep[0m  [20/21], [94mLoss[0m : 2.58567

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.508, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53017
[1mStep[0m  [2/21], [94mLoss[0m : 2.35474
[1mStep[0m  [4/21], [94mLoss[0m : 2.50442
[1mStep[0m  [6/21], [94mLoss[0m : 2.44177
[1mStep[0m  [8/21], [94mLoss[0m : 2.55444
[1mStep[0m  [10/21], [94mLoss[0m : 2.51461
[1mStep[0m  [12/21], [94mLoss[0m : 2.46226
[1mStep[0m  [14/21], [94mLoss[0m : 2.44965
[1mStep[0m  [16/21], [94mLoss[0m : 2.50408
[1mStep[0m  [18/21], [94mLoss[0m : 2.55607
[1mStep[0m  [20/21], [94mLoss[0m : 2.40686

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.523, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.38692
[1mStep[0m  [2/21], [94mLoss[0m : 2.53550
[1mStep[0m  [4/21], [94mLoss[0m : 2.51235
[1mStep[0m  [6/21], [94mLoss[0m : 2.53620
[1mStep[0m  [8/21], [94mLoss[0m : 2.49693
[1mStep[0m  [10/21], [94mLoss[0m : 2.44807
[1mStep[0m  [12/21], [94mLoss[0m : 2.44220
[1mStep[0m  [14/21], [94mLoss[0m : 2.56742
[1mStep[0m  [16/21], [94mLoss[0m : 2.63383
[1mStep[0m  [18/21], [94mLoss[0m : 2.47876
[1mStep[0m  [20/21], [94mLoss[0m : 2.44844

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.475, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.38051
[1mStep[0m  [2/21], [94mLoss[0m : 2.51898
[1mStep[0m  [4/21], [94mLoss[0m : 2.41045
[1mStep[0m  [6/21], [94mLoss[0m : 2.43850
[1mStep[0m  [8/21], [94mLoss[0m : 2.28715
[1mStep[0m  [10/21], [94mLoss[0m : 2.48311
[1mStep[0m  [12/21], [94mLoss[0m : 2.41618
[1mStep[0m  [14/21], [94mLoss[0m : 2.50677
[1mStep[0m  [16/21], [94mLoss[0m : 2.57181
[1mStep[0m  [18/21], [94mLoss[0m : 2.36234
[1mStep[0m  [20/21], [94mLoss[0m : 2.39084

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.552, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44866
[1mStep[0m  [2/21], [94mLoss[0m : 2.55594
[1mStep[0m  [4/21], [94mLoss[0m : 2.39564
[1mStep[0m  [6/21], [94mLoss[0m : 2.37850
[1mStep[0m  [8/21], [94mLoss[0m : 2.51310
[1mStep[0m  [10/21], [94mLoss[0m : 2.51255
[1mStep[0m  [12/21], [94mLoss[0m : 2.52171
[1mStep[0m  [14/21], [94mLoss[0m : 2.44442
[1mStep[0m  [16/21], [94mLoss[0m : 2.34152
[1mStep[0m  [18/21], [94mLoss[0m : 2.52141
[1mStep[0m  [20/21], [94mLoss[0m : 2.44971

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.496, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48265
[1mStep[0m  [2/21], [94mLoss[0m : 2.43334
[1mStep[0m  [4/21], [94mLoss[0m : 2.48587
[1mStep[0m  [6/21], [94mLoss[0m : 2.38101
[1mStep[0m  [8/21], [94mLoss[0m : 2.28564
[1mStep[0m  [10/21], [94mLoss[0m : 2.41112
[1mStep[0m  [12/21], [94mLoss[0m : 2.32147
[1mStep[0m  [14/21], [94mLoss[0m : 2.56106
[1mStep[0m  [16/21], [94mLoss[0m : 2.41374
[1mStep[0m  [18/21], [94mLoss[0m : 2.21986
[1mStep[0m  [20/21], [94mLoss[0m : 2.59967

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.528, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53120
[1mStep[0m  [2/21], [94mLoss[0m : 2.38145
[1mStep[0m  [4/21], [94mLoss[0m : 2.52573
[1mStep[0m  [6/21], [94mLoss[0m : 2.31175
[1mStep[0m  [8/21], [94mLoss[0m : 2.54546
[1mStep[0m  [10/21], [94mLoss[0m : 2.50406
[1mStep[0m  [12/21], [94mLoss[0m : 2.37799
[1mStep[0m  [14/21], [94mLoss[0m : 2.39960
[1mStep[0m  [16/21], [94mLoss[0m : 2.53914
[1mStep[0m  [18/21], [94mLoss[0m : 2.31649
[1mStep[0m  [20/21], [94mLoss[0m : 2.38365

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.692, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53634
[1mStep[0m  [2/21], [94mLoss[0m : 2.46920
[1mStep[0m  [4/21], [94mLoss[0m : 2.35280
[1mStep[0m  [6/21], [94mLoss[0m : 2.41266
[1mStep[0m  [8/21], [94mLoss[0m : 2.33056
[1mStep[0m  [10/21], [94mLoss[0m : 2.51035
[1mStep[0m  [12/21], [94mLoss[0m : 2.54865
[1mStep[0m  [14/21], [94mLoss[0m : 2.39239
[1mStep[0m  [16/21], [94mLoss[0m : 2.47743
[1mStep[0m  [18/21], [94mLoss[0m : 2.52627
[1mStep[0m  [20/21], [94mLoss[0m : 2.25265

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.684, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.34949
[1mStep[0m  [2/21], [94mLoss[0m : 2.34256
[1mStep[0m  [4/21], [94mLoss[0m : 2.39635
[1mStep[0m  [6/21], [94mLoss[0m : 2.31214
[1mStep[0m  [8/21], [94mLoss[0m : 2.33927
[1mStep[0m  [10/21], [94mLoss[0m : 2.30292
[1mStep[0m  [12/21], [94mLoss[0m : 2.46388
[1mStep[0m  [14/21], [94mLoss[0m : 2.57629
[1mStep[0m  [16/21], [94mLoss[0m : 2.30553
[1mStep[0m  [18/21], [94mLoss[0m : 2.45225
[1mStep[0m  [20/21], [94mLoss[0m : 2.36156

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.401, [92mTest[0m: 2.586, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42205
[1mStep[0m  [2/21], [94mLoss[0m : 2.45316
[1mStep[0m  [4/21], [94mLoss[0m : 2.37089
[1mStep[0m  [6/21], [94mLoss[0m : 2.44717
[1mStep[0m  [8/21], [94mLoss[0m : 2.38026
[1mStep[0m  [10/21], [94mLoss[0m : 2.35865
[1mStep[0m  [12/21], [94mLoss[0m : 2.26713
[1mStep[0m  [14/21], [94mLoss[0m : 2.44141
[1mStep[0m  [16/21], [94mLoss[0m : 2.34461
[1mStep[0m  [18/21], [94mLoss[0m : 2.42134
[1mStep[0m  [20/21], [94mLoss[0m : 2.39049

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.390, [92mTest[0m: 2.632, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.38794
[1mStep[0m  [2/21], [94mLoss[0m : 2.36356
[1mStep[0m  [4/21], [94mLoss[0m : 2.26131
[1mStep[0m  [6/21], [94mLoss[0m : 2.27895
[1mStep[0m  [8/21], [94mLoss[0m : 2.47329
[1mStep[0m  [10/21], [94mLoss[0m : 2.41639
[1mStep[0m  [12/21], [94mLoss[0m : 2.52673
[1mStep[0m  [14/21], [94mLoss[0m : 2.44852
[1mStep[0m  [16/21], [94mLoss[0m : 2.42782
[1mStep[0m  [18/21], [94mLoss[0m : 2.26851
[1mStep[0m  [20/21], [94mLoss[0m : 2.30340

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.393, [92mTest[0m: 2.560, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40560
[1mStep[0m  [2/21], [94mLoss[0m : 2.43945
[1mStep[0m  [4/21], [94mLoss[0m : 2.39795
[1mStep[0m  [6/21], [94mLoss[0m : 2.38189
[1mStep[0m  [8/21], [94mLoss[0m : 2.26886
[1mStep[0m  [10/21], [94mLoss[0m : 2.39078
[1mStep[0m  [12/21], [94mLoss[0m : 2.37258
[1mStep[0m  [14/21], [94mLoss[0m : 2.37303
[1mStep[0m  [16/21], [94mLoss[0m : 2.39477
[1mStep[0m  [18/21], [94mLoss[0m : 2.32234
[1mStep[0m  [20/21], [94mLoss[0m : 2.32659

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.361, [92mTest[0m: 2.581, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.50717
[1mStep[0m  [2/21], [94mLoss[0m : 2.37524
[1mStep[0m  [4/21], [94mLoss[0m : 2.39531
[1mStep[0m  [6/21], [94mLoss[0m : 2.57120
[1mStep[0m  [8/21], [94mLoss[0m : 2.34011
[1mStep[0m  [10/21], [94mLoss[0m : 2.27681
[1mStep[0m  [12/21], [94mLoss[0m : 2.48453
[1mStep[0m  [14/21], [94mLoss[0m : 2.34010
[1mStep[0m  [16/21], [94mLoss[0m : 2.31585
[1mStep[0m  [18/21], [94mLoss[0m : 2.34464
[1mStep[0m  [20/21], [94mLoss[0m : 2.39403

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.370, [92mTest[0m: 2.648, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.22605
[1mStep[0m  [2/21], [94mLoss[0m : 2.35342
[1mStep[0m  [4/21], [94mLoss[0m : 2.43909
[1mStep[0m  [6/21], [94mLoss[0m : 2.44902
[1mStep[0m  [8/21], [94mLoss[0m : 2.37385
[1mStep[0m  [10/21], [94mLoss[0m : 2.36328
[1mStep[0m  [12/21], [94mLoss[0m : 2.44391
[1mStep[0m  [14/21], [94mLoss[0m : 2.23413
[1mStep[0m  [16/21], [94mLoss[0m : 2.34706
[1mStep[0m  [18/21], [94mLoss[0m : 2.37589
[1mStep[0m  [20/21], [94mLoss[0m : 2.31581

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.358, [92mTest[0m: 2.543, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.25291
[1mStep[0m  [2/21], [94mLoss[0m : 2.29290
[1mStep[0m  [4/21], [94mLoss[0m : 2.48886
[1mStep[0m  [6/21], [94mLoss[0m : 2.34186
[1mStep[0m  [8/21], [94mLoss[0m : 2.25992
[1mStep[0m  [10/21], [94mLoss[0m : 2.27998
[1mStep[0m  [12/21], [94mLoss[0m : 2.46991
[1mStep[0m  [14/21], [94mLoss[0m : 2.26226
[1mStep[0m  [16/21], [94mLoss[0m : 2.28325
[1mStep[0m  [18/21], [94mLoss[0m : 2.33838
[1mStep[0m  [20/21], [94mLoss[0m : 2.26786

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.315, [92mTest[0m: 2.560, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.24825
[1mStep[0m  [2/21], [94mLoss[0m : 2.44681
[1mStep[0m  [4/21], [94mLoss[0m : 2.34311
[1mStep[0m  [6/21], [94mLoss[0m : 2.29906
[1mStep[0m  [8/21], [94mLoss[0m : 2.12810
[1mStep[0m  [10/21], [94mLoss[0m : 2.32888
[1mStep[0m  [12/21], [94mLoss[0m : 2.49518
[1mStep[0m  [14/21], [94mLoss[0m : 2.33900
[1mStep[0m  [16/21], [94mLoss[0m : 2.35038
[1mStep[0m  [18/21], [94mLoss[0m : 2.29841
[1mStep[0m  [20/21], [94mLoss[0m : 2.44230

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.327, [92mTest[0m: 2.561, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.24784
[1mStep[0m  [2/21], [94mLoss[0m : 2.31752
[1mStep[0m  [4/21], [94mLoss[0m : 2.33655
[1mStep[0m  [6/21], [94mLoss[0m : 2.12673
[1mStep[0m  [8/21], [94mLoss[0m : 2.53680
[1mStep[0m  [10/21], [94mLoss[0m : 2.27730
[1mStep[0m  [12/21], [94mLoss[0m : 2.19888
[1mStep[0m  [14/21], [94mLoss[0m : 2.30011
[1mStep[0m  [16/21], [94mLoss[0m : 2.30737
[1mStep[0m  [18/21], [94mLoss[0m : 2.45875
[1mStep[0m  [20/21], [94mLoss[0m : 2.28934

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.329, [92mTest[0m: 2.511, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40893
[1mStep[0m  [2/21], [94mLoss[0m : 2.20921
[1mStep[0m  [4/21], [94mLoss[0m : 2.42791
[1mStep[0m  [6/21], [94mLoss[0m : 2.53642
[1mStep[0m  [8/21], [94mLoss[0m : 2.30427
[1mStep[0m  [10/21], [94mLoss[0m : 2.35836
[1mStep[0m  [12/21], [94mLoss[0m : 2.23126
[1mStep[0m  [14/21], [94mLoss[0m : 2.37045
[1mStep[0m  [16/21], [94mLoss[0m : 2.22263
[1mStep[0m  [18/21], [94mLoss[0m : 2.23174
[1mStep[0m  [20/21], [94mLoss[0m : 2.33913

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.316, [92mTest[0m: 2.603, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.17881
[1mStep[0m  [2/21], [94mLoss[0m : 2.38804
[1mStep[0m  [4/21], [94mLoss[0m : 2.33509
[1mStep[0m  [6/21], [94mLoss[0m : 2.21712
[1mStep[0m  [8/21], [94mLoss[0m : 2.29126
[1mStep[0m  [10/21], [94mLoss[0m : 2.30531
[1mStep[0m  [12/21], [94mLoss[0m : 2.35394
[1mStep[0m  [14/21], [94mLoss[0m : 2.21949
[1mStep[0m  [16/21], [94mLoss[0m : 2.31881
[1mStep[0m  [18/21], [94mLoss[0m : 2.31514
[1mStep[0m  [20/21], [94mLoss[0m : 2.32796

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.317, [92mTest[0m: 2.564, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.25920
[1mStep[0m  [2/21], [94mLoss[0m : 2.19565
[1mStep[0m  [4/21], [94mLoss[0m : 2.34286
[1mStep[0m  [6/21], [94mLoss[0m : 2.12529
[1mStep[0m  [8/21], [94mLoss[0m : 2.40450
[1mStep[0m  [10/21], [94mLoss[0m : 2.35346
[1mStep[0m  [12/21], [94mLoss[0m : 2.28683
[1mStep[0m  [14/21], [94mLoss[0m : 2.23739
[1mStep[0m  [16/21], [94mLoss[0m : 2.38319
[1mStep[0m  [18/21], [94mLoss[0m : 2.33521
[1mStep[0m  [20/21], [94mLoss[0m : 2.36775

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.295, [92mTest[0m: 2.521, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42410
[1mStep[0m  [2/21], [94mLoss[0m : 2.22565
[1mStep[0m  [4/21], [94mLoss[0m : 2.40443
[1mStep[0m  [6/21], [94mLoss[0m : 2.27033
[1mStep[0m  [8/21], [94mLoss[0m : 2.32965
[1mStep[0m  [10/21], [94mLoss[0m : 2.35006
[1mStep[0m  [12/21], [94mLoss[0m : 2.33693
[1mStep[0m  [14/21], [94mLoss[0m : 2.26568
[1mStep[0m  [16/21], [94mLoss[0m : 2.32448
[1mStep[0m  [18/21], [94mLoss[0m : 2.33941
[1mStep[0m  [20/21], [94mLoss[0m : 2.23151

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.283, [92mTest[0m: 2.588, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.30262
[1mStep[0m  [2/21], [94mLoss[0m : 2.31610
[1mStep[0m  [4/21], [94mLoss[0m : 2.22733
[1mStep[0m  [6/21], [94mLoss[0m : 2.44273
[1mStep[0m  [8/21], [94mLoss[0m : 2.34807
[1mStep[0m  [10/21], [94mLoss[0m : 2.27910
[1mStep[0m  [12/21], [94mLoss[0m : 2.17755
[1mStep[0m  [14/21], [94mLoss[0m : 2.40740
[1mStep[0m  [16/21], [94mLoss[0m : 2.35404
[1mStep[0m  [18/21], [94mLoss[0m : 2.33115
[1mStep[0m  [20/21], [94mLoss[0m : 2.28586

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.262, [92mTest[0m: 2.546, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.38598
[1mStep[0m  [2/21], [94mLoss[0m : 2.30542
[1mStep[0m  [4/21], [94mLoss[0m : 2.15357
[1mStep[0m  [6/21], [94mLoss[0m : 2.33895
[1mStep[0m  [8/21], [94mLoss[0m : 2.11587
[1mStep[0m  [10/21], [94mLoss[0m : 2.11000
[1mStep[0m  [12/21], [94mLoss[0m : 2.33933
[1mStep[0m  [14/21], [94mLoss[0m : 2.40846
[1mStep[0m  [16/21], [94mLoss[0m : 2.17154
[1mStep[0m  [18/21], [94mLoss[0m : 2.26379
[1mStep[0m  [20/21], [94mLoss[0m : 2.20775

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.260, [92mTest[0m: 2.573, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.17830
[1mStep[0m  [2/21], [94mLoss[0m : 2.23025
[1mStep[0m  [4/21], [94mLoss[0m : 2.09937
[1mStep[0m  [6/21], [94mLoss[0m : 2.32036
[1mStep[0m  [8/21], [94mLoss[0m : 2.26104
[1mStep[0m  [10/21], [94mLoss[0m : 2.11814
[1mStep[0m  [12/21], [94mLoss[0m : 2.19784
[1mStep[0m  [14/21], [94mLoss[0m : 2.18840
[1mStep[0m  [16/21], [94mLoss[0m : 2.25517
[1mStep[0m  [18/21], [94mLoss[0m : 2.28092
[1mStep[0m  [20/21], [94mLoss[0m : 2.22778

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.225, [92mTest[0m: 2.522, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.27522
[1mStep[0m  [2/21], [94mLoss[0m : 2.30400
[1mStep[0m  [4/21], [94mLoss[0m : 2.30253
[1mStep[0m  [6/21], [94mLoss[0m : 2.25773
[1mStep[0m  [8/21], [94mLoss[0m : 2.27037
[1mStep[0m  [10/21], [94mLoss[0m : 2.23373
[1mStep[0m  [12/21], [94mLoss[0m : 2.23902
[1mStep[0m  [14/21], [94mLoss[0m : 2.29556
[1mStep[0m  [16/21], [94mLoss[0m : 2.35023
[1mStep[0m  [18/21], [94mLoss[0m : 2.21965
[1mStep[0m  [20/21], [94mLoss[0m : 2.35299

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.262, [92mTest[0m: 2.520, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.527
====================================

Phase 2 - Evaluation MAE:  2.5273280143737793
MAE score P1      2.491659
MAE score P2      2.527328
loss              2.224655
learning_rate     0.002575
batch_size             512
hidden_sizes         [100]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.1
weight_decay        0.0001
Name: 23, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 10.62940
[1mStep[0m  [4/42], [94mLoss[0m : 11.07277
[1mStep[0m  [8/42], [94mLoss[0m : 10.76066
[1mStep[0m  [12/42], [94mLoss[0m : 10.96221
[1mStep[0m  [16/42], [94mLoss[0m : 10.81915
[1mStep[0m  [20/42], [94mLoss[0m : 10.84678
[1mStep[0m  [24/42], [94mLoss[0m : 10.49152
[1mStep[0m  [28/42], [94mLoss[0m : 11.07516
[1mStep[0m  [32/42], [94mLoss[0m : 11.10958
[1mStep[0m  [36/42], [94mLoss[0m : 11.06569
[1mStep[0m  [40/42], [94mLoss[0m : 11.21678

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.816, [92mTest[0m: 10.960, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.99812
[1mStep[0m  [4/42], [94mLoss[0m : 10.82690
[1mStep[0m  [8/42], [94mLoss[0m : 10.92830
[1mStep[0m  [12/42], [94mLoss[0m : 10.33748
[1mStep[0m  [16/42], [94mLoss[0m : 10.78499
[1mStep[0m  [20/42], [94mLoss[0m : 11.29974
[1mStep[0m  [24/42], [94mLoss[0m : 10.75877
[1mStep[0m  [28/42], [94mLoss[0m : 10.39764
[1mStep[0m  [32/42], [94mLoss[0m : 10.80183
[1mStep[0m  [36/42], [94mLoss[0m : 10.17480
[1mStep[0m  [40/42], [94mLoss[0m : 10.38597

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.658, [92mTest[0m: 10.705, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.75839
[1mStep[0m  [4/42], [94mLoss[0m : 10.46525
[1mStep[0m  [8/42], [94mLoss[0m : 10.21187
[1mStep[0m  [12/42], [94mLoss[0m : 10.58777
[1mStep[0m  [16/42], [94mLoss[0m : 10.22918
[1mStep[0m  [20/42], [94mLoss[0m : 10.69511
[1mStep[0m  [24/42], [94mLoss[0m : 10.97577
[1mStep[0m  [28/42], [94mLoss[0m : 10.23966
[1mStep[0m  [32/42], [94mLoss[0m : 10.25502
[1mStep[0m  [36/42], [94mLoss[0m : 10.66827
[1mStep[0m  [40/42], [94mLoss[0m : 10.26940

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.519, [92mTest[0m: 10.516, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.08498
[1mStep[0m  [4/42], [94mLoss[0m : 10.51803
[1mStep[0m  [8/42], [94mLoss[0m : 10.28135
[1mStep[0m  [12/42], [94mLoss[0m : 10.16322
[1mStep[0m  [16/42], [94mLoss[0m : 10.44765
[1mStep[0m  [20/42], [94mLoss[0m : 10.47416
[1mStep[0m  [24/42], [94mLoss[0m : 10.37538
[1mStep[0m  [28/42], [94mLoss[0m : 10.40776
[1mStep[0m  [32/42], [94mLoss[0m : 10.10006
[1mStep[0m  [36/42], [94mLoss[0m : 10.25029
[1mStep[0m  [40/42], [94mLoss[0m : 10.16311

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.358, [92mTest[0m: 10.320, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.69888
[1mStep[0m  [4/42], [94mLoss[0m : 10.36818
[1mStep[0m  [8/42], [94mLoss[0m : 10.31569
[1mStep[0m  [12/42], [94mLoss[0m : 10.14233
[1mStep[0m  [16/42], [94mLoss[0m : 10.23915
[1mStep[0m  [20/42], [94mLoss[0m : 10.14372
[1mStep[0m  [24/42], [94mLoss[0m : 10.22243
[1mStep[0m  [28/42], [94mLoss[0m : 9.98533
[1mStep[0m  [32/42], [94mLoss[0m : 10.06644
[1mStep[0m  [36/42], [94mLoss[0m : 10.11083
[1mStep[0m  [40/42], [94mLoss[0m : 9.99892

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.196, [92mTest[0m: 10.142, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.29126
[1mStep[0m  [4/42], [94mLoss[0m : 10.07959
[1mStep[0m  [8/42], [94mLoss[0m : 10.01073
[1mStep[0m  [12/42], [94mLoss[0m : 9.73790
[1mStep[0m  [16/42], [94mLoss[0m : 10.12740
[1mStep[0m  [20/42], [94mLoss[0m : 9.96662
[1mStep[0m  [24/42], [94mLoss[0m : 9.56067
[1mStep[0m  [28/42], [94mLoss[0m : 9.86184
[1mStep[0m  [32/42], [94mLoss[0m : 10.39434
[1mStep[0m  [36/42], [94mLoss[0m : 9.91673
[1mStep[0m  [40/42], [94mLoss[0m : 9.85002

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.038, [92mTest[0m: 9.932, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.77476
[1mStep[0m  [4/42], [94mLoss[0m : 9.80445
[1mStep[0m  [8/42], [94mLoss[0m : 9.95499
[1mStep[0m  [12/42], [94mLoss[0m : 10.12400
[1mStep[0m  [16/42], [94mLoss[0m : 9.47887
[1mStep[0m  [20/42], [94mLoss[0m : 9.86234
[1mStep[0m  [24/42], [94mLoss[0m : 9.55264
[1mStep[0m  [28/42], [94mLoss[0m : 9.62749
[1mStep[0m  [32/42], [94mLoss[0m : 9.75227
[1mStep[0m  [36/42], [94mLoss[0m : 9.76743
[1mStep[0m  [40/42], [94mLoss[0m : 9.89485

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 9.869, [92mTest[0m: 9.771, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.48078
[1mStep[0m  [4/42], [94mLoss[0m : 9.83242
[1mStep[0m  [8/42], [94mLoss[0m : 9.83791
[1mStep[0m  [12/42], [94mLoss[0m : 9.85942
[1mStep[0m  [16/42], [94mLoss[0m : 9.46675
[1mStep[0m  [20/42], [94mLoss[0m : 9.55882
[1mStep[0m  [24/42], [94mLoss[0m : 9.75280
[1mStep[0m  [28/42], [94mLoss[0m : 9.86976
[1mStep[0m  [32/42], [94mLoss[0m : 9.76596
[1mStep[0m  [36/42], [94mLoss[0m : 9.51815
[1mStep[0m  [40/42], [94mLoss[0m : 9.67111

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 9.698, [92mTest[0m: 9.562, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.76638
[1mStep[0m  [4/42], [94mLoss[0m : 9.32603
[1mStep[0m  [8/42], [94mLoss[0m : 9.22882
[1mStep[0m  [12/42], [94mLoss[0m : 9.36637
[1mStep[0m  [16/42], [94mLoss[0m : 9.47255
[1mStep[0m  [20/42], [94mLoss[0m : 10.00445
[1mStep[0m  [24/42], [94mLoss[0m : 9.51590
[1mStep[0m  [28/42], [94mLoss[0m : 9.51592
[1mStep[0m  [32/42], [94mLoss[0m : 9.30279
[1mStep[0m  [36/42], [94mLoss[0m : 9.35092
[1mStep[0m  [40/42], [94mLoss[0m : 9.44336

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 9.504, [92mTest[0m: 9.373, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.74887
[1mStep[0m  [4/42], [94mLoss[0m : 8.96546
[1mStep[0m  [8/42], [94mLoss[0m : 9.08146
[1mStep[0m  [12/42], [94mLoss[0m : 9.43164
[1mStep[0m  [16/42], [94mLoss[0m : 9.11560
[1mStep[0m  [20/42], [94mLoss[0m : 9.50409
[1mStep[0m  [24/42], [94mLoss[0m : 8.69806
[1mStep[0m  [28/42], [94mLoss[0m : 9.30338
[1mStep[0m  [32/42], [94mLoss[0m : 9.29753
[1mStep[0m  [36/42], [94mLoss[0m : 9.36238
[1mStep[0m  [40/42], [94mLoss[0m : 9.01422

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 9.306, [92mTest[0m: 9.127, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.88757
[1mStep[0m  [4/42], [94mLoss[0m : 9.39152
[1mStep[0m  [8/42], [94mLoss[0m : 9.09831
[1mStep[0m  [12/42], [94mLoss[0m : 9.11469
[1mStep[0m  [16/42], [94mLoss[0m : 9.04053
[1mStep[0m  [20/42], [94mLoss[0m : 9.40627
[1mStep[0m  [24/42], [94mLoss[0m : 8.93616
[1mStep[0m  [28/42], [94mLoss[0m : 9.05321
[1mStep[0m  [32/42], [94mLoss[0m : 9.09700
[1mStep[0m  [36/42], [94mLoss[0m : 8.98078
[1mStep[0m  [40/42], [94mLoss[0m : 8.96060

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 9.095, [92mTest[0m: 8.863, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.71376
[1mStep[0m  [4/42], [94mLoss[0m : 8.82615
[1mStep[0m  [8/42], [94mLoss[0m : 8.81689
[1mStep[0m  [12/42], [94mLoss[0m : 8.65096
[1mStep[0m  [16/42], [94mLoss[0m : 8.71141
[1mStep[0m  [20/42], [94mLoss[0m : 9.13537
[1mStep[0m  [24/42], [94mLoss[0m : 8.84176
[1mStep[0m  [28/42], [94mLoss[0m : 8.67264
[1mStep[0m  [32/42], [94mLoss[0m : 8.51207
[1mStep[0m  [36/42], [94mLoss[0m : 8.99518
[1mStep[0m  [40/42], [94mLoss[0m : 8.38225

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 8.875, [92mTest[0m: 8.629, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.19173
[1mStep[0m  [4/42], [94mLoss[0m : 8.51167
[1mStep[0m  [8/42], [94mLoss[0m : 8.78625
[1mStep[0m  [12/42], [94mLoss[0m : 8.21293
[1mStep[0m  [16/42], [94mLoss[0m : 8.41781
[1mStep[0m  [20/42], [94mLoss[0m : 8.75670
[1mStep[0m  [24/42], [94mLoss[0m : 8.47341
[1mStep[0m  [28/42], [94mLoss[0m : 8.36543
[1mStep[0m  [32/42], [94mLoss[0m : 8.93817
[1mStep[0m  [36/42], [94mLoss[0m : 8.70254
[1mStep[0m  [40/42], [94mLoss[0m : 8.80670

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 8.616, [92mTest[0m: 8.359, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.82080
[1mStep[0m  [4/42], [94mLoss[0m : 8.38056
[1mStep[0m  [8/42], [94mLoss[0m : 8.41434
[1mStep[0m  [12/42], [94mLoss[0m : 8.34690
[1mStep[0m  [16/42], [94mLoss[0m : 8.08890
[1mStep[0m  [20/42], [94mLoss[0m : 8.15334
[1mStep[0m  [24/42], [94mLoss[0m : 8.62827
[1mStep[0m  [28/42], [94mLoss[0m : 8.23903
[1mStep[0m  [32/42], [94mLoss[0m : 8.36494
[1mStep[0m  [36/42], [94mLoss[0m : 8.12455
[1mStep[0m  [40/42], [94mLoss[0m : 8.31258

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 8.344, [92mTest[0m: 8.055, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.11525
[1mStep[0m  [4/42], [94mLoss[0m : 8.35051
[1mStep[0m  [8/42], [94mLoss[0m : 8.11582
[1mStep[0m  [12/42], [94mLoss[0m : 7.99165
[1mStep[0m  [16/42], [94mLoss[0m : 7.99448
[1mStep[0m  [20/42], [94mLoss[0m : 8.67801
[1mStep[0m  [24/42], [94mLoss[0m : 7.71983
[1mStep[0m  [28/42], [94mLoss[0m : 8.02390
[1mStep[0m  [32/42], [94mLoss[0m : 7.51867
[1mStep[0m  [36/42], [94mLoss[0m : 7.58079
[1mStep[0m  [40/42], [94mLoss[0m : 8.49798

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 8.047, [92mTest[0m: 7.673, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.73710
[1mStep[0m  [4/42], [94mLoss[0m : 8.24244
[1mStep[0m  [8/42], [94mLoss[0m : 7.55545
[1mStep[0m  [12/42], [94mLoss[0m : 7.82008
[1mStep[0m  [16/42], [94mLoss[0m : 7.74550
[1mStep[0m  [20/42], [94mLoss[0m : 8.15219
[1mStep[0m  [24/42], [94mLoss[0m : 7.53297
[1mStep[0m  [28/42], [94mLoss[0m : 7.51196
[1mStep[0m  [32/42], [94mLoss[0m : 8.06621
[1mStep[0m  [36/42], [94mLoss[0m : 7.40914
[1mStep[0m  [40/42], [94mLoss[0m : 7.53738

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 7.724, [92mTest[0m: 7.330, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.54340
[1mStep[0m  [4/42], [94mLoss[0m : 7.58178
[1mStep[0m  [8/42], [94mLoss[0m : 7.73656
[1mStep[0m  [12/42], [94mLoss[0m : 7.74732
[1mStep[0m  [16/42], [94mLoss[0m : 7.61706
[1mStep[0m  [20/42], [94mLoss[0m : 7.58195
[1mStep[0m  [24/42], [94mLoss[0m : 7.62596
[1mStep[0m  [28/42], [94mLoss[0m : 7.69756
[1mStep[0m  [32/42], [94mLoss[0m : 7.12597
[1mStep[0m  [36/42], [94mLoss[0m : 7.17809
[1mStep[0m  [40/42], [94mLoss[0m : 7.28366

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 7.403, [92mTest[0m: 6.909, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.15791
[1mStep[0m  [4/42], [94mLoss[0m : 7.14576
[1mStep[0m  [8/42], [94mLoss[0m : 7.38687
[1mStep[0m  [12/42], [94mLoss[0m : 7.44423
[1mStep[0m  [16/42], [94mLoss[0m : 7.18800
[1mStep[0m  [20/42], [94mLoss[0m : 6.97860
[1mStep[0m  [24/42], [94mLoss[0m : 7.00425
[1mStep[0m  [28/42], [94mLoss[0m : 6.93259
[1mStep[0m  [32/42], [94mLoss[0m : 7.05150
[1mStep[0m  [36/42], [94mLoss[0m : 6.79890
[1mStep[0m  [40/42], [94mLoss[0m : 6.71402

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 7.084, [92mTest[0m: 6.550, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.87865
[1mStep[0m  [4/42], [94mLoss[0m : 6.62139
[1mStep[0m  [8/42], [94mLoss[0m : 7.25060
[1mStep[0m  [12/42], [94mLoss[0m : 6.55692
[1mStep[0m  [16/42], [94mLoss[0m : 6.72759
[1mStep[0m  [20/42], [94mLoss[0m : 6.60185
[1mStep[0m  [24/42], [94mLoss[0m : 6.60004
[1mStep[0m  [28/42], [94mLoss[0m : 6.71267
[1mStep[0m  [32/42], [94mLoss[0m : 6.69754
[1mStep[0m  [36/42], [94mLoss[0m : 6.54189
[1mStep[0m  [40/42], [94mLoss[0m : 6.73639

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 6.750, [92mTest[0m: 6.224, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.62889
[1mStep[0m  [4/42], [94mLoss[0m : 6.70990
[1mStep[0m  [8/42], [94mLoss[0m : 6.35967
[1mStep[0m  [12/42], [94mLoss[0m : 6.73899
[1mStep[0m  [16/42], [94mLoss[0m : 6.31439
[1mStep[0m  [20/42], [94mLoss[0m : 6.39471
[1mStep[0m  [24/42], [94mLoss[0m : 6.56220
[1mStep[0m  [28/42], [94mLoss[0m : 6.32006
[1mStep[0m  [32/42], [94mLoss[0m : 5.92549
[1mStep[0m  [36/42], [94mLoss[0m : 6.51596
[1mStep[0m  [40/42], [94mLoss[0m : 5.97246

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 6.434, [92mTest[0m: 5.892, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.02101
[1mStep[0m  [4/42], [94mLoss[0m : 6.28606
[1mStep[0m  [8/42], [94mLoss[0m : 5.86651
[1mStep[0m  [12/42], [94mLoss[0m : 6.27774
[1mStep[0m  [16/42], [94mLoss[0m : 6.11730
[1mStep[0m  [20/42], [94mLoss[0m : 5.85010
[1mStep[0m  [24/42], [94mLoss[0m : 6.17982
[1mStep[0m  [28/42], [94mLoss[0m : 6.28238
[1mStep[0m  [32/42], [94mLoss[0m : 6.14533
[1mStep[0m  [36/42], [94mLoss[0m : 5.66039
[1mStep[0m  [40/42], [94mLoss[0m : 6.16961

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 6.123, [92mTest[0m: 5.616, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.27428
[1mStep[0m  [4/42], [94mLoss[0m : 5.89856
[1mStep[0m  [8/42], [94mLoss[0m : 6.06154
[1mStep[0m  [12/42], [94mLoss[0m : 5.72819
[1mStep[0m  [16/42], [94mLoss[0m : 5.70312
[1mStep[0m  [20/42], [94mLoss[0m : 5.76148
[1mStep[0m  [24/42], [94mLoss[0m : 5.53492
[1mStep[0m  [28/42], [94mLoss[0m : 5.56646
[1mStep[0m  [32/42], [94mLoss[0m : 5.85722
[1mStep[0m  [36/42], [94mLoss[0m : 5.65880
[1mStep[0m  [40/42], [94mLoss[0m : 5.65766

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 5.847, [92mTest[0m: 5.246, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.03096
[1mStep[0m  [4/42], [94mLoss[0m : 5.53444
[1mStep[0m  [8/42], [94mLoss[0m : 5.77744
[1mStep[0m  [12/42], [94mLoss[0m : 5.62125
[1mStep[0m  [16/42], [94mLoss[0m : 5.48959
[1mStep[0m  [20/42], [94mLoss[0m : 5.42124
[1mStep[0m  [24/42], [94mLoss[0m : 5.43549
[1mStep[0m  [28/42], [94mLoss[0m : 5.34847
[1mStep[0m  [32/42], [94mLoss[0m : 5.60465
[1mStep[0m  [36/42], [94mLoss[0m : 5.18869
[1mStep[0m  [40/42], [94mLoss[0m : 5.24297

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 5.581, [92mTest[0m: 4.946, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.30212
[1mStep[0m  [4/42], [94mLoss[0m : 5.52056
[1mStep[0m  [8/42], [94mLoss[0m : 5.48362
[1mStep[0m  [12/42], [94mLoss[0m : 5.22635
[1mStep[0m  [16/42], [94mLoss[0m : 5.74183
[1mStep[0m  [20/42], [94mLoss[0m : 5.21225
[1mStep[0m  [24/42], [94mLoss[0m : 5.36117
[1mStep[0m  [28/42], [94mLoss[0m : 5.33473
[1mStep[0m  [32/42], [94mLoss[0m : 5.42095
[1mStep[0m  [36/42], [94mLoss[0m : 5.27191
[1mStep[0m  [40/42], [94mLoss[0m : 5.30029

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 5.294, [92mTest[0m: 4.749, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.05528
[1mStep[0m  [4/42], [94mLoss[0m : 4.89897
[1mStep[0m  [8/42], [94mLoss[0m : 5.64076
[1mStep[0m  [12/42], [94mLoss[0m : 5.09544
[1mStep[0m  [16/42], [94mLoss[0m : 4.80824
[1mStep[0m  [20/42], [94mLoss[0m : 5.08762
[1mStep[0m  [24/42], [94mLoss[0m : 5.22839
[1mStep[0m  [28/42], [94mLoss[0m : 5.22523
[1mStep[0m  [32/42], [94mLoss[0m : 4.99952
[1mStep[0m  [36/42], [94mLoss[0m : 5.11489
[1mStep[0m  [40/42], [94mLoss[0m : 4.82289

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 5.047, [92mTest[0m: 4.490, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.13286
[1mStep[0m  [4/42], [94mLoss[0m : 4.83375
[1mStep[0m  [8/42], [94mLoss[0m : 4.83189
[1mStep[0m  [12/42], [94mLoss[0m : 5.05659
[1mStep[0m  [16/42], [94mLoss[0m : 4.63641
[1mStep[0m  [20/42], [94mLoss[0m : 4.61451
[1mStep[0m  [24/42], [94mLoss[0m : 4.63623
[1mStep[0m  [28/42], [94mLoss[0m : 4.67008
[1mStep[0m  [32/42], [94mLoss[0m : 4.61289
[1mStep[0m  [36/42], [94mLoss[0m : 4.67700
[1mStep[0m  [40/42], [94mLoss[0m : 4.73743

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 4.814, [92mTest[0m: 4.252, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.47058
[1mStep[0m  [4/42], [94mLoss[0m : 4.71814
[1mStep[0m  [8/42], [94mLoss[0m : 4.56818
[1mStep[0m  [12/42], [94mLoss[0m : 4.52979
[1mStep[0m  [16/42], [94mLoss[0m : 4.32136
[1mStep[0m  [20/42], [94mLoss[0m : 4.40359
[1mStep[0m  [24/42], [94mLoss[0m : 4.32110
[1mStep[0m  [28/42], [94mLoss[0m : 4.46537
[1mStep[0m  [32/42], [94mLoss[0m : 4.28255
[1mStep[0m  [36/42], [94mLoss[0m : 4.45446
[1mStep[0m  [40/42], [94mLoss[0m : 4.44184

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 4.563, [92mTest[0m: 4.106, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.28679
[1mStep[0m  [4/42], [94mLoss[0m : 4.21667
[1mStep[0m  [8/42], [94mLoss[0m : 4.23509
[1mStep[0m  [12/42], [94mLoss[0m : 4.61756
[1mStep[0m  [16/42], [94mLoss[0m : 4.34128
[1mStep[0m  [20/42], [94mLoss[0m : 4.28807
[1mStep[0m  [24/42], [94mLoss[0m : 4.36975
[1mStep[0m  [28/42], [94mLoss[0m : 4.64118
[1mStep[0m  [32/42], [94mLoss[0m : 3.97020
[1mStep[0m  [36/42], [94mLoss[0m : 4.27691
[1mStep[0m  [40/42], [94mLoss[0m : 4.07914

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 4.319, [92mTest[0m: 3.774, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.29573
[1mStep[0m  [4/42], [94mLoss[0m : 4.22587
[1mStep[0m  [8/42], [94mLoss[0m : 4.26625
[1mStep[0m  [12/42], [94mLoss[0m : 3.91529
[1mStep[0m  [16/42], [94mLoss[0m : 4.11344
[1mStep[0m  [20/42], [94mLoss[0m : 3.89248
[1mStep[0m  [24/42], [94mLoss[0m : 4.05669
[1mStep[0m  [28/42], [94mLoss[0m : 3.62365
[1mStep[0m  [32/42], [94mLoss[0m : 4.15900
[1mStep[0m  [36/42], [94mLoss[0m : 4.18283
[1mStep[0m  [40/42], [94mLoss[0m : 4.15685

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 4.100, [92mTest[0m: 3.632, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.91979
[1mStep[0m  [4/42], [94mLoss[0m : 4.09641
[1mStep[0m  [8/42], [94mLoss[0m : 4.04733
[1mStep[0m  [12/42], [94mLoss[0m : 4.00774
[1mStep[0m  [16/42], [94mLoss[0m : 3.94030
[1mStep[0m  [20/42], [94mLoss[0m : 3.70071
[1mStep[0m  [24/42], [94mLoss[0m : 4.08232
[1mStep[0m  [28/42], [94mLoss[0m : 4.05179
[1mStep[0m  [32/42], [94mLoss[0m : 4.26585
[1mStep[0m  [36/42], [94mLoss[0m : 3.41111
[1mStep[0m  [40/42], [94mLoss[0m : 3.85838

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 3.901, [92mTest[0m: 3.427, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 3.248
====================================

Phase 1 - Evaluation MAE:  3.2481065818241666
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 3.96419
[1mStep[0m  [4/42], [94mLoss[0m : 3.67616
[1mStep[0m  [8/42], [94mLoss[0m : 3.89969
[1mStep[0m  [12/42], [94mLoss[0m : 3.93154
[1mStep[0m  [16/42], [94mLoss[0m : 3.70091
[1mStep[0m  [20/42], [94mLoss[0m : 3.44878
[1mStep[0m  [24/42], [94mLoss[0m : 3.69243
[1mStep[0m  [28/42], [94mLoss[0m : 3.45501
[1mStep[0m  [32/42], [94mLoss[0m : 3.67790
[1mStep[0m  [36/42], [94mLoss[0m : 3.86444
[1mStep[0m  [40/42], [94mLoss[0m : 3.59382

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.696, [92mTest[0m: 3.257, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.44843
[1mStep[0m  [4/42], [94mLoss[0m : 3.50546
[1mStep[0m  [8/42], [94mLoss[0m : 3.57386
[1mStep[0m  [12/42], [94mLoss[0m : 3.18366
[1mStep[0m  [16/42], [94mLoss[0m : 3.59236
[1mStep[0m  [20/42], [94mLoss[0m : 3.55109
[1mStep[0m  [24/42], [94mLoss[0m : 3.50706
[1mStep[0m  [28/42], [94mLoss[0m : 3.24454
[1mStep[0m  [32/42], [94mLoss[0m : 3.48111
[1mStep[0m  [36/42], [94mLoss[0m : 3.55536
[1mStep[0m  [40/42], [94mLoss[0m : 3.20441

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.457, [92mTest[0m: 3.140, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.07516
[1mStep[0m  [4/42], [94mLoss[0m : 3.50883
[1mStep[0m  [8/42], [94mLoss[0m : 3.34521
[1mStep[0m  [12/42], [94mLoss[0m : 3.20329
[1mStep[0m  [16/42], [94mLoss[0m : 3.25195
[1mStep[0m  [20/42], [94mLoss[0m : 3.20820
[1mStep[0m  [24/42], [94mLoss[0m : 3.31093
[1mStep[0m  [28/42], [94mLoss[0m : 3.05822
[1mStep[0m  [32/42], [94mLoss[0m : 2.94242
[1mStep[0m  [36/42], [94mLoss[0m : 3.41815
[1mStep[0m  [40/42], [94mLoss[0m : 3.10822

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.284, [92mTest[0m: 2.971, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.01140
[1mStep[0m  [4/42], [94mLoss[0m : 3.24345
[1mStep[0m  [8/42], [94mLoss[0m : 3.13240
[1mStep[0m  [12/42], [94mLoss[0m : 3.25595
[1mStep[0m  [16/42], [94mLoss[0m : 3.17511
[1mStep[0m  [20/42], [94mLoss[0m : 3.03720
[1mStep[0m  [24/42], [94mLoss[0m : 3.32464
[1mStep[0m  [28/42], [94mLoss[0m : 3.01819
[1mStep[0m  [32/42], [94mLoss[0m : 3.20839
[1mStep[0m  [36/42], [94mLoss[0m : 2.96393
[1mStep[0m  [40/42], [94mLoss[0m : 2.90584

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 3.119, [92mTest[0m: 2.815, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.85603
[1mStep[0m  [4/42], [94mLoss[0m : 2.84570
[1mStep[0m  [8/42], [94mLoss[0m : 3.20660
[1mStep[0m  [12/42], [94mLoss[0m : 3.15049
[1mStep[0m  [16/42], [94mLoss[0m : 3.03785
[1mStep[0m  [20/42], [94mLoss[0m : 2.89988
[1mStep[0m  [24/42], [94mLoss[0m : 2.72247
[1mStep[0m  [28/42], [94mLoss[0m : 2.79094
[1mStep[0m  [32/42], [94mLoss[0m : 2.97844
[1mStep[0m  [36/42], [94mLoss[0m : 2.83769
[1mStep[0m  [40/42], [94mLoss[0m : 2.85763

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.985, [92mTest[0m: 2.742, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.09206
[1mStep[0m  [4/42], [94mLoss[0m : 2.66560
[1mStep[0m  [8/42], [94mLoss[0m : 2.77774
[1mStep[0m  [12/42], [94mLoss[0m : 2.91727
[1mStep[0m  [16/42], [94mLoss[0m : 3.14057
[1mStep[0m  [20/42], [94mLoss[0m : 2.60473
[1mStep[0m  [24/42], [94mLoss[0m : 2.77697
[1mStep[0m  [28/42], [94mLoss[0m : 2.94004
[1mStep[0m  [32/42], [94mLoss[0m : 3.05048
[1mStep[0m  [36/42], [94mLoss[0m : 2.74515
[1mStep[0m  [40/42], [94mLoss[0m : 2.88622

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.880, [92mTest[0m: 2.665, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.98382
[1mStep[0m  [4/42], [94mLoss[0m : 2.63320
[1mStep[0m  [8/42], [94mLoss[0m : 2.57350
[1mStep[0m  [12/42], [94mLoss[0m : 3.22582
[1mStep[0m  [16/42], [94mLoss[0m : 2.57387
[1mStep[0m  [20/42], [94mLoss[0m : 2.80299
[1mStep[0m  [24/42], [94mLoss[0m : 2.80573
[1mStep[0m  [28/42], [94mLoss[0m : 2.63419
[1mStep[0m  [32/42], [94mLoss[0m : 3.00889
[1mStep[0m  [36/42], [94mLoss[0m : 2.89633
[1mStep[0m  [40/42], [94mLoss[0m : 2.78328

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.813, [92mTest[0m: 2.628, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.74449
[1mStep[0m  [4/42], [94mLoss[0m : 2.86980
[1mStep[0m  [8/42], [94mLoss[0m : 2.86940
[1mStep[0m  [12/42], [94mLoss[0m : 3.02470
[1mStep[0m  [16/42], [94mLoss[0m : 2.85129
[1mStep[0m  [20/42], [94mLoss[0m : 2.87436
[1mStep[0m  [24/42], [94mLoss[0m : 2.81009
[1mStep[0m  [28/42], [94mLoss[0m : 2.55231
[1mStep[0m  [32/42], [94mLoss[0m : 2.71717
[1mStep[0m  [36/42], [94mLoss[0m : 2.53438
[1mStep[0m  [40/42], [94mLoss[0m : 2.75931

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.741, [92mTest[0m: 2.653, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.05188
[1mStep[0m  [4/42], [94mLoss[0m : 2.60378
[1mStep[0m  [8/42], [94mLoss[0m : 2.79874
[1mStep[0m  [12/42], [94mLoss[0m : 2.73886
[1mStep[0m  [16/42], [94mLoss[0m : 2.61170
[1mStep[0m  [20/42], [94mLoss[0m : 2.40267
[1mStep[0m  [24/42], [94mLoss[0m : 2.81439
[1mStep[0m  [28/42], [94mLoss[0m : 2.67051
[1mStep[0m  [32/42], [94mLoss[0m : 2.83047
[1mStep[0m  [36/42], [94mLoss[0m : 2.75383
[1mStep[0m  [40/42], [94mLoss[0m : 2.71641

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.710, [92mTest[0m: 2.589, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.68785
[1mStep[0m  [4/42], [94mLoss[0m : 2.63281
[1mStep[0m  [8/42], [94mLoss[0m : 2.37434
[1mStep[0m  [12/42], [94mLoss[0m : 2.75528
[1mStep[0m  [16/42], [94mLoss[0m : 2.44230
[1mStep[0m  [20/42], [94mLoss[0m : 2.73046
[1mStep[0m  [24/42], [94mLoss[0m : 2.49145
[1mStep[0m  [28/42], [94mLoss[0m : 2.70525
[1mStep[0m  [32/42], [94mLoss[0m : 2.83273
[1mStep[0m  [36/42], [94mLoss[0m : 2.64675
[1mStep[0m  [40/42], [94mLoss[0m : 2.66120

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.658, [92mTest[0m: 2.544, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53703
[1mStep[0m  [4/42], [94mLoss[0m : 2.76948
[1mStep[0m  [8/42], [94mLoss[0m : 2.89206
[1mStep[0m  [12/42], [94mLoss[0m : 2.68719
[1mStep[0m  [16/42], [94mLoss[0m : 2.49411
[1mStep[0m  [20/42], [94mLoss[0m : 2.74487
[1mStep[0m  [24/42], [94mLoss[0m : 2.62163
[1mStep[0m  [28/42], [94mLoss[0m : 2.35014
[1mStep[0m  [32/42], [94mLoss[0m : 2.73642
[1mStep[0m  [36/42], [94mLoss[0m : 2.55716
[1mStep[0m  [40/42], [94mLoss[0m : 2.54935

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.619, [92mTest[0m: 2.577, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58093
[1mStep[0m  [4/42], [94mLoss[0m : 2.64454
[1mStep[0m  [8/42], [94mLoss[0m : 2.77874
[1mStep[0m  [12/42], [94mLoss[0m : 2.73459
[1mStep[0m  [16/42], [94mLoss[0m : 2.55458
[1mStep[0m  [20/42], [94mLoss[0m : 2.40355
[1mStep[0m  [24/42], [94mLoss[0m : 2.50473
[1mStep[0m  [28/42], [94mLoss[0m : 2.36949
[1mStep[0m  [32/42], [94mLoss[0m : 2.54316
[1mStep[0m  [36/42], [94mLoss[0m : 2.63068
[1mStep[0m  [40/42], [94mLoss[0m : 2.62760

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.586, [92mTest[0m: 2.573, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54091
[1mStep[0m  [4/42], [94mLoss[0m : 2.79237
[1mStep[0m  [8/42], [94mLoss[0m : 2.62821
[1mStep[0m  [12/42], [94mLoss[0m : 2.52989
[1mStep[0m  [16/42], [94mLoss[0m : 2.39094
[1mStep[0m  [20/42], [94mLoss[0m : 2.59317
[1mStep[0m  [24/42], [94mLoss[0m : 2.46731
[1mStep[0m  [28/42], [94mLoss[0m : 2.39464
[1mStep[0m  [32/42], [94mLoss[0m : 2.81586
[1mStep[0m  [36/42], [94mLoss[0m : 2.35943
[1mStep[0m  [40/42], [94mLoss[0m : 2.70750

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.567, [92mTest[0m: 2.565, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53588
[1mStep[0m  [4/42], [94mLoss[0m : 2.51300
[1mStep[0m  [8/42], [94mLoss[0m : 2.36784
[1mStep[0m  [12/42], [94mLoss[0m : 2.23267
[1mStep[0m  [16/42], [94mLoss[0m : 2.57416
[1mStep[0m  [20/42], [94mLoss[0m : 2.63434
[1mStep[0m  [24/42], [94mLoss[0m : 2.31630
[1mStep[0m  [28/42], [94mLoss[0m : 2.60350
[1mStep[0m  [32/42], [94mLoss[0m : 2.38137
[1mStep[0m  [36/42], [94mLoss[0m : 2.54545
[1mStep[0m  [40/42], [94mLoss[0m : 2.57330

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.514, [92mTest[0m: 2.588, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.71690
[1mStep[0m  [4/42], [94mLoss[0m : 2.65383
[1mStep[0m  [8/42], [94mLoss[0m : 2.58676
[1mStep[0m  [12/42], [94mLoss[0m : 2.38772
[1mStep[0m  [16/42], [94mLoss[0m : 2.38162
[1mStep[0m  [20/42], [94mLoss[0m : 2.40428
[1mStep[0m  [24/42], [94mLoss[0m : 2.62970
[1mStep[0m  [28/42], [94mLoss[0m : 2.40196
[1mStep[0m  [32/42], [94mLoss[0m : 2.40459
[1mStep[0m  [36/42], [94mLoss[0m : 2.44714
[1mStep[0m  [40/42], [94mLoss[0m : 2.49696

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.575, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52641
[1mStep[0m  [4/42], [94mLoss[0m : 2.57113
[1mStep[0m  [8/42], [94mLoss[0m : 2.30939
[1mStep[0m  [12/42], [94mLoss[0m : 2.54143
[1mStep[0m  [16/42], [94mLoss[0m : 2.57501
[1mStep[0m  [20/42], [94mLoss[0m : 2.67071
[1mStep[0m  [24/42], [94mLoss[0m : 2.24419
[1mStep[0m  [28/42], [94mLoss[0m : 2.69516
[1mStep[0m  [32/42], [94mLoss[0m : 2.39609
[1mStep[0m  [36/42], [94mLoss[0m : 2.64112
[1mStep[0m  [40/42], [94mLoss[0m : 2.54381

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.541, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49423
[1mStep[0m  [4/42], [94mLoss[0m : 2.19783
[1mStep[0m  [8/42], [94mLoss[0m : 2.52429
[1mStep[0m  [12/42], [94mLoss[0m : 2.50249
[1mStep[0m  [16/42], [94mLoss[0m : 2.48267
[1mStep[0m  [20/42], [94mLoss[0m : 2.65541
[1mStep[0m  [24/42], [94mLoss[0m : 2.59887
[1mStep[0m  [28/42], [94mLoss[0m : 2.29985
[1mStep[0m  [32/42], [94mLoss[0m : 2.67648
[1mStep[0m  [36/42], [94mLoss[0m : 2.50148
[1mStep[0m  [40/42], [94mLoss[0m : 2.49367

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.538, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35744
[1mStep[0m  [4/42], [94mLoss[0m : 2.16215
[1mStep[0m  [8/42], [94mLoss[0m : 2.40660
[1mStep[0m  [12/42], [94mLoss[0m : 2.58884
[1mStep[0m  [16/42], [94mLoss[0m : 2.33288
[1mStep[0m  [20/42], [94mLoss[0m : 2.53785
[1mStep[0m  [24/42], [94mLoss[0m : 2.38337
[1mStep[0m  [28/42], [94mLoss[0m : 2.55024
[1mStep[0m  [32/42], [94mLoss[0m : 2.64829
[1mStep[0m  [36/42], [94mLoss[0m : 2.36623
[1mStep[0m  [40/42], [94mLoss[0m : 2.54314

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.569, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42573
[1mStep[0m  [4/42], [94mLoss[0m : 2.50235
[1mStep[0m  [8/42], [94mLoss[0m : 2.39964
[1mStep[0m  [12/42], [94mLoss[0m : 2.45281
[1mStep[0m  [16/42], [94mLoss[0m : 2.25855
[1mStep[0m  [20/42], [94mLoss[0m : 2.37361
[1mStep[0m  [24/42], [94mLoss[0m : 2.41055
[1mStep[0m  [28/42], [94mLoss[0m : 2.32655
[1mStep[0m  [32/42], [94mLoss[0m : 2.39946
[1mStep[0m  [36/42], [94mLoss[0m : 2.50049
[1mStep[0m  [40/42], [94mLoss[0m : 2.50645

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.558, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24284
[1mStep[0m  [4/42], [94mLoss[0m : 2.50902
[1mStep[0m  [8/42], [94mLoss[0m : 2.50873
[1mStep[0m  [12/42], [94mLoss[0m : 2.27108
[1mStep[0m  [16/42], [94mLoss[0m : 2.32114
[1mStep[0m  [20/42], [94mLoss[0m : 2.64522
[1mStep[0m  [24/42], [94mLoss[0m : 2.36353
[1mStep[0m  [28/42], [94mLoss[0m : 2.15819
[1mStep[0m  [32/42], [94mLoss[0m : 2.51028
[1mStep[0m  [36/42], [94mLoss[0m : 2.53694
[1mStep[0m  [40/42], [94mLoss[0m : 2.03675

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.577, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.23847
[1mStep[0m  [4/42], [94mLoss[0m : 2.27316
[1mStep[0m  [8/42], [94mLoss[0m : 2.42376
[1mStep[0m  [12/42], [94mLoss[0m : 2.47302
[1mStep[0m  [16/42], [94mLoss[0m : 2.37247
[1mStep[0m  [20/42], [94mLoss[0m : 2.24542
[1mStep[0m  [24/42], [94mLoss[0m : 2.58412
[1mStep[0m  [28/42], [94mLoss[0m : 2.49074
[1mStep[0m  [32/42], [94mLoss[0m : 2.57040
[1mStep[0m  [36/42], [94mLoss[0m : 2.38074
[1mStep[0m  [40/42], [94mLoss[0m : 2.40016

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.588, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.67413
[1mStep[0m  [4/42], [94mLoss[0m : 2.23845
[1mStep[0m  [8/42], [94mLoss[0m : 2.28007
[1mStep[0m  [12/42], [94mLoss[0m : 2.44460
[1mStep[0m  [16/42], [94mLoss[0m : 2.33772
[1mStep[0m  [20/42], [94mLoss[0m : 2.52087
[1mStep[0m  [24/42], [94mLoss[0m : 2.49535
[1mStep[0m  [28/42], [94mLoss[0m : 2.24163
[1mStep[0m  [32/42], [94mLoss[0m : 2.45061
[1mStep[0m  [36/42], [94mLoss[0m : 2.48977
[1mStep[0m  [40/42], [94mLoss[0m : 2.39707

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.546, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52239
[1mStep[0m  [4/42], [94mLoss[0m : 2.38311
[1mStep[0m  [8/42], [94mLoss[0m : 2.47110
[1mStep[0m  [12/42], [94mLoss[0m : 2.27219
[1mStep[0m  [16/42], [94mLoss[0m : 2.42006
[1mStep[0m  [20/42], [94mLoss[0m : 2.44410
[1mStep[0m  [24/42], [94mLoss[0m : 2.56341
[1mStep[0m  [28/42], [94mLoss[0m : 2.35530
[1mStep[0m  [32/42], [94mLoss[0m : 2.39927
[1mStep[0m  [36/42], [94mLoss[0m : 2.28382
[1mStep[0m  [40/42], [94mLoss[0m : 2.27282

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.384, [92mTest[0m: 2.582, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35708
[1mStep[0m  [4/42], [94mLoss[0m : 2.36512
[1mStep[0m  [8/42], [94mLoss[0m : 2.30496
[1mStep[0m  [12/42], [94mLoss[0m : 2.22806
[1mStep[0m  [16/42], [94mLoss[0m : 2.24168
[1mStep[0m  [20/42], [94mLoss[0m : 2.51194
[1mStep[0m  [24/42], [94mLoss[0m : 2.24151
[1mStep[0m  [28/42], [94mLoss[0m : 2.30798
[1mStep[0m  [32/42], [94mLoss[0m : 2.55202
[1mStep[0m  [36/42], [94mLoss[0m : 2.33579
[1mStep[0m  [40/42], [94mLoss[0m : 2.36376

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.372, [92mTest[0m: 2.512, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53307
[1mStep[0m  [4/42], [94mLoss[0m : 2.19537
[1mStep[0m  [8/42], [94mLoss[0m : 2.41423
[1mStep[0m  [12/42], [94mLoss[0m : 2.16711
[1mStep[0m  [16/42], [94mLoss[0m : 2.30111
[1mStep[0m  [20/42], [94mLoss[0m : 2.20396
[1mStep[0m  [24/42], [94mLoss[0m : 2.53214
[1mStep[0m  [28/42], [94mLoss[0m : 2.06080
[1mStep[0m  [32/42], [94mLoss[0m : 2.32850
[1mStep[0m  [36/42], [94mLoss[0m : 2.45353
[1mStep[0m  [40/42], [94mLoss[0m : 2.52644

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.372, [92mTest[0m: 2.496, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29310
[1mStep[0m  [4/42], [94mLoss[0m : 2.11997
[1mStep[0m  [8/42], [94mLoss[0m : 2.46138
[1mStep[0m  [12/42], [94mLoss[0m : 2.28682
[1mStep[0m  [16/42], [94mLoss[0m : 2.36429
[1mStep[0m  [20/42], [94mLoss[0m : 2.30135
[1mStep[0m  [24/42], [94mLoss[0m : 2.21829
[1mStep[0m  [28/42], [94mLoss[0m : 2.44851
[1mStep[0m  [32/42], [94mLoss[0m : 2.32673
[1mStep[0m  [36/42], [94mLoss[0m : 2.25062
[1mStep[0m  [40/42], [94mLoss[0m : 2.40726

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.334, [92mTest[0m: 2.520, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42489
[1mStep[0m  [4/42], [94mLoss[0m : 2.43419
[1mStep[0m  [8/42], [94mLoss[0m : 2.33855
[1mStep[0m  [12/42], [94mLoss[0m : 2.37764
[1mStep[0m  [16/42], [94mLoss[0m : 2.24185
[1mStep[0m  [20/42], [94mLoss[0m : 2.34002
[1mStep[0m  [24/42], [94mLoss[0m : 2.27318
[1mStep[0m  [28/42], [94mLoss[0m : 2.14371
[1mStep[0m  [32/42], [94mLoss[0m : 2.25791
[1mStep[0m  [36/42], [94mLoss[0m : 2.26036
[1mStep[0m  [40/42], [94mLoss[0m : 2.50067

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.355, [92mTest[0m: 2.542, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24077
[1mStep[0m  [4/42], [94mLoss[0m : 2.37968
[1mStep[0m  [8/42], [94mLoss[0m : 2.51210
[1mStep[0m  [12/42], [94mLoss[0m : 2.14585
[1mStep[0m  [16/42], [94mLoss[0m : 2.38583
[1mStep[0m  [20/42], [94mLoss[0m : 2.49068
[1mStep[0m  [24/42], [94mLoss[0m : 2.29436
[1mStep[0m  [28/42], [94mLoss[0m : 2.21563
[1mStep[0m  [32/42], [94mLoss[0m : 2.46575
[1mStep[0m  [36/42], [94mLoss[0m : 2.35939
[1mStep[0m  [40/42], [94mLoss[0m : 2.30707

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.327, [92mTest[0m: 2.538, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.11670
[1mStep[0m  [4/42], [94mLoss[0m : 2.34757
[1mStep[0m  [8/42], [94mLoss[0m : 2.09644
[1mStep[0m  [12/42], [94mLoss[0m : 2.37475
[1mStep[0m  [16/42], [94mLoss[0m : 2.16731
[1mStep[0m  [20/42], [94mLoss[0m : 2.26102
[1mStep[0m  [24/42], [94mLoss[0m : 2.31323
[1mStep[0m  [28/42], [94mLoss[0m : 2.24678
[1mStep[0m  [32/42], [94mLoss[0m : 2.28606
[1mStep[0m  [36/42], [94mLoss[0m : 2.34260
[1mStep[0m  [40/42], [94mLoss[0m : 2.35735

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.312, [92mTest[0m: 2.474, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.04781
[1mStep[0m  [4/42], [94mLoss[0m : 2.32006
[1mStep[0m  [8/42], [94mLoss[0m : 2.23608
[1mStep[0m  [12/42], [94mLoss[0m : 2.40892
[1mStep[0m  [16/42], [94mLoss[0m : 2.20815
[1mStep[0m  [20/42], [94mLoss[0m : 2.24067
[1mStep[0m  [24/42], [94mLoss[0m : 2.18231
[1mStep[0m  [28/42], [94mLoss[0m : 2.47723
[1mStep[0m  [32/42], [94mLoss[0m : 2.25850
[1mStep[0m  [36/42], [94mLoss[0m : 2.38799
[1mStep[0m  [40/42], [94mLoss[0m : 2.10684

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.302, [92mTest[0m: 2.451, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.483
====================================

Phase 2 - Evaluation MAE:  2.48323609147753
MAE score P1       3.248107
MAE score P2       2.483236
loss                 2.3019
learning_rate      0.002575
batch_size              256
hidden_sizes      [250, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.1
weight_decay         0.0001
Name: 24, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 11.02873
[1mStep[0m  [4/42], [94mLoss[0m : 10.92704
[1mStep[0m  [8/42], [94mLoss[0m : 10.95283
[1mStep[0m  [12/42], [94mLoss[0m : 10.95854
[1mStep[0m  [16/42], [94mLoss[0m : 10.60158
[1mStep[0m  [20/42], [94mLoss[0m : 10.65926
[1mStep[0m  [24/42], [94mLoss[0m : 10.72127
[1mStep[0m  [28/42], [94mLoss[0m : 10.65014
[1mStep[0m  [32/42], [94mLoss[0m : 10.78037
[1mStep[0m  [36/42], [94mLoss[0m : 10.79733
[1mStep[0m  [40/42], [94mLoss[0m : 10.99525

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.867, [92mTest[0m: 10.968, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.71054
[1mStep[0m  [4/42], [94mLoss[0m : 10.61274
[1mStep[0m  [8/42], [94mLoss[0m : 10.65213
[1mStep[0m  [12/42], [94mLoss[0m : 10.44591
[1mStep[0m  [16/42], [94mLoss[0m : 10.60466
[1mStep[0m  [20/42], [94mLoss[0m : 10.72285
[1mStep[0m  [24/42], [94mLoss[0m : 10.75062
[1mStep[0m  [28/42], [94mLoss[0m : 10.53354
[1mStep[0m  [32/42], [94mLoss[0m : 10.50675
[1mStep[0m  [36/42], [94mLoss[0m : 10.71975
[1mStep[0m  [40/42], [94mLoss[0m : 10.35002

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.606, [92mTest[0m: 10.627, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.64687
[1mStep[0m  [4/42], [94mLoss[0m : 10.24296
[1mStep[0m  [8/42], [94mLoss[0m : 10.73359
[1mStep[0m  [12/42], [94mLoss[0m : 10.55678
[1mStep[0m  [16/42], [94mLoss[0m : 10.75987
[1mStep[0m  [20/42], [94mLoss[0m : 9.88298
[1mStep[0m  [24/42], [94mLoss[0m : 10.57359
[1mStep[0m  [28/42], [94mLoss[0m : 10.17601
[1mStep[0m  [32/42], [94mLoss[0m : 10.05954
[1mStep[0m  [36/42], [94mLoss[0m : 10.11012
[1mStep[0m  [40/42], [94mLoss[0m : 10.14159

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.329, [92mTest[0m: 10.276, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.85156
[1mStep[0m  [4/42], [94mLoss[0m : 10.68856
[1mStep[0m  [8/42], [94mLoss[0m : 10.05452
[1mStep[0m  [12/42], [94mLoss[0m : 10.61922
[1mStep[0m  [16/42], [94mLoss[0m : 10.02965
[1mStep[0m  [20/42], [94mLoss[0m : 10.02516
[1mStep[0m  [24/42], [94mLoss[0m : 10.42671
[1mStep[0m  [28/42], [94mLoss[0m : 10.21340
[1mStep[0m  [32/42], [94mLoss[0m : 10.08838
[1mStep[0m  [36/42], [94mLoss[0m : 10.31219
[1mStep[0m  [40/42], [94mLoss[0m : 9.75249

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.045, [92mTest[0m: 9.960, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.75080
[1mStep[0m  [4/42], [94mLoss[0m : 10.18579
[1mStep[0m  [8/42], [94mLoss[0m : 9.69535
[1mStep[0m  [12/42], [94mLoss[0m : 9.58671
[1mStep[0m  [16/42], [94mLoss[0m : 9.96421
[1mStep[0m  [20/42], [94mLoss[0m : 9.69531
[1mStep[0m  [24/42], [94mLoss[0m : 9.84731
[1mStep[0m  [28/42], [94mLoss[0m : 9.80677
[1mStep[0m  [32/42], [94mLoss[0m : 9.77368
[1mStep[0m  [36/42], [94mLoss[0m : 9.25640
[1mStep[0m  [40/42], [94mLoss[0m : 9.42050

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 9.741, [92mTest[0m: 9.628, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.49471
[1mStep[0m  [4/42], [94mLoss[0m : 9.63729
[1mStep[0m  [8/42], [94mLoss[0m : 9.44206
[1mStep[0m  [12/42], [94mLoss[0m : 9.26758
[1mStep[0m  [16/42], [94mLoss[0m : 9.82324
[1mStep[0m  [20/42], [94mLoss[0m : 9.16561
[1mStep[0m  [24/42], [94mLoss[0m : 9.56318
[1mStep[0m  [28/42], [94mLoss[0m : 9.53224
[1mStep[0m  [32/42], [94mLoss[0m : 9.15101
[1mStep[0m  [36/42], [94mLoss[0m : 9.42430
[1mStep[0m  [40/42], [94mLoss[0m : 9.28028

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 9.410, [92mTest[0m: 9.250, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.20942
[1mStep[0m  [4/42], [94mLoss[0m : 9.26513
[1mStep[0m  [8/42], [94mLoss[0m : 9.04098
[1mStep[0m  [12/42], [94mLoss[0m : 9.19862
[1mStep[0m  [16/42], [94mLoss[0m : 9.07304
[1mStep[0m  [20/42], [94mLoss[0m : 9.02617
[1mStep[0m  [24/42], [94mLoss[0m : 8.88619
[1mStep[0m  [28/42], [94mLoss[0m : 8.93210
[1mStep[0m  [32/42], [94mLoss[0m : 9.43208
[1mStep[0m  [36/42], [94mLoss[0m : 8.82532
[1mStep[0m  [40/42], [94mLoss[0m : 8.65368

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 9.033, [92mTest[0m: 8.822, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.59450
[1mStep[0m  [4/42], [94mLoss[0m : 8.67535
[1mStep[0m  [8/42], [94mLoss[0m : 9.14303
[1mStep[0m  [12/42], [94mLoss[0m : 8.76559
[1mStep[0m  [16/42], [94mLoss[0m : 8.98120
[1mStep[0m  [20/42], [94mLoss[0m : 8.91983
[1mStep[0m  [24/42], [94mLoss[0m : 8.86079
[1mStep[0m  [28/42], [94mLoss[0m : 8.36464
[1mStep[0m  [32/42], [94mLoss[0m : 8.26758
[1mStep[0m  [36/42], [94mLoss[0m : 8.15586
[1mStep[0m  [40/42], [94mLoss[0m : 8.38718

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 8.598, [92mTest[0m: 8.310, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.83328
[1mStep[0m  [4/42], [94mLoss[0m : 8.24635
[1mStep[0m  [8/42], [94mLoss[0m : 8.47020
[1mStep[0m  [12/42], [94mLoss[0m : 8.20883
[1mStep[0m  [16/42], [94mLoss[0m : 8.41978
[1mStep[0m  [20/42], [94mLoss[0m : 7.91644
[1mStep[0m  [24/42], [94mLoss[0m : 7.70673
[1mStep[0m  [28/42], [94mLoss[0m : 7.96294
[1mStep[0m  [32/42], [94mLoss[0m : 7.85013
[1mStep[0m  [36/42], [94mLoss[0m : 7.89166
[1mStep[0m  [40/42], [94mLoss[0m : 8.01855

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 8.098, [92mTest[0m: 7.722, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.89869
[1mStep[0m  [4/42], [94mLoss[0m : 8.10703
[1mStep[0m  [8/42], [94mLoss[0m : 7.74501
[1mStep[0m  [12/42], [94mLoss[0m : 7.46393
[1mStep[0m  [16/42], [94mLoss[0m : 7.89485
[1mStep[0m  [20/42], [94mLoss[0m : 7.63492
[1mStep[0m  [24/42], [94mLoss[0m : 7.23451
[1mStep[0m  [28/42], [94mLoss[0m : 7.05671
[1mStep[0m  [32/42], [94mLoss[0m : 7.71909
[1mStep[0m  [36/42], [94mLoss[0m : 7.75995
[1mStep[0m  [40/42], [94mLoss[0m : 7.00170

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 7.548, [92mTest[0m: 7.171, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.95849
[1mStep[0m  [4/42], [94mLoss[0m : 7.43217
[1mStep[0m  [8/42], [94mLoss[0m : 7.26125
[1mStep[0m  [12/42], [94mLoss[0m : 7.11769
[1mStep[0m  [16/42], [94mLoss[0m : 7.18767
[1mStep[0m  [20/42], [94mLoss[0m : 6.68981
[1mStep[0m  [24/42], [94mLoss[0m : 6.85885
[1mStep[0m  [28/42], [94mLoss[0m : 6.94795
[1mStep[0m  [32/42], [94mLoss[0m : 6.79396
[1mStep[0m  [36/42], [94mLoss[0m : 6.51630
[1mStep[0m  [40/42], [94mLoss[0m : 6.91281

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 6.948, [92mTest[0m: 6.483, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.77047
[1mStep[0m  [4/42], [94mLoss[0m : 6.24728
[1mStep[0m  [8/42], [94mLoss[0m : 6.61671
[1mStep[0m  [12/42], [94mLoss[0m : 6.56183
[1mStep[0m  [16/42], [94mLoss[0m : 6.42056
[1mStep[0m  [20/42], [94mLoss[0m : 6.52838
[1mStep[0m  [24/42], [94mLoss[0m : 6.38721
[1mStep[0m  [28/42], [94mLoss[0m : 6.16458
[1mStep[0m  [32/42], [94mLoss[0m : 6.05754
[1mStep[0m  [36/42], [94mLoss[0m : 6.01868
[1mStep[0m  [40/42], [94mLoss[0m : 6.06430

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 6.357, [92mTest[0m: 5.810, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.80540
[1mStep[0m  [4/42], [94mLoss[0m : 6.07906
[1mStep[0m  [8/42], [94mLoss[0m : 6.42990
[1mStep[0m  [12/42], [94mLoss[0m : 5.82055
[1mStep[0m  [16/42], [94mLoss[0m : 6.13025
[1mStep[0m  [20/42], [94mLoss[0m : 5.96835
[1mStep[0m  [24/42], [94mLoss[0m : 5.59681
[1mStep[0m  [28/42], [94mLoss[0m : 6.09338
[1mStep[0m  [32/42], [94mLoss[0m : 5.64910
[1mStep[0m  [36/42], [94mLoss[0m : 5.76002
[1mStep[0m  [40/42], [94mLoss[0m : 5.91894

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 5.813, [92mTest[0m: 5.262, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.80248
[1mStep[0m  [4/42], [94mLoss[0m : 5.56447
[1mStep[0m  [8/42], [94mLoss[0m : 5.44844
[1mStep[0m  [12/42], [94mLoss[0m : 5.63911
[1mStep[0m  [16/42], [94mLoss[0m : 5.07827
[1mStep[0m  [20/42], [94mLoss[0m : 5.20264
[1mStep[0m  [24/42], [94mLoss[0m : 5.44829
[1mStep[0m  [28/42], [94mLoss[0m : 5.52422
[1mStep[0m  [32/42], [94mLoss[0m : 5.50199
[1mStep[0m  [36/42], [94mLoss[0m : 4.72394
[1mStep[0m  [40/42], [94mLoss[0m : 4.73141

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 5.231, [92mTest[0m: 4.694, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.04524
[1mStep[0m  [4/42], [94mLoss[0m : 5.04139
[1mStep[0m  [8/42], [94mLoss[0m : 4.65264
[1mStep[0m  [12/42], [94mLoss[0m : 4.71463
[1mStep[0m  [16/42], [94mLoss[0m : 5.03001
[1mStep[0m  [20/42], [94mLoss[0m : 4.53618
[1mStep[0m  [24/42], [94mLoss[0m : 4.36112
[1mStep[0m  [28/42], [94mLoss[0m : 4.58942
[1mStep[0m  [32/42], [94mLoss[0m : 4.86073
[1mStep[0m  [36/42], [94mLoss[0m : 4.30758
[1mStep[0m  [40/42], [94mLoss[0m : 4.30121

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 4.648, [92mTest[0m: 4.199, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.34546
[1mStep[0m  [4/42], [94mLoss[0m : 4.41426
[1mStep[0m  [8/42], [94mLoss[0m : 4.06533
[1mStep[0m  [12/42], [94mLoss[0m : 4.48524
[1mStep[0m  [16/42], [94mLoss[0m : 3.81451
[1mStep[0m  [20/42], [94mLoss[0m : 4.27254
[1mStep[0m  [24/42], [94mLoss[0m : 4.55712
[1mStep[0m  [28/42], [94mLoss[0m : 3.77506
[1mStep[0m  [32/42], [94mLoss[0m : 4.14413
[1mStep[0m  [36/42], [94mLoss[0m : 3.90303
[1mStep[0m  [40/42], [94mLoss[0m : 4.01485

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 4.128, [92mTest[0m: 3.648, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.77463
[1mStep[0m  [4/42], [94mLoss[0m : 3.78951
[1mStep[0m  [8/42], [94mLoss[0m : 3.90319
[1mStep[0m  [12/42], [94mLoss[0m : 3.86657
[1mStep[0m  [16/42], [94mLoss[0m : 3.83793
[1mStep[0m  [20/42], [94mLoss[0m : 3.66606
[1mStep[0m  [24/42], [94mLoss[0m : 3.43265
[1mStep[0m  [28/42], [94mLoss[0m : 3.70621
[1mStep[0m  [32/42], [94mLoss[0m : 3.51793
[1mStep[0m  [36/42], [94mLoss[0m : 3.28073
[1mStep[0m  [40/42], [94mLoss[0m : 3.51606

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 3.644, [92mTest[0m: 3.295, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.90700
[1mStep[0m  [4/42], [94mLoss[0m : 3.36860
[1mStep[0m  [8/42], [94mLoss[0m : 3.06385
[1mStep[0m  [12/42], [94mLoss[0m : 3.34750
[1mStep[0m  [16/42], [94mLoss[0m : 3.28229
[1mStep[0m  [20/42], [94mLoss[0m : 3.40834
[1mStep[0m  [24/42], [94mLoss[0m : 2.95884
[1mStep[0m  [28/42], [94mLoss[0m : 3.26463
[1mStep[0m  [32/42], [94mLoss[0m : 3.32468
[1mStep[0m  [36/42], [94mLoss[0m : 3.06223
[1mStep[0m  [40/42], [94mLoss[0m : 3.09377

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 3.286, [92mTest[0m: 2.927, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.28104
[1mStep[0m  [4/42], [94mLoss[0m : 2.83405
[1mStep[0m  [8/42], [94mLoss[0m : 2.89448
[1mStep[0m  [12/42], [94mLoss[0m : 3.14555
[1mStep[0m  [16/42], [94mLoss[0m : 3.06063
[1mStep[0m  [20/42], [94mLoss[0m : 3.21286
[1mStep[0m  [24/42], [94mLoss[0m : 2.62678
[1mStep[0m  [28/42], [94mLoss[0m : 3.04226
[1mStep[0m  [32/42], [94mLoss[0m : 2.89838
[1mStep[0m  [36/42], [94mLoss[0m : 3.05335
[1mStep[0m  [40/42], [94mLoss[0m : 2.87801

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 3.005, [92mTest[0m: 2.711, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.89713
[1mStep[0m  [4/42], [94mLoss[0m : 2.82859
[1mStep[0m  [8/42], [94mLoss[0m : 2.84156
[1mStep[0m  [12/42], [94mLoss[0m : 2.65688
[1mStep[0m  [16/42], [94mLoss[0m : 2.72601
[1mStep[0m  [20/42], [94mLoss[0m : 2.70285
[1mStep[0m  [24/42], [94mLoss[0m : 2.84599
[1mStep[0m  [28/42], [94mLoss[0m : 2.73238
[1mStep[0m  [32/42], [94mLoss[0m : 2.85991
[1mStep[0m  [36/42], [94mLoss[0m : 2.74524
[1mStep[0m  [40/42], [94mLoss[0m : 2.46661

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.819, [92mTest[0m: 2.567, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.69544
[1mStep[0m  [4/42], [94mLoss[0m : 2.82069
[1mStep[0m  [8/42], [94mLoss[0m : 2.63373
[1mStep[0m  [12/42], [94mLoss[0m : 2.78258
[1mStep[0m  [16/42], [94mLoss[0m : 2.74962
[1mStep[0m  [20/42], [94mLoss[0m : 2.77855
[1mStep[0m  [24/42], [94mLoss[0m : 2.58201
[1mStep[0m  [28/42], [94mLoss[0m : 2.98630
[1mStep[0m  [32/42], [94mLoss[0m : 2.74749
[1mStep[0m  [36/42], [94mLoss[0m : 2.93693
[1mStep[0m  [40/42], [94mLoss[0m : 2.70113

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.729, [92mTest[0m: 2.476, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.65064
[1mStep[0m  [4/42], [94mLoss[0m : 2.73523
[1mStep[0m  [8/42], [94mLoss[0m : 2.75582
[1mStep[0m  [12/42], [94mLoss[0m : 2.63339
[1mStep[0m  [16/42], [94mLoss[0m : 2.45308
[1mStep[0m  [20/42], [94mLoss[0m : 2.48442
[1mStep[0m  [24/42], [94mLoss[0m : 2.73982
[1mStep[0m  [28/42], [94mLoss[0m : 2.87038
[1mStep[0m  [32/42], [94mLoss[0m : 2.76878
[1mStep[0m  [36/42], [94mLoss[0m : 2.49692
[1mStep[0m  [40/42], [94mLoss[0m : 2.75843

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.667, [92mTest[0m: 2.415, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.70263
[1mStep[0m  [4/42], [94mLoss[0m : 2.59032
[1mStep[0m  [8/42], [94mLoss[0m : 2.38237
[1mStep[0m  [12/42], [94mLoss[0m : 2.69507
[1mStep[0m  [16/42], [94mLoss[0m : 2.58326
[1mStep[0m  [20/42], [94mLoss[0m : 2.68756
[1mStep[0m  [24/42], [94mLoss[0m : 2.64543
[1mStep[0m  [28/42], [94mLoss[0m : 2.71912
[1mStep[0m  [32/42], [94mLoss[0m : 2.68267
[1mStep[0m  [36/42], [94mLoss[0m : 2.44903
[1mStep[0m  [40/42], [94mLoss[0m : 2.84144

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.635, [92mTest[0m: 2.396, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.69042
[1mStep[0m  [4/42], [94mLoss[0m : 2.72368
[1mStep[0m  [8/42], [94mLoss[0m : 2.60742
[1mStep[0m  [12/42], [94mLoss[0m : 2.51179
[1mStep[0m  [16/42], [94mLoss[0m : 2.70106
[1mStep[0m  [20/42], [94mLoss[0m : 2.40674
[1mStep[0m  [24/42], [94mLoss[0m : 2.57123
[1mStep[0m  [28/42], [94mLoss[0m : 2.41715
[1mStep[0m  [32/42], [94mLoss[0m : 2.38798
[1mStep[0m  [36/42], [94mLoss[0m : 2.60473
[1mStep[0m  [40/42], [94mLoss[0m : 2.64594

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.610, [92mTest[0m: 2.387, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42725
[1mStep[0m  [4/42], [94mLoss[0m : 2.52673
[1mStep[0m  [8/42], [94mLoss[0m : 2.56380
[1mStep[0m  [12/42], [94mLoss[0m : 2.58566
[1mStep[0m  [16/42], [94mLoss[0m : 2.64204
[1mStep[0m  [20/42], [94mLoss[0m : 2.72652
[1mStep[0m  [24/42], [94mLoss[0m : 2.61466
[1mStep[0m  [28/42], [94mLoss[0m : 2.63994
[1mStep[0m  [32/42], [94mLoss[0m : 2.64264
[1mStep[0m  [36/42], [94mLoss[0m : 2.72103
[1mStep[0m  [40/42], [94mLoss[0m : 2.57989

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.627, [92mTest[0m: 2.371, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57666
[1mStep[0m  [4/42], [94mLoss[0m : 2.58254
[1mStep[0m  [8/42], [94mLoss[0m : 2.40688
[1mStep[0m  [12/42], [94mLoss[0m : 2.50508
[1mStep[0m  [16/42], [94mLoss[0m : 2.55263
[1mStep[0m  [20/42], [94mLoss[0m : 2.75117
[1mStep[0m  [24/42], [94mLoss[0m : 2.76504
[1mStep[0m  [28/42], [94mLoss[0m : 2.83501
[1mStep[0m  [32/42], [94mLoss[0m : 2.48535
[1mStep[0m  [36/42], [94mLoss[0m : 2.42859
[1mStep[0m  [40/42], [94mLoss[0m : 2.68522

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.604, [92mTest[0m: 2.375, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52354
[1mStep[0m  [4/42], [94mLoss[0m : 2.61373
[1mStep[0m  [8/42], [94mLoss[0m : 2.46480
[1mStep[0m  [12/42], [94mLoss[0m : 2.66299
[1mStep[0m  [16/42], [94mLoss[0m : 2.60295
[1mStep[0m  [20/42], [94mLoss[0m : 2.75218
[1mStep[0m  [24/42], [94mLoss[0m : 2.33616
[1mStep[0m  [28/42], [94mLoss[0m : 2.43544
[1mStep[0m  [32/42], [94mLoss[0m : 2.51875
[1mStep[0m  [36/42], [94mLoss[0m : 2.69029
[1mStep[0m  [40/42], [94mLoss[0m : 2.53264

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.568, [92mTest[0m: 2.370, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.67631
[1mStep[0m  [4/42], [94mLoss[0m : 2.36940
[1mStep[0m  [8/42], [94mLoss[0m : 2.56533
[1mStep[0m  [12/42], [94mLoss[0m : 2.73744
[1mStep[0m  [16/42], [94mLoss[0m : 2.19156
[1mStep[0m  [20/42], [94mLoss[0m : 2.41689
[1mStep[0m  [24/42], [94mLoss[0m : 2.75062
[1mStep[0m  [28/42], [94mLoss[0m : 2.57694
[1mStep[0m  [32/42], [94mLoss[0m : 2.69475
[1mStep[0m  [36/42], [94mLoss[0m : 2.60863
[1mStep[0m  [40/42], [94mLoss[0m : 2.45758

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.580, [92mTest[0m: 2.357, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.66603
[1mStep[0m  [4/42], [94mLoss[0m : 2.62701
[1mStep[0m  [8/42], [94mLoss[0m : 2.75256
[1mStep[0m  [12/42], [94mLoss[0m : 2.38331
[1mStep[0m  [16/42], [94mLoss[0m : 2.68679
[1mStep[0m  [20/42], [94mLoss[0m : 2.55108
[1mStep[0m  [24/42], [94mLoss[0m : 2.55504
[1mStep[0m  [28/42], [94mLoss[0m : 2.57396
[1mStep[0m  [32/42], [94mLoss[0m : 2.86925
[1mStep[0m  [36/42], [94mLoss[0m : 2.52399
[1mStep[0m  [40/42], [94mLoss[0m : 2.44832

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.574, [92mTest[0m: 2.365, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35443
[1mStep[0m  [4/42], [94mLoss[0m : 2.61200
[1mStep[0m  [8/42], [94mLoss[0m : 2.79420
[1mStep[0m  [12/42], [94mLoss[0m : 2.74670
[1mStep[0m  [16/42], [94mLoss[0m : 2.58928
[1mStep[0m  [20/42], [94mLoss[0m : 2.53620
[1mStep[0m  [24/42], [94mLoss[0m : 2.53349
[1mStep[0m  [28/42], [94mLoss[0m : 2.48328
[1mStep[0m  [32/42], [94mLoss[0m : 2.48006
[1mStep[0m  [36/42], [94mLoss[0m : 2.53802
[1mStep[0m  [40/42], [94mLoss[0m : 2.58043

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.575, [92mTest[0m: 2.364, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.357
====================================

Phase 1 - Evaluation MAE:  2.357337713241577
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 2.29622
[1mStep[0m  [4/42], [94mLoss[0m : 2.49210
[1mStep[0m  [8/42], [94mLoss[0m : 2.66271
[1mStep[0m  [12/42], [94mLoss[0m : 2.60517
[1mStep[0m  [16/42], [94mLoss[0m : 2.53566
[1mStep[0m  [20/42], [94mLoss[0m : 2.72138
[1mStep[0m  [24/42], [94mLoss[0m : 2.93480
[1mStep[0m  [28/42], [94mLoss[0m : 2.92143
[1mStep[0m  [32/42], [94mLoss[0m : 2.60945
[1mStep[0m  [36/42], [94mLoss[0m : 2.65541
[1mStep[0m  [40/42], [94mLoss[0m : 2.73330

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.596, [92mTest[0m: 2.357, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51508
[1mStep[0m  [4/42], [94mLoss[0m : 2.52539
[1mStep[0m  [8/42], [94mLoss[0m : 2.71727
[1mStep[0m  [12/42], [94mLoss[0m : 2.56075
[1mStep[0m  [16/42], [94mLoss[0m : 2.63944
[1mStep[0m  [20/42], [94mLoss[0m : 2.16183
[1mStep[0m  [24/42], [94mLoss[0m : 2.64980
[1mStep[0m  [28/42], [94mLoss[0m : 2.50964
[1mStep[0m  [32/42], [94mLoss[0m : 2.72618
[1mStep[0m  [36/42], [94mLoss[0m : 2.66551
[1mStep[0m  [40/42], [94mLoss[0m : 2.57579

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.585, [92mTest[0m: 2.388, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38442
[1mStep[0m  [4/42], [94mLoss[0m : 2.53629
[1mStep[0m  [8/42], [94mLoss[0m : 2.46578
[1mStep[0m  [12/42], [94mLoss[0m : 2.55254
[1mStep[0m  [16/42], [94mLoss[0m : 2.42984
[1mStep[0m  [20/42], [94mLoss[0m : 2.71497
[1mStep[0m  [24/42], [94mLoss[0m : 2.56970
[1mStep[0m  [28/42], [94mLoss[0m : 2.57814
[1mStep[0m  [32/42], [94mLoss[0m : 2.77498
[1mStep[0m  [36/42], [94mLoss[0m : 2.39505
[1mStep[0m  [40/42], [94mLoss[0m : 2.51015

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.552, [92mTest[0m: 2.395, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.66584
[1mStep[0m  [4/42], [94mLoss[0m : 2.45766
[1mStep[0m  [8/42], [94mLoss[0m : 2.32667
[1mStep[0m  [12/42], [94mLoss[0m : 2.62113
[1mStep[0m  [16/42], [94mLoss[0m : 2.62022
[1mStep[0m  [20/42], [94mLoss[0m : 2.54605
[1mStep[0m  [24/42], [94mLoss[0m : 2.29340
[1mStep[0m  [28/42], [94mLoss[0m : 2.65868
[1mStep[0m  [32/42], [94mLoss[0m : 2.39818
[1mStep[0m  [36/42], [94mLoss[0m : 2.38656
[1mStep[0m  [40/42], [94mLoss[0m : 2.52171

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.552, [92mTest[0m: 2.427, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.22973
[1mStep[0m  [4/42], [94mLoss[0m : 2.52709
[1mStep[0m  [8/42], [94mLoss[0m : 2.63842
[1mStep[0m  [12/42], [94mLoss[0m : 2.54640
[1mStep[0m  [16/42], [94mLoss[0m : 2.43210
[1mStep[0m  [20/42], [94mLoss[0m : 2.70723
[1mStep[0m  [24/42], [94mLoss[0m : 2.63398
[1mStep[0m  [28/42], [94mLoss[0m : 2.46318
[1mStep[0m  [32/42], [94mLoss[0m : 2.41930
[1mStep[0m  [36/42], [94mLoss[0m : 2.60948
[1mStep[0m  [40/42], [94mLoss[0m : 2.63459

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.414, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55186
[1mStep[0m  [4/42], [94mLoss[0m : 2.53550
[1mStep[0m  [8/42], [94mLoss[0m : 2.62994
[1mStep[0m  [12/42], [94mLoss[0m : 2.45834
[1mStep[0m  [16/42], [94mLoss[0m : 2.67992
[1mStep[0m  [20/42], [94mLoss[0m : 2.33846
[1mStep[0m  [24/42], [94mLoss[0m : 2.59362
[1mStep[0m  [28/42], [94mLoss[0m : 2.31901
[1mStep[0m  [32/42], [94mLoss[0m : 2.39676
[1mStep[0m  [36/42], [94mLoss[0m : 2.30706
[1mStep[0m  [40/42], [94mLoss[0m : 2.27184

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.438, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39259
[1mStep[0m  [4/42], [94mLoss[0m : 2.25680
[1mStep[0m  [8/42], [94mLoss[0m : 2.19765
[1mStep[0m  [12/42], [94mLoss[0m : 2.44343
[1mStep[0m  [16/42], [94mLoss[0m : 2.60681
[1mStep[0m  [20/42], [94mLoss[0m : 2.34706
[1mStep[0m  [24/42], [94mLoss[0m : 2.61216
[1mStep[0m  [28/42], [94mLoss[0m : 2.35641
[1mStep[0m  [32/42], [94mLoss[0m : 2.68254
[1mStep[0m  [36/42], [94mLoss[0m : 2.45767
[1mStep[0m  [40/42], [94mLoss[0m : 2.39955

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.414, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41255
[1mStep[0m  [4/42], [94mLoss[0m : 2.28823
[1mStep[0m  [8/42], [94mLoss[0m : 2.46128
[1mStep[0m  [12/42], [94mLoss[0m : 2.46184
[1mStep[0m  [16/42], [94mLoss[0m : 2.40247
[1mStep[0m  [20/42], [94mLoss[0m : 2.43992
[1mStep[0m  [24/42], [94mLoss[0m : 2.55330
[1mStep[0m  [28/42], [94mLoss[0m : 2.41254
[1mStep[0m  [32/42], [94mLoss[0m : 2.39147
[1mStep[0m  [36/42], [94mLoss[0m : 2.46959
[1mStep[0m  [40/42], [94mLoss[0m : 2.45438

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.459, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44152
[1mStep[0m  [4/42], [94mLoss[0m : 2.40652
[1mStep[0m  [8/42], [94mLoss[0m : 2.46036
[1mStep[0m  [12/42], [94mLoss[0m : 2.46967
[1mStep[0m  [16/42], [94mLoss[0m : 2.27101
[1mStep[0m  [20/42], [94mLoss[0m : 2.47269
[1mStep[0m  [24/42], [94mLoss[0m : 2.59977
[1mStep[0m  [28/42], [94mLoss[0m : 2.39000
[1mStep[0m  [32/42], [94mLoss[0m : 2.63423
[1mStep[0m  [36/42], [94mLoss[0m : 2.52308
[1mStep[0m  [40/42], [94mLoss[0m : 2.51908

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.513, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28002
[1mStep[0m  [4/42], [94mLoss[0m : 2.64274
[1mStep[0m  [8/42], [94mLoss[0m : 2.61174
[1mStep[0m  [12/42], [94mLoss[0m : 2.33520
[1mStep[0m  [16/42], [94mLoss[0m : 2.53959
[1mStep[0m  [20/42], [94mLoss[0m : 2.29443
[1mStep[0m  [24/42], [94mLoss[0m : 2.26443
[1mStep[0m  [28/42], [94mLoss[0m : 2.50610
[1mStep[0m  [32/42], [94mLoss[0m : 2.58734
[1mStep[0m  [36/42], [94mLoss[0m : 2.45594
[1mStep[0m  [40/42], [94mLoss[0m : 2.36175

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.496, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44812
[1mStep[0m  [4/42], [94mLoss[0m : 2.49562
[1mStep[0m  [8/42], [94mLoss[0m : 2.15495
[1mStep[0m  [12/42], [94mLoss[0m : 2.35304
[1mStep[0m  [16/42], [94mLoss[0m : 2.53337
[1mStep[0m  [20/42], [94mLoss[0m : 2.61996
[1mStep[0m  [24/42], [94mLoss[0m : 2.47372
[1mStep[0m  [28/42], [94mLoss[0m : 2.39883
[1mStep[0m  [32/42], [94mLoss[0m : 2.32673
[1mStep[0m  [36/42], [94mLoss[0m : 2.31579
[1mStep[0m  [40/42], [94mLoss[0m : 2.45503

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.504, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.17017
[1mStep[0m  [4/42], [94mLoss[0m : 2.41564
[1mStep[0m  [8/42], [94mLoss[0m : 2.58385
[1mStep[0m  [12/42], [94mLoss[0m : 2.38044
[1mStep[0m  [16/42], [94mLoss[0m : 2.15697
[1mStep[0m  [20/42], [94mLoss[0m : 2.52840
[1mStep[0m  [24/42], [94mLoss[0m : 2.38379
[1mStep[0m  [28/42], [94mLoss[0m : 2.36909
[1mStep[0m  [32/42], [94mLoss[0m : 2.36219
[1mStep[0m  [36/42], [94mLoss[0m : 2.26652
[1mStep[0m  [40/42], [94mLoss[0m : 2.44007

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.387, [92mTest[0m: 2.428, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24986
[1mStep[0m  [4/42], [94mLoss[0m : 2.46538
[1mStep[0m  [8/42], [94mLoss[0m : 2.35566
[1mStep[0m  [12/42], [94mLoss[0m : 2.46360
[1mStep[0m  [16/42], [94mLoss[0m : 2.12191
[1mStep[0m  [20/42], [94mLoss[0m : 2.44530
[1mStep[0m  [24/42], [94mLoss[0m : 2.33421
[1mStep[0m  [28/42], [94mLoss[0m : 2.39538
[1mStep[0m  [32/42], [94mLoss[0m : 2.52837
[1mStep[0m  [36/42], [94mLoss[0m : 2.40514
[1mStep[0m  [40/42], [94mLoss[0m : 2.43961

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.338, [92mTest[0m: 2.459, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29426
[1mStep[0m  [4/42], [94mLoss[0m : 2.30648
[1mStep[0m  [8/42], [94mLoss[0m : 2.30629
[1mStep[0m  [12/42], [94mLoss[0m : 2.33054
[1mStep[0m  [16/42], [94mLoss[0m : 2.18240
[1mStep[0m  [20/42], [94mLoss[0m : 2.32490
[1mStep[0m  [24/42], [94mLoss[0m : 2.34686
[1mStep[0m  [28/42], [94mLoss[0m : 2.18885
[1mStep[0m  [32/42], [94mLoss[0m : 2.34383
[1mStep[0m  [36/42], [94mLoss[0m : 2.18075
[1mStep[0m  [40/42], [94mLoss[0m : 2.25035

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.316, [92mTest[0m: 2.562, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.07467
[1mStep[0m  [4/42], [94mLoss[0m : 2.28375
[1mStep[0m  [8/42], [94mLoss[0m : 2.25025
[1mStep[0m  [12/42], [94mLoss[0m : 2.01058
[1mStep[0m  [16/42], [94mLoss[0m : 2.34652
[1mStep[0m  [20/42], [94mLoss[0m : 2.47357
[1mStep[0m  [24/42], [94mLoss[0m : 2.44997
[1mStep[0m  [28/42], [94mLoss[0m : 2.38838
[1mStep[0m  [32/42], [94mLoss[0m : 2.21962
[1mStep[0m  [36/42], [94mLoss[0m : 2.28365
[1mStep[0m  [40/42], [94mLoss[0m : 2.47000

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.304, [92mTest[0m: 2.461, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.08825
[1mStep[0m  [4/42], [94mLoss[0m : 2.48046
[1mStep[0m  [8/42], [94mLoss[0m : 2.38282
[1mStep[0m  [12/42], [94mLoss[0m : 2.26414
[1mStep[0m  [16/42], [94mLoss[0m : 2.34289
[1mStep[0m  [20/42], [94mLoss[0m : 2.29885
[1mStep[0m  [24/42], [94mLoss[0m : 2.34916
[1mStep[0m  [28/42], [94mLoss[0m : 2.25003
[1mStep[0m  [32/42], [94mLoss[0m : 2.20519
[1mStep[0m  [36/42], [94mLoss[0m : 2.19200
[1mStep[0m  [40/42], [94mLoss[0m : 2.34816

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.261, [92mTest[0m: 2.463, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.20938
[1mStep[0m  [4/42], [94mLoss[0m : 2.19026
[1mStep[0m  [8/42], [94mLoss[0m : 2.43047
[1mStep[0m  [12/42], [94mLoss[0m : 2.30579
[1mStep[0m  [16/42], [94mLoss[0m : 2.48903
[1mStep[0m  [20/42], [94mLoss[0m : 2.33582
[1mStep[0m  [24/42], [94mLoss[0m : 2.12729
[1mStep[0m  [28/42], [94mLoss[0m : 2.41448
[1mStep[0m  [32/42], [94mLoss[0m : 2.40501
[1mStep[0m  [36/42], [94mLoss[0m : 2.17178
[1mStep[0m  [40/42], [94mLoss[0m : 2.42685

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.258, [92mTest[0m: 2.538, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33130
[1mStep[0m  [4/42], [94mLoss[0m : 2.13896
[1mStep[0m  [8/42], [94mLoss[0m : 2.11742
[1mStep[0m  [12/42], [94mLoss[0m : 2.14893
[1mStep[0m  [16/42], [94mLoss[0m : 2.42442
[1mStep[0m  [20/42], [94mLoss[0m : 2.29227
[1mStep[0m  [24/42], [94mLoss[0m : 2.44335
[1mStep[0m  [28/42], [94mLoss[0m : 2.01522
[1mStep[0m  [32/42], [94mLoss[0m : 2.14932
[1mStep[0m  [36/42], [94mLoss[0m : 2.21986
[1mStep[0m  [40/42], [94mLoss[0m : 2.13656

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.228, [92mTest[0m: 2.548, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21286
[1mStep[0m  [4/42], [94mLoss[0m : 2.21483
[1mStep[0m  [8/42], [94mLoss[0m : 2.29491
[1mStep[0m  [12/42], [94mLoss[0m : 2.08896
[1mStep[0m  [16/42], [94mLoss[0m : 2.21651
[1mStep[0m  [20/42], [94mLoss[0m : 2.39433
[1mStep[0m  [24/42], [94mLoss[0m : 2.16991
[1mStep[0m  [28/42], [94mLoss[0m : 2.32576
[1mStep[0m  [32/42], [94mLoss[0m : 2.23059
[1mStep[0m  [36/42], [94mLoss[0m : 2.31991
[1mStep[0m  [40/42], [94mLoss[0m : 2.16619

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.192, [92mTest[0m: 2.547, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.12115
[1mStep[0m  [4/42], [94mLoss[0m : 2.03715
[1mStep[0m  [8/42], [94mLoss[0m : 2.04401
[1mStep[0m  [12/42], [94mLoss[0m : 2.17641
[1mStep[0m  [16/42], [94mLoss[0m : 2.07322
[1mStep[0m  [20/42], [94mLoss[0m : 2.18925
[1mStep[0m  [24/42], [94mLoss[0m : 2.03264
[1mStep[0m  [28/42], [94mLoss[0m : 2.15374
[1mStep[0m  [32/42], [94mLoss[0m : 2.16564
[1mStep[0m  [36/42], [94mLoss[0m : 2.21182
[1mStep[0m  [40/42], [94mLoss[0m : 2.29095

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.187, [92mTest[0m: 2.476, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.30592
[1mStep[0m  [4/42], [94mLoss[0m : 2.11233
[1mStep[0m  [8/42], [94mLoss[0m : 2.10715
[1mStep[0m  [12/42], [94mLoss[0m : 2.63728
[1mStep[0m  [16/42], [94mLoss[0m : 2.16307
[1mStep[0m  [20/42], [94mLoss[0m : 2.29323
[1mStep[0m  [24/42], [94mLoss[0m : 2.27705
[1mStep[0m  [28/42], [94mLoss[0m : 1.99866
[1mStep[0m  [32/42], [94mLoss[0m : 2.35176
[1mStep[0m  [36/42], [94mLoss[0m : 2.09229
[1mStep[0m  [40/42], [94mLoss[0m : 2.10932

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.171, [92mTest[0m: 2.520, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.20001
[1mStep[0m  [4/42], [94mLoss[0m : 2.17707
[1mStep[0m  [8/42], [94mLoss[0m : 2.15664
[1mStep[0m  [12/42], [94mLoss[0m : 2.15417
[1mStep[0m  [16/42], [94mLoss[0m : 2.25645
[1mStep[0m  [20/42], [94mLoss[0m : 2.00491
[1mStep[0m  [24/42], [94mLoss[0m : 2.18389
[1mStep[0m  [28/42], [94mLoss[0m : 2.07821
[1mStep[0m  [32/42], [94mLoss[0m : 2.13904
[1mStep[0m  [36/42], [94mLoss[0m : 2.13489
[1mStep[0m  [40/42], [94mLoss[0m : 2.17374

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.135, [92mTest[0m: 2.530, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.09017
[1mStep[0m  [4/42], [94mLoss[0m : 2.22378
[1mStep[0m  [8/42], [94mLoss[0m : 2.11264
[1mStep[0m  [12/42], [94mLoss[0m : 2.04505
[1mStep[0m  [16/42], [94mLoss[0m : 2.14598
[1mStep[0m  [20/42], [94mLoss[0m : 2.06778
[1mStep[0m  [24/42], [94mLoss[0m : 2.02348
[1mStep[0m  [28/42], [94mLoss[0m : 2.31326
[1mStep[0m  [32/42], [94mLoss[0m : 2.23261
[1mStep[0m  [36/42], [94mLoss[0m : 2.09122
[1mStep[0m  [40/42], [94mLoss[0m : 2.07395

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.109, [92mTest[0m: 2.556, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.92957
[1mStep[0m  [4/42], [94mLoss[0m : 2.19193
[1mStep[0m  [8/42], [94mLoss[0m : 2.15197
[1mStep[0m  [12/42], [94mLoss[0m : 2.24505
[1mStep[0m  [16/42], [94mLoss[0m : 2.04973
[1mStep[0m  [20/42], [94mLoss[0m : 2.17785
[1mStep[0m  [24/42], [94mLoss[0m : 2.00121
[1mStep[0m  [28/42], [94mLoss[0m : 2.01379
[1mStep[0m  [32/42], [94mLoss[0m : 2.14515
[1mStep[0m  [36/42], [94mLoss[0m : 2.16415
[1mStep[0m  [40/42], [94mLoss[0m : 2.03315

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.094, [92mTest[0m: 2.499, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.07476
[1mStep[0m  [4/42], [94mLoss[0m : 2.05944
[1mStep[0m  [8/42], [94mLoss[0m : 2.06288
[1mStep[0m  [12/42], [94mLoss[0m : 2.23418
[1mStep[0m  [16/42], [94mLoss[0m : 1.97634
[1mStep[0m  [20/42], [94mLoss[0m : 1.94578
[1mStep[0m  [24/42], [94mLoss[0m : 1.93745
[1mStep[0m  [28/42], [94mLoss[0m : 2.08158
[1mStep[0m  [32/42], [94mLoss[0m : 2.09142
[1mStep[0m  [36/42], [94mLoss[0m : 2.23073
[1mStep[0m  [40/42], [94mLoss[0m : 2.19752

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.086, [92mTest[0m: 2.562, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.85594
[1mStep[0m  [4/42], [94mLoss[0m : 1.98442
[1mStep[0m  [8/42], [94mLoss[0m : 2.22157
[1mStep[0m  [12/42], [94mLoss[0m : 2.11665
[1mStep[0m  [16/42], [94mLoss[0m : 1.90145
[1mStep[0m  [20/42], [94mLoss[0m : 1.91526
[1mStep[0m  [24/42], [94mLoss[0m : 1.95015
[1mStep[0m  [28/42], [94mLoss[0m : 1.97783
[1mStep[0m  [32/42], [94mLoss[0m : 2.05420
[1mStep[0m  [36/42], [94mLoss[0m : 2.18540
[1mStep[0m  [40/42], [94mLoss[0m : 2.15115

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.047, [92mTest[0m: 2.550, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.90567
[1mStep[0m  [4/42], [94mLoss[0m : 1.84628
[1mStep[0m  [8/42], [94mLoss[0m : 2.02198
[1mStep[0m  [12/42], [94mLoss[0m : 2.03596
[1mStep[0m  [16/42], [94mLoss[0m : 2.11525
[1mStep[0m  [20/42], [94mLoss[0m : 1.83231
[1mStep[0m  [24/42], [94mLoss[0m : 2.01859
[1mStep[0m  [28/42], [94mLoss[0m : 1.91894
[1mStep[0m  [32/42], [94mLoss[0m : 2.09225
[1mStep[0m  [36/42], [94mLoss[0m : 1.98490
[1mStep[0m  [40/42], [94mLoss[0m : 2.06394

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.020, [92mTest[0m: 2.447, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.88971
[1mStep[0m  [4/42], [94mLoss[0m : 1.83493
[1mStep[0m  [8/42], [94mLoss[0m : 1.90890
[1mStep[0m  [12/42], [94mLoss[0m : 2.00049
[1mStep[0m  [16/42], [94mLoss[0m : 1.87960
[1mStep[0m  [20/42], [94mLoss[0m : 2.09431
[1mStep[0m  [24/42], [94mLoss[0m : 1.93222
[1mStep[0m  [28/42], [94mLoss[0m : 2.00351
[1mStep[0m  [32/42], [94mLoss[0m : 1.86029
[1mStep[0m  [36/42], [94mLoss[0m : 1.87643
[1mStep[0m  [40/42], [94mLoss[0m : 2.15996

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.983, [92mTest[0m: 2.535, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.94020
[1mStep[0m  [4/42], [94mLoss[0m : 1.99213
[1mStep[0m  [8/42], [94mLoss[0m : 2.00447
[1mStep[0m  [12/42], [94mLoss[0m : 2.19816
[1mStep[0m  [16/42], [94mLoss[0m : 2.02485
[1mStep[0m  [20/42], [94mLoss[0m : 1.99511
[1mStep[0m  [24/42], [94mLoss[0m : 1.98042
[1mStep[0m  [28/42], [94mLoss[0m : 1.89004
[1mStep[0m  [32/42], [94mLoss[0m : 2.00187
[1mStep[0m  [36/42], [94mLoss[0m : 1.87961
[1mStep[0m  [40/42], [94mLoss[0m : 1.96633

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.978, [92mTest[0m: 2.528, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.17221
[1mStep[0m  [4/42], [94mLoss[0m : 2.04397
[1mStep[0m  [8/42], [94mLoss[0m : 1.82604
[1mStep[0m  [12/42], [94mLoss[0m : 2.11065
[1mStep[0m  [16/42], [94mLoss[0m : 2.04062
[1mStep[0m  [20/42], [94mLoss[0m : 1.92218
[1mStep[0m  [24/42], [94mLoss[0m : 1.93200
[1mStep[0m  [28/42], [94mLoss[0m : 1.88399
[1mStep[0m  [32/42], [94mLoss[0m : 2.04920
[1mStep[0m  [36/42], [94mLoss[0m : 2.13696
[1mStep[0m  [40/42], [94mLoss[0m : 1.86944

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.977, [92mTest[0m: 2.496, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.523
====================================

Phase 2 - Evaluation MAE:  2.522916282926287
MAE score P1       2.357338
MAE score P2       2.522916
loss               1.977134
learning_rate      0.002575
batch_size              256
hidden_sizes      [200, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.5
weight_decay          0.001
Name: 25, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 11.03587
[1mStep[0m  [4/42], [94mLoss[0m : 10.63221
[1mStep[0m  [8/42], [94mLoss[0m : 10.19478
[1mStep[0m  [12/42], [94mLoss[0m : 9.07415
[1mStep[0m  [16/42], [94mLoss[0m : 7.89628
[1mStep[0m  [20/42], [94mLoss[0m : 6.31434
[1mStep[0m  [24/42], [94mLoss[0m : 5.03869
[1mStep[0m  [28/42], [94mLoss[0m : 4.37034
[1mStep[0m  [32/42], [94mLoss[0m : 2.88987
[1mStep[0m  [36/42], [94mLoss[0m : 2.87276
[1mStep[0m  [40/42], [94mLoss[0m : 2.77946

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.528, [92mTest[0m: 11.146, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.87013
[1mStep[0m  [4/42], [94mLoss[0m : 2.82970
[1mStep[0m  [8/42], [94mLoss[0m : 2.66517
[1mStep[0m  [12/42], [94mLoss[0m : 2.51731
[1mStep[0m  [16/42], [94mLoss[0m : 2.75600
[1mStep[0m  [20/42], [94mLoss[0m : 2.60509
[1mStep[0m  [24/42], [94mLoss[0m : 2.72445
[1mStep[0m  [28/42], [94mLoss[0m : 2.54381
[1mStep[0m  [32/42], [94mLoss[0m : 2.53638
[1mStep[0m  [36/42], [94mLoss[0m : 2.77996
[1mStep[0m  [40/42], [94mLoss[0m : 2.50213

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.670, [92mTest[0m: 2.949, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.78602
[1mStep[0m  [4/42], [94mLoss[0m : 2.44121
[1mStep[0m  [8/42], [94mLoss[0m : 2.23595
[1mStep[0m  [12/42], [94mLoss[0m : 2.49538
[1mStep[0m  [16/42], [94mLoss[0m : 2.52241
[1mStep[0m  [20/42], [94mLoss[0m : 2.59861
[1mStep[0m  [24/42], [94mLoss[0m : 2.54076
[1mStep[0m  [28/42], [94mLoss[0m : 2.78462
[1mStep[0m  [32/42], [94mLoss[0m : 2.58861
[1mStep[0m  [36/42], [94mLoss[0m : 2.58105
[1mStep[0m  [40/42], [94mLoss[0m : 2.46594

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.546, [92mTest[0m: 2.571, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48101
[1mStep[0m  [4/42], [94mLoss[0m : 2.49275
[1mStep[0m  [8/42], [94mLoss[0m : 2.61334
[1mStep[0m  [12/42], [94mLoss[0m : 2.46786
[1mStep[0m  [16/42], [94mLoss[0m : 2.42287
[1mStep[0m  [20/42], [94mLoss[0m : 2.53810
[1mStep[0m  [24/42], [94mLoss[0m : 2.62724
[1mStep[0m  [28/42], [94mLoss[0m : 2.60053
[1mStep[0m  [32/42], [94mLoss[0m : 2.51632
[1mStep[0m  [36/42], [94mLoss[0m : 2.59577
[1mStep[0m  [40/42], [94mLoss[0m : 2.45102

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.520, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52733
[1mStep[0m  [4/42], [94mLoss[0m : 2.56652
[1mStep[0m  [8/42], [94mLoss[0m : 2.30875
[1mStep[0m  [12/42], [94mLoss[0m : 2.56706
[1mStep[0m  [16/42], [94mLoss[0m : 2.71959
[1mStep[0m  [20/42], [94mLoss[0m : 2.53801
[1mStep[0m  [24/42], [94mLoss[0m : 2.56289
[1mStep[0m  [28/42], [94mLoss[0m : 2.64306
[1mStep[0m  [32/42], [94mLoss[0m : 2.52203
[1mStep[0m  [36/42], [94mLoss[0m : 2.31834
[1mStep[0m  [40/42], [94mLoss[0m : 2.53150

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.415, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.71246
[1mStep[0m  [4/42], [94mLoss[0m : 2.51141
[1mStep[0m  [8/42], [94mLoss[0m : 2.60454
[1mStep[0m  [12/42], [94mLoss[0m : 2.33340
[1mStep[0m  [16/42], [94mLoss[0m : 2.53064
[1mStep[0m  [20/42], [94mLoss[0m : 2.60121
[1mStep[0m  [24/42], [94mLoss[0m : 2.36213
[1mStep[0m  [28/42], [94mLoss[0m : 2.20847
[1mStep[0m  [32/42], [94mLoss[0m : 2.20717
[1mStep[0m  [36/42], [94mLoss[0m : 2.72205
[1mStep[0m  [40/42], [94mLoss[0m : 2.53842

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.435, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52679
[1mStep[0m  [4/42], [94mLoss[0m : 2.20503
[1mStep[0m  [8/42], [94mLoss[0m : 2.67945
[1mStep[0m  [12/42], [94mLoss[0m : 2.47872
[1mStep[0m  [16/42], [94mLoss[0m : 2.33847
[1mStep[0m  [20/42], [94mLoss[0m : 2.45917
[1mStep[0m  [24/42], [94mLoss[0m : 2.57027
[1mStep[0m  [28/42], [94mLoss[0m : 2.49802
[1mStep[0m  [32/42], [94mLoss[0m : 2.38688
[1mStep[0m  [36/42], [94mLoss[0m : 2.34758
[1mStep[0m  [40/42], [94mLoss[0m : 2.36187

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.361, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.65761
[1mStep[0m  [4/42], [94mLoss[0m : 2.55533
[1mStep[0m  [8/42], [94mLoss[0m : 2.51035
[1mStep[0m  [12/42], [94mLoss[0m : 2.39767
[1mStep[0m  [16/42], [94mLoss[0m : 2.41315
[1mStep[0m  [20/42], [94mLoss[0m : 2.15757
[1mStep[0m  [24/42], [94mLoss[0m : 2.37726
[1mStep[0m  [28/42], [94mLoss[0m : 2.63485
[1mStep[0m  [32/42], [94mLoss[0m : 2.44934
[1mStep[0m  [36/42], [94mLoss[0m : 2.38687
[1mStep[0m  [40/42], [94mLoss[0m : 2.48518

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.438, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47068
[1mStep[0m  [4/42], [94mLoss[0m : 2.35501
[1mStep[0m  [8/42], [94mLoss[0m : 2.43346
[1mStep[0m  [12/42], [94mLoss[0m : 2.51699
[1mStep[0m  [16/42], [94mLoss[0m : 2.53182
[1mStep[0m  [20/42], [94mLoss[0m : 2.38427
[1mStep[0m  [24/42], [94mLoss[0m : 2.28138
[1mStep[0m  [28/42], [94mLoss[0m : 2.52581
[1mStep[0m  [32/42], [94mLoss[0m : 2.49780
[1mStep[0m  [36/42], [94mLoss[0m : 2.56781
[1mStep[0m  [40/42], [94mLoss[0m : 2.30592

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.365, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42389
[1mStep[0m  [4/42], [94mLoss[0m : 2.40339
[1mStep[0m  [8/42], [94mLoss[0m : 2.38288
[1mStep[0m  [12/42], [94mLoss[0m : 2.32844
[1mStep[0m  [16/42], [94mLoss[0m : 2.32848
[1mStep[0m  [20/42], [94mLoss[0m : 2.52218
[1mStep[0m  [24/42], [94mLoss[0m : 2.33610
[1mStep[0m  [28/42], [94mLoss[0m : 2.36383
[1mStep[0m  [32/42], [94mLoss[0m : 2.43016
[1mStep[0m  [36/42], [94mLoss[0m : 2.58226
[1mStep[0m  [40/42], [94mLoss[0m : 2.33505

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.362, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.73540
[1mStep[0m  [4/42], [94mLoss[0m : 2.50110
[1mStep[0m  [8/42], [94mLoss[0m : 2.58263
[1mStep[0m  [12/42], [94mLoss[0m : 2.16470
[1mStep[0m  [16/42], [94mLoss[0m : 2.52169
[1mStep[0m  [20/42], [94mLoss[0m : 2.38926
[1mStep[0m  [24/42], [94mLoss[0m : 2.27390
[1mStep[0m  [28/42], [94mLoss[0m : 2.56440
[1mStep[0m  [32/42], [94mLoss[0m : 2.38367
[1mStep[0m  [36/42], [94mLoss[0m : 2.34783
[1mStep[0m  [40/42], [94mLoss[0m : 2.32494

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.356, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42032
[1mStep[0m  [4/42], [94mLoss[0m : 2.36962
[1mStep[0m  [8/42], [94mLoss[0m : 2.35078
[1mStep[0m  [12/42], [94mLoss[0m : 2.31565
[1mStep[0m  [16/42], [94mLoss[0m : 2.18212
[1mStep[0m  [20/42], [94mLoss[0m : 2.41110
[1mStep[0m  [24/42], [94mLoss[0m : 2.34029
[1mStep[0m  [28/42], [94mLoss[0m : 2.32636
[1mStep[0m  [32/42], [94mLoss[0m : 2.46847
[1mStep[0m  [36/42], [94mLoss[0m : 2.29439
[1mStep[0m  [40/42], [94mLoss[0m : 2.31046

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.396, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.68500
[1mStep[0m  [4/42], [94mLoss[0m : 2.48731
[1mStep[0m  [8/42], [94mLoss[0m : 2.37947
[1mStep[0m  [12/42], [94mLoss[0m : 2.34794
[1mStep[0m  [16/42], [94mLoss[0m : 2.50132
[1mStep[0m  [20/42], [94mLoss[0m : 2.46635
[1mStep[0m  [24/42], [94mLoss[0m : 2.59536
[1mStep[0m  [28/42], [94mLoss[0m : 2.29680
[1mStep[0m  [32/42], [94mLoss[0m : 2.40973
[1mStep[0m  [36/42], [94mLoss[0m : 2.44273
[1mStep[0m  [40/42], [94mLoss[0m : 2.29818

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.371, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.65639
[1mStep[0m  [4/42], [94mLoss[0m : 2.42386
[1mStep[0m  [8/42], [94mLoss[0m : 2.35689
[1mStep[0m  [12/42], [94mLoss[0m : 2.42480
[1mStep[0m  [16/42], [94mLoss[0m : 2.26470
[1mStep[0m  [20/42], [94mLoss[0m : 2.31639
[1mStep[0m  [24/42], [94mLoss[0m : 2.49267
[1mStep[0m  [28/42], [94mLoss[0m : 2.38986
[1mStep[0m  [32/42], [94mLoss[0m : 2.42376
[1mStep[0m  [36/42], [94mLoss[0m : 2.71024
[1mStep[0m  [40/42], [94mLoss[0m : 2.51135

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.390, [92mTest[0m: 2.340, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35539
[1mStep[0m  [4/42], [94mLoss[0m : 2.39473
[1mStep[0m  [8/42], [94mLoss[0m : 2.49285
[1mStep[0m  [12/42], [94mLoss[0m : 2.39388
[1mStep[0m  [16/42], [94mLoss[0m : 2.36284
[1mStep[0m  [20/42], [94mLoss[0m : 2.43432
[1mStep[0m  [24/42], [94mLoss[0m : 2.50117
[1mStep[0m  [28/42], [94mLoss[0m : 2.40692
[1mStep[0m  [32/42], [94mLoss[0m : 2.36901
[1mStep[0m  [36/42], [94mLoss[0m : 2.47251
[1mStep[0m  [40/42], [94mLoss[0m : 2.68338

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.396, [92mTest[0m: 2.351, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31112
[1mStep[0m  [4/42], [94mLoss[0m : 2.43845
[1mStep[0m  [8/42], [94mLoss[0m : 2.53874
[1mStep[0m  [12/42], [94mLoss[0m : 2.12074
[1mStep[0m  [16/42], [94mLoss[0m : 2.29731
[1mStep[0m  [20/42], [94mLoss[0m : 2.52807
[1mStep[0m  [24/42], [94mLoss[0m : 2.02647
[1mStep[0m  [28/42], [94mLoss[0m : 2.34896
[1mStep[0m  [32/42], [94mLoss[0m : 2.37376
[1mStep[0m  [36/42], [94mLoss[0m : 2.53285
[1mStep[0m  [40/42], [94mLoss[0m : 2.28774

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.377, [92mTest[0m: 2.317, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43439
[1mStep[0m  [4/42], [94mLoss[0m : 2.58073
[1mStep[0m  [8/42], [94mLoss[0m : 2.43851
[1mStep[0m  [12/42], [94mLoss[0m : 2.37693
[1mStep[0m  [16/42], [94mLoss[0m : 2.18677
[1mStep[0m  [20/42], [94mLoss[0m : 2.40213
[1mStep[0m  [24/42], [94mLoss[0m : 2.36587
[1mStep[0m  [28/42], [94mLoss[0m : 2.28644
[1mStep[0m  [32/42], [94mLoss[0m : 2.37279
[1mStep[0m  [36/42], [94mLoss[0m : 2.39098
[1mStep[0m  [40/42], [94mLoss[0m : 2.31673

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.363, [92mTest[0m: 2.333, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.30784
[1mStep[0m  [4/42], [94mLoss[0m : 2.21063
[1mStep[0m  [8/42], [94mLoss[0m : 2.41290
[1mStep[0m  [12/42], [94mLoss[0m : 2.42817
[1mStep[0m  [16/42], [94mLoss[0m : 2.45170
[1mStep[0m  [20/42], [94mLoss[0m : 2.32522
[1mStep[0m  [24/42], [94mLoss[0m : 2.23537
[1mStep[0m  [28/42], [94mLoss[0m : 2.43734
[1mStep[0m  [32/42], [94mLoss[0m : 2.64706
[1mStep[0m  [36/42], [94mLoss[0m : 2.39544
[1mStep[0m  [40/42], [94mLoss[0m : 2.26985

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.379, [92mTest[0m: 2.324, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36959
[1mStep[0m  [4/42], [94mLoss[0m : 2.48214
[1mStep[0m  [8/42], [94mLoss[0m : 2.20995
[1mStep[0m  [12/42], [94mLoss[0m : 2.27499
[1mStep[0m  [16/42], [94mLoss[0m : 2.33240
[1mStep[0m  [20/42], [94mLoss[0m : 2.30618
[1mStep[0m  [24/42], [94mLoss[0m : 2.38661
[1mStep[0m  [28/42], [94mLoss[0m : 2.29009
[1mStep[0m  [32/42], [94mLoss[0m : 2.42470
[1mStep[0m  [36/42], [94mLoss[0m : 2.68415
[1mStep[0m  [40/42], [94mLoss[0m : 2.37459

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.358, [92mTest[0m: 2.365, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29767
[1mStep[0m  [4/42], [94mLoss[0m : 2.47243
[1mStep[0m  [8/42], [94mLoss[0m : 2.52433
[1mStep[0m  [12/42], [94mLoss[0m : 2.46785
[1mStep[0m  [16/42], [94mLoss[0m : 2.59293
[1mStep[0m  [20/42], [94mLoss[0m : 2.30834
[1mStep[0m  [24/42], [94mLoss[0m : 2.45123
[1mStep[0m  [28/42], [94mLoss[0m : 2.14439
[1mStep[0m  [32/42], [94mLoss[0m : 2.14920
[1mStep[0m  [36/42], [94mLoss[0m : 2.42518
[1mStep[0m  [40/42], [94mLoss[0m : 2.53183

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.371, [92mTest[0m: 2.336, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27724
[1mStep[0m  [4/42], [94mLoss[0m : 2.35478
[1mStep[0m  [8/42], [94mLoss[0m : 2.45541
[1mStep[0m  [12/42], [94mLoss[0m : 2.27777
[1mStep[0m  [16/42], [94mLoss[0m : 2.34900
[1mStep[0m  [20/42], [94mLoss[0m : 2.44351
[1mStep[0m  [24/42], [94mLoss[0m : 2.43958
[1mStep[0m  [28/42], [94mLoss[0m : 2.20208
[1mStep[0m  [32/42], [94mLoss[0m : 2.42353
[1mStep[0m  [36/42], [94mLoss[0m : 2.39863
[1mStep[0m  [40/42], [94mLoss[0m : 2.21566

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.351, [92mTest[0m: 2.321, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31133
[1mStep[0m  [4/42], [94mLoss[0m : 2.30734
[1mStep[0m  [8/42], [94mLoss[0m : 2.16188
[1mStep[0m  [12/42], [94mLoss[0m : 2.40987
[1mStep[0m  [16/42], [94mLoss[0m : 2.42746
[1mStep[0m  [20/42], [94mLoss[0m : 2.55051
[1mStep[0m  [24/42], [94mLoss[0m : 2.40441
[1mStep[0m  [28/42], [94mLoss[0m : 2.47749
[1mStep[0m  [32/42], [94mLoss[0m : 2.35674
[1mStep[0m  [36/42], [94mLoss[0m : 2.25630
[1mStep[0m  [40/42], [94mLoss[0m : 2.25483

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.337, [92mTest[0m: 2.303, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32666
[1mStep[0m  [4/42], [94mLoss[0m : 2.32457
[1mStep[0m  [8/42], [94mLoss[0m : 2.46043
[1mStep[0m  [12/42], [94mLoss[0m : 2.57200
[1mStep[0m  [16/42], [94mLoss[0m : 2.61664
[1mStep[0m  [20/42], [94mLoss[0m : 2.48283
[1mStep[0m  [24/42], [94mLoss[0m : 2.39071
[1mStep[0m  [28/42], [94mLoss[0m : 2.58670
[1mStep[0m  [32/42], [94mLoss[0m : 2.18487
[1mStep[0m  [36/42], [94mLoss[0m : 2.38533
[1mStep[0m  [40/42], [94mLoss[0m : 2.32487

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.341, [92mTest[0m: 2.283, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44928
[1mStep[0m  [4/42], [94mLoss[0m : 2.38726
[1mStep[0m  [8/42], [94mLoss[0m : 2.57593
[1mStep[0m  [12/42], [94mLoss[0m : 2.52871
[1mStep[0m  [16/42], [94mLoss[0m : 2.20898
[1mStep[0m  [20/42], [94mLoss[0m : 2.19426
[1mStep[0m  [24/42], [94mLoss[0m : 2.31324
[1mStep[0m  [28/42], [94mLoss[0m : 2.37368
[1mStep[0m  [32/42], [94mLoss[0m : 2.23371
[1mStep[0m  [36/42], [94mLoss[0m : 2.45027
[1mStep[0m  [40/42], [94mLoss[0m : 2.34660

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.345, [92mTest[0m: 2.312, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28018
[1mStep[0m  [4/42], [94mLoss[0m : 2.31288
[1mStep[0m  [8/42], [94mLoss[0m : 2.35006
[1mStep[0m  [12/42], [94mLoss[0m : 2.30357
[1mStep[0m  [16/42], [94mLoss[0m : 2.20371
[1mStep[0m  [20/42], [94mLoss[0m : 2.31522
[1mStep[0m  [24/42], [94mLoss[0m : 2.33768
[1mStep[0m  [28/42], [94mLoss[0m : 2.27859
[1mStep[0m  [32/42], [94mLoss[0m : 2.34468
[1mStep[0m  [36/42], [94mLoss[0m : 2.41370
[1mStep[0m  [40/42], [94mLoss[0m : 2.15280

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.345, [92mTest[0m: 2.320, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35313
[1mStep[0m  [4/42], [94mLoss[0m : 2.18125
[1mStep[0m  [8/42], [94mLoss[0m : 2.43352
[1mStep[0m  [12/42], [94mLoss[0m : 2.34199
[1mStep[0m  [16/42], [94mLoss[0m : 2.34053
[1mStep[0m  [20/42], [94mLoss[0m : 2.15684
[1mStep[0m  [24/42], [94mLoss[0m : 2.31182
[1mStep[0m  [28/42], [94mLoss[0m : 2.55984
[1mStep[0m  [32/42], [94mLoss[0m : 2.42339
[1mStep[0m  [36/42], [94mLoss[0m : 2.34267
[1mStep[0m  [40/42], [94mLoss[0m : 2.37125

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.324, [92mTest[0m: 2.326, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24114
[1mStep[0m  [4/42], [94mLoss[0m : 2.32055
[1mStep[0m  [8/42], [94mLoss[0m : 2.38577
[1mStep[0m  [12/42], [94mLoss[0m : 2.43345
[1mStep[0m  [16/42], [94mLoss[0m : 2.51441
[1mStep[0m  [20/42], [94mLoss[0m : 2.25655
[1mStep[0m  [24/42], [94mLoss[0m : 2.70730
[1mStep[0m  [28/42], [94mLoss[0m : 2.36417
[1mStep[0m  [32/42], [94mLoss[0m : 2.41161
[1mStep[0m  [36/42], [94mLoss[0m : 2.04567
[1mStep[0m  [40/42], [94mLoss[0m : 2.27722

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.333, [92mTest[0m: 2.346, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24219
[1mStep[0m  [4/42], [94mLoss[0m : 2.33381
[1mStep[0m  [8/42], [94mLoss[0m : 2.26174
[1mStep[0m  [12/42], [94mLoss[0m : 2.30856
[1mStep[0m  [16/42], [94mLoss[0m : 2.43411
[1mStep[0m  [20/42], [94mLoss[0m : 2.38520
[1mStep[0m  [24/42], [94mLoss[0m : 2.29733
[1mStep[0m  [28/42], [94mLoss[0m : 2.25349
[1mStep[0m  [32/42], [94mLoss[0m : 2.37707
[1mStep[0m  [36/42], [94mLoss[0m : 2.36819
[1mStep[0m  [40/42], [94mLoss[0m : 2.36527

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.333, [92mTest[0m: 2.302, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57197
[1mStep[0m  [4/42], [94mLoss[0m : 2.43450
[1mStep[0m  [8/42], [94mLoss[0m : 2.28688
[1mStep[0m  [12/42], [94mLoss[0m : 2.32485
[1mStep[0m  [16/42], [94mLoss[0m : 2.20541
[1mStep[0m  [20/42], [94mLoss[0m : 2.28329
[1mStep[0m  [24/42], [94mLoss[0m : 2.33896
[1mStep[0m  [28/42], [94mLoss[0m : 2.39588
[1mStep[0m  [32/42], [94mLoss[0m : 2.27589
[1mStep[0m  [36/42], [94mLoss[0m : 2.36866
[1mStep[0m  [40/42], [94mLoss[0m : 2.31647

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.313, [92mTest[0m: 2.317, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39678
[1mStep[0m  [4/42], [94mLoss[0m : 2.45553
[1mStep[0m  [8/42], [94mLoss[0m : 2.20954
[1mStep[0m  [12/42], [94mLoss[0m : 2.28374
[1mStep[0m  [16/42], [94mLoss[0m : 2.27735
[1mStep[0m  [20/42], [94mLoss[0m : 2.25308
[1mStep[0m  [24/42], [94mLoss[0m : 2.15441
[1mStep[0m  [28/42], [94mLoss[0m : 2.32268
[1mStep[0m  [32/42], [94mLoss[0m : 2.32549
[1mStep[0m  [36/42], [94mLoss[0m : 2.39061
[1mStep[0m  [40/42], [94mLoss[0m : 2.19779

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.314, [92mTest[0m: 2.296, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.302
====================================

Phase 1 - Evaluation MAE:  2.3022294555391585
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 2.36242
[1mStep[0m  [4/42], [94mLoss[0m : 2.36624
[1mStep[0m  [8/42], [94mLoss[0m : 2.59233
[1mStep[0m  [12/42], [94mLoss[0m : 2.35588
[1mStep[0m  [16/42], [94mLoss[0m : 2.43277
[1mStep[0m  [20/42], [94mLoss[0m : 2.15269
[1mStep[0m  [24/42], [94mLoss[0m : 2.30102
[1mStep[0m  [28/42], [94mLoss[0m : 2.47105
[1mStep[0m  [32/42], [94mLoss[0m : 2.73398
[1mStep[0m  [36/42], [94mLoss[0m : 2.64503
[1mStep[0m  [40/42], [94mLoss[0m : 2.23104

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.305, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32469
[1mStep[0m  [4/42], [94mLoss[0m : 2.46197
[1mStep[0m  [8/42], [94mLoss[0m : 2.28752
[1mStep[0m  [12/42], [94mLoss[0m : 2.43434
[1mStep[0m  [16/42], [94mLoss[0m : 2.34131
[1mStep[0m  [20/42], [94mLoss[0m : 2.31836
[1mStep[0m  [24/42], [94mLoss[0m : 2.32083
[1mStep[0m  [28/42], [94mLoss[0m : 2.42352
[1mStep[0m  [32/42], [94mLoss[0m : 2.47197
[1mStep[0m  [36/42], [94mLoss[0m : 2.30959
[1mStep[0m  [40/42], [94mLoss[0m : 2.23183

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.358, [92mTest[0m: 2.378, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.14530
[1mStep[0m  [4/42], [94mLoss[0m : 2.34582
[1mStep[0m  [8/42], [94mLoss[0m : 2.16741
[1mStep[0m  [12/42], [94mLoss[0m : 2.45035
[1mStep[0m  [16/42], [94mLoss[0m : 2.33353
[1mStep[0m  [20/42], [94mLoss[0m : 2.30413
[1mStep[0m  [24/42], [94mLoss[0m : 2.18627
[1mStep[0m  [28/42], [94mLoss[0m : 2.18151
[1mStep[0m  [32/42], [94mLoss[0m : 2.15200
[1mStep[0m  [36/42], [94mLoss[0m : 2.15259
[1mStep[0m  [40/42], [94mLoss[0m : 2.30452

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.274, [92mTest[0m: 2.723, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.03711
[1mStep[0m  [4/42], [94mLoss[0m : 2.43255
[1mStep[0m  [8/42], [94mLoss[0m : 2.24454
[1mStep[0m  [12/42], [94mLoss[0m : 2.10623
[1mStep[0m  [16/42], [94mLoss[0m : 2.42084
[1mStep[0m  [20/42], [94mLoss[0m : 2.26269
[1mStep[0m  [24/42], [94mLoss[0m : 2.12815
[1mStep[0m  [28/42], [94mLoss[0m : 2.29408
[1mStep[0m  [32/42], [94mLoss[0m : 2.18798
[1mStep[0m  [36/42], [94mLoss[0m : 2.38191
[1mStep[0m  [40/42], [94mLoss[0m : 2.05089

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.191, [92mTest[0m: 2.473, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.14818
[1mStep[0m  [4/42], [94mLoss[0m : 1.91737
[1mStep[0m  [8/42], [94mLoss[0m : 2.13953
[1mStep[0m  [12/42], [94mLoss[0m : 2.05494
[1mStep[0m  [16/42], [94mLoss[0m : 2.23918
[1mStep[0m  [20/42], [94mLoss[0m : 2.17423
[1mStep[0m  [24/42], [94mLoss[0m : 2.19300
[1mStep[0m  [28/42], [94mLoss[0m : 2.26151
[1mStep[0m  [32/42], [94mLoss[0m : 1.91666
[1mStep[0m  [36/42], [94mLoss[0m : 2.32678
[1mStep[0m  [40/42], [94mLoss[0m : 2.05332

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.123, [92mTest[0m: 2.817, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.87253
[1mStep[0m  [4/42], [94mLoss[0m : 2.00037
[1mStep[0m  [8/42], [94mLoss[0m : 1.78299
[1mStep[0m  [12/42], [94mLoss[0m : 1.92671
[1mStep[0m  [16/42], [94mLoss[0m : 2.13048
[1mStep[0m  [20/42], [94mLoss[0m : 2.06470
[1mStep[0m  [24/42], [94mLoss[0m : 1.99349
[1mStep[0m  [28/42], [94mLoss[0m : 2.22894
[1mStep[0m  [32/42], [94mLoss[0m : 1.99102
[1mStep[0m  [36/42], [94mLoss[0m : 2.04202
[1mStep[0m  [40/42], [94mLoss[0m : 2.08604

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.052, [92mTest[0m: 2.971, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.07447
[1mStep[0m  [4/42], [94mLoss[0m : 2.01043
[1mStep[0m  [8/42], [94mLoss[0m : 1.84052
[1mStep[0m  [12/42], [94mLoss[0m : 1.91917
[1mStep[0m  [16/42], [94mLoss[0m : 2.04390
[1mStep[0m  [20/42], [94mLoss[0m : 1.99918
[1mStep[0m  [24/42], [94mLoss[0m : 2.03138
[1mStep[0m  [28/42], [94mLoss[0m : 1.96622
[1mStep[0m  [32/42], [94mLoss[0m : 2.10297
[1mStep[0m  [36/42], [94mLoss[0m : 2.23904
[1mStep[0m  [40/42], [94mLoss[0m : 2.04084

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.992, [92mTest[0m: 2.439, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.57244
[1mStep[0m  [4/42], [94mLoss[0m : 1.83320
[1mStep[0m  [8/42], [94mLoss[0m : 2.03216
[1mStep[0m  [12/42], [94mLoss[0m : 1.89522
[1mStep[0m  [16/42], [94mLoss[0m : 2.07949
[1mStep[0m  [20/42], [94mLoss[0m : 1.75551
[1mStep[0m  [24/42], [94mLoss[0m : 1.88673
[1mStep[0m  [28/42], [94mLoss[0m : 2.05892
[1mStep[0m  [32/42], [94mLoss[0m : 2.04700
[1mStep[0m  [36/42], [94mLoss[0m : 1.98972
[1mStep[0m  [40/42], [94mLoss[0m : 1.97851

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.921, [92mTest[0m: 2.431, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.89010
[1mStep[0m  [4/42], [94mLoss[0m : 1.98977
[1mStep[0m  [8/42], [94mLoss[0m : 1.94397
[1mStep[0m  [12/42], [94mLoss[0m : 1.84710
[1mStep[0m  [16/42], [94mLoss[0m : 2.00638
[1mStep[0m  [20/42], [94mLoss[0m : 1.79949
[1mStep[0m  [24/42], [94mLoss[0m : 1.96226
[1mStep[0m  [28/42], [94mLoss[0m : 1.71344
[1mStep[0m  [32/42], [94mLoss[0m : 1.82905
[1mStep[0m  [36/42], [94mLoss[0m : 1.80955
[1mStep[0m  [40/42], [94mLoss[0m : 1.93815

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.875, [92mTest[0m: 2.420, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.91762
[1mStep[0m  [4/42], [94mLoss[0m : 1.76768
[1mStep[0m  [8/42], [94mLoss[0m : 1.93120
[1mStep[0m  [12/42], [94mLoss[0m : 1.82246
[1mStep[0m  [16/42], [94mLoss[0m : 1.65254
[1mStep[0m  [20/42], [94mLoss[0m : 1.78585
[1mStep[0m  [24/42], [94mLoss[0m : 1.73036
[1mStep[0m  [28/42], [94mLoss[0m : 1.85417
[1mStep[0m  [32/42], [94mLoss[0m : 1.80662
[1mStep[0m  [36/42], [94mLoss[0m : 1.83629
[1mStep[0m  [40/42], [94mLoss[0m : 1.65778

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.812, [92mTest[0m: 2.566, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.79482
[1mStep[0m  [4/42], [94mLoss[0m : 1.71312
[1mStep[0m  [8/42], [94mLoss[0m : 1.94701
[1mStep[0m  [12/42], [94mLoss[0m : 1.69498
[1mStep[0m  [16/42], [94mLoss[0m : 1.59611
[1mStep[0m  [20/42], [94mLoss[0m : 1.63487
[1mStep[0m  [24/42], [94mLoss[0m : 1.75005
[1mStep[0m  [28/42], [94mLoss[0m : 1.79467
[1mStep[0m  [32/42], [94mLoss[0m : 1.85543
[1mStep[0m  [36/42], [94mLoss[0m : 1.84143
[1mStep[0m  [40/42], [94mLoss[0m : 2.06842

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.764, [92mTest[0m: 2.442, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.66328
[1mStep[0m  [4/42], [94mLoss[0m : 1.66952
[1mStep[0m  [8/42], [94mLoss[0m : 1.81685
[1mStep[0m  [12/42], [94mLoss[0m : 1.66885
[1mStep[0m  [16/42], [94mLoss[0m : 1.80120
[1mStep[0m  [20/42], [94mLoss[0m : 1.55761
[1mStep[0m  [24/42], [94mLoss[0m : 1.85743
[1mStep[0m  [28/42], [94mLoss[0m : 1.71092
[1mStep[0m  [32/42], [94mLoss[0m : 1.50600
[1mStep[0m  [36/42], [94mLoss[0m : 1.52061
[1mStep[0m  [40/42], [94mLoss[0m : 1.92072

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.740, [92mTest[0m: 2.513, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.59938
[1mStep[0m  [4/42], [94mLoss[0m : 1.78029
[1mStep[0m  [8/42], [94mLoss[0m : 1.68228
[1mStep[0m  [12/42], [94mLoss[0m : 1.79528
[1mStep[0m  [16/42], [94mLoss[0m : 1.64408
[1mStep[0m  [20/42], [94mLoss[0m : 1.58150
[1mStep[0m  [24/42], [94mLoss[0m : 1.56960
[1mStep[0m  [28/42], [94mLoss[0m : 1.65141
[1mStep[0m  [32/42], [94mLoss[0m : 1.67126
[1mStep[0m  [36/42], [94mLoss[0m : 1.80903
[1mStep[0m  [40/42], [94mLoss[0m : 1.66912

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.679, [92mTest[0m: 2.480, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.73244
[1mStep[0m  [4/42], [94mLoss[0m : 1.72361
[1mStep[0m  [8/42], [94mLoss[0m : 1.68411
[1mStep[0m  [12/42], [94mLoss[0m : 1.69563
[1mStep[0m  [16/42], [94mLoss[0m : 1.58873
[1mStep[0m  [20/42], [94mLoss[0m : 1.63200
[1mStep[0m  [24/42], [94mLoss[0m : 1.47827
[1mStep[0m  [28/42], [94mLoss[0m : 1.64827
[1mStep[0m  [32/42], [94mLoss[0m : 1.59430
[1mStep[0m  [36/42], [94mLoss[0m : 1.58200
[1mStep[0m  [40/42], [94mLoss[0m : 1.74165

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.644, [92mTest[0m: 2.434, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.51495
[1mStep[0m  [4/42], [94mLoss[0m : 1.60201
[1mStep[0m  [8/42], [94mLoss[0m : 1.59837
[1mStep[0m  [12/42], [94mLoss[0m : 1.60939
[1mStep[0m  [16/42], [94mLoss[0m : 1.58368
[1mStep[0m  [20/42], [94mLoss[0m : 1.69310
[1mStep[0m  [24/42], [94mLoss[0m : 1.67491
[1mStep[0m  [28/42], [94mLoss[0m : 1.55751
[1mStep[0m  [32/42], [94mLoss[0m : 1.59543
[1mStep[0m  [36/42], [94mLoss[0m : 1.69712
[1mStep[0m  [40/42], [94mLoss[0m : 1.73075

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.623, [92mTest[0m: 2.452, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.51442
[1mStep[0m  [4/42], [94mLoss[0m : 1.59403
[1mStep[0m  [8/42], [94mLoss[0m : 1.60413
[1mStep[0m  [12/42], [94mLoss[0m : 1.61977
[1mStep[0m  [16/42], [94mLoss[0m : 1.37616
[1mStep[0m  [20/42], [94mLoss[0m : 1.57613
[1mStep[0m  [24/42], [94mLoss[0m : 1.67368
[1mStep[0m  [28/42], [94mLoss[0m : 1.57319
[1mStep[0m  [32/42], [94mLoss[0m : 1.46991
[1mStep[0m  [36/42], [94mLoss[0m : 1.43535
[1mStep[0m  [40/42], [94mLoss[0m : 1.45417

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.587, [92mTest[0m: 2.495, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.43083
[1mStep[0m  [4/42], [94mLoss[0m : 1.48993
[1mStep[0m  [8/42], [94mLoss[0m : 1.55315
[1mStep[0m  [12/42], [94mLoss[0m : 1.53103
[1mStep[0m  [16/42], [94mLoss[0m : 1.65944
[1mStep[0m  [20/42], [94mLoss[0m : 1.45280
[1mStep[0m  [24/42], [94mLoss[0m : 1.39074
[1mStep[0m  [28/42], [94mLoss[0m : 1.65077
[1mStep[0m  [32/42], [94mLoss[0m : 1.63733
[1mStep[0m  [36/42], [94mLoss[0m : 1.57444
[1mStep[0m  [40/42], [94mLoss[0m : 1.59067

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.551, [92mTest[0m: 2.505, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.47830
[1mStep[0m  [4/42], [94mLoss[0m : 1.52736
[1mStep[0m  [8/42], [94mLoss[0m : 1.51066
[1mStep[0m  [12/42], [94mLoss[0m : 1.57633
[1mStep[0m  [16/42], [94mLoss[0m : 1.49515
[1mStep[0m  [20/42], [94mLoss[0m : 1.52054
[1mStep[0m  [24/42], [94mLoss[0m : 1.38668
[1mStep[0m  [28/42], [94mLoss[0m : 1.44919
[1mStep[0m  [32/42], [94mLoss[0m : 1.38121
[1mStep[0m  [36/42], [94mLoss[0m : 1.48357
[1mStep[0m  [40/42], [94mLoss[0m : 1.48664

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.506, [92mTest[0m: 2.520, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.43496
[1mStep[0m  [4/42], [94mLoss[0m : 1.47164
[1mStep[0m  [8/42], [94mLoss[0m : 1.47839
[1mStep[0m  [12/42], [94mLoss[0m : 1.41213
[1mStep[0m  [16/42], [94mLoss[0m : 1.43390
[1mStep[0m  [20/42], [94mLoss[0m : 1.45717
[1mStep[0m  [24/42], [94mLoss[0m : 1.41597
[1mStep[0m  [28/42], [94mLoss[0m : 1.45460
[1mStep[0m  [32/42], [94mLoss[0m : 1.54997
[1mStep[0m  [36/42], [94mLoss[0m : 1.38223
[1mStep[0m  [40/42], [94mLoss[0m : 1.46648

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.485, [92mTest[0m: 2.518, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.37041
[1mStep[0m  [4/42], [94mLoss[0m : 1.57255
[1mStep[0m  [8/42], [94mLoss[0m : 1.45302
[1mStep[0m  [12/42], [94mLoss[0m : 1.31346
[1mStep[0m  [16/42], [94mLoss[0m : 1.53733
[1mStep[0m  [20/42], [94mLoss[0m : 1.50949
[1mStep[0m  [24/42], [94mLoss[0m : 1.52528
[1mStep[0m  [28/42], [94mLoss[0m : 1.40243
[1mStep[0m  [32/42], [94mLoss[0m : 1.54464
[1mStep[0m  [36/42], [94mLoss[0m : 1.40563
[1mStep[0m  [40/42], [94mLoss[0m : 1.57410

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.464, [92mTest[0m: 2.517, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.44152
[1mStep[0m  [4/42], [94mLoss[0m : 1.42651
[1mStep[0m  [8/42], [94mLoss[0m : 1.39092
[1mStep[0m  [12/42], [94mLoss[0m : 1.47256
[1mStep[0m  [16/42], [94mLoss[0m : 1.51093
[1mStep[0m  [20/42], [94mLoss[0m : 1.40560
[1mStep[0m  [24/42], [94mLoss[0m : 1.28160
[1mStep[0m  [28/42], [94mLoss[0m : 1.39093
[1mStep[0m  [32/42], [94mLoss[0m : 1.42790
[1mStep[0m  [36/42], [94mLoss[0m : 1.35452
[1mStep[0m  [40/42], [94mLoss[0m : 1.47969

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.416, [92mTest[0m: 2.569, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.41253
[1mStep[0m  [4/42], [94mLoss[0m : 1.31235
[1mStep[0m  [8/42], [94mLoss[0m : 1.41532
[1mStep[0m  [12/42], [94mLoss[0m : 1.47781
[1mStep[0m  [16/42], [94mLoss[0m : 1.35809
[1mStep[0m  [20/42], [94mLoss[0m : 1.44661
[1mStep[0m  [24/42], [94mLoss[0m : 1.39337
[1mStep[0m  [28/42], [94mLoss[0m : 1.50287
[1mStep[0m  [32/42], [94mLoss[0m : 1.31652
[1mStep[0m  [36/42], [94mLoss[0m : 1.38275
[1mStep[0m  [40/42], [94mLoss[0m : 1.45104

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.411, [92mTest[0m: 2.504, [96mlr[0m: 0.0023175
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 21 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.479
====================================

Phase 2 - Evaluation MAE:  2.479413730757577
MAE score P1      2.302229
MAE score P2      2.479414
loss              1.411483
learning_rate     0.002575
batch_size             256
hidden_sizes         [100]
epochs                  30
activation            relu
optimizer              sgd
early stopping        True
dropout                0.2
momentum               0.9
weight_decay         0.001
Name: 26, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 10.98425
[1mStep[0m  [8/84], [94mLoss[0m : 10.25844
[1mStep[0m  [16/84], [94mLoss[0m : 10.17101
[1mStep[0m  [24/84], [94mLoss[0m : 9.05837
[1mStep[0m  [32/84], [94mLoss[0m : 8.53090
[1mStep[0m  [40/84], [94mLoss[0m : 7.92159
[1mStep[0m  [48/84], [94mLoss[0m : 6.96160
[1mStep[0m  [56/84], [94mLoss[0m : 6.17799
[1mStep[0m  [64/84], [94mLoss[0m : 6.62972
[1mStep[0m  [72/84], [94mLoss[0m : 5.71058
[1mStep[0m  [80/84], [94mLoss[0m : 5.13631

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 7.854, [92mTest[0m: 10.641, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.71140
[1mStep[0m  [8/84], [94mLoss[0m : 4.02672
[1mStep[0m  [16/84], [94mLoss[0m : 4.35022
[1mStep[0m  [24/84], [94mLoss[0m : 3.64913
[1mStep[0m  [32/84], [94mLoss[0m : 3.32878
[1mStep[0m  [40/84], [94mLoss[0m : 3.67017
[1mStep[0m  [48/84], [94mLoss[0m : 3.56636
[1mStep[0m  [56/84], [94mLoss[0m : 3.54145
[1mStep[0m  [64/84], [94mLoss[0m : 3.34192
[1mStep[0m  [72/84], [94mLoss[0m : 3.62824
[1mStep[0m  [80/84], [94mLoss[0m : 3.12488

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.700, [92mTest[0m: 4.991, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.73304
[1mStep[0m  [8/84], [94mLoss[0m : 3.54151
[1mStep[0m  [16/84], [94mLoss[0m : 3.29999
[1mStep[0m  [24/84], [94mLoss[0m : 2.95829
[1mStep[0m  [32/84], [94mLoss[0m : 2.96096
[1mStep[0m  [40/84], [94mLoss[0m : 2.84727
[1mStep[0m  [48/84], [94mLoss[0m : 2.57016
[1mStep[0m  [56/84], [94mLoss[0m : 2.89391
[1mStep[0m  [64/84], [94mLoss[0m : 2.64163
[1mStep[0m  [72/84], [94mLoss[0m : 2.68337
[1mStep[0m  [80/84], [94mLoss[0m : 2.80778

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.851, [92mTest[0m: 2.769, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.76786
[1mStep[0m  [8/84], [94mLoss[0m : 3.09391
[1mStep[0m  [16/84], [94mLoss[0m : 2.67911
[1mStep[0m  [24/84], [94mLoss[0m : 2.49490
[1mStep[0m  [32/84], [94mLoss[0m : 2.87928
[1mStep[0m  [40/84], [94mLoss[0m : 2.52871
[1mStep[0m  [48/84], [94mLoss[0m : 2.81951
[1mStep[0m  [56/84], [94mLoss[0m : 2.80087
[1mStep[0m  [64/84], [94mLoss[0m : 2.61526
[1mStep[0m  [72/84], [94mLoss[0m : 2.89421
[1mStep[0m  [80/84], [94mLoss[0m : 2.64057

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.732, [92mTest[0m: 2.463, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.79792
[1mStep[0m  [8/84], [94mLoss[0m : 2.33438
[1mStep[0m  [16/84], [94mLoss[0m : 2.72064
[1mStep[0m  [24/84], [94mLoss[0m : 2.45363
[1mStep[0m  [32/84], [94mLoss[0m : 2.50301
[1mStep[0m  [40/84], [94mLoss[0m : 2.91555
[1mStep[0m  [48/84], [94mLoss[0m : 2.84871
[1mStep[0m  [56/84], [94mLoss[0m : 2.57511
[1mStep[0m  [64/84], [94mLoss[0m : 2.72636
[1mStep[0m  [72/84], [94mLoss[0m : 2.82529
[1mStep[0m  [80/84], [94mLoss[0m : 2.43234

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.697, [92mTest[0m: 2.427, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.51147
[1mStep[0m  [8/84], [94mLoss[0m : 2.68868
[1mStep[0m  [16/84], [94mLoss[0m : 2.39122
[1mStep[0m  [24/84], [94mLoss[0m : 2.82955
[1mStep[0m  [32/84], [94mLoss[0m : 2.68924
[1mStep[0m  [40/84], [94mLoss[0m : 3.03340
[1mStep[0m  [48/84], [94mLoss[0m : 2.64764
[1mStep[0m  [56/84], [94mLoss[0m : 2.95528
[1mStep[0m  [64/84], [94mLoss[0m : 2.62200
[1mStep[0m  [72/84], [94mLoss[0m : 2.52559
[1mStep[0m  [80/84], [94mLoss[0m : 2.47156

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.674, [92mTest[0m: 2.407, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.58605
[1mStep[0m  [8/84], [94mLoss[0m : 2.86839
[1mStep[0m  [16/84], [94mLoss[0m : 2.66062
[1mStep[0m  [24/84], [94mLoss[0m : 3.02036
[1mStep[0m  [32/84], [94mLoss[0m : 2.54562
[1mStep[0m  [40/84], [94mLoss[0m : 2.72706
[1mStep[0m  [48/84], [94mLoss[0m : 2.38611
[1mStep[0m  [56/84], [94mLoss[0m : 2.43559
[1mStep[0m  [64/84], [94mLoss[0m : 2.50896
[1mStep[0m  [72/84], [94mLoss[0m : 2.76839
[1mStep[0m  [80/84], [94mLoss[0m : 2.55810

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.687, [92mTest[0m: 2.398, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.68626
[1mStep[0m  [8/84], [94mLoss[0m : 2.74101
[1mStep[0m  [16/84], [94mLoss[0m : 3.08487
[1mStep[0m  [24/84], [94mLoss[0m : 2.69991
[1mStep[0m  [32/84], [94mLoss[0m : 2.76653
[1mStep[0m  [40/84], [94mLoss[0m : 2.93415
[1mStep[0m  [48/84], [94mLoss[0m : 2.52995
[1mStep[0m  [56/84], [94mLoss[0m : 2.74969
[1mStep[0m  [64/84], [94mLoss[0m : 2.79734
[1mStep[0m  [72/84], [94mLoss[0m : 2.55607
[1mStep[0m  [80/84], [94mLoss[0m : 2.88949

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.662, [92mTest[0m: 2.385, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60860
[1mStep[0m  [8/84], [94mLoss[0m : 2.53230
[1mStep[0m  [16/84], [94mLoss[0m : 2.64272
[1mStep[0m  [24/84], [94mLoss[0m : 2.72869
[1mStep[0m  [32/84], [94mLoss[0m : 3.05368
[1mStep[0m  [40/84], [94mLoss[0m : 2.41968
[1mStep[0m  [48/84], [94mLoss[0m : 2.62045
[1mStep[0m  [56/84], [94mLoss[0m : 2.96557
[1mStep[0m  [64/84], [94mLoss[0m : 2.75487
[1mStep[0m  [72/84], [94mLoss[0m : 2.61741
[1mStep[0m  [80/84], [94mLoss[0m : 2.74983

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.698, [92mTest[0m: 2.384, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.83740
[1mStep[0m  [8/84], [94mLoss[0m : 2.79100
[1mStep[0m  [16/84], [94mLoss[0m : 2.45426
[1mStep[0m  [24/84], [94mLoss[0m : 2.61967
[1mStep[0m  [32/84], [94mLoss[0m : 2.61522
[1mStep[0m  [40/84], [94mLoss[0m : 2.41729
[1mStep[0m  [48/84], [94mLoss[0m : 2.55477
[1mStep[0m  [56/84], [94mLoss[0m : 2.60250
[1mStep[0m  [64/84], [94mLoss[0m : 2.82001
[1mStep[0m  [72/84], [94mLoss[0m : 2.50408
[1mStep[0m  [80/84], [94mLoss[0m : 2.81163

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.665, [92mTest[0m: 2.377, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31600
[1mStep[0m  [8/84], [94mLoss[0m : 2.35501
[1mStep[0m  [16/84], [94mLoss[0m : 2.53575
[1mStep[0m  [24/84], [94mLoss[0m : 2.32992
[1mStep[0m  [32/84], [94mLoss[0m : 2.59698
[1mStep[0m  [40/84], [94mLoss[0m : 2.95625
[1mStep[0m  [48/84], [94mLoss[0m : 2.38220
[1mStep[0m  [56/84], [94mLoss[0m : 2.77505
[1mStep[0m  [64/84], [94mLoss[0m : 2.88165
[1mStep[0m  [72/84], [94mLoss[0m : 2.62633
[1mStep[0m  [80/84], [94mLoss[0m : 2.70872

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.659, [92mTest[0m: 2.378, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.88255
[1mStep[0m  [8/84], [94mLoss[0m : 2.51560
[1mStep[0m  [16/84], [94mLoss[0m : 2.38722
[1mStep[0m  [24/84], [94mLoss[0m : 2.53816
[1mStep[0m  [32/84], [94mLoss[0m : 2.40863
[1mStep[0m  [40/84], [94mLoss[0m : 2.73552
[1mStep[0m  [48/84], [94mLoss[0m : 2.58317
[1mStep[0m  [56/84], [94mLoss[0m : 2.38551
[1mStep[0m  [64/84], [94mLoss[0m : 2.73552
[1mStep[0m  [72/84], [94mLoss[0m : 2.75763
[1mStep[0m  [80/84], [94mLoss[0m : 2.60569

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.638, [92mTest[0m: 2.364, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67228
[1mStep[0m  [8/84], [94mLoss[0m : 2.93641
[1mStep[0m  [16/84], [94mLoss[0m : 2.75285
[1mStep[0m  [24/84], [94mLoss[0m : 2.76136
[1mStep[0m  [32/84], [94mLoss[0m : 2.69025
[1mStep[0m  [40/84], [94mLoss[0m : 2.60223
[1mStep[0m  [48/84], [94mLoss[0m : 2.85602
[1mStep[0m  [56/84], [94mLoss[0m : 2.63958
[1mStep[0m  [64/84], [94mLoss[0m : 2.75592
[1mStep[0m  [72/84], [94mLoss[0m : 2.95163
[1mStep[0m  [80/84], [94mLoss[0m : 2.57449

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.655, [92mTest[0m: 2.368, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67635
[1mStep[0m  [8/84], [94mLoss[0m : 2.64797
[1mStep[0m  [16/84], [94mLoss[0m : 2.78109
[1mStep[0m  [24/84], [94mLoss[0m : 2.45488
[1mStep[0m  [32/84], [94mLoss[0m : 2.59042
[1mStep[0m  [40/84], [94mLoss[0m : 2.54705
[1mStep[0m  [48/84], [94mLoss[0m : 2.61438
[1mStep[0m  [56/84], [94mLoss[0m : 2.49646
[1mStep[0m  [64/84], [94mLoss[0m : 2.84746
[1mStep[0m  [72/84], [94mLoss[0m : 2.50363
[1mStep[0m  [80/84], [94mLoss[0m : 2.58186

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.624, [92mTest[0m: 2.361, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.73720
[1mStep[0m  [8/84], [94mLoss[0m : 2.62804
[1mStep[0m  [16/84], [94mLoss[0m : 2.49072
[1mStep[0m  [24/84], [94mLoss[0m : 2.87860
[1mStep[0m  [32/84], [94mLoss[0m : 2.57143
[1mStep[0m  [40/84], [94mLoss[0m : 2.40344
[1mStep[0m  [48/84], [94mLoss[0m : 2.65364
[1mStep[0m  [56/84], [94mLoss[0m : 2.87263
[1mStep[0m  [64/84], [94mLoss[0m : 2.67813
[1mStep[0m  [72/84], [94mLoss[0m : 2.47186
[1mStep[0m  [80/84], [94mLoss[0m : 2.64773

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.631, [92mTest[0m: 2.356, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.76823
[1mStep[0m  [8/84], [94mLoss[0m : 2.35332
[1mStep[0m  [16/84], [94mLoss[0m : 2.41869
[1mStep[0m  [24/84], [94mLoss[0m : 2.81576
[1mStep[0m  [32/84], [94mLoss[0m : 2.63954
[1mStep[0m  [40/84], [94mLoss[0m : 2.64572
[1mStep[0m  [48/84], [94mLoss[0m : 2.46332
[1mStep[0m  [56/84], [94mLoss[0m : 2.63946
[1mStep[0m  [64/84], [94mLoss[0m : 3.03192
[1mStep[0m  [72/84], [94mLoss[0m : 2.62418
[1mStep[0m  [80/84], [94mLoss[0m : 2.47933

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.623, [92mTest[0m: 2.361, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.64688
[1mStep[0m  [8/84], [94mLoss[0m : 2.22078
[1mStep[0m  [16/84], [94mLoss[0m : 2.65024
[1mStep[0m  [24/84], [94mLoss[0m : 2.88953
[1mStep[0m  [32/84], [94mLoss[0m : 2.26253
[1mStep[0m  [40/84], [94mLoss[0m : 2.73692
[1mStep[0m  [48/84], [94mLoss[0m : 2.59297
[1mStep[0m  [56/84], [94mLoss[0m : 2.89835
[1mStep[0m  [64/84], [94mLoss[0m : 2.60548
[1mStep[0m  [72/84], [94mLoss[0m : 2.53221
[1mStep[0m  [80/84], [94mLoss[0m : 2.89324

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.623, [92mTest[0m: 2.352, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53795
[1mStep[0m  [8/84], [94mLoss[0m : 2.64071
[1mStep[0m  [16/84], [94mLoss[0m : 2.47600
[1mStep[0m  [24/84], [94mLoss[0m : 2.73338
[1mStep[0m  [32/84], [94mLoss[0m : 2.62372
[1mStep[0m  [40/84], [94mLoss[0m : 2.85453
[1mStep[0m  [48/84], [94mLoss[0m : 2.19629
[1mStep[0m  [56/84], [94mLoss[0m : 2.95447
[1mStep[0m  [64/84], [94mLoss[0m : 2.74198
[1mStep[0m  [72/84], [94mLoss[0m : 2.71532
[1mStep[0m  [80/84], [94mLoss[0m : 2.70523

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.626, [92mTest[0m: 2.349, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.73275
[1mStep[0m  [8/84], [94mLoss[0m : 2.35025
[1mStep[0m  [16/84], [94mLoss[0m : 2.40210
[1mStep[0m  [24/84], [94mLoss[0m : 2.79353
[1mStep[0m  [32/84], [94mLoss[0m : 2.65365
[1mStep[0m  [40/84], [94mLoss[0m : 2.23877
[1mStep[0m  [48/84], [94mLoss[0m : 2.63566
[1mStep[0m  [56/84], [94mLoss[0m : 2.52497
[1mStep[0m  [64/84], [94mLoss[0m : 2.65668
[1mStep[0m  [72/84], [94mLoss[0m : 2.89153
[1mStep[0m  [80/84], [94mLoss[0m : 2.44091

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.610, [92mTest[0m: 2.352, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46046
[1mStep[0m  [8/84], [94mLoss[0m : 2.61487
[1mStep[0m  [16/84], [94mLoss[0m : 2.34529
[1mStep[0m  [24/84], [94mLoss[0m : 2.71326
[1mStep[0m  [32/84], [94mLoss[0m : 2.53340
[1mStep[0m  [40/84], [94mLoss[0m : 2.54073
[1mStep[0m  [48/84], [94mLoss[0m : 2.52716
[1mStep[0m  [56/84], [94mLoss[0m : 2.61506
[1mStep[0m  [64/84], [94mLoss[0m : 2.58620
[1mStep[0m  [72/84], [94mLoss[0m : 2.37004
[1mStep[0m  [80/84], [94mLoss[0m : 2.28811

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.615, [92mTest[0m: 2.347, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40017
[1mStep[0m  [8/84], [94mLoss[0m : 3.14184
[1mStep[0m  [16/84], [94mLoss[0m : 2.59985
[1mStep[0m  [24/84], [94mLoss[0m : 2.54242
[1mStep[0m  [32/84], [94mLoss[0m : 2.50735
[1mStep[0m  [40/84], [94mLoss[0m : 2.41728
[1mStep[0m  [48/84], [94mLoss[0m : 2.55529
[1mStep[0m  [56/84], [94mLoss[0m : 2.35580
[1mStep[0m  [64/84], [94mLoss[0m : 2.66227
[1mStep[0m  [72/84], [94mLoss[0m : 2.68410
[1mStep[0m  [80/84], [94mLoss[0m : 2.69600

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.592, [92mTest[0m: 2.342, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48166
[1mStep[0m  [8/84], [94mLoss[0m : 2.34894
[1mStep[0m  [16/84], [94mLoss[0m : 2.94699
[1mStep[0m  [24/84], [94mLoss[0m : 2.64365
[1mStep[0m  [32/84], [94mLoss[0m : 2.29456
[1mStep[0m  [40/84], [94mLoss[0m : 3.15108
[1mStep[0m  [48/84], [94mLoss[0m : 2.69301
[1mStep[0m  [56/84], [94mLoss[0m : 2.80055
[1mStep[0m  [64/84], [94mLoss[0m : 2.70928
[1mStep[0m  [72/84], [94mLoss[0m : 2.75645
[1mStep[0m  [80/84], [94mLoss[0m : 2.53517

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.618, [92mTest[0m: 2.350, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.77633
[1mStep[0m  [8/84], [94mLoss[0m : 2.70557
[1mStep[0m  [16/84], [94mLoss[0m : 2.67249
[1mStep[0m  [24/84], [94mLoss[0m : 2.75921
[1mStep[0m  [32/84], [94mLoss[0m : 2.63778
[1mStep[0m  [40/84], [94mLoss[0m : 2.72450
[1mStep[0m  [48/84], [94mLoss[0m : 2.55612
[1mStep[0m  [56/84], [94mLoss[0m : 2.40150
[1mStep[0m  [64/84], [94mLoss[0m : 2.37386
[1mStep[0m  [72/84], [94mLoss[0m : 2.75767
[1mStep[0m  [80/84], [94mLoss[0m : 2.54245

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.618, [92mTest[0m: 2.349, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55126
[1mStep[0m  [8/84], [94mLoss[0m : 2.68291
[1mStep[0m  [16/84], [94mLoss[0m : 2.65540
[1mStep[0m  [24/84], [94mLoss[0m : 2.42182
[1mStep[0m  [32/84], [94mLoss[0m : 2.63754
[1mStep[0m  [40/84], [94mLoss[0m : 2.69376
[1mStep[0m  [48/84], [94mLoss[0m : 2.44721
[1mStep[0m  [56/84], [94mLoss[0m : 2.68529
[1mStep[0m  [64/84], [94mLoss[0m : 2.52564
[1mStep[0m  [72/84], [94mLoss[0m : 2.84236
[1mStep[0m  [80/84], [94mLoss[0m : 2.44436

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.638, [92mTest[0m: 2.347, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.62653
[1mStep[0m  [8/84], [94mLoss[0m : 2.71127
[1mStep[0m  [16/84], [94mLoss[0m : 2.98585
[1mStep[0m  [24/84], [94mLoss[0m : 2.61986
[1mStep[0m  [32/84], [94mLoss[0m : 2.45216
[1mStep[0m  [40/84], [94mLoss[0m : 2.43297
[1mStep[0m  [48/84], [94mLoss[0m : 2.99741
[1mStep[0m  [56/84], [94mLoss[0m : 2.92301
[1mStep[0m  [64/84], [94mLoss[0m : 2.68115
[1mStep[0m  [72/84], [94mLoss[0m : 2.69998
[1mStep[0m  [80/84], [94mLoss[0m : 2.60731

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.622, [92mTest[0m: 2.347, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67545
[1mStep[0m  [8/84], [94mLoss[0m : 2.69133
[1mStep[0m  [16/84], [94mLoss[0m : 2.69560
[1mStep[0m  [24/84], [94mLoss[0m : 2.59554
[1mStep[0m  [32/84], [94mLoss[0m : 2.98796
[1mStep[0m  [40/84], [94mLoss[0m : 2.50288
[1mStep[0m  [48/84], [94mLoss[0m : 2.68575
[1mStep[0m  [56/84], [94mLoss[0m : 2.71424
[1mStep[0m  [64/84], [94mLoss[0m : 2.61695
[1mStep[0m  [72/84], [94mLoss[0m : 2.72935
[1mStep[0m  [80/84], [94mLoss[0m : 2.93518

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.622, [92mTest[0m: 2.350, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42990
[1mStep[0m  [8/84], [94mLoss[0m : 2.73216
[1mStep[0m  [16/84], [94mLoss[0m : 2.82322
[1mStep[0m  [24/84], [94mLoss[0m : 2.73319
[1mStep[0m  [32/84], [94mLoss[0m : 2.56863
[1mStep[0m  [40/84], [94mLoss[0m : 2.85035
[1mStep[0m  [48/84], [94mLoss[0m : 2.62799
[1mStep[0m  [56/84], [94mLoss[0m : 2.53425
[1mStep[0m  [64/84], [94mLoss[0m : 2.78780
[1mStep[0m  [72/84], [94mLoss[0m : 2.71797
[1mStep[0m  [80/84], [94mLoss[0m : 2.95522

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.624, [92mTest[0m: 2.345, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45161
[1mStep[0m  [8/84], [94mLoss[0m : 2.80819
[1mStep[0m  [16/84], [94mLoss[0m : 2.79329
[1mStep[0m  [24/84], [94mLoss[0m : 2.57990
[1mStep[0m  [32/84], [94mLoss[0m : 2.33979
[1mStep[0m  [40/84], [94mLoss[0m : 2.73021
[1mStep[0m  [48/84], [94mLoss[0m : 2.08947
[1mStep[0m  [56/84], [94mLoss[0m : 2.74892
[1mStep[0m  [64/84], [94mLoss[0m : 2.49133
[1mStep[0m  [72/84], [94mLoss[0m : 2.62324
[1mStep[0m  [80/84], [94mLoss[0m : 2.69706

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.613, [92mTest[0m: 2.342, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45803
[1mStep[0m  [8/84], [94mLoss[0m : 2.77723
[1mStep[0m  [16/84], [94mLoss[0m : 2.60452
[1mStep[0m  [24/84], [94mLoss[0m : 2.86911
[1mStep[0m  [32/84], [94mLoss[0m : 2.71978
[1mStep[0m  [40/84], [94mLoss[0m : 2.60622
[1mStep[0m  [48/84], [94mLoss[0m : 2.56781
[1mStep[0m  [56/84], [94mLoss[0m : 2.81271
[1mStep[0m  [64/84], [94mLoss[0m : 2.37885
[1mStep[0m  [72/84], [94mLoss[0m : 2.70546
[1mStep[0m  [80/84], [94mLoss[0m : 2.68588

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.597, [92mTest[0m: 2.345, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.38281
[1mStep[0m  [8/84], [94mLoss[0m : 2.81013
[1mStep[0m  [16/84], [94mLoss[0m : 2.33914
[1mStep[0m  [24/84], [94mLoss[0m : 2.69857
[1mStep[0m  [32/84], [94mLoss[0m : 2.41311
[1mStep[0m  [40/84], [94mLoss[0m : 2.69449
[1mStep[0m  [48/84], [94mLoss[0m : 2.68402
[1mStep[0m  [56/84], [94mLoss[0m : 2.70209
[1mStep[0m  [64/84], [94mLoss[0m : 2.41790
[1mStep[0m  [72/84], [94mLoss[0m : 2.61388
[1mStep[0m  [80/84], [94mLoss[0m : 2.55936

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.618, [92mTest[0m: 2.345, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.336
====================================

Phase 1 - Evaluation MAE:  2.3364625913756236
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 2.72889
[1mStep[0m  [8/84], [94mLoss[0m : 2.88620
[1mStep[0m  [16/84], [94mLoss[0m : 2.76014
[1mStep[0m  [24/84], [94mLoss[0m : 2.88894
[1mStep[0m  [32/84], [94mLoss[0m : 2.65801
[1mStep[0m  [40/84], [94mLoss[0m : 2.41954
[1mStep[0m  [48/84], [94mLoss[0m : 2.33257
[1mStep[0m  [56/84], [94mLoss[0m : 2.52298
[1mStep[0m  [64/84], [94mLoss[0m : 2.83475
[1mStep[0m  [72/84], [94mLoss[0m : 2.71538
[1mStep[0m  [80/84], [94mLoss[0m : 2.61999

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.636, [92mTest[0m: 2.341, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.65578
[1mStep[0m  [8/84], [94mLoss[0m : 2.72800
[1mStep[0m  [16/84], [94mLoss[0m : 2.73340
[1mStep[0m  [24/84], [94mLoss[0m : 2.88833
[1mStep[0m  [32/84], [94mLoss[0m : 2.43890
[1mStep[0m  [40/84], [94mLoss[0m : 2.65411
[1mStep[0m  [48/84], [94mLoss[0m : 2.62505
[1mStep[0m  [56/84], [94mLoss[0m : 2.82538
[1mStep[0m  [64/84], [94mLoss[0m : 2.91763
[1mStep[0m  [72/84], [94mLoss[0m : 3.01576
[1mStep[0m  [80/84], [94mLoss[0m : 2.77853

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.619, [92mTest[0m: 2.415, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52243
[1mStep[0m  [8/84], [94mLoss[0m : 2.88406
[1mStep[0m  [16/84], [94mLoss[0m : 2.37260
[1mStep[0m  [24/84], [94mLoss[0m : 2.56470
[1mStep[0m  [32/84], [94mLoss[0m : 2.77104
[1mStep[0m  [40/84], [94mLoss[0m : 2.45713
[1mStep[0m  [48/84], [94mLoss[0m : 2.67052
[1mStep[0m  [56/84], [94mLoss[0m : 2.66483
[1mStep[0m  [64/84], [94mLoss[0m : 2.75777
[1mStep[0m  [72/84], [94mLoss[0m : 2.70778
[1mStep[0m  [80/84], [94mLoss[0m : 2.07982

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.566, [92mTest[0m: 2.502, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.63211
[1mStep[0m  [8/84], [94mLoss[0m : 2.53911
[1mStep[0m  [16/84], [94mLoss[0m : 2.40602
[1mStep[0m  [24/84], [94mLoss[0m : 2.38200
[1mStep[0m  [32/84], [94mLoss[0m : 2.63277
[1mStep[0m  [40/84], [94mLoss[0m : 2.68352
[1mStep[0m  [48/84], [94mLoss[0m : 2.36537
[1mStep[0m  [56/84], [94mLoss[0m : 2.79808
[1mStep[0m  [64/84], [94mLoss[0m : 2.60757
[1mStep[0m  [72/84], [94mLoss[0m : 2.66669
[1mStep[0m  [80/84], [94mLoss[0m : 3.01454

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.565, [92mTest[0m: 2.574, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35605
[1mStep[0m  [8/84], [94mLoss[0m : 2.44654
[1mStep[0m  [16/84], [94mLoss[0m : 2.32450
[1mStep[0m  [24/84], [94mLoss[0m : 2.78229
[1mStep[0m  [32/84], [94mLoss[0m : 2.97244
[1mStep[0m  [40/84], [94mLoss[0m : 2.39979
[1mStep[0m  [48/84], [94mLoss[0m : 2.33615
[1mStep[0m  [56/84], [94mLoss[0m : 2.59672
[1mStep[0m  [64/84], [94mLoss[0m : 2.34804
[1mStep[0m  [72/84], [94mLoss[0m : 2.55951
[1mStep[0m  [80/84], [94mLoss[0m : 2.81307

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.545, [92mTest[0m: 2.568, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.58017
[1mStep[0m  [8/84], [94mLoss[0m : 2.47536
[1mStep[0m  [16/84], [94mLoss[0m : 2.35851
[1mStep[0m  [24/84], [94mLoss[0m : 2.64938
[1mStep[0m  [32/84], [94mLoss[0m : 2.44100
[1mStep[0m  [40/84], [94mLoss[0m : 2.73601
[1mStep[0m  [48/84], [94mLoss[0m : 2.42181
[1mStep[0m  [56/84], [94mLoss[0m : 2.38370
[1mStep[0m  [64/84], [94mLoss[0m : 2.52652
[1mStep[0m  [72/84], [94mLoss[0m : 2.49386
[1mStep[0m  [80/84], [94mLoss[0m : 2.40157

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.538, [92mTest[0m: 2.570, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37376
[1mStep[0m  [8/84], [94mLoss[0m : 2.62195
[1mStep[0m  [16/84], [94mLoss[0m : 2.45019
[1mStep[0m  [24/84], [94mLoss[0m : 2.50347
[1mStep[0m  [32/84], [94mLoss[0m : 2.41618
[1mStep[0m  [40/84], [94mLoss[0m : 2.65943
[1mStep[0m  [48/84], [94mLoss[0m : 2.71914
[1mStep[0m  [56/84], [94mLoss[0m : 2.65480
[1mStep[0m  [64/84], [94mLoss[0m : 2.41796
[1mStep[0m  [72/84], [94mLoss[0m : 2.34890
[1mStep[0m  [80/84], [94mLoss[0m : 2.48371

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.577, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53974
[1mStep[0m  [8/84], [94mLoss[0m : 2.37739
[1mStep[0m  [16/84], [94mLoss[0m : 2.47725
[1mStep[0m  [24/84], [94mLoss[0m : 2.61532
[1mStep[0m  [32/84], [94mLoss[0m : 2.73825
[1mStep[0m  [40/84], [94mLoss[0m : 2.16655
[1mStep[0m  [48/84], [94mLoss[0m : 2.49458
[1mStep[0m  [56/84], [94mLoss[0m : 2.53318
[1mStep[0m  [64/84], [94mLoss[0m : 2.30629
[1mStep[0m  [72/84], [94mLoss[0m : 2.47607
[1mStep[0m  [80/84], [94mLoss[0m : 2.36271

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.638, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59005
[1mStep[0m  [8/84], [94mLoss[0m : 2.61026
[1mStep[0m  [16/84], [94mLoss[0m : 2.05216
[1mStep[0m  [24/84], [94mLoss[0m : 2.29984
[1mStep[0m  [32/84], [94mLoss[0m : 2.50760
[1mStep[0m  [40/84], [94mLoss[0m : 2.40336
[1mStep[0m  [48/84], [94mLoss[0m : 2.55586
[1mStep[0m  [56/84], [94mLoss[0m : 2.79591
[1mStep[0m  [64/84], [94mLoss[0m : 2.47166
[1mStep[0m  [72/84], [94mLoss[0m : 2.65967
[1mStep[0m  [80/84], [94mLoss[0m : 2.44049

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.613, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.10736
[1mStep[0m  [8/84], [94mLoss[0m : 2.34308
[1mStep[0m  [16/84], [94mLoss[0m : 2.64363
[1mStep[0m  [24/84], [94mLoss[0m : 2.49946
[1mStep[0m  [32/84], [94mLoss[0m : 2.47536
[1mStep[0m  [40/84], [94mLoss[0m : 2.41271
[1mStep[0m  [48/84], [94mLoss[0m : 2.23241
[1mStep[0m  [56/84], [94mLoss[0m : 2.14367
[1mStep[0m  [64/84], [94mLoss[0m : 1.84180
[1mStep[0m  [72/84], [94mLoss[0m : 2.60257
[1mStep[0m  [80/84], [94mLoss[0m : 2.19749

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.650, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30132
[1mStep[0m  [8/84], [94mLoss[0m : 2.33974
[1mStep[0m  [16/84], [94mLoss[0m : 2.83677
[1mStep[0m  [24/84], [94mLoss[0m : 2.37090
[1mStep[0m  [32/84], [94mLoss[0m : 2.46201
[1mStep[0m  [40/84], [94mLoss[0m : 2.53459
[1mStep[0m  [48/84], [94mLoss[0m : 2.33463
[1mStep[0m  [56/84], [94mLoss[0m : 2.86888
[1mStep[0m  [64/84], [94mLoss[0m : 2.30529
[1mStep[0m  [72/84], [94mLoss[0m : 2.20415
[1mStep[0m  [80/84], [94mLoss[0m : 2.47139

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.600, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45357
[1mStep[0m  [8/84], [94mLoss[0m : 2.06363
[1mStep[0m  [16/84], [94mLoss[0m : 2.29882
[1mStep[0m  [24/84], [94mLoss[0m : 2.26859
[1mStep[0m  [32/84], [94mLoss[0m : 2.15684
[1mStep[0m  [40/84], [94mLoss[0m : 2.30807
[1mStep[0m  [48/84], [94mLoss[0m : 2.59936
[1mStep[0m  [56/84], [94mLoss[0m : 2.32310
[1mStep[0m  [64/84], [94mLoss[0m : 2.51329
[1mStep[0m  [72/84], [94mLoss[0m : 2.51610
[1mStep[0m  [80/84], [94mLoss[0m : 2.65875

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.612, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.62341
[1mStep[0m  [8/84], [94mLoss[0m : 2.27306
[1mStep[0m  [16/84], [94mLoss[0m : 2.30498
[1mStep[0m  [24/84], [94mLoss[0m : 2.52827
[1mStep[0m  [32/84], [94mLoss[0m : 2.29282
[1mStep[0m  [40/84], [94mLoss[0m : 2.15777
[1mStep[0m  [48/84], [94mLoss[0m : 2.30051
[1mStep[0m  [56/84], [94mLoss[0m : 2.34660
[1mStep[0m  [64/84], [94mLoss[0m : 2.64079
[1mStep[0m  [72/84], [94mLoss[0m : 2.53061
[1mStep[0m  [80/84], [94mLoss[0m : 2.39568

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.380, [92mTest[0m: 2.612, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.63527
[1mStep[0m  [8/84], [94mLoss[0m : 2.38214
[1mStep[0m  [16/84], [94mLoss[0m : 2.51086
[1mStep[0m  [24/84], [94mLoss[0m : 2.69670
[1mStep[0m  [32/84], [94mLoss[0m : 2.62968
[1mStep[0m  [40/84], [94mLoss[0m : 2.21821
[1mStep[0m  [48/84], [94mLoss[0m : 2.40381
[1mStep[0m  [56/84], [94mLoss[0m : 2.44769
[1mStep[0m  [64/84], [94mLoss[0m : 2.49672
[1mStep[0m  [72/84], [94mLoss[0m : 2.48238
[1mStep[0m  [80/84], [94mLoss[0m : 2.30755

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.371, [92mTest[0m: 2.591, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35152
[1mStep[0m  [8/84], [94mLoss[0m : 2.29711
[1mStep[0m  [16/84], [94mLoss[0m : 2.32456
[1mStep[0m  [24/84], [94mLoss[0m : 2.37208
[1mStep[0m  [32/84], [94mLoss[0m : 2.71720
[1mStep[0m  [40/84], [94mLoss[0m : 2.35560
[1mStep[0m  [48/84], [94mLoss[0m : 2.21839
[1mStep[0m  [56/84], [94mLoss[0m : 2.33796
[1mStep[0m  [64/84], [94mLoss[0m : 2.54778
[1mStep[0m  [72/84], [94mLoss[0m : 2.38229
[1mStep[0m  [80/84], [94mLoss[0m : 2.51829

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.362, [92mTest[0m: 2.620, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.29580
[1mStep[0m  [8/84], [94mLoss[0m : 2.24982
[1mStep[0m  [16/84], [94mLoss[0m : 2.10303
[1mStep[0m  [24/84], [94mLoss[0m : 2.28354
[1mStep[0m  [32/84], [94mLoss[0m : 1.99974
[1mStep[0m  [40/84], [94mLoss[0m : 2.15712
[1mStep[0m  [48/84], [94mLoss[0m : 2.28980
[1mStep[0m  [56/84], [94mLoss[0m : 2.10761
[1mStep[0m  [64/84], [94mLoss[0m : 2.30543
[1mStep[0m  [72/84], [94mLoss[0m : 2.11183
[1mStep[0m  [80/84], [94mLoss[0m : 2.30854

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.330, [92mTest[0m: 2.638, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.23461
[1mStep[0m  [8/84], [94mLoss[0m : 2.50849
[1mStep[0m  [16/84], [94mLoss[0m : 2.47595
[1mStep[0m  [24/84], [94mLoss[0m : 2.41855
[1mStep[0m  [32/84], [94mLoss[0m : 2.19748
[1mStep[0m  [40/84], [94mLoss[0m : 2.22115
[1mStep[0m  [48/84], [94mLoss[0m : 2.16520
[1mStep[0m  [56/84], [94mLoss[0m : 2.31923
[1mStep[0m  [64/84], [94mLoss[0m : 2.35104
[1mStep[0m  [72/84], [94mLoss[0m : 2.04224
[1mStep[0m  [80/84], [94mLoss[0m : 2.04095

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.274, [92mTest[0m: 2.639, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.99485
[1mStep[0m  [8/84], [94mLoss[0m : 2.37609
[1mStep[0m  [16/84], [94mLoss[0m : 2.25885
[1mStep[0m  [24/84], [94mLoss[0m : 2.06307
[1mStep[0m  [32/84], [94mLoss[0m : 2.26372
[1mStep[0m  [40/84], [94mLoss[0m : 2.24250
[1mStep[0m  [48/84], [94mLoss[0m : 2.35537
[1mStep[0m  [56/84], [94mLoss[0m : 2.17189
[1mStep[0m  [64/84], [94mLoss[0m : 2.38452
[1mStep[0m  [72/84], [94mLoss[0m : 2.27482
[1mStep[0m  [80/84], [94mLoss[0m : 2.36718

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.258, [92mTest[0m: 2.589, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.05734
[1mStep[0m  [8/84], [94mLoss[0m : 2.39193
[1mStep[0m  [16/84], [94mLoss[0m : 2.35137
[1mStep[0m  [24/84], [94mLoss[0m : 2.11807
[1mStep[0m  [32/84], [94mLoss[0m : 2.32232
[1mStep[0m  [40/84], [94mLoss[0m : 2.37778
[1mStep[0m  [48/84], [94mLoss[0m : 2.41486
[1mStep[0m  [56/84], [94mLoss[0m : 2.03760
[1mStep[0m  [64/84], [94mLoss[0m : 2.33301
[1mStep[0m  [72/84], [94mLoss[0m : 2.20554
[1mStep[0m  [80/84], [94mLoss[0m : 2.48355

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.216, [92mTest[0m: 2.598, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30599
[1mStep[0m  [8/84], [94mLoss[0m : 1.98011
[1mStep[0m  [16/84], [94mLoss[0m : 2.06547
[1mStep[0m  [24/84], [94mLoss[0m : 2.18533
[1mStep[0m  [32/84], [94mLoss[0m : 2.18054
[1mStep[0m  [40/84], [94mLoss[0m : 1.98253
[1mStep[0m  [48/84], [94mLoss[0m : 2.14080
[1mStep[0m  [56/84], [94mLoss[0m : 1.88065
[1mStep[0m  [64/84], [94mLoss[0m : 2.18850
[1mStep[0m  [72/84], [94mLoss[0m : 2.11405
[1mStep[0m  [80/84], [94mLoss[0m : 2.00553

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.153, [92mTest[0m: 2.570, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.90808
[1mStep[0m  [8/84], [94mLoss[0m : 2.00007
[1mStep[0m  [16/84], [94mLoss[0m : 2.24364
[1mStep[0m  [24/84], [94mLoss[0m : 2.00668
[1mStep[0m  [32/84], [94mLoss[0m : 2.38815
[1mStep[0m  [40/84], [94mLoss[0m : 2.11274
[1mStep[0m  [48/84], [94mLoss[0m : 2.17270
[1mStep[0m  [56/84], [94mLoss[0m : 2.12888
[1mStep[0m  [64/84], [94mLoss[0m : 2.15828
[1mStep[0m  [72/84], [94mLoss[0m : 2.22210
[1mStep[0m  [80/84], [94mLoss[0m : 2.01265

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.148, [92mTest[0m: 2.639, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.23595
[1mStep[0m  [8/84], [94mLoss[0m : 2.12191
[1mStep[0m  [16/84], [94mLoss[0m : 2.12931
[1mStep[0m  [24/84], [94mLoss[0m : 2.45571
[1mStep[0m  [32/84], [94mLoss[0m : 2.21311
[1mStep[0m  [40/84], [94mLoss[0m : 2.07577
[1mStep[0m  [48/84], [94mLoss[0m : 2.31396
[1mStep[0m  [56/84], [94mLoss[0m : 2.13498
[1mStep[0m  [64/84], [94mLoss[0m : 2.11673
[1mStep[0m  [72/84], [94mLoss[0m : 2.18553
[1mStep[0m  [80/84], [94mLoss[0m : 1.93601

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.158, [92mTest[0m: 2.554, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.90230
[1mStep[0m  [8/84], [94mLoss[0m : 2.09907
[1mStep[0m  [16/84], [94mLoss[0m : 1.93866
[1mStep[0m  [24/84], [94mLoss[0m : 1.91501
[1mStep[0m  [32/84], [94mLoss[0m : 2.14846
[1mStep[0m  [40/84], [94mLoss[0m : 2.26624
[1mStep[0m  [48/84], [94mLoss[0m : 2.21499
[1mStep[0m  [56/84], [94mLoss[0m : 1.85110
[1mStep[0m  [64/84], [94mLoss[0m : 2.36827
[1mStep[0m  [72/84], [94mLoss[0m : 2.12099
[1mStep[0m  [80/84], [94mLoss[0m : 1.78378

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.096, [92mTest[0m: 2.507, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.29160
[1mStep[0m  [8/84], [94mLoss[0m : 2.12861
[1mStep[0m  [16/84], [94mLoss[0m : 2.17262
[1mStep[0m  [24/84], [94mLoss[0m : 2.35064
[1mStep[0m  [32/84], [94mLoss[0m : 2.08815
[1mStep[0m  [40/84], [94mLoss[0m : 2.13285
[1mStep[0m  [48/84], [94mLoss[0m : 1.97083
[1mStep[0m  [56/84], [94mLoss[0m : 2.12633
[1mStep[0m  [64/84], [94mLoss[0m : 1.83983
[1mStep[0m  [72/84], [94mLoss[0m : 2.15666
[1mStep[0m  [80/84], [94mLoss[0m : 1.94465

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.041, [92mTest[0m: 2.544, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.19409
[1mStep[0m  [8/84], [94mLoss[0m : 1.81641
[1mStep[0m  [16/84], [94mLoss[0m : 1.92879
[1mStep[0m  [24/84], [94mLoss[0m : 1.90803
[1mStep[0m  [32/84], [94mLoss[0m : 2.24028
[1mStep[0m  [40/84], [94mLoss[0m : 1.99085
[1mStep[0m  [48/84], [94mLoss[0m : 2.09990
[1mStep[0m  [56/84], [94mLoss[0m : 1.92034
[1mStep[0m  [64/84], [94mLoss[0m : 2.07671
[1mStep[0m  [72/84], [94mLoss[0m : 2.10499
[1mStep[0m  [80/84], [94mLoss[0m : 2.24516

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.065, [92mTest[0m: 2.623, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.03493
[1mStep[0m  [8/84], [94mLoss[0m : 2.18113
[1mStep[0m  [16/84], [94mLoss[0m : 2.19753
[1mStep[0m  [24/84], [94mLoss[0m : 2.07793
[1mStep[0m  [32/84], [94mLoss[0m : 2.01651
[1mStep[0m  [40/84], [94mLoss[0m : 2.17581
[1mStep[0m  [48/84], [94mLoss[0m : 1.85376
[1mStep[0m  [56/84], [94mLoss[0m : 2.18114
[1mStep[0m  [64/84], [94mLoss[0m : 2.01624
[1mStep[0m  [72/84], [94mLoss[0m : 2.39949
[1mStep[0m  [80/84], [94mLoss[0m : 1.99131

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.017, [92mTest[0m: 2.626, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.85698
[1mStep[0m  [8/84], [94mLoss[0m : 1.86482
[1mStep[0m  [16/84], [94mLoss[0m : 1.82963
[1mStep[0m  [24/84], [94mLoss[0m : 2.09244
[1mStep[0m  [32/84], [94mLoss[0m : 1.84642
[1mStep[0m  [40/84], [94mLoss[0m : 1.97750
[1mStep[0m  [48/84], [94mLoss[0m : 2.00380
[1mStep[0m  [56/84], [94mLoss[0m : 2.16874
[1mStep[0m  [64/84], [94mLoss[0m : 2.15767
[1mStep[0m  [72/84], [94mLoss[0m : 1.75135
[1mStep[0m  [80/84], [94mLoss[0m : 2.19374

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.013, [92mTest[0m: 2.534, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.87495
[1mStep[0m  [8/84], [94mLoss[0m : 2.05484
[1mStep[0m  [16/84], [94mLoss[0m : 1.90413
[1mStep[0m  [24/84], [94mLoss[0m : 2.10865
[1mStep[0m  [32/84], [94mLoss[0m : 1.77815
[1mStep[0m  [40/84], [94mLoss[0m : 2.13685
[1mStep[0m  [48/84], [94mLoss[0m : 1.81776
[1mStep[0m  [56/84], [94mLoss[0m : 1.85228
[1mStep[0m  [64/84], [94mLoss[0m : 1.98855
[1mStep[0m  [72/84], [94mLoss[0m : 1.97359
[1mStep[0m  [80/84], [94mLoss[0m : 1.93208

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.966, [92mTest[0m: 2.471, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.08680
[1mStep[0m  [8/84], [94mLoss[0m : 2.15070
[1mStep[0m  [16/84], [94mLoss[0m : 1.96496
[1mStep[0m  [24/84], [94mLoss[0m : 1.75931
[1mStep[0m  [32/84], [94mLoss[0m : 2.08415
[1mStep[0m  [40/84], [94mLoss[0m : 1.93418
[1mStep[0m  [48/84], [94mLoss[0m : 2.08322
[1mStep[0m  [56/84], [94mLoss[0m : 1.67962
[1mStep[0m  [64/84], [94mLoss[0m : 1.97182
[1mStep[0m  [72/84], [94mLoss[0m : 1.82541
[1mStep[0m  [80/84], [94mLoss[0m : 1.92162

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.957, [92mTest[0m: 2.580, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.83398
[1mStep[0m  [8/84], [94mLoss[0m : 1.85208
[1mStep[0m  [16/84], [94mLoss[0m : 1.96685
[1mStep[0m  [24/84], [94mLoss[0m : 1.62730
[1mStep[0m  [32/84], [94mLoss[0m : 1.86714
[1mStep[0m  [40/84], [94mLoss[0m : 1.79833
[1mStep[0m  [48/84], [94mLoss[0m : 2.01830
[1mStep[0m  [56/84], [94mLoss[0m : 1.95019
[1mStep[0m  [64/84], [94mLoss[0m : 1.96334
[1mStep[0m  [72/84], [94mLoss[0m : 2.25141
[1mStep[0m  [80/84], [94mLoss[0m : 1.92365

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.936, [92mTest[0m: 2.577, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.475
====================================

Phase 2 - Evaluation MAE:  2.4746275629316057
MAE score P1       2.336463
MAE score P2       2.474628
loss                1.93638
learning_rate      0.002575
batch_size              128
hidden_sizes      [250, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.5
weight_decay           0.01
Name: 27, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/21], [94mLoss[0m : 11.04744
[1mStep[0m  [2/21], [94mLoss[0m : 10.77202
[1mStep[0m  [4/21], [94mLoss[0m : 9.67573
[1mStep[0m  [6/21], [94mLoss[0m : 8.26967
[1mStep[0m  [8/21], [94mLoss[0m : 6.40094
[1mStep[0m  [10/21], [94mLoss[0m : 4.76937
[1mStep[0m  [12/21], [94mLoss[0m : 3.04332
[1mStep[0m  [14/21], [94mLoss[0m : 2.78434
[1mStep[0m  [16/21], [94mLoss[0m : 3.15438
[1mStep[0m  [18/21], [94mLoss[0m : 3.46552
[1mStep[0m  [20/21], [94mLoss[0m : 3.71054

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.031, [92mTest[0m: 11.145, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.96232
[1mStep[0m  [2/21], [94mLoss[0m : 3.71283
[1mStep[0m  [4/21], [94mLoss[0m : 3.30503
[1mStep[0m  [6/21], [94mLoss[0m : 3.13555
[1mStep[0m  [8/21], [94mLoss[0m : 2.75249
[1mStep[0m  [10/21], [94mLoss[0m : 2.49838
[1mStep[0m  [12/21], [94mLoss[0m : 2.66140
[1mStep[0m  [14/21], [94mLoss[0m : 2.75167
[1mStep[0m  [16/21], [94mLoss[0m : 2.70108
[1mStep[0m  [18/21], [94mLoss[0m : 2.73134
[1mStep[0m  [20/21], [94mLoss[0m : 2.67064

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.947, [92mTest[0m: 3.918, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.55209
[1mStep[0m  [2/21], [94mLoss[0m : 2.35582
[1mStep[0m  [4/21], [94mLoss[0m : 2.39434
[1mStep[0m  [6/21], [94mLoss[0m : 2.37267
[1mStep[0m  [8/21], [94mLoss[0m : 2.49915
[1mStep[0m  [10/21], [94mLoss[0m : 2.60838
[1mStep[0m  [12/21], [94mLoss[0m : 2.47864
[1mStep[0m  [14/21], [94mLoss[0m : 2.63425
[1mStep[0m  [16/21], [94mLoss[0m : 2.50318
[1mStep[0m  [18/21], [94mLoss[0m : 2.47050
[1mStep[0m  [20/21], [94mLoss[0m : 2.47317

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.517, [92mTest[0m: 2.492, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.55862
[1mStep[0m  [2/21], [94mLoss[0m : 2.48628
[1mStep[0m  [4/21], [94mLoss[0m : 2.46039
[1mStep[0m  [6/21], [94mLoss[0m : 2.45163
[1mStep[0m  [8/21], [94mLoss[0m : 2.50498
[1mStep[0m  [10/21], [94mLoss[0m : 2.41257
[1mStep[0m  [12/21], [94mLoss[0m : 2.54090
[1mStep[0m  [14/21], [94mLoss[0m : 2.57062
[1mStep[0m  [16/21], [94mLoss[0m : 2.30376
[1mStep[0m  [18/21], [94mLoss[0m : 2.50103
[1mStep[0m  [20/21], [94mLoss[0m : 2.38567

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.428, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42793
[1mStep[0m  [2/21], [94mLoss[0m : 2.54936
[1mStep[0m  [4/21], [94mLoss[0m : 2.44969
[1mStep[0m  [6/21], [94mLoss[0m : 2.48313
[1mStep[0m  [8/21], [94mLoss[0m : 2.49755
[1mStep[0m  [10/21], [94mLoss[0m : 2.44102
[1mStep[0m  [12/21], [94mLoss[0m : 2.34856
[1mStep[0m  [14/21], [94mLoss[0m : 2.63743
[1mStep[0m  [16/21], [94mLoss[0m : 2.40466
[1mStep[0m  [18/21], [94mLoss[0m : 2.43271
[1mStep[0m  [20/21], [94mLoss[0m : 2.47230

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.413, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53034
[1mStep[0m  [2/21], [94mLoss[0m : 2.43589
[1mStep[0m  [4/21], [94mLoss[0m : 2.53214
[1mStep[0m  [6/21], [94mLoss[0m : 2.33605
[1mStep[0m  [8/21], [94mLoss[0m : 2.38720
[1mStep[0m  [10/21], [94mLoss[0m : 2.33895
[1mStep[0m  [12/21], [94mLoss[0m : 2.42046
[1mStep[0m  [14/21], [94mLoss[0m : 2.49071
[1mStep[0m  [16/21], [94mLoss[0m : 2.35287
[1mStep[0m  [18/21], [94mLoss[0m : 2.52583
[1mStep[0m  [20/21], [94mLoss[0m : 2.43723

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.371, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42933
[1mStep[0m  [2/21], [94mLoss[0m : 2.48982
[1mStep[0m  [4/21], [94mLoss[0m : 2.45005
[1mStep[0m  [6/21], [94mLoss[0m : 2.47816
[1mStep[0m  [8/21], [94mLoss[0m : 2.40104
[1mStep[0m  [10/21], [94mLoss[0m : 2.48508
[1mStep[0m  [12/21], [94mLoss[0m : 2.43278
[1mStep[0m  [14/21], [94mLoss[0m : 2.46730
[1mStep[0m  [16/21], [94mLoss[0m : 2.53361
[1mStep[0m  [18/21], [94mLoss[0m : 2.53956
[1mStep[0m  [20/21], [94mLoss[0m : 2.29985

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.368, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.55811
[1mStep[0m  [2/21], [94mLoss[0m : 2.49824
[1mStep[0m  [4/21], [94mLoss[0m : 2.47259
[1mStep[0m  [6/21], [94mLoss[0m : 2.39895
[1mStep[0m  [8/21], [94mLoss[0m : 2.47012
[1mStep[0m  [10/21], [94mLoss[0m : 2.28311
[1mStep[0m  [12/21], [94mLoss[0m : 2.35919
[1mStep[0m  [14/21], [94mLoss[0m : 2.37378
[1mStep[0m  [16/21], [94mLoss[0m : 2.47061
[1mStep[0m  [18/21], [94mLoss[0m : 2.40960
[1mStep[0m  [20/21], [94mLoss[0m : 2.52746

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.364, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.47491
[1mStep[0m  [2/21], [94mLoss[0m : 2.55531
[1mStep[0m  [4/21], [94mLoss[0m : 2.41923
[1mStep[0m  [6/21], [94mLoss[0m : 2.41538
[1mStep[0m  [8/21], [94mLoss[0m : 2.44716
[1mStep[0m  [10/21], [94mLoss[0m : 2.37207
[1mStep[0m  [12/21], [94mLoss[0m : 2.40327
[1mStep[0m  [14/21], [94mLoss[0m : 2.26368
[1mStep[0m  [16/21], [94mLoss[0m : 2.39239
[1mStep[0m  [18/21], [94mLoss[0m : 2.36452
[1mStep[0m  [20/21], [94mLoss[0m : 2.48312

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.352, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43448
[1mStep[0m  [2/21], [94mLoss[0m : 2.27824
[1mStep[0m  [4/21], [94mLoss[0m : 2.48972
[1mStep[0m  [6/21], [94mLoss[0m : 2.32753
[1mStep[0m  [8/21], [94mLoss[0m : 2.36873
[1mStep[0m  [10/21], [94mLoss[0m : 2.38070
[1mStep[0m  [12/21], [94mLoss[0m : 2.42538
[1mStep[0m  [14/21], [94mLoss[0m : 2.36750
[1mStep[0m  [16/21], [94mLoss[0m : 2.32535
[1mStep[0m  [18/21], [94mLoss[0m : 2.48405
[1mStep[0m  [20/21], [94mLoss[0m : 2.43730

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.352, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.32111
[1mStep[0m  [2/21], [94mLoss[0m : 2.47298
[1mStep[0m  [4/21], [94mLoss[0m : 2.40345
[1mStep[0m  [6/21], [94mLoss[0m : 2.37692
[1mStep[0m  [8/21], [94mLoss[0m : 2.32808
[1mStep[0m  [10/21], [94mLoss[0m : 2.45418
[1mStep[0m  [12/21], [94mLoss[0m : 2.58342
[1mStep[0m  [14/21], [94mLoss[0m : 2.49384
[1mStep[0m  [16/21], [94mLoss[0m : 2.32698
[1mStep[0m  [18/21], [94mLoss[0m : 2.45094
[1mStep[0m  [20/21], [94mLoss[0m : 2.47821

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.346, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.34237
[1mStep[0m  [2/21], [94mLoss[0m : 2.45923
[1mStep[0m  [4/21], [94mLoss[0m : 2.43117
[1mStep[0m  [6/21], [94mLoss[0m : 2.47620
[1mStep[0m  [8/21], [94mLoss[0m : 2.46212
[1mStep[0m  [10/21], [94mLoss[0m : 2.42922
[1mStep[0m  [12/21], [94mLoss[0m : 2.54493
[1mStep[0m  [14/21], [94mLoss[0m : 2.36202
[1mStep[0m  [16/21], [94mLoss[0m : 2.27929
[1mStep[0m  [18/21], [94mLoss[0m : 2.45925
[1mStep[0m  [20/21], [94mLoss[0m : 2.52131

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.345, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.38251
[1mStep[0m  [2/21], [94mLoss[0m : 2.36743
[1mStep[0m  [4/21], [94mLoss[0m : 2.34246
[1mStep[0m  [6/21], [94mLoss[0m : 2.39720
[1mStep[0m  [8/21], [94mLoss[0m : 2.45204
[1mStep[0m  [10/21], [94mLoss[0m : 2.35789
[1mStep[0m  [12/21], [94mLoss[0m : 2.28233
[1mStep[0m  [14/21], [94mLoss[0m : 2.43396
[1mStep[0m  [16/21], [94mLoss[0m : 2.63634
[1mStep[0m  [18/21], [94mLoss[0m : 2.36229
[1mStep[0m  [20/21], [94mLoss[0m : 2.54284

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.343, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52004
[1mStep[0m  [2/21], [94mLoss[0m : 2.50014
[1mStep[0m  [4/21], [94mLoss[0m : 2.44073
[1mStep[0m  [6/21], [94mLoss[0m : 2.52696
[1mStep[0m  [8/21], [94mLoss[0m : 2.53070
[1mStep[0m  [10/21], [94mLoss[0m : 2.51551
[1mStep[0m  [12/21], [94mLoss[0m : 2.35391
[1mStep[0m  [14/21], [94mLoss[0m : 2.47301
[1mStep[0m  [16/21], [94mLoss[0m : 2.36615
[1mStep[0m  [18/21], [94mLoss[0m : 2.28594
[1mStep[0m  [20/21], [94mLoss[0m : 2.32786

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.341, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.41351
[1mStep[0m  [2/21], [94mLoss[0m : 2.40266
[1mStep[0m  [4/21], [94mLoss[0m : 2.55118
[1mStep[0m  [6/21], [94mLoss[0m : 2.50906
[1mStep[0m  [8/21], [94mLoss[0m : 2.39143
[1mStep[0m  [10/21], [94mLoss[0m : 2.42679
[1mStep[0m  [12/21], [94mLoss[0m : 2.38628
[1mStep[0m  [14/21], [94mLoss[0m : 2.42518
[1mStep[0m  [16/21], [94mLoss[0m : 2.44534
[1mStep[0m  [18/21], [94mLoss[0m : 2.27094
[1mStep[0m  [20/21], [94mLoss[0m : 2.31789

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.334, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.45326
[1mStep[0m  [2/21], [94mLoss[0m : 2.56242
[1mStep[0m  [4/21], [94mLoss[0m : 2.29584
[1mStep[0m  [6/21], [94mLoss[0m : 2.53411
[1mStep[0m  [8/21], [94mLoss[0m : 2.43125
[1mStep[0m  [10/21], [94mLoss[0m : 2.40275
[1mStep[0m  [12/21], [94mLoss[0m : 2.44122
[1mStep[0m  [14/21], [94mLoss[0m : 2.46746
[1mStep[0m  [16/21], [94mLoss[0m : 2.38422
[1mStep[0m  [18/21], [94mLoss[0m : 2.45122
[1mStep[0m  [20/21], [94mLoss[0m : 2.29925

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.349, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52990
[1mStep[0m  [2/21], [94mLoss[0m : 2.51250
[1mStep[0m  [4/21], [94mLoss[0m : 2.45766
[1mStep[0m  [6/21], [94mLoss[0m : 2.44139
[1mStep[0m  [8/21], [94mLoss[0m : 2.34203
[1mStep[0m  [10/21], [94mLoss[0m : 2.31505
[1mStep[0m  [12/21], [94mLoss[0m : 2.36639
[1mStep[0m  [14/21], [94mLoss[0m : 2.40521
[1mStep[0m  [16/21], [94mLoss[0m : 2.50944
[1mStep[0m  [18/21], [94mLoss[0m : 2.45801
[1mStep[0m  [20/21], [94mLoss[0m : 2.48838

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.398, [92mTest[0m: 2.334, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.36934
[1mStep[0m  [2/21], [94mLoss[0m : 2.39101
[1mStep[0m  [4/21], [94mLoss[0m : 2.32152
[1mStep[0m  [6/21], [94mLoss[0m : 2.48522
[1mStep[0m  [8/21], [94mLoss[0m : 2.47125
[1mStep[0m  [10/21], [94mLoss[0m : 2.38441
[1mStep[0m  [12/21], [94mLoss[0m : 2.44924
[1mStep[0m  [14/21], [94mLoss[0m : 2.33279
[1mStep[0m  [16/21], [94mLoss[0m : 2.39165
[1mStep[0m  [18/21], [94mLoss[0m : 2.48466
[1mStep[0m  [20/21], [94mLoss[0m : 2.38512

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.335, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.37161
[1mStep[0m  [2/21], [94mLoss[0m : 2.45718
[1mStep[0m  [4/21], [94mLoss[0m : 2.46525
[1mStep[0m  [6/21], [94mLoss[0m : 2.51329
[1mStep[0m  [8/21], [94mLoss[0m : 2.43669
[1mStep[0m  [10/21], [94mLoss[0m : 2.36063
[1mStep[0m  [12/21], [94mLoss[0m : 2.31638
[1mStep[0m  [14/21], [94mLoss[0m : 2.43916
[1mStep[0m  [16/21], [94mLoss[0m : 2.46207
[1mStep[0m  [18/21], [94mLoss[0m : 2.48135
[1mStep[0m  [20/21], [94mLoss[0m : 2.42831

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.341, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48419
[1mStep[0m  [2/21], [94mLoss[0m : 2.26906
[1mStep[0m  [4/21], [94mLoss[0m : 2.35860
[1mStep[0m  [6/21], [94mLoss[0m : 2.39098
[1mStep[0m  [8/21], [94mLoss[0m : 2.46539
[1mStep[0m  [10/21], [94mLoss[0m : 2.41051
[1mStep[0m  [12/21], [94mLoss[0m : 2.39079
[1mStep[0m  [14/21], [94mLoss[0m : 2.35195
[1mStep[0m  [16/21], [94mLoss[0m : 2.46288
[1mStep[0m  [18/21], [94mLoss[0m : 2.46230
[1mStep[0m  [20/21], [94mLoss[0m : 2.26277

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.337, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48080
[1mStep[0m  [2/21], [94mLoss[0m : 2.37186
[1mStep[0m  [4/21], [94mLoss[0m : 2.41010
[1mStep[0m  [6/21], [94mLoss[0m : 2.32973
[1mStep[0m  [8/21], [94mLoss[0m : 2.40772
[1mStep[0m  [10/21], [94mLoss[0m : 2.38042
[1mStep[0m  [12/21], [94mLoss[0m : 2.38084
[1mStep[0m  [14/21], [94mLoss[0m : 2.50396
[1mStep[0m  [16/21], [94mLoss[0m : 2.27532
[1mStep[0m  [18/21], [94mLoss[0m : 2.33316
[1mStep[0m  [20/21], [94mLoss[0m : 2.34973

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.342, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.38637
[1mStep[0m  [2/21], [94mLoss[0m : 2.42064
[1mStep[0m  [4/21], [94mLoss[0m : 2.52340
[1mStep[0m  [6/21], [94mLoss[0m : 2.31132
[1mStep[0m  [8/21], [94mLoss[0m : 2.44493
[1mStep[0m  [10/21], [94mLoss[0m : 2.42895
[1mStep[0m  [12/21], [94mLoss[0m : 2.50560
[1mStep[0m  [14/21], [94mLoss[0m : 2.36914
[1mStep[0m  [16/21], [94mLoss[0m : 2.36452
[1mStep[0m  [18/21], [94mLoss[0m : 2.37678
[1mStep[0m  [20/21], [94mLoss[0m : 2.28805

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.336, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.41845
[1mStep[0m  [2/21], [94mLoss[0m : 2.19691
[1mStep[0m  [4/21], [94mLoss[0m : 2.34888
[1mStep[0m  [6/21], [94mLoss[0m : 2.26208
[1mStep[0m  [8/21], [94mLoss[0m : 2.48517
[1mStep[0m  [10/21], [94mLoss[0m : 2.41522
[1mStep[0m  [12/21], [94mLoss[0m : 2.34780
[1mStep[0m  [14/21], [94mLoss[0m : 2.38015
[1mStep[0m  [16/21], [94mLoss[0m : 2.40030
[1mStep[0m  [18/21], [94mLoss[0m : 2.45871
[1mStep[0m  [20/21], [94mLoss[0m : 2.50708

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.332, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44462
[1mStep[0m  [2/21], [94mLoss[0m : 2.42355
[1mStep[0m  [4/21], [94mLoss[0m : 2.37498
[1mStep[0m  [6/21], [94mLoss[0m : 2.42357
[1mStep[0m  [8/21], [94mLoss[0m : 2.36109
[1mStep[0m  [10/21], [94mLoss[0m : 2.29238
[1mStep[0m  [12/21], [94mLoss[0m : 2.43065
[1mStep[0m  [14/21], [94mLoss[0m : 2.51350
[1mStep[0m  [16/21], [94mLoss[0m : 2.44798
[1mStep[0m  [18/21], [94mLoss[0m : 2.32249
[1mStep[0m  [20/21], [94mLoss[0m : 2.56459

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.331, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.34932
[1mStep[0m  [2/21], [94mLoss[0m : 2.54868
[1mStep[0m  [4/21], [94mLoss[0m : 2.30451
[1mStep[0m  [6/21], [94mLoss[0m : 2.44658
[1mStep[0m  [8/21], [94mLoss[0m : 2.46940
[1mStep[0m  [10/21], [94mLoss[0m : 2.40731
[1mStep[0m  [12/21], [94mLoss[0m : 2.33060
[1mStep[0m  [14/21], [94mLoss[0m : 2.32651
[1mStep[0m  [16/21], [94mLoss[0m : 2.34249
[1mStep[0m  [18/21], [94mLoss[0m : 2.61514
[1mStep[0m  [20/21], [94mLoss[0m : 2.32961

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.332, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43273
[1mStep[0m  [2/21], [94mLoss[0m : 2.43027
[1mStep[0m  [4/21], [94mLoss[0m : 2.34596
[1mStep[0m  [6/21], [94mLoss[0m : 2.39526
[1mStep[0m  [8/21], [94mLoss[0m : 2.26453
[1mStep[0m  [10/21], [94mLoss[0m : 2.36990
[1mStep[0m  [12/21], [94mLoss[0m : 2.32643
[1mStep[0m  [14/21], [94mLoss[0m : 2.47912
[1mStep[0m  [16/21], [94mLoss[0m : 2.40711
[1mStep[0m  [18/21], [94mLoss[0m : 2.60710
[1mStep[0m  [20/21], [94mLoss[0m : 2.35178

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.339, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.31272
[1mStep[0m  [2/21], [94mLoss[0m : 2.51139
[1mStep[0m  [4/21], [94mLoss[0m : 2.34438
[1mStep[0m  [6/21], [94mLoss[0m : 2.38514
[1mStep[0m  [8/21], [94mLoss[0m : 2.46822
[1mStep[0m  [10/21], [94mLoss[0m : 2.21060
[1mStep[0m  [12/21], [94mLoss[0m : 2.47531
[1mStep[0m  [14/21], [94mLoss[0m : 2.39379
[1mStep[0m  [16/21], [94mLoss[0m : 2.38520
[1mStep[0m  [18/21], [94mLoss[0m : 2.49647
[1mStep[0m  [20/21], [94mLoss[0m : 2.55853

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.336, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46382
[1mStep[0m  [2/21], [94mLoss[0m : 2.41390
[1mStep[0m  [4/21], [94mLoss[0m : 2.24734
[1mStep[0m  [6/21], [94mLoss[0m : 2.65504
[1mStep[0m  [8/21], [94mLoss[0m : 2.28835
[1mStep[0m  [10/21], [94mLoss[0m : 2.37060
[1mStep[0m  [12/21], [94mLoss[0m : 2.57976
[1mStep[0m  [14/21], [94mLoss[0m : 2.26942
[1mStep[0m  [16/21], [94mLoss[0m : 2.34949
[1mStep[0m  [18/21], [94mLoss[0m : 2.33483
[1mStep[0m  [20/21], [94mLoss[0m : 2.32603

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.330, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.55541
[1mStep[0m  [2/21], [94mLoss[0m : 2.47996
[1mStep[0m  [4/21], [94mLoss[0m : 2.59445
[1mStep[0m  [6/21], [94mLoss[0m : 2.27891
[1mStep[0m  [8/21], [94mLoss[0m : 2.40177
[1mStep[0m  [10/21], [94mLoss[0m : 2.34006
[1mStep[0m  [12/21], [94mLoss[0m : 2.53688
[1mStep[0m  [14/21], [94mLoss[0m : 2.35023
[1mStep[0m  [16/21], [94mLoss[0m : 2.47746
[1mStep[0m  [18/21], [94mLoss[0m : 2.52275
[1mStep[0m  [20/21], [94mLoss[0m : 2.27065

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.330, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.38797
[1mStep[0m  [2/21], [94mLoss[0m : 2.43180
[1mStep[0m  [4/21], [94mLoss[0m : 2.48873
[1mStep[0m  [6/21], [94mLoss[0m : 2.49647
[1mStep[0m  [8/21], [94mLoss[0m : 2.47823
[1mStep[0m  [10/21], [94mLoss[0m : 2.26231
[1mStep[0m  [12/21], [94mLoss[0m : 2.55155
[1mStep[0m  [14/21], [94mLoss[0m : 2.46707
[1mStep[0m  [16/21], [94mLoss[0m : 2.47390
[1mStep[0m  [18/21], [94mLoss[0m : 2.31956
[1mStep[0m  [20/21], [94mLoss[0m : 2.28009

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.337, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.334
====================================

Phase 1 - Evaluation MAE:  2.333980083465576
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/21], [94mLoss[0m : 2.42512
[1mStep[0m  [2/21], [94mLoss[0m : 2.41297
[1mStep[0m  [4/21], [94mLoss[0m : 2.39979
[1mStep[0m  [6/21], [94mLoss[0m : 2.34589
[1mStep[0m  [8/21], [94mLoss[0m : 2.42937
[1mStep[0m  [10/21], [94mLoss[0m : 2.40846
[1mStep[0m  [12/21], [94mLoss[0m : 2.37721
[1mStep[0m  [14/21], [94mLoss[0m : 2.49354
[1mStep[0m  [16/21], [94mLoss[0m : 2.33743
[1mStep[0m  [18/21], [94mLoss[0m : 2.38340
[1mStep[0m  [20/21], [94mLoss[0m : 2.40722

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.327, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48731
[1mStep[0m  [2/21], [94mLoss[0m : 2.39646
[1mStep[0m  [4/21], [94mLoss[0m : 2.35380
[1mStep[0m  [6/21], [94mLoss[0m : 2.57886
[1mStep[0m  [8/21], [94mLoss[0m : 2.34372
[1mStep[0m  [10/21], [94mLoss[0m : 2.44316
[1mStep[0m  [12/21], [94mLoss[0m : 2.45526
[1mStep[0m  [14/21], [94mLoss[0m : 2.33589
[1mStep[0m  [16/21], [94mLoss[0m : 2.30917
[1mStep[0m  [18/21], [94mLoss[0m : 2.29805
[1mStep[0m  [20/21], [94mLoss[0m : 2.25755

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.381, [92mTest[0m: 2.397, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.18869
[1mStep[0m  [2/21], [94mLoss[0m : 2.31080
[1mStep[0m  [4/21], [94mLoss[0m : 2.33963
[1mStep[0m  [6/21], [94mLoss[0m : 2.39362
[1mStep[0m  [8/21], [94mLoss[0m : 2.32329
[1mStep[0m  [10/21], [94mLoss[0m : 2.48756
[1mStep[0m  [12/21], [94mLoss[0m : 2.24023
[1mStep[0m  [14/21], [94mLoss[0m : 2.43313
[1mStep[0m  [16/21], [94mLoss[0m : 2.48876
[1mStep[0m  [18/21], [94mLoss[0m : 2.33783
[1mStep[0m  [20/21], [94mLoss[0m : 2.33771

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.347, [92mTest[0m: 2.416, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.30684
[1mStep[0m  [2/21], [94mLoss[0m : 2.47477
[1mStep[0m  [4/21], [94mLoss[0m : 2.32492
[1mStep[0m  [6/21], [94mLoss[0m : 2.42052
[1mStep[0m  [8/21], [94mLoss[0m : 2.32051
[1mStep[0m  [10/21], [94mLoss[0m : 2.41767
[1mStep[0m  [12/21], [94mLoss[0m : 2.28825
[1mStep[0m  [14/21], [94mLoss[0m : 2.35325
[1mStep[0m  [16/21], [94mLoss[0m : 2.24279
[1mStep[0m  [18/21], [94mLoss[0m : 2.21352
[1mStep[0m  [20/21], [94mLoss[0m : 2.35103

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.328, [92mTest[0m: 2.376, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.23507
[1mStep[0m  [2/21], [94mLoss[0m : 2.41604
[1mStep[0m  [4/21], [94mLoss[0m : 2.05793
[1mStep[0m  [6/21], [94mLoss[0m : 2.39414
[1mStep[0m  [8/21], [94mLoss[0m : 2.32756
[1mStep[0m  [10/21], [94mLoss[0m : 2.17298
[1mStep[0m  [12/21], [94mLoss[0m : 2.39262
[1mStep[0m  [14/21], [94mLoss[0m : 2.32157
[1mStep[0m  [16/21], [94mLoss[0m : 2.21758
[1mStep[0m  [18/21], [94mLoss[0m : 2.20300
[1mStep[0m  [20/21], [94mLoss[0m : 2.27892

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.286, [92mTest[0m: 2.358, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.19232
[1mStep[0m  [2/21], [94mLoss[0m : 2.28633
[1mStep[0m  [4/21], [94mLoss[0m : 2.35429
[1mStep[0m  [6/21], [94mLoss[0m : 2.21685
[1mStep[0m  [8/21], [94mLoss[0m : 2.29527
[1mStep[0m  [10/21], [94mLoss[0m : 2.25601
[1mStep[0m  [12/21], [94mLoss[0m : 2.20782
[1mStep[0m  [14/21], [94mLoss[0m : 2.31859
[1mStep[0m  [16/21], [94mLoss[0m : 2.19304
[1mStep[0m  [18/21], [94mLoss[0m : 2.26500
[1mStep[0m  [20/21], [94mLoss[0m : 2.31388

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.239, [92mTest[0m: 2.354, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.35218
[1mStep[0m  [2/21], [94mLoss[0m : 2.22282
[1mStep[0m  [4/21], [94mLoss[0m : 2.10232
[1mStep[0m  [6/21], [94mLoss[0m : 2.30662
[1mStep[0m  [8/21], [94mLoss[0m : 2.23035
[1mStep[0m  [10/21], [94mLoss[0m : 2.16003
[1mStep[0m  [12/21], [94mLoss[0m : 2.14534
[1mStep[0m  [14/21], [94mLoss[0m : 2.06623
[1mStep[0m  [16/21], [94mLoss[0m : 2.25539
[1mStep[0m  [18/21], [94mLoss[0m : 2.15388
[1mStep[0m  [20/21], [94mLoss[0m : 2.28380

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.221, [92mTest[0m: 2.344, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.21232
[1mStep[0m  [2/21], [94mLoss[0m : 2.07079
[1mStep[0m  [4/21], [94mLoss[0m : 2.16847
[1mStep[0m  [6/21], [94mLoss[0m : 2.27699
[1mStep[0m  [8/21], [94mLoss[0m : 2.15561
[1mStep[0m  [10/21], [94mLoss[0m : 2.13491
[1mStep[0m  [12/21], [94mLoss[0m : 2.07494
[1mStep[0m  [14/21], [94mLoss[0m : 2.16342
[1mStep[0m  [16/21], [94mLoss[0m : 2.15593
[1mStep[0m  [18/21], [94mLoss[0m : 2.24143
[1mStep[0m  [20/21], [94mLoss[0m : 2.30086

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.162, [92mTest[0m: 2.339, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.96630
[1mStep[0m  [2/21], [94mLoss[0m : 2.14596
[1mStep[0m  [4/21], [94mLoss[0m : 2.15307
[1mStep[0m  [6/21], [94mLoss[0m : 2.01431
[1mStep[0m  [8/21], [94mLoss[0m : 1.98151
[1mStep[0m  [10/21], [94mLoss[0m : 2.17239
[1mStep[0m  [12/21], [94mLoss[0m : 2.24507
[1mStep[0m  [14/21], [94mLoss[0m : 2.20829
[1mStep[0m  [16/21], [94mLoss[0m : 2.10903
[1mStep[0m  [18/21], [94mLoss[0m : 2.30962
[1mStep[0m  [20/21], [94mLoss[0m : 2.09219

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.128, [92mTest[0m: 2.345, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.87960
[1mStep[0m  [2/21], [94mLoss[0m : 2.10234
[1mStep[0m  [4/21], [94mLoss[0m : 2.05188
[1mStep[0m  [6/21], [94mLoss[0m : 2.18650
[1mStep[0m  [8/21], [94mLoss[0m : 2.03722
[1mStep[0m  [10/21], [94mLoss[0m : 2.04460
[1mStep[0m  [12/21], [94mLoss[0m : 2.00643
[1mStep[0m  [14/21], [94mLoss[0m : 2.11295
[1mStep[0m  [16/21], [94mLoss[0m : 2.06294
[1mStep[0m  [18/21], [94mLoss[0m : 2.10714
[1mStep[0m  [20/21], [94mLoss[0m : 2.13799

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.082, [92mTest[0m: 2.348, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.91672
[1mStep[0m  [2/21], [94mLoss[0m : 2.24340
[1mStep[0m  [4/21], [94mLoss[0m : 1.97292
[1mStep[0m  [6/21], [94mLoss[0m : 1.96917
[1mStep[0m  [8/21], [94mLoss[0m : 1.90600
[1mStep[0m  [10/21], [94mLoss[0m : 1.98429
[1mStep[0m  [12/21], [94mLoss[0m : 2.06066
[1mStep[0m  [14/21], [94mLoss[0m : 2.10957
[1mStep[0m  [16/21], [94mLoss[0m : 1.91576
[1mStep[0m  [18/21], [94mLoss[0m : 1.99322
[1mStep[0m  [20/21], [94mLoss[0m : 2.23981

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.036, [92mTest[0m: 2.362, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.98254
[1mStep[0m  [2/21], [94mLoss[0m : 2.03483
[1mStep[0m  [4/21], [94mLoss[0m : 1.89054
[1mStep[0m  [6/21], [94mLoss[0m : 1.89383
[1mStep[0m  [8/21], [94mLoss[0m : 1.99461
[1mStep[0m  [10/21], [94mLoss[0m : 2.07802
[1mStep[0m  [12/21], [94mLoss[0m : 1.97371
[1mStep[0m  [14/21], [94mLoss[0m : 1.85066
[1mStep[0m  [16/21], [94mLoss[0m : 2.05982
[1mStep[0m  [18/21], [94mLoss[0m : 2.10111
[1mStep[0m  [20/21], [94mLoss[0m : 1.95056

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.986, [92mTest[0m: 2.377, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.99679
[1mStep[0m  [2/21], [94mLoss[0m : 2.01788
[1mStep[0m  [4/21], [94mLoss[0m : 1.79933
[1mStep[0m  [6/21], [94mLoss[0m : 1.96152
[1mStep[0m  [8/21], [94mLoss[0m : 1.80571
[1mStep[0m  [10/21], [94mLoss[0m : 1.88328
[1mStep[0m  [12/21], [94mLoss[0m : 1.90282
[1mStep[0m  [14/21], [94mLoss[0m : 2.03589
[1mStep[0m  [16/21], [94mLoss[0m : 1.95006
[1mStep[0m  [18/21], [94mLoss[0m : 1.92940
[1mStep[0m  [20/21], [94mLoss[0m : 1.95964

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.929, [92mTest[0m: 2.411, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.92457
[1mStep[0m  [2/21], [94mLoss[0m : 1.94094
[1mStep[0m  [4/21], [94mLoss[0m : 1.99358
[1mStep[0m  [6/21], [94mLoss[0m : 1.82375
[1mStep[0m  [8/21], [94mLoss[0m : 1.83258
[1mStep[0m  [10/21], [94mLoss[0m : 1.92935
[1mStep[0m  [12/21], [94mLoss[0m : 1.81024
[1mStep[0m  [14/21], [94mLoss[0m : 1.85419
[1mStep[0m  [16/21], [94mLoss[0m : 1.94150
[1mStep[0m  [18/21], [94mLoss[0m : 2.05238
[1mStep[0m  [20/21], [94mLoss[0m : 1.76173

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.891, [92mTest[0m: 2.400, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.82378
[1mStep[0m  [2/21], [94mLoss[0m : 1.89772
[1mStep[0m  [4/21], [94mLoss[0m : 1.64710
[1mStep[0m  [6/21], [94mLoss[0m : 1.77247
[1mStep[0m  [8/21], [94mLoss[0m : 1.72504
[1mStep[0m  [10/21], [94mLoss[0m : 1.95986
[1mStep[0m  [12/21], [94mLoss[0m : 1.84860
[1mStep[0m  [14/21], [94mLoss[0m : 1.86635
[1mStep[0m  [16/21], [94mLoss[0m : 1.92950
[1mStep[0m  [18/21], [94mLoss[0m : 1.75581
[1mStep[0m  [20/21], [94mLoss[0m : 1.74794

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.840, [92mTest[0m: 2.499, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.76017
[1mStep[0m  [2/21], [94mLoss[0m : 1.68030
[1mStep[0m  [4/21], [94mLoss[0m : 1.73971
[1mStep[0m  [6/21], [94mLoss[0m : 1.81663
[1mStep[0m  [8/21], [94mLoss[0m : 1.93879
[1mStep[0m  [10/21], [94mLoss[0m : 1.94102
[1mStep[0m  [12/21], [94mLoss[0m : 1.71070
[1mStep[0m  [14/21], [94mLoss[0m : 1.92008
[1mStep[0m  [16/21], [94mLoss[0m : 1.71615
[1mStep[0m  [18/21], [94mLoss[0m : 1.88481
[1mStep[0m  [20/21], [94mLoss[0m : 1.77294

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.801, [92mTest[0m: 2.469, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.67881
[1mStep[0m  [2/21], [94mLoss[0m : 1.91861
[1mStep[0m  [4/21], [94mLoss[0m : 1.78877
[1mStep[0m  [6/21], [94mLoss[0m : 1.74629
[1mStep[0m  [8/21], [94mLoss[0m : 1.75762
[1mStep[0m  [10/21], [94mLoss[0m : 1.67942
[1mStep[0m  [12/21], [94mLoss[0m : 1.62513
[1mStep[0m  [14/21], [94mLoss[0m : 1.68777
[1mStep[0m  [16/21], [94mLoss[0m : 1.88259
[1mStep[0m  [18/21], [94mLoss[0m : 1.75044
[1mStep[0m  [20/21], [94mLoss[0m : 1.79051

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.749, [92mTest[0m: 2.454, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.69094
[1mStep[0m  [2/21], [94mLoss[0m : 1.70754
[1mStep[0m  [4/21], [94mLoss[0m : 1.86378
[1mStep[0m  [6/21], [94mLoss[0m : 1.74696
[1mStep[0m  [8/21], [94mLoss[0m : 1.62626
[1mStep[0m  [10/21], [94mLoss[0m : 1.75063
[1mStep[0m  [12/21], [94mLoss[0m : 1.70034
[1mStep[0m  [14/21], [94mLoss[0m : 1.65922
[1mStep[0m  [16/21], [94mLoss[0m : 1.75063
[1mStep[0m  [18/21], [94mLoss[0m : 1.65304
[1mStep[0m  [20/21], [94mLoss[0m : 1.74139

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.718, [92mTest[0m: 2.429, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.72559
[1mStep[0m  [2/21], [94mLoss[0m : 1.61361
[1mStep[0m  [4/21], [94mLoss[0m : 1.68457
[1mStep[0m  [6/21], [94mLoss[0m : 1.66390
[1mStep[0m  [8/21], [94mLoss[0m : 1.83460
[1mStep[0m  [10/21], [94mLoss[0m : 1.67319
[1mStep[0m  [12/21], [94mLoss[0m : 1.68900
[1mStep[0m  [14/21], [94mLoss[0m : 1.70601
[1mStep[0m  [16/21], [94mLoss[0m : 1.67096
[1mStep[0m  [18/21], [94mLoss[0m : 1.64506
[1mStep[0m  [20/21], [94mLoss[0m : 1.69950

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.680, [92mTest[0m: 2.447, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.57733
[1mStep[0m  [2/21], [94mLoss[0m : 1.63633
[1mStep[0m  [4/21], [94mLoss[0m : 1.69208
[1mStep[0m  [6/21], [94mLoss[0m : 1.60773
[1mStep[0m  [8/21], [94mLoss[0m : 1.61662
[1mStep[0m  [10/21], [94mLoss[0m : 1.65019
[1mStep[0m  [12/21], [94mLoss[0m : 1.56851
[1mStep[0m  [14/21], [94mLoss[0m : 1.64588
[1mStep[0m  [16/21], [94mLoss[0m : 1.59738
[1mStep[0m  [18/21], [94mLoss[0m : 1.63868
[1mStep[0m  [20/21], [94mLoss[0m : 1.64344

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.631, [92mTest[0m: 2.450, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.58549
[1mStep[0m  [2/21], [94mLoss[0m : 1.62914
[1mStep[0m  [4/21], [94mLoss[0m : 1.57476
[1mStep[0m  [6/21], [94mLoss[0m : 1.65353
[1mStep[0m  [8/21], [94mLoss[0m : 1.56191
[1mStep[0m  [10/21], [94mLoss[0m : 1.52638
[1mStep[0m  [12/21], [94mLoss[0m : 1.68742
[1mStep[0m  [14/21], [94mLoss[0m : 1.59873
[1mStep[0m  [16/21], [94mLoss[0m : 1.64552
[1mStep[0m  [18/21], [94mLoss[0m : 1.59927
[1mStep[0m  [20/21], [94mLoss[0m : 1.59390

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.586, [92mTest[0m: 2.546, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.48772
[1mStep[0m  [2/21], [94mLoss[0m : 1.59536
[1mStep[0m  [4/21], [94mLoss[0m : 1.66457
[1mStep[0m  [6/21], [94mLoss[0m : 1.61374
[1mStep[0m  [8/21], [94mLoss[0m : 1.48417
[1mStep[0m  [10/21], [94mLoss[0m : 1.65562
[1mStep[0m  [12/21], [94mLoss[0m : 1.47840
[1mStep[0m  [14/21], [94mLoss[0m : 1.51702
[1mStep[0m  [16/21], [94mLoss[0m : 1.49964
[1mStep[0m  [18/21], [94mLoss[0m : 1.60259
[1mStep[0m  [20/21], [94mLoss[0m : 1.55179

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.565, [92mTest[0m: 2.478, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.55560
[1mStep[0m  [2/21], [94mLoss[0m : 1.48758
[1mStep[0m  [4/21], [94mLoss[0m : 1.51927
[1mStep[0m  [6/21], [94mLoss[0m : 1.44946
[1mStep[0m  [8/21], [94mLoss[0m : 1.43670
[1mStep[0m  [10/21], [94mLoss[0m : 1.51041
[1mStep[0m  [12/21], [94mLoss[0m : 1.45194
[1mStep[0m  [14/21], [94mLoss[0m : 1.49895
[1mStep[0m  [16/21], [94mLoss[0m : 1.56670
[1mStep[0m  [18/21], [94mLoss[0m : 1.50815
[1mStep[0m  [20/21], [94mLoss[0m : 1.73013

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.534, [92mTest[0m: 2.485, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.52021
[1mStep[0m  [2/21], [94mLoss[0m : 1.53305
[1mStep[0m  [4/21], [94mLoss[0m : 1.44259
[1mStep[0m  [6/21], [94mLoss[0m : 1.48007
[1mStep[0m  [8/21], [94mLoss[0m : 1.51167
[1mStep[0m  [10/21], [94mLoss[0m : 1.57249
[1mStep[0m  [12/21], [94mLoss[0m : 1.53942
[1mStep[0m  [14/21], [94mLoss[0m : 1.44408
[1mStep[0m  [16/21], [94mLoss[0m : 1.52458
[1mStep[0m  [18/21], [94mLoss[0m : 1.51169
[1mStep[0m  [20/21], [94mLoss[0m : 1.43199

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.506, [92mTest[0m: 2.474, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.59261
[1mStep[0m  [2/21], [94mLoss[0m : 1.42140
[1mStep[0m  [4/21], [94mLoss[0m : 1.42565
[1mStep[0m  [6/21], [94mLoss[0m : 1.36098
[1mStep[0m  [8/21], [94mLoss[0m : 1.45751
[1mStep[0m  [10/21], [94mLoss[0m : 1.40674
[1mStep[0m  [12/21], [94mLoss[0m : 1.39920
[1mStep[0m  [14/21], [94mLoss[0m : 1.54017
[1mStep[0m  [16/21], [94mLoss[0m : 1.50519
[1mStep[0m  [18/21], [94mLoss[0m : 1.42104
[1mStep[0m  [20/21], [94mLoss[0m : 1.62425

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.464, [92mTest[0m: 2.479, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.39174
[1mStep[0m  [2/21], [94mLoss[0m : 1.43606
[1mStep[0m  [4/21], [94mLoss[0m : 1.43574
[1mStep[0m  [6/21], [94mLoss[0m : 1.41574
[1mStep[0m  [8/21], [94mLoss[0m : 1.49868
[1mStep[0m  [10/21], [94mLoss[0m : 1.39039
[1mStep[0m  [12/21], [94mLoss[0m : 1.47651
[1mStep[0m  [14/21], [94mLoss[0m : 1.49014
[1mStep[0m  [16/21], [94mLoss[0m : 1.35942
[1mStep[0m  [18/21], [94mLoss[0m : 1.51285
[1mStep[0m  [20/21], [94mLoss[0m : 1.41139

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.449, [92mTest[0m: 2.491, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.38668
[1mStep[0m  [2/21], [94mLoss[0m : 1.48085
[1mStep[0m  [4/21], [94mLoss[0m : 1.33645
[1mStep[0m  [6/21], [94mLoss[0m : 1.34307
[1mStep[0m  [8/21], [94mLoss[0m : 1.37020
[1mStep[0m  [10/21], [94mLoss[0m : 1.44878
[1mStep[0m  [12/21], [94mLoss[0m : 1.37915
[1mStep[0m  [14/21], [94mLoss[0m : 1.48719
[1mStep[0m  [16/21], [94mLoss[0m : 1.64825
[1mStep[0m  [18/21], [94mLoss[0m : 1.38980
[1mStep[0m  [20/21], [94mLoss[0m : 1.51481

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.436, [92mTest[0m: 2.478, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.43771
[1mStep[0m  [2/21], [94mLoss[0m : 1.45429
[1mStep[0m  [4/21], [94mLoss[0m : 1.39502
[1mStep[0m  [6/21], [94mLoss[0m : 1.30788
[1mStep[0m  [8/21], [94mLoss[0m : 1.45301
[1mStep[0m  [10/21], [94mLoss[0m : 1.45267
[1mStep[0m  [12/21], [94mLoss[0m : 1.50924
[1mStep[0m  [14/21], [94mLoss[0m : 1.37511
[1mStep[0m  [16/21], [94mLoss[0m : 1.36285
[1mStep[0m  [18/21], [94mLoss[0m : 1.48916
[1mStep[0m  [20/21], [94mLoss[0m : 1.40498

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.410, [92mTest[0m: 2.517, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.36265
[1mStep[0m  [2/21], [94mLoss[0m : 1.35379
[1mStep[0m  [4/21], [94mLoss[0m : 1.34134
[1mStep[0m  [6/21], [94mLoss[0m : 1.44653
[1mStep[0m  [8/21], [94mLoss[0m : 1.31725
[1mStep[0m  [10/21], [94mLoss[0m : 1.40056
[1mStep[0m  [12/21], [94mLoss[0m : 1.42035
[1mStep[0m  [14/21], [94mLoss[0m : 1.38709
[1mStep[0m  [16/21], [94mLoss[0m : 1.39402
[1mStep[0m  [18/21], [94mLoss[0m : 1.43074
[1mStep[0m  [20/21], [94mLoss[0m : 1.45677

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.392, [92mTest[0m: 2.513, [96mlr[0m: 0.0023175
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 28 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.516
====================================

Phase 2 - Evaluation MAE:  2.516035693032401
MAE score P1       2.33398
MAE score P2      2.516036
loss              1.391747
learning_rate     0.002575
batch_size             512
hidden_sizes         [250]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping        True
dropout                0.2
momentum               0.9
weight_decay          0.01
Name: 28, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
