no change     /home/modelrep/sadiya/miniconda/condabin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda-env
no change     /home/modelrep/sadiya/miniconda/bin/activate
no change     /home/modelrep/sadiya/miniconda/bin/deactivate
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.sh
no change     /home/modelrep/sadiya/miniconda/etc/fish/conf.d/conda.fish
no change     /home/modelrep/sadiya/miniconda/shell/condabin/Conda.psm1
no change     /home/modelrep/sadiya/miniconda/shell/condabin/conda-hook.ps1
no change     /home/modelrep/sadiya/miniconda/lib/python3.10/site-packages/xontrib/conda.xsh
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.csh
no change     /home/modelrep/sadiya/.bashrc
No action taken.
-----------------
Process ID:  11
Number of Nodes:  1
Number of Array Tasks:  16
-----------------
Using device: cuda

AMD Instinct MI210
Memory Usage:
Allocated: 0.0 GB
Cached:    0.0 GB
theta
whole_spec
delta
alpha
beta
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 11.08122
[1mStep[0m  [8/84], [94mLoss[0m : 10.34869
[1mStep[0m  [16/84], [94mLoss[0m : 9.68590
[1mStep[0m  [24/84], [94mLoss[0m : 8.00320
[1mStep[0m  [32/84], [94mLoss[0m : 7.65980
[1mStep[0m  [40/84], [94mLoss[0m : 6.50158
[1mStep[0m  [48/84], [94mLoss[0m : 5.71371
[1mStep[0m  [56/84], [94mLoss[0m : 4.78773
[1mStep[0m  [64/84], [94mLoss[0m : 4.32251
[1mStep[0m  [72/84], [94mLoss[0m : 3.73344
[1mStep[0m  [80/84], [94mLoss[0m : 4.14354

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.767, [92mTest[0m: 11.176, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.66810
[1mStep[0m  [8/84], [94mLoss[0m : 3.01618
[1mStep[0m  [16/84], [94mLoss[0m : 3.38487
[1mStep[0m  [24/84], [94mLoss[0m : 2.95420
[1mStep[0m  [32/84], [94mLoss[0m : 2.48703
[1mStep[0m  [40/84], [94mLoss[0m : 2.86059
[1mStep[0m  [48/84], [94mLoss[0m : 3.16759
[1mStep[0m  [56/84], [94mLoss[0m : 2.41940
[1mStep[0m  [64/84], [94mLoss[0m : 2.86356
[1mStep[0m  [72/84], [94mLoss[0m : 2.44363
[1mStep[0m  [80/84], [94mLoss[0m : 3.12529

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.919, [92mTest[0m: 3.313, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.97500
[1mStep[0m  [8/84], [94mLoss[0m : 2.80763
[1mStep[0m  [16/84], [94mLoss[0m : 2.42773
[1mStep[0m  [24/84], [94mLoss[0m : 3.19796
[1mStep[0m  [32/84], [94mLoss[0m : 2.95413
[1mStep[0m  [40/84], [94mLoss[0m : 2.57779
[1mStep[0m  [48/84], [94mLoss[0m : 2.35775
[1mStep[0m  [56/84], [94mLoss[0m : 2.77554
[1mStep[0m  [64/84], [94mLoss[0m : 2.58883
[1mStep[0m  [72/84], [94mLoss[0m : 2.52031
[1mStep[0m  [80/84], [94mLoss[0m : 2.88745

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.709, [92mTest[0m: 2.450, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35412
[1mStep[0m  [8/84], [94mLoss[0m : 2.67599
[1mStep[0m  [16/84], [94mLoss[0m : 2.69241
[1mStep[0m  [24/84], [94mLoss[0m : 2.82317
[1mStep[0m  [32/84], [94mLoss[0m : 2.97734
[1mStep[0m  [40/84], [94mLoss[0m : 2.40852
[1mStep[0m  [48/84], [94mLoss[0m : 2.89008
[1mStep[0m  [56/84], [94mLoss[0m : 2.85170
[1mStep[0m  [64/84], [94mLoss[0m : 2.72246
[1mStep[0m  [72/84], [94mLoss[0m : 2.85578
[1mStep[0m  [80/84], [94mLoss[0m : 2.60434

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.685, [92mTest[0m: 2.411, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.66122
[1mStep[0m  [8/84], [94mLoss[0m : 2.41656
[1mStep[0m  [16/84], [94mLoss[0m : 2.75816
[1mStep[0m  [24/84], [94mLoss[0m : 2.28892
[1mStep[0m  [32/84], [94mLoss[0m : 2.76571
[1mStep[0m  [40/84], [94mLoss[0m : 2.77701
[1mStep[0m  [48/84], [94mLoss[0m : 2.68642
[1mStep[0m  [56/84], [94mLoss[0m : 2.49496
[1mStep[0m  [64/84], [94mLoss[0m : 2.85890
[1mStep[0m  [72/84], [94mLoss[0m : 2.55456
[1mStep[0m  [80/84], [94mLoss[0m : 2.65713

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.680, [92mTest[0m: 2.391, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.80369
[1mStep[0m  [8/84], [94mLoss[0m : 2.62562
[1mStep[0m  [16/84], [94mLoss[0m : 2.51381
[1mStep[0m  [24/84], [94mLoss[0m : 2.58820
[1mStep[0m  [32/84], [94mLoss[0m : 2.80803
[1mStep[0m  [40/84], [94mLoss[0m : 2.78026
[1mStep[0m  [48/84], [94mLoss[0m : 2.48370
[1mStep[0m  [56/84], [94mLoss[0m : 2.80717
[1mStep[0m  [64/84], [94mLoss[0m : 2.51843
[1mStep[0m  [72/84], [94mLoss[0m : 2.60026
[1mStep[0m  [80/84], [94mLoss[0m : 2.44791

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.670, [92mTest[0m: 2.375, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55656
[1mStep[0m  [8/84], [94mLoss[0m : 2.79867
[1mStep[0m  [16/84], [94mLoss[0m : 2.79069
[1mStep[0m  [24/84], [94mLoss[0m : 2.70132
[1mStep[0m  [32/84], [94mLoss[0m : 2.26311
[1mStep[0m  [40/84], [94mLoss[0m : 2.41269
[1mStep[0m  [48/84], [94mLoss[0m : 2.33083
[1mStep[0m  [56/84], [94mLoss[0m : 2.54123
[1mStep[0m  [64/84], [94mLoss[0m : 2.82535
[1mStep[0m  [72/84], [94mLoss[0m : 2.92218
[1mStep[0m  [80/84], [94mLoss[0m : 2.55056

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.641, [92mTest[0m: 2.362, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.70902
[1mStep[0m  [8/84], [94mLoss[0m : 2.81062
[1mStep[0m  [16/84], [94mLoss[0m : 2.14158
[1mStep[0m  [24/84], [94mLoss[0m : 2.67065
[1mStep[0m  [32/84], [94mLoss[0m : 2.58749
[1mStep[0m  [40/84], [94mLoss[0m : 2.86931
[1mStep[0m  [48/84], [94mLoss[0m : 2.54566
[1mStep[0m  [56/84], [94mLoss[0m : 2.70684
[1mStep[0m  [64/84], [94mLoss[0m : 2.63640
[1mStep[0m  [72/84], [94mLoss[0m : 2.48335
[1mStep[0m  [80/84], [94mLoss[0m : 2.36059

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.649, [92mTest[0m: 2.361, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50621
[1mStep[0m  [8/84], [94mLoss[0m : 2.67775
[1mStep[0m  [16/84], [94mLoss[0m : 2.50422
[1mStep[0m  [24/84], [94mLoss[0m : 2.57541
[1mStep[0m  [32/84], [94mLoss[0m : 2.50819
[1mStep[0m  [40/84], [94mLoss[0m : 2.76969
[1mStep[0m  [48/84], [94mLoss[0m : 2.89577
[1mStep[0m  [56/84], [94mLoss[0m : 2.58904
[1mStep[0m  [64/84], [94mLoss[0m : 2.45079
[1mStep[0m  [72/84], [94mLoss[0m : 2.83915
[1mStep[0m  [80/84], [94mLoss[0m : 2.43461

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.652, [92mTest[0m: 2.356, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.82145
[1mStep[0m  [8/84], [94mLoss[0m : 2.48691
[1mStep[0m  [16/84], [94mLoss[0m : 2.44550
[1mStep[0m  [24/84], [94mLoss[0m : 2.48502
[1mStep[0m  [32/84], [94mLoss[0m : 2.70023
[1mStep[0m  [40/84], [94mLoss[0m : 2.19301
[1mStep[0m  [48/84], [94mLoss[0m : 2.42951
[1mStep[0m  [56/84], [94mLoss[0m : 2.74344
[1mStep[0m  [64/84], [94mLoss[0m : 2.73786
[1mStep[0m  [72/84], [94mLoss[0m : 2.64502
[1mStep[0m  [80/84], [94mLoss[0m : 2.13130

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.640, [92mTest[0m: 2.355, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.76550
[1mStep[0m  [8/84], [94mLoss[0m : 2.80689
[1mStep[0m  [16/84], [94mLoss[0m : 2.49983
[1mStep[0m  [24/84], [94mLoss[0m : 2.79102
[1mStep[0m  [32/84], [94mLoss[0m : 2.42428
[1mStep[0m  [40/84], [94mLoss[0m : 2.52600
[1mStep[0m  [48/84], [94mLoss[0m : 2.82521
[1mStep[0m  [56/84], [94mLoss[0m : 2.76430
[1mStep[0m  [64/84], [94mLoss[0m : 2.55011
[1mStep[0m  [72/84], [94mLoss[0m : 2.79345
[1mStep[0m  [80/84], [94mLoss[0m : 2.54191

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.611, [92mTest[0m: 2.354, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.75176
[1mStep[0m  [8/84], [94mLoss[0m : 2.82955
[1mStep[0m  [16/84], [94mLoss[0m : 2.35276
[1mStep[0m  [24/84], [94mLoss[0m : 2.70772
[1mStep[0m  [32/84], [94mLoss[0m : 2.66193
[1mStep[0m  [40/84], [94mLoss[0m : 2.75682
[1mStep[0m  [48/84], [94mLoss[0m : 2.65995
[1mStep[0m  [56/84], [94mLoss[0m : 2.76074
[1mStep[0m  [64/84], [94mLoss[0m : 2.73885
[1mStep[0m  [72/84], [94mLoss[0m : 2.47222
[1mStep[0m  [80/84], [94mLoss[0m : 2.74640

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.636, [92mTest[0m: 2.341, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60954
[1mStep[0m  [8/84], [94mLoss[0m : 2.61465
[1mStep[0m  [16/84], [94mLoss[0m : 2.70525
[1mStep[0m  [24/84], [94mLoss[0m : 2.48978
[1mStep[0m  [32/84], [94mLoss[0m : 2.64710
[1mStep[0m  [40/84], [94mLoss[0m : 2.40996
[1mStep[0m  [48/84], [94mLoss[0m : 2.49910
[1mStep[0m  [56/84], [94mLoss[0m : 2.68939
[1mStep[0m  [64/84], [94mLoss[0m : 2.71630
[1mStep[0m  [72/84], [94mLoss[0m : 2.58908
[1mStep[0m  [80/84], [94mLoss[0m : 2.63669

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.628, [92mTest[0m: 2.344, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.72794
[1mStep[0m  [8/84], [94mLoss[0m : 2.85517
[1mStep[0m  [16/84], [94mLoss[0m : 2.67167
[1mStep[0m  [24/84], [94mLoss[0m : 2.72334
[1mStep[0m  [32/84], [94mLoss[0m : 2.85772
[1mStep[0m  [40/84], [94mLoss[0m : 2.83002
[1mStep[0m  [48/84], [94mLoss[0m : 2.93480
[1mStep[0m  [56/84], [94mLoss[0m : 2.63686
[1mStep[0m  [64/84], [94mLoss[0m : 2.29354
[1mStep[0m  [72/84], [94mLoss[0m : 2.51225
[1mStep[0m  [80/84], [94mLoss[0m : 2.56777

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.651, [92mTest[0m: 2.343, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46075
[1mStep[0m  [8/84], [94mLoss[0m : 2.78515
[1mStep[0m  [16/84], [94mLoss[0m : 2.45412
[1mStep[0m  [24/84], [94mLoss[0m : 2.57334
[1mStep[0m  [32/84], [94mLoss[0m : 2.82815
[1mStep[0m  [40/84], [94mLoss[0m : 2.62383
[1mStep[0m  [48/84], [94mLoss[0m : 2.72276
[1mStep[0m  [56/84], [94mLoss[0m : 2.61224
[1mStep[0m  [64/84], [94mLoss[0m : 2.30441
[1mStep[0m  [72/84], [94mLoss[0m : 2.29035
[1mStep[0m  [80/84], [94mLoss[0m : 2.68379

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.625, [92mTest[0m: 2.346, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.64131
[1mStep[0m  [8/84], [94mLoss[0m : 2.57722
[1mStep[0m  [16/84], [94mLoss[0m : 2.66174
[1mStep[0m  [24/84], [94mLoss[0m : 2.57622
[1mStep[0m  [32/84], [94mLoss[0m : 2.62561
[1mStep[0m  [40/84], [94mLoss[0m : 2.42732
[1mStep[0m  [48/84], [94mLoss[0m : 2.59878
[1mStep[0m  [56/84], [94mLoss[0m : 2.56289
[1mStep[0m  [64/84], [94mLoss[0m : 2.59106
[1mStep[0m  [72/84], [94mLoss[0m : 2.59749
[1mStep[0m  [80/84], [94mLoss[0m : 2.46524

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.634, [92mTest[0m: 2.348, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49356
[1mStep[0m  [8/84], [94mLoss[0m : 2.61795
[1mStep[0m  [16/84], [94mLoss[0m : 2.39448
[1mStep[0m  [24/84], [94mLoss[0m : 2.59436
[1mStep[0m  [32/84], [94mLoss[0m : 2.42297
[1mStep[0m  [40/84], [94mLoss[0m : 2.36428
[1mStep[0m  [48/84], [94mLoss[0m : 2.56887
[1mStep[0m  [56/84], [94mLoss[0m : 2.70997
[1mStep[0m  [64/84], [94mLoss[0m : 2.67079
[1mStep[0m  [72/84], [94mLoss[0m : 2.53573
[1mStep[0m  [80/84], [94mLoss[0m : 2.22626

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.615, [92mTest[0m: 2.342, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.57251
[1mStep[0m  [8/84], [94mLoss[0m : 2.67347
[1mStep[0m  [16/84], [94mLoss[0m : 2.59758
[1mStep[0m  [24/84], [94mLoss[0m : 2.39533
[1mStep[0m  [32/84], [94mLoss[0m : 2.32463
[1mStep[0m  [40/84], [94mLoss[0m : 2.44889
[1mStep[0m  [48/84], [94mLoss[0m : 2.37375
[1mStep[0m  [56/84], [94mLoss[0m : 2.78705
[1mStep[0m  [64/84], [94mLoss[0m : 2.89994
[1mStep[0m  [72/84], [94mLoss[0m : 2.59153
[1mStep[0m  [80/84], [94mLoss[0m : 2.66972

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.606, [92mTest[0m: 2.338, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.79788
[1mStep[0m  [8/84], [94mLoss[0m : 2.61689
[1mStep[0m  [16/84], [94mLoss[0m : 2.45898
[1mStep[0m  [24/84], [94mLoss[0m : 2.67681
[1mStep[0m  [32/84], [94mLoss[0m : 2.84590
[1mStep[0m  [40/84], [94mLoss[0m : 2.57701
[1mStep[0m  [48/84], [94mLoss[0m : 2.51688
[1mStep[0m  [56/84], [94mLoss[0m : 2.85275
[1mStep[0m  [64/84], [94mLoss[0m : 2.55819
[1mStep[0m  [72/84], [94mLoss[0m : 2.81449
[1mStep[0m  [80/84], [94mLoss[0m : 2.58878

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.619, [92mTest[0m: 2.343, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.82603
[1mStep[0m  [8/84], [94mLoss[0m : 2.47660
[1mStep[0m  [16/84], [94mLoss[0m : 2.72559
[1mStep[0m  [24/84], [94mLoss[0m : 2.53178
[1mStep[0m  [32/84], [94mLoss[0m : 2.33415
[1mStep[0m  [40/84], [94mLoss[0m : 2.73798
[1mStep[0m  [48/84], [94mLoss[0m : 2.76946
[1mStep[0m  [56/84], [94mLoss[0m : 2.58693
[1mStep[0m  [64/84], [94mLoss[0m : 2.47092
[1mStep[0m  [72/84], [94mLoss[0m : 2.64658
[1mStep[0m  [80/84], [94mLoss[0m : 2.61107

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.603, [92mTest[0m: 2.335, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47313
[1mStep[0m  [8/84], [94mLoss[0m : 2.47030
[1mStep[0m  [16/84], [94mLoss[0m : 2.52046
[1mStep[0m  [24/84], [94mLoss[0m : 2.75078
[1mStep[0m  [32/84], [94mLoss[0m : 2.62293
[1mStep[0m  [40/84], [94mLoss[0m : 2.46143
[1mStep[0m  [48/84], [94mLoss[0m : 2.81295
[1mStep[0m  [56/84], [94mLoss[0m : 2.41142
[1mStep[0m  [64/84], [94mLoss[0m : 2.72785
[1mStep[0m  [72/84], [94mLoss[0m : 2.85511
[1mStep[0m  [80/84], [94mLoss[0m : 2.50420

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.595, [92mTest[0m: 2.342, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52741
[1mStep[0m  [8/84], [94mLoss[0m : 2.74604
[1mStep[0m  [16/84], [94mLoss[0m : 2.21592
[1mStep[0m  [24/84], [94mLoss[0m : 2.68789
[1mStep[0m  [32/84], [94mLoss[0m : 2.61837
[1mStep[0m  [40/84], [94mLoss[0m : 2.72146
[1mStep[0m  [48/84], [94mLoss[0m : 2.83032
[1mStep[0m  [56/84], [94mLoss[0m : 2.48981
[1mStep[0m  [64/84], [94mLoss[0m : 2.67937
[1mStep[0m  [72/84], [94mLoss[0m : 2.63594
[1mStep[0m  [80/84], [94mLoss[0m : 2.49775

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.606, [92mTest[0m: 2.336, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.83219
[1mStep[0m  [8/84], [94mLoss[0m : 2.36680
[1mStep[0m  [16/84], [94mLoss[0m : 2.34536
[1mStep[0m  [24/84], [94mLoss[0m : 2.43750
[1mStep[0m  [32/84], [94mLoss[0m : 2.79288
[1mStep[0m  [40/84], [94mLoss[0m : 2.29005
[1mStep[0m  [48/84], [94mLoss[0m : 2.66246
[1mStep[0m  [56/84], [94mLoss[0m : 2.57323
[1mStep[0m  [64/84], [94mLoss[0m : 2.96915
[1mStep[0m  [72/84], [94mLoss[0m : 2.78685
[1mStep[0m  [80/84], [94mLoss[0m : 2.53400

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.604, [92mTest[0m: 2.338, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60909
[1mStep[0m  [8/84], [94mLoss[0m : 2.50935
[1mStep[0m  [16/84], [94mLoss[0m : 2.56796
[1mStep[0m  [24/84], [94mLoss[0m : 2.63812
[1mStep[0m  [32/84], [94mLoss[0m : 2.68373
[1mStep[0m  [40/84], [94mLoss[0m : 2.35354
[1mStep[0m  [48/84], [94mLoss[0m : 2.70035
[1mStep[0m  [56/84], [94mLoss[0m : 2.88768
[1mStep[0m  [64/84], [94mLoss[0m : 2.59954
[1mStep[0m  [72/84], [94mLoss[0m : 2.51239
[1mStep[0m  [80/84], [94mLoss[0m : 2.71477

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.589, [92mTest[0m: 2.345, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.62420
[1mStep[0m  [8/84], [94mLoss[0m : 2.62661
[1mStep[0m  [16/84], [94mLoss[0m : 2.38702
[1mStep[0m  [24/84], [94mLoss[0m : 2.71001
[1mStep[0m  [32/84], [94mLoss[0m : 2.63290
[1mStep[0m  [40/84], [94mLoss[0m : 2.63316
[1mStep[0m  [48/84], [94mLoss[0m : 2.26532
[1mStep[0m  [56/84], [94mLoss[0m : 2.61516
[1mStep[0m  [64/84], [94mLoss[0m : 2.50051
[1mStep[0m  [72/84], [94mLoss[0m : 2.59731
[1mStep[0m  [80/84], [94mLoss[0m : 2.50906

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.592, [92mTest[0m: 2.339, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.24046
[1mStep[0m  [8/84], [94mLoss[0m : 2.34932
[1mStep[0m  [16/84], [94mLoss[0m : 2.59278
[1mStep[0m  [24/84], [94mLoss[0m : 2.79880
[1mStep[0m  [32/84], [94mLoss[0m : 2.44554
[1mStep[0m  [40/84], [94mLoss[0m : 2.49041
[1mStep[0m  [48/84], [94mLoss[0m : 2.86793
[1mStep[0m  [56/84], [94mLoss[0m : 2.35615
[1mStep[0m  [64/84], [94mLoss[0m : 2.89047
[1mStep[0m  [72/84], [94mLoss[0m : 2.39796
[1mStep[0m  [80/84], [94mLoss[0m : 2.28002

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.620, [92mTest[0m: 2.328, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41843
[1mStep[0m  [8/84], [94mLoss[0m : 2.67848
[1mStep[0m  [16/84], [94mLoss[0m : 2.34113
[1mStep[0m  [24/84], [94mLoss[0m : 2.32716
[1mStep[0m  [32/84], [94mLoss[0m : 2.83640
[1mStep[0m  [40/84], [94mLoss[0m : 2.77451
[1mStep[0m  [48/84], [94mLoss[0m : 2.51160
[1mStep[0m  [56/84], [94mLoss[0m : 2.29268
[1mStep[0m  [64/84], [94mLoss[0m : 2.54705
[1mStep[0m  [72/84], [94mLoss[0m : 2.69298
[1mStep[0m  [80/84], [94mLoss[0m : 2.81190

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.580, [92mTest[0m: 2.337, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47840
[1mStep[0m  [8/84], [94mLoss[0m : 2.68086
[1mStep[0m  [16/84], [94mLoss[0m : 2.53414
[1mStep[0m  [24/84], [94mLoss[0m : 2.71297
[1mStep[0m  [32/84], [94mLoss[0m : 2.53324
[1mStep[0m  [40/84], [94mLoss[0m : 2.41141
[1mStep[0m  [48/84], [94mLoss[0m : 3.10157
[1mStep[0m  [56/84], [94mLoss[0m : 2.68225
[1mStep[0m  [64/84], [94mLoss[0m : 2.47922
[1mStep[0m  [72/84], [94mLoss[0m : 2.17778
[1mStep[0m  [80/84], [94mLoss[0m : 2.65861

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.593, [92mTest[0m: 2.339, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.84718
[1mStep[0m  [8/84], [94mLoss[0m : 2.50930
[1mStep[0m  [16/84], [94mLoss[0m : 2.68925
[1mStep[0m  [24/84], [94mLoss[0m : 2.49844
[1mStep[0m  [32/84], [94mLoss[0m : 2.24102
[1mStep[0m  [40/84], [94mLoss[0m : 2.36052
[1mStep[0m  [48/84], [94mLoss[0m : 2.34545
[1mStep[0m  [56/84], [94mLoss[0m : 2.22256
[1mStep[0m  [64/84], [94mLoss[0m : 2.29085
[1mStep[0m  [72/84], [94mLoss[0m : 2.62615
[1mStep[0m  [80/84], [94mLoss[0m : 2.43276

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.588, [92mTest[0m: 2.341, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.74972
[1mStep[0m  [8/84], [94mLoss[0m : 2.90437
[1mStep[0m  [16/84], [94mLoss[0m : 2.58717
[1mStep[0m  [24/84], [94mLoss[0m : 2.84988
[1mStep[0m  [32/84], [94mLoss[0m : 2.57058
[1mStep[0m  [40/84], [94mLoss[0m : 2.88260
[1mStep[0m  [48/84], [94mLoss[0m : 2.83576
[1mStep[0m  [56/84], [94mLoss[0m : 2.56448
[1mStep[0m  [64/84], [94mLoss[0m : 2.68128
[1mStep[0m  [72/84], [94mLoss[0m : 2.42183
[1mStep[0m  [80/84], [94mLoss[0m : 2.25957

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.592, [92mTest[0m: 2.341, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.330
====================================

Phase 1 - Evaluation MAE:  2.330377561705453
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 2.50477
[1mStep[0m  [8/84], [94mLoss[0m : 2.60140
[1mStep[0m  [16/84], [94mLoss[0m : 2.58339
[1mStep[0m  [24/84], [94mLoss[0m : 2.03128
[1mStep[0m  [32/84], [94mLoss[0m : 2.42231
[1mStep[0m  [40/84], [94mLoss[0m : 2.70562
[1mStep[0m  [48/84], [94mLoss[0m : 2.43567
[1mStep[0m  [56/84], [94mLoss[0m : 2.63999
[1mStep[0m  [64/84], [94mLoss[0m : 2.57160
[1mStep[0m  [72/84], [94mLoss[0m : 2.91521
[1mStep[0m  [80/84], [94mLoss[0m : 2.73277

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.590, [92mTest[0m: 2.334, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40279
[1mStep[0m  [8/84], [94mLoss[0m : 3.13035
[1mStep[0m  [16/84], [94mLoss[0m : 2.44502
[1mStep[0m  [24/84], [94mLoss[0m : 2.53421
[1mStep[0m  [32/84], [94mLoss[0m : 2.82385
[1mStep[0m  [40/84], [94mLoss[0m : 2.42020
[1mStep[0m  [48/84], [94mLoss[0m : 2.30446
[1mStep[0m  [56/84], [94mLoss[0m : 2.40054
[1mStep[0m  [64/84], [94mLoss[0m : 2.73293
[1mStep[0m  [72/84], [94mLoss[0m : 2.70206
[1mStep[0m  [80/84], [94mLoss[0m : 2.85592

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.574, [92mTest[0m: 2.431, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30410
[1mStep[0m  [8/84], [94mLoss[0m : 2.55660
[1mStep[0m  [16/84], [94mLoss[0m : 2.78540
[1mStep[0m  [24/84], [94mLoss[0m : 2.63185
[1mStep[0m  [32/84], [94mLoss[0m : 2.68211
[1mStep[0m  [40/84], [94mLoss[0m : 2.48050
[1mStep[0m  [48/84], [94mLoss[0m : 2.61075
[1mStep[0m  [56/84], [94mLoss[0m : 2.46159
[1mStep[0m  [64/84], [94mLoss[0m : 2.40881
[1mStep[0m  [72/84], [94mLoss[0m : 2.53369
[1mStep[0m  [80/84], [94mLoss[0m : 2.73926

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.545, [92mTest[0m: 2.721, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41906
[1mStep[0m  [8/84], [94mLoss[0m : 2.43309
[1mStep[0m  [16/84], [94mLoss[0m : 2.70120
[1mStep[0m  [24/84], [94mLoss[0m : 2.22871
[1mStep[0m  [32/84], [94mLoss[0m : 2.49099
[1mStep[0m  [40/84], [94mLoss[0m : 2.49370
[1mStep[0m  [48/84], [94mLoss[0m : 2.53355
[1mStep[0m  [56/84], [94mLoss[0m : 2.52265
[1mStep[0m  [64/84], [94mLoss[0m : 2.47893
[1mStep[0m  [72/84], [94mLoss[0m : 2.10528
[1mStep[0m  [80/84], [94mLoss[0m : 2.60494

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.686, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.69631
[1mStep[0m  [8/84], [94mLoss[0m : 2.35693
[1mStep[0m  [16/84], [94mLoss[0m : 2.23898
[1mStep[0m  [24/84], [94mLoss[0m : 2.77959
[1mStep[0m  [32/84], [94mLoss[0m : 2.28661
[1mStep[0m  [40/84], [94mLoss[0m : 2.35178
[1mStep[0m  [48/84], [94mLoss[0m : 2.43414
[1mStep[0m  [56/84], [94mLoss[0m : 2.49854
[1mStep[0m  [64/84], [94mLoss[0m : 2.48388
[1mStep[0m  [72/84], [94mLoss[0m : 2.62814
[1mStep[0m  [80/84], [94mLoss[0m : 2.55129

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.647, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27612
[1mStep[0m  [8/84], [94mLoss[0m : 2.50427
[1mStep[0m  [16/84], [94mLoss[0m : 2.80225
[1mStep[0m  [24/84], [94mLoss[0m : 2.24427
[1mStep[0m  [32/84], [94mLoss[0m : 2.07694
[1mStep[0m  [40/84], [94mLoss[0m : 2.53446
[1mStep[0m  [48/84], [94mLoss[0m : 2.35680
[1mStep[0m  [56/84], [94mLoss[0m : 2.33420
[1mStep[0m  [64/84], [94mLoss[0m : 2.93977
[1mStep[0m  [72/84], [94mLoss[0m : 2.45704
[1mStep[0m  [80/84], [94mLoss[0m : 2.65784

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.635, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40596
[1mStep[0m  [8/84], [94mLoss[0m : 2.37055
[1mStep[0m  [16/84], [94mLoss[0m : 2.41287
[1mStep[0m  [24/84], [94mLoss[0m : 2.41949
[1mStep[0m  [32/84], [94mLoss[0m : 2.67459
[1mStep[0m  [40/84], [94mLoss[0m : 2.60254
[1mStep[0m  [48/84], [94mLoss[0m : 2.37534
[1mStep[0m  [56/84], [94mLoss[0m : 2.42087
[1mStep[0m  [64/84], [94mLoss[0m : 2.34411
[1mStep[0m  [72/84], [94mLoss[0m : 2.49032
[1mStep[0m  [80/84], [94mLoss[0m : 2.16834

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.667, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33820
[1mStep[0m  [8/84], [94mLoss[0m : 2.40262
[1mStep[0m  [16/84], [94mLoss[0m : 2.23353
[1mStep[0m  [24/84], [94mLoss[0m : 2.15999
[1mStep[0m  [32/84], [94mLoss[0m : 2.54063
[1mStep[0m  [40/84], [94mLoss[0m : 2.60188
[1mStep[0m  [48/84], [94mLoss[0m : 2.17185
[1mStep[0m  [56/84], [94mLoss[0m : 2.40217
[1mStep[0m  [64/84], [94mLoss[0m : 2.48767
[1mStep[0m  [72/84], [94mLoss[0m : 2.62547
[1mStep[0m  [80/84], [94mLoss[0m : 2.16921

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.581, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.62904
[1mStep[0m  [8/84], [94mLoss[0m : 2.38861
[1mStep[0m  [16/84], [94mLoss[0m : 2.37410
[1mStep[0m  [24/84], [94mLoss[0m : 2.39278
[1mStep[0m  [32/84], [94mLoss[0m : 2.40753
[1mStep[0m  [40/84], [94mLoss[0m : 2.21652
[1mStep[0m  [48/84], [94mLoss[0m : 2.06459
[1mStep[0m  [56/84], [94mLoss[0m : 2.35451
[1mStep[0m  [64/84], [94mLoss[0m : 2.20379
[1mStep[0m  [72/84], [94mLoss[0m : 2.16254
[1mStep[0m  [80/84], [94mLoss[0m : 2.44157

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.352, [92mTest[0m: 2.616, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27100
[1mStep[0m  [8/84], [94mLoss[0m : 2.13486
[1mStep[0m  [16/84], [94mLoss[0m : 2.28641
[1mStep[0m  [24/84], [94mLoss[0m : 2.36025
[1mStep[0m  [32/84], [94mLoss[0m : 2.30654
[1mStep[0m  [40/84], [94mLoss[0m : 2.07882
[1mStep[0m  [48/84], [94mLoss[0m : 2.30581
[1mStep[0m  [56/84], [94mLoss[0m : 2.56772
[1mStep[0m  [64/84], [94mLoss[0m : 2.44531
[1mStep[0m  [72/84], [94mLoss[0m : 2.36880
[1mStep[0m  [80/84], [94mLoss[0m : 2.26849

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.336, [92mTest[0m: 2.602, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27136
[1mStep[0m  [8/84], [94mLoss[0m : 2.18779
[1mStep[0m  [16/84], [94mLoss[0m : 1.92037
[1mStep[0m  [24/84], [94mLoss[0m : 2.36337
[1mStep[0m  [32/84], [94mLoss[0m : 2.35738
[1mStep[0m  [40/84], [94mLoss[0m : 2.10212
[1mStep[0m  [48/84], [94mLoss[0m : 2.17563
[1mStep[0m  [56/84], [94mLoss[0m : 2.14110
[1mStep[0m  [64/84], [94mLoss[0m : 2.53673
[1mStep[0m  [72/84], [94mLoss[0m : 2.32640
[1mStep[0m  [80/84], [94mLoss[0m : 2.54954

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.293, [92mTest[0m: 2.620, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.11017
[1mStep[0m  [8/84], [94mLoss[0m : 2.27207
[1mStep[0m  [16/84], [94mLoss[0m : 2.38688
[1mStep[0m  [24/84], [94mLoss[0m : 2.20921
[1mStep[0m  [32/84], [94mLoss[0m : 2.28553
[1mStep[0m  [40/84], [94mLoss[0m : 2.09655
[1mStep[0m  [48/84], [94mLoss[0m : 2.48476
[1mStep[0m  [56/84], [94mLoss[0m : 2.24317
[1mStep[0m  [64/84], [94mLoss[0m : 2.38517
[1mStep[0m  [72/84], [94mLoss[0m : 2.17664
[1mStep[0m  [80/84], [94mLoss[0m : 2.27937

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.275, [92mTest[0m: 2.496, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.01415
[1mStep[0m  [8/84], [94mLoss[0m : 2.38073
[1mStep[0m  [16/84], [94mLoss[0m : 2.36164
[1mStep[0m  [24/84], [94mLoss[0m : 2.33148
[1mStep[0m  [32/84], [94mLoss[0m : 2.44913
[1mStep[0m  [40/84], [94mLoss[0m : 2.56394
[1mStep[0m  [48/84], [94mLoss[0m : 2.30799
[1mStep[0m  [56/84], [94mLoss[0m : 2.19762
[1mStep[0m  [64/84], [94mLoss[0m : 2.30986
[1mStep[0m  [72/84], [94mLoss[0m : 2.23670
[1mStep[0m  [80/84], [94mLoss[0m : 2.03007

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.250, [92mTest[0m: 2.505, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.07787
[1mStep[0m  [8/84], [94mLoss[0m : 2.17165
[1mStep[0m  [16/84], [94mLoss[0m : 2.16245
[1mStep[0m  [24/84], [94mLoss[0m : 2.56666
[1mStep[0m  [32/84], [94mLoss[0m : 2.36887
[1mStep[0m  [40/84], [94mLoss[0m : 2.24952
[1mStep[0m  [48/84], [94mLoss[0m : 1.94841
[1mStep[0m  [56/84], [94mLoss[0m : 2.31642
[1mStep[0m  [64/84], [94mLoss[0m : 2.26459
[1mStep[0m  [72/84], [94mLoss[0m : 2.10724
[1mStep[0m  [80/84], [94mLoss[0m : 2.21440

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.196, [92mTest[0m: 2.541, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.97793
[1mStep[0m  [8/84], [94mLoss[0m : 2.12813
[1mStep[0m  [16/84], [94mLoss[0m : 1.98466
[1mStep[0m  [24/84], [94mLoss[0m : 1.92487
[1mStep[0m  [32/84], [94mLoss[0m : 2.27038
[1mStep[0m  [40/84], [94mLoss[0m : 2.08231
[1mStep[0m  [48/84], [94mLoss[0m : 2.00678
[1mStep[0m  [56/84], [94mLoss[0m : 2.10652
[1mStep[0m  [64/84], [94mLoss[0m : 1.99456
[1mStep[0m  [72/84], [94mLoss[0m : 2.38368
[1mStep[0m  [80/84], [94mLoss[0m : 2.18541

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.170, [92mTest[0m: 2.536, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.88854
[1mStep[0m  [8/84], [94mLoss[0m : 2.19994
[1mStep[0m  [16/84], [94mLoss[0m : 1.97336
[1mStep[0m  [24/84], [94mLoss[0m : 2.03810
[1mStep[0m  [32/84], [94mLoss[0m : 2.36015
[1mStep[0m  [40/84], [94mLoss[0m : 2.04729
[1mStep[0m  [48/84], [94mLoss[0m : 2.33349
[1mStep[0m  [56/84], [94mLoss[0m : 1.99219
[1mStep[0m  [64/84], [94mLoss[0m : 2.07328
[1mStep[0m  [72/84], [94mLoss[0m : 2.26460
[1mStep[0m  [80/84], [94mLoss[0m : 2.01735

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.096, [92mTest[0m: 2.572, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.92135
[1mStep[0m  [8/84], [94mLoss[0m : 1.91962
[1mStep[0m  [16/84], [94mLoss[0m : 1.96848
[1mStep[0m  [24/84], [94mLoss[0m : 2.48475
[1mStep[0m  [32/84], [94mLoss[0m : 1.84540
[1mStep[0m  [40/84], [94mLoss[0m : 1.95807
[1mStep[0m  [48/84], [94mLoss[0m : 2.05061
[1mStep[0m  [56/84], [94mLoss[0m : 2.23190
[1mStep[0m  [64/84], [94mLoss[0m : 1.97357
[1mStep[0m  [72/84], [94mLoss[0m : 1.98152
[1mStep[0m  [80/84], [94mLoss[0m : 2.15502

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.119, [92mTest[0m: 2.580, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.91385
[1mStep[0m  [8/84], [94mLoss[0m : 2.11751
[1mStep[0m  [16/84], [94mLoss[0m : 2.05654
[1mStep[0m  [24/84], [94mLoss[0m : 1.58556
[1mStep[0m  [32/84], [94mLoss[0m : 1.90820
[1mStep[0m  [40/84], [94mLoss[0m : 1.99859
[1mStep[0m  [48/84], [94mLoss[0m : 2.21807
[1mStep[0m  [56/84], [94mLoss[0m : 2.08610
[1mStep[0m  [64/84], [94mLoss[0m : 2.28060
[1mStep[0m  [72/84], [94mLoss[0m : 1.77469
[1mStep[0m  [80/84], [94mLoss[0m : 2.07159

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.036, [92mTest[0m: 2.538, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.82329
[1mStep[0m  [8/84], [94mLoss[0m : 1.95739
[1mStep[0m  [16/84], [94mLoss[0m : 2.15652
[1mStep[0m  [24/84], [94mLoss[0m : 1.68211
[1mStep[0m  [32/84], [94mLoss[0m : 1.78269
[1mStep[0m  [40/84], [94mLoss[0m : 2.41233
[1mStep[0m  [48/84], [94mLoss[0m : 2.14346
[1mStep[0m  [56/84], [94mLoss[0m : 1.75947
[1mStep[0m  [64/84], [94mLoss[0m : 2.13760
[1mStep[0m  [72/84], [94mLoss[0m : 2.18601
[1mStep[0m  [80/84], [94mLoss[0m : 1.91507

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.039, [92mTest[0m: 2.683, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.02979
[1mStep[0m  [8/84], [94mLoss[0m : 1.97757
[1mStep[0m  [16/84], [94mLoss[0m : 1.94481
[1mStep[0m  [24/84], [94mLoss[0m : 1.92904
[1mStep[0m  [32/84], [94mLoss[0m : 2.06539
[1mStep[0m  [40/84], [94mLoss[0m : 1.86602
[1mStep[0m  [48/84], [94mLoss[0m : 2.17993
[1mStep[0m  [56/84], [94mLoss[0m : 2.32704
[1mStep[0m  [64/84], [94mLoss[0m : 1.95142
[1mStep[0m  [72/84], [94mLoss[0m : 1.74808
[1mStep[0m  [80/84], [94mLoss[0m : 2.07666

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.998, [92mTest[0m: 2.478, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.02635
[1mStep[0m  [8/84], [94mLoss[0m : 1.90246
[1mStep[0m  [16/84], [94mLoss[0m : 1.88834
[1mStep[0m  [24/84], [94mLoss[0m : 1.74680
[1mStep[0m  [32/84], [94mLoss[0m : 2.06980
[1mStep[0m  [40/84], [94mLoss[0m : 1.86291
[1mStep[0m  [48/84], [94mLoss[0m : 1.98510
[1mStep[0m  [56/84], [94mLoss[0m : 1.83149
[1mStep[0m  [64/84], [94mLoss[0m : 1.98643
[1mStep[0m  [72/84], [94mLoss[0m : 1.91442
[1mStep[0m  [80/84], [94mLoss[0m : 2.11484

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.957, [92mTest[0m: 2.534, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.13913
[1mStep[0m  [8/84], [94mLoss[0m : 1.72966
[1mStep[0m  [16/84], [94mLoss[0m : 1.99315
[1mStep[0m  [24/84], [94mLoss[0m : 2.01277
[1mStep[0m  [32/84], [94mLoss[0m : 2.08887
[1mStep[0m  [40/84], [94mLoss[0m : 2.08505
[1mStep[0m  [48/84], [94mLoss[0m : 1.89294
[1mStep[0m  [56/84], [94mLoss[0m : 2.01322
[1mStep[0m  [64/84], [94mLoss[0m : 1.82761
[1mStep[0m  [72/84], [94mLoss[0m : 1.78288
[1mStep[0m  [80/84], [94mLoss[0m : 1.78385

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.927, [92mTest[0m: 2.455, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.95526
[1mStep[0m  [8/84], [94mLoss[0m : 1.82009
[1mStep[0m  [16/84], [94mLoss[0m : 1.86763
[1mStep[0m  [24/84], [94mLoss[0m : 1.94463
[1mStep[0m  [32/84], [94mLoss[0m : 1.97149
[1mStep[0m  [40/84], [94mLoss[0m : 1.88443
[1mStep[0m  [48/84], [94mLoss[0m : 2.01292
[1mStep[0m  [56/84], [94mLoss[0m : 1.84600
[1mStep[0m  [64/84], [94mLoss[0m : 1.88592
[1mStep[0m  [72/84], [94mLoss[0m : 2.03021
[1mStep[0m  [80/84], [94mLoss[0m : 2.03091

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.914, [92mTest[0m: 2.555, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.85580
[1mStep[0m  [8/84], [94mLoss[0m : 1.98821
[1mStep[0m  [16/84], [94mLoss[0m : 1.96153
[1mStep[0m  [24/84], [94mLoss[0m : 1.62680
[1mStep[0m  [32/84], [94mLoss[0m : 1.55208
[1mStep[0m  [40/84], [94mLoss[0m : 1.94410
[1mStep[0m  [48/84], [94mLoss[0m : 1.89459
[1mStep[0m  [56/84], [94mLoss[0m : 1.81631
[1mStep[0m  [64/84], [94mLoss[0m : 1.96963
[1mStep[0m  [72/84], [94mLoss[0m : 1.86204
[1mStep[0m  [80/84], [94mLoss[0m : 1.94054

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.874, [92mTest[0m: 2.517, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.00915
[1mStep[0m  [8/84], [94mLoss[0m : 1.82107
[1mStep[0m  [16/84], [94mLoss[0m : 1.91090
[1mStep[0m  [24/84], [94mLoss[0m : 1.60733
[1mStep[0m  [32/84], [94mLoss[0m : 1.63309
[1mStep[0m  [40/84], [94mLoss[0m : 1.92692
[1mStep[0m  [48/84], [94mLoss[0m : 1.85286
[1mStep[0m  [56/84], [94mLoss[0m : 2.14676
[1mStep[0m  [64/84], [94mLoss[0m : 1.79339
[1mStep[0m  [72/84], [94mLoss[0m : 1.75249
[1mStep[0m  [80/84], [94mLoss[0m : 2.01406

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.884, [92mTest[0m: 2.489, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.05970
[1mStep[0m  [8/84], [94mLoss[0m : 1.93502
[1mStep[0m  [16/84], [94mLoss[0m : 2.07699
[1mStep[0m  [24/84], [94mLoss[0m : 1.75886
[1mStep[0m  [32/84], [94mLoss[0m : 1.93497
[1mStep[0m  [40/84], [94mLoss[0m : 1.80346
[1mStep[0m  [48/84], [94mLoss[0m : 1.99111
[1mStep[0m  [56/84], [94mLoss[0m : 1.60750
[1mStep[0m  [64/84], [94mLoss[0m : 1.82146
[1mStep[0m  [72/84], [94mLoss[0m : 1.82091
[1mStep[0m  [80/84], [94mLoss[0m : 1.89682

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.845, [92mTest[0m: 2.551, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.77453
[1mStep[0m  [8/84], [94mLoss[0m : 1.75062
[1mStep[0m  [16/84], [94mLoss[0m : 1.81036
[1mStep[0m  [24/84], [94mLoss[0m : 1.81780
[1mStep[0m  [32/84], [94mLoss[0m : 1.65439
[1mStep[0m  [40/84], [94mLoss[0m : 1.82721
[1mStep[0m  [48/84], [94mLoss[0m : 1.72083
[1mStep[0m  [56/84], [94mLoss[0m : 1.92691
[1mStep[0m  [64/84], [94mLoss[0m : 1.89280
[1mStep[0m  [72/84], [94mLoss[0m : 1.80132
[1mStep[0m  [80/84], [94mLoss[0m : 1.95244

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.822, [92mTest[0m: 2.669, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.73515
[1mStep[0m  [8/84], [94mLoss[0m : 1.52322
[1mStep[0m  [16/84], [94mLoss[0m : 1.84780
[1mStep[0m  [24/84], [94mLoss[0m : 1.96427
[1mStep[0m  [32/84], [94mLoss[0m : 1.90819
[1mStep[0m  [40/84], [94mLoss[0m : 1.82265
[1mStep[0m  [48/84], [94mLoss[0m : 1.91188
[1mStep[0m  [56/84], [94mLoss[0m : 1.96232
[1mStep[0m  [64/84], [94mLoss[0m : 1.81814
[1mStep[0m  [72/84], [94mLoss[0m : 1.99453
[1mStep[0m  [80/84], [94mLoss[0m : 1.78874

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.816, [92mTest[0m: 2.666, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.81156
[1mStep[0m  [8/84], [94mLoss[0m : 1.89567
[1mStep[0m  [16/84], [94mLoss[0m : 1.65667
[1mStep[0m  [24/84], [94mLoss[0m : 1.71573
[1mStep[0m  [32/84], [94mLoss[0m : 1.53637
[1mStep[0m  [40/84], [94mLoss[0m : 1.75017
[1mStep[0m  [48/84], [94mLoss[0m : 1.93789
[1mStep[0m  [56/84], [94mLoss[0m : 1.64947
[1mStep[0m  [64/84], [94mLoss[0m : 1.67469
[1mStep[0m  [72/84], [94mLoss[0m : 2.13035
[1mStep[0m  [80/84], [94mLoss[0m : 1.91846

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.781, [92mTest[0m: 2.563, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.68864
[1mStep[0m  [8/84], [94mLoss[0m : 1.94219
[1mStep[0m  [16/84], [94mLoss[0m : 2.12545
[1mStep[0m  [24/84], [94mLoss[0m : 1.74670
[1mStep[0m  [32/84], [94mLoss[0m : 1.95523
[1mStep[0m  [40/84], [94mLoss[0m : 1.66980
[1mStep[0m  [48/84], [94mLoss[0m : 1.81379
[1mStep[0m  [56/84], [94mLoss[0m : 1.78281
[1mStep[0m  [64/84], [94mLoss[0m : 1.93343
[1mStep[0m  [72/84], [94mLoss[0m : 1.92270
[1mStep[0m  [80/84], [94mLoss[0m : 1.93658

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.768, [92mTest[0m: 2.572, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.622
====================================

Phase 2 - Evaluation MAE:  2.622489188398634
MAE score P1       2.330378
MAE score P2       2.622489
loss               1.768056
learning_rate      0.007525
batch_size              128
hidden_sizes      [250, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.1
weight_decay          0.001
Name: 0, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 11.04815
[1mStep[0m  [8/84], [94mLoss[0m : 7.59594
[1mStep[0m  [16/84], [94mLoss[0m : 2.89774
[1mStep[0m  [24/84], [94mLoss[0m : 3.59808
[1mStep[0m  [32/84], [94mLoss[0m : 3.03120
[1mStep[0m  [40/84], [94mLoss[0m : 2.76296
[1mStep[0m  [48/84], [94mLoss[0m : 3.01964
[1mStep[0m  [56/84], [94mLoss[0m : 3.04647
[1mStep[0m  [64/84], [94mLoss[0m : 2.48253
[1mStep[0m  [72/84], [94mLoss[0m : 2.83852
[1mStep[0m  [80/84], [94mLoss[0m : 2.64729

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.829, [92mTest[0m: 10.952, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.85975
[1mStep[0m  [8/84], [94mLoss[0m : 3.03446
[1mStep[0m  [16/84], [94mLoss[0m : 2.58848
[1mStep[0m  [24/84], [94mLoss[0m : 2.49280
[1mStep[0m  [32/84], [94mLoss[0m : 2.81090
[1mStep[0m  [40/84], [94mLoss[0m : 2.47730
[1mStep[0m  [48/84], [94mLoss[0m : 2.66753
[1mStep[0m  [56/84], [94mLoss[0m : 2.84087
[1mStep[0m  [64/84], [94mLoss[0m : 2.47505
[1mStep[0m  [72/84], [94mLoss[0m : 2.51308
[1mStep[0m  [80/84], [94mLoss[0m : 2.84239

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.634, [92mTest[0m: 2.565, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.69998
[1mStep[0m  [8/84], [94mLoss[0m : 2.49661
[1mStep[0m  [16/84], [94mLoss[0m : 2.43076
[1mStep[0m  [24/84], [94mLoss[0m : 2.79246
[1mStep[0m  [32/84], [94mLoss[0m : 2.85717
[1mStep[0m  [40/84], [94mLoss[0m : 2.49995
[1mStep[0m  [48/84], [94mLoss[0m : 2.81240
[1mStep[0m  [56/84], [94mLoss[0m : 2.62486
[1mStep[0m  [64/84], [94mLoss[0m : 2.47725
[1mStep[0m  [72/84], [94mLoss[0m : 2.65787
[1mStep[0m  [80/84], [94mLoss[0m : 2.20623

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.588, [92mTest[0m: 2.401, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.74398
[1mStep[0m  [8/84], [94mLoss[0m : 2.37157
[1mStep[0m  [16/84], [94mLoss[0m : 2.22773
[1mStep[0m  [24/84], [94mLoss[0m : 2.60936
[1mStep[0m  [32/84], [94mLoss[0m : 2.55371
[1mStep[0m  [40/84], [94mLoss[0m : 2.23707
[1mStep[0m  [48/84], [94mLoss[0m : 2.50560
[1mStep[0m  [56/84], [94mLoss[0m : 2.35066
[1mStep[0m  [64/84], [94mLoss[0m : 2.35783
[1mStep[0m  [72/84], [94mLoss[0m : 2.39650
[1mStep[0m  [80/84], [94mLoss[0m : 2.33298

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.536, [92mTest[0m: 2.406, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.61319
[1mStep[0m  [8/84], [94mLoss[0m : 2.48755
[1mStep[0m  [16/84], [94mLoss[0m : 2.49053
[1mStep[0m  [24/84], [94mLoss[0m : 2.65018
[1mStep[0m  [32/84], [94mLoss[0m : 2.35641
[1mStep[0m  [40/84], [94mLoss[0m : 2.34241
[1mStep[0m  [48/84], [94mLoss[0m : 2.58361
[1mStep[0m  [56/84], [94mLoss[0m : 2.51106
[1mStep[0m  [64/84], [94mLoss[0m : 2.34113
[1mStep[0m  [72/84], [94mLoss[0m : 2.79184
[1mStep[0m  [80/84], [94mLoss[0m : 2.58165

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.519, [92mTest[0m: 2.384, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60293
[1mStep[0m  [8/84], [94mLoss[0m : 2.18919
[1mStep[0m  [16/84], [94mLoss[0m : 2.41954
[1mStep[0m  [24/84], [94mLoss[0m : 2.61850
[1mStep[0m  [32/84], [94mLoss[0m : 2.34823
[1mStep[0m  [40/84], [94mLoss[0m : 2.35326
[1mStep[0m  [48/84], [94mLoss[0m : 2.37695
[1mStep[0m  [56/84], [94mLoss[0m : 2.69294
[1mStep[0m  [64/84], [94mLoss[0m : 2.57371
[1mStep[0m  [72/84], [94mLoss[0m : 2.41522
[1mStep[0m  [80/84], [94mLoss[0m : 2.57410

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.392, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.73818
[1mStep[0m  [8/84], [94mLoss[0m : 2.25936
[1mStep[0m  [16/84], [94mLoss[0m : 2.54348
[1mStep[0m  [24/84], [94mLoss[0m : 2.84733
[1mStep[0m  [32/84], [94mLoss[0m : 2.35035
[1mStep[0m  [40/84], [94mLoss[0m : 2.52190
[1mStep[0m  [48/84], [94mLoss[0m : 2.21062
[1mStep[0m  [56/84], [94mLoss[0m : 2.64941
[1mStep[0m  [64/84], [94mLoss[0m : 2.33969
[1mStep[0m  [72/84], [94mLoss[0m : 2.24050
[1mStep[0m  [80/84], [94mLoss[0m : 2.75245

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.367, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53748
[1mStep[0m  [8/84], [94mLoss[0m : 2.59472
[1mStep[0m  [16/84], [94mLoss[0m : 2.39997
[1mStep[0m  [24/84], [94mLoss[0m : 2.55864
[1mStep[0m  [32/84], [94mLoss[0m : 2.46000
[1mStep[0m  [40/84], [94mLoss[0m : 2.52818
[1mStep[0m  [48/84], [94mLoss[0m : 2.35423
[1mStep[0m  [56/84], [94mLoss[0m : 2.54911
[1mStep[0m  [64/84], [94mLoss[0m : 2.53265
[1mStep[0m  [72/84], [94mLoss[0m : 2.43330
[1mStep[0m  [80/84], [94mLoss[0m : 2.10288

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.350, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.24417
[1mStep[0m  [8/84], [94mLoss[0m : 2.27591
[1mStep[0m  [16/84], [94mLoss[0m : 2.38215
[1mStep[0m  [24/84], [94mLoss[0m : 2.12647
[1mStep[0m  [32/84], [94mLoss[0m : 2.41382
[1mStep[0m  [40/84], [94mLoss[0m : 2.63287
[1mStep[0m  [48/84], [94mLoss[0m : 2.33624
[1mStep[0m  [56/84], [94mLoss[0m : 2.47527
[1mStep[0m  [64/84], [94mLoss[0m : 2.22866
[1mStep[0m  [72/84], [94mLoss[0m : 2.21824
[1mStep[0m  [80/84], [94mLoss[0m : 2.41093

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.343, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.12044
[1mStep[0m  [8/84], [94mLoss[0m : 2.42858
[1mStep[0m  [16/84], [94mLoss[0m : 2.34784
[1mStep[0m  [24/84], [94mLoss[0m : 2.33326
[1mStep[0m  [32/84], [94mLoss[0m : 2.52572
[1mStep[0m  [40/84], [94mLoss[0m : 2.64227
[1mStep[0m  [48/84], [94mLoss[0m : 2.56918
[1mStep[0m  [56/84], [94mLoss[0m : 2.48822
[1mStep[0m  [64/84], [94mLoss[0m : 2.68330
[1mStep[0m  [72/84], [94mLoss[0m : 2.53588
[1mStep[0m  [80/84], [94mLoss[0m : 2.20679

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.374, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.09604
[1mStep[0m  [8/84], [94mLoss[0m : 2.32273
[1mStep[0m  [16/84], [94mLoss[0m : 2.02480
[1mStep[0m  [24/84], [94mLoss[0m : 2.47632
[1mStep[0m  [32/84], [94mLoss[0m : 2.40088
[1mStep[0m  [40/84], [94mLoss[0m : 2.49273
[1mStep[0m  [48/84], [94mLoss[0m : 2.34775
[1mStep[0m  [56/84], [94mLoss[0m : 2.63629
[1mStep[0m  [64/84], [94mLoss[0m : 2.61127
[1mStep[0m  [72/84], [94mLoss[0m : 2.20740
[1mStep[0m  [80/84], [94mLoss[0m : 2.22330

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.390, [92mTest[0m: 2.353, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.19364
[1mStep[0m  [8/84], [94mLoss[0m : 2.59206
[1mStep[0m  [16/84], [94mLoss[0m : 2.35400
[1mStep[0m  [24/84], [94mLoss[0m : 2.03861
[1mStep[0m  [32/84], [94mLoss[0m : 2.39759
[1mStep[0m  [40/84], [94mLoss[0m : 2.30985
[1mStep[0m  [48/84], [94mLoss[0m : 2.26948
[1mStep[0m  [56/84], [94mLoss[0m : 2.38526
[1mStep[0m  [64/84], [94mLoss[0m : 2.25161
[1mStep[0m  [72/84], [94mLoss[0m : 2.34925
[1mStep[0m  [80/84], [94mLoss[0m : 2.56459

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.373, [92mTest[0m: 2.330, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37011
[1mStep[0m  [8/84], [94mLoss[0m : 2.60540
[1mStep[0m  [16/84], [94mLoss[0m : 2.23525
[1mStep[0m  [24/84], [94mLoss[0m : 2.12577
[1mStep[0m  [32/84], [94mLoss[0m : 2.34426
[1mStep[0m  [40/84], [94mLoss[0m : 2.26603
[1mStep[0m  [48/84], [94mLoss[0m : 2.32198
[1mStep[0m  [56/84], [94mLoss[0m : 2.71783
[1mStep[0m  [64/84], [94mLoss[0m : 2.36106
[1mStep[0m  [72/84], [94mLoss[0m : 2.26582
[1mStep[0m  [80/84], [94mLoss[0m : 2.53096

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.368, [92mTest[0m: 2.335, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30960
[1mStep[0m  [8/84], [94mLoss[0m : 2.14408
[1mStep[0m  [16/84], [94mLoss[0m : 2.26112
[1mStep[0m  [24/84], [94mLoss[0m : 2.50146
[1mStep[0m  [32/84], [94mLoss[0m : 2.09880
[1mStep[0m  [40/84], [94mLoss[0m : 2.58746
[1mStep[0m  [48/84], [94mLoss[0m : 2.44208
[1mStep[0m  [56/84], [94mLoss[0m : 2.52176
[1mStep[0m  [64/84], [94mLoss[0m : 2.42585
[1mStep[0m  [72/84], [94mLoss[0m : 2.55387
[1mStep[0m  [80/84], [94mLoss[0m : 2.35493

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.372, [92mTest[0m: 2.324, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.68903
[1mStep[0m  [8/84], [94mLoss[0m : 2.59003
[1mStep[0m  [16/84], [94mLoss[0m : 2.50015
[1mStep[0m  [24/84], [94mLoss[0m : 2.33406
[1mStep[0m  [32/84], [94mLoss[0m : 2.28939
[1mStep[0m  [40/84], [94mLoss[0m : 2.41165
[1mStep[0m  [48/84], [94mLoss[0m : 2.35256
[1mStep[0m  [56/84], [94mLoss[0m : 2.56057
[1mStep[0m  [64/84], [94mLoss[0m : 2.43784
[1mStep[0m  [72/84], [94mLoss[0m : 2.51220
[1mStep[0m  [80/84], [94mLoss[0m : 2.39589

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.377, [92mTest[0m: 2.337, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44000
[1mStep[0m  [8/84], [94mLoss[0m : 2.34768
[1mStep[0m  [16/84], [94mLoss[0m : 2.11873
[1mStep[0m  [24/84], [94mLoss[0m : 2.03216
[1mStep[0m  [32/84], [94mLoss[0m : 2.37232
[1mStep[0m  [40/84], [94mLoss[0m : 2.29457
[1mStep[0m  [48/84], [94mLoss[0m : 2.24260
[1mStep[0m  [56/84], [94mLoss[0m : 2.39096
[1mStep[0m  [64/84], [94mLoss[0m : 2.49251
[1mStep[0m  [72/84], [94mLoss[0m : 2.14779
[1mStep[0m  [80/84], [94mLoss[0m : 2.39353

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.347, [92mTest[0m: 2.322, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.25663
[1mStep[0m  [8/84], [94mLoss[0m : 2.31221
[1mStep[0m  [16/84], [94mLoss[0m : 2.35086
[1mStep[0m  [24/84], [94mLoss[0m : 2.22309
[1mStep[0m  [32/84], [94mLoss[0m : 2.45903
[1mStep[0m  [40/84], [94mLoss[0m : 2.27812
[1mStep[0m  [48/84], [94mLoss[0m : 2.14851
[1mStep[0m  [56/84], [94mLoss[0m : 2.30996
[1mStep[0m  [64/84], [94mLoss[0m : 2.32570
[1mStep[0m  [72/84], [94mLoss[0m : 2.15945
[1mStep[0m  [80/84], [94mLoss[0m : 2.09490

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.332, [92mTest[0m: 2.335, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.11720
[1mStep[0m  [8/84], [94mLoss[0m : 2.42581
[1mStep[0m  [16/84], [94mLoss[0m : 2.41217
[1mStep[0m  [24/84], [94mLoss[0m : 2.20412
[1mStep[0m  [32/84], [94mLoss[0m : 2.15635
[1mStep[0m  [40/84], [94mLoss[0m : 2.39438
[1mStep[0m  [48/84], [94mLoss[0m : 2.23200
[1mStep[0m  [56/84], [94mLoss[0m : 2.08947
[1mStep[0m  [64/84], [94mLoss[0m : 2.18730
[1mStep[0m  [72/84], [94mLoss[0m : 2.56177
[1mStep[0m  [80/84], [94mLoss[0m : 2.58938

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.335, [92mTest[0m: 2.313, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.20437
[1mStep[0m  [8/84], [94mLoss[0m : 2.62956
[1mStep[0m  [16/84], [94mLoss[0m : 2.10822
[1mStep[0m  [24/84], [94mLoss[0m : 2.29435
[1mStep[0m  [32/84], [94mLoss[0m : 2.63222
[1mStep[0m  [40/84], [94mLoss[0m : 2.23649
[1mStep[0m  [48/84], [94mLoss[0m : 2.13846
[1mStep[0m  [56/84], [94mLoss[0m : 2.21889
[1mStep[0m  [64/84], [94mLoss[0m : 2.15882
[1mStep[0m  [72/84], [94mLoss[0m : 2.17648
[1mStep[0m  [80/84], [94mLoss[0m : 2.64248

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.317, [92mTest[0m: 2.306, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.95738
[1mStep[0m  [8/84], [94mLoss[0m : 2.43801
[1mStep[0m  [16/84], [94mLoss[0m : 2.05862
[1mStep[0m  [24/84], [94mLoss[0m : 1.99681
[1mStep[0m  [32/84], [94mLoss[0m : 2.39340
[1mStep[0m  [40/84], [94mLoss[0m : 2.39843
[1mStep[0m  [48/84], [94mLoss[0m : 2.31934
[1mStep[0m  [56/84], [94mLoss[0m : 2.25555
[1mStep[0m  [64/84], [94mLoss[0m : 2.27399
[1mStep[0m  [72/84], [94mLoss[0m : 2.49139
[1mStep[0m  [80/84], [94mLoss[0m : 2.10894

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.307, [92mTest[0m: 2.340, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42894
[1mStep[0m  [8/84], [94mLoss[0m : 2.02371
[1mStep[0m  [16/84], [94mLoss[0m : 2.42992
[1mStep[0m  [24/84], [94mLoss[0m : 2.70267
[1mStep[0m  [32/84], [94mLoss[0m : 2.27877
[1mStep[0m  [40/84], [94mLoss[0m : 2.20783
[1mStep[0m  [48/84], [94mLoss[0m : 2.34907
[1mStep[0m  [56/84], [94mLoss[0m : 2.10231
[1mStep[0m  [64/84], [94mLoss[0m : 2.18778
[1mStep[0m  [72/84], [94mLoss[0m : 2.37244
[1mStep[0m  [80/84], [94mLoss[0m : 2.12861

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.290, [92mTest[0m: 2.338, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.97455
[1mStep[0m  [8/84], [94mLoss[0m : 2.24590
[1mStep[0m  [16/84], [94mLoss[0m : 2.25075
[1mStep[0m  [24/84], [94mLoss[0m : 2.42121
[1mStep[0m  [32/84], [94mLoss[0m : 1.94223
[1mStep[0m  [40/84], [94mLoss[0m : 2.20018
[1mStep[0m  [48/84], [94mLoss[0m : 2.12353
[1mStep[0m  [56/84], [94mLoss[0m : 2.30537
[1mStep[0m  [64/84], [94mLoss[0m : 2.31783
[1mStep[0m  [72/84], [94mLoss[0m : 2.49569
[1mStep[0m  [80/84], [94mLoss[0m : 2.30093

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.296, [92mTest[0m: 2.311, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27524
[1mStep[0m  [8/84], [94mLoss[0m : 2.35314
[1mStep[0m  [16/84], [94mLoss[0m : 2.41035
[1mStep[0m  [24/84], [94mLoss[0m : 2.23801
[1mStep[0m  [32/84], [94mLoss[0m : 2.30297
[1mStep[0m  [40/84], [94mLoss[0m : 2.45674
[1mStep[0m  [48/84], [94mLoss[0m : 2.42112
[1mStep[0m  [56/84], [94mLoss[0m : 2.25754
[1mStep[0m  [64/84], [94mLoss[0m : 2.32831
[1mStep[0m  [72/84], [94mLoss[0m : 2.47330
[1mStep[0m  [80/84], [94mLoss[0m : 2.08718

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.299, [92mTest[0m: 2.326, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30458
[1mStep[0m  [8/84], [94mLoss[0m : 2.29972
[1mStep[0m  [16/84], [94mLoss[0m : 2.10347
[1mStep[0m  [24/84], [94mLoss[0m : 1.90350
[1mStep[0m  [32/84], [94mLoss[0m : 2.41335
[1mStep[0m  [40/84], [94mLoss[0m : 2.02787
[1mStep[0m  [48/84], [94mLoss[0m : 2.22026
[1mStep[0m  [56/84], [94mLoss[0m : 2.06731
[1mStep[0m  [64/84], [94mLoss[0m : 2.30371
[1mStep[0m  [72/84], [94mLoss[0m : 2.55500
[1mStep[0m  [80/84], [94mLoss[0m : 2.44011

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.287, [92mTest[0m: 2.317, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44630
[1mStep[0m  [8/84], [94mLoss[0m : 2.19307
[1mStep[0m  [16/84], [94mLoss[0m : 2.11286
[1mStep[0m  [24/84], [94mLoss[0m : 2.25117
[1mStep[0m  [32/84], [94mLoss[0m : 2.34944
[1mStep[0m  [40/84], [94mLoss[0m : 1.97337
[1mStep[0m  [48/84], [94mLoss[0m : 2.37789
[1mStep[0m  [56/84], [94mLoss[0m : 2.27746
[1mStep[0m  [64/84], [94mLoss[0m : 2.36982
[1mStep[0m  [72/84], [94mLoss[0m : 2.08859
[1mStep[0m  [80/84], [94mLoss[0m : 2.48189

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.278, [92mTest[0m: 2.320, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.22464
[1mStep[0m  [8/84], [94mLoss[0m : 2.33590
[1mStep[0m  [16/84], [94mLoss[0m : 2.25983
[1mStep[0m  [24/84], [94mLoss[0m : 2.28263
[1mStep[0m  [32/84], [94mLoss[0m : 2.29492
[1mStep[0m  [40/84], [94mLoss[0m : 2.58942
[1mStep[0m  [48/84], [94mLoss[0m : 2.29103
[1mStep[0m  [56/84], [94mLoss[0m : 2.35516
[1mStep[0m  [64/84], [94mLoss[0m : 2.40174
[1mStep[0m  [72/84], [94mLoss[0m : 2.40228
[1mStep[0m  [80/84], [94mLoss[0m : 2.45753

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.288, [92mTest[0m: 2.326, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.10223
[1mStep[0m  [8/84], [94mLoss[0m : 2.29545
[1mStep[0m  [16/84], [94mLoss[0m : 2.17139
[1mStep[0m  [24/84], [94mLoss[0m : 2.15941
[1mStep[0m  [32/84], [94mLoss[0m : 2.25755
[1mStep[0m  [40/84], [94mLoss[0m : 2.23131
[1mStep[0m  [48/84], [94mLoss[0m : 2.21595
[1mStep[0m  [56/84], [94mLoss[0m : 1.98227
[1mStep[0m  [64/84], [94mLoss[0m : 2.31048
[1mStep[0m  [72/84], [94mLoss[0m : 2.26636
[1mStep[0m  [80/84], [94mLoss[0m : 2.24860

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.253, [92mTest[0m: 2.319, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60361
[1mStep[0m  [8/84], [94mLoss[0m : 2.28272
[1mStep[0m  [16/84], [94mLoss[0m : 2.30897
[1mStep[0m  [24/84], [94mLoss[0m : 2.05989
[1mStep[0m  [32/84], [94mLoss[0m : 2.11078
[1mStep[0m  [40/84], [94mLoss[0m : 2.37914
[1mStep[0m  [48/84], [94mLoss[0m : 2.40110
[1mStep[0m  [56/84], [94mLoss[0m : 1.74302
[1mStep[0m  [64/84], [94mLoss[0m : 2.14731
[1mStep[0m  [72/84], [94mLoss[0m : 2.16610
[1mStep[0m  [80/84], [94mLoss[0m : 2.09615

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.253, [92mTest[0m: 2.329, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30356
[1mStep[0m  [8/84], [94mLoss[0m : 2.06879
[1mStep[0m  [16/84], [94mLoss[0m : 2.60638
[1mStep[0m  [24/84], [94mLoss[0m : 2.11743
[1mStep[0m  [32/84], [94mLoss[0m : 2.01547
[1mStep[0m  [40/84], [94mLoss[0m : 2.47197
[1mStep[0m  [48/84], [94mLoss[0m : 2.39488
[1mStep[0m  [56/84], [94mLoss[0m : 2.21605
[1mStep[0m  [64/84], [94mLoss[0m : 2.17059
[1mStep[0m  [72/84], [94mLoss[0m : 2.21797
[1mStep[0m  [80/84], [94mLoss[0m : 2.10892

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.269, [92mTest[0m: 2.348, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30816
[1mStep[0m  [8/84], [94mLoss[0m : 2.24629
[1mStep[0m  [16/84], [94mLoss[0m : 2.10378
[1mStep[0m  [24/84], [94mLoss[0m : 2.30151
[1mStep[0m  [32/84], [94mLoss[0m : 2.38172
[1mStep[0m  [40/84], [94mLoss[0m : 2.35304
[1mStep[0m  [48/84], [94mLoss[0m : 2.25320
[1mStep[0m  [56/84], [94mLoss[0m : 2.19639
[1mStep[0m  [64/84], [94mLoss[0m : 2.49988
[1mStep[0m  [72/84], [94mLoss[0m : 2.23192
[1mStep[0m  [80/84], [94mLoss[0m : 2.16243

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.257, [92mTest[0m: 2.304, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.325
====================================

Phase 1 - Evaluation MAE:  2.325331994465419
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 2.18750
[1mStep[0m  [8/84], [94mLoss[0m : 2.33759
[1mStep[0m  [16/84], [94mLoss[0m : 2.45554
[1mStep[0m  [24/84], [94mLoss[0m : 2.56161
[1mStep[0m  [32/84], [94mLoss[0m : 2.44135
[1mStep[0m  [40/84], [94mLoss[0m : 2.43053
[1mStep[0m  [48/84], [94mLoss[0m : 2.68573
[1mStep[0m  [56/84], [94mLoss[0m : 2.67205
[1mStep[0m  [64/84], [94mLoss[0m : 2.45356
[1mStep[0m  [72/84], [94mLoss[0m : 2.46281
[1mStep[0m  [80/84], [94mLoss[0m : 2.23009

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.332, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.18285
[1mStep[0m  [8/84], [94mLoss[0m : 2.29726
[1mStep[0m  [16/84], [94mLoss[0m : 2.31819
[1mStep[0m  [24/84], [94mLoss[0m : 2.44527
[1mStep[0m  [32/84], [94mLoss[0m : 2.39393
[1mStep[0m  [40/84], [94mLoss[0m : 2.21365
[1mStep[0m  [48/84], [94mLoss[0m : 2.48244
[1mStep[0m  [56/84], [94mLoss[0m : 2.11572
[1mStep[0m  [64/84], [94mLoss[0m : 2.24742
[1mStep[0m  [72/84], [94mLoss[0m : 2.40907
[1mStep[0m  [80/84], [94mLoss[0m : 2.22679

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.321, [92mTest[0m: 2.385, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.83956
[1mStep[0m  [8/84], [94mLoss[0m : 2.24865
[1mStep[0m  [16/84], [94mLoss[0m : 2.11988
[1mStep[0m  [24/84], [94mLoss[0m : 1.89145
[1mStep[0m  [32/84], [94mLoss[0m : 2.20128
[1mStep[0m  [40/84], [94mLoss[0m : 2.33193
[1mStep[0m  [48/84], [94mLoss[0m : 2.21274
[1mStep[0m  [56/84], [94mLoss[0m : 2.33481
[1mStep[0m  [64/84], [94mLoss[0m : 2.04691
[1mStep[0m  [72/84], [94mLoss[0m : 2.03794
[1mStep[0m  [80/84], [94mLoss[0m : 2.02999

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.219, [92mTest[0m: 2.354, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.24623
[1mStep[0m  [8/84], [94mLoss[0m : 2.12726
[1mStep[0m  [16/84], [94mLoss[0m : 2.17419
[1mStep[0m  [24/84], [94mLoss[0m : 2.35050
[1mStep[0m  [32/84], [94mLoss[0m : 2.25133
[1mStep[0m  [40/84], [94mLoss[0m : 2.43996
[1mStep[0m  [48/84], [94mLoss[0m : 2.27639
[1mStep[0m  [56/84], [94mLoss[0m : 2.25838
[1mStep[0m  [64/84], [94mLoss[0m : 2.31128
[1mStep[0m  [72/84], [94mLoss[0m : 2.11908
[1mStep[0m  [80/84], [94mLoss[0m : 2.07670

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.140, [92mTest[0m: 2.372, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.96074
[1mStep[0m  [8/84], [94mLoss[0m : 1.88135
[1mStep[0m  [16/84], [94mLoss[0m : 2.00181
[1mStep[0m  [24/84], [94mLoss[0m : 2.16013
[1mStep[0m  [32/84], [94mLoss[0m : 2.07618
[1mStep[0m  [40/84], [94mLoss[0m : 1.93099
[1mStep[0m  [48/84], [94mLoss[0m : 2.18346
[1mStep[0m  [56/84], [94mLoss[0m : 1.98747
[1mStep[0m  [64/84], [94mLoss[0m : 2.14021
[1mStep[0m  [72/84], [94mLoss[0m : 2.13227
[1mStep[0m  [80/84], [94mLoss[0m : 2.10487

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.064, [92mTest[0m: 2.396, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.19545
[1mStep[0m  [8/84], [94mLoss[0m : 2.33352
[1mStep[0m  [16/84], [94mLoss[0m : 2.22171
[1mStep[0m  [24/84], [94mLoss[0m : 1.83057
[1mStep[0m  [32/84], [94mLoss[0m : 2.02923
[1mStep[0m  [40/84], [94mLoss[0m : 1.97017
[1mStep[0m  [48/84], [94mLoss[0m : 2.10615
[1mStep[0m  [56/84], [94mLoss[0m : 2.28732
[1mStep[0m  [64/84], [94mLoss[0m : 1.88118
[1mStep[0m  [72/84], [94mLoss[0m : 1.99774
[1mStep[0m  [80/84], [94mLoss[0m : 1.85386

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.008, [92mTest[0m: 2.408, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.11864
[1mStep[0m  [8/84], [94mLoss[0m : 1.97418
[1mStep[0m  [16/84], [94mLoss[0m : 1.77594
[1mStep[0m  [24/84], [94mLoss[0m : 2.18115
[1mStep[0m  [32/84], [94mLoss[0m : 2.15925
[1mStep[0m  [40/84], [94mLoss[0m : 2.06330
[1mStep[0m  [48/84], [94mLoss[0m : 1.80249
[1mStep[0m  [56/84], [94mLoss[0m : 1.95359
[1mStep[0m  [64/84], [94mLoss[0m : 1.81049
[1mStep[0m  [72/84], [94mLoss[0m : 2.11716
[1mStep[0m  [80/84], [94mLoss[0m : 2.05913

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.956, [92mTest[0m: 2.387, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.93013
[1mStep[0m  [8/84], [94mLoss[0m : 1.74469
[1mStep[0m  [16/84], [94mLoss[0m : 1.85040
[1mStep[0m  [24/84], [94mLoss[0m : 2.06485
[1mStep[0m  [32/84], [94mLoss[0m : 1.86802
[1mStep[0m  [40/84], [94mLoss[0m : 1.84754
[1mStep[0m  [48/84], [94mLoss[0m : 1.82325
[1mStep[0m  [56/84], [94mLoss[0m : 1.84379
[1mStep[0m  [64/84], [94mLoss[0m : 1.90854
[1mStep[0m  [72/84], [94mLoss[0m : 1.61339
[1mStep[0m  [80/84], [94mLoss[0m : 2.00548

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.901, [92mTest[0m: 2.479, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.77286
[1mStep[0m  [8/84], [94mLoss[0m : 1.88602
[1mStep[0m  [16/84], [94mLoss[0m : 2.13026
[1mStep[0m  [24/84], [94mLoss[0m : 1.81652
[1mStep[0m  [32/84], [94mLoss[0m : 2.12225
[1mStep[0m  [40/84], [94mLoss[0m : 2.00425
[1mStep[0m  [48/84], [94mLoss[0m : 2.02627
[1mStep[0m  [56/84], [94mLoss[0m : 1.97111
[1mStep[0m  [64/84], [94mLoss[0m : 1.79890
[1mStep[0m  [72/84], [94mLoss[0m : 1.90757
[1mStep[0m  [80/84], [94mLoss[0m : 2.07654

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.832, [92mTest[0m: 2.469, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.80046
[1mStep[0m  [8/84], [94mLoss[0m : 1.64785
[1mStep[0m  [16/84], [94mLoss[0m : 1.52744
[1mStep[0m  [24/84], [94mLoss[0m : 1.83257
[1mStep[0m  [32/84], [94mLoss[0m : 1.79635
[1mStep[0m  [40/84], [94mLoss[0m : 1.85158
[1mStep[0m  [48/84], [94mLoss[0m : 1.86853
[1mStep[0m  [56/84], [94mLoss[0m : 1.86984
[1mStep[0m  [64/84], [94mLoss[0m : 1.67703
[1mStep[0m  [72/84], [94mLoss[0m : 1.75094
[1mStep[0m  [80/84], [94mLoss[0m : 1.80211

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.759, [92mTest[0m: 2.435, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.69381
[1mStep[0m  [8/84], [94mLoss[0m : 1.66214
[1mStep[0m  [16/84], [94mLoss[0m : 1.41379
[1mStep[0m  [24/84], [94mLoss[0m : 1.80738
[1mStep[0m  [32/84], [94mLoss[0m : 1.56825
[1mStep[0m  [40/84], [94mLoss[0m : 1.61303
[1mStep[0m  [48/84], [94mLoss[0m : 1.95712
[1mStep[0m  [56/84], [94mLoss[0m : 1.68721
[1mStep[0m  [64/84], [94mLoss[0m : 1.59911
[1mStep[0m  [72/84], [94mLoss[0m : 1.80598
[1mStep[0m  [80/84], [94mLoss[0m : 1.96762

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.707, [92mTest[0m: 2.500, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.49039
[1mStep[0m  [8/84], [94mLoss[0m : 1.70082
[1mStep[0m  [16/84], [94mLoss[0m : 1.72189
[1mStep[0m  [24/84], [94mLoss[0m : 1.70494
[1mStep[0m  [32/84], [94mLoss[0m : 1.71514
[1mStep[0m  [40/84], [94mLoss[0m : 1.53143
[1mStep[0m  [48/84], [94mLoss[0m : 1.65045
[1mStep[0m  [56/84], [94mLoss[0m : 1.69914
[1mStep[0m  [64/84], [94mLoss[0m : 1.68185
[1mStep[0m  [72/84], [94mLoss[0m : 1.87510
[1mStep[0m  [80/84], [94mLoss[0m : 1.81716

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.663, [92mTest[0m: 2.452, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.59162
[1mStep[0m  [8/84], [94mLoss[0m : 1.37287
[1mStep[0m  [16/84], [94mLoss[0m : 1.36256
[1mStep[0m  [24/84], [94mLoss[0m : 1.51996
[1mStep[0m  [32/84], [94mLoss[0m : 1.72989
[1mStep[0m  [40/84], [94mLoss[0m : 1.69138
[1mStep[0m  [48/84], [94mLoss[0m : 1.57003
[1mStep[0m  [56/84], [94mLoss[0m : 1.62422
[1mStep[0m  [64/84], [94mLoss[0m : 1.35675
[1mStep[0m  [72/84], [94mLoss[0m : 1.79706
[1mStep[0m  [80/84], [94mLoss[0m : 1.70147

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.613, [92mTest[0m: 2.522, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.53232
[1mStep[0m  [8/84], [94mLoss[0m : 1.69849
[1mStep[0m  [16/84], [94mLoss[0m : 1.63121
[1mStep[0m  [24/84], [94mLoss[0m : 1.59648
[1mStep[0m  [32/84], [94mLoss[0m : 1.82823
[1mStep[0m  [40/84], [94mLoss[0m : 1.71998
[1mStep[0m  [48/84], [94mLoss[0m : 1.52284
[1mStep[0m  [56/84], [94mLoss[0m : 1.69540
[1mStep[0m  [64/84], [94mLoss[0m : 1.62858
[1mStep[0m  [72/84], [94mLoss[0m : 1.37977
[1mStep[0m  [80/84], [94mLoss[0m : 1.76599

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.613, [92mTest[0m: 2.537, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.44116
[1mStep[0m  [8/84], [94mLoss[0m : 1.68463
[1mStep[0m  [16/84], [94mLoss[0m : 1.46014
[1mStep[0m  [24/84], [94mLoss[0m : 1.66306
[1mStep[0m  [32/84], [94mLoss[0m : 1.77267
[1mStep[0m  [40/84], [94mLoss[0m : 1.68703
[1mStep[0m  [48/84], [94mLoss[0m : 1.39984
[1mStep[0m  [56/84], [94mLoss[0m : 1.57935
[1mStep[0m  [64/84], [94mLoss[0m : 1.41935
[1mStep[0m  [72/84], [94mLoss[0m : 1.67956
[1mStep[0m  [80/84], [94mLoss[0m : 1.67161

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.577, [92mTest[0m: 2.555, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.46841
[1mStep[0m  [8/84], [94mLoss[0m : 1.40388
[1mStep[0m  [16/84], [94mLoss[0m : 1.51323
[1mStep[0m  [24/84], [94mLoss[0m : 1.46017
[1mStep[0m  [32/84], [94mLoss[0m : 1.51266
[1mStep[0m  [40/84], [94mLoss[0m : 1.53578
[1mStep[0m  [48/84], [94mLoss[0m : 1.51199
[1mStep[0m  [56/84], [94mLoss[0m : 1.73135
[1mStep[0m  [64/84], [94mLoss[0m : 1.58754
[1mStep[0m  [72/84], [94mLoss[0m : 1.57003
[1mStep[0m  [80/84], [94mLoss[0m : 1.70690

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.522, [92mTest[0m: 2.477, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.56944
[1mStep[0m  [8/84], [94mLoss[0m : 1.46766
[1mStep[0m  [16/84], [94mLoss[0m : 1.42612
[1mStep[0m  [24/84], [94mLoss[0m : 1.51426
[1mStep[0m  [32/84], [94mLoss[0m : 1.53655
[1mStep[0m  [40/84], [94mLoss[0m : 1.52640
[1mStep[0m  [48/84], [94mLoss[0m : 1.37632
[1mStep[0m  [56/84], [94mLoss[0m : 1.49510
[1mStep[0m  [64/84], [94mLoss[0m : 1.47091
[1mStep[0m  [72/84], [94mLoss[0m : 1.62234
[1mStep[0m  [80/84], [94mLoss[0m : 1.45045

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.484, [92mTest[0m: 2.513, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.60267
[1mStep[0m  [8/84], [94mLoss[0m : 1.43457
[1mStep[0m  [16/84], [94mLoss[0m : 1.53547
[1mStep[0m  [24/84], [94mLoss[0m : 1.36791
[1mStep[0m  [32/84], [94mLoss[0m : 1.49961
[1mStep[0m  [40/84], [94mLoss[0m : 1.48232
[1mStep[0m  [48/84], [94mLoss[0m : 1.29918
[1mStep[0m  [56/84], [94mLoss[0m : 1.61744
[1mStep[0m  [64/84], [94mLoss[0m : 1.37222
[1mStep[0m  [72/84], [94mLoss[0m : 1.27186
[1mStep[0m  [80/84], [94mLoss[0m : 1.75564

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.472, [92mTest[0m: 2.496, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.31998
[1mStep[0m  [8/84], [94mLoss[0m : 1.51771
[1mStep[0m  [16/84], [94mLoss[0m : 1.42810
[1mStep[0m  [24/84], [94mLoss[0m : 1.37915
[1mStep[0m  [32/84], [94mLoss[0m : 1.52513
[1mStep[0m  [40/84], [94mLoss[0m : 1.35930
[1mStep[0m  [48/84], [94mLoss[0m : 1.33100
[1mStep[0m  [56/84], [94mLoss[0m : 1.49449
[1mStep[0m  [64/84], [94mLoss[0m : 1.32762
[1mStep[0m  [72/84], [94mLoss[0m : 1.49621
[1mStep[0m  [80/84], [94mLoss[0m : 1.57655

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.441, [92mTest[0m: 2.559, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.17993
[1mStep[0m  [8/84], [94mLoss[0m : 1.31391
[1mStep[0m  [16/84], [94mLoss[0m : 1.42667
[1mStep[0m  [24/84], [94mLoss[0m : 1.43572
[1mStep[0m  [32/84], [94mLoss[0m : 1.23125
[1mStep[0m  [40/84], [94mLoss[0m : 1.32226
[1mStep[0m  [48/84], [94mLoss[0m : 1.33337
[1mStep[0m  [56/84], [94mLoss[0m : 1.60275
[1mStep[0m  [64/84], [94mLoss[0m : 1.30067
[1mStep[0m  [72/84], [94mLoss[0m : 1.34046
[1mStep[0m  [80/84], [94mLoss[0m : 1.48838

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.400, [92mTest[0m: 2.539, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.37231
[1mStep[0m  [8/84], [94mLoss[0m : 1.23418
[1mStep[0m  [16/84], [94mLoss[0m : 1.36769
[1mStep[0m  [24/84], [94mLoss[0m : 1.06104
[1mStep[0m  [32/84], [94mLoss[0m : 1.35057
[1mStep[0m  [40/84], [94mLoss[0m : 1.48208
[1mStep[0m  [48/84], [94mLoss[0m : 1.53640
[1mStep[0m  [56/84], [94mLoss[0m : 1.42235
[1mStep[0m  [64/84], [94mLoss[0m : 1.20520
[1mStep[0m  [72/84], [94mLoss[0m : 1.30845
[1mStep[0m  [80/84], [94mLoss[0m : 1.27702

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.345, [92mTest[0m: 2.567, [96mlr[0m: 0.006772500000000002
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 20 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.487
====================================

Phase 2 - Evaluation MAE:  2.4871589967182706
MAE score P1        2.325332
MAE score P2        2.487159
loss                1.344862
learning_rate       0.007525
batch_size               128
hidden_sizes      [300, 100]
epochs                    30
activation              relu
optimizer                sgd
early stopping          True
dropout                  0.3
momentum                 0.9
weight_decay          0.0001
Name: 1, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 11.23463
[1mStep[0m  [8/84], [94mLoss[0m : 9.94366
[1mStep[0m  [16/84], [94mLoss[0m : 9.38896
[1mStep[0m  [24/84], [94mLoss[0m : 8.69245
[1mStep[0m  [32/84], [94mLoss[0m : 6.60408
[1mStep[0m  [40/84], [94mLoss[0m : 6.17646
[1mStep[0m  [48/84], [94mLoss[0m : 4.79742
[1mStep[0m  [56/84], [94mLoss[0m : 3.78293
[1mStep[0m  [64/84], [94mLoss[0m : 3.01722
[1mStep[0m  [72/84], [94mLoss[0m : 2.89463
[1mStep[0m  [80/84], [94mLoss[0m : 3.14007

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.242, [92mTest[0m: 11.106, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.61659
[1mStep[0m  [8/84], [94mLoss[0m : 2.78391
[1mStep[0m  [16/84], [94mLoss[0m : 2.74948
[1mStep[0m  [24/84], [94mLoss[0m : 2.71041
[1mStep[0m  [32/84], [94mLoss[0m : 2.66238
[1mStep[0m  [40/84], [94mLoss[0m : 2.76339
[1mStep[0m  [48/84], [94mLoss[0m : 2.64523
[1mStep[0m  [56/84], [94mLoss[0m : 2.82132
[1mStep[0m  [64/84], [94mLoss[0m : 2.33400
[1mStep[0m  [72/84], [94mLoss[0m : 2.39922
[1mStep[0m  [80/84], [94mLoss[0m : 2.44690

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.735, [92mTest[0m: 2.927, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.85968
[1mStep[0m  [8/84], [94mLoss[0m : 2.61534
[1mStep[0m  [16/84], [94mLoss[0m : 2.52730
[1mStep[0m  [24/84], [94mLoss[0m : 2.58817
[1mStep[0m  [32/84], [94mLoss[0m : 2.68953
[1mStep[0m  [40/84], [94mLoss[0m : 2.59022
[1mStep[0m  [48/84], [94mLoss[0m : 2.68236
[1mStep[0m  [56/84], [94mLoss[0m : 2.58704
[1mStep[0m  [64/84], [94mLoss[0m : 2.80770
[1mStep[0m  [72/84], [94mLoss[0m : 2.76050
[1mStep[0m  [80/84], [94mLoss[0m : 2.82932

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.649, [92mTest[0m: 2.443, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33557
[1mStep[0m  [8/84], [94mLoss[0m : 2.51130
[1mStep[0m  [16/84], [94mLoss[0m : 2.47202
[1mStep[0m  [24/84], [94mLoss[0m : 2.74833
[1mStep[0m  [32/84], [94mLoss[0m : 2.82877
[1mStep[0m  [40/84], [94mLoss[0m : 2.78504
[1mStep[0m  [48/84], [94mLoss[0m : 2.41640
[1mStep[0m  [56/84], [94mLoss[0m : 2.71337
[1mStep[0m  [64/84], [94mLoss[0m : 2.43292
[1mStep[0m  [72/84], [94mLoss[0m : 2.73172
[1mStep[0m  [80/84], [94mLoss[0m : 2.39789

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.603, [92mTest[0m: 2.442, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43255
[1mStep[0m  [8/84], [94mLoss[0m : 2.46210
[1mStep[0m  [16/84], [94mLoss[0m : 2.48922
[1mStep[0m  [24/84], [94mLoss[0m : 3.11806
[1mStep[0m  [32/84], [94mLoss[0m : 2.89610
[1mStep[0m  [40/84], [94mLoss[0m : 2.65209
[1mStep[0m  [48/84], [94mLoss[0m : 2.45524
[1mStep[0m  [56/84], [94mLoss[0m : 2.26891
[1mStep[0m  [64/84], [94mLoss[0m : 2.58104
[1mStep[0m  [72/84], [94mLoss[0m : 2.90029
[1mStep[0m  [80/84], [94mLoss[0m : 2.53836

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.577, [92mTest[0m: 2.367, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.70698
[1mStep[0m  [8/84], [94mLoss[0m : 2.50547
[1mStep[0m  [16/84], [94mLoss[0m : 2.51377
[1mStep[0m  [24/84], [94mLoss[0m : 2.77357
[1mStep[0m  [32/84], [94mLoss[0m : 2.70768
[1mStep[0m  [40/84], [94mLoss[0m : 2.32367
[1mStep[0m  [48/84], [94mLoss[0m : 2.94790
[1mStep[0m  [56/84], [94mLoss[0m : 2.78331
[1mStep[0m  [64/84], [94mLoss[0m : 2.58630
[1mStep[0m  [72/84], [94mLoss[0m : 2.39764
[1mStep[0m  [80/84], [94mLoss[0m : 2.46132

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.545, [92mTest[0m: 2.377, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.32370
[1mStep[0m  [8/84], [94mLoss[0m : 2.58878
[1mStep[0m  [16/84], [94mLoss[0m : 3.17832
[1mStep[0m  [24/84], [94mLoss[0m : 2.44972
[1mStep[0m  [32/84], [94mLoss[0m : 2.98417
[1mStep[0m  [40/84], [94mLoss[0m : 2.36479
[1mStep[0m  [48/84], [94mLoss[0m : 2.43755
[1mStep[0m  [56/84], [94mLoss[0m : 2.37902
[1mStep[0m  [64/84], [94mLoss[0m : 2.46286
[1mStep[0m  [72/84], [94mLoss[0m : 2.51609
[1mStep[0m  [80/84], [94mLoss[0m : 2.78166

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.361, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.70981
[1mStep[0m  [8/84], [94mLoss[0m : 2.28868
[1mStep[0m  [16/84], [94mLoss[0m : 2.84397
[1mStep[0m  [24/84], [94mLoss[0m : 2.62974
[1mStep[0m  [32/84], [94mLoss[0m : 2.48860
[1mStep[0m  [40/84], [94mLoss[0m : 2.35637
[1mStep[0m  [48/84], [94mLoss[0m : 2.73152
[1mStep[0m  [56/84], [94mLoss[0m : 2.49948
[1mStep[0m  [64/84], [94mLoss[0m : 2.94863
[1mStep[0m  [72/84], [94mLoss[0m : 2.48051
[1mStep[0m  [80/84], [94mLoss[0m : 2.37942

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.389, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.65017
[1mStep[0m  [8/84], [94mLoss[0m : 2.42979
[1mStep[0m  [16/84], [94mLoss[0m : 2.84993
[1mStep[0m  [24/84], [94mLoss[0m : 2.40594
[1mStep[0m  [32/84], [94mLoss[0m : 2.67089
[1mStep[0m  [40/84], [94mLoss[0m : 2.49651
[1mStep[0m  [48/84], [94mLoss[0m : 3.16399
[1mStep[0m  [56/84], [94mLoss[0m : 2.35642
[1mStep[0m  [64/84], [94mLoss[0m : 2.38507
[1mStep[0m  [72/84], [94mLoss[0m : 2.57839
[1mStep[0m  [80/84], [94mLoss[0m : 2.41693

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.348, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42920
[1mStep[0m  [8/84], [94mLoss[0m : 2.56532
[1mStep[0m  [16/84], [94mLoss[0m : 2.60603
[1mStep[0m  [24/84], [94mLoss[0m : 2.04059
[1mStep[0m  [32/84], [94mLoss[0m : 2.47864
[1mStep[0m  [40/84], [94mLoss[0m : 2.53637
[1mStep[0m  [48/84], [94mLoss[0m : 2.63236
[1mStep[0m  [56/84], [94mLoss[0m : 2.29459
[1mStep[0m  [64/84], [94mLoss[0m : 2.94440
[1mStep[0m  [72/84], [94mLoss[0m : 2.46651
[1mStep[0m  [80/84], [94mLoss[0m : 2.53358

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.363, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.75191
[1mStep[0m  [8/84], [94mLoss[0m : 2.41472
[1mStep[0m  [16/84], [94mLoss[0m : 2.23333
[1mStep[0m  [24/84], [94mLoss[0m : 2.36706
[1mStep[0m  [32/84], [94mLoss[0m : 2.30008
[1mStep[0m  [40/84], [94mLoss[0m : 2.31749
[1mStep[0m  [48/84], [94mLoss[0m : 2.35116
[1mStep[0m  [56/84], [94mLoss[0m : 2.86376
[1mStep[0m  [64/84], [94mLoss[0m : 2.55070
[1mStep[0m  [72/84], [94mLoss[0m : 2.31980
[1mStep[0m  [80/84], [94mLoss[0m : 2.65062

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.339, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44787
[1mStep[0m  [8/84], [94mLoss[0m : 2.75180
[1mStep[0m  [16/84], [94mLoss[0m : 2.74603
[1mStep[0m  [24/84], [94mLoss[0m : 2.38537
[1mStep[0m  [32/84], [94mLoss[0m : 2.45255
[1mStep[0m  [40/84], [94mLoss[0m : 2.53355
[1mStep[0m  [48/84], [94mLoss[0m : 2.55552
[1mStep[0m  [56/84], [94mLoss[0m : 2.77519
[1mStep[0m  [64/84], [94mLoss[0m : 2.27378
[1mStep[0m  [72/84], [94mLoss[0m : 2.73444
[1mStep[0m  [80/84], [94mLoss[0m : 2.47523

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.343, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.66710
[1mStep[0m  [8/84], [94mLoss[0m : 2.37290
[1mStep[0m  [16/84], [94mLoss[0m : 2.58243
[1mStep[0m  [24/84], [94mLoss[0m : 2.59018
[1mStep[0m  [32/84], [94mLoss[0m : 2.50366
[1mStep[0m  [40/84], [94mLoss[0m : 2.66877
[1mStep[0m  [48/84], [94mLoss[0m : 2.61361
[1mStep[0m  [56/84], [94mLoss[0m : 2.30246
[1mStep[0m  [64/84], [94mLoss[0m : 2.68232
[1mStep[0m  [72/84], [94mLoss[0m : 2.69274
[1mStep[0m  [80/84], [94mLoss[0m : 2.68086

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.335, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54820
[1mStep[0m  [8/84], [94mLoss[0m : 2.26314
[1mStep[0m  [16/84], [94mLoss[0m : 2.67105
[1mStep[0m  [24/84], [94mLoss[0m : 2.45463
[1mStep[0m  [32/84], [94mLoss[0m : 2.37361
[1mStep[0m  [40/84], [94mLoss[0m : 2.59050
[1mStep[0m  [48/84], [94mLoss[0m : 2.37646
[1mStep[0m  [56/84], [94mLoss[0m : 2.18238
[1mStep[0m  [64/84], [94mLoss[0m : 2.36835
[1mStep[0m  [72/84], [94mLoss[0m : 2.55373
[1mStep[0m  [80/84], [94mLoss[0m : 2.57829

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.333, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43840
[1mStep[0m  [8/84], [94mLoss[0m : 2.62228
[1mStep[0m  [16/84], [94mLoss[0m : 2.23619
[1mStep[0m  [24/84], [94mLoss[0m : 2.37147
[1mStep[0m  [32/84], [94mLoss[0m : 2.51269
[1mStep[0m  [40/84], [94mLoss[0m : 2.54340
[1mStep[0m  [48/84], [94mLoss[0m : 2.44922
[1mStep[0m  [56/84], [94mLoss[0m : 2.42489
[1mStep[0m  [64/84], [94mLoss[0m : 2.50218
[1mStep[0m  [72/84], [94mLoss[0m : 2.42630
[1mStep[0m  [80/84], [94mLoss[0m : 2.55221

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.345, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54876
[1mStep[0m  [8/84], [94mLoss[0m : 2.73463
[1mStep[0m  [16/84], [94mLoss[0m : 2.47966
[1mStep[0m  [24/84], [94mLoss[0m : 2.50079
[1mStep[0m  [32/84], [94mLoss[0m : 2.61361
[1mStep[0m  [40/84], [94mLoss[0m : 2.43062
[1mStep[0m  [48/84], [94mLoss[0m : 2.29047
[1mStep[0m  [56/84], [94mLoss[0m : 2.58596
[1mStep[0m  [64/84], [94mLoss[0m : 2.51100
[1mStep[0m  [72/84], [94mLoss[0m : 2.26958
[1mStep[0m  [80/84], [94mLoss[0m : 2.63846

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.326, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.29637
[1mStep[0m  [8/84], [94mLoss[0m : 2.48712
[1mStep[0m  [16/84], [94mLoss[0m : 2.28588
[1mStep[0m  [24/84], [94mLoss[0m : 2.64952
[1mStep[0m  [32/84], [94mLoss[0m : 2.37251
[1mStep[0m  [40/84], [94mLoss[0m : 2.44543
[1mStep[0m  [48/84], [94mLoss[0m : 2.45115
[1mStep[0m  [56/84], [94mLoss[0m : 2.51194
[1mStep[0m  [64/84], [94mLoss[0m : 2.55316
[1mStep[0m  [72/84], [94mLoss[0m : 2.12032
[1mStep[0m  [80/84], [94mLoss[0m : 2.39892

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.325, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31388
[1mStep[0m  [8/84], [94mLoss[0m : 2.53627
[1mStep[0m  [16/84], [94mLoss[0m : 2.47675
[1mStep[0m  [24/84], [94mLoss[0m : 2.39246
[1mStep[0m  [32/84], [94mLoss[0m : 2.23806
[1mStep[0m  [40/84], [94mLoss[0m : 2.25527
[1mStep[0m  [48/84], [94mLoss[0m : 2.71780
[1mStep[0m  [56/84], [94mLoss[0m : 2.51670
[1mStep[0m  [64/84], [94mLoss[0m : 2.38308
[1mStep[0m  [72/84], [94mLoss[0m : 2.38896
[1mStep[0m  [80/84], [94mLoss[0m : 2.88346

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.333, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26550
[1mStep[0m  [8/84], [94mLoss[0m : 2.66258
[1mStep[0m  [16/84], [94mLoss[0m : 2.14841
[1mStep[0m  [24/84], [94mLoss[0m : 2.47021
[1mStep[0m  [32/84], [94mLoss[0m : 2.24864
[1mStep[0m  [40/84], [94mLoss[0m : 2.38691
[1mStep[0m  [48/84], [94mLoss[0m : 2.19214
[1mStep[0m  [56/84], [94mLoss[0m : 2.62395
[1mStep[0m  [64/84], [94mLoss[0m : 2.45764
[1mStep[0m  [72/84], [94mLoss[0m : 2.11769
[1mStep[0m  [80/84], [94mLoss[0m : 2.28777

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.386, [92mTest[0m: 2.323, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.39503
[1mStep[0m  [8/84], [94mLoss[0m : 2.34233
[1mStep[0m  [16/84], [94mLoss[0m : 2.71546
[1mStep[0m  [24/84], [94mLoss[0m : 2.31462
[1mStep[0m  [32/84], [94mLoss[0m : 2.43725
[1mStep[0m  [40/84], [94mLoss[0m : 2.25286
[1mStep[0m  [48/84], [94mLoss[0m : 2.45743
[1mStep[0m  [56/84], [94mLoss[0m : 2.28585
[1mStep[0m  [64/84], [94mLoss[0m : 2.43945
[1mStep[0m  [72/84], [94mLoss[0m : 2.51102
[1mStep[0m  [80/84], [94mLoss[0m : 2.27341

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.391, [92mTest[0m: 2.327, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55727
[1mStep[0m  [8/84], [94mLoss[0m : 2.45747
[1mStep[0m  [16/84], [94mLoss[0m : 2.71395
[1mStep[0m  [24/84], [94mLoss[0m : 2.35358
[1mStep[0m  [32/84], [94mLoss[0m : 2.46551
[1mStep[0m  [40/84], [94mLoss[0m : 2.30511
[1mStep[0m  [48/84], [94mLoss[0m : 2.40403
[1mStep[0m  [56/84], [94mLoss[0m : 2.35557
[1mStep[0m  [64/84], [94mLoss[0m : 2.21617
[1mStep[0m  [72/84], [94mLoss[0m : 2.41669
[1mStep[0m  [80/84], [94mLoss[0m : 2.14608

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.338, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44587
[1mStep[0m  [8/84], [94mLoss[0m : 2.24921
[1mStep[0m  [16/84], [94mLoss[0m : 2.30700
[1mStep[0m  [24/84], [94mLoss[0m : 2.00267
[1mStep[0m  [32/84], [94mLoss[0m : 2.09105
[1mStep[0m  [40/84], [94mLoss[0m : 2.38416
[1mStep[0m  [48/84], [94mLoss[0m : 2.44610
[1mStep[0m  [56/84], [94mLoss[0m : 2.68137
[1mStep[0m  [64/84], [94mLoss[0m : 2.40553
[1mStep[0m  [72/84], [94mLoss[0m : 2.34057
[1mStep[0m  [80/84], [94mLoss[0m : 2.52434

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.381, [92mTest[0m: 2.330, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47658
[1mStep[0m  [8/84], [94mLoss[0m : 2.45097
[1mStep[0m  [16/84], [94mLoss[0m : 2.66187
[1mStep[0m  [24/84], [94mLoss[0m : 2.15326
[1mStep[0m  [32/84], [94mLoss[0m : 2.46453
[1mStep[0m  [40/84], [94mLoss[0m : 2.56279
[1mStep[0m  [48/84], [94mLoss[0m : 2.56428
[1mStep[0m  [56/84], [94mLoss[0m : 2.47150
[1mStep[0m  [64/84], [94mLoss[0m : 2.35468
[1mStep[0m  [72/84], [94mLoss[0m : 2.53345
[1mStep[0m  [80/84], [94mLoss[0m : 2.58492

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.394, [92mTest[0m: 2.331, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37942
[1mStep[0m  [8/84], [94mLoss[0m : 2.58952
[1mStep[0m  [16/84], [94mLoss[0m : 2.22108
[1mStep[0m  [24/84], [94mLoss[0m : 2.50764
[1mStep[0m  [32/84], [94mLoss[0m : 2.54276
[1mStep[0m  [40/84], [94mLoss[0m : 2.43084
[1mStep[0m  [48/84], [94mLoss[0m : 2.19551
[1mStep[0m  [56/84], [94mLoss[0m : 2.39192
[1mStep[0m  [64/84], [94mLoss[0m : 2.36489
[1mStep[0m  [72/84], [94mLoss[0m : 2.76276
[1mStep[0m  [80/84], [94mLoss[0m : 2.34013

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.394, [92mTest[0m: 2.324, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27049
[1mStep[0m  [8/84], [94mLoss[0m : 2.43089
[1mStep[0m  [16/84], [94mLoss[0m : 2.39485
[1mStep[0m  [24/84], [94mLoss[0m : 2.09738
[1mStep[0m  [32/84], [94mLoss[0m : 2.69011
[1mStep[0m  [40/84], [94mLoss[0m : 2.54597
[1mStep[0m  [48/84], [94mLoss[0m : 2.31055
[1mStep[0m  [56/84], [94mLoss[0m : 2.46919
[1mStep[0m  [64/84], [94mLoss[0m : 2.73096
[1mStep[0m  [72/84], [94mLoss[0m : 2.49250
[1mStep[0m  [80/84], [94mLoss[0m : 2.20581

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.372, [92mTest[0m: 2.328, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26567
[1mStep[0m  [8/84], [94mLoss[0m : 2.38006
[1mStep[0m  [16/84], [94mLoss[0m : 2.16274
[1mStep[0m  [24/84], [94mLoss[0m : 2.17166
[1mStep[0m  [32/84], [94mLoss[0m : 2.37275
[1mStep[0m  [40/84], [94mLoss[0m : 2.10486
[1mStep[0m  [48/84], [94mLoss[0m : 2.29756
[1mStep[0m  [56/84], [94mLoss[0m : 2.22360
[1mStep[0m  [64/84], [94mLoss[0m : 2.53450
[1mStep[0m  [72/84], [94mLoss[0m : 2.44317
[1mStep[0m  [80/84], [94mLoss[0m : 2.05012

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.355, [92mTest[0m: 2.320, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53427
[1mStep[0m  [8/84], [94mLoss[0m : 2.26431
[1mStep[0m  [16/84], [94mLoss[0m : 2.35555
[1mStep[0m  [24/84], [94mLoss[0m : 2.66911
[1mStep[0m  [32/84], [94mLoss[0m : 2.28551
[1mStep[0m  [40/84], [94mLoss[0m : 2.56362
[1mStep[0m  [48/84], [94mLoss[0m : 2.44757
[1mStep[0m  [56/84], [94mLoss[0m : 2.49546
[1mStep[0m  [64/84], [94mLoss[0m : 2.18869
[1mStep[0m  [72/84], [94mLoss[0m : 2.31819
[1mStep[0m  [80/84], [94mLoss[0m : 2.12282

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.360, [92mTest[0m: 2.331, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.74047
[1mStep[0m  [8/84], [94mLoss[0m : 2.33635
[1mStep[0m  [16/84], [94mLoss[0m : 2.39477
[1mStep[0m  [24/84], [94mLoss[0m : 2.28413
[1mStep[0m  [32/84], [94mLoss[0m : 2.57324
[1mStep[0m  [40/84], [94mLoss[0m : 2.08067
[1mStep[0m  [48/84], [94mLoss[0m : 2.41741
[1mStep[0m  [56/84], [94mLoss[0m : 2.51308
[1mStep[0m  [64/84], [94mLoss[0m : 2.36719
[1mStep[0m  [72/84], [94mLoss[0m : 2.48799
[1mStep[0m  [80/84], [94mLoss[0m : 2.38280

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.361, [92mTest[0m: 2.313, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.19464
[1mStep[0m  [8/84], [94mLoss[0m : 2.47249
[1mStep[0m  [16/84], [94mLoss[0m : 2.45271
[1mStep[0m  [24/84], [94mLoss[0m : 2.26137
[1mStep[0m  [32/84], [94mLoss[0m : 2.42933
[1mStep[0m  [40/84], [94mLoss[0m : 2.43799
[1mStep[0m  [48/84], [94mLoss[0m : 2.40082
[1mStep[0m  [56/84], [94mLoss[0m : 2.21596
[1mStep[0m  [64/84], [94mLoss[0m : 2.29838
[1mStep[0m  [72/84], [94mLoss[0m : 2.42319
[1mStep[0m  [80/84], [94mLoss[0m : 2.67345

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.372, [92mTest[0m: 2.332, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.25370
[1mStep[0m  [8/84], [94mLoss[0m : 2.27085
[1mStep[0m  [16/84], [94mLoss[0m : 2.38396
[1mStep[0m  [24/84], [94mLoss[0m : 1.99659
[1mStep[0m  [32/84], [94mLoss[0m : 2.31933
[1mStep[0m  [40/84], [94mLoss[0m : 2.66946
[1mStep[0m  [48/84], [94mLoss[0m : 2.54651
[1mStep[0m  [56/84], [94mLoss[0m : 2.40891
[1mStep[0m  [64/84], [94mLoss[0m : 2.32170
[1mStep[0m  [72/84], [94mLoss[0m : 2.46521
[1mStep[0m  [80/84], [94mLoss[0m : 2.39105

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.338, [92mTest[0m: 2.316, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.326
====================================

Phase 1 - Evaluation MAE:  2.326129470552717
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 2.56645
[1mStep[0m  [8/84], [94mLoss[0m : 2.49348
[1mStep[0m  [16/84], [94mLoss[0m : 2.58628
[1mStep[0m  [24/84], [94mLoss[0m : 2.50567
[1mStep[0m  [32/84], [94mLoss[0m : 2.31591
[1mStep[0m  [40/84], [94mLoss[0m : 2.41783
[1mStep[0m  [48/84], [94mLoss[0m : 2.09345
[1mStep[0m  [56/84], [94mLoss[0m : 2.50711
[1mStep[0m  [64/84], [94mLoss[0m : 2.55867
[1mStep[0m  [72/84], [94mLoss[0m : 2.36709
[1mStep[0m  [80/84], [94mLoss[0m : 2.27894

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.324, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.69432
[1mStep[0m  [8/84], [94mLoss[0m : 2.60247
[1mStep[0m  [16/84], [94mLoss[0m : 1.92547
[1mStep[0m  [24/84], [94mLoss[0m : 2.35523
[1mStep[0m  [32/84], [94mLoss[0m : 2.21390
[1mStep[0m  [40/84], [94mLoss[0m : 2.54307
[1mStep[0m  [48/84], [94mLoss[0m : 2.43904
[1mStep[0m  [56/84], [94mLoss[0m : 2.55727
[1mStep[0m  [64/84], [94mLoss[0m : 2.60874
[1mStep[0m  [72/84], [94mLoss[0m : 2.26835
[1mStep[0m  [80/84], [94mLoss[0m : 2.42703

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.410, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37774
[1mStep[0m  [8/84], [94mLoss[0m : 2.51851
[1mStep[0m  [16/84], [94mLoss[0m : 2.33410
[1mStep[0m  [24/84], [94mLoss[0m : 2.34690
[1mStep[0m  [32/84], [94mLoss[0m : 2.14817
[1mStep[0m  [40/84], [94mLoss[0m : 2.38833
[1mStep[0m  [48/84], [94mLoss[0m : 2.32441
[1mStep[0m  [56/84], [94mLoss[0m : 2.46805
[1mStep[0m  [64/84], [94mLoss[0m : 1.98299
[1mStep[0m  [72/84], [94mLoss[0m : 2.43446
[1mStep[0m  [80/84], [94mLoss[0m : 2.06930

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.330, [92mTest[0m: 2.351, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27323
[1mStep[0m  [8/84], [94mLoss[0m : 2.20723
[1mStep[0m  [16/84], [94mLoss[0m : 2.35807
[1mStep[0m  [24/84], [94mLoss[0m : 2.01387
[1mStep[0m  [32/84], [94mLoss[0m : 2.77673
[1mStep[0m  [40/84], [94mLoss[0m : 2.44693
[1mStep[0m  [48/84], [94mLoss[0m : 2.49688
[1mStep[0m  [56/84], [94mLoss[0m : 2.39121
[1mStep[0m  [64/84], [94mLoss[0m : 2.46286
[1mStep[0m  [72/84], [94mLoss[0m : 2.14468
[1mStep[0m  [80/84], [94mLoss[0m : 2.18961

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.279, [92mTest[0m: 2.350, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.12480
[1mStep[0m  [8/84], [94mLoss[0m : 2.09803
[1mStep[0m  [16/84], [94mLoss[0m : 2.12331
[1mStep[0m  [24/84], [94mLoss[0m : 1.91344
[1mStep[0m  [32/84], [94mLoss[0m : 2.34731
[1mStep[0m  [40/84], [94mLoss[0m : 2.32048
[1mStep[0m  [48/84], [94mLoss[0m : 2.32446
[1mStep[0m  [56/84], [94mLoss[0m : 2.09523
[1mStep[0m  [64/84], [94mLoss[0m : 2.31729
[1mStep[0m  [72/84], [94mLoss[0m : 2.22717
[1mStep[0m  [80/84], [94mLoss[0m : 2.08449

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.219, [92mTest[0m: 2.345, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.19353
[1mStep[0m  [8/84], [94mLoss[0m : 2.20662
[1mStep[0m  [16/84], [94mLoss[0m : 2.19076
[1mStep[0m  [24/84], [94mLoss[0m : 1.97171
[1mStep[0m  [32/84], [94mLoss[0m : 2.25445
[1mStep[0m  [40/84], [94mLoss[0m : 2.21873
[1mStep[0m  [48/84], [94mLoss[0m : 2.34748
[1mStep[0m  [56/84], [94mLoss[0m : 2.05858
[1mStep[0m  [64/84], [94mLoss[0m : 2.13943
[1mStep[0m  [72/84], [94mLoss[0m : 1.96775
[1mStep[0m  [80/84], [94mLoss[0m : 2.13774

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.162, [92mTest[0m: 2.383, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.12748
[1mStep[0m  [8/84], [94mLoss[0m : 2.43928
[1mStep[0m  [16/84], [94mLoss[0m : 2.16342
[1mStep[0m  [24/84], [94mLoss[0m : 1.94904
[1mStep[0m  [32/84], [94mLoss[0m : 2.45256
[1mStep[0m  [40/84], [94mLoss[0m : 2.06057
[1mStep[0m  [48/84], [94mLoss[0m : 2.50536
[1mStep[0m  [56/84], [94mLoss[0m : 1.98348
[1mStep[0m  [64/84], [94mLoss[0m : 1.98674
[1mStep[0m  [72/84], [94mLoss[0m : 1.91335
[1mStep[0m  [80/84], [94mLoss[0m : 2.12136

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.094, [92mTest[0m: 2.394, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.15163
[1mStep[0m  [8/84], [94mLoss[0m : 1.83406
[1mStep[0m  [16/84], [94mLoss[0m : 2.19016
[1mStep[0m  [24/84], [94mLoss[0m : 2.23755
[1mStep[0m  [32/84], [94mLoss[0m : 1.94368
[1mStep[0m  [40/84], [94mLoss[0m : 1.93587
[1mStep[0m  [48/84], [94mLoss[0m : 1.96004
[1mStep[0m  [56/84], [94mLoss[0m : 2.08676
[1mStep[0m  [64/84], [94mLoss[0m : 1.99047
[1mStep[0m  [72/84], [94mLoss[0m : 2.16140
[1mStep[0m  [80/84], [94mLoss[0m : 1.87789

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.060, [92mTest[0m: 2.429, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.66225
[1mStep[0m  [8/84], [94mLoss[0m : 1.91464
[1mStep[0m  [16/84], [94mLoss[0m : 1.75892
[1mStep[0m  [24/84], [94mLoss[0m : 2.00101
[1mStep[0m  [32/84], [94mLoss[0m : 1.98378
[1mStep[0m  [40/84], [94mLoss[0m : 2.08048
[1mStep[0m  [48/84], [94mLoss[0m : 1.86551
[1mStep[0m  [56/84], [94mLoss[0m : 2.04279
[1mStep[0m  [64/84], [94mLoss[0m : 2.11927
[1mStep[0m  [72/84], [94mLoss[0m : 2.04320
[1mStep[0m  [80/84], [94mLoss[0m : 1.99271

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.996, [92mTest[0m: 2.450, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.94591
[1mStep[0m  [8/84], [94mLoss[0m : 1.94783
[1mStep[0m  [16/84], [94mLoss[0m : 1.94038
[1mStep[0m  [24/84], [94mLoss[0m : 2.28625
[1mStep[0m  [32/84], [94mLoss[0m : 1.80660
[1mStep[0m  [40/84], [94mLoss[0m : 2.11191
[1mStep[0m  [48/84], [94mLoss[0m : 2.09092
[1mStep[0m  [56/84], [94mLoss[0m : 1.88715
[1mStep[0m  [64/84], [94mLoss[0m : 2.09624
[1mStep[0m  [72/84], [94mLoss[0m : 2.08442
[1mStep[0m  [80/84], [94mLoss[0m : 2.21808

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.970, [92mTest[0m: 2.422, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.61967
[1mStep[0m  [8/84], [94mLoss[0m : 1.69807
[1mStep[0m  [16/84], [94mLoss[0m : 2.38011
[1mStep[0m  [24/84], [94mLoss[0m : 1.72183
[1mStep[0m  [32/84], [94mLoss[0m : 2.22580
[1mStep[0m  [40/84], [94mLoss[0m : 1.87081
[1mStep[0m  [48/84], [94mLoss[0m : 1.89736
[1mStep[0m  [56/84], [94mLoss[0m : 1.70879
[1mStep[0m  [64/84], [94mLoss[0m : 2.06052
[1mStep[0m  [72/84], [94mLoss[0m : 1.70365
[1mStep[0m  [80/84], [94mLoss[0m : 1.73380

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.920, [92mTest[0m: 2.414, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.80838
[1mStep[0m  [8/84], [94mLoss[0m : 2.01360
[1mStep[0m  [16/84], [94mLoss[0m : 1.97185
[1mStep[0m  [24/84], [94mLoss[0m : 1.89075
[1mStep[0m  [32/84], [94mLoss[0m : 1.93575
[1mStep[0m  [40/84], [94mLoss[0m : 1.73881
[1mStep[0m  [48/84], [94mLoss[0m : 1.70554
[1mStep[0m  [56/84], [94mLoss[0m : 1.76665
[1mStep[0m  [64/84], [94mLoss[0m : 2.13350
[1mStep[0m  [72/84], [94mLoss[0m : 1.85731
[1mStep[0m  [80/84], [94mLoss[0m : 1.95724

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.878, [92mTest[0m: 2.448, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.81900
[1mStep[0m  [8/84], [94mLoss[0m : 1.85343
[1mStep[0m  [16/84], [94mLoss[0m : 1.83412
[1mStep[0m  [24/84], [94mLoss[0m : 2.00262
[1mStep[0m  [32/84], [94mLoss[0m : 1.85025
[1mStep[0m  [40/84], [94mLoss[0m : 1.94295
[1mStep[0m  [48/84], [94mLoss[0m : 1.85889
[1mStep[0m  [56/84], [94mLoss[0m : 1.73433
[1mStep[0m  [64/84], [94mLoss[0m : 1.85950
[1mStep[0m  [72/84], [94mLoss[0m : 2.08929
[1mStep[0m  [80/84], [94mLoss[0m : 1.84150

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.845, [92mTest[0m: 2.399, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.09873
[1mStep[0m  [8/84], [94mLoss[0m : 1.78483
[1mStep[0m  [16/84], [94mLoss[0m : 1.65351
[1mStep[0m  [24/84], [94mLoss[0m : 1.72710
[1mStep[0m  [32/84], [94mLoss[0m : 1.72719
[1mStep[0m  [40/84], [94mLoss[0m : 2.13104
[1mStep[0m  [48/84], [94mLoss[0m : 1.76228
[1mStep[0m  [56/84], [94mLoss[0m : 1.64092
[1mStep[0m  [64/84], [94mLoss[0m : 1.66396
[1mStep[0m  [72/84], [94mLoss[0m : 1.55514
[1mStep[0m  [80/84], [94mLoss[0m : 1.90986

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.811, [92mTest[0m: 2.472, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.81377
[1mStep[0m  [8/84], [94mLoss[0m : 1.66371
[1mStep[0m  [16/84], [94mLoss[0m : 1.51778
[1mStep[0m  [24/84], [94mLoss[0m : 1.72013
[1mStep[0m  [32/84], [94mLoss[0m : 1.64416
[1mStep[0m  [40/84], [94mLoss[0m : 1.95279
[1mStep[0m  [48/84], [94mLoss[0m : 1.76287
[1mStep[0m  [56/84], [94mLoss[0m : 1.65706
[1mStep[0m  [64/84], [94mLoss[0m : 1.80879
[1mStep[0m  [72/84], [94mLoss[0m : 1.81251
[1mStep[0m  [80/84], [94mLoss[0m : 1.64416

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.773, [92mTest[0m: 2.508, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.67068
[1mStep[0m  [8/84], [94mLoss[0m : 1.46386
[1mStep[0m  [16/84], [94mLoss[0m : 1.80332
[1mStep[0m  [24/84], [94mLoss[0m : 1.61703
[1mStep[0m  [32/84], [94mLoss[0m : 1.75577
[1mStep[0m  [40/84], [94mLoss[0m : 1.68566
[1mStep[0m  [48/84], [94mLoss[0m : 1.98120
[1mStep[0m  [56/84], [94mLoss[0m : 1.59533
[1mStep[0m  [64/84], [94mLoss[0m : 1.63693
[1mStep[0m  [72/84], [94mLoss[0m : 1.75442
[1mStep[0m  [80/84], [94mLoss[0m : 1.74527

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.738, [92mTest[0m: 2.448, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.73046
[1mStep[0m  [8/84], [94mLoss[0m : 1.72809
[1mStep[0m  [16/84], [94mLoss[0m : 1.63578
[1mStep[0m  [24/84], [94mLoss[0m : 1.64837
[1mStep[0m  [32/84], [94mLoss[0m : 1.55024
[1mStep[0m  [40/84], [94mLoss[0m : 1.88976
[1mStep[0m  [48/84], [94mLoss[0m : 1.87982
[1mStep[0m  [56/84], [94mLoss[0m : 1.67417
[1mStep[0m  [64/84], [94mLoss[0m : 2.24415
[1mStep[0m  [72/84], [94mLoss[0m : 1.84203
[1mStep[0m  [80/84], [94mLoss[0m : 1.75952

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.718, [92mTest[0m: 2.506, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.60047
[1mStep[0m  [8/84], [94mLoss[0m : 1.54114
[1mStep[0m  [16/84], [94mLoss[0m : 1.65749
[1mStep[0m  [24/84], [94mLoss[0m : 1.80191
[1mStep[0m  [32/84], [94mLoss[0m : 1.53968
[1mStep[0m  [40/84], [94mLoss[0m : 1.92701
[1mStep[0m  [48/84], [94mLoss[0m : 1.68868
[1mStep[0m  [56/84], [94mLoss[0m : 1.95484
[1mStep[0m  [64/84], [94mLoss[0m : 1.46768
[1mStep[0m  [72/84], [94mLoss[0m : 1.58549
[1mStep[0m  [80/84], [94mLoss[0m : 1.56809

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.685, [92mTest[0m: 2.472, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.82746
[1mStep[0m  [8/84], [94mLoss[0m : 1.35982
[1mStep[0m  [16/84], [94mLoss[0m : 1.63490
[1mStep[0m  [24/84], [94mLoss[0m : 1.82682
[1mStep[0m  [32/84], [94mLoss[0m : 1.45539
[1mStep[0m  [40/84], [94mLoss[0m : 1.58915
[1mStep[0m  [48/84], [94mLoss[0m : 1.85825
[1mStep[0m  [56/84], [94mLoss[0m : 1.43467
[1mStep[0m  [64/84], [94mLoss[0m : 1.75191
[1mStep[0m  [72/84], [94mLoss[0m : 1.68924
[1mStep[0m  [80/84], [94mLoss[0m : 1.92803

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.647, [92mTest[0m: 2.434, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.58889
[1mStep[0m  [8/84], [94mLoss[0m : 1.61239
[1mStep[0m  [16/84], [94mLoss[0m : 1.64752
[1mStep[0m  [24/84], [94mLoss[0m : 1.58915
[1mStep[0m  [32/84], [94mLoss[0m : 1.35922
[1mStep[0m  [40/84], [94mLoss[0m : 1.51665
[1mStep[0m  [48/84], [94mLoss[0m : 1.72367
[1mStep[0m  [56/84], [94mLoss[0m : 1.71983
[1mStep[0m  [64/84], [94mLoss[0m : 1.67649
[1mStep[0m  [72/84], [94mLoss[0m : 1.54659
[1mStep[0m  [80/84], [94mLoss[0m : 1.71348

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.614, [92mTest[0m: 2.525, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.46004
[1mStep[0m  [8/84], [94mLoss[0m : 1.58317
[1mStep[0m  [16/84], [94mLoss[0m : 1.65384
[1mStep[0m  [24/84], [94mLoss[0m : 1.74578
[1mStep[0m  [32/84], [94mLoss[0m : 1.69788
[1mStep[0m  [40/84], [94mLoss[0m : 1.50002
[1mStep[0m  [48/84], [94mLoss[0m : 1.66086
[1mStep[0m  [56/84], [94mLoss[0m : 1.91424
[1mStep[0m  [64/84], [94mLoss[0m : 1.65225
[1mStep[0m  [72/84], [94mLoss[0m : 1.74271
[1mStep[0m  [80/84], [94mLoss[0m : 1.68680

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.603, [92mTest[0m: 2.520, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.64179
[1mStep[0m  [8/84], [94mLoss[0m : 1.47004
[1mStep[0m  [16/84], [94mLoss[0m : 1.43883
[1mStep[0m  [24/84], [94mLoss[0m : 1.66142
[1mStep[0m  [32/84], [94mLoss[0m : 1.49740
[1mStep[0m  [40/84], [94mLoss[0m : 1.59440
[1mStep[0m  [48/84], [94mLoss[0m : 1.71151
[1mStep[0m  [56/84], [94mLoss[0m : 1.61877
[1mStep[0m  [64/84], [94mLoss[0m : 1.48902
[1mStep[0m  [72/84], [94mLoss[0m : 1.54106
[1mStep[0m  [80/84], [94mLoss[0m : 1.85443

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.564, [92mTest[0m: 2.497, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.33593
[1mStep[0m  [8/84], [94mLoss[0m : 1.59947
[1mStep[0m  [16/84], [94mLoss[0m : 1.37411
[1mStep[0m  [24/84], [94mLoss[0m : 1.44137
[1mStep[0m  [32/84], [94mLoss[0m : 1.61728
[1mStep[0m  [40/84], [94mLoss[0m : 1.68126
[1mStep[0m  [48/84], [94mLoss[0m : 1.69935
[1mStep[0m  [56/84], [94mLoss[0m : 1.49168
[1mStep[0m  [64/84], [94mLoss[0m : 1.55824
[1mStep[0m  [72/84], [94mLoss[0m : 1.49877
[1mStep[0m  [80/84], [94mLoss[0m : 1.40238

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.547, [92mTest[0m: 2.489, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.78844
[1mStep[0m  [8/84], [94mLoss[0m : 1.52527
[1mStep[0m  [16/84], [94mLoss[0m : 1.41629
[1mStep[0m  [24/84], [94mLoss[0m : 1.60331
[1mStep[0m  [32/84], [94mLoss[0m : 1.63967
[1mStep[0m  [40/84], [94mLoss[0m : 1.37881
[1mStep[0m  [48/84], [94mLoss[0m : 1.32874
[1mStep[0m  [56/84], [94mLoss[0m : 1.67262
[1mStep[0m  [64/84], [94mLoss[0m : 1.45601
[1mStep[0m  [72/84], [94mLoss[0m : 1.47701
[1mStep[0m  [80/84], [94mLoss[0m : 1.59199

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.519, [92mTest[0m: 2.571, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.45712
[1mStep[0m  [8/84], [94mLoss[0m : 1.70058
[1mStep[0m  [16/84], [94mLoss[0m : 1.57951
[1mStep[0m  [24/84], [94mLoss[0m : 1.46114
[1mStep[0m  [32/84], [94mLoss[0m : 1.33692
[1mStep[0m  [40/84], [94mLoss[0m : 1.51457
[1mStep[0m  [48/84], [94mLoss[0m : 1.32722
[1mStep[0m  [56/84], [94mLoss[0m : 1.42566
[1mStep[0m  [64/84], [94mLoss[0m : 1.58924
[1mStep[0m  [72/84], [94mLoss[0m : 1.65266
[1mStep[0m  [80/84], [94mLoss[0m : 1.70112

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.513, [92mTest[0m: 2.637, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.39112
[1mStep[0m  [8/84], [94mLoss[0m : 1.40724
[1mStep[0m  [16/84], [94mLoss[0m : 1.38390
[1mStep[0m  [24/84], [94mLoss[0m : 1.42575
[1mStep[0m  [32/84], [94mLoss[0m : 1.45521
[1mStep[0m  [40/84], [94mLoss[0m : 1.68990
[1mStep[0m  [48/84], [94mLoss[0m : 1.68557
[1mStep[0m  [56/84], [94mLoss[0m : 1.59596
[1mStep[0m  [64/84], [94mLoss[0m : 1.39136
[1mStep[0m  [72/84], [94mLoss[0m : 1.46616
[1mStep[0m  [80/84], [94mLoss[0m : 1.66211

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.479, [92mTest[0m: 2.444, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.49312
[1mStep[0m  [8/84], [94mLoss[0m : 1.33615
[1mStep[0m  [16/84], [94mLoss[0m : 1.50220
[1mStep[0m  [24/84], [94mLoss[0m : 1.46128
[1mStep[0m  [32/84], [94mLoss[0m : 1.35055
[1mStep[0m  [40/84], [94mLoss[0m : 1.35991
[1mStep[0m  [48/84], [94mLoss[0m : 1.55722
[1mStep[0m  [56/84], [94mLoss[0m : 1.32311
[1mStep[0m  [64/84], [94mLoss[0m : 1.29885
[1mStep[0m  [72/84], [94mLoss[0m : 1.62878
[1mStep[0m  [80/84], [94mLoss[0m : 1.53329

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.483, [92mTest[0m: 2.501, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.53483
[1mStep[0m  [8/84], [94mLoss[0m : 1.49491
[1mStep[0m  [16/84], [94mLoss[0m : 1.69792
[1mStep[0m  [24/84], [94mLoss[0m : 1.50415
[1mStep[0m  [32/84], [94mLoss[0m : 1.57357
[1mStep[0m  [40/84], [94mLoss[0m : 1.53328
[1mStep[0m  [48/84], [94mLoss[0m : 1.73819
[1mStep[0m  [56/84], [94mLoss[0m : 1.65096
[1mStep[0m  [64/84], [94mLoss[0m : 1.22148
[1mStep[0m  [72/84], [94mLoss[0m : 1.49150
[1mStep[0m  [80/84], [94mLoss[0m : 1.66737

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.471, [92mTest[0m: 2.509, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.41500
[1mStep[0m  [8/84], [94mLoss[0m : 1.42143
[1mStep[0m  [16/84], [94mLoss[0m : 1.31054
[1mStep[0m  [24/84], [94mLoss[0m : 1.34897
[1mStep[0m  [32/84], [94mLoss[0m : 1.32881
[1mStep[0m  [40/84], [94mLoss[0m : 1.64679
[1mStep[0m  [48/84], [94mLoss[0m : 1.50777
[1mStep[0m  [56/84], [94mLoss[0m : 1.49920
[1mStep[0m  [64/84], [94mLoss[0m : 1.42338
[1mStep[0m  [72/84], [94mLoss[0m : 1.48884
[1mStep[0m  [80/84], [94mLoss[0m : 1.34019

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.432, [92mTest[0m: 2.516, [96mlr[0m: 0.006772500000000002
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 28 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.505
====================================

Phase 2 - Evaluation MAE:  2.5052718009267534
MAE score P1       2.326129
MAE score P2       2.505272
loss               1.432325
learning_rate      0.007525
batch_size              128
hidden_sizes      [200, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping         True
dropout                 0.2
momentum                0.5
weight_decay           0.01
Name: 2, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 10.79479
[1mStep[0m  [4/42], [94mLoss[0m : 9.69808
[1mStep[0m  [8/42], [94mLoss[0m : 8.92778
[1mStep[0m  [12/42], [94mLoss[0m : 8.38661
[1mStep[0m  [16/42], [94mLoss[0m : 7.20713
[1mStep[0m  [20/42], [94mLoss[0m : 6.02830
[1mStep[0m  [24/42], [94mLoss[0m : 5.42781
[1mStep[0m  [28/42], [94mLoss[0m : 4.96775
[1mStep[0m  [32/42], [94mLoss[0m : 4.27946
[1mStep[0m  [36/42], [94mLoss[0m : 3.84146
[1mStep[0m  [40/42], [94mLoss[0m : 3.44953

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.469, [92mTest[0m: 10.673, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.36753
[1mStep[0m  [4/42], [94mLoss[0m : 3.15347
[1mStep[0m  [8/42], [94mLoss[0m : 2.83406
[1mStep[0m  [12/42], [94mLoss[0m : 2.64598
[1mStep[0m  [16/42], [94mLoss[0m : 2.88275
[1mStep[0m  [20/42], [94mLoss[0m : 3.01827
[1mStep[0m  [24/42], [94mLoss[0m : 2.67637
[1mStep[0m  [28/42], [94mLoss[0m : 2.64262
[1mStep[0m  [32/42], [94mLoss[0m : 2.63095
[1mStep[0m  [36/42], [94mLoss[0m : 2.51895
[1mStep[0m  [40/42], [94mLoss[0m : 2.57434

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.822, [92mTest[0m: 3.302, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47913
[1mStep[0m  [4/42], [94mLoss[0m : 2.40301
[1mStep[0m  [8/42], [94mLoss[0m : 2.46621
[1mStep[0m  [12/42], [94mLoss[0m : 2.44085
[1mStep[0m  [16/42], [94mLoss[0m : 2.51330
[1mStep[0m  [20/42], [94mLoss[0m : 2.63889
[1mStep[0m  [24/42], [94mLoss[0m : 2.53619
[1mStep[0m  [28/42], [94mLoss[0m : 2.72252
[1mStep[0m  [32/42], [94mLoss[0m : 2.74531
[1mStep[0m  [36/42], [94mLoss[0m : 2.44231
[1mStep[0m  [40/42], [94mLoss[0m : 2.49756

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.550, [92mTest[0m: 2.479, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.65268
[1mStep[0m  [4/42], [94mLoss[0m : 2.58051
[1mStep[0m  [8/42], [94mLoss[0m : 2.18769
[1mStep[0m  [12/42], [94mLoss[0m : 2.45316
[1mStep[0m  [16/42], [94mLoss[0m : 2.73878
[1mStep[0m  [20/42], [94mLoss[0m : 2.57605
[1mStep[0m  [24/42], [94mLoss[0m : 2.35662
[1mStep[0m  [28/42], [94mLoss[0m : 2.72553
[1mStep[0m  [32/42], [94mLoss[0m : 2.62743
[1mStep[0m  [36/42], [94mLoss[0m : 2.43051
[1mStep[0m  [40/42], [94mLoss[0m : 2.50098

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.510, [92mTest[0m: 2.424, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34311
[1mStep[0m  [4/42], [94mLoss[0m : 2.37518
[1mStep[0m  [8/42], [94mLoss[0m : 2.56398
[1mStep[0m  [12/42], [94mLoss[0m : 2.46873
[1mStep[0m  [16/42], [94mLoss[0m : 2.55811
[1mStep[0m  [20/42], [94mLoss[0m : 2.44214
[1mStep[0m  [24/42], [94mLoss[0m : 2.48167
[1mStep[0m  [28/42], [94mLoss[0m : 2.34795
[1mStep[0m  [32/42], [94mLoss[0m : 2.63917
[1mStep[0m  [36/42], [94mLoss[0m : 2.37984
[1mStep[0m  [40/42], [94mLoss[0m : 2.43442

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.410, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49892
[1mStep[0m  [4/42], [94mLoss[0m : 2.46130
[1mStep[0m  [8/42], [94mLoss[0m : 2.48699
[1mStep[0m  [12/42], [94mLoss[0m : 2.38972
[1mStep[0m  [16/42], [94mLoss[0m : 2.22375
[1mStep[0m  [20/42], [94mLoss[0m : 2.34349
[1mStep[0m  [24/42], [94mLoss[0m : 2.58577
[1mStep[0m  [28/42], [94mLoss[0m : 2.56269
[1mStep[0m  [32/42], [94mLoss[0m : 2.66738
[1mStep[0m  [36/42], [94mLoss[0m : 2.35125
[1mStep[0m  [40/42], [94mLoss[0m : 2.57102

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.390, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52149
[1mStep[0m  [4/42], [94mLoss[0m : 2.70436
[1mStep[0m  [8/42], [94mLoss[0m : 2.55617
[1mStep[0m  [12/42], [94mLoss[0m : 2.47721
[1mStep[0m  [16/42], [94mLoss[0m : 2.53698
[1mStep[0m  [20/42], [94mLoss[0m : 2.56296
[1mStep[0m  [24/42], [94mLoss[0m : 2.53516
[1mStep[0m  [28/42], [94mLoss[0m : 2.53539
[1mStep[0m  [32/42], [94mLoss[0m : 2.43131
[1mStep[0m  [36/42], [94mLoss[0m : 2.35820
[1mStep[0m  [40/42], [94mLoss[0m : 2.47597

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.382, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49655
[1mStep[0m  [4/42], [94mLoss[0m : 2.48327
[1mStep[0m  [8/42], [94mLoss[0m : 2.51845
[1mStep[0m  [12/42], [94mLoss[0m : 2.47960
[1mStep[0m  [16/42], [94mLoss[0m : 2.52485
[1mStep[0m  [20/42], [94mLoss[0m : 2.27042
[1mStep[0m  [24/42], [94mLoss[0m : 2.50592
[1mStep[0m  [28/42], [94mLoss[0m : 2.73340
[1mStep[0m  [32/42], [94mLoss[0m : 2.52146
[1mStep[0m  [36/42], [94mLoss[0m : 2.42012
[1mStep[0m  [40/42], [94mLoss[0m : 2.42236

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.380, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56188
[1mStep[0m  [4/42], [94mLoss[0m : 2.26656
[1mStep[0m  [8/42], [94mLoss[0m : 2.14447
[1mStep[0m  [12/42], [94mLoss[0m : 2.70071
[1mStep[0m  [16/42], [94mLoss[0m : 2.47182
[1mStep[0m  [20/42], [94mLoss[0m : 2.60381
[1mStep[0m  [24/42], [94mLoss[0m : 2.76000
[1mStep[0m  [28/42], [94mLoss[0m : 2.45385
[1mStep[0m  [32/42], [94mLoss[0m : 2.47609
[1mStep[0m  [36/42], [94mLoss[0m : 2.28752
[1mStep[0m  [40/42], [94mLoss[0m : 2.46156

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.375, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52572
[1mStep[0m  [4/42], [94mLoss[0m : 2.36574
[1mStep[0m  [8/42], [94mLoss[0m : 2.39528
[1mStep[0m  [12/42], [94mLoss[0m : 2.45240
[1mStep[0m  [16/42], [94mLoss[0m : 2.27250
[1mStep[0m  [20/42], [94mLoss[0m : 2.44714
[1mStep[0m  [24/42], [94mLoss[0m : 2.37606
[1mStep[0m  [28/42], [94mLoss[0m : 2.74654
[1mStep[0m  [32/42], [94mLoss[0m : 2.49471
[1mStep[0m  [36/42], [94mLoss[0m : 2.49458
[1mStep[0m  [40/42], [94mLoss[0m : 2.49399

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.358, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64404
[1mStep[0m  [4/42], [94mLoss[0m : 2.43810
[1mStep[0m  [8/42], [94mLoss[0m : 2.55956
[1mStep[0m  [12/42], [94mLoss[0m : 2.43230
[1mStep[0m  [16/42], [94mLoss[0m : 2.48825
[1mStep[0m  [20/42], [94mLoss[0m : 2.40616
[1mStep[0m  [24/42], [94mLoss[0m : 2.68595
[1mStep[0m  [28/42], [94mLoss[0m : 2.41283
[1mStep[0m  [32/42], [94mLoss[0m : 2.39672
[1mStep[0m  [36/42], [94mLoss[0m : 2.48891
[1mStep[0m  [40/42], [94mLoss[0m : 2.71792

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.364, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57405
[1mStep[0m  [4/42], [94mLoss[0m : 2.48751
[1mStep[0m  [8/42], [94mLoss[0m : 2.49016
[1mStep[0m  [12/42], [94mLoss[0m : 2.50775
[1mStep[0m  [16/42], [94mLoss[0m : 2.52907
[1mStep[0m  [20/42], [94mLoss[0m : 2.37990
[1mStep[0m  [24/42], [94mLoss[0m : 2.39570
[1mStep[0m  [28/42], [94mLoss[0m : 2.34042
[1mStep[0m  [32/42], [94mLoss[0m : 2.33113
[1mStep[0m  [36/42], [94mLoss[0m : 2.59178
[1mStep[0m  [40/42], [94mLoss[0m : 2.45128

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.363, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49190
[1mStep[0m  [4/42], [94mLoss[0m : 2.76469
[1mStep[0m  [8/42], [94mLoss[0m : 2.57577
[1mStep[0m  [12/42], [94mLoss[0m : 2.46382
[1mStep[0m  [16/42], [94mLoss[0m : 2.49516
[1mStep[0m  [20/42], [94mLoss[0m : 2.44904
[1mStep[0m  [24/42], [94mLoss[0m : 2.35180
[1mStep[0m  [28/42], [94mLoss[0m : 2.81357
[1mStep[0m  [32/42], [94mLoss[0m : 2.41941
[1mStep[0m  [36/42], [94mLoss[0m : 2.41205
[1mStep[0m  [40/42], [94mLoss[0m : 2.33740

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.352, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39371
[1mStep[0m  [4/42], [94mLoss[0m : 2.32328
[1mStep[0m  [8/42], [94mLoss[0m : 2.39731
[1mStep[0m  [12/42], [94mLoss[0m : 2.38788
[1mStep[0m  [16/42], [94mLoss[0m : 2.51826
[1mStep[0m  [20/42], [94mLoss[0m : 2.61285
[1mStep[0m  [24/42], [94mLoss[0m : 2.48487
[1mStep[0m  [28/42], [94mLoss[0m : 2.79365
[1mStep[0m  [32/42], [94mLoss[0m : 2.53897
[1mStep[0m  [36/42], [94mLoss[0m : 2.51448
[1mStep[0m  [40/42], [94mLoss[0m : 2.27911

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.352, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48948
[1mStep[0m  [4/42], [94mLoss[0m : 2.64897
[1mStep[0m  [8/42], [94mLoss[0m : 2.45845
[1mStep[0m  [12/42], [94mLoss[0m : 2.47623
[1mStep[0m  [16/42], [94mLoss[0m : 2.25934
[1mStep[0m  [20/42], [94mLoss[0m : 2.40665
[1mStep[0m  [24/42], [94mLoss[0m : 2.26518
[1mStep[0m  [28/42], [94mLoss[0m : 2.32538
[1mStep[0m  [32/42], [94mLoss[0m : 2.67863
[1mStep[0m  [36/42], [94mLoss[0m : 2.39082
[1mStep[0m  [40/42], [94mLoss[0m : 2.61772

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.344, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28733
[1mStep[0m  [4/42], [94mLoss[0m : 2.57493
[1mStep[0m  [8/42], [94mLoss[0m : 2.40007
[1mStep[0m  [12/42], [94mLoss[0m : 2.37014
[1mStep[0m  [16/42], [94mLoss[0m : 2.45713
[1mStep[0m  [20/42], [94mLoss[0m : 2.31622
[1mStep[0m  [24/42], [94mLoss[0m : 2.38140
[1mStep[0m  [28/42], [94mLoss[0m : 2.57273
[1mStep[0m  [32/42], [94mLoss[0m : 2.58362
[1mStep[0m  [36/42], [94mLoss[0m : 2.58556
[1mStep[0m  [40/42], [94mLoss[0m : 2.37684

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.343, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45042
[1mStep[0m  [4/42], [94mLoss[0m : 2.57280
[1mStep[0m  [8/42], [94mLoss[0m : 2.46848
[1mStep[0m  [12/42], [94mLoss[0m : 2.47906
[1mStep[0m  [16/42], [94mLoss[0m : 2.44461
[1mStep[0m  [20/42], [94mLoss[0m : 2.46704
[1mStep[0m  [24/42], [94mLoss[0m : 2.43138
[1mStep[0m  [28/42], [94mLoss[0m : 2.46987
[1mStep[0m  [32/42], [94mLoss[0m : 2.54350
[1mStep[0m  [36/42], [94mLoss[0m : 2.47368
[1mStep[0m  [40/42], [94mLoss[0m : 2.50744

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.343, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49748
[1mStep[0m  [4/42], [94mLoss[0m : 2.27537
[1mStep[0m  [8/42], [94mLoss[0m : 2.28485
[1mStep[0m  [12/42], [94mLoss[0m : 2.31882
[1mStep[0m  [16/42], [94mLoss[0m : 2.71388
[1mStep[0m  [20/42], [94mLoss[0m : 2.53480
[1mStep[0m  [24/42], [94mLoss[0m : 2.47049
[1mStep[0m  [28/42], [94mLoss[0m : 2.40765
[1mStep[0m  [32/42], [94mLoss[0m : 2.59926
[1mStep[0m  [36/42], [94mLoss[0m : 2.68423
[1mStep[0m  [40/42], [94mLoss[0m : 2.37384

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.349, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43310
[1mStep[0m  [4/42], [94mLoss[0m : 2.46651
[1mStep[0m  [8/42], [94mLoss[0m : 2.38228
[1mStep[0m  [12/42], [94mLoss[0m : 2.56169
[1mStep[0m  [16/42], [94mLoss[0m : 2.24004
[1mStep[0m  [20/42], [94mLoss[0m : 2.42260
[1mStep[0m  [24/42], [94mLoss[0m : 2.49247
[1mStep[0m  [28/42], [94mLoss[0m : 2.74077
[1mStep[0m  [32/42], [94mLoss[0m : 2.52094
[1mStep[0m  [36/42], [94mLoss[0m : 2.39545
[1mStep[0m  [40/42], [94mLoss[0m : 2.30817

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.343, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46533
[1mStep[0m  [4/42], [94mLoss[0m : 2.49905
[1mStep[0m  [8/42], [94mLoss[0m : 2.49078
[1mStep[0m  [12/42], [94mLoss[0m : 2.59689
[1mStep[0m  [16/42], [94mLoss[0m : 2.48907
[1mStep[0m  [20/42], [94mLoss[0m : 2.29670
[1mStep[0m  [24/42], [94mLoss[0m : 2.54292
[1mStep[0m  [28/42], [94mLoss[0m : 2.50337
[1mStep[0m  [32/42], [94mLoss[0m : 2.36089
[1mStep[0m  [36/42], [94mLoss[0m : 2.49174
[1mStep[0m  [40/42], [94mLoss[0m : 2.48110

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.348, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43816
[1mStep[0m  [4/42], [94mLoss[0m : 2.39081
[1mStep[0m  [8/42], [94mLoss[0m : 2.48681
[1mStep[0m  [12/42], [94mLoss[0m : 2.31572
[1mStep[0m  [16/42], [94mLoss[0m : 2.73462
[1mStep[0m  [20/42], [94mLoss[0m : 2.41019
[1mStep[0m  [24/42], [94mLoss[0m : 2.42284
[1mStep[0m  [28/42], [94mLoss[0m : 2.35020
[1mStep[0m  [32/42], [94mLoss[0m : 2.52733
[1mStep[0m  [36/42], [94mLoss[0m : 2.61638
[1mStep[0m  [40/42], [94mLoss[0m : 2.42760

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.338, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59561
[1mStep[0m  [4/42], [94mLoss[0m : 2.36643
[1mStep[0m  [8/42], [94mLoss[0m : 2.41028
[1mStep[0m  [12/42], [94mLoss[0m : 2.31289
[1mStep[0m  [16/42], [94mLoss[0m : 2.26234
[1mStep[0m  [20/42], [94mLoss[0m : 2.19957
[1mStep[0m  [24/42], [94mLoss[0m : 2.35119
[1mStep[0m  [28/42], [94mLoss[0m : 2.49473
[1mStep[0m  [32/42], [94mLoss[0m : 2.62380
[1mStep[0m  [36/42], [94mLoss[0m : 2.45280
[1mStep[0m  [40/42], [94mLoss[0m : 2.33466

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.334, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47844
[1mStep[0m  [4/42], [94mLoss[0m : 2.51739
[1mStep[0m  [8/42], [94mLoss[0m : 2.27825
[1mStep[0m  [12/42], [94mLoss[0m : 2.57853
[1mStep[0m  [16/42], [94mLoss[0m : 2.55877
[1mStep[0m  [20/42], [94mLoss[0m : 2.55579
[1mStep[0m  [24/42], [94mLoss[0m : 2.28606
[1mStep[0m  [28/42], [94mLoss[0m : 2.62361
[1mStep[0m  [32/42], [94mLoss[0m : 2.44303
[1mStep[0m  [36/42], [94mLoss[0m : 2.45285
[1mStep[0m  [40/42], [94mLoss[0m : 2.39756

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.336, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38240
[1mStep[0m  [4/42], [94mLoss[0m : 2.51317
[1mStep[0m  [8/42], [94mLoss[0m : 2.33294
[1mStep[0m  [12/42], [94mLoss[0m : 2.15656
[1mStep[0m  [16/42], [94mLoss[0m : 2.49249
[1mStep[0m  [20/42], [94mLoss[0m : 2.41265
[1mStep[0m  [24/42], [94mLoss[0m : 2.38783
[1mStep[0m  [28/42], [94mLoss[0m : 2.40433
[1mStep[0m  [32/42], [94mLoss[0m : 2.56322
[1mStep[0m  [36/42], [94mLoss[0m : 2.30779
[1mStep[0m  [40/42], [94mLoss[0m : 2.35430

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.337, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62347
[1mStep[0m  [4/42], [94mLoss[0m : 2.28784
[1mStep[0m  [8/42], [94mLoss[0m : 2.47180
[1mStep[0m  [12/42], [94mLoss[0m : 2.55970
[1mStep[0m  [16/42], [94mLoss[0m : 2.39804
[1mStep[0m  [20/42], [94mLoss[0m : 2.56868
[1mStep[0m  [24/42], [94mLoss[0m : 2.21986
[1mStep[0m  [28/42], [94mLoss[0m : 2.49473
[1mStep[0m  [32/42], [94mLoss[0m : 2.40287
[1mStep[0m  [36/42], [94mLoss[0m : 2.33435
[1mStep[0m  [40/42], [94mLoss[0m : 2.62465

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.329, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49502
[1mStep[0m  [4/42], [94mLoss[0m : 2.63144
[1mStep[0m  [8/42], [94mLoss[0m : 2.58880
[1mStep[0m  [12/42], [94mLoss[0m : 2.57043
[1mStep[0m  [16/42], [94mLoss[0m : 2.54261
[1mStep[0m  [20/42], [94mLoss[0m : 2.44169
[1mStep[0m  [24/42], [94mLoss[0m : 2.45629
[1mStep[0m  [28/42], [94mLoss[0m : 2.45294
[1mStep[0m  [32/42], [94mLoss[0m : 2.38311
[1mStep[0m  [36/42], [94mLoss[0m : 2.33131
[1mStep[0m  [40/42], [94mLoss[0m : 2.64234

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.337, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58132
[1mStep[0m  [4/42], [94mLoss[0m : 2.41780
[1mStep[0m  [8/42], [94mLoss[0m : 2.34603
[1mStep[0m  [12/42], [94mLoss[0m : 2.54872
[1mStep[0m  [16/42], [94mLoss[0m : 2.57910
[1mStep[0m  [20/42], [94mLoss[0m : 2.29346
[1mStep[0m  [24/42], [94mLoss[0m : 2.36126
[1mStep[0m  [28/42], [94mLoss[0m : 2.34290
[1mStep[0m  [32/42], [94mLoss[0m : 2.44660
[1mStep[0m  [36/42], [94mLoss[0m : 2.40136
[1mStep[0m  [40/42], [94mLoss[0m : 2.42728

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.332, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59711
[1mStep[0m  [4/42], [94mLoss[0m : 2.47551
[1mStep[0m  [8/42], [94mLoss[0m : 2.39145
[1mStep[0m  [12/42], [94mLoss[0m : 2.49120
[1mStep[0m  [16/42], [94mLoss[0m : 2.56852
[1mStep[0m  [20/42], [94mLoss[0m : 2.28764
[1mStep[0m  [24/42], [94mLoss[0m : 2.40107
[1mStep[0m  [28/42], [94mLoss[0m : 2.32347
[1mStep[0m  [32/42], [94mLoss[0m : 2.64258
[1mStep[0m  [36/42], [94mLoss[0m : 2.45630
[1mStep[0m  [40/42], [94mLoss[0m : 2.42539

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.327, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41272
[1mStep[0m  [4/42], [94mLoss[0m : 2.37412
[1mStep[0m  [8/42], [94mLoss[0m : 2.64944
[1mStep[0m  [12/42], [94mLoss[0m : 2.04394
[1mStep[0m  [16/42], [94mLoss[0m : 2.48449
[1mStep[0m  [20/42], [94mLoss[0m : 2.20950
[1mStep[0m  [24/42], [94mLoss[0m : 2.59187
[1mStep[0m  [28/42], [94mLoss[0m : 2.45182
[1mStep[0m  [32/42], [94mLoss[0m : 2.28177
[1mStep[0m  [36/42], [94mLoss[0m : 2.43802
[1mStep[0m  [40/42], [94mLoss[0m : 2.45098

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.337, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46399
[1mStep[0m  [4/42], [94mLoss[0m : 2.35537
[1mStep[0m  [8/42], [94mLoss[0m : 2.44727
[1mStep[0m  [12/42], [94mLoss[0m : 2.55694
[1mStep[0m  [16/42], [94mLoss[0m : 2.33927
[1mStep[0m  [20/42], [94mLoss[0m : 2.36876
[1mStep[0m  [24/42], [94mLoss[0m : 2.54510
[1mStep[0m  [28/42], [94mLoss[0m : 2.46394
[1mStep[0m  [32/42], [94mLoss[0m : 2.44984
[1mStep[0m  [36/42], [94mLoss[0m : 2.35672
[1mStep[0m  [40/42], [94mLoss[0m : 2.41286

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.327, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.332
====================================

Phase 1 - Evaluation MAE:  2.3322817598070418
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 2.36917
[1mStep[0m  [4/42], [94mLoss[0m : 2.37044
[1mStep[0m  [8/42], [94mLoss[0m : 2.47367
[1mStep[0m  [12/42], [94mLoss[0m : 2.40671
[1mStep[0m  [16/42], [94mLoss[0m : 2.46376
[1mStep[0m  [20/42], [94mLoss[0m : 2.61134
[1mStep[0m  [24/42], [94mLoss[0m : 2.50532
[1mStep[0m  [28/42], [94mLoss[0m : 2.39678
[1mStep[0m  [32/42], [94mLoss[0m : 2.37501
[1mStep[0m  [36/42], [94mLoss[0m : 2.47546
[1mStep[0m  [40/42], [94mLoss[0m : 2.45905

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.335, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42435
[1mStep[0m  [4/42], [94mLoss[0m : 2.62406
[1mStep[0m  [8/42], [94mLoss[0m : 2.50529
[1mStep[0m  [12/42], [94mLoss[0m : 2.53393
[1mStep[0m  [16/42], [94mLoss[0m : 2.43056
[1mStep[0m  [20/42], [94mLoss[0m : 2.51915
[1mStep[0m  [24/42], [94mLoss[0m : 2.63990
[1mStep[0m  [28/42], [94mLoss[0m : 2.79384
[1mStep[0m  [32/42], [94mLoss[0m : 2.24726
[1mStep[0m  [36/42], [94mLoss[0m : 2.42827
[1mStep[0m  [40/42], [94mLoss[0m : 2.56254

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.330, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.20724
[1mStep[0m  [4/42], [94mLoss[0m : 2.33060
[1mStep[0m  [8/42], [94mLoss[0m : 2.49413
[1mStep[0m  [12/42], [94mLoss[0m : 2.34474
[1mStep[0m  [16/42], [94mLoss[0m : 2.39166
[1mStep[0m  [20/42], [94mLoss[0m : 2.19838
[1mStep[0m  [24/42], [94mLoss[0m : 2.16252
[1mStep[0m  [28/42], [94mLoss[0m : 2.25996
[1mStep[0m  [32/42], [94mLoss[0m : 2.53118
[1mStep[0m  [36/42], [94mLoss[0m : 2.48212
[1mStep[0m  [40/42], [94mLoss[0m : 2.38592

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.441, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33891
[1mStep[0m  [4/42], [94mLoss[0m : 2.39296
[1mStep[0m  [8/42], [94mLoss[0m : 2.19074
[1mStep[0m  [12/42], [94mLoss[0m : 2.32269
[1mStep[0m  [16/42], [94mLoss[0m : 2.44699
[1mStep[0m  [20/42], [94mLoss[0m : 2.27502
[1mStep[0m  [24/42], [94mLoss[0m : 2.27589
[1mStep[0m  [28/42], [94mLoss[0m : 2.27279
[1mStep[0m  [32/42], [94mLoss[0m : 2.49548
[1mStep[0m  [36/42], [94mLoss[0m : 2.40352
[1mStep[0m  [40/42], [94mLoss[0m : 2.40577

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.395, [92mTest[0m: 2.488, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40235
[1mStep[0m  [4/42], [94mLoss[0m : 2.13498
[1mStep[0m  [8/42], [94mLoss[0m : 2.43377
[1mStep[0m  [12/42], [94mLoss[0m : 2.35024
[1mStep[0m  [16/42], [94mLoss[0m : 2.55955
[1mStep[0m  [20/42], [94mLoss[0m : 2.31495
[1mStep[0m  [24/42], [94mLoss[0m : 2.54533
[1mStep[0m  [28/42], [94mLoss[0m : 2.22070
[1mStep[0m  [32/42], [94mLoss[0m : 2.43037
[1mStep[0m  [36/42], [94mLoss[0m : 2.18476
[1mStep[0m  [40/42], [94mLoss[0m : 2.35039

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.379, [92mTest[0m: 2.525, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24587
[1mStep[0m  [4/42], [94mLoss[0m : 2.32347
[1mStep[0m  [8/42], [94mLoss[0m : 2.39351
[1mStep[0m  [12/42], [94mLoss[0m : 2.39572
[1mStep[0m  [16/42], [94mLoss[0m : 2.51506
[1mStep[0m  [20/42], [94mLoss[0m : 2.27738
[1mStep[0m  [24/42], [94mLoss[0m : 2.53865
[1mStep[0m  [28/42], [94mLoss[0m : 2.39789
[1mStep[0m  [32/42], [94mLoss[0m : 2.14619
[1mStep[0m  [36/42], [94mLoss[0m : 2.35747
[1mStep[0m  [40/42], [94mLoss[0m : 2.31616

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.357, [92mTest[0m: 2.517, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42170
[1mStep[0m  [4/42], [94mLoss[0m : 2.13227
[1mStep[0m  [8/42], [94mLoss[0m : 2.36855
[1mStep[0m  [12/42], [94mLoss[0m : 2.36684
[1mStep[0m  [16/42], [94mLoss[0m : 2.36544
[1mStep[0m  [20/42], [94mLoss[0m : 2.48542
[1mStep[0m  [24/42], [94mLoss[0m : 2.52868
[1mStep[0m  [28/42], [94mLoss[0m : 2.42686
[1mStep[0m  [32/42], [94mLoss[0m : 2.11172
[1mStep[0m  [36/42], [94mLoss[0m : 2.57931
[1mStep[0m  [40/42], [94mLoss[0m : 2.18524

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.327, [92mTest[0m: 2.557, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34022
[1mStep[0m  [4/42], [94mLoss[0m : 2.27292
[1mStep[0m  [8/42], [94mLoss[0m : 2.38174
[1mStep[0m  [12/42], [94mLoss[0m : 2.47526
[1mStep[0m  [16/42], [94mLoss[0m : 2.19550
[1mStep[0m  [20/42], [94mLoss[0m : 2.38059
[1mStep[0m  [24/42], [94mLoss[0m : 2.32648
[1mStep[0m  [28/42], [94mLoss[0m : 2.44097
[1mStep[0m  [32/42], [94mLoss[0m : 2.31395
[1mStep[0m  [36/42], [94mLoss[0m : 2.31438
[1mStep[0m  [40/42], [94mLoss[0m : 2.15997

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.309, [92mTest[0m: 2.572, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.11548
[1mStep[0m  [4/42], [94mLoss[0m : 2.37044
[1mStep[0m  [8/42], [94mLoss[0m : 2.48690
[1mStep[0m  [12/42], [94mLoss[0m : 2.09948
[1mStep[0m  [16/42], [94mLoss[0m : 2.19354
[1mStep[0m  [20/42], [94mLoss[0m : 2.31911
[1mStep[0m  [24/42], [94mLoss[0m : 2.25008
[1mStep[0m  [28/42], [94mLoss[0m : 2.26003
[1mStep[0m  [32/42], [94mLoss[0m : 2.25211
[1mStep[0m  [36/42], [94mLoss[0m : 2.21852
[1mStep[0m  [40/42], [94mLoss[0m : 2.36562

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.282, [92mTest[0m: 2.590, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.14981
[1mStep[0m  [4/42], [94mLoss[0m : 2.20386
[1mStep[0m  [8/42], [94mLoss[0m : 2.20694
[1mStep[0m  [12/42], [94mLoss[0m : 2.08911
[1mStep[0m  [16/42], [94mLoss[0m : 2.26471
[1mStep[0m  [20/42], [94mLoss[0m : 2.36405
[1mStep[0m  [24/42], [94mLoss[0m : 2.29345
[1mStep[0m  [28/42], [94mLoss[0m : 2.28033
[1mStep[0m  [32/42], [94mLoss[0m : 2.18734
[1mStep[0m  [36/42], [94mLoss[0m : 2.40339
[1mStep[0m  [40/42], [94mLoss[0m : 2.27568

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.252, [92mTest[0m: 2.544, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.13706
[1mStep[0m  [4/42], [94mLoss[0m : 2.28789
[1mStep[0m  [8/42], [94mLoss[0m : 2.43272
[1mStep[0m  [12/42], [94mLoss[0m : 2.32184
[1mStep[0m  [16/42], [94mLoss[0m : 2.38330
[1mStep[0m  [20/42], [94mLoss[0m : 2.15345
[1mStep[0m  [24/42], [94mLoss[0m : 2.02617
[1mStep[0m  [28/42], [94mLoss[0m : 2.16232
[1mStep[0m  [32/42], [94mLoss[0m : 2.24228
[1mStep[0m  [36/42], [94mLoss[0m : 2.19755
[1mStep[0m  [40/42], [94mLoss[0m : 2.27957

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.236, [92mTest[0m: 2.526, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29297
[1mStep[0m  [4/42], [94mLoss[0m : 2.33221
[1mStep[0m  [8/42], [94mLoss[0m : 2.22891
[1mStep[0m  [12/42], [94mLoss[0m : 2.12610
[1mStep[0m  [16/42], [94mLoss[0m : 2.32563
[1mStep[0m  [20/42], [94mLoss[0m : 2.29677
[1mStep[0m  [24/42], [94mLoss[0m : 2.11673
[1mStep[0m  [28/42], [94mLoss[0m : 2.19461
[1mStep[0m  [32/42], [94mLoss[0m : 2.30406
[1mStep[0m  [36/42], [94mLoss[0m : 2.46987
[1mStep[0m  [40/42], [94mLoss[0m : 2.08284

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.208, [92mTest[0m: 2.548, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.23564
[1mStep[0m  [4/42], [94mLoss[0m : 2.27815
[1mStep[0m  [8/42], [94mLoss[0m : 2.22662
[1mStep[0m  [12/42], [94mLoss[0m : 2.26254
[1mStep[0m  [16/42], [94mLoss[0m : 2.48916
[1mStep[0m  [20/42], [94mLoss[0m : 2.27488
[1mStep[0m  [24/42], [94mLoss[0m : 2.36303
[1mStep[0m  [28/42], [94mLoss[0m : 2.24694
[1mStep[0m  [32/42], [94mLoss[0m : 2.17935
[1mStep[0m  [36/42], [94mLoss[0m : 1.97404
[1mStep[0m  [40/42], [94mLoss[0m : 2.27350

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.175, [92mTest[0m: 2.542, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.20832
[1mStep[0m  [4/42], [94mLoss[0m : 2.19171
[1mStep[0m  [8/42], [94mLoss[0m : 2.21213
[1mStep[0m  [12/42], [94mLoss[0m : 2.22235
[1mStep[0m  [16/42], [94mLoss[0m : 2.29354
[1mStep[0m  [20/42], [94mLoss[0m : 2.09631
[1mStep[0m  [24/42], [94mLoss[0m : 2.19412
[1mStep[0m  [28/42], [94mLoss[0m : 2.11642
[1mStep[0m  [32/42], [94mLoss[0m : 2.02192
[1mStep[0m  [36/42], [94mLoss[0m : 2.03797
[1mStep[0m  [40/42], [94mLoss[0m : 2.21835

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.138, [92mTest[0m: 2.549, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.92189
[1mStep[0m  [4/42], [94mLoss[0m : 1.92675
[1mStep[0m  [8/42], [94mLoss[0m : 2.00902
[1mStep[0m  [12/42], [94mLoss[0m : 2.01940
[1mStep[0m  [16/42], [94mLoss[0m : 2.21229
[1mStep[0m  [20/42], [94mLoss[0m : 2.08881
[1mStep[0m  [24/42], [94mLoss[0m : 2.08572
[1mStep[0m  [28/42], [94mLoss[0m : 1.98372
[1mStep[0m  [32/42], [94mLoss[0m : 2.10054
[1mStep[0m  [36/42], [94mLoss[0m : 2.04865
[1mStep[0m  [40/42], [94mLoss[0m : 2.10866

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.108, [92mTest[0m: 2.563, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.97575
[1mStep[0m  [4/42], [94mLoss[0m : 2.25590
[1mStep[0m  [8/42], [94mLoss[0m : 2.04538
[1mStep[0m  [12/42], [94mLoss[0m : 1.95151
[1mStep[0m  [16/42], [94mLoss[0m : 2.07749
[1mStep[0m  [20/42], [94mLoss[0m : 2.20480
[1mStep[0m  [24/42], [94mLoss[0m : 2.05254
[1mStep[0m  [28/42], [94mLoss[0m : 2.07573
[1mStep[0m  [32/42], [94mLoss[0m : 1.94659
[1mStep[0m  [36/42], [94mLoss[0m : 2.18471
[1mStep[0m  [40/42], [94mLoss[0m : 2.11908

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.084, [92mTest[0m: 2.592, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.94228
[1mStep[0m  [4/42], [94mLoss[0m : 2.01008
[1mStep[0m  [8/42], [94mLoss[0m : 2.11045
[1mStep[0m  [12/42], [94mLoss[0m : 2.15019
[1mStep[0m  [16/42], [94mLoss[0m : 2.07239
[1mStep[0m  [20/42], [94mLoss[0m : 1.99136
[1mStep[0m  [24/42], [94mLoss[0m : 2.07096
[1mStep[0m  [28/42], [94mLoss[0m : 2.07690
[1mStep[0m  [32/42], [94mLoss[0m : 1.95971
[1mStep[0m  [36/42], [94mLoss[0m : 2.09078
[1mStep[0m  [40/42], [94mLoss[0m : 2.19754

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.037, [92mTest[0m: 2.537, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.02284
[1mStep[0m  [4/42], [94mLoss[0m : 2.04512
[1mStep[0m  [8/42], [94mLoss[0m : 2.08122
[1mStep[0m  [12/42], [94mLoss[0m : 1.96556
[1mStep[0m  [16/42], [94mLoss[0m : 1.81652
[1mStep[0m  [20/42], [94mLoss[0m : 2.11473
[1mStep[0m  [24/42], [94mLoss[0m : 2.06353
[1mStep[0m  [28/42], [94mLoss[0m : 1.86081
[1mStep[0m  [32/42], [94mLoss[0m : 2.25719
[1mStep[0m  [36/42], [94mLoss[0m : 2.08569
[1mStep[0m  [40/42], [94mLoss[0m : 2.16846

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.004, [92mTest[0m: 2.521, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.01675
[1mStep[0m  [4/42], [94mLoss[0m : 1.86962
[1mStep[0m  [8/42], [94mLoss[0m : 1.97459
[1mStep[0m  [12/42], [94mLoss[0m : 2.01516
[1mStep[0m  [16/42], [94mLoss[0m : 1.88939
[1mStep[0m  [20/42], [94mLoss[0m : 1.95592
[1mStep[0m  [24/42], [94mLoss[0m : 1.67064
[1mStep[0m  [28/42], [94mLoss[0m : 1.82843
[1mStep[0m  [32/42], [94mLoss[0m : 2.13006
[1mStep[0m  [36/42], [94mLoss[0m : 1.96907
[1mStep[0m  [40/42], [94mLoss[0m : 2.04860

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.957, [92mTest[0m: 2.628, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.85587
[1mStep[0m  [4/42], [94mLoss[0m : 1.88010
[1mStep[0m  [8/42], [94mLoss[0m : 2.06493
[1mStep[0m  [12/42], [94mLoss[0m : 2.00649
[1mStep[0m  [16/42], [94mLoss[0m : 1.88291
[1mStep[0m  [20/42], [94mLoss[0m : 1.95648
[1mStep[0m  [24/42], [94mLoss[0m : 1.91299
[1mStep[0m  [28/42], [94mLoss[0m : 1.80052
[1mStep[0m  [32/42], [94mLoss[0m : 2.00560
[1mStep[0m  [36/42], [94mLoss[0m : 1.91316
[1mStep[0m  [40/42], [94mLoss[0m : 1.99306

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.920, [92mTest[0m: 2.541, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.82225
[1mStep[0m  [4/42], [94mLoss[0m : 1.69264
[1mStep[0m  [8/42], [94mLoss[0m : 1.83547
[1mStep[0m  [12/42], [94mLoss[0m : 2.25824
[1mStep[0m  [16/42], [94mLoss[0m : 1.76679
[1mStep[0m  [20/42], [94mLoss[0m : 1.86195
[1mStep[0m  [24/42], [94mLoss[0m : 1.98755
[1mStep[0m  [28/42], [94mLoss[0m : 1.82065
[1mStep[0m  [32/42], [94mLoss[0m : 1.94398
[1mStep[0m  [36/42], [94mLoss[0m : 1.83573
[1mStep[0m  [40/42], [94mLoss[0m : 1.76092

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.892, [92mTest[0m: 2.758, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.85990
[1mStep[0m  [4/42], [94mLoss[0m : 1.86732
[1mStep[0m  [8/42], [94mLoss[0m : 1.85498
[1mStep[0m  [12/42], [94mLoss[0m : 1.72039
[1mStep[0m  [16/42], [94mLoss[0m : 1.79111
[1mStep[0m  [20/42], [94mLoss[0m : 1.86619
[1mStep[0m  [24/42], [94mLoss[0m : 1.82863
[1mStep[0m  [28/42], [94mLoss[0m : 1.69575
[1mStep[0m  [32/42], [94mLoss[0m : 2.05465
[1mStep[0m  [36/42], [94mLoss[0m : 2.00909
[1mStep[0m  [40/42], [94mLoss[0m : 1.81324

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.882, [92mTest[0m: 2.504, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.87575
[1mStep[0m  [4/42], [94mLoss[0m : 2.02012
[1mStep[0m  [8/42], [94mLoss[0m : 1.83508
[1mStep[0m  [12/42], [94mLoss[0m : 1.96524
[1mStep[0m  [16/42], [94mLoss[0m : 1.80954
[1mStep[0m  [20/42], [94mLoss[0m : 1.86164
[1mStep[0m  [24/42], [94mLoss[0m : 1.85029
[1mStep[0m  [28/42], [94mLoss[0m : 2.01945
[1mStep[0m  [32/42], [94mLoss[0m : 1.81231
[1mStep[0m  [36/42], [94mLoss[0m : 1.84976
[1mStep[0m  [40/42], [94mLoss[0m : 1.77724

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.846, [92mTest[0m: 2.486, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.73465
[1mStep[0m  [4/42], [94mLoss[0m : 1.78609
[1mStep[0m  [8/42], [94mLoss[0m : 1.68401
[1mStep[0m  [12/42], [94mLoss[0m : 1.83329
[1mStep[0m  [16/42], [94mLoss[0m : 1.74315
[1mStep[0m  [20/42], [94mLoss[0m : 1.81399
[1mStep[0m  [24/42], [94mLoss[0m : 1.74749
[1mStep[0m  [28/42], [94mLoss[0m : 1.93782
[1mStep[0m  [32/42], [94mLoss[0m : 1.74611
[1mStep[0m  [36/42], [94mLoss[0m : 1.87103
[1mStep[0m  [40/42], [94mLoss[0m : 1.86075

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.806, [92mTest[0m: 2.543, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.85344
[1mStep[0m  [4/42], [94mLoss[0m : 1.69399
[1mStep[0m  [8/42], [94mLoss[0m : 1.82670
[1mStep[0m  [12/42], [94mLoss[0m : 1.70114
[1mStep[0m  [16/42], [94mLoss[0m : 1.80966
[1mStep[0m  [20/42], [94mLoss[0m : 1.80343
[1mStep[0m  [24/42], [94mLoss[0m : 1.75122
[1mStep[0m  [28/42], [94mLoss[0m : 2.04112
[1mStep[0m  [32/42], [94mLoss[0m : 1.80142
[1mStep[0m  [36/42], [94mLoss[0m : 1.86525
[1mStep[0m  [40/42], [94mLoss[0m : 1.81789

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.790, [92mTest[0m: 2.518, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.80291
[1mStep[0m  [4/42], [94mLoss[0m : 1.53387
[1mStep[0m  [8/42], [94mLoss[0m : 1.91377
[1mStep[0m  [12/42], [94mLoss[0m : 1.74103
[1mStep[0m  [16/42], [94mLoss[0m : 1.63770
[1mStep[0m  [20/42], [94mLoss[0m : 1.93024
[1mStep[0m  [24/42], [94mLoss[0m : 2.01521
[1mStep[0m  [28/42], [94mLoss[0m : 1.78467
[1mStep[0m  [32/42], [94mLoss[0m : 1.83596
[1mStep[0m  [36/42], [94mLoss[0m : 1.67882
[1mStep[0m  [40/42], [94mLoss[0m : 1.84923

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.760, [92mTest[0m: 2.463, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.73090
[1mStep[0m  [4/42], [94mLoss[0m : 1.62540
[1mStep[0m  [8/42], [94mLoss[0m : 1.58629
[1mStep[0m  [12/42], [94mLoss[0m : 1.62833
[1mStep[0m  [16/42], [94mLoss[0m : 1.69090
[1mStep[0m  [20/42], [94mLoss[0m : 1.87420
[1mStep[0m  [24/42], [94mLoss[0m : 1.82659
[1mStep[0m  [28/42], [94mLoss[0m : 1.68766
[1mStep[0m  [32/42], [94mLoss[0m : 1.73380
[1mStep[0m  [36/42], [94mLoss[0m : 1.89879
[1mStep[0m  [40/42], [94mLoss[0m : 1.81911

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.738, [92mTest[0m: 2.526, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.95374
[1mStep[0m  [4/42], [94mLoss[0m : 1.53860
[1mStep[0m  [8/42], [94mLoss[0m : 1.75716
[1mStep[0m  [12/42], [94mLoss[0m : 1.60206
[1mStep[0m  [16/42], [94mLoss[0m : 1.66884
[1mStep[0m  [20/42], [94mLoss[0m : 1.60536
[1mStep[0m  [24/42], [94mLoss[0m : 1.73900
[1mStep[0m  [28/42], [94mLoss[0m : 1.57481
[1mStep[0m  [32/42], [94mLoss[0m : 1.68326
[1mStep[0m  [36/42], [94mLoss[0m : 1.84532
[1mStep[0m  [40/42], [94mLoss[0m : 1.66805

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.718, [92mTest[0m: 2.485, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.68555
[1mStep[0m  [4/42], [94mLoss[0m : 1.59233
[1mStep[0m  [8/42], [94mLoss[0m : 1.88891
[1mStep[0m  [12/42], [94mLoss[0m : 1.63047
[1mStep[0m  [16/42], [94mLoss[0m : 1.74375
[1mStep[0m  [20/42], [94mLoss[0m : 1.66823
[1mStep[0m  [24/42], [94mLoss[0m : 1.75377
[1mStep[0m  [28/42], [94mLoss[0m : 1.75741
[1mStep[0m  [32/42], [94mLoss[0m : 1.70064
[1mStep[0m  [36/42], [94mLoss[0m : 1.59139
[1mStep[0m  [40/42], [94mLoss[0m : 1.68193

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.689, [92mTest[0m: 2.555, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.74344
[1mStep[0m  [4/42], [94mLoss[0m : 1.71441
[1mStep[0m  [8/42], [94mLoss[0m : 1.66339
[1mStep[0m  [12/42], [94mLoss[0m : 1.67329
[1mStep[0m  [16/42], [94mLoss[0m : 1.62598
[1mStep[0m  [20/42], [94mLoss[0m : 1.38743
[1mStep[0m  [24/42], [94mLoss[0m : 1.74993
[1mStep[0m  [28/42], [94mLoss[0m : 1.70546
[1mStep[0m  [32/42], [94mLoss[0m : 1.66841
[1mStep[0m  [36/42], [94mLoss[0m : 1.89138
[1mStep[0m  [40/42], [94mLoss[0m : 1.76005

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.673, [92mTest[0m: 2.660, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.561
====================================

Phase 2 - Evaluation MAE:  2.561463338988168
MAE score P1        2.332282
MAE score P2        2.561463
loss                1.673365
learning_rate       0.007525
batch_size               256
hidden_sizes      [300, 100]
epochs                    30
activation           sigmoid
optimizer                sgd
early stopping         False
dropout                  0.2
momentum                 0.1
weight_decay           0.001
Name: 3, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 11.10500
[1mStep[0m  [4/42], [94mLoss[0m : 10.92534
[1mStep[0m  [8/42], [94mLoss[0m : 10.63715
[1mStep[0m  [12/42], [94mLoss[0m : 10.41090
[1mStep[0m  [16/42], [94mLoss[0m : 10.29745
[1mStep[0m  [20/42], [94mLoss[0m : 10.78114
[1mStep[0m  [24/42], [94mLoss[0m : 10.39938
[1mStep[0m  [28/42], [94mLoss[0m : 10.65548
[1mStep[0m  [32/42], [94mLoss[0m : 10.54286
[1mStep[0m  [36/42], [94mLoss[0m : 10.22235
[1mStep[0m  [40/42], [94mLoss[0m : 10.50344

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.604, [92mTest[0m: 10.844, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.19988
[1mStep[0m  [4/42], [94mLoss[0m : 10.08173
[1mStep[0m  [8/42], [94mLoss[0m : 9.94748
[1mStep[0m  [12/42], [94mLoss[0m : 10.42758
[1mStep[0m  [16/42], [94mLoss[0m : 10.21732
[1mStep[0m  [20/42], [94mLoss[0m : 9.54481
[1mStep[0m  [24/42], [94mLoss[0m : 9.85145
[1mStep[0m  [28/42], [94mLoss[0m : 9.94477
[1mStep[0m  [32/42], [94mLoss[0m : 9.45605
[1mStep[0m  [36/42], [94mLoss[0m : 9.77677
[1mStep[0m  [40/42], [94mLoss[0m : 9.74480

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.970, [92mTest[0m: 10.139, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.57649
[1mStep[0m  [4/42], [94mLoss[0m : 9.09873
[1mStep[0m  [8/42], [94mLoss[0m : 9.52975
[1mStep[0m  [12/42], [94mLoss[0m : 9.30264
[1mStep[0m  [16/42], [94mLoss[0m : 9.35788
[1mStep[0m  [20/42], [94mLoss[0m : 9.26307
[1mStep[0m  [24/42], [94mLoss[0m : 9.40554
[1mStep[0m  [28/42], [94mLoss[0m : 9.16586
[1mStep[0m  [32/42], [94mLoss[0m : 9.23800
[1mStep[0m  [36/42], [94mLoss[0m : 9.02790
[1mStep[0m  [40/42], [94mLoss[0m : 9.06529

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.224, [92mTest[0m: 9.213, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.85940
[1mStep[0m  [4/42], [94mLoss[0m : 8.47356
[1mStep[0m  [8/42], [94mLoss[0m : 8.62650
[1mStep[0m  [12/42], [94mLoss[0m : 8.69949
[1mStep[0m  [16/42], [94mLoss[0m : 8.91448
[1mStep[0m  [20/42], [94mLoss[0m : 8.31582
[1mStep[0m  [24/42], [94mLoss[0m : 8.28268
[1mStep[0m  [28/42], [94mLoss[0m : 8.14469
[1mStep[0m  [32/42], [94mLoss[0m : 8.08230
[1mStep[0m  [36/42], [94mLoss[0m : 7.73413
[1mStep[0m  [40/42], [94mLoss[0m : 7.75969

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 8.374, [92mTest[0m: 8.092, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.02804
[1mStep[0m  [4/42], [94mLoss[0m : 8.07506
[1mStep[0m  [8/42], [94mLoss[0m : 7.81802
[1mStep[0m  [12/42], [94mLoss[0m : 7.55227
[1mStep[0m  [16/42], [94mLoss[0m : 7.59362
[1mStep[0m  [20/42], [94mLoss[0m : 7.45263
[1mStep[0m  [24/42], [94mLoss[0m : 7.69909
[1mStep[0m  [28/42], [94mLoss[0m : 7.28332
[1mStep[0m  [32/42], [94mLoss[0m : 7.37537
[1mStep[0m  [36/42], [94mLoss[0m : 7.39381
[1mStep[0m  [40/42], [94mLoss[0m : 7.40416

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 7.583, [92mTest[0m: 7.171, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.79107
[1mStep[0m  [4/42], [94mLoss[0m : 7.16405
[1mStep[0m  [8/42], [94mLoss[0m : 6.98643
[1mStep[0m  [12/42], [94mLoss[0m : 6.95901
[1mStep[0m  [16/42], [94mLoss[0m : 6.75233
[1mStep[0m  [20/42], [94mLoss[0m : 6.90267
[1mStep[0m  [24/42], [94mLoss[0m : 6.99201
[1mStep[0m  [28/42], [94mLoss[0m : 7.06570
[1mStep[0m  [32/42], [94mLoss[0m : 6.61521
[1mStep[0m  [36/42], [94mLoss[0m : 6.50790
[1mStep[0m  [40/42], [94mLoss[0m : 6.46286

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 6.887, [92mTest[0m: 6.591, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.45591
[1mStep[0m  [4/42], [94mLoss[0m : 6.11593
[1mStep[0m  [8/42], [94mLoss[0m : 6.32480
[1mStep[0m  [12/42], [94mLoss[0m : 6.12119
[1mStep[0m  [16/42], [94mLoss[0m : 6.07660
[1mStep[0m  [20/42], [94mLoss[0m : 6.26704
[1mStep[0m  [24/42], [94mLoss[0m : 6.09136
[1mStep[0m  [28/42], [94mLoss[0m : 6.36710
[1mStep[0m  [32/42], [94mLoss[0m : 5.86767
[1mStep[0m  [36/42], [94mLoss[0m : 5.85264
[1mStep[0m  [40/42], [94mLoss[0m : 6.29364

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 6.233, [92mTest[0m: 5.670, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.22164
[1mStep[0m  [4/42], [94mLoss[0m : 5.99562
[1mStep[0m  [8/42], [94mLoss[0m : 5.82273
[1mStep[0m  [12/42], [94mLoss[0m : 5.58100
[1mStep[0m  [16/42], [94mLoss[0m : 5.43300
[1mStep[0m  [20/42], [94mLoss[0m : 5.40428
[1mStep[0m  [24/42], [94mLoss[0m : 5.39115
[1mStep[0m  [28/42], [94mLoss[0m : 5.23084
[1mStep[0m  [32/42], [94mLoss[0m : 5.00785
[1mStep[0m  [36/42], [94mLoss[0m : 5.09157
[1mStep[0m  [40/42], [94mLoss[0m : 5.16943

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 5.488, [92mTest[0m: 4.928, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.31519
[1mStep[0m  [4/42], [94mLoss[0m : 4.84346
[1mStep[0m  [8/42], [94mLoss[0m : 5.04563
[1mStep[0m  [12/42], [94mLoss[0m : 5.00810
[1mStep[0m  [16/42], [94mLoss[0m : 4.90284
[1mStep[0m  [20/42], [94mLoss[0m : 4.96001
[1mStep[0m  [24/42], [94mLoss[0m : 4.31999
[1mStep[0m  [28/42], [94mLoss[0m : 4.67762
[1mStep[0m  [32/42], [94mLoss[0m : 4.37697
[1mStep[0m  [36/42], [94mLoss[0m : 4.69930
[1mStep[0m  [40/42], [94mLoss[0m : 4.40393

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 4.697, [92mTest[0m: 4.051, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.20840
[1mStep[0m  [4/42], [94mLoss[0m : 4.22279
[1mStep[0m  [8/42], [94mLoss[0m : 4.15335
[1mStep[0m  [12/42], [94mLoss[0m : 4.15171
[1mStep[0m  [16/42], [94mLoss[0m : 3.92295
[1mStep[0m  [20/42], [94mLoss[0m : 3.88805
[1mStep[0m  [24/42], [94mLoss[0m : 3.79582
[1mStep[0m  [28/42], [94mLoss[0m : 3.74421
[1mStep[0m  [32/42], [94mLoss[0m : 3.47907
[1mStep[0m  [36/42], [94mLoss[0m : 3.40173
[1mStep[0m  [40/42], [94mLoss[0m : 3.31216

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 3.848, [92mTest[0m: 3.349, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.17677
[1mStep[0m  [4/42], [94mLoss[0m : 3.01254
[1mStep[0m  [8/42], [94mLoss[0m : 3.31655
[1mStep[0m  [12/42], [94mLoss[0m : 3.49671
[1mStep[0m  [16/42], [94mLoss[0m : 2.97172
[1mStep[0m  [20/42], [94mLoss[0m : 3.29947
[1mStep[0m  [24/42], [94mLoss[0m : 2.95853
[1mStep[0m  [28/42], [94mLoss[0m : 3.20224
[1mStep[0m  [32/42], [94mLoss[0m : 3.01080
[1mStep[0m  [36/42], [94mLoss[0m : 2.99926
[1mStep[0m  [40/42], [94mLoss[0m : 3.00504

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 3.112, [92mTest[0m: 2.771, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.87278
[1mStep[0m  [4/42], [94mLoss[0m : 2.80019
[1mStep[0m  [8/42], [94mLoss[0m : 2.84303
[1mStep[0m  [12/42], [94mLoss[0m : 2.36162
[1mStep[0m  [16/42], [94mLoss[0m : 2.89366
[1mStep[0m  [20/42], [94mLoss[0m : 2.60855
[1mStep[0m  [24/42], [94mLoss[0m : 2.81039
[1mStep[0m  [28/42], [94mLoss[0m : 2.52552
[1mStep[0m  [32/42], [94mLoss[0m : 2.44656
[1mStep[0m  [36/42], [94mLoss[0m : 2.46464
[1mStep[0m  [40/42], [94mLoss[0m : 2.74154

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.727, [92mTest[0m: 2.437, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.92099
[1mStep[0m  [4/42], [94mLoss[0m : 2.62119
[1mStep[0m  [8/42], [94mLoss[0m : 2.30935
[1mStep[0m  [12/42], [94mLoss[0m : 2.58754
[1mStep[0m  [16/42], [94mLoss[0m : 2.44810
[1mStep[0m  [20/42], [94mLoss[0m : 2.52620
[1mStep[0m  [24/42], [94mLoss[0m : 2.44081
[1mStep[0m  [28/42], [94mLoss[0m : 2.67722
[1mStep[0m  [32/42], [94mLoss[0m : 2.47963
[1mStep[0m  [36/42], [94mLoss[0m : 2.45705
[1mStep[0m  [40/42], [94mLoss[0m : 2.41211

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.620, [92mTest[0m: 2.402, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58522
[1mStep[0m  [4/42], [94mLoss[0m : 2.76919
[1mStep[0m  [8/42], [94mLoss[0m : 2.69912
[1mStep[0m  [12/42], [94mLoss[0m : 2.57805
[1mStep[0m  [16/42], [94mLoss[0m : 2.38068
[1mStep[0m  [20/42], [94mLoss[0m : 2.57429
[1mStep[0m  [24/42], [94mLoss[0m : 2.60992
[1mStep[0m  [28/42], [94mLoss[0m : 2.64136
[1mStep[0m  [32/42], [94mLoss[0m : 2.33526
[1mStep[0m  [36/42], [94mLoss[0m : 2.55231
[1mStep[0m  [40/42], [94mLoss[0m : 2.62681

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.556, [92mTest[0m: 2.408, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.63884
[1mStep[0m  [4/42], [94mLoss[0m : 2.73100
[1mStep[0m  [8/42], [94mLoss[0m : 2.67983
[1mStep[0m  [12/42], [94mLoss[0m : 2.60715
[1mStep[0m  [16/42], [94mLoss[0m : 2.73582
[1mStep[0m  [20/42], [94mLoss[0m : 2.26343
[1mStep[0m  [24/42], [94mLoss[0m : 2.57919
[1mStep[0m  [28/42], [94mLoss[0m : 2.59581
[1mStep[0m  [32/42], [94mLoss[0m : 2.42134
[1mStep[0m  [36/42], [94mLoss[0m : 2.42638
[1mStep[0m  [40/42], [94mLoss[0m : 2.61264

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.557, [92mTest[0m: 2.425, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59217
[1mStep[0m  [4/42], [94mLoss[0m : 2.50352
[1mStep[0m  [8/42], [94mLoss[0m : 2.49031
[1mStep[0m  [12/42], [94mLoss[0m : 2.69894
[1mStep[0m  [16/42], [94mLoss[0m : 2.62545
[1mStep[0m  [20/42], [94mLoss[0m : 2.41985
[1mStep[0m  [24/42], [94mLoss[0m : 2.31989
[1mStep[0m  [28/42], [94mLoss[0m : 2.64351
[1mStep[0m  [32/42], [94mLoss[0m : 2.47647
[1mStep[0m  [36/42], [94mLoss[0m : 2.51381
[1mStep[0m  [40/42], [94mLoss[0m : 2.63662

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.539, [92mTest[0m: 2.371, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57862
[1mStep[0m  [4/42], [94mLoss[0m : 2.49589
[1mStep[0m  [8/42], [94mLoss[0m : 2.46459
[1mStep[0m  [12/42], [94mLoss[0m : 2.32730
[1mStep[0m  [16/42], [94mLoss[0m : 2.57560
[1mStep[0m  [20/42], [94mLoss[0m : 2.51576
[1mStep[0m  [24/42], [94mLoss[0m : 2.53275
[1mStep[0m  [28/42], [94mLoss[0m : 2.56321
[1mStep[0m  [32/42], [94mLoss[0m : 2.52746
[1mStep[0m  [36/42], [94mLoss[0m : 2.54475
[1mStep[0m  [40/42], [94mLoss[0m : 2.57760

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.526, [92mTest[0m: 2.432, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57989
[1mStep[0m  [4/42], [94mLoss[0m : 2.36834
[1mStep[0m  [8/42], [94mLoss[0m : 2.44291
[1mStep[0m  [12/42], [94mLoss[0m : 2.55041
[1mStep[0m  [16/42], [94mLoss[0m : 2.68990
[1mStep[0m  [20/42], [94mLoss[0m : 2.50274
[1mStep[0m  [24/42], [94mLoss[0m : 2.38921
[1mStep[0m  [28/42], [94mLoss[0m : 2.80628
[1mStep[0m  [32/42], [94mLoss[0m : 2.60832
[1mStep[0m  [36/42], [94mLoss[0m : 2.47850
[1mStep[0m  [40/42], [94mLoss[0m : 2.56356

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.423, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62338
[1mStep[0m  [4/42], [94mLoss[0m : 2.56334
[1mStep[0m  [8/42], [94mLoss[0m : 2.53962
[1mStep[0m  [12/42], [94mLoss[0m : 2.62430
[1mStep[0m  [16/42], [94mLoss[0m : 2.37641
[1mStep[0m  [20/42], [94mLoss[0m : 2.47192
[1mStep[0m  [24/42], [94mLoss[0m : 2.35820
[1mStep[0m  [28/42], [94mLoss[0m : 2.29034
[1mStep[0m  [32/42], [94mLoss[0m : 2.59473
[1mStep[0m  [36/42], [94mLoss[0m : 2.58805
[1mStep[0m  [40/42], [94mLoss[0m : 2.51461

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.515, [92mTest[0m: 2.383, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60183
[1mStep[0m  [4/42], [94mLoss[0m : 2.37534
[1mStep[0m  [8/42], [94mLoss[0m : 2.21812
[1mStep[0m  [12/42], [94mLoss[0m : 2.45392
[1mStep[0m  [16/42], [94mLoss[0m : 2.39164
[1mStep[0m  [20/42], [94mLoss[0m : 2.60037
[1mStep[0m  [24/42], [94mLoss[0m : 2.57750
[1mStep[0m  [28/42], [94mLoss[0m : 2.51724
[1mStep[0m  [32/42], [94mLoss[0m : 2.64423
[1mStep[0m  [36/42], [94mLoss[0m : 2.60907
[1mStep[0m  [40/42], [94mLoss[0m : 2.44254

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.368, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53600
[1mStep[0m  [4/42], [94mLoss[0m : 2.31777
[1mStep[0m  [8/42], [94mLoss[0m : 2.55114
[1mStep[0m  [12/42], [94mLoss[0m : 2.73492
[1mStep[0m  [16/42], [94mLoss[0m : 2.50420
[1mStep[0m  [20/42], [94mLoss[0m : 2.41373
[1mStep[0m  [24/42], [94mLoss[0m : 2.73436
[1mStep[0m  [28/42], [94mLoss[0m : 2.55391
[1mStep[0m  [32/42], [94mLoss[0m : 2.62615
[1mStep[0m  [36/42], [94mLoss[0m : 2.54310
[1mStep[0m  [40/42], [94mLoss[0m : 2.53602

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.381, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53572
[1mStep[0m  [4/42], [94mLoss[0m : 2.57032
[1mStep[0m  [8/42], [94mLoss[0m : 2.67414
[1mStep[0m  [12/42], [94mLoss[0m : 2.44364
[1mStep[0m  [16/42], [94mLoss[0m : 2.50394
[1mStep[0m  [20/42], [94mLoss[0m : 2.67584
[1mStep[0m  [24/42], [94mLoss[0m : 2.64629
[1mStep[0m  [28/42], [94mLoss[0m : 2.59111
[1mStep[0m  [32/42], [94mLoss[0m : 2.35722
[1mStep[0m  [36/42], [94mLoss[0m : 2.42056
[1mStep[0m  [40/42], [94mLoss[0m : 2.53643

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.364, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39762
[1mStep[0m  [4/42], [94mLoss[0m : 2.69914
[1mStep[0m  [8/42], [94mLoss[0m : 2.25757
[1mStep[0m  [12/42], [94mLoss[0m : 2.61461
[1mStep[0m  [16/42], [94mLoss[0m : 2.41840
[1mStep[0m  [20/42], [94mLoss[0m : 2.45342
[1mStep[0m  [24/42], [94mLoss[0m : 2.32056
[1mStep[0m  [28/42], [94mLoss[0m : 2.30442
[1mStep[0m  [32/42], [94mLoss[0m : 2.54095
[1mStep[0m  [36/42], [94mLoss[0m : 2.37308
[1mStep[0m  [40/42], [94mLoss[0m : 2.45529

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.377, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55231
[1mStep[0m  [4/42], [94mLoss[0m : 2.39051
[1mStep[0m  [8/42], [94mLoss[0m : 2.35305
[1mStep[0m  [12/42], [94mLoss[0m : 2.42811
[1mStep[0m  [16/42], [94mLoss[0m : 2.44137
[1mStep[0m  [20/42], [94mLoss[0m : 2.44183
[1mStep[0m  [24/42], [94mLoss[0m : 2.50232
[1mStep[0m  [28/42], [94mLoss[0m : 2.35966
[1mStep[0m  [32/42], [94mLoss[0m : 2.45104
[1mStep[0m  [36/42], [94mLoss[0m : 2.56614
[1mStep[0m  [40/42], [94mLoss[0m : 2.30983

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.369, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58944
[1mStep[0m  [4/42], [94mLoss[0m : 2.52127
[1mStep[0m  [8/42], [94mLoss[0m : 2.57685
[1mStep[0m  [12/42], [94mLoss[0m : 2.59484
[1mStep[0m  [16/42], [94mLoss[0m : 2.51304
[1mStep[0m  [20/42], [94mLoss[0m : 2.47163
[1mStep[0m  [24/42], [94mLoss[0m : 2.69371
[1mStep[0m  [28/42], [94mLoss[0m : 2.41654
[1mStep[0m  [32/42], [94mLoss[0m : 2.49723
[1mStep[0m  [36/42], [94mLoss[0m : 2.49605
[1mStep[0m  [40/42], [94mLoss[0m : 2.50642

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.377, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41655
[1mStep[0m  [4/42], [94mLoss[0m : 2.32407
[1mStep[0m  [8/42], [94mLoss[0m : 2.29037
[1mStep[0m  [12/42], [94mLoss[0m : 2.72796
[1mStep[0m  [16/42], [94mLoss[0m : 2.56009
[1mStep[0m  [20/42], [94mLoss[0m : 2.53898
[1mStep[0m  [24/42], [94mLoss[0m : 2.68264
[1mStep[0m  [28/42], [94mLoss[0m : 2.37051
[1mStep[0m  [32/42], [94mLoss[0m : 2.37052
[1mStep[0m  [36/42], [94mLoss[0m : 2.48675
[1mStep[0m  [40/42], [94mLoss[0m : 2.60389

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.398, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.23406
[1mStep[0m  [4/42], [94mLoss[0m : 2.49946
[1mStep[0m  [8/42], [94mLoss[0m : 2.49453
[1mStep[0m  [12/42], [94mLoss[0m : 2.43547
[1mStep[0m  [16/42], [94mLoss[0m : 2.29026
[1mStep[0m  [20/42], [94mLoss[0m : 2.47764
[1mStep[0m  [24/42], [94mLoss[0m : 2.32255
[1mStep[0m  [28/42], [94mLoss[0m : 2.53963
[1mStep[0m  [32/42], [94mLoss[0m : 2.49299
[1mStep[0m  [36/42], [94mLoss[0m : 2.29683
[1mStep[0m  [40/42], [94mLoss[0m : 2.55620

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.357, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43764
[1mStep[0m  [4/42], [94mLoss[0m : 2.43472
[1mStep[0m  [8/42], [94mLoss[0m : 2.43897
[1mStep[0m  [12/42], [94mLoss[0m : 2.52700
[1mStep[0m  [16/42], [94mLoss[0m : 2.42514
[1mStep[0m  [20/42], [94mLoss[0m : 2.65056
[1mStep[0m  [24/42], [94mLoss[0m : 2.49416
[1mStep[0m  [28/42], [94mLoss[0m : 2.45331
[1mStep[0m  [32/42], [94mLoss[0m : 2.44907
[1mStep[0m  [36/42], [94mLoss[0m : 2.55793
[1mStep[0m  [40/42], [94mLoss[0m : 2.41165

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.388, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39387
[1mStep[0m  [4/42], [94mLoss[0m : 2.34996
[1mStep[0m  [8/42], [94mLoss[0m : 2.51839
[1mStep[0m  [12/42], [94mLoss[0m : 2.61694
[1mStep[0m  [16/42], [94mLoss[0m : 2.38744
[1mStep[0m  [20/42], [94mLoss[0m : 2.39663
[1mStep[0m  [24/42], [94mLoss[0m : 2.61759
[1mStep[0m  [28/42], [94mLoss[0m : 2.41208
[1mStep[0m  [32/42], [94mLoss[0m : 2.50227
[1mStep[0m  [36/42], [94mLoss[0m : 2.32716
[1mStep[0m  [40/42], [94mLoss[0m : 2.47858

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.345, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44416
[1mStep[0m  [4/42], [94mLoss[0m : 2.31939
[1mStep[0m  [8/42], [94mLoss[0m : 2.48195
[1mStep[0m  [12/42], [94mLoss[0m : 2.58178
[1mStep[0m  [16/42], [94mLoss[0m : 2.43913
[1mStep[0m  [20/42], [94mLoss[0m : 2.59751
[1mStep[0m  [24/42], [94mLoss[0m : 2.32370
[1mStep[0m  [28/42], [94mLoss[0m : 2.27377
[1mStep[0m  [32/42], [94mLoss[0m : 2.28104
[1mStep[0m  [36/42], [94mLoss[0m : 2.26582
[1mStep[0m  [40/42], [94mLoss[0m : 2.44230

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.405, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.373
====================================

Phase 1 - Evaluation MAE:  2.3729117597852434
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 2.30392
[1mStep[0m  [4/42], [94mLoss[0m : 2.68647
[1mStep[0m  [8/42], [94mLoss[0m : 2.60017
[1mStep[0m  [12/42], [94mLoss[0m : 2.49959
[1mStep[0m  [16/42], [94mLoss[0m : 2.54779
[1mStep[0m  [20/42], [94mLoss[0m : 2.49788
[1mStep[0m  [24/42], [94mLoss[0m : 2.57441
[1mStep[0m  [28/42], [94mLoss[0m : 2.48662
[1mStep[0m  [32/42], [94mLoss[0m : 2.39908
[1mStep[0m  [36/42], [94mLoss[0m : 2.54301
[1mStep[0m  [40/42], [94mLoss[0m : 2.50377

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.510, [92mTest[0m: 2.374, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.70353
[1mStep[0m  [4/42], [94mLoss[0m : 2.42968
[1mStep[0m  [8/42], [94mLoss[0m : 2.53493
[1mStep[0m  [12/42], [94mLoss[0m : 2.40869
[1mStep[0m  [16/42], [94mLoss[0m : 2.67203
[1mStep[0m  [20/42], [94mLoss[0m : 2.49849
[1mStep[0m  [24/42], [94mLoss[0m : 2.54119
[1mStep[0m  [28/42], [94mLoss[0m : 2.45279
[1mStep[0m  [32/42], [94mLoss[0m : 2.42600
[1mStep[0m  [36/42], [94mLoss[0m : 2.42781
[1mStep[0m  [40/42], [94mLoss[0m : 2.41709

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.339, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60642
[1mStep[0m  [4/42], [94mLoss[0m : 2.31104
[1mStep[0m  [8/42], [94mLoss[0m : 2.42242
[1mStep[0m  [12/42], [94mLoss[0m : 2.27100
[1mStep[0m  [16/42], [94mLoss[0m : 2.34527
[1mStep[0m  [20/42], [94mLoss[0m : 2.56324
[1mStep[0m  [24/42], [94mLoss[0m : 2.34744
[1mStep[0m  [28/42], [94mLoss[0m : 2.65361
[1mStep[0m  [32/42], [94mLoss[0m : 2.46603
[1mStep[0m  [36/42], [94mLoss[0m : 2.60811
[1mStep[0m  [40/42], [94mLoss[0m : 2.27116

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.453, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50267
[1mStep[0m  [4/42], [94mLoss[0m : 2.17395
[1mStep[0m  [8/42], [94mLoss[0m : 2.39326
[1mStep[0m  [12/42], [94mLoss[0m : 2.35140
[1mStep[0m  [16/42], [94mLoss[0m : 2.64197
[1mStep[0m  [20/42], [94mLoss[0m : 2.47997
[1mStep[0m  [24/42], [94mLoss[0m : 2.46497
[1mStep[0m  [28/42], [94mLoss[0m : 2.52379
[1mStep[0m  [32/42], [94mLoss[0m : 2.44023
[1mStep[0m  [36/42], [94mLoss[0m : 2.30143
[1mStep[0m  [40/42], [94mLoss[0m : 2.44474

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.552, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44994
[1mStep[0m  [4/42], [94mLoss[0m : 2.41144
[1mStep[0m  [8/42], [94mLoss[0m : 2.41471
[1mStep[0m  [12/42], [94mLoss[0m : 2.15219
[1mStep[0m  [16/42], [94mLoss[0m : 2.28270
[1mStep[0m  [20/42], [94mLoss[0m : 2.38992
[1mStep[0m  [24/42], [94mLoss[0m : 2.41526
[1mStep[0m  [28/42], [94mLoss[0m : 2.51257
[1mStep[0m  [32/42], [94mLoss[0m : 2.28005
[1mStep[0m  [36/42], [94mLoss[0m : 2.26897
[1mStep[0m  [40/42], [94mLoss[0m : 2.24259

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.366, [92mTest[0m: 2.573, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28353
[1mStep[0m  [4/42], [94mLoss[0m : 2.28957
[1mStep[0m  [8/42], [94mLoss[0m : 2.37267
[1mStep[0m  [12/42], [94mLoss[0m : 2.20611
[1mStep[0m  [16/42], [94mLoss[0m : 2.46902
[1mStep[0m  [20/42], [94mLoss[0m : 2.31143
[1mStep[0m  [24/42], [94mLoss[0m : 2.28646
[1mStep[0m  [28/42], [94mLoss[0m : 2.33074
[1mStep[0m  [32/42], [94mLoss[0m : 2.25484
[1mStep[0m  [36/42], [94mLoss[0m : 2.31606
[1mStep[0m  [40/42], [94mLoss[0m : 2.27511

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.327, [92mTest[0m: 2.556, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.23308
[1mStep[0m  [4/42], [94mLoss[0m : 2.33513
[1mStep[0m  [8/42], [94mLoss[0m : 2.34753
[1mStep[0m  [12/42], [94mLoss[0m : 2.35756
[1mStep[0m  [16/42], [94mLoss[0m : 2.24519
[1mStep[0m  [20/42], [94mLoss[0m : 2.32515
[1mStep[0m  [24/42], [94mLoss[0m : 2.19506
[1mStep[0m  [28/42], [94mLoss[0m : 2.29402
[1mStep[0m  [32/42], [94mLoss[0m : 2.13260
[1mStep[0m  [36/42], [94mLoss[0m : 2.22068
[1mStep[0m  [40/42], [94mLoss[0m : 2.05206

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.286, [92mTest[0m: 2.551, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36004
[1mStep[0m  [4/42], [94mLoss[0m : 2.27262
[1mStep[0m  [8/42], [94mLoss[0m : 2.40448
[1mStep[0m  [12/42], [94mLoss[0m : 2.36115
[1mStep[0m  [16/42], [94mLoss[0m : 1.97603
[1mStep[0m  [20/42], [94mLoss[0m : 2.37447
[1mStep[0m  [24/42], [94mLoss[0m : 2.05891
[1mStep[0m  [28/42], [94mLoss[0m : 2.29694
[1mStep[0m  [32/42], [94mLoss[0m : 2.20167
[1mStep[0m  [36/42], [94mLoss[0m : 2.27847
[1mStep[0m  [40/42], [94mLoss[0m : 2.47303

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.266, [92mTest[0m: 2.648, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39762
[1mStep[0m  [4/42], [94mLoss[0m : 2.14507
[1mStep[0m  [8/42], [94mLoss[0m : 2.25377
[1mStep[0m  [12/42], [94mLoss[0m : 2.11975
[1mStep[0m  [16/42], [94mLoss[0m : 2.22241
[1mStep[0m  [20/42], [94mLoss[0m : 2.49313
[1mStep[0m  [24/42], [94mLoss[0m : 2.13424
[1mStep[0m  [28/42], [94mLoss[0m : 2.30370
[1mStep[0m  [32/42], [94mLoss[0m : 2.13502
[1mStep[0m  [36/42], [94mLoss[0m : 2.20534
[1mStep[0m  [40/42], [94mLoss[0m : 2.13458

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.214, [92mTest[0m: 2.536, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.08723
[1mStep[0m  [4/42], [94mLoss[0m : 2.21584
[1mStep[0m  [8/42], [94mLoss[0m : 2.20511
[1mStep[0m  [12/42], [94mLoss[0m : 2.34293
[1mStep[0m  [16/42], [94mLoss[0m : 2.01055
[1mStep[0m  [20/42], [94mLoss[0m : 2.20859
[1mStep[0m  [24/42], [94mLoss[0m : 2.19839
[1mStep[0m  [28/42], [94mLoss[0m : 2.17331
[1mStep[0m  [32/42], [94mLoss[0m : 2.15297
[1mStep[0m  [36/42], [94mLoss[0m : 2.05209
[1mStep[0m  [40/42], [94mLoss[0m : 2.12963

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.167, [92mTest[0m: 2.647, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.13469
[1mStep[0m  [4/42], [94mLoss[0m : 2.17424
[1mStep[0m  [8/42], [94mLoss[0m : 2.13358
[1mStep[0m  [12/42], [94mLoss[0m : 2.16017
[1mStep[0m  [16/42], [94mLoss[0m : 2.24330
[1mStep[0m  [20/42], [94mLoss[0m : 2.12418
[1mStep[0m  [24/42], [94mLoss[0m : 2.13630
[1mStep[0m  [28/42], [94mLoss[0m : 2.23640
[1mStep[0m  [32/42], [94mLoss[0m : 2.18163
[1mStep[0m  [36/42], [94mLoss[0m : 2.06612
[1mStep[0m  [40/42], [94mLoss[0m : 2.23704

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.149, [92mTest[0m: 2.700, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.11191
[1mStep[0m  [4/42], [94mLoss[0m : 1.97883
[1mStep[0m  [8/42], [94mLoss[0m : 2.07696
[1mStep[0m  [12/42], [94mLoss[0m : 2.10808
[1mStep[0m  [16/42], [94mLoss[0m : 2.08793
[1mStep[0m  [20/42], [94mLoss[0m : 2.13850
[1mStep[0m  [24/42], [94mLoss[0m : 2.22280
[1mStep[0m  [28/42], [94mLoss[0m : 2.14333
[1mStep[0m  [32/42], [94mLoss[0m : 2.38344
[1mStep[0m  [36/42], [94mLoss[0m : 2.06203
[1mStep[0m  [40/42], [94mLoss[0m : 2.29645

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.116, [92mTest[0m: 2.794, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.91245
[1mStep[0m  [4/42], [94mLoss[0m : 1.95836
[1mStep[0m  [8/42], [94mLoss[0m : 1.85423
[1mStep[0m  [12/42], [94mLoss[0m : 2.12491
[1mStep[0m  [16/42], [94mLoss[0m : 1.99997
[1mStep[0m  [20/42], [94mLoss[0m : 2.08301
[1mStep[0m  [24/42], [94mLoss[0m : 2.24034
[1mStep[0m  [28/42], [94mLoss[0m : 2.02378
[1mStep[0m  [32/42], [94mLoss[0m : 1.86030
[1mStep[0m  [36/42], [94mLoss[0m : 2.19189
[1mStep[0m  [40/42], [94mLoss[0m : 2.09188

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.060, [92mTest[0m: 2.698, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.03619
[1mStep[0m  [4/42], [94mLoss[0m : 2.01629
[1mStep[0m  [8/42], [94mLoss[0m : 2.06149
[1mStep[0m  [12/42], [94mLoss[0m : 1.92716
[1mStep[0m  [16/42], [94mLoss[0m : 1.95545
[1mStep[0m  [20/42], [94mLoss[0m : 2.08966
[1mStep[0m  [24/42], [94mLoss[0m : 2.02688
[1mStep[0m  [28/42], [94mLoss[0m : 2.21663
[1mStep[0m  [32/42], [94mLoss[0m : 2.10022
[1mStep[0m  [36/42], [94mLoss[0m : 2.20231
[1mStep[0m  [40/42], [94mLoss[0m : 2.06663

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.044, [92mTest[0m: 2.593, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.13112
[1mStep[0m  [4/42], [94mLoss[0m : 2.11843
[1mStep[0m  [8/42], [94mLoss[0m : 1.97749
[1mStep[0m  [12/42], [94mLoss[0m : 1.88251
[1mStep[0m  [16/42], [94mLoss[0m : 1.75648
[1mStep[0m  [20/42], [94mLoss[0m : 2.02107
[1mStep[0m  [24/42], [94mLoss[0m : 1.93511
[1mStep[0m  [28/42], [94mLoss[0m : 1.78544
[1mStep[0m  [32/42], [94mLoss[0m : 2.05098
[1mStep[0m  [36/42], [94mLoss[0m : 1.93141
[1mStep[0m  [40/42], [94mLoss[0m : 2.13564

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.993, [92mTest[0m: 2.584, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.96424
[1mStep[0m  [4/42], [94mLoss[0m : 1.90621
[1mStep[0m  [8/42], [94mLoss[0m : 2.13913
[1mStep[0m  [12/42], [94mLoss[0m : 1.81209
[1mStep[0m  [16/42], [94mLoss[0m : 1.97650
[1mStep[0m  [20/42], [94mLoss[0m : 1.86454
[1mStep[0m  [24/42], [94mLoss[0m : 1.99435
[1mStep[0m  [28/42], [94mLoss[0m : 1.87355
[1mStep[0m  [32/42], [94mLoss[0m : 1.98896
[1mStep[0m  [36/42], [94mLoss[0m : 1.89528
[1mStep[0m  [40/42], [94mLoss[0m : 1.96597

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.946, [92mTest[0m: 2.556, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.88432
[1mStep[0m  [4/42], [94mLoss[0m : 2.02634
[1mStep[0m  [8/42], [94mLoss[0m : 1.94518
[1mStep[0m  [12/42], [94mLoss[0m : 2.08626
[1mStep[0m  [16/42], [94mLoss[0m : 1.75992
[1mStep[0m  [20/42], [94mLoss[0m : 1.92917
[1mStep[0m  [24/42], [94mLoss[0m : 2.08945
[1mStep[0m  [28/42], [94mLoss[0m : 1.98159
[1mStep[0m  [32/42], [94mLoss[0m : 1.94174
[1mStep[0m  [36/42], [94mLoss[0m : 1.96060
[1mStep[0m  [40/42], [94mLoss[0m : 1.90990

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.933, [92mTest[0m: 2.647, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.90978
[1mStep[0m  [4/42], [94mLoss[0m : 1.68934
[1mStep[0m  [8/42], [94mLoss[0m : 1.84471
[1mStep[0m  [12/42], [94mLoss[0m : 1.90910
[1mStep[0m  [16/42], [94mLoss[0m : 1.78860
[1mStep[0m  [20/42], [94mLoss[0m : 1.81706
[1mStep[0m  [24/42], [94mLoss[0m : 1.87327
[1mStep[0m  [28/42], [94mLoss[0m : 1.87412
[1mStep[0m  [32/42], [94mLoss[0m : 2.04392
[1mStep[0m  [36/42], [94mLoss[0m : 1.98380
[1mStep[0m  [40/42], [94mLoss[0m : 2.10997

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.898, [92mTest[0m: 2.497, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.81528
[1mStep[0m  [4/42], [94mLoss[0m : 1.89575
[1mStep[0m  [8/42], [94mLoss[0m : 1.93991
[1mStep[0m  [12/42], [94mLoss[0m : 1.96462
[1mStep[0m  [16/42], [94mLoss[0m : 1.93895
[1mStep[0m  [20/42], [94mLoss[0m : 1.92236
[1mStep[0m  [24/42], [94mLoss[0m : 1.66960
[1mStep[0m  [28/42], [94mLoss[0m : 1.93006
[1mStep[0m  [32/42], [94mLoss[0m : 1.76173
[1mStep[0m  [36/42], [94mLoss[0m : 2.06244
[1mStep[0m  [40/42], [94mLoss[0m : 1.83572

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.860, [92mTest[0m: 2.580, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.87161
[1mStep[0m  [4/42], [94mLoss[0m : 1.74367
[1mStep[0m  [8/42], [94mLoss[0m : 1.85959
[1mStep[0m  [12/42], [94mLoss[0m : 1.78001
[1mStep[0m  [16/42], [94mLoss[0m : 1.79975
[1mStep[0m  [20/42], [94mLoss[0m : 1.83989
[1mStep[0m  [24/42], [94mLoss[0m : 1.90715
[1mStep[0m  [28/42], [94mLoss[0m : 1.71117
[1mStep[0m  [32/42], [94mLoss[0m : 1.92057
[1mStep[0m  [36/42], [94mLoss[0m : 1.95907
[1mStep[0m  [40/42], [94mLoss[0m : 1.77778

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.837, [92mTest[0m: 2.600, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.80001
[1mStep[0m  [4/42], [94mLoss[0m : 1.92801
[1mStep[0m  [8/42], [94mLoss[0m : 1.91453
[1mStep[0m  [12/42], [94mLoss[0m : 1.65313
[1mStep[0m  [16/42], [94mLoss[0m : 2.04500
[1mStep[0m  [20/42], [94mLoss[0m : 1.77416
[1mStep[0m  [24/42], [94mLoss[0m : 1.87794
[1mStep[0m  [28/42], [94mLoss[0m : 2.04387
[1mStep[0m  [32/42], [94mLoss[0m : 1.66936
[1mStep[0m  [36/42], [94mLoss[0m : 1.97310
[1mStep[0m  [40/42], [94mLoss[0m : 1.82436

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.808, [92mTest[0m: 2.690, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.60343
[1mStep[0m  [4/42], [94mLoss[0m : 1.83277
[1mStep[0m  [8/42], [94mLoss[0m : 1.48628
[1mStep[0m  [12/42], [94mLoss[0m : 2.03074
[1mStep[0m  [16/42], [94mLoss[0m : 1.81616
[1mStep[0m  [20/42], [94mLoss[0m : 1.50009
[1mStep[0m  [24/42], [94mLoss[0m : 1.82051
[1mStep[0m  [28/42], [94mLoss[0m : 1.72987
[1mStep[0m  [32/42], [94mLoss[0m : 1.61271
[1mStep[0m  [36/42], [94mLoss[0m : 1.87134
[1mStep[0m  [40/42], [94mLoss[0m : 1.77988

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.762, [92mTest[0m: 2.750, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.72720
[1mStep[0m  [4/42], [94mLoss[0m : 1.71935
[1mStep[0m  [8/42], [94mLoss[0m : 1.81035
[1mStep[0m  [12/42], [94mLoss[0m : 1.77453
[1mStep[0m  [16/42], [94mLoss[0m : 1.69549
[1mStep[0m  [20/42], [94mLoss[0m : 1.81424
[1mStep[0m  [24/42], [94mLoss[0m : 1.72799
[1mStep[0m  [28/42], [94mLoss[0m : 1.74274
[1mStep[0m  [32/42], [94mLoss[0m : 1.83477
[1mStep[0m  [36/42], [94mLoss[0m : 1.88144
[1mStep[0m  [40/42], [94mLoss[0m : 1.72720

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.764, [92mTest[0m: 2.678, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.63310
[1mStep[0m  [4/42], [94mLoss[0m : 1.58769
[1mStep[0m  [8/42], [94mLoss[0m : 1.59674
[1mStep[0m  [12/42], [94mLoss[0m : 1.72672
[1mStep[0m  [16/42], [94mLoss[0m : 1.67733
[1mStep[0m  [20/42], [94mLoss[0m : 1.71455
[1mStep[0m  [24/42], [94mLoss[0m : 1.75733
[1mStep[0m  [28/42], [94mLoss[0m : 1.74998
[1mStep[0m  [32/42], [94mLoss[0m : 1.88208
[1mStep[0m  [36/42], [94mLoss[0m : 1.59918
[1mStep[0m  [40/42], [94mLoss[0m : 1.67481

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.707, [92mTest[0m: 2.575, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.77202
[1mStep[0m  [4/42], [94mLoss[0m : 1.73697
[1mStep[0m  [8/42], [94mLoss[0m : 1.73016
[1mStep[0m  [12/42], [94mLoss[0m : 1.61395
[1mStep[0m  [16/42], [94mLoss[0m : 1.77346
[1mStep[0m  [20/42], [94mLoss[0m : 1.93701
[1mStep[0m  [24/42], [94mLoss[0m : 1.72760
[1mStep[0m  [28/42], [94mLoss[0m : 1.71363
[1mStep[0m  [32/42], [94mLoss[0m : 1.88969
[1mStep[0m  [36/42], [94mLoss[0m : 1.56032
[1mStep[0m  [40/42], [94mLoss[0m : 1.75811

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.705, [92mTest[0m: 2.621, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.49392
[1mStep[0m  [4/42], [94mLoss[0m : 1.49175
[1mStep[0m  [8/42], [94mLoss[0m : 1.65753
[1mStep[0m  [12/42], [94mLoss[0m : 1.79497
[1mStep[0m  [16/42], [94mLoss[0m : 1.58382
[1mStep[0m  [20/42], [94mLoss[0m : 1.64337
[1mStep[0m  [24/42], [94mLoss[0m : 1.85167
[1mStep[0m  [28/42], [94mLoss[0m : 1.80882
[1mStep[0m  [32/42], [94mLoss[0m : 1.63144
[1mStep[0m  [36/42], [94mLoss[0m : 1.65811
[1mStep[0m  [40/42], [94mLoss[0m : 1.58849

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.665, [92mTest[0m: 2.686, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.62799
[1mStep[0m  [4/42], [94mLoss[0m : 1.73990
[1mStep[0m  [8/42], [94mLoss[0m : 1.60628
[1mStep[0m  [12/42], [94mLoss[0m : 1.46454
[1mStep[0m  [16/42], [94mLoss[0m : 1.69186
[1mStep[0m  [20/42], [94mLoss[0m : 1.70060
[1mStep[0m  [24/42], [94mLoss[0m : 1.62515
[1mStep[0m  [28/42], [94mLoss[0m : 1.64173
[1mStep[0m  [32/42], [94mLoss[0m : 1.61310
[1mStep[0m  [36/42], [94mLoss[0m : 1.70575
[1mStep[0m  [40/42], [94mLoss[0m : 1.69971

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.644, [92mTest[0m: 2.688, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.56398
[1mStep[0m  [4/42], [94mLoss[0m : 1.76685
[1mStep[0m  [8/42], [94mLoss[0m : 1.66494
[1mStep[0m  [12/42], [94mLoss[0m : 1.56819
[1mStep[0m  [16/42], [94mLoss[0m : 1.65114
[1mStep[0m  [20/42], [94mLoss[0m : 1.50884
[1mStep[0m  [24/42], [94mLoss[0m : 1.54579
[1mStep[0m  [28/42], [94mLoss[0m : 1.54332
[1mStep[0m  [32/42], [94mLoss[0m : 1.67053
[1mStep[0m  [36/42], [94mLoss[0m : 1.74154
[1mStep[0m  [40/42], [94mLoss[0m : 1.60672

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.632, [92mTest[0m: 2.700, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.75665
[1mStep[0m  [4/42], [94mLoss[0m : 1.65342
[1mStep[0m  [8/42], [94mLoss[0m : 1.45100
[1mStep[0m  [12/42], [94mLoss[0m : 1.65541
[1mStep[0m  [16/42], [94mLoss[0m : 1.69701
[1mStep[0m  [20/42], [94mLoss[0m : 1.71870
[1mStep[0m  [24/42], [94mLoss[0m : 1.62908
[1mStep[0m  [28/42], [94mLoss[0m : 1.54841
[1mStep[0m  [32/42], [94mLoss[0m : 1.62132
[1mStep[0m  [36/42], [94mLoss[0m : 1.53871
[1mStep[0m  [40/42], [94mLoss[0m : 1.61055

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.628, [92mTest[0m: 2.673, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.58905
[1mStep[0m  [4/42], [94mLoss[0m : 1.75885
[1mStep[0m  [8/42], [94mLoss[0m : 1.50672
[1mStep[0m  [12/42], [94mLoss[0m : 1.60252
[1mStep[0m  [16/42], [94mLoss[0m : 1.65837
[1mStep[0m  [20/42], [94mLoss[0m : 1.45975
[1mStep[0m  [24/42], [94mLoss[0m : 1.65024
[1mStep[0m  [28/42], [94mLoss[0m : 1.39964
[1mStep[0m  [32/42], [94mLoss[0m : 1.66401
[1mStep[0m  [36/42], [94mLoss[0m : 1.56283
[1mStep[0m  [40/42], [94mLoss[0m : 1.55241

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.579, [92mTest[0m: 2.566, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.718
====================================

Phase 2 - Evaluation MAE:  2.7184663840702603
MAE score P1      2.372912
MAE score P2      2.718466
loss              1.579491
learning_rate     0.007525
batch_size             256
hidden_sizes         [300]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.1
weight_decay          0.01
Name: 4, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 10.94336
[1mStep[0m  [8/84], [94mLoss[0m : 10.49538
[1mStep[0m  [16/84], [94mLoss[0m : 9.98899
[1mStep[0m  [24/84], [94mLoss[0m : 8.60467
[1mStep[0m  [32/84], [94mLoss[0m : 7.69978
[1mStep[0m  [40/84], [94mLoss[0m : 6.51427
[1mStep[0m  [48/84], [94mLoss[0m : 5.92188
[1mStep[0m  [56/84], [94mLoss[0m : 5.46296
[1mStep[0m  [64/84], [94mLoss[0m : 4.14171
[1mStep[0m  [72/84], [94mLoss[0m : 3.76418
[1mStep[0m  [80/84], [94mLoss[0m : 2.53915

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.862, [92mTest[0m: 10.882, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.63982
[1mStep[0m  [8/84], [94mLoss[0m : 2.38444
[1mStep[0m  [16/84], [94mLoss[0m : 2.88800
[1mStep[0m  [24/84], [94mLoss[0m : 2.85549
[1mStep[0m  [32/84], [94mLoss[0m : 2.90288
[1mStep[0m  [40/84], [94mLoss[0m : 2.69820
[1mStep[0m  [48/84], [94mLoss[0m : 2.83248
[1mStep[0m  [56/84], [94mLoss[0m : 2.87362
[1mStep[0m  [64/84], [94mLoss[0m : 2.59463
[1mStep[0m  [72/84], [94mLoss[0m : 3.05537
[1mStep[0m  [80/84], [94mLoss[0m : 2.17918

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.637, [92mTest[0m: 2.910, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.68497
[1mStep[0m  [8/84], [94mLoss[0m : 2.48030
[1mStep[0m  [16/84], [94mLoss[0m : 2.47977
[1mStep[0m  [24/84], [94mLoss[0m : 2.61257
[1mStep[0m  [32/84], [94mLoss[0m : 2.37385
[1mStep[0m  [40/84], [94mLoss[0m : 3.04624
[1mStep[0m  [48/84], [94mLoss[0m : 2.44643
[1mStep[0m  [56/84], [94mLoss[0m : 2.49008
[1mStep[0m  [64/84], [94mLoss[0m : 2.63180
[1mStep[0m  [72/84], [94mLoss[0m : 2.63181
[1mStep[0m  [80/84], [94mLoss[0m : 2.51929

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.542, [92mTest[0m: 2.411, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42192
[1mStep[0m  [8/84], [94mLoss[0m : 2.49001
[1mStep[0m  [16/84], [94mLoss[0m : 2.32932
[1mStep[0m  [24/84], [94mLoss[0m : 2.45845
[1mStep[0m  [32/84], [94mLoss[0m : 2.37193
[1mStep[0m  [40/84], [94mLoss[0m : 2.18444
[1mStep[0m  [48/84], [94mLoss[0m : 2.34935
[1mStep[0m  [56/84], [94mLoss[0m : 2.56949
[1mStep[0m  [64/84], [94mLoss[0m : 2.58994
[1mStep[0m  [72/84], [94mLoss[0m : 2.38832
[1mStep[0m  [80/84], [94mLoss[0m : 2.42636

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.385, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31935
[1mStep[0m  [8/84], [94mLoss[0m : 2.05840
[1mStep[0m  [16/84], [94mLoss[0m : 2.41529
[1mStep[0m  [24/84], [94mLoss[0m : 2.69368
[1mStep[0m  [32/84], [94mLoss[0m : 2.47033
[1mStep[0m  [40/84], [94mLoss[0m : 2.52871
[1mStep[0m  [48/84], [94mLoss[0m : 2.33892
[1mStep[0m  [56/84], [94mLoss[0m : 2.85881
[1mStep[0m  [64/84], [94mLoss[0m : 2.46615
[1mStep[0m  [72/84], [94mLoss[0m : 2.57477
[1mStep[0m  [80/84], [94mLoss[0m : 2.44994

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.346, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60778
[1mStep[0m  [8/84], [94mLoss[0m : 2.53253
[1mStep[0m  [16/84], [94mLoss[0m : 2.53953
[1mStep[0m  [24/84], [94mLoss[0m : 2.16983
[1mStep[0m  [32/84], [94mLoss[0m : 2.69480
[1mStep[0m  [40/84], [94mLoss[0m : 2.55146
[1mStep[0m  [48/84], [94mLoss[0m : 2.46028
[1mStep[0m  [56/84], [94mLoss[0m : 2.57978
[1mStep[0m  [64/84], [94mLoss[0m : 2.81507
[1mStep[0m  [72/84], [94mLoss[0m : 2.42403
[1mStep[0m  [80/84], [94mLoss[0m : 2.37506

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.373, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.65034
[1mStep[0m  [8/84], [94mLoss[0m : 2.61163
[1mStep[0m  [16/84], [94mLoss[0m : 2.58379
[1mStep[0m  [24/84], [94mLoss[0m : 2.60975
[1mStep[0m  [32/84], [94mLoss[0m : 2.28047
[1mStep[0m  [40/84], [94mLoss[0m : 2.36433
[1mStep[0m  [48/84], [94mLoss[0m : 2.52471
[1mStep[0m  [56/84], [94mLoss[0m : 2.50096
[1mStep[0m  [64/84], [94mLoss[0m : 2.36183
[1mStep[0m  [72/84], [94mLoss[0m : 2.43693
[1mStep[0m  [80/84], [94mLoss[0m : 2.69938

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.352, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46341
[1mStep[0m  [8/84], [94mLoss[0m : 2.56668
[1mStep[0m  [16/84], [94mLoss[0m : 2.25733
[1mStep[0m  [24/84], [94mLoss[0m : 2.27344
[1mStep[0m  [32/84], [94mLoss[0m : 2.52584
[1mStep[0m  [40/84], [94mLoss[0m : 2.44833
[1mStep[0m  [48/84], [94mLoss[0m : 2.19825
[1mStep[0m  [56/84], [94mLoss[0m : 2.45622
[1mStep[0m  [64/84], [94mLoss[0m : 2.45210
[1mStep[0m  [72/84], [94mLoss[0m : 2.73901
[1mStep[0m  [80/84], [94mLoss[0m : 2.41888

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.347, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43412
[1mStep[0m  [8/84], [94mLoss[0m : 2.59374
[1mStep[0m  [16/84], [94mLoss[0m : 2.20927
[1mStep[0m  [24/84], [94mLoss[0m : 2.61244
[1mStep[0m  [32/84], [94mLoss[0m : 2.37097
[1mStep[0m  [40/84], [94mLoss[0m : 2.37652
[1mStep[0m  [48/84], [94mLoss[0m : 2.27302
[1mStep[0m  [56/84], [94mLoss[0m : 2.64430
[1mStep[0m  [64/84], [94mLoss[0m : 2.34752
[1mStep[0m  [72/84], [94mLoss[0m : 2.40356
[1mStep[0m  [80/84], [94mLoss[0m : 2.17407

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.363, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.23594
[1mStep[0m  [8/84], [94mLoss[0m : 2.30581
[1mStep[0m  [16/84], [94mLoss[0m : 2.42303
[1mStep[0m  [24/84], [94mLoss[0m : 2.60063
[1mStep[0m  [32/84], [94mLoss[0m : 2.31633
[1mStep[0m  [40/84], [94mLoss[0m : 2.62389
[1mStep[0m  [48/84], [94mLoss[0m : 2.31475
[1mStep[0m  [56/84], [94mLoss[0m : 2.54200
[1mStep[0m  [64/84], [94mLoss[0m : 2.42576
[1mStep[0m  [72/84], [94mLoss[0m : 2.14586
[1mStep[0m  [80/84], [94mLoss[0m : 2.62359

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.398, [92mTest[0m: 2.355, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50843
[1mStep[0m  [8/84], [94mLoss[0m : 2.32721
[1mStep[0m  [16/84], [94mLoss[0m : 2.25199
[1mStep[0m  [24/84], [94mLoss[0m : 2.54381
[1mStep[0m  [32/84], [94mLoss[0m : 2.31696
[1mStep[0m  [40/84], [94mLoss[0m : 2.49016
[1mStep[0m  [48/84], [94mLoss[0m : 2.29974
[1mStep[0m  [56/84], [94mLoss[0m : 2.25440
[1mStep[0m  [64/84], [94mLoss[0m : 2.43520
[1mStep[0m  [72/84], [94mLoss[0m : 2.38247
[1mStep[0m  [80/84], [94mLoss[0m : 2.34082

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.368, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43628
[1mStep[0m  [8/84], [94mLoss[0m : 2.52817
[1mStep[0m  [16/84], [94mLoss[0m : 2.26416
[1mStep[0m  [24/84], [94mLoss[0m : 2.36828
[1mStep[0m  [32/84], [94mLoss[0m : 2.62787
[1mStep[0m  [40/84], [94mLoss[0m : 2.20489
[1mStep[0m  [48/84], [94mLoss[0m : 2.76045
[1mStep[0m  [56/84], [94mLoss[0m : 2.42291
[1mStep[0m  [64/84], [94mLoss[0m : 2.49941
[1mStep[0m  [72/84], [94mLoss[0m : 2.30176
[1mStep[0m  [80/84], [94mLoss[0m : 2.39172

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.386, [92mTest[0m: 2.341, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.28360
[1mStep[0m  [8/84], [94mLoss[0m : 2.31490
[1mStep[0m  [16/84], [94mLoss[0m : 2.50715
[1mStep[0m  [24/84], [94mLoss[0m : 2.37174
[1mStep[0m  [32/84], [94mLoss[0m : 2.32338
[1mStep[0m  [40/84], [94mLoss[0m : 2.12514
[1mStep[0m  [48/84], [94mLoss[0m : 2.36549
[1mStep[0m  [56/84], [94mLoss[0m : 2.34162
[1mStep[0m  [64/84], [94mLoss[0m : 2.33689
[1mStep[0m  [72/84], [94mLoss[0m : 2.19067
[1mStep[0m  [80/84], [94mLoss[0m : 2.39602

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.379, [92mTest[0m: 2.339, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.62029
[1mStep[0m  [8/84], [94mLoss[0m : 2.53165
[1mStep[0m  [16/84], [94mLoss[0m : 2.22378
[1mStep[0m  [24/84], [94mLoss[0m : 2.48707
[1mStep[0m  [32/84], [94mLoss[0m : 1.95812
[1mStep[0m  [40/84], [94mLoss[0m : 2.63664
[1mStep[0m  [48/84], [94mLoss[0m : 2.25266
[1mStep[0m  [56/84], [94mLoss[0m : 2.63437
[1mStep[0m  [64/84], [94mLoss[0m : 2.41902
[1mStep[0m  [72/84], [94mLoss[0m : 2.45978
[1mStep[0m  [80/84], [94mLoss[0m : 2.41579

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.382, [92mTest[0m: 2.337, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31570
[1mStep[0m  [8/84], [94mLoss[0m : 2.59979
[1mStep[0m  [16/84], [94mLoss[0m : 2.65197
[1mStep[0m  [24/84], [94mLoss[0m : 2.23573
[1mStep[0m  [32/84], [94mLoss[0m : 2.45531
[1mStep[0m  [40/84], [94mLoss[0m : 2.40638
[1mStep[0m  [48/84], [94mLoss[0m : 2.18931
[1mStep[0m  [56/84], [94mLoss[0m : 2.28546
[1mStep[0m  [64/84], [94mLoss[0m : 2.33423
[1mStep[0m  [72/84], [94mLoss[0m : 2.61322
[1mStep[0m  [80/84], [94mLoss[0m : 2.45832

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.374, [92mTest[0m: 2.356, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.14218
[1mStep[0m  [8/84], [94mLoss[0m : 2.41600
[1mStep[0m  [16/84], [94mLoss[0m : 2.45315
[1mStep[0m  [24/84], [94mLoss[0m : 2.13757
[1mStep[0m  [32/84], [94mLoss[0m : 2.50619
[1mStep[0m  [40/84], [94mLoss[0m : 2.57489
[1mStep[0m  [48/84], [94mLoss[0m : 2.61909
[1mStep[0m  [56/84], [94mLoss[0m : 2.26540
[1mStep[0m  [64/84], [94mLoss[0m : 2.59832
[1mStep[0m  [72/84], [94mLoss[0m : 2.38791
[1mStep[0m  [80/84], [94mLoss[0m : 2.29112

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.374, [92mTest[0m: 2.385, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.07286
[1mStep[0m  [8/84], [94mLoss[0m : 2.14214
[1mStep[0m  [16/84], [94mLoss[0m : 2.32559
[1mStep[0m  [24/84], [94mLoss[0m : 2.22274
[1mStep[0m  [32/84], [94mLoss[0m : 2.43406
[1mStep[0m  [40/84], [94mLoss[0m : 2.37577
[1mStep[0m  [48/84], [94mLoss[0m : 2.28449
[1mStep[0m  [56/84], [94mLoss[0m : 2.33351
[1mStep[0m  [64/84], [94mLoss[0m : 2.33619
[1mStep[0m  [72/84], [94mLoss[0m : 2.35467
[1mStep[0m  [80/84], [94mLoss[0m : 2.34532

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.359, [92mTest[0m: 2.327, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.51017
[1mStep[0m  [8/84], [94mLoss[0m : 2.22153
[1mStep[0m  [16/84], [94mLoss[0m : 2.16568
[1mStep[0m  [24/84], [94mLoss[0m : 2.25626
[1mStep[0m  [32/84], [94mLoss[0m : 2.27206
[1mStep[0m  [40/84], [94mLoss[0m : 2.49325
[1mStep[0m  [48/84], [94mLoss[0m : 2.34277
[1mStep[0m  [56/84], [94mLoss[0m : 2.64514
[1mStep[0m  [64/84], [94mLoss[0m : 2.10084
[1mStep[0m  [72/84], [94mLoss[0m : 2.50340
[1mStep[0m  [80/84], [94mLoss[0m : 1.99322

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.359, [92mTest[0m: 2.349, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45605
[1mStep[0m  [8/84], [94mLoss[0m : 2.34618
[1mStep[0m  [16/84], [94mLoss[0m : 2.09291
[1mStep[0m  [24/84], [94mLoss[0m : 2.07291
[1mStep[0m  [32/84], [94mLoss[0m : 2.42055
[1mStep[0m  [40/84], [94mLoss[0m : 2.32898
[1mStep[0m  [48/84], [94mLoss[0m : 2.24442
[1mStep[0m  [56/84], [94mLoss[0m : 2.18498
[1mStep[0m  [64/84], [94mLoss[0m : 2.44230
[1mStep[0m  [72/84], [94mLoss[0m : 2.44281
[1mStep[0m  [80/84], [94mLoss[0m : 2.35854

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.363, [92mTest[0m: 2.375, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43698
[1mStep[0m  [8/84], [94mLoss[0m : 2.30975
[1mStep[0m  [16/84], [94mLoss[0m : 2.10449
[1mStep[0m  [24/84], [94mLoss[0m : 2.15391
[1mStep[0m  [32/84], [94mLoss[0m : 2.54964
[1mStep[0m  [40/84], [94mLoss[0m : 2.49232
[1mStep[0m  [48/84], [94mLoss[0m : 2.30922
[1mStep[0m  [56/84], [94mLoss[0m : 2.58831
[1mStep[0m  [64/84], [94mLoss[0m : 2.41030
[1mStep[0m  [72/84], [94mLoss[0m : 2.30031
[1mStep[0m  [80/84], [94mLoss[0m : 2.18241

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.354, [92mTest[0m: 2.327, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.01379
[1mStep[0m  [8/84], [94mLoss[0m : 2.47869
[1mStep[0m  [16/84], [94mLoss[0m : 2.19957
[1mStep[0m  [24/84], [94mLoss[0m : 2.50298
[1mStep[0m  [32/84], [94mLoss[0m : 2.37841
[1mStep[0m  [40/84], [94mLoss[0m : 2.40239
[1mStep[0m  [48/84], [94mLoss[0m : 2.21736
[1mStep[0m  [56/84], [94mLoss[0m : 2.31921
[1mStep[0m  [64/84], [94mLoss[0m : 2.24719
[1mStep[0m  [72/84], [94mLoss[0m : 2.65037
[1mStep[0m  [80/84], [94mLoss[0m : 2.34722

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.349, [92mTest[0m: 2.334, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47624
[1mStep[0m  [8/84], [94mLoss[0m : 2.41694
[1mStep[0m  [16/84], [94mLoss[0m : 2.35098
[1mStep[0m  [24/84], [94mLoss[0m : 2.46491
[1mStep[0m  [32/84], [94mLoss[0m : 2.49459
[1mStep[0m  [40/84], [94mLoss[0m : 2.03950
[1mStep[0m  [48/84], [94mLoss[0m : 2.34470
[1mStep[0m  [56/84], [94mLoss[0m : 1.97040
[1mStep[0m  [64/84], [94mLoss[0m : 1.99565
[1mStep[0m  [72/84], [94mLoss[0m : 2.14681
[1mStep[0m  [80/84], [94mLoss[0m : 2.20311

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.331, [92mTest[0m: 2.346, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26384
[1mStep[0m  [8/84], [94mLoss[0m : 2.21850
[1mStep[0m  [16/84], [94mLoss[0m : 2.80871
[1mStep[0m  [24/84], [94mLoss[0m : 2.41054
[1mStep[0m  [32/84], [94mLoss[0m : 2.08174
[1mStep[0m  [40/84], [94mLoss[0m : 2.38121
[1mStep[0m  [48/84], [94mLoss[0m : 2.23496
[1mStep[0m  [56/84], [94mLoss[0m : 2.87911
[1mStep[0m  [64/84], [94mLoss[0m : 2.14608
[1mStep[0m  [72/84], [94mLoss[0m : 2.42145
[1mStep[0m  [80/84], [94mLoss[0m : 2.63373

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.333, [92mTest[0m: 2.323, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.32002
[1mStep[0m  [8/84], [94mLoss[0m : 2.33235
[1mStep[0m  [16/84], [94mLoss[0m : 2.11591
[1mStep[0m  [24/84], [94mLoss[0m : 2.32414
[1mStep[0m  [32/84], [94mLoss[0m : 2.56310
[1mStep[0m  [40/84], [94mLoss[0m : 2.58705
[1mStep[0m  [48/84], [94mLoss[0m : 2.43106
[1mStep[0m  [56/84], [94mLoss[0m : 2.74315
[1mStep[0m  [64/84], [94mLoss[0m : 2.47154
[1mStep[0m  [72/84], [94mLoss[0m : 2.50977
[1mStep[0m  [80/84], [94mLoss[0m : 2.46129

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.358, [92mTest[0m: 2.328, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.09249
[1mStep[0m  [8/84], [94mLoss[0m : 2.65512
[1mStep[0m  [16/84], [94mLoss[0m : 2.62801
[1mStep[0m  [24/84], [94mLoss[0m : 2.23498
[1mStep[0m  [32/84], [94mLoss[0m : 2.08244
[1mStep[0m  [40/84], [94mLoss[0m : 2.59107
[1mStep[0m  [48/84], [94mLoss[0m : 2.44069
[1mStep[0m  [56/84], [94mLoss[0m : 2.13944
[1mStep[0m  [64/84], [94mLoss[0m : 2.44799
[1mStep[0m  [72/84], [94mLoss[0m : 2.40930
[1mStep[0m  [80/84], [94mLoss[0m : 2.37938

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.334, [92mTest[0m: 2.335, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.16220
[1mStep[0m  [8/84], [94mLoss[0m : 2.33576
[1mStep[0m  [16/84], [94mLoss[0m : 2.35221
[1mStep[0m  [24/84], [94mLoss[0m : 2.54959
[1mStep[0m  [32/84], [94mLoss[0m : 2.34659
[1mStep[0m  [40/84], [94mLoss[0m : 2.35077
[1mStep[0m  [48/84], [94mLoss[0m : 2.16983
[1mStep[0m  [56/84], [94mLoss[0m : 2.58100
[1mStep[0m  [64/84], [94mLoss[0m : 2.46824
[1mStep[0m  [72/84], [94mLoss[0m : 2.14171
[1mStep[0m  [80/84], [94mLoss[0m : 2.04986

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.327, [92mTest[0m: 2.327, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30707
[1mStep[0m  [8/84], [94mLoss[0m : 2.04473
[1mStep[0m  [16/84], [94mLoss[0m : 2.31331
[1mStep[0m  [24/84], [94mLoss[0m : 2.31000
[1mStep[0m  [32/84], [94mLoss[0m : 2.40767
[1mStep[0m  [40/84], [94mLoss[0m : 2.47683
[1mStep[0m  [48/84], [94mLoss[0m : 2.31825
[1mStep[0m  [56/84], [94mLoss[0m : 2.38725
[1mStep[0m  [64/84], [94mLoss[0m : 2.50880
[1mStep[0m  [72/84], [94mLoss[0m : 2.25065
[1mStep[0m  [80/84], [94mLoss[0m : 2.17254

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.321, [92mTest[0m: 2.328, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48706
[1mStep[0m  [8/84], [94mLoss[0m : 2.63692
[1mStep[0m  [16/84], [94mLoss[0m : 2.10995
[1mStep[0m  [24/84], [94mLoss[0m : 2.05411
[1mStep[0m  [32/84], [94mLoss[0m : 2.41471
[1mStep[0m  [40/84], [94mLoss[0m : 2.42114
[1mStep[0m  [48/84], [94mLoss[0m : 2.26580
[1mStep[0m  [56/84], [94mLoss[0m : 2.26788
[1mStep[0m  [64/84], [94mLoss[0m : 2.47556
[1mStep[0m  [72/84], [94mLoss[0m : 2.40971
[1mStep[0m  [80/84], [94mLoss[0m : 2.34950

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.338, [92mTest[0m: 2.336, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.19010
[1mStep[0m  [8/84], [94mLoss[0m : 2.52799
[1mStep[0m  [16/84], [94mLoss[0m : 2.45683
[1mStep[0m  [24/84], [94mLoss[0m : 2.37986
[1mStep[0m  [32/84], [94mLoss[0m : 2.22995
[1mStep[0m  [40/84], [94mLoss[0m : 2.35315
[1mStep[0m  [48/84], [94mLoss[0m : 2.44841
[1mStep[0m  [56/84], [94mLoss[0m : 2.38259
[1mStep[0m  [64/84], [94mLoss[0m : 2.19417
[1mStep[0m  [72/84], [94mLoss[0m : 2.54183
[1mStep[0m  [80/84], [94mLoss[0m : 2.27463

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.318, [92mTest[0m: 2.331, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.29743
[1mStep[0m  [8/84], [94mLoss[0m : 2.04422
[1mStep[0m  [16/84], [94mLoss[0m : 2.01071
[1mStep[0m  [24/84], [94mLoss[0m : 2.27983
[1mStep[0m  [32/84], [94mLoss[0m : 2.28064
[1mStep[0m  [40/84], [94mLoss[0m : 2.20411
[1mStep[0m  [48/84], [94mLoss[0m : 2.19980
[1mStep[0m  [56/84], [94mLoss[0m : 2.47120
[1mStep[0m  [64/84], [94mLoss[0m : 2.50846
[1mStep[0m  [72/84], [94mLoss[0m : 2.29025
[1mStep[0m  [80/84], [94mLoss[0m : 2.56581

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.317, [92mTest[0m: 2.344, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.332
====================================

Phase 1 - Evaluation MAE:  2.33168579850878
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 2.19339
[1mStep[0m  [8/84], [94mLoss[0m : 2.57062
[1mStep[0m  [16/84], [94mLoss[0m : 2.53472
[1mStep[0m  [24/84], [94mLoss[0m : 2.37329
[1mStep[0m  [32/84], [94mLoss[0m : 2.17842
[1mStep[0m  [40/84], [94mLoss[0m : 2.72032
[1mStep[0m  [48/84], [94mLoss[0m : 2.86771
[1mStep[0m  [56/84], [94mLoss[0m : 2.34220
[1mStep[0m  [64/84], [94mLoss[0m : 2.54132
[1mStep[0m  [72/84], [94mLoss[0m : 2.48057
[1mStep[0m  [80/84], [94mLoss[0m : 2.52541

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.338, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42910
[1mStep[0m  [8/84], [94mLoss[0m : 2.13843
[1mStep[0m  [16/84], [94mLoss[0m : 2.30920
[1mStep[0m  [24/84], [94mLoss[0m : 2.36382
[1mStep[0m  [32/84], [94mLoss[0m : 2.34057
[1mStep[0m  [40/84], [94mLoss[0m : 2.06803
[1mStep[0m  [48/84], [94mLoss[0m : 2.06081
[1mStep[0m  [56/84], [94mLoss[0m : 1.96663
[1mStep[0m  [64/84], [94mLoss[0m : 2.17189
[1mStep[0m  [72/84], [94mLoss[0m : 2.29008
[1mStep[0m  [80/84], [94mLoss[0m : 2.06820

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.291, [92mTest[0m: 2.372, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.92146
[1mStep[0m  [8/84], [94mLoss[0m : 2.39373
[1mStep[0m  [16/84], [94mLoss[0m : 1.95320
[1mStep[0m  [24/84], [94mLoss[0m : 1.92874
[1mStep[0m  [32/84], [94mLoss[0m : 2.27337
[1mStep[0m  [40/84], [94mLoss[0m : 2.04674
[1mStep[0m  [48/84], [94mLoss[0m : 2.17463
[1mStep[0m  [56/84], [94mLoss[0m : 2.23421
[1mStep[0m  [64/84], [94mLoss[0m : 2.06281
[1mStep[0m  [72/84], [94mLoss[0m : 2.19775
[1mStep[0m  [80/84], [94mLoss[0m : 2.52394

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.178, [92mTest[0m: 2.349, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.98108
[1mStep[0m  [8/84], [94mLoss[0m : 2.15806
[1mStep[0m  [16/84], [94mLoss[0m : 2.52882
[1mStep[0m  [24/84], [94mLoss[0m : 2.17035
[1mStep[0m  [32/84], [94mLoss[0m : 2.01004
[1mStep[0m  [40/84], [94mLoss[0m : 2.50490
[1mStep[0m  [48/84], [94mLoss[0m : 2.05052
[1mStep[0m  [56/84], [94mLoss[0m : 2.06559
[1mStep[0m  [64/84], [94mLoss[0m : 2.15142
[1mStep[0m  [72/84], [94mLoss[0m : 2.21532
[1mStep[0m  [80/84], [94mLoss[0m : 2.26392

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.121, [92mTest[0m: 2.376, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.98505
[1mStep[0m  [8/84], [94mLoss[0m : 2.06765
[1mStep[0m  [16/84], [94mLoss[0m : 1.75266
[1mStep[0m  [24/84], [94mLoss[0m : 2.19122
[1mStep[0m  [32/84], [94mLoss[0m : 2.00767
[1mStep[0m  [40/84], [94mLoss[0m : 1.77450
[1mStep[0m  [48/84], [94mLoss[0m : 2.04428
[1mStep[0m  [56/84], [94mLoss[0m : 2.47454
[1mStep[0m  [64/84], [94mLoss[0m : 1.82469
[1mStep[0m  [72/84], [94mLoss[0m : 1.99578
[1mStep[0m  [80/84], [94mLoss[0m : 2.12605

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.035, [92mTest[0m: 2.346, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.13029
[1mStep[0m  [8/84], [94mLoss[0m : 1.64594
[1mStep[0m  [16/84], [94mLoss[0m : 1.91304
[1mStep[0m  [24/84], [94mLoss[0m : 1.90460
[1mStep[0m  [32/84], [94mLoss[0m : 1.83895
[1mStep[0m  [40/84], [94mLoss[0m : 1.91687
[1mStep[0m  [48/84], [94mLoss[0m : 2.05960
[1mStep[0m  [56/84], [94mLoss[0m : 1.74839
[1mStep[0m  [64/84], [94mLoss[0m : 1.90348
[1mStep[0m  [72/84], [94mLoss[0m : 1.94380
[1mStep[0m  [80/84], [94mLoss[0m : 1.83126

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 1.968, [92mTest[0m: 2.369, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.73701
[1mStep[0m  [8/84], [94mLoss[0m : 1.52152
[1mStep[0m  [16/84], [94mLoss[0m : 1.88720
[1mStep[0m  [24/84], [94mLoss[0m : 1.95879
[1mStep[0m  [32/84], [94mLoss[0m : 1.87579
[1mStep[0m  [40/84], [94mLoss[0m : 1.78235
[1mStep[0m  [48/84], [94mLoss[0m : 1.66142
[1mStep[0m  [56/84], [94mLoss[0m : 1.99838
[1mStep[0m  [64/84], [94mLoss[0m : 2.09358
[1mStep[0m  [72/84], [94mLoss[0m : 2.01776
[1mStep[0m  [80/84], [94mLoss[0m : 1.91601

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.877, [92mTest[0m: 2.392, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.85899
[1mStep[0m  [8/84], [94mLoss[0m : 1.73699
[1mStep[0m  [16/84], [94mLoss[0m : 1.94082
[1mStep[0m  [24/84], [94mLoss[0m : 1.76442
[1mStep[0m  [32/84], [94mLoss[0m : 1.61914
[1mStep[0m  [40/84], [94mLoss[0m : 1.89061
[1mStep[0m  [48/84], [94mLoss[0m : 1.69082
[1mStep[0m  [56/84], [94mLoss[0m : 1.82685
[1mStep[0m  [64/84], [94mLoss[0m : 1.56772
[1mStep[0m  [72/84], [94mLoss[0m : 1.68774
[1mStep[0m  [80/84], [94mLoss[0m : 1.93850

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.820, [92mTest[0m: 2.423, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.71366
[1mStep[0m  [8/84], [94mLoss[0m : 1.78304
[1mStep[0m  [16/84], [94mLoss[0m : 1.68344
[1mStep[0m  [24/84], [94mLoss[0m : 1.72248
[1mStep[0m  [32/84], [94mLoss[0m : 1.66354
[1mStep[0m  [40/84], [94mLoss[0m : 1.77593
[1mStep[0m  [48/84], [94mLoss[0m : 1.74145
[1mStep[0m  [56/84], [94mLoss[0m : 1.63318
[1mStep[0m  [64/84], [94mLoss[0m : 1.58550
[1mStep[0m  [72/84], [94mLoss[0m : 1.82140
[1mStep[0m  [80/84], [94mLoss[0m : 1.77436

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.765, [92mTest[0m: 2.431, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.67223
[1mStep[0m  [8/84], [94mLoss[0m : 1.75676
[1mStep[0m  [16/84], [94mLoss[0m : 1.56349
[1mStep[0m  [24/84], [94mLoss[0m : 1.67201
[1mStep[0m  [32/84], [94mLoss[0m : 1.84329
[1mStep[0m  [40/84], [94mLoss[0m : 1.92006
[1mStep[0m  [48/84], [94mLoss[0m : 1.62312
[1mStep[0m  [56/84], [94mLoss[0m : 1.71967
[1mStep[0m  [64/84], [94mLoss[0m : 1.61521
[1mStep[0m  [72/84], [94mLoss[0m : 1.77994
[1mStep[0m  [80/84], [94mLoss[0m : 2.04440

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.723, [92mTest[0m: 2.422, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.47558
[1mStep[0m  [8/84], [94mLoss[0m : 1.38992
[1mStep[0m  [16/84], [94mLoss[0m : 1.67203
[1mStep[0m  [24/84], [94mLoss[0m : 1.69803
[1mStep[0m  [32/84], [94mLoss[0m : 1.85139
[1mStep[0m  [40/84], [94mLoss[0m : 1.60893
[1mStep[0m  [48/84], [94mLoss[0m : 1.58926
[1mStep[0m  [56/84], [94mLoss[0m : 1.67967
[1mStep[0m  [64/84], [94mLoss[0m : 1.75500
[1mStep[0m  [72/84], [94mLoss[0m : 1.74850
[1mStep[0m  [80/84], [94mLoss[0m : 1.71013

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.674, [92mTest[0m: 2.408, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.62314
[1mStep[0m  [8/84], [94mLoss[0m : 1.76125
[1mStep[0m  [16/84], [94mLoss[0m : 1.57265
[1mStep[0m  [24/84], [94mLoss[0m : 1.64353
[1mStep[0m  [32/84], [94mLoss[0m : 1.65828
[1mStep[0m  [40/84], [94mLoss[0m : 1.73346
[1mStep[0m  [48/84], [94mLoss[0m : 1.50259
[1mStep[0m  [56/84], [94mLoss[0m : 1.71507
[1mStep[0m  [64/84], [94mLoss[0m : 1.63393
[1mStep[0m  [72/84], [94mLoss[0m : 1.48387
[1mStep[0m  [80/84], [94mLoss[0m : 1.64980

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.625, [92mTest[0m: 2.478, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.39282
[1mStep[0m  [8/84], [94mLoss[0m : 1.44655
[1mStep[0m  [16/84], [94mLoss[0m : 1.63972
[1mStep[0m  [24/84], [94mLoss[0m : 1.41947
[1mStep[0m  [32/84], [94mLoss[0m : 1.72249
[1mStep[0m  [40/84], [94mLoss[0m : 1.49848
[1mStep[0m  [48/84], [94mLoss[0m : 1.65200
[1mStep[0m  [56/84], [94mLoss[0m : 1.51704
[1mStep[0m  [64/84], [94mLoss[0m : 1.80084
[1mStep[0m  [72/84], [94mLoss[0m : 1.45862
[1mStep[0m  [80/84], [94mLoss[0m : 1.73058

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.600, [92mTest[0m: 2.448, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.52849
[1mStep[0m  [8/84], [94mLoss[0m : 1.51674
[1mStep[0m  [16/84], [94mLoss[0m : 1.62173
[1mStep[0m  [24/84], [94mLoss[0m : 1.49930
[1mStep[0m  [32/84], [94mLoss[0m : 1.61334
[1mStep[0m  [40/84], [94mLoss[0m : 1.28198
[1mStep[0m  [48/84], [94mLoss[0m : 1.32781
[1mStep[0m  [56/84], [94mLoss[0m : 1.48298
[1mStep[0m  [64/84], [94mLoss[0m : 1.62831
[1mStep[0m  [72/84], [94mLoss[0m : 1.45199
[1mStep[0m  [80/84], [94mLoss[0m : 1.76074

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.536, [92mTest[0m: 2.498, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.33986
[1mStep[0m  [8/84], [94mLoss[0m : 1.36385
[1mStep[0m  [16/84], [94mLoss[0m : 1.57127
[1mStep[0m  [24/84], [94mLoss[0m : 1.35627
[1mStep[0m  [32/84], [94mLoss[0m : 1.50988
[1mStep[0m  [40/84], [94mLoss[0m : 1.43549
[1mStep[0m  [48/84], [94mLoss[0m : 1.54642
[1mStep[0m  [56/84], [94mLoss[0m : 1.52330
[1mStep[0m  [64/84], [94mLoss[0m : 1.70216
[1mStep[0m  [72/84], [94mLoss[0m : 1.56368
[1mStep[0m  [80/84], [94mLoss[0m : 1.53482

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.496, [92mTest[0m: 2.436, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.39971
[1mStep[0m  [8/84], [94mLoss[0m : 1.25139
[1mStep[0m  [16/84], [94mLoss[0m : 1.32239
[1mStep[0m  [24/84], [94mLoss[0m : 1.41329
[1mStep[0m  [32/84], [94mLoss[0m : 1.34814
[1mStep[0m  [40/84], [94mLoss[0m : 1.49382
[1mStep[0m  [48/84], [94mLoss[0m : 1.43298
[1mStep[0m  [56/84], [94mLoss[0m : 1.62843
[1mStep[0m  [64/84], [94mLoss[0m : 1.48709
[1mStep[0m  [72/84], [94mLoss[0m : 1.32934
[1mStep[0m  [80/84], [94mLoss[0m : 1.61703

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.463, [92mTest[0m: 2.555, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.35333
[1mStep[0m  [8/84], [94mLoss[0m : 1.44212
[1mStep[0m  [16/84], [94mLoss[0m : 1.37292
[1mStep[0m  [24/84], [94mLoss[0m : 1.68222
[1mStep[0m  [32/84], [94mLoss[0m : 1.32463
[1mStep[0m  [40/84], [94mLoss[0m : 1.55602
[1mStep[0m  [48/84], [94mLoss[0m : 1.44496
[1mStep[0m  [56/84], [94mLoss[0m : 1.31172
[1mStep[0m  [64/84], [94mLoss[0m : 1.57127
[1mStep[0m  [72/84], [94mLoss[0m : 1.43191
[1mStep[0m  [80/84], [94mLoss[0m : 1.49302

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.452, [92mTest[0m: 2.440, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.37405
[1mStep[0m  [8/84], [94mLoss[0m : 1.16177
[1mStep[0m  [16/84], [94mLoss[0m : 1.35858
[1mStep[0m  [24/84], [94mLoss[0m : 1.37718
[1mStep[0m  [32/84], [94mLoss[0m : 1.39924
[1mStep[0m  [40/84], [94mLoss[0m : 1.40456
[1mStep[0m  [48/84], [94mLoss[0m : 1.41519
[1mStep[0m  [56/84], [94mLoss[0m : 1.45291
[1mStep[0m  [64/84], [94mLoss[0m : 1.55218
[1mStep[0m  [72/84], [94mLoss[0m : 1.54031
[1mStep[0m  [80/84], [94mLoss[0m : 1.73887

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.410, [92mTest[0m: 2.529, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.51514
[1mStep[0m  [8/84], [94mLoss[0m : 1.39427
[1mStep[0m  [16/84], [94mLoss[0m : 1.59588
[1mStep[0m  [24/84], [94mLoss[0m : 1.51769
[1mStep[0m  [32/84], [94mLoss[0m : 1.45319
[1mStep[0m  [40/84], [94mLoss[0m : 1.36384
[1mStep[0m  [48/84], [94mLoss[0m : 1.45034
[1mStep[0m  [56/84], [94mLoss[0m : 1.36957
[1mStep[0m  [64/84], [94mLoss[0m : 1.33620
[1mStep[0m  [72/84], [94mLoss[0m : 1.22778
[1mStep[0m  [80/84], [94mLoss[0m : 1.25556

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.386, [92mTest[0m: 2.506, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.39648
[1mStep[0m  [8/84], [94mLoss[0m : 1.30011
[1mStep[0m  [16/84], [94mLoss[0m : 1.30612
[1mStep[0m  [24/84], [94mLoss[0m : 1.64065
[1mStep[0m  [32/84], [94mLoss[0m : 1.45152
[1mStep[0m  [40/84], [94mLoss[0m : 1.09620
[1mStep[0m  [48/84], [94mLoss[0m : 1.29892
[1mStep[0m  [56/84], [94mLoss[0m : 1.45401
[1mStep[0m  [64/84], [94mLoss[0m : 1.27492
[1mStep[0m  [72/84], [94mLoss[0m : 1.33729
[1mStep[0m  [80/84], [94mLoss[0m : 1.19804

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.331, [92mTest[0m: 2.530, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.16188
[1mStep[0m  [8/84], [94mLoss[0m : 1.13838
[1mStep[0m  [16/84], [94mLoss[0m : 1.27969
[1mStep[0m  [24/84], [94mLoss[0m : 1.33369
[1mStep[0m  [32/84], [94mLoss[0m : 1.30088
[1mStep[0m  [40/84], [94mLoss[0m : 1.57596
[1mStep[0m  [48/84], [94mLoss[0m : 1.16421
[1mStep[0m  [56/84], [94mLoss[0m : 1.16229
[1mStep[0m  [64/84], [94mLoss[0m : 1.22288
[1mStep[0m  [72/84], [94mLoss[0m : 1.39213
[1mStep[0m  [80/84], [94mLoss[0m : 1.17298

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.306, [92mTest[0m: 2.497, [96mlr[0m: 0.006772500000000002
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 20 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.494
====================================

Phase 2 - Evaluation MAE:  2.4940301094736372
MAE score P1      2.331686
MAE score P2       2.49403
loss                1.3061
learning_rate     0.007525
batch_size             128
hidden_sizes         [250]
epochs                  30
activation            tanh
optimizer              sgd
early stopping        True
dropout                0.2
momentum               0.9
weight_decay        0.0001
Name: 5, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 10.24483
[1mStep[0m  [8/84], [94mLoss[0m : 9.22018
[1mStep[0m  [16/84], [94mLoss[0m : 8.58401
[1mStep[0m  [24/84], [94mLoss[0m : 7.18717
[1mStep[0m  [32/84], [94mLoss[0m : 6.29775
[1mStep[0m  [40/84], [94mLoss[0m : 5.59200
[1mStep[0m  [48/84], [94mLoss[0m : 4.38467
[1mStep[0m  [56/84], [94mLoss[0m : 4.08935
[1mStep[0m  [64/84], [94mLoss[0m : 3.87073
[1mStep[0m  [72/84], [94mLoss[0m : 3.18497
[1mStep[0m  [80/84], [94mLoss[0m : 2.98297

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 5.801, [92mTest[0m: 11.004, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.06656
[1mStep[0m  [8/84], [94mLoss[0m : 3.11712
[1mStep[0m  [16/84], [94mLoss[0m : 2.67936
[1mStep[0m  [24/84], [94mLoss[0m : 2.67076
[1mStep[0m  [32/84], [94mLoss[0m : 2.88052
[1mStep[0m  [40/84], [94mLoss[0m : 2.57102
[1mStep[0m  [48/84], [94mLoss[0m : 2.48770
[1mStep[0m  [56/84], [94mLoss[0m : 2.31209
[1mStep[0m  [64/84], [94mLoss[0m : 2.59509
[1mStep[0m  [72/84], [94mLoss[0m : 2.62155
[1mStep[0m  [80/84], [94mLoss[0m : 2.81292

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.655, [92mTest[0m: 3.377, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36654
[1mStep[0m  [8/84], [94mLoss[0m : 2.41563
[1mStep[0m  [16/84], [94mLoss[0m : 2.74721
[1mStep[0m  [24/84], [94mLoss[0m : 2.50802
[1mStep[0m  [32/84], [94mLoss[0m : 2.48978
[1mStep[0m  [40/84], [94mLoss[0m : 2.66003
[1mStep[0m  [48/84], [94mLoss[0m : 2.54735
[1mStep[0m  [56/84], [94mLoss[0m : 2.39673
[1mStep[0m  [64/84], [94mLoss[0m : 2.69221
[1mStep[0m  [72/84], [94mLoss[0m : 2.62697
[1mStep[0m  [80/84], [94mLoss[0m : 2.63092

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.585, [92mTest[0m: 2.747, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49156
[1mStep[0m  [8/84], [94mLoss[0m : 3.04541
[1mStep[0m  [16/84], [94mLoss[0m : 2.55294
[1mStep[0m  [24/84], [94mLoss[0m : 2.49317
[1mStep[0m  [32/84], [94mLoss[0m : 2.75337
[1mStep[0m  [40/84], [94mLoss[0m : 2.20629
[1mStep[0m  [48/84], [94mLoss[0m : 2.57493
[1mStep[0m  [56/84], [94mLoss[0m : 2.53949
[1mStep[0m  [64/84], [94mLoss[0m : 2.29367
[1mStep[0m  [72/84], [94mLoss[0m : 2.45341
[1mStep[0m  [80/84], [94mLoss[0m : 2.34757

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.561, [92mTest[0m: 2.515, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.21958
[1mStep[0m  [8/84], [94mLoss[0m : 2.44374
[1mStep[0m  [16/84], [94mLoss[0m : 2.52393
[1mStep[0m  [24/84], [94mLoss[0m : 2.68097
[1mStep[0m  [32/84], [94mLoss[0m : 2.34223
[1mStep[0m  [40/84], [94mLoss[0m : 2.57077
[1mStep[0m  [48/84], [94mLoss[0m : 2.19826
[1mStep[0m  [56/84], [94mLoss[0m : 2.59690
[1mStep[0m  [64/84], [94mLoss[0m : 2.60315
[1mStep[0m  [72/84], [94mLoss[0m : 2.66812
[1mStep[0m  [80/84], [94mLoss[0m : 2.48986

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.433, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.34975
[1mStep[0m  [8/84], [94mLoss[0m : 2.42488
[1mStep[0m  [16/84], [94mLoss[0m : 2.37896
[1mStep[0m  [24/84], [94mLoss[0m : 2.42794
[1mStep[0m  [32/84], [94mLoss[0m : 2.28106
[1mStep[0m  [40/84], [94mLoss[0m : 2.49826
[1mStep[0m  [48/84], [94mLoss[0m : 2.59092
[1mStep[0m  [56/84], [94mLoss[0m : 2.52330
[1mStep[0m  [64/84], [94mLoss[0m : 2.25744
[1mStep[0m  [72/84], [94mLoss[0m : 2.70763
[1mStep[0m  [80/84], [94mLoss[0m : 2.53762

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.450, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46299
[1mStep[0m  [8/84], [94mLoss[0m : 2.47055
[1mStep[0m  [16/84], [94mLoss[0m : 2.33017
[1mStep[0m  [24/84], [94mLoss[0m : 2.44508
[1mStep[0m  [32/84], [94mLoss[0m : 2.32193
[1mStep[0m  [40/84], [94mLoss[0m : 2.41534
[1mStep[0m  [48/84], [94mLoss[0m : 2.36015
[1mStep[0m  [56/84], [94mLoss[0m : 2.34638
[1mStep[0m  [64/84], [94mLoss[0m : 2.55938
[1mStep[0m  [72/84], [94mLoss[0m : 2.81900
[1mStep[0m  [80/84], [94mLoss[0m : 2.48106

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.438, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50187
[1mStep[0m  [8/84], [94mLoss[0m : 2.63162
[1mStep[0m  [16/84], [94mLoss[0m : 2.56367
[1mStep[0m  [24/84], [94mLoss[0m : 2.62868
[1mStep[0m  [32/84], [94mLoss[0m : 2.73782
[1mStep[0m  [40/84], [94mLoss[0m : 2.45273
[1mStep[0m  [48/84], [94mLoss[0m : 2.58676
[1mStep[0m  [56/84], [94mLoss[0m : 2.63182
[1mStep[0m  [64/84], [94mLoss[0m : 2.28359
[1mStep[0m  [72/84], [94mLoss[0m : 2.60739
[1mStep[0m  [80/84], [94mLoss[0m : 2.51558

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.399, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.28988
[1mStep[0m  [8/84], [94mLoss[0m : 2.19727
[1mStep[0m  [16/84], [94mLoss[0m : 2.90147
[1mStep[0m  [24/84], [94mLoss[0m : 2.30218
[1mStep[0m  [32/84], [94mLoss[0m : 2.37621
[1mStep[0m  [40/84], [94mLoss[0m : 2.46108
[1mStep[0m  [48/84], [94mLoss[0m : 2.36963
[1mStep[0m  [56/84], [94mLoss[0m : 2.59271
[1mStep[0m  [64/84], [94mLoss[0m : 2.50886
[1mStep[0m  [72/84], [94mLoss[0m : 2.40072
[1mStep[0m  [80/84], [94mLoss[0m : 2.56020

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.402, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.29587
[1mStep[0m  [8/84], [94mLoss[0m : 2.50449
[1mStep[0m  [16/84], [94mLoss[0m : 2.40078
[1mStep[0m  [24/84], [94mLoss[0m : 2.24162
[1mStep[0m  [32/84], [94mLoss[0m : 2.58072
[1mStep[0m  [40/84], [94mLoss[0m : 2.27025
[1mStep[0m  [48/84], [94mLoss[0m : 2.48208
[1mStep[0m  [56/84], [94mLoss[0m : 2.40105
[1mStep[0m  [64/84], [94mLoss[0m : 2.42348
[1mStep[0m  [72/84], [94mLoss[0m : 2.48624
[1mStep[0m  [80/84], [94mLoss[0m : 2.61483

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.383, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.65715
[1mStep[0m  [8/84], [94mLoss[0m : 2.44989
[1mStep[0m  [16/84], [94mLoss[0m : 1.99377
[1mStep[0m  [24/84], [94mLoss[0m : 2.59204
[1mStep[0m  [32/84], [94mLoss[0m : 2.74774
[1mStep[0m  [40/84], [94mLoss[0m : 2.25128
[1mStep[0m  [48/84], [94mLoss[0m : 2.40768
[1mStep[0m  [56/84], [94mLoss[0m : 2.53154
[1mStep[0m  [64/84], [94mLoss[0m : 2.72267
[1mStep[0m  [72/84], [94mLoss[0m : 2.20481
[1mStep[0m  [80/84], [94mLoss[0m : 2.76276

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.367, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41563
[1mStep[0m  [8/84], [94mLoss[0m : 2.26683
[1mStep[0m  [16/84], [94mLoss[0m : 2.48261
[1mStep[0m  [24/84], [94mLoss[0m : 1.96166
[1mStep[0m  [32/84], [94mLoss[0m : 2.55050
[1mStep[0m  [40/84], [94mLoss[0m : 2.46654
[1mStep[0m  [48/84], [94mLoss[0m : 2.34281
[1mStep[0m  [56/84], [94mLoss[0m : 2.32202
[1mStep[0m  [64/84], [94mLoss[0m : 2.38111
[1mStep[0m  [72/84], [94mLoss[0m : 2.57161
[1mStep[0m  [80/84], [94mLoss[0m : 2.43363

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.367, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.80837
[1mStep[0m  [8/84], [94mLoss[0m : 2.30494
[1mStep[0m  [16/84], [94mLoss[0m : 2.46135
[1mStep[0m  [24/84], [94mLoss[0m : 2.44230
[1mStep[0m  [32/84], [94mLoss[0m : 2.56329
[1mStep[0m  [40/84], [94mLoss[0m : 2.42623
[1mStep[0m  [48/84], [94mLoss[0m : 2.62508
[1mStep[0m  [56/84], [94mLoss[0m : 2.34285
[1mStep[0m  [64/84], [94mLoss[0m : 1.99054
[1mStep[0m  [72/84], [94mLoss[0m : 2.52676
[1mStep[0m  [80/84], [94mLoss[0m : 2.45282

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.344, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.62582
[1mStep[0m  [8/84], [94mLoss[0m : 2.26706
[1mStep[0m  [16/84], [94mLoss[0m : 2.72950
[1mStep[0m  [24/84], [94mLoss[0m : 2.43866
[1mStep[0m  [32/84], [94mLoss[0m : 2.40847
[1mStep[0m  [40/84], [94mLoss[0m : 2.58094
[1mStep[0m  [48/84], [94mLoss[0m : 2.14348
[1mStep[0m  [56/84], [94mLoss[0m : 2.53007
[1mStep[0m  [64/84], [94mLoss[0m : 2.48045
[1mStep[0m  [72/84], [94mLoss[0m : 2.28157
[1mStep[0m  [80/84], [94mLoss[0m : 2.18714

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.356, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.24671
[1mStep[0m  [8/84], [94mLoss[0m : 2.51151
[1mStep[0m  [16/84], [94mLoss[0m : 2.33846
[1mStep[0m  [24/84], [94mLoss[0m : 2.36347
[1mStep[0m  [32/84], [94mLoss[0m : 2.52465
[1mStep[0m  [40/84], [94mLoss[0m : 2.70308
[1mStep[0m  [48/84], [94mLoss[0m : 2.59902
[1mStep[0m  [56/84], [94mLoss[0m : 2.48734
[1mStep[0m  [64/84], [94mLoss[0m : 2.65775
[1mStep[0m  [72/84], [94mLoss[0m : 2.51106
[1mStep[0m  [80/84], [94mLoss[0m : 2.65017

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.374, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48592
[1mStep[0m  [8/84], [94mLoss[0m : 2.34034
[1mStep[0m  [16/84], [94mLoss[0m : 2.62469
[1mStep[0m  [24/84], [94mLoss[0m : 2.24012
[1mStep[0m  [32/84], [94mLoss[0m : 2.43941
[1mStep[0m  [40/84], [94mLoss[0m : 2.20131
[1mStep[0m  [48/84], [94mLoss[0m : 2.52301
[1mStep[0m  [56/84], [94mLoss[0m : 2.42130
[1mStep[0m  [64/84], [94mLoss[0m : 2.43650
[1mStep[0m  [72/84], [94mLoss[0m : 2.17100
[1mStep[0m  [80/84], [94mLoss[0m : 2.32705

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.391, [92mTest[0m: 2.354, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.18950
[1mStep[0m  [8/84], [94mLoss[0m : 2.44782
[1mStep[0m  [16/84], [94mLoss[0m : 2.48911
[1mStep[0m  [24/84], [94mLoss[0m : 2.14182
[1mStep[0m  [32/84], [94mLoss[0m : 2.55313
[1mStep[0m  [40/84], [94mLoss[0m : 2.21821
[1mStep[0m  [48/84], [94mLoss[0m : 2.40602
[1mStep[0m  [56/84], [94mLoss[0m : 2.29328
[1mStep[0m  [64/84], [94mLoss[0m : 2.27931
[1mStep[0m  [72/84], [94mLoss[0m : 2.15597
[1mStep[0m  [80/84], [94mLoss[0m : 2.51771

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.347, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41623
[1mStep[0m  [8/84], [94mLoss[0m : 2.28532
[1mStep[0m  [16/84], [94mLoss[0m : 2.49317
[1mStep[0m  [24/84], [94mLoss[0m : 2.24371
[1mStep[0m  [32/84], [94mLoss[0m : 2.17090
[1mStep[0m  [40/84], [94mLoss[0m : 2.87865
[1mStep[0m  [48/84], [94mLoss[0m : 2.51786
[1mStep[0m  [56/84], [94mLoss[0m : 2.45631
[1mStep[0m  [64/84], [94mLoss[0m : 2.32439
[1mStep[0m  [72/84], [94mLoss[0m : 2.64355
[1mStep[0m  [80/84], [94mLoss[0m : 2.07272

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.381, [92mTest[0m: 2.338, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46763
[1mStep[0m  [8/84], [94mLoss[0m : 2.17603
[1mStep[0m  [16/84], [94mLoss[0m : 2.21284
[1mStep[0m  [24/84], [94mLoss[0m : 2.29941
[1mStep[0m  [32/84], [94mLoss[0m : 2.25989
[1mStep[0m  [40/84], [94mLoss[0m : 2.30551
[1mStep[0m  [48/84], [94mLoss[0m : 2.56591
[1mStep[0m  [56/84], [94mLoss[0m : 2.70981
[1mStep[0m  [64/84], [94mLoss[0m : 2.22479
[1mStep[0m  [72/84], [94mLoss[0m : 2.44553
[1mStep[0m  [80/84], [94mLoss[0m : 2.15466

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.344, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.09097
[1mStep[0m  [8/84], [94mLoss[0m : 2.32853
[1mStep[0m  [16/84], [94mLoss[0m : 2.31247
[1mStep[0m  [24/84], [94mLoss[0m : 2.57628
[1mStep[0m  [32/84], [94mLoss[0m : 2.45935
[1mStep[0m  [40/84], [94mLoss[0m : 2.37335
[1mStep[0m  [48/84], [94mLoss[0m : 2.10806
[1mStep[0m  [56/84], [94mLoss[0m : 2.31588
[1mStep[0m  [64/84], [94mLoss[0m : 2.34640
[1mStep[0m  [72/84], [94mLoss[0m : 2.28073
[1mStep[0m  [80/84], [94mLoss[0m : 2.43345

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.376, [92mTest[0m: 2.343, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49925
[1mStep[0m  [8/84], [94mLoss[0m : 2.41354
[1mStep[0m  [16/84], [94mLoss[0m : 2.18231
[1mStep[0m  [24/84], [94mLoss[0m : 2.38318
[1mStep[0m  [32/84], [94mLoss[0m : 2.39418
[1mStep[0m  [40/84], [94mLoss[0m : 2.38254
[1mStep[0m  [48/84], [94mLoss[0m : 2.36533
[1mStep[0m  [56/84], [94mLoss[0m : 2.31211
[1mStep[0m  [64/84], [94mLoss[0m : 2.44263
[1mStep[0m  [72/84], [94mLoss[0m : 2.27230
[1mStep[0m  [80/84], [94mLoss[0m : 2.46376

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.384, [92mTest[0m: 2.354, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52281
[1mStep[0m  [8/84], [94mLoss[0m : 2.08469
[1mStep[0m  [16/84], [94mLoss[0m : 2.25984
[1mStep[0m  [24/84], [94mLoss[0m : 2.39704
[1mStep[0m  [32/84], [94mLoss[0m : 2.34444
[1mStep[0m  [40/84], [94mLoss[0m : 2.23224
[1mStep[0m  [48/84], [94mLoss[0m : 2.72991
[1mStep[0m  [56/84], [94mLoss[0m : 2.39749
[1mStep[0m  [64/84], [94mLoss[0m : 2.23193
[1mStep[0m  [72/84], [94mLoss[0m : 2.34869
[1mStep[0m  [80/84], [94mLoss[0m : 2.45419

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.370, [92mTest[0m: 2.377, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43833
[1mStep[0m  [8/84], [94mLoss[0m : 2.19076
[1mStep[0m  [16/84], [94mLoss[0m : 2.57545
[1mStep[0m  [24/84], [94mLoss[0m : 2.09409
[1mStep[0m  [32/84], [94mLoss[0m : 2.18154
[1mStep[0m  [40/84], [94mLoss[0m : 2.29882
[1mStep[0m  [48/84], [94mLoss[0m : 2.42803
[1mStep[0m  [56/84], [94mLoss[0m : 2.39628
[1mStep[0m  [64/84], [94mLoss[0m : 2.12066
[1mStep[0m  [72/84], [94mLoss[0m : 2.06870
[1mStep[0m  [80/84], [94mLoss[0m : 2.61831

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.343, [92mTest[0m: 2.335, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59364
[1mStep[0m  [8/84], [94mLoss[0m : 2.30524
[1mStep[0m  [16/84], [94mLoss[0m : 2.42453
[1mStep[0m  [24/84], [94mLoss[0m : 2.13868
[1mStep[0m  [32/84], [94mLoss[0m : 2.26889
[1mStep[0m  [40/84], [94mLoss[0m : 2.28091
[1mStep[0m  [48/84], [94mLoss[0m : 2.20984
[1mStep[0m  [56/84], [94mLoss[0m : 2.30175
[1mStep[0m  [64/84], [94mLoss[0m : 2.36039
[1mStep[0m  [72/84], [94mLoss[0m : 2.30624
[1mStep[0m  [80/84], [94mLoss[0m : 2.53116

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.337, [92mTest[0m: 2.338, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40368
[1mStep[0m  [8/84], [94mLoss[0m : 2.24050
[1mStep[0m  [16/84], [94mLoss[0m : 2.68700
[1mStep[0m  [24/84], [94mLoss[0m : 2.32783
[1mStep[0m  [32/84], [94mLoss[0m : 2.45461
[1mStep[0m  [40/84], [94mLoss[0m : 2.02960
[1mStep[0m  [48/84], [94mLoss[0m : 2.25562
[1mStep[0m  [56/84], [94mLoss[0m : 2.38233
[1mStep[0m  [64/84], [94mLoss[0m : 2.47646
[1mStep[0m  [72/84], [94mLoss[0m : 2.33002
[1mStep[0m  [80/84], [94mLoss[0m : 2.39667

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.342, [92mTest[0m: 2.346, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.21274
[1mStep[0m  [8/84], [94mLoss[0m : 2.33835
[1mStep[0m  [16/84], [94mLoss[0m : 2.12752
[1mStep[0m  [24/84], [94mLoss[0m : 2.17622
[1mStep[0m  [32/84], [94mLoss[0m : 2.41656
[1mStep[0m  [40/84], [94mLoss[0m : 2.15029
[1mStep[0m  [48/84], [94mLoss[0m : 2.38863
[1mStep[0m  [56/84], [94mLoss[0m : 2.17683
[1mStep[0m  [64/84], [94mLoss[0m : 2.55970
[1mStep[0m  [72/84], [94mLoss[0m : 2.32131
[1mStep[0m  [80/84], [94mLoss[0m : 2.43719

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.344, [92mTest[0m: 2.341, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36074
[1mStep[0m  [8/84], [94mLoss[0m : 2.54071
[1mStep[0m  [16/84], [94mLoss[0m : 2.38090
[1mStep[0m  [24/84], [94mLoss[0m : 2.13781
[1mStep[0m  [32/84], [94mLoss[0m : 2.38677
[1mStep[0m  [40/84], [94mLoss[0m : 2.54949
[1mStep[0m  [48/84], [94mLoss[0m : 2.13408
[1mStep[0m  [56/84], [94mLoss[0m : 2.33147
[1mStep[0m  [64/84], [94mLoss[0m : 2.48007
[1mStep[0m  [72/84], [94mLoss[0m : 2.27038
[1mStep[0m  [80/84], [94mLoss[0m : 2.64242

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.333, [92mTest[0m: 2.333, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.24328
[1mStep[0m  [8/84], [94mLoss[0m : 2.22733
[1mStep[0m  [16/84], [94mLoss[0m : 2.72645
[1mStep[0m  [24/84], [94mLoss[0m : 2.17569
[1mStep[0m  [32/84], [94mLoss[0m : 2.01907
[1mStep[0m  [40/84], [94mLoss[0m : 2.38911
[1mStep[0m  [48/84], [94mLoss[0m : 2.38352
[1mStep[0m  [56/84], [94mLoss[0m : 2.29702
[1mStep[0m  [64/84], [94mLoss[0m : 2.21036
[1mStep[0m  [72/84], [94mLoss[0m : 2.16777
[1mStep[0m  [80/84], [94mLoss[0m : 2.05264

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.332, [92mTest[0m: 2.333, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35825
[1mStep[0m  [8/84], [94mLoss[0m : 2.41869
[1mStep[0m  [16/84], [94mLoss[0m : 2.52298
[1mStep[0m  [24/84], [94mLoss[0m : 2.18365
[1mStep[0m  [32/84], [94mLoss[0m : 2.38347
[1mStep[0m  [40/84], [94mLoss[0m : 2.41450
[1mStep[0m  [48/84], [94mLoss[0m : 2.37387
[1mStep[0m  [56/84], [94mLoss[0m : 2.30548
[1mStep[0m  [64/84], [94mLoss[0m : 2.40981
[1mStep[0m  [72/84], [94mLoss[0m : 2.24677
[1mStep[0m  [80/84], [94mLoss[0m : 2.35206

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.327, [92mTest[0m: 2.316, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.19597
[1mStep[0m  [8/84], [94mLoss[0m : 2.24559
[1mStep[0m  [16/84], [94mLoss[0m : 2.27148
[1mStep[0m  [24/84], [94mLoss[0m : 2.34563
[1mStep[0m  [32/84], [94mLoss[0m : 2.44198
[1mStep[0m  [40/84], [94mLoss[0m : 2.15871
[1mStep[0m  [48/84], [94mLoss[0m : 2.25025
[1mStep[0m  [56/84], [94mLoss[0m : 2.28193
[1mStep[0m  [64/84], [94mLoss[0m : 2.13097
[1mStep[0m  [72/84], [94mLoss[0m : 2.30091
[1mStep[0m  [80/84], [94mLoss[0m : 2.22545

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.337, [92mTest[0m: 2.318, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.335
====================================

Phase 1 - Evaluation MAE:  2.3354389795235226
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 2.35594
[1mStep[0m  [8/84], [94mLoss[0m : 2.28675
[1mStep[0m  [16/84], [94mLoss[0m : 2.42899
[1mStep[0m  [24/84], [94mLoss[0m : 2.47174
[1mStep[0m  [32/84], [94mLoss[0m : 2.34310
[1mStep[0m  [40/84], [94mLoss[0m : 2.34964
[1mStep[0m  [48/84], [94mLoss[0m : 2.29986
[1mStep[0m  [56/84], [94mLoss[0m : 2.20493
[1mStep[0m  [64/84], [94mLoss[0m : 2.46727
[1mStep[0m  [72/84], [94mLoss[0m : 2.49978
[1mStep[0m  [80/84], [94mLoss[0m : 2.56514

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.335, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67000
[1mStep[0m  [8/84], [94mLoss[0m : 2.39343
[1mStep[0m  [16/84], [94mLoss[0m : 2.07611
[1mStep[0m  [24/84], [94mLoss[0m : 2.29947
[1mStep[0m  [32/84], [94mLoss[0m : 2.46402
[1mStep[0m  [40/84], [94mLoss[0m : 2.33293
[1mStep[0m  [48/84], [94mLoss[0m : 2.61735
[1mStep[0m  [56/84], [94mLoss[0m : 2.52701
[1mStep[0m  [64/84], [94mLoss[0m : 2.39853
[1mStep[0m  [72/84], [94mLoss[0m : 2.24750
[1mStep[0m  [80/84], [94mLoss[0m : 2.58371

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.440, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27858
[1mStep[0m  [8/84], [94mLoss[0m : 2.33474
[1mStep[0m  [16/84], [94mLoss[0m : 2.15589
[1mStep[0m  [24/84], [94mLoss[0m : 2.33552
[1mStep[0m  [32/84], [94mLoss[0m : 2.50378
[1mStep[0m  [40/84], [94mLoss[0m : 2.45077
[1mStep[0m  [48/84], [94mLoss[0m : 2.28305
[1mStep[0m  [56/84], [94mLoss[0m : 2.36811
[1mStep[0m  [64/84], [94mLoss[0m : 2.09059
[1mStep[0m  [72/84], [94mLoss[0m : 2.36982
[1mStep[0m  [80/84], [94mLoss[0m : 2.23298

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.353, [92mTest[0m: 2.437, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.25657
[1mStep[0m  [8/84], [94mLoss[0m : 1.95627
[1mStep[0m  [16/84], [94mLoss[0m : 2.44279
[1mStep[0m  [24/84], [94mLoss[0m : 2.38878
[1mStep[0m  [32/84], [94mLoss[0m : 2.57916
[1mStep[0m  [40/84], [94mLoss[0m : 2.59500
[1mStep[0m  [48/84], [94mLoss[0m : 2.52668
[1mStep[0m  [56/84], [94mLoss[0m : 2.48819
[1mStep[0m  [64/84], [94mLoss[0m : 2.35580
[1mStep[0m  [72/84], [94mLoss[0m : 2.35333
[1mStep[0m  [80/84], [94mLoss[0m : 2.49127

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.318, [92mTest[0m: 2.543, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27592
[1mStep[0m  [8/84], [94mLoss[0m : 2.28945
[1mStep[0m  [16/84], [94mLoss[0m : 2.40460
[1mStep[0m  [24/84], [94mLoss[0m : 2.12202
[1mStep[0m  [32/84], [94mLoss[0m : 2.14171
[1mStep[0m  [40/84], [94mLoss[0m : 2.15314
[1mStep[0m  [48/84], [94mLoss[0m : 2.26332
[1mStep[0m  [56/84], [94mLoss[0m : 2.34490
[1mStep[0m  [64/84], [94mLoss[0m : 2.08633
[1mStep[0m  [72/84], [94mLoss[0m : 2.18147
[1mStep[0m  [80/84], [94mLoss[0m : 1.97643

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.241, [92mTest[0m: 2.370, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.15415
[1mStep[0m  [8/84], [94mLoss[0m : 2.45174
[1mStep[0m  [16/84], [94mLoss[0m : 1.89526
[1mStep[0m  [24/84], [94mLoss[0m : 2.33256
[1mStep[0m  [32/84], [94mLoss[0m : 1.83338
[1mStep[0m  [40/84], [94mLoss[0m : 1.99587
[1mStep[0m  [48/84], [94mLoss[0m : 2.09332
[1mStep[0m  [56/84], [94mLoss[0m : 1.93524
[1mStep[0m  [64/84], [94mLoss[0m : 2.51845
[1mStep[0m  [72/84], [94mLoss[0m : 1.94457
[1mStep[0m  [80/84], [94mLoss[0m : 2.26110

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.209, [92mTest[0m: 2.394, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.90382
[1mStep[0m  [8/84], [94mLoss[0m : 1.92298
[1mStep[0m  [16/84], [94mLoss[0m : 1.97394
[1mStep[0m  [24/84], [94mLoss[0m : 2.15519
[1mStep[0m  [32/84], [94mLoss[0m : 2.15051
[1mStep[0m  [40/84], [94mLoss[0m : 1.94608
[1mStep[0m  [48/84], [94mLoss[0m : 2.22995
[1mStep[0m  [56/84], [94mLoss[0m : 2.07419
[1mStep[0m  [64/84], [94mLoss[0m : 1.77434
[1mStep[0m  [72/84], [94mLoss[0m : 2.44542
[1mStep[0m  [80/84], [94mLoss[0m : 2.22314

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.156, [92mTest[0m: 2.419, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.02271
[1mStep[0m  [8/84], [94mLoss[0m : 1.87391
[1mStep[0m  [16/84], [94mLoss[0m : 2.21000
[1mStep[0m  [24/84], [94mLoss[0m : 1.96457
[1mStep[0m  [32/84], [94mLoss[0m : 2.05618
[1mStep[0m  [40/84], [94mLoss[0m : 2.13017
[1mStep[0m  [48/84], [94mLoss[0m : 2.14919
[1mStep[0m  [56/84], [94mLoss[0m : 2.23124
[1mStep[0m  [64/84], [94mLoss[0m : 2.04823
[1mStep[0m  [72/84], [94mLoss[0m : 2.10399
[1mStep[0m  [80/84], [94mLoss[0m : 2.09114

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.123, [92mTest[0m: 2.515, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.97964
[1mStep[0m  [8/84], [94mLoss[0m : 2.16474
[1mStep[0m  [16/84], [94mLoss[0m : 2.19912
[1mStep[0m  [24/84], [94mLoss[0m : 1.91406
[1mStep[0m  [32/84], [94mLoss[0m : 1.97811
[1mStep[0m  [40/84], [94mLoss[0m : 1.79511
[1mStep[0m  [48/84], [94mLoss[0m : 1.89656
[1mStep[0m  [56/84], [94mLoss[0m : 2.18764
[1mStep[0m  [64/84], [94mLoss[0m : 2.06565
[1mStep[0m  [72/84], [94mLoss[0m : 1.96235
[1mStep[0m  [80/84], [94mLoss[0m : 1.85823

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.048, [92mTest[0m: 2.397, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.71254
[1mStep[0m  [8/84], [94mLoss[0m : 1.98771
[1mStep[0m  [16/84], [94mLoss[0m : 2.17066
[1mStep[0m  [24/84], [94mLoss[0m : 1.99407
[1mStep[0m  [32/84], [94mLoss[0m : 1.85251
[1mStep[0m  [40/84], [94mLoss[0m : 1.96068
[1mStep[0m  [48/84], [94mLoss[0m : 1.93404
[1mStep[0m  [56/84], [94mLoss[0m : 2.06701
[1mStep[0m  [64/84], [94mLoss[0m : 2.05594
[1mStep[0m  [72/84], [94mLoss[0m : 1.89509
[1mStep[0m  [80/84], [94mLoss[0m : 1.78418

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.021, [92mTest[0m: 2.427, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.00374
[1mStep[0m  [8/84], [94mLoss[0m : 1.87995
[1mStep[0m  [16/84], [94mLoss[0m : 1.86421
[1mStep[0m  [24/84], [94mLoss[0m : 2.07789
[1mStep[0m  [32/84], [94mLoss[0m : 1.78612
[1mStep[0m  [40/84], [94mLoss[0m : 2.18868
[1mStep[0m  [48/84], [94mLoss[0m : 2.08936
[1mStep[0m  [56/84], [94mLoss[0m : 2.15049
[1mStep[0m  [64/84], [94mLoss[0m : 2.29167
[1mStep[0m  [72/84], [94mLoss[0m : 2.07613
[1mStep[0m  [80/84], [94mLoss[0m : 2.02949

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.982, [92mTest[0m: 2.431, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.80043
[1mStep[0m  [8/84], [94mLoss[0m : 1.93345
[1mStep[0m  [16/84], [94mLoss[0m : 1.91267
[1mStep[0m  [24/84], [94mLoss[0m : 1.88426
[1mStep[0m  [32/84], [94mLoss[0m : 1.81725
[1mStep[0m  [40/84], [94mLoss[0m : 1.89725
[1mStep[0m  [48/84], [94mLoss[0m : 1.81475
[1mStep[0m  [56/84], [94mLoss[0m : 1.99658
[1mStep[0m  [64/84], [94mLoss[0m : 1.98930
[1mStep[0m  [72/84], [94mLoss[0m : 1.89444
[1mStep[0m  [80/84], [94mLoss[0m : 2.01518

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.930, [92mTest[0m: 2.515, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.88880
[1mStep[0m  [8/84], [94mLoss[0m : 1.69879
[1mStep[0m  [16/84], [94mLoss[0m : 1.92962
[1mStep[0m  [24/84], [94mLoss[0m : 1.88623
[1mStep[0m  [32/84], [94mLoss[0m : 1.88580
[1mStep[0m  [40/84], [94mLoss[0m : 1.86472
[1mStep[0m  [48/84], [94mLoss[0m : 1.65077
[1mStep[0m  [56/84], [94mLoss[0m : 2.12685
[1mStep[0m  [64/84], [94mLoss[0m : 1.94005
[1mStep[0m  [72/84], [94mLoss[0m : 1.80795
[1mStep[0m  [80/84], [94mLoss[0m : 1.69578

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.905, [92mTest[0m: 2.484, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.85782
[1mStep[0m  [8/84], [94mLoss[0m : 1.71513
[1mStep[0m  [16/84], [94mLoss[0m : 1.79798
[1mStep[0m  [24/84], [94mLoss[0m : 1.90127
[1mStep[0m  [32/84], [94mLoss[0m : 1.84044
[1mStep[0m  [40/84], [94mLoss[0m : 2.03939
[1mStep[0m  [48/84], [94mLoss[0m : 2.20025
[1mStep[0m  [56/84], [94mLoss[0m : 1.66933
[1mStep[0m  [64/84], [94mLoss[0m : 1.92803
[1mStep[0m  [72/84], [94mLoss[0m : 1.84088
[1mStep[0m  [80/84], [94mLoss[0m : 1.79603

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.871, [92mTest[0m: 2.405, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.93912
[1mStep[0m  [8/84], [94mLoss[0m : 2.12841
[1mStep[0m  [16/84], [94mLoss[0m : 2.00265
[1mStep[0m  [24/84], [94mLoss[0m : 1.83766
[1mStep[0m  [32/84], [94mLoss[0m : 1.77068
[1mStep[0m  [40/84], [94mLoss[0m : 1.83681
[1mStep[0m  [48/84], [94mLoss[0m : 2.09037
[1mStep[0m  [56/84], [94mLoss[0m : 2.09564
[1mStep[0m  [64/84], [94mLoss[0m : 1.91780
[1mStep[0m  [72/84], [94mLoss[0m : 1.65443
[1mStep[0m  [80/84], [94mLoss[0m : 1.84178

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.854, [92mTest[0m: 2.434, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.78530
[1mStep[0m  [8/84], [94mLoss[0m : 1.69401
[1mStep[0m  [16/84], [94mLoss[0m : 1.64054
[1mStep[0m  [24/84], [94mLoss[0m : 2.01776
[1mStep[0m  [32/84], [94mLoss[0m : 1.80772
[1mStep[0m  [40/84], [94mLoss[0m : 1.64141
[1mStep[0m  [48/84], [94mLoss[0m : 2.00218
[1mStep[0m  [56/84], [94mLoss[0m : 1.90471
[1mStep[0m  [64/84], [94mLoss[0m : 1.67974
[1mStep[0m  [72/84], [94mLoss[0m : 1.74013
[1mStep[0m  [80/84], [94mLoss[0m : 1.80651

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.809, [92mTest[0m: 2.490, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.54798
[1mStep[0m  [8/84], [94mLoss[0m : 1.59422
[1mStep[0m  [16/84], [94mLoss[0m : 1.81800
[1mStep[0m  [24/84], [94mLoss[0m : 1.74682
[1mStep[0m  [32/84], [94mLoss[0m : 1.64928
[1mStep[0m  [40/84], [94mLoss[0m : 2.04368
[1mStep[0m  [48/84], [94mLoss[0m : 1.88636
[1mStep[0m  [56/84], [94mLoss[0m : 1.90262
[1mStep[0m  [64/84], [94mLoss[0m : 1.75826
[1mStep[0m  [72/84], [94mLoss[0m : 1.72564
[1mStep[0m  [80/84], [94mLoss[0m : 1.88749

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.783, [92mTest[0m: 2.526, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.65110
[1mStep[0m  [8/84], [94mLoss[0m : 1.77454
[1mStep[0m  [16/84], [94mLoss[0m : 1.55811
[1mStep[0m  [24/84], [94mLoss[0m : 1.83836
[1mStep[0m  [32/84], [94mLoss[0m : 1.61234
[1mStep[0m  [40/84], [94mLoss[0m : 1.81762
[1mStep[0m  [48/84], [94mLoss[0m : 1.64433
[1mStep[0m  [56/84], [94mLoss[0m : 1.66358
[1mStep[0m  [64/84], [94mLoss[0m : 1.80366
[1mStep[0m  [72/84], [94mLoss[0m : 1.80464
[1mStep[0m  [80/84], [94mLoss[0m : 1.86278

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.735, [92mTest[0m: 2.500, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.74529
[1mStep[0m  [8/84], [94mLoss[0m : 1.57124
[1mStep[0m  [16/84], [94mLoss[0m : 1.69960
[1mStep[0m  [24/84], [94mLoss[0m : 1.90288
[1mStep[0m  [32/84], [94mLoss[0m : 1.86340
[1mStep[0m  [40/84], [94mLoss[0m : 1.54940
[1mStep[0m  [48/84], [94mLoss[0m : 1.79894
[1mStep[0m  [56/84], [94mLoss[0m : 1.75802
[1mStep[0m  [64/84], [94mLoss[0m : 1.71900
[1mStep[0m  [72/84], [94mLoss[0m : 1.66014
[1mStep[0m  [80/84], [94mLoss[0m : 1.55354

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.700, [92mTest[0m: 2.517, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.57516
[1mStep[0m  [8/84], [94mLoss[0m : 1.57865
[1mStep[0m  [16/84], [94mLoss[0m : 1.69870
[1mStep[0m  [24/84], [94mLoss[0m : 1.75568
[1mStep[0m  [32/84], [94mLoss[0m : 1.48611
[1mStep[0m  [40/84], [94mLoss[0m : 1.86955
[1mStep[0m  [48/84], [94mLoss[0m : 1.67760
[1mStep[0m  [56/84], [94mLoss[0m : 1.74187
[1mStep[0m  [64/84], [94mLoss[0m : 1.66804
[1mStep[0m  [72/84], [94mLoss[0m : 1.80617
[1mStep[0m  [80/84], [94mLoss[0m : 1.61583

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.673, [92mTest[0m: 2.495, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.52211
[1mStep[0m  [8/84], [94mLoss[0m : 1.68243
[1mStep[0m  [16/84], [94mLoss[0m : 1.53008
[1mStep[0m  [24/84], [94mLoss[0m : 1.63912
[1mStep[0m  [32/84], [94mLoss[0m : 1.55626
[1mStep[0m  [40/84], [94mLoss[0m : 1.52270
[1mStep[0m  [48/84], [94mLoss[0m : 1.61786
[1mStep[0m  [56/84], [94mLoss[0m : 1.41424
[1mStep[0m  [64/84], [94mLoss[0m : 1.67360
[1mStep[0m  [72/84], [94mLoss[0m : 1.72126
[1mStep[0m  [80/84], [94mLoss[0m : 1.77138

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.640, [92mTest[0m: 2.486, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.70821
[1mStep[0m  [8/84], [94mLoss[0m : 1.62825
[1mStep[0m  [16/84], [94mLoss[0m : 1.61797
[1mStep[0m  [24/84], [94mLoss[0m : 1.80828
[1mStep[0m  [32/84], [94mLoss[0m : 1.47029
[1mStep[0m  [40/84], [94mLoss[0m : 1.55181
[1mStep[0m  [48/84], [94mLoss[0m : 1.55482
[1mStep[0m  [56/84], [94mLoss[0m : 2.11939
[1mStep[0m  [64/84], [94mLoss[0m : 1.52926
[1mStep[0m  [72/84], [94mLoss[0m : 1.43229
[1mStep[0m  [80/84], [94mLoss[0m : 1.63967

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.599, [92mTest[0m: 2.484, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.52705
[1mStep[0m  [8/84], [94mLoss[0m : 1.42098
[1mStep[0m  [16/84], [94mLoss[0m : 1.68203
[1mStep[0m  [24/84], [94mLoss[0m : 1.49012
[1mStep[0m  [32/84], [94mLoss[0m : 1.36104
[1mStep[0m  [40/84], [94mLoss[0m : 1.59948
[1mStep[0m  [48/84], [94mLoss[0m : 1.52870
[1mStep[0m  [56/84], [94mLoss[0m : 1.44514
[1mStep[0m  [64/84], [94mLoss[0m : 1.67797
[1mStep[0m  [72/84], [94mLoss[0m : 1.56363
[1mStep[0m  [80/84], [94mLoss[0m : 1.41348

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.594, [92mTest[0m: 2.502, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.40544
[1mStep[0m  [8/84], [94mLoss[0m : 1.41255
[1mStep[0m  [16/84], [94mLoss[0m : 1.28131
[1mStep[0m  [24/84], [94mLoss[0m : 1.73507
[1mStep[0m  [32/84], [94mLoss[0m : 1.57066
[1mStep[0m  [40/84], [94mLoss[0m : 1.48944
[1mStep[0m  [48/84], [94mLoss[0m : 1.50344
[1mStep[0m  [56/84], [94mLoss[0m : 1.63103
[1mStep[0m  [64/84], [94mLoss[0m : 1.66647
[1mStep[0m  [72/84], [94mLoss[0m : 1.68270
[1mStep[0m  [80/84], [94mLoss[0m : 1.74525

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.563, [92mTest[0m: 2.509, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.65438
[1mStep[0m  [8/84], [94mLoss[0m : 1.54951
[1mStep[0m  [16/84], [94mLoss[0m : 1.45339
[1mStep[0m  [24/84], [94mLoss[0m : 1.52018
[1mStep[0m  [32/84], [94mLoss[0m : 1.49165
[1mStep[0m  [40/84], [94mLoss[0m : 1.55601
[1mStep[0m  [48/84], [94mLoss[0m : 1.48478
[1mStep[0m  [56/84], [94mLoss[0m : 1.47998
[1mStep[0m  [64/84], [94mLoss[0m : 1.45876
[1mStep[0m  [72/84], [94mLoss[0m : 1.44025
[1mStep[0m  [80/84], [94mLoss[0m : 1.75853

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.541, [92mTest[0m: 2.486, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.44142
[1mStep[0m  [8/84], [94mLoss[0m : 1.69623
[1mStep[0m  [16/84], [94mLoss[0m : 1.79595
[1mStep[0m  [24/84], [94mLoss[0m : 1.52741
[1mStep[0m  [32/84], [94mLoss[0m : 1.63588
[1mStep[0m  [40/84], [94mLoss[0m : 1.90054
[1mStep[0m  [48/84], [94mLoss[0m : 1.53098
[1mStep[0m  [56/84], [94mLoss[0m : 1.39128
[1mStep[0m  [64/84], [94mLoss[0m : 1.55556
[1mStep[0m  [72/84], [94mLoss[0m : 1.62586
[1mStep[0m  [80/84], [94mLoss[0m : 1.59182

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.530, [92mTest[0m: 2.529, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.60791
[1mStep[0m  [8/84], [94mLoss[0m : 1.55871
[1mStep[0m  [16/84], [94mLoss[0m : 1.55871
[1mStep[0m  [24/84], [94mLoss[0m : 1.42459
[1mStep[0m  [32/84], [94mLoss[0m : 1.52209
[1mStep[0m  [40/84], [94mLoss[0m : 1.47119
[1mStep[0m  [48/84], [94mLoss[0m : 1.41605
[1mStep[0m  [56/84], [94mLoss[0m : 1.56948
[1mStep[0m  [64/84], [94mLoss[0m : 1.87062
[1mStep[0m  [72/84], [94mLoss[0m : 1.57441
[1mStep[0m  [80/84], [94mLoss[0m : 1.59969

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.528, [92mTest[0m: 2.488, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.72103
[1mStep[0m  [8/84], [94mLoss[0m : 1.32611
[1mStep[0m  [16/84], [94mLoss[0m : 1.59069
[1mStep[0m  [24/84], [94mLoss[0m : 1.52501
[1mStep[0m  [32/84], [94mLoss[0m : 1.48851
[1mStep[0m  [40/84], [94mLoss[0m : 1.35964
[1mStep[0m  [48/84], [94mLoss[0m : 1.34920
[1mStep[0m  [56/84], [94mLoss[0m : 1.48085
[1mStep[0m  [64/84], [94mLoss[0m : 1.35749
[1mStep[0m  [72/84], [94mLoss[0m : 1.42904
[1mStep[0m  [80/84], [94mLoss[0m : 1.36342

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.506, [92mTest[0m: 2.481, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.34615
[1mStep[0m  [8/84], [94mLoss[0m : 1.28780
[1mStep[0m  [16/84], [94mLoss[0m : 1.50600
[1mStep[0m  [24/84], [94mLoss[0m : 1.53492
[1mStep[0m  [32/84], [94mLoss[0m : 1.51340
[1mStep[0m  [40/84], [94mLoss[0m : 1.42484
[1mStep[0m  [48/84], [94mLoss[0m : 1.55322
[1mStep[0m  [56/84], [94mLoss[0m : 1.40139
[1mStep[0m  [64/84], [94mLoss[0m : 1.89976
[1mStep[0m  [72/84], [94mLoss[0m : 1.50350
[1mStep[0m  [80/84], [94mLoss[0m : 1.18890

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.493, [92mTest[0m: 2.520, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.55767
[1mStep[0m  [8/84], [94mLoss[0m : 1.41411
[1mStep[0m  [16/84], [94mLoss[0m : 1.48709
[1mStep[0m  [24/84], [94mLoss[0m : 1.65539
[1mStep[0m  [32/84], [94mLoss[0m : 1.47942
[1mStep[0m  [40/84], [94mLoss[0m : 1.26614
[1mStep[0m  [48/84], [94mLoss[0m : 1.56853
[1mStep[0m  [56/84], [94mLoss[0m : 1.68890
[1mStep[0m  [64/84], [94mLoss[0m : 1.56780
[1mStep[0m  [72/84], [94mLoss[0m : 1.54357
[1mStep[0m  [80/84], [94mLoss[0m : 1.34204

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.475, [92mTest[0m: 2.517, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.568
====================================

Phase 2 - Evaluation MAE:  2.5682510222707475
MAE score P1        2.335439
MAE score P2        2.568251
loss                1.474685
learning_rate       0.007525
batch_size               128
hidden_sizes      [300, 100]
epochs                    30
activation              relu
optimizer                sgd
early stopping         False
dropout                  0.2
momentum                 0.1
weight_decay            0.01
Name: 6, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 10.63670
[1mStep[0m  [4/42], [94mLoss[0m : 10.96583
[1mStep[0m  [8/42], [94mLoss[0m : 10.79660
[1mStep[0m  [12/42], [94mLoss[0m : 10.32884
[1mStep[0m  [16/42], [94mLoss[0m : 10.71859
[1mStep[0m  [20/42], [94mLoss[0m : 10.76501
[1mStep[0m  [24/42], [94mLoss[0m : 10.43428
[1mStep[0m  [28/42], [94mLoss[0m : 10.42978
[1mStep[0m  [32/42], [94mLoss[0m : 10.40984
[1mStep[0m  [36/42], [94mLoss[0m : 10.35655
[1mStep[0m  [40/42], [94mLoss[0m : 10.21898

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.561, [92mTest[0m: 10.934, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.57436
[1mStep[0m  [4/42], [94mLoss[0m : 9.76740
[1mStep[0m  [8/42], [94mLoss[0m : 9.67837
[1mStep[0m  [12/42], [94mLoss[0m : 9.86158
[1mStep[0m  [16/42], [94mLoss[0m : 10.03784
[1mStep[0m  [20/42], [94mLoss[0m : 9.79774
[1mStep[0m  [24/42], [94mLoss[0m : 9.74987
[1mStep[0m  [28/42], [94mLoss[0m : 9.81559
[1mStep[0m  [32/42], [94mLoss[0m : 9.12922
[1mStep[0m  [36/42], [94mLoss[0m : 9.57183
[1mStep[0m  [40/42], [94mLoss[0m : 9.16899

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.752, [92mTest[0m: 10.031, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.34431
[1mStep[0m  [4/42], [94mLoss[0m : 8.61021
[1mStep[0m  [8/42], [94mLoss[0m : 8.91652
[1mStep[0m  [12/42], [94mLoss[0m : 9.03866
[1mStep[0m  [16/42], [94mLoss[0m : 8.96802
[1mStep[0m  [20/42], [94mLoss[0m : 8.60033
[1mStep[0m  [24/42], [94mLoss[0m : 8.53194
[1mStep[0m  [28/42], [94mLoss[0m : 8.70751
[1mStep[0m  [32/42], [94mLoss[0m : 8.40148
[1mStep[0m  [36/42], [94mLoss[0m : 8.24429
[1mStep[0m  [40/42], [94mLoss[0m : 7.60663

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 8.744, [92mTest[0m: 9.038, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.20453
[1mStep[0m  [4/42], [94mLoss[0m : 8.29039
[1mStep[0m  [8/42], [94mLoss[0m : 7.44071
[1mStep[0m  [12/42], [94mLoss[0m : 7.62143
[1mStep[0m  [16/42], [94mLoss[0m : 7.24820
[1mStep[0m  [20/42], [94mLoss[0m : 7.47427
[1mStep[0m  [24/42], [94mLoss[0m : 7.02380
[1mStep[0m  [28/42], [94mLoss[0m : 6.78769
[1mStep[0m  [32/42], [94mLoss[0m : 6.52986
[1mStep[0m  [36/42], [94mLoss[0m : 6.58428
[1mStep[0m  [40/42], [94mLoss[0m : 6.16253

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 7.321, [92mTest[0m: 7.653, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.20863
[1mStep[0m  [4/42], [94mLoss[0m : 6.73779
[1mStep[0m  [8/42], [94mLoss[0m : 6.09124
[1mStep[0m  [12/42], [94mLoss[0m : 6.26916
[1mStep[0m  [16/42], [94mLoss[0m : 6.28536
[1mStep[0m  [20/42], [94mLoss[0m : 6.34090
[1mStep[0m  [24/42], [94mLoss[0m : 5.85427
[1mStep[0m  [28/42], [94mLoss[0m : 5.64598
[1mStep[0m  [32/42], [94mLoss[0m : 5.53198
[1mStep[0m  [36/42], [94mLoss[0m : 5.00907
[1mStep[0m  [40/42], [94mLoss[0m : 5.51391

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 5.856, [92mTest[0m: 6.021, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.15263
[1mStep[0m  [4/42], [94mLoss[0m : 4.64587
[1mStep[0m  [8/42], [94mLoss[0m : 5.01054
[1mStep[0m  [12/42], [94mLoss[0m : 4.84998
[1mStep[0m  [16/42], [94mLoss[0m : 4.54269
[1mStep[0m  [20/42], [94mLoss[0m : 4.36560
[1mStep[0m  [24/42], [94mLoss[0m : 4.17241
[1mStep[0m  [28/42], [94mLoss[0m : 3.96742
[1mStep[0m  [32/42], [94mLoss[0m : 3.93333
[1mStep[0m  [36/42], [94mLoss[0m : 3.73100
[1mStep[0m  [40/42], [94mLoss[0m : 3.49967

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 4.448, [92mTest[0m: 4.250, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.67587
[1mStep[0m  [4/42], [94mLoss[0m : 3.38566
[1mStep[0m  [8/42], [94mLoss[0m : 3.78692
[1mStep[0m  [12/42], [94mLoss[0m : 3.58773
[1mStep[0m  [16/42], [94mLoss[0m : 3.32091
[1mStep[0m  [20/42], [94mLoss[0m : 3.63723
[1mStep[0m  [24/42], [94mLoss[0m : 3.09936
[1mStep[0m  [28/42], [94mLoss[0m : 3.36546
[1mStep[0m  [32/42], [94mLoss[0m : 3.33443
[1mStep[0m  [36/42], [94mLoss[0m : 2.96746
[1mStep[0m  [40/42], [94mLoss[0m : 3.13639

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 3.383, [92mTest[0m: 3.127, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.86841
[1mStep[0m  [4/42], [94mLoss[0m : 3.20975
[1mStep[0m  [8/42], [94mLoss[0m : 2.80873
[1mStep[0m  [12/42], [94mLoss[0m : 3.18641
[1mStep[0m  [16/42], [94mLoss[0m : 3.00897
[1mStep[0m  [20/42], [94mLoss[0m : 3.16164
[1mStep[0m  [24/42], [94mLoss[0m : 2.75402
[1mStep[0m  [28/42], [94mLoss[0m : 2.94351
[1mStep[0m  [32/42], [94mLoss[0m : 3.03071
[1mStep[0m  [36/42], [94mLoss[0m : 3.01443
[1mStep[0m  [40/42], [94mLoss[0m : 2.71947

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.953, [92mTest[0m: 2.523, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.85258
[1mStep[0m  [4/42], [94mLoss[0m : 2.91380
[1mStep[0m  [8/42], [94mLoss[0m : 2.68189
[1mStep[0m  [12/42], [94mLoss[0m : 2.81510
[1mStep[0m  [16/42], [94mLoss[0m : 2.99844
[1mStep[0m  [20/42], [94mLoss[0m : 3.02390
[1mStep[0m  [24/42], [94mLoss[0m : 2.62277
[1mStep[0m  [28/42], [94mLoss[0m : 2.72392
[1mStep[0m  [32/42], [94mLoss[0m : 2.93837
[1mStep[0m  [36/42], [94mLoss[0m : 2.87578
[1mStep[0m  [40/42], [94mLoss[0m : 2.89615

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.829, [92mTest[0m: 2.409, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57043
[1mStep[0m  [4/42], [94mLoss[0m : 2.95788
[1mStep[0m  [8/42], [94mLoss[0m : 2.89127
[1mStep[0m  [12/42], [94mLoss[0m : 2.71256
[1mStep[0m  [16/42], [94mLoss[0m : 2.98060
[1mStep[0m  [20/42], [94mLoss[0m : 2.56266
[1mStep[0m  [24/42], [94mLoss[0m : 2.74848
[1mStep[0m  [28/42], [94mLoss[0m : 2.75094
[1mStep[0m  [32/42], [94mLoss[0m : 2.97848
[1mStep[0m  [36/42], [94mLoss[0m : 2.92200
[1mStep[0m  [40/42], [94mLoss[0m : 2.82364

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.834, [92mTest[0m: 2.392, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.72786
[1mStep[0m  [4/42], [94mLoss[0m : 2.76935
[1mStep[0m  [8/42], [94mLoss[0m : 2.98724
[1mStep[0m  [12/42], [94mLoss[0m : 2.78319
[1mStep[0m  [16/42], [94mLoss[0m : 3.00872
[1mStep[0m  [20/42], [94mLoss[0m : 2.87299
[1mStep[0m  [24/42], [94mLoss[0m : 2.98055
[1mStep[0m  [28/42], [94mLoss[0m : 2.73518
[1mStep[0m  [32/42], [94mLoss[0m : 2.70847
[1mStep[0m  [36/42], [94mLoss[0m : 2.70199
[1mStep[0m  [40/42], [94mLoss[0m : 2.72766

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.825, [92mTest[0m: 2.368, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54826
[1mStep[0m  [4/42], [94mLoss[0m : 2.72863
[1mStep[0m  [8/42], [94mLoss[0m : 2.60719
[1mStep[0m  [12/42], [94mLoss[0m : 2.98651
[1mStep[0m  [16/42], [94mLoss[0m : 2.78780
[1mStep[0m  [20/42], [94mLoss[0m : 2.81726
[1mStep[0m  [24/42], [94mLoss[0m : 2.78081
[1mStep[0m  [28/42], [94mLoss[0m : 2.76046
[1mStep[0m  [32/42], [94mLoss[0m : 2.76617
[1mStep[0m  [36/42], [94mLoss[0m : 2.73468
[1mStep[0m  [40/42], [94mLoss[0m : 2.68458

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.762, [92mTest[0m: 2.361, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.71698
[1mStep[0m  [4/42], [94mLoss[0m : 2.68955
[1mStep[0m  [8/42], [94mLoss[0m : 2.88224
[1mStep[0m  [12/42], [94mLoss[0m : 2.85366
[1mStep[0m  [16/42], [94mLoss[0m : 3.06042
[1mStep[0m  [20/42], [94mLoss[0m : 2.90739
[1mStep[0m  [24/42], [94mLoss[0m : 2.59071
[1mStep[0m  [28/42], [94mLoss[0m : 2.83784
[1mStep[0m  [32/42], [94mLoss[0m : 2.59461
[1mStep[0m  [36/42], [94mLoss[0m : 2.83187
[1mStep[0m  [40/42], [94mLoss[0m : 2.99361

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.759, [92mTest[0m: 2.338, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.81663
[1mStep[0m  [4/42], [94mLoss[0m : 2.72897
[1mStep[0m  [8/42], [94mLoss[0m : 2.50253
[1mStep[0m  [12/42], [94mLoss[0m : 2.65303
[1mStep[0m  [16/42], [94mLoss[0m : 2.93997
[1mStep[0m  [20/42], [94mLoss[0m : 2.93672
[1mStep[0m  [24/42], [94mLoss[0m : 2.79997
[1mStep[0m  [28/42], [94mLoss[0m : 2.91106
[1mStep[0m  [32/42], [94mLoss[0m : 2.84265
[1mStep[0m  [36/42], [94mLoss[0m : 2.54637
[1mStep[0m  [40/42], [94mLoss[0m : 2.87643

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.727, [92mTest[0m: 2.344, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.94875
[1mStep[0m  [4/42], [94mLoss[0m : 2.58748
[1mStep[0m  [8/42], [94mLoss[0m : 2.85004
[1mStep[0m  [12/42], [94mLoss[0m : 2.55597
[1mStep[0m  [16/42], [94mLoss[0m : 2.87454
[1mStep[0m  [20/42], [94mLoss[0m : 2.83633
[1mStep[0m  [24/42], [94mLoss[0m : 2.52133
[1mStep[0m  [28/42], [94mLoss[0m : 2.73002
[1mStep[0m  [32/42], [94mLoss[0m : 2.89330
[1mStep[0m  [36/42], [94mLoss[0m : 2.57558
[1mStep[0m  [40/42], [94mLoss[0m : 2.69593

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.728, [92mTest[0m: 2.331, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.77612
[1mStep[0m  [4/42], [94mLoss[0m : 2.86257
[1mStep[0m  [8/42], [94mLoss[0m : 2.49463
[1mStep[0m  [12/42], [94mLoss[0m : 2.46349
[1mStep[0m  [16/42], [94mLoss[0m : 2.83570
[1mStep[0m  [20/42], [94mLoss[0m : 2.59911
[1mStep[0m  [24/42], [94mLoss[0m : 2.65655
[1mStep[0m  [28/42], [94mLoss[0m : 2.58663
[1mStep[0m  [32/42], [94mLoss[0m : 3.01131
[1mStep[0m  [36/42], [94mLoss[0m : 2.87816
[1mStep[0m  [40/42], [94mLoss[0m : 2.85296

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.713, [92mTest[0m: 2.342, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.76210
[1mStep[0m  [4/42], [94mLoss[0m : 2.81401
[1mStep[0m  [8/42], [94mLoss[0m : 2.69726
[1mStep[0m  [12/42], [94mLoss[0m : 2.77570
[1mStep[0m  [16/42], [94mLoss[0m : 2.62864
[1mStep[0m  [20/42], [94mLoss[0m : 2.76015
[1mStep[0m  [24/42], [94mLoss[0m : 2.71663
[1mStep[0m  [28/42], [94mLoss[0m : 3.00299
[1mStep[0m  [32/42], [94mLoss[0m : 2.66620
[1mStep[0m  [36/42], [94mLoss[0m : 2.53937
[1mStep[0m  [40/42], [94mLoss[0m : 2.62850

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.707, [92mTest[0m: 2.323, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.82008
[1mStep[0m  [4/42], [94mLoss[0m : 2.58258
[1mStep[0m  [8/42], [94mLoss[0m : 2.68569
[1mStep[0m  [12/42], [94mLoss[0m : 2.83843
[1mStep[0m  [16/42], [94mLoss[0m : 2.67171
[1mStep[0m  [20/42], [94mLoss[0m : 2.55349
[1mStep[0m  [24/42], [94mLoss[0m : 2.61992
[1mStep[0m  [28/42], [94mLoss[0m : 2.74796
[1mStep[0m  [32/42], [94mLoss[0m : 2.76569
[1mStep[0m  [36/42], [94mLoss[0m : 2.73081
[1mStep[0m  [40/42], [94mLoss[0m : 2.73795

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.677, [92mTest[0m: 2.334, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.71496
[1mStep[0m  [4/42], [94mLoss[0m : 2.66522
[1mStep[0m  [8/42], [94mLoss[0m : 2.99330
[1mStep[0m  [12/42], [94mLoss[0m : 2.76438
[1mStep[0m  [16/42], [94mLoss[0m : 2.89231
[1mStep[0m  [20/42], [94mLoss[0m : 2.75485
[1mStep[0m  [24/42], [94mLoss[0m : 2.90445
[1mStep[0m  [28/42], [94mLoss[0m : 2.82786
[1mStep[0m  [32/42], [94mLoss[0m : 2.39878
[1mStep[0m  [36/42], [94mLoss[0m : 2.62499
[1mStep[0m  [40/42], [94mLoss[0m : 2.57075

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.673, [92mTest[0m: 2.332, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61774
[1mStep[0m  [4/42], [94mLoss[0m : 2.60047
[1mStep[0m  [8/42], [94mLoss[0m : 2.68333
[1mStep[0m  [12/42], [94mLoss[0m : 2.72880
[1mStep[0m  [16/42], [94mLoss[0m : 2.59285
[1mStep[0m  [20/42], [94mLoss[0m : 2.63815
[1mStep[0m  [24/42], [94mLoss[0m : 2.58719
[1mStep[0m  [28/42], [94mLoss[0m : 2.65874
[1mStep[0m  [32/42], [94mLoss[0m : 2.58799
[1mStep[0m  [36/42], [94mLoss[0m : 2.51136
[1mStep[0m  [40/42], [94mLoss[0m : 2.96782

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.662, [92mTest[0m: 2.336, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.78429
[1mStep[0m  [4/42], [94mLoss[0m : 2.65624
[1mStep[0m  [8/42], [94mLoss[0m : 2.60450
[1mStep[0m  [12/42], [94mLoss[0m : 2.53320
[1mStep[0m  [16/42], [94mLoss[0m : 2.78363
[1mStep[0m  [20/42], [94mLoss[0m : 2.49599
[1mStep[0m  [24/42], [94mLoss[0m : 2.52320
[1mStep[0m  [28/42], [94mLoss[0m : 2.58394
[1mStep[0m  [32/42], [94mLoss[0m : 2.53974
[1mStep[0m  [36/42], [94mLoss[0m : 2.58768
[1mStep[0m  [40/42], [94mLoss[0m : 2.51940

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.637, [92mTest[0m: 2.328, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56766
[1mStep[0m  [4/42], [94mLoss[0m : 2.58773
[1mStep[0m  [8/42], [94mLoss[0m : 2.80827
[1mStep[0m  [12/42], [94mLoss[0m : 2.69123
[1mStep[0m  [16/42], [94mLoss[0m : 2.59480
[1mStep[0m  [20/42], [94mLoss[0m : 2.68963
[1mStep[0m  [24/42], [94mLoss[0m : 2.67288
[1mStep[0m  [28/42], [94mLoss[0m : 2.63334
[1mStep[0m  [32/42], [94mLoss[0m : 2.74539
[1mStep[0m  [36/42], [94mLoss[0m : 2.70452
[1mStep[0m  [40/42], [94mLoss[0m : 2.68201

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.628, [92mTest[0m: 2.325, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59251
[1mStep[0m  [4/42], [94mLoss[0m : 2.52614
[1mStep[0m  [8/42], [94mLoss[0m : 2.54277
[1mStep[0m  [12/42], [94mLoss[0m : 2.77885
[1mStep[0m  [16/42], [94mLoss[0m : 2.61816
[1mStep[0m  [20/42], [94mLoss[0m : 2.66148
[1mStep[0m  [24/42], [94mLoss[0m : 2.47910
[1mStep[0m  [28/42], [94mLoss[0m : 2.34069
[1mStep[0m  [32/42], [94mLoss[0m : 2.72709
[1mStep[0m  [36/42], [94mLoss[0m : 2.82024
[1mStep[0m  [40/42], [94mLoss[0m : 2.40828

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.616, [92mTest[0m: 2.332, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.73321
[1mStep[0m  [4/42], [94mLoss[0m : 2.49624
[1mStep[0m  [8/42], [94mLoss[0m : 2.63145
[1mStep[0m  [12/42], [94mLoss[0m : 2.64549
[1mStep[0m  [16/42], [94mLoss[0m : 2.30163
[1mStep[0m  [20/42], [94mLoss[0m : 2.61830
[1mStep[0m  [24/42], [94mLoss[0m : 2.66638
[1mStep[0m  [28/42], [94mLoss[0m : 2.59351
[1mStep[0m  [32/42], [94mLoss[0m : 2.78673
[1mStep[0m  [36/42], [94mLoss[0m : 2.71674
[1mStep[0m  [40/42], [94mLoss[0m : 2.56977

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.610, [92mTest[0m: 2.332, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.68183
[1mStep[0m  [4/42], [94mLoss[0m : 2.67426
[1mStep[0m  [8/42], [94mLoss[0m : 2.89352
[1mStep[0m  [12/42], [94mLoss[0m : 2.38311
[1mStep[0m  [16/42], [94mLoss[0m : 2.46477
[1mStep[0m  [20/42], [94mLoss[0m : 2.66418
[1mStep[0m  [24/42], [94mLoss[0m : 2.71462
[1mStep[0m  [28/42], [94mLoss[0m : 2.62661
[1mStep[0m  [32/42], [94mLoss[0m : 2.72573
[1mStep[0m  [36/42], [94mLoss[0m : 2.62024
[1mStep[0m  [40/42], [94mLoss[0m : 2.52888

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.595, [92mTest[0m: 2.331, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60640
[1mStep[0m  [4/42], [94mLoss[0m : 2.70835
[1mStep[0m  [8/42], [94mLoss[0m : 2.72256
[1mStep[0m  [12/42], [94mLoss[0m : 2.71177
[1mStep[0m  [16/42], [94mLoss[0m : 2.60461
[1mStep[0m  [20/42], [94mLoss[0m : 2.51538
[1mStep[0m  [24/42], [94mLoss[0m : 2.61740
[1mStep[0m  [28/42], [94mLoss[0m : 2.56302
[1mStep[0m  [32/42], [94mLoss[0m : 2.54821
[1mStep[0m  [36/42], [94mLoss[0m : 2.63806
[1mStep[0m  [40/42], [94mLoss[0m : 2.80428

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.618, [92mTest[0m: 2.323, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56814
[1mStep[0m  [4/42], [94mLoss[0m : 2.67785
[1mStep[0m  [8/42], [94mLoss[0m : 2.68378
[1mStep[0m  [12/42], [94mLoss[0m : 2.71332
[1mStep[0m  [16/42], [94mLoss[0m : 2.56133
[1mStep[0m  [20/42], [94mLoss[0m : 2.40075
[1mStep[0m  [24/42], [94mLoss[0m : 2.45320
[1mStep[0m  [28/42], [94mLoss[0m : 2.56664
[1mStep[0m  [32/42], [94mLoss[0m : 2.71939
[1mStep[0m  [36/42], [94mLoss[0m : 2.54334
[1mStep[0m  [40/42], [94mLoss[0m : 2.67881

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.611, [92mTest[0m: 2.323, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41930
[1mStep[0m  [4/42], [94mLoss[0m : 2.54896
[1mStep[0m  [8/42], [94mLoss[0m : 2.80398
[1mStep[0m  [12/42], [94mLoss[0m : 2.51283
[1mStep[0m  [16/42], [94mLoss[0m : 2.62717
[1mStep[0m  [20/42], [94mLoss[0m : 2.59879
[1mStep[0m  [24/42], [94mLoss[0m : 2.42838
[1mStep[0m  [28/42], [94mLoss[0m : 2.79302
[1mStep[0m  [32/42], [94mLoss[0m : 2.70001
[1mStep[0m  [36/42], [94mLoss[0m : 2.63522
[1mStep[0m  [40/42], [94mLoss[0m : 2.45588

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.593, [92mTest[0m: 2.319, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60753
[1mStep[0m  [4/42], [94mLoss[0m : 2.70817
[1mStep[0m  [8/42], [94mLoss[0m : 2.81762
[1mStep[0m  [12/42], [94mLoss[0m : 2.60026
[1mStep[0m  [16/42], [94mLoss[0m : 2.56434
[1mStep[0m  [20/42], [94mLoss[0m : 2.53762
[1mStep[0m  [24/42], [94mLoss[0m : 2.41588
[1mStep[0m  [28/42], [94mLoss[0m : 2.66326
[1mStep[0m  [32/42], [94mLoss[0m : 2.57405
[1mStep[0m  [36/42], [94mLoss[0m : 2.59439
[1mStep[0m  [40/42], [94mLoss[0m : 2.40601

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.585, [92mTest[0m: 2.326, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40464
[1mStep[0m  [4/42], [94mLoss[0m : 2.45275
[1mStep[0m  [8/42], [94mLoss[0m : 2.61410
[1mStep[0m  [12/42], [94mLoss[0m : 2.46790
[1mStep[0m  [16/42], [94mLoss[0m : 2.52027
[1mStep[0m  [20/42], [94mLoss[0m : 2.75355
[1mStep[0m  [24/42], [94mLoss[0m : 2.32232
[1mStep[0m  [28/42], [94mLoss[0m : 2.81398
[1mStep[0m  [32/42], [94mLoss[0m : 2.71646
[1mStep[0m  [36/42], [94mLoss[0m : 2.42381
[1mStep[0m  [40/42], [94mLoss[0m : 2.66910

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.573, [92mTest[0m: 2.319, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.325
====================================

Phase 1 - Evaluation MAE:  2.3251096180507114
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 2.41155
[1mStep[0m  [4/42], [94mLoss[0m : 2.63339
[1mStep[0m  [8/42], [94mLoss[0m : 2.45097
[1mStep[0m  [12/42], [94mLoss[0m : 2.58399
[1mStep[0m  [16/42], [94mLoss[0m : 2.69442
[1mStep[0m  [20/42], [94mLoss[0m : 2.41248
[1mStep[0m  [24/42], [94mLoss[0m : 2.61199
[1mStep[0m  [28/42], [94mLoss[0m : 2.63071
[1mStep[0m  [32/42], [94mLoss[0m : 2.56515
[1mStep[0m  [36/42], [94mLoss[0m : 2.70618
[1mStep[0m  [40/42], [94mLoss[0m : 2.68497

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.630, [92mTest[0m: 2.325, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41946
[1mStep[0m  [4/42], [94mLoss[0m : 2.64825
[1mStep[0m  [8/42], [94mLoss[0m : 2.44691
[1mStep[0m  [12/42], [94mLoss[0m : 2.54411
[1mStep[0m  [16/42], [94mLoss[0m : 2.60640
[1mStep[0m  [20/42], [94mLoss[0m : 2.87948
[1mStep[0m  [24/42], [94mLoss[0m : 2.61806
[1mStep[0m  [28/42], [94mLoss[0m : 2.47312
[1mStep[0m  [32/42], [94mLoss[0m : 2.66201
[1mStep[0m  [36/42], [94mLoss[0m : 2.57642
[1mStep[0m  [40/42], [94mLoss[0m : 2.46442

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.566, [92mTest[0m: 2.327, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55460
[1mStep[0m  [4/42], [94mLoss[0m : 2.46367
[1mStep[0m  [8/42], [94mLoss[0m : 2.54836
[1mStep[0m  [12/42], [94mLoss[0m : 2.45466
[1mStep[0m  [16/42], [94mLoss[0m : 2.65241
[1mStep[0m  [20/42], [94mLoss[0m : 2.45748
[1mStep[0m  [24/42], [94mLoss[0m : 2.56839
[1mStep[0m  [28/42], [94mLoss[0m : 2.42372
[1mStep[0m  [32/42], [94mLoss[0m : 2.24787
[1mStep[0m  [36/42], [94mLoss[0m : 2.41276
[1mStep[0m  [40/42], [94mLoss[0m : 2.52410

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.380, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41436
[1mStep[0m  [4/42], [94mLoss[0m : 2.60387
[1mStep[0m  [8/42], [94mLoss[0m : 2.55095
[1mStep[0m  [12/42], [94mLoss[0m : 2.63433
[1mStep[0m  [16/42], [94mLoss[0m : 2.50284
[1mStep[0m  [20/42], [94mLoss[0m : 2.34842
[1mStep[0m  [24/42], [94mLoss[0m : 2.44222
[1mStep[0m  [28/42], [94mLoss[0m : 2.41844
[1mStep[0m  [32/42], [94mLoss[0m : 2.55011
[1mStep[0m  [36/42], [94mLoss[0m : 2.52542
[1mStep[0m  [40/42], [94mLoss[0m : 2.64110

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.434, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50649
[1mStep[0m  [4/42], [94mLoss[0m : 2.16584
[1mStep[0m  [8/42], [94mLoss[0m : 2.53840
[1mStep[0m  [12/42], [94mLoss[0m : 2.61914
[1mStep[0m  [16/42], [94mLoss[0m : 2.69289
[1mStep[0m  [20/42], [94mLoss[0m : 2.54335
[1mStep[0m  [24/42], [94mLoss[0m : 2.37027
[1mStep[0m  [28/42], [94mLoss[0m : 2.44061
[1mStep[0m  [32/42], [94mLoss[0m : 2.37408
[1mStep[0m  [36/42], [94mLoss[0m : 2.42492
[1mStep[0m  [40/42], [94mLoss[0m : 2.48880

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.410, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36001
[1mStep[0m  [4/42], [94mLoss[0m : 2.29941
[1mStep[0m  [8/42], [94mLoss[0m : 2.25685
[1mStep[0m  [12/42], [94mLoss[0m : 2.47886
[1mStep[0m  [16/42], [94mLoss[0m : 2.48485
[1mStep[0m  [20/42], [94mLoss[0m : 2.38569
[1mStep[0m  [24/42], [94mLoss[0m : 2.37807
[1mStep[0m  [28/42], [94mLoss[0m : 2.50105
[1mStep[0m  [32/42], [94mLoss[0m : 2.42745
[1mStep[0m  [36/42], [94mLoss[0m : 2.57693
[1mStep[0m  [40/42], [94mLoss[0m : 2.47686

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.375, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.30199
[1mStep[0m  [4/42], [94mLoss[0m : 2.44926
[1mStep[0m  [8/42], [94mLoss[0m : 2.30441
[1mStep[0m  [12/42], [94mLoss[0m : 2.24500
[1mStep[0m  [16/42], [94mLoss[0m : 2.42241
[1mStep[0m  [20/42], [94mLoss[0m : 2.37390
[1mStep[0m  [24/42], [94mLoss[0m : 2.37970
[1mStep[0m  [28/42], [94mLoss[0m : 2.31933
[1mStep[0m  [32/42], [94mLoss[0m : 2.22041
[1mStep[0m  [36/42], [94mLoss[0m : 2.23520
[1mStep[0m  [40/42], [94mLoss[0m : 2.46607

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.365, [92mTest[0m: 2.367, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21240
[1mStep[0m  [4/42], [94mLoss[0m : 2.17394
[1mStep[0m  [8/42], [94mLoss[0m : 2.41107
[1mStep[0m  [12/42], [94mLoss[0m : 2.29076
[1mStep[0m  [16/42], [94mLoss[0m : 2.23317
[1mStep[0m  [20/42], [94mLoss[0m : 2.15323
[1mStep[0m  [24/42], [94mLoss[0m : 2.43466
[1mStep[0m  [28/42], [94mLoss[0m : 2.20440
[1mStep[0m  [32/42], [94mLoss[0m : 2.30621
[1mStep[0m  [36/42], [94mLoss[0m : 2.50149
[1mStep[0m  [40/42], [94mLoss[0m : 2.53620

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.334, [92mTest[0m: 2.369, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29280
[1mStep[0m  [4/42], [94mLoss[0m : 2.36456
[1mStep[0m  [8/42], [94mLoss[0m : 2.24199
[1mStep[0m  [12/42], [94mLoss[0m : 2.47307
[1mStep[0m  [16/42], [94mLoss[0m : 2.01323
[1mStep[0m  [20/42], [94mLoss[0m : 2.41967
[1mStep[0m  [24/42], [94mLoss[0m : 2.29826
[1mStep[0m  [28/42], [94mLoss[0m : 2.28143
[1mStep[0m  [32/42], [94mLoss[0m : 2.34102
[1mStep[0m  [36/42], [94mLoss[0m : 2.10843
[1mStep[0m  [40/42], [94mLoss[0m : 2.07972

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.306, [92mTest[0m: 2.398, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24924
[1mStep[0m  [4/42], [94mLoss[0m : 2.23793
[1mStep[0m  [8/42], [94mLoss[0m : 2.12175
[1mStep[0m  [12/42], [94mLoss[0m : 2.09588
[1mStep[0m  [16/42], [94mLoss[0m : 2.13910
[1mStep[0m  [20/42], [94mLoss[0m : 2.19664
[1mStep[0m  [24/42], [94mLoss[0m : 2.43707
[1mStep[0m  [28/42], [94mLoss[0m : 2.45010
[1mStep[0m  [32/42], [94mLoss[0m : 2.20396
[1mStep[0m  [36/42], [94mLoss[0m : 2.15969
[1mStep[0m  [40/42], [94mLoss[0m : 2.43421

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.262, [92mTest[0m: 2.378, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.18412
[1mStep[0m  [4/42], [94mLoss[0m : 2.37401
[1mStep[0m  [8/42], [94mLoss[0m : 2.30077
[1mStep[0m  [12/42], [94mLoss[0m : 2.41766
[1mStep[0m  [16/42], [94mLoss[0m : 2.16921
[1mStep[0m  [20/42], [94mLoss[0m : 2.24119
[1mStep[0m  [24/42], [94mLoss[0m : 2.24108
[1mStep[0m  [28/42], [94mLoss[0m : 2.27012
[1mStep[0m  [32/42], [94mLoss[0m : 2.15148
[1mStep[0m  [36/42], [94mLoss[0m : 2.33233
[1mStep[0m  [40/42], [94mLoss[0m : 2.21452

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.227, [92mTest[0m: 2.368, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27182
[1mStep[0m  [4/42], [94mLoss[0m : 2.25702
[1mStep[0m  [8/42], [94mLoss[0m : 2.11322
[1mStep[0m  [12/42], [94mLoss[0m : 2.04680
[1mStep[0m  [16/42], [94mLoss[0m : 2.14924
[1mStep[0m  [20/42], [94mLoss[0m : 2.10438
[1mStep[0m  [24/42], [94mLoss[0m : 2.13796
[1mStep[0m  [28/42], [94mLoss[0m : 2.06853
[1mStep[0m  [32/42], [94mLoss[0m : 2.32846
[1mStep[0m  [36/42], [94mLoss[0m : 2.32428
[1mStep[0m  [40/42], [94mLoss[0m : 2.25139

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.173, [92mTest[0m: 2.415, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.30660
[1mStep[0m  [4/42], [94mLoss[0m : 2.14101
[1mStep[0m  [8/42], [94mLoss[0m : 2.18232
[1mStep[0m  [12/42], [94mLoss[0m : 2.14581
[1mStep[0m  [16/42], [94mLoss[0m : 2.13818
[1mStep[0m  [20/42], [94mLoss[0m : 2.15206
[1mStep[0m  [24/42], [94mLoss[0m : 2.06282
[1mStep[0m  [28/42], [94mLoss[0m : 2.06131
[1mStep[0m  [32/42], [94mLoss[0m : 2.00940
[1mStep[0m  [36/42], [94mLoss[0m : 2.11288
[1mStep[0m  [40/42], [94mLoss[0m : 2.09220

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.152, [92mTest[0m: 2.423, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26642
[1mStep[0m  [4/42], [94mLoss[0m : 2.05032
[1mStep[0m  [8/42], [94mLoss[0m : 2.06280
[1mStep[0m  [12/42], [94mLoss[0m : 2.03065
[1mStep[0m  [16/42], [94mLoss[0m : 2.24939
[1mStep[0m  [20/42], [94mLoss[0m : 2.11672
[1mStep[0m  [24/42], [94mLoss[0m : 1.95172
[1mStep[0m  [28/42], [94mLoss[0m : 2.22261
[1mStep[0m  [32/42], [94mLoss[0m : 2.21991
[1mStep[0m  [36/42], [94mLoss[0m : 2.08236
[1mStep[0m  [40/42], [94mLoss[0m : 2.19940

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.117, [92mTest[0m: 2.364, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28243
[1mStep[0m  [4/42], [94mLoss[0m : 1.97152
[1mStep[0m  [8/42], [94mLoss[0m : 2.24956
[1mStep[0m  [12/42], [94mLoss[0m : 2.22532
[1mStep[0m  [16/42], [94mLoss[0m : 1.83913
[1mStep[0m  [20/42], [94mLoss[0m : 1.90355
[1mStep[0m  [24/42], [94mLoss[0m : 2.03083
[1mStep[0m  [28/42], [94mLoss[0m : 2.13683
[1mStep[0m  [32/42], [94mLoss[0m : 2.07292
[1mStep[0m  [36/42], [94mLoss[0m : 2.08117
[1mStep[0m  [40/42], [94mLoss[0m : 2.07502

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.058, [92mTest[0m: 2.460, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.06192
[1mStep[0m  [4/42], [94mLoss[0m : 1.93158
[1mStep[0m  [8/42], [94mLoss[0m : 2.04289
[1mStep[0m  [12/42], [94mLoss[0m : 1.98430
[1mStep[0m  [16/42], [94mLoss[0m : 1.95738
[1mStep[0m  [20/42], [94mLoss[0m : 1.97818
[1mStep[0m  [24/42], [94mLoss[0m : 1.76332
[1mStep[0m  [28/42], [94mLoss[0m : 1.94347
[1mStep[0m  [32/42], [94mLoss[0m : 2.09064
[1mStep[0m  [36/42], [94mLoss[0m : 2.24529
[1mStep[0m  [40/42], [94mLoss[0m : 2.05679

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.034, [92mTest[0m: 2.403, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.12205
[1mStep[0m  [4/42], [94mLoss[0m : 1.85824
[1mStep[0m  [8/42], [94mLoss[0m : 2.05437
[1mStep[0m  [12/42], [94mLoss[0m : 1.98062
[1mStep[0m  [16/42], [94mLoss[0m : 1.96023
[1mStep[0m  [20/42], [94mLoss[0m : 1.89209
[1mStep[0m  [24/42], [94mLoss[0m : 1.98372
[1mStep[0m  [28/42], [94mLoss[0m : 1.88118
[1mStep[0m  [32/42], [94mLoss[0m : 1.98363
[1mStep[0m  [36/42], [94mLoss[0m : 2.03578
[1mStep[0m  [40/42], [94mLoss[0m : 2.22083

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.002, [92mTest[0m: 2.491, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.02264
[1mStep[0m  [4/42], [94mLoss[0m : 1.96245
[1mStep[0m  [8/42], [94mLoss[0m : 1.85485
[1mStep[0m  [12/42], [94mLoss[0m : 1.89768
[1mStep[0m  [16/42], [94mLoss[0m : 2.12782
[1mStep[0m  [20/42], [94mLoss[0m : 2.04045
[1mStep[0m  [24/42], [94mLoss[0m : 2.06945
[1mStep[0m  [28/42], [94mLoss[0m : 2.13213
[1mStep[0m  [32/42], [94mLoss[0m : 1.99247
[1mStep[0m  [36/42], [94mLoss[0m : 1.97139
[1mStep[0m  [40/42], [94mLoss[0m : 2.00884

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.976, [92mTest[0m: 2.416, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.91930
[1mStep[0m  [4/42], [94mLoss[0m : 1.97342
[1mStep[0m  [8/42], [94mLoss[0m : 2.02059
[1mStep[0m  [12/42], [94mLoss[0m : 1.76635
[1mStep[0m  [16/42], [94mLoss[0m : 1.91004
[1mStep[0m  [20/42], [94mLoss[0m : 1.81407
[1mStep[0m  [24/42], [94mLoss[0m : 2.02540
[1mStep[0m  [28/42], [94mLoss[0m : 2.02125
[1mStep[0m  [32/42], [94mLoss[0m : 1.96421
[1mStep[0m  [36/42], [94mLoss[0m : 1.99285
[1mStep[0m  [40/42], [94mLoss[0m : 1.99744

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.933, [92mTest[0m: 2.538, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.87527
[1mStep[0m  [4/42], [94mLoss[0m : 1.80381
[1mStep[0m  [8/42], [94mLoss[0m : 1.77863
[1mStep[0m  [12/42], [94mLoss[0m : 2.02557
[1mStep[0m  [16/42], [94mLoss[0m : 1.77624
[1mStep[0m  [20/42], [94mLoss[0m : 1.99064
[1mStep[0m  [24/42], [94mLoss[0m : 1.79042
[1mStep[0m  [28/42], [94mLoss[0m : 2.00696
[1mStep[0m  [32/42], [94mLoss[0m : 1.95735
[1mStep[0m  [36/42], [94mLoss[0m : 1.90679
[1mStep[0m  [40/42], [94mLoss[0m : 1.85387

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.880, [92mTest[0m: 2.478, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.81373
[1mStep[0m  [4/42], [94mLoss[0m : 1.70730
[1mStep[0m  [8/42], [94mLoss[0m : 1.77191
[1mStep[0m  [12/42], [94mLoss[0m : 1.79409
[1mStep[0m  [16/42], [94mLoss[0m : 2.02842
[1mStep[0m  [20/42], [94mLoss[0m : 1.94541
[1mStep[0m  [24/42], [94mLoss[0m : 2.07032
[1mStep[0m  [28/42], [94mLoss[0m : 2.06649
[1mStep[0m  [32/42], [94mLoss[0m : 2.02840
[1mStep[0m  [36/42], [94mLoss[0m : 1.71890
[1mStep[0m  [40/42], [94mLoss[0m : 1.82933

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.861, [92mTest[0m: 2.449, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.69026
[1mStep[0m  [4/42], [94mLoss[0m : 2.04084
[1mStep[0m  [8/42], [94mLoss[0m : 1.95982
[1mStep[0m  [12/42], [94mLoss[0m : 1.88543
[1mStep[0m  [16/42], [94mLoss[0m : 1.77516
[1mStep[0m  [20/42], [94mLoss[0m : 1.89499
[1mStep[0m  [24/42], [94mLoss[0m : 1.73003
[1mStep[0m  [28/42], [94mLoss[0m : 1.89455
[1mStep[0m  [32/42], [94mLoss[0m : 1.92654
[1mStep[0m  [36/42], [94mLoss[0m : 1.97178
[1mStep[0m  [40/42], [94mLoss[0m : 1.92017

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.843, [92mTest[0m: 2.445, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.72104
[1mStep[0m  [4/42], [94mLoss[0m : 1.84561
[1mStep[0m  [8/42], [94mLoss[0m : 1.81526
[1mStep[0m  [12/42], [94mLoss[0m : 1.79050
[1mStep[0m  [16/42], [94mLoss[0m : 2.09444
[1mStep[0m  [20/42], [94mLoss[0m : 1.69327
[1mStep[0m  [24/42], [94mLoss[0m : 1.89239
[1mStep[0m  [28/42], [94mLoss[0m : 1.73078
[1mStep[0m  [32/42], [94mLoss[0m : 1.99587
[1mStep[0m  [36/42], [94mLoss[0m : 1.91110
[1mStep[0m  [40/42], [94mLoss[0m : 2.03074

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.817, [92mTest[0m: 2.462, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.62957
[1mStep[0m  [4/42], [94mLoss[0m : 1.78502
[1mStep[0m  [8/42], [94mLoss[0m : 1.71155
[1mStep[0m  [12/42], [94mLoss[0m : 1.66440
[1mStep[0m  [16/42], [94mLoss[0m : 1.86203
[1mStep[0m  [20/42], [94mLoss[0m : 1.73174
[1mStep[0m  [24/42], [94mLoss[0m : 1.64382
[1mStep[0m  [28/42], [94mLoss[0m : 1.74099
[1mStep[0m  [32/42], [94mLoss[0m : 1.72170
[1mStep[0m  [36/42], [94mLoss[0m : 1.80467
[1mStep[0m  [40/42], [94mLoss[0m : 2.11920

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.799, [92mTest[0m: 2.478, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.66389
[1mStep[0m  [4/42], [94mLoss[0m : 1.62198
[1mStep[0m  [8/42], [94mLoss[0m : 1.62975
[1mStep[0m  [12/42], [94mLoss[0m : 1.90304
[1mStep[0m  [16/42], [94mLoss[0m : 1.68879
[1mStep[0m  [20/42], [94mLoss[0m : 1.82920
[1mStep[0m  [24/42], [94mLoss[0m : 1.85661
[1mStep[0m  [28/42], [94mLoss[0m : 1.65759
[1mStep[0m  [32/42], [94mLoss[0m : 1.88877
[1mStep[0m  [36/42], [94mLoss[0m : 1.73376
[1mStep[0m  [40/42], [94mLoss[0m : 1.71874

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.774, [92mTest[0m: 2.457, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.64982
[1mStep[0m  [4/42], [94mLoss[0m : 1.56841
[1mStep[0m  [8/42], [94mLoss[0m : 1.81104
[1mStep[0m  [12/42], [94mLoss[0m : 1.70090
[1mStep[0m  [16/42], [94mLoss[0m : 1.63042
[1mStep[0m  [20/42], [94mLoss[0m : 1.71462
[1mStep[0m  [24/42], [94mLoss[0m : 1.70043
[1mStep[0m  [28/42], [94mLoss[0m : 1.83927
[1mStep[0m  [32/42], [94mLoss[0m : 1.65106
[1mStep[0m  [36/42], [94mLoss[0m : 1.81942
[1mStep[0m  [40/42], [94mLoss[0m : 1.77124

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.729, [92mTest[0m: 2.502, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.71053
[1mStep[0m  [4/42], [94mLoss[0m : 1.77274
[1mStep[0m  [8/42], [94mLoss[0m : 1.46846
[1mStep[0m  [12/42], [94mLoss[0m : 1.86788
[1mStep[0m  [16/42], [94mLoss[0m : 1.72881
[1mStep[0m  [20/42], [94mLoss[0m : 1.78392
[1mStep[0m  [24/42], [94mLoss[0m : 1.67591
[1mStep[0m  [28/42], [94mLoss[0m : 1.69232
[1mStep[0m  [32/42], [94mLoss[0m : 1.69456
[1mStep[0m  [36/42], [94mLoss[0m : 1.82219
[1mStep[0m  [40/42], [94mLoss[0m : 1.66074

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.714, [92mTest[0m: 2.482, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.75729
[1mStep[0m  [4/42], [94mLoss[0m : 1.68193
[1mStep[0m  [8/42], [94mLoss[0m : 1.62546
[1mStep[0m  [12/42], [94mLoss[0m : 1.63867
[1mStep[0m  [16/42], [94mLoss[0m : 1.67015
[1mStep[0m  [20/42], [94mLoss[0m : 1.60891
[1mStep[0m  [24/42], [94mLoss[0m : 1.70549
[1mStep[0m  [28/42], [94mLoss[0m : 1.74684
[1mStep[0m  [32/42], [94mLoss[0m : 1.65619
[1mStep[0m  [36/42], [94mLoss[0m : 1.67878
[1mStep[0m  [40/42], [94mLoss[0m : 1.58295

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.690, [92mTest[0m: 2.511, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.74588
[1mStep[0m  [4/42], [94mLoss[0m : 1.65506
[1mStep[0m  [8/42], [94mLoss[0m : 1.88152
[1mStep[0m  [12/42], [94mLoss[0m : 1.84043
[1mStep[0m  [16/42], [94mLoss[0m : 1.80786
[1mStep[0m  [20/42], [94mLoss[0m : 1.73506
[1mStep[0m  [24/42], [94mLoss[0m : 1.67230
[1mStep[0m  [28/42], [94mLoss[0m : 1.80194
[1mStep[0m  [32/42], [94mLoss[0m : 1.75978
[1mStep[0m  [36/42], [94mLoss[0m : 1.75191
[1mStep[0m  [40/42], [94mLoss[0m : 1.65733

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.661, [92mTest[0m: 2.477, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.62526
[1mStep[0m  [4/42], [94mLoss[0m : 1.66737
[1mStep[0m  [8/42], [94mLoss[0m : 1.58112
[1mStep[0m  [12/42], [94mLoss[0m : 1.54751
[1mStep[0m  [16/42], [94mLoss[0m : 1.65319
[1mStep[0m  [20/42], [94mLoss[0m : 1.55169
[1mStep[0m  [24/42], [94mLoss[0m : 1.65282
[1mStep[0m  [28/42], [94mLoss[0m : 1.66027
[1mStep[0m  [32/42], [94mLoss[0m : 1.67809
[1mStep[0m  [36/42], [94mLoss[0m : 1.56144
[1mStep[0m  [40/42], [94mLoss[0m : 1.58834

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.668, [92mTest[0m: 2.489, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.450
====================================

Phase 2 - Evaluation MAE:  2.449551752635411
MAE score P1        2.32511
MAE score P2       2.449552
loss               1.661443
learning_rate      0.007525
batch_size              256
hidden_sizes      [250, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.5
weight_decay         0.0001
Name: 7, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 11.37543
[1mStep[0m  [4/42], [94mLoss[0m : 10.83506
[1mStep[0m  [8/42], [94mLoss[0m : 10.43515
[1mStep[0m  [12/42], [94mLoss[0m : 10.77794
[1mStep[0m  [16/42], [94mLoss[0m : 10.56358
[1mStep[0m  [20/42], [94mLoss[0m : 10.46384
[1mStep[0m  [24/42], [94mLoss[0m : 10.47155
[1mStep[0m  [28/42], [94mLoss[0m : 10.31992
[1mStep[0m  [32/42], [94mLoss[0m : 10.60278
[1mStep[0m  [36/42], [94mLoss[0m : 9.98746
[1mStep[0m  [40/42], [94mLoss[0m : 10.11959

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.591, [92mTest[0m: 10.749, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.28218
[1mStep[0m  [4/42], [94mLoss[0m : 10.49447
[1mStep[0m  [8/42], [94mLoss[0m : 10.37443
[1mStep[0m  [12/42], [94mLoss[0m : 10.02222
[1mStep[0m  [16/42], [94mLoss[0m : 10.09066
[1mStep[0m  [20/42], [94mLoss[0m : 10.05244
[1mStep[0m  [24/42], [94mLoss[0m : 9.72093
[1mStep[0m  [28/42], [94mLoss[0m : 9.80998
[1mStep[0m  [32/42], [94mLoss[0m : 9.74027
[1mStep[0m  [36/42], [94mLoss[0m : 9.76577
[1mStep[0m  [40/42], [94mLoss[0m : 9.93688

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.033, [92mTest[0m: 10.199, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.43466
[1mStep[0m  [4/42], [94mLoss[0m : 9.70963
[1mStep[0m  [8/42], [94mLoss[0m : 9.73539
[1mStep[0m  [12/42], [94mLoss[0m : 8.94165
[1mStep[0m  [16/42], [94mLoss[0m : 9.60951
[1mStep[0m  [20/42], [94mLoss[0m : 9.45742
[1mStep[0m  [24/42], [94mLoss[0m : 9.61861
[1mStep[0m  [28/42], [94mLoss[0m : 9.11681
[1mStep[0m  [32/42], [94mLoss[0m : 9.82667
[1mStep[0m  [36/42], [94mLoss[0m : 9.11781
[1mStep[0m  [40/42], [94mLoss[0m : 8.77777

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.385, [92mTest[0m: 9.469, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.27906
[1mStep[0m  [4/42], [94mLoss[0m : 9.23682
[1mStep[0m  [8/42], [94mLoss[0m : 8.92109
[1mStep[0m  [12/42], [94mLoss[0m : 8.71798
[1mStep[0m  [16/42], [94mLoss[0m : 8.80866
[1mStep[0m  [20/42], [94mLoss[0m : 8.86947
[1mStep[0m  [24/42], [94mLoss[0m : 8.25451
[1mStep[0m  [28/42], [94mLoss[0m : 8.67261
[1mStep[0m  [32/42], [94mLoss[0m : 8.24666
[1mStep[0m  [36/42], [94mLoss[0m : 8.12656
[1mStep[0m  [40/42], [94mLoss[0m : 8.35616

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 8.572, [92mTest[0m: 8.667, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.27978
[1mStep[0m  [4/42], [94mLoss[0m : 7.56276
[1mStep[0m  [8/42], [94mLoss[0m : 7.97071
[1mStep[0m  [12/42], [94mLoss[0m : 7.67458
[1mStep[0m  [16/42], [94mLoss[0m : 8.02997
[1mStep[0m  [20/42], [94mLoss[0m : 7.66491
[1mStep[0m  [24/42], [94mLoss[0m : 7.57342
[1mStep[0m  [28/42], [94mLoss[0m : 7.59555
[1mStep[0m  [32/42], [94mLoss[0m : 7.28451
[1mStep[0m  [36/42], [94mLoss[0m : 7.33624
[1mStep[0m  [40/42], [94mLoss[0m : 7.42091

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 7.599, [92mTest[0m: 7.585, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.74574
[1mStep[0m  [4/42], [94mLoss[0m : 6.85270
[1mStep[0m  [8/42], [94mLoss[0m : 6.63405
[1mStep[0m  [12/42], [94mLoss[0m : 7.14080
[1mStep[0m  [16/42], [94mLoss[0m : 6.75705
[1mStep[0m  [20/42], [94mLoss[0m : 6.78561
[1mStep[0m  [24/42], [94mLoss[0m : 6.85924
[1mStep[0m  [28/42], [94mLoss[0m : 7.05236
[1mStep[0m  [32/42], [94mLoss[0m : 6.54761
[1mStep[0m  [36/42], [94mLoss[0m : 6.72826
[1mStep[0m  [40/42], [94mLoss[0m : 6.42043

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 6.734, [92mTest[0m: 6.470, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.40922
[1mStep[0m  [4/42], [94mLoss[0m : 6.42966
[1mStep[0m  [8/42], [94mLoss[0m : 6.26593
[1mStep[0m  [12/42], [94mLoss[0m : 6.40905
[1mStep[0m  [16/42], [94mLoss[0m : 5.99459
[1mStep[0m  [20/42], [94mLoss[0m : 6.07134
[1mStep[0m  [24/42], [94mLoss[0m : 6.24004
[1mStep[0m  [28/42], [94mLoss[0m : 5.97660
[1mStep[0m  [32/42], [94mLoss[0m : 5.85460
[1mStep[0m  [36/42], [94mLoss[0m : 5.84295
[1mStep[0m  [40/42], [94mLoss[0m : 5.43829

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 6.043, [92mTest[0m: 5.616, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.38830
[1mStep[0m  [4/42], [94mLoss[0m : 5.58743
[1mStep[0m  [8/42], [94mLoss[0m : 5.86432
[1mStep[0m  [12/42], [94mLoss[0m : 5.39872
[1mStep[0m  [16/42], [94mLoss[0m : 5.66189
[1mStep[0m  [20/42], [94mLoss[0m : 5.40114
[1mStep[0m  [24/42], [94mLoss[0m : 5.65610
[1mStep[0m  [28/42], [94mLoss[0m : 5.61021
[1mStep[0m  [32/42], [94mLoss[0m : 5.23251
[1mStep[0m  [36/42], [94mLoss[0m : 5.15452
[1mStep[0m  [40/42], [94mLoss[0m : 4.70335

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 5.412, [92mTest[0m: 4.775, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.41550
[1mStep[0m  [4/42], [94mLoss[0m : 5.27421
[1mStep[0m  [8/42], [94mLoss[0m : 5.11380
[1mStep[0m  [12/42], [94mLoss[0m : 5.17228
[1mStep[0m  [16/42], [94mLoss[0m : 4.88976
[1mStep[0m  [20/42], [94mLoss[0m : 4.68533
[1mStep[0m  [24/42], [94mLoss[0m : 4.73982
[1mStep[0m  [28/42], [94mLoss[0m : 4.88564
[1mStep[0m  [32/42], [94mLoss[0m : 4.79699
[1mStep[0m  [36/42], [94mLoss[0m : 5.03769
[1mStep[0m  [40/42], [94mLoss[0m : 4.47403

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 4.781, [92mTest[0m: 4.150, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.49204
[1mStep[0m  [4/42], [94mLoss[0m : 4.29833
[1mStep[0m  [8/42], [94mLoss[0m : 4.45418
[1mStep[0m  [12/42], [94mLoss[0m : 4.32309
[1mStep[0m  [16/42], [94mLoss[0m : 4.23499
[1mStep[0m  [20/42], [94mLoss[0m : 4.14495
[1mStep[0m  [24/42], [94mLoss[0m : 4.18055
[1mStep[0m  [28/42], [94mLoss[0m : 3.64501
[1mStep[0m  [32/42], [94mLoss[0m : 3.79686
[1mStep[0m  [36/42], [94mLoss[0m : 3.99355
[1mStep[0m  [40/42], [94mLoss[0m : 3.82375

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 4.082, [92mTest[0m: 3.698, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.52111
[1mStep[0m  [4/42], [94mLoss[0m : 3.44444
[1mStep[0m  [8/42], [94mLoss[0m : 3.36482
[1mStep[0m  [12/42], [94mLoss[0m : 3.76848
[1mStep[0m  [16/42], [94mLoss[0m : 3.05554
[1mStep[0m  [20/42], [94mLoss[0m : 3.45388
[1mStep[0m  [24/42], [94mLoss[0m : 3.44470
[1mStep[0m  [28/42], [94mLoss[0m : 3.05823
[1mStep[0m  [32/42], [94mLoss[0m : 3.33288
[1mStep[0m  [36/42], [94mLoss[0m : 3.07924
[1mStep[0m  [40/42], [94mLoss[0m : 3.13627

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 3.398, [92mTest[0m: 3.007, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.07255
[1mStep[0m  [4/42], [94mLoss[0m : 3.30104
[1mStep[0m  [8/42], [94mLoss[0m : 2.91949
[1mStep[0m  [12/42], [94mLoss[0m : 2.83213
[1mStep[0m  [16/42], [94mLoss[0m : 2.80425
[1mStep[0m  [20/42], [94mLoss[0m : 2.96113
[1mStep[0m  [24/42], [94mLoss[0m : 2.94246
[1mStep[0m  [28/42], [94mLoss[0m : 2.91375
[1mStep[0m  [32/42], [94mLoss[0m : 2.66859
[1mStep[0m  [36/42], [94mLoss[0m : 2.97447
[1mStep[0m  [40/42], [94mLoss[0m : 2.86736

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.864, [92mTest[0m: 2.618, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51039
[1mStep[0m  [4/42], [94mLoss[0m : 3.00908
[1mStep[0m  [8/42], [94mLoss[0m : 2.61616
[1mStep[0m  [12/42], [94mLoss[0m : 3.16120
[1mStep[0m  [16/42], [94mLoss[0m : 2.46966
[1mStep[0m  [20/42], [94mLoss[0m : 2.90267
[1mStep[0m  [24/42], [94mLoss[0m : 2.90728
[1mStep[0m  [28/42], [94mLoss[0m : 2.62831
[1mStep[0m  [32/42], [94mLoss[0m : 2.84471
[1mStep[0m  [36/42], [94mLoss[0m : 2.59964
[1mStep[0m  [40/42], [94mLoss[0m : 2.57350

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.674, [92mTest[0m: 2.424, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.79850
[1mStep[0m  [4/42], [94mLoss[0m : 2.39909
[1mStep[0m  [8/42], [94mLoss[0m : 2.63568
[1mStep[0m  [12/42], [94mLoss[0m : 2.72252
[1mStep[0m  [16/42], [94mLoss[0m : 2.88070
[1mStep[0m  [20/42], [94mLoss[0m : 2.51579
[1mStep[0m  [24/42], [94mLoss[0m : 2.58065
[1mStep[0m  [28/42], [94mLoss[0m : 2.75925
[1mStep[0m  [32/42], [94mLoss[0m : 2.71844
[1mStep[0m  [36/42], [94mLoss[0m : 2.39705
[1mStep[0m  [40/42], [94mLoss[0m : 2.71932

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.588, [92mTest[0m: 2.374, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61880
[1mStep[0m  [4/42], [94mLoss[0m : 2.67264
[1mStep[0m  [8/42], [94mLoss[0m : 2.23912
[1mStep[0m  [12/42], [94mLoss[0m : 2.89533
[1mStep[0m  [16/42], [94mLoss[0m : 2.69242
[1mStep[0m  [20/42], [94mLoss[0m : 2.42380
[1mStep[0m  [24/42], [94mLoss[0m : 2.62783
[1mStep[0m  [28/42], [94mLoss[0m : 2.59844
[1mStep[0m  [32/42], [94mLoss[0m : 2.47078
[1mStep[0m  [36/42], [94mLoss[0m : 2.47367
[1mStep[0m  [40/42], [94mLoss[0m : 2.46244

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.573, [92mTest[0m: 2.373, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.68227
[1mStep[0m  [4/42], [94mLoss[0m : 2.42060
[1mStep[0m  [8/42], [94mLoss[0m : 2.33940
[1mStep[0m  [12/42], [94mLoss[0m : 2.56793
[1mStep[0m  [16/42], [94mLoss[0m : 2.64873
[1mStep[0m  [20/42], [94mLoss[0m : 2.66039
[1mStep[0m  [24/42], [94mLoss[0m : 2.84656
[1mStep[0m  [28/42], [94mLoss[0m : 2.65410
[1mStep[0m  [32/42], [94mLoss[0m : 2.49028
[1mStep[0m  [36/42], [94mLoss[0m : 2.73396
[1mStep[0m  [40/42], [94mLoss[0m : 2.52470

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.550, [92mTest[0m: 2.369, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.63375
[1mStep[0m  [4/42], [94mLoss[0m : 2.39396
[1mStep[0m  [8/42], [94mLoss[0m : 2.74621
[1mStep[0m  [12/42], [94mLoss[0m : 2.73064
[1mStep[0m  [16/42], [94mLoss[0m : 2.55389
[1mStep[0m  [20/42], [94mLoss[0m : 2.48611
[1mStep[0m  [24/42], [94mLoss[0m : 2.53405
[1mStep[0m  [28/42], [94mLoss[0m : 2.52485
[1mStep[0m  [32/42], [94mLoss[0m : 2.53284
[1mStep[0m  [36/42], [94mLoss[0m : 2.62583
[1mStep[0m  [40/42], [94mLoss[0m : 2.45089

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.545, [92mTest[0m: 2.384, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58383
[1mStep[0m  [4/42], [94mLoss[0m : 2.56720
[1mStep[0m  [8/42], [94mLoss[0m : 2.54676
[1mStep[0m  [12/42], [94mLoss[0m : 2.54203
[1mStep[0m  [16/42], [94mLoss[0m : 2.49719
[1mStep[0m  [20/42], [94mLoss[0m : 2.63537
[1mStep[0m  [24/42], [94mLoss[0m : 2.40305
[1mStep[0m  [28/42], [94mLoss[0m : 2.52243
[1mStep[0m  [32/42], [94mLoss[0m : 2.74145
[1mStep[0m  [36/42], [94mLoss[0m : 2.69664
[1mStep[0m  [40/42], [94mLoss[0m : 2.56623

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.535, [92mTest[0m: 2.364, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61117
[1mStep[0m  [4/42], [94mLoss[0m : 2.37930
[1mStep[0m  [8/42], [94mLoss[0m : 2.50155
[1mStep[0m  [12/42], [94mLoss[0m : 2.52147
[1mStep[0m  [16/42], [94mLoss[0m : 2.55003
[1mStep[0m  [20/42], [94mLoss[0m : 2.60416
[1mStep[0m  [24/42], [94mLoss[0m : 2.49303
[1mStep[0m  [28/42], [94mLoss[0m : 2.56824
[1mStep[0m  [32/42], [94mLoss[0m : 2.74798
[1mStep[0m  [36/42], [94mLoss[0m : 2.35003
[1mStep[0m  [40/42], [94mLoss[0m : 2.53131

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.519, [92mTest[0m: 2.380, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.72801
[1mStep[0m  [4/42], [94mLoss[0m : 2.38300
[1mStep[0m  [8/42], [94mLoss[0m : 2.50174
[1mStep[0m  [12/42], [94mLoss[0m : 2.60786
[1mStep[0m  [16/42], [94mLoss[0m : 2.45500
[1mStep[0m  [20/42], [94mLoss[0m : 2.55658
[1mStep[0m  [24/42], [94mLoss[0m : 2.36066
[1mStep[0m  [28/42], [94mLoss[0m : 2.27159
[1mStep[0m  [32/42], [94mLoss[0m : 2.48441
[1mStep[0m  [36/42], [94mLoss[0m : 2.68847
[1mStep[0m  [40/42], [94mLoss[0m : 2.37666

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.352, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47531
[1mStep[0m  [4/42], [94mLoss[0m : 2.37827
[1mStep[0m  [8/42], [94mLoss[0m : 2.44030
[1mStep[0m  [12/42], [94mLoss[0m : 2.38668
[1mStep[0m  [16/42], [94mLoss[0m : 2.63035
[1mStep[0m  [20/42], [94mLoss[0m : 2.71539
[1mStep[0m  [24/42], [94mLoss[0m : 2.48686
[1mStep[0m  [28/42], [94mLoss[0m : 2.59731
[1mStep[0m  [32/42], [94mLoss[0m : 2.66636
[1mStep[0m  [36/42], [94mLoss[0m : 2.67167
[1mStep[0m  [40/42], [94mLoss[0m : 2.54659

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.365, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42441
[1mStep[0m  [4/42], [94mLoss[0m : 2.49206
[1mStep[0m  [8/42], [94mLoss[0m : 2.58035
[1mStep[0m  [12/42], [94mLoss[0m : 2.27556
[1mStep[0m  [16/42], [94mLoss[0m : 2.38059
[1mStep[0m  [20/42], [94mLoss[0m : 2.48772
[1mStep[0m  [24/42], [94mLoss[0m : 2.45479
[1mStep[0m  [28/42], [94mLoss[0m : 2.50908
[1mStep[0m  [32/42], [94mLoss[0m : 2.45901
[1mStep[0m  [36/42], [94mLoss[0m : 2.46019
[1mStep[0m  [40/42], [94mLoss[0m : 2.38609

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.362, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49010
[1mStep[0m  [4/42], [94mLoss[0m : 2.40930
[1mStep[0m  [8/42], [94mLoss[0m : 2.44078
[1mStep[0m  [12/42], [94mLoss[0m : 2.45028
[1mStep[0m  [16/42], [94mLoss[0m : 2.48984
[1mStep[0m  [20/42], [94mLoss[0m : 2.34673
[1mStep[0m  [24/42], [94mLoss[0m : 2.54912
[1mStep[0m  [28/42], [94mLoss[0m : 2.38370
[1mStep[0m  [32/42], [94mLoss[0m : 2.50470
[1mStep[0m  [36/42], [94mLoss[0m : 2.60766
[1mStep[0m  [40/42], [94mLoss[0m : 2.54107

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.356, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44987
[1mStep[0m  [4/42], [94mLoss[0m : 2.40861
[1mStep[0m  [8/42], [94mLoss[0m : 2.48186
[1mStep[0m  [12/42], [94mLoss[0m : 2.43442
[1mStep[0m  [16/42], [94mLoss[0m : 2.43442
[1mStep[0m  [20/42], [94mLoss[0m : 2.33980
[1mStep[0m  [24/42], [94mLoss[0m : 2.36212
[1mStep[0m  [28/42], [94mLoss[0m : 2.69989
[1mStep[0m  [32/42], [94mLoss[0m : 2.32382
[1mStep[0m  [36/42], [94mLoss[0m : 2.43086
[1mStep[0m  [40/42], [94mLoss[0m : 2.33552

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.363, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64803
[1mStep[0m  [4/42], [94mLoss[0m : 2.43815
[1mStep[0m  [8/42], [94mLoss[0m : 2.44087
[1mStep[0m  [12/42], [94mLoss[0m : 2.47221
[1mStep[0m  [16/42], [94mLoss[0m : 2.42213
[1mStep[0m  [20/42], [94mLoss[0m : 2.48594
[1mStep[0m  [24/42], [94mLoss[0m : 2.61291
[1mStep[0m  [28/42], [94mLoss[0m : 2.45825
[1mStep[0m  [32/42], [94mLoss[0m : 2.28695
[1mStep[0m  [36/42], [94mLoss[0m : 2.50284
[1mStep[0m  [40/42], [94mLoss[0m : 2.55768

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.342, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57065
[1mStep[0m  [4/42], [94mLoss[0m : 2.50652
[1mStep[0m  [8/42], [94mLoss[0m : 2.45643
[1mStep[0m  [12/42], [94mLoss[0m : 2.44300
[1mStep[0m  [16/42], [94mLoss[0m : 2.52189
[1mStep[0m  [20/42], [94mLoss[0m : 2.46707
[1mStep[0m  [24/42], [94mLoss[0m : 2.45875
[1mStep[0m  [28/42], [94mLoss[0m : 2.47779
[1mStep[0m  [32/42], [94mLoss[0m : 2.38436
[1mStep[0m  [36/42], [94mLoss[0m : 2.31581
[1mStep[0m  [40/42], [94mLoss[0m : 2.41392

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.346, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57771
[1mStep[0m  [4/42], [94mLoss[0m : 2.47428
[1mStep[0m  [8/42], [94mLoss[0m : 2.48659
[1mStep[0m  [12/42], [94mLoss[0m : 2.33918
[1mStep[0m  [16/42], [94mLoss[0m : 2.33124
[1mStep[0m  [20/42], [94mLoss[0m : 2.41068
[1mStep[0m  [24/42], [94mLoss[0m : 2.49314
[1mStep[0m  [28/42], [94mLoss[0m : 2.41981
[1mStep[0m  [32/42], [94mLoss[0m : 2.47841
[1mStep[0m  [36/42], [94mLoss[0m : 2.21301
[1mStep[0m  [40/42], [94mLoss[0m : 2.38063

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.342, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.66497
[1mStep[0m  [4/42], [94mLoss[0m : 2.72795
[1mStep[0m  [8/42], [94mLoss[0m : 2.26130
[1mStep[0m  [12/42], [94mLoss[0m : 2.41905
[1mStep[0m  [16/42], [94mLoss[0m : 2.33557
[1mStep[0m  [20/42], [94mLoss[0m : 2.50338
[1mStep[0m  [24/42], [94mLoss[0m : 2.55550
[1mStep[0m  [28/42], [94mLoss[0m : 2.58525
[1mStep[0m  [32/42], [94mLoss[0m : 2.45825
[1mStep[0m  [36/42], [94mLoss[0m : 2.28198
[1mStep[0m  [40/42], [94mLoss[0m : 2.55369

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.362, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39256
[1mStep[0m  [4/42], [94mLoss[0m : 2.42177
[1mStep[0m  [8/42], [94mLoss[0m : 2.46758
[1mStep[0m  [12/42], [94mLoss[0m : 2.59072
[1mStep[0m  [16/42], [94mLoss[0m : 2.70244
[1mStep[0m  [20/42], [94mLoss[0m : 2.38754
[1mStep[0m  [24/42], [94mLoss[0m : 2.57745
[1mStep[0m  [28/42], [94mLoss[0m : 2.67713
[1mStep[0m  [32/42], [94mLoss[0m : 2.47090
[1mStep[0m  [36/42], [94mLoss[0m : 2.38813
[1mStep[0m  [40/42], [94mLoss[0m : 2.69902

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.351, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40135
[1mStep[0m  [4/42], [94mLoss[0m : 2.31439
[1mStep[0m  [8/42], [94mLoss[0m : 2.60577
[1mStep[0m  [12/42], [94mLoss[0m : 2.46701
[1mStep[0m  [16/42], [94mLoss[0m : 2.52997
[1mStep[0m  [20/42], [94mLoss[0m : 2.59409
[1mStep[0m  [24/42], [94mLoss[0m : 2.63644
[1mStep[0m  [28/42], [94mLoss[0m : 2.38704
[1mStep[0m  [32/42], [94mLoss[0m : 2.49620
[1mStep[0m  [36/42], [94mLoss[0m : 2.34693
[1mStep[0m  [40/42], [94mLoss[0m : 2.46944

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.344, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.341
====================================

Phase 1 - Evaluation MAE:  2.340895346232823
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 2.28304
[1mStep[0m  [4/42], [94mLoss[0m : 2.38584
[1mStep[0m  [8/42], [94mLoss[0m : 2.44496
[1mStep[0m  [12/42], [94mLoss[0m : 2.44308
[1mStep[0m  [16/42], [94mLoss[0m : 2.43347
[1mStep[0m  [20/42], [94mLoss[0m : 2.43031
[1mStep[0m  [24/42], [94mLoss[0m : 2.33378
[1mStep[0m  [28/42], [94mLoss[0m : 2.71052
[1mStep[0m  [32/42], [94mLoss[0m : 2.79286
[1mStep[0m  [36/42], [94mLoss[0m : 2.50802
[1mStep[0m  [40/42], [94mLoss[0m : 2.39101

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.335, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.77295
[1mStep[0m  [4/42], [94mLoss[0m : 2.68489
[1mStep[0m  [8/42], [94mLoss[0m : 2.52661
[1mStep[0m  [12/42], [94mLoss[0m : 2.47901
[1mStep[0m  [16/42], [94mLoss[0m : 2.33327
[1mStep[0m  [20/42], [94mLoss[0m : 2.63512
[1mStep[0m  [24/42], [94mLoss[0m : 2.44006
[1mStep[0m  [28/42], [94mLoss[0m : 2.41027
[1mStep[0m  [32/42], [94mLoss[0m : 2.25807
[1mStep[0m  [36/42], [94mLoss[0m : 2.53368
[1mStep[0m  [40/42], [94mLoss[0m : 2.48107

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.403, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49251
[1mStep[0m  [4/42], [94mLoss[0m : 2.37006
[1mStep[0m  [8/42], [94mLoss[0m : 2.71107
[1mStep[0m  [12/42], [94mLoss[0m : 2.62607
[1mStep[0m  [16/42], [94mLoss[0m : 2.49762
[1mStep[0m  [20/42], [94mLoss[0m : 2.61339
[1mStep[0m  [24/42], [94mLoss[0m : 2.47008
[1mStep[0m  [28/42], [94mLoss[0m : 2.63893
[1mStep[0m  [32/42], [94mLoss[0m : 2.63907
[1mStep[0m  [36/42], [94mLoss[0m : 2.31988
[1mStep[0m  [40/42], [94mLoss[0m : 2.54645

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.434, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.30150
[1mStep[0m  [4/42], [94mLoss[0m : 2.15142
[1mStep[0m  [8/42], [94mLoss[0m : 2.52233
[1mStep[0m  [12/42], [94mLoss[0m : 2.19599
[1mStep[0m  [16/42], [94mLoss[0m : 2.41321
[1mStep[0m  [20/42], [94mLoss[0m : 2.22775
[1mStep[0m  [24/42], [94mLoss[0m : 2.67583
[1mStep[0m  [28/42], [94mLoss[0m : 2.23491
[1mStep[0m  [32/42], [94mLoss[0m : 2.29862
[1mStep[0m  [36/42], [94mLoss[0m : 2.32893
[1mStep[0m  [40/42], [94mLoss[0m : 2.54467

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.488, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34916
[1mStep[0m  [4/42], [94mLoss[0m : 2.31413
[1mStep[0m  [8/42], [94mLoss[0m : 2.38423
[1mStep[0m  [12/42], [94mLoss[0m : 2.29721
[1mStep[0m  [16/42], [94mLoss[0m : 2.12409
[1mStep[0m  [20/42], [94mLoss[0m : 2.46060
[1mStep[0m  [24/42], [94mLoss[0m : 2.10150
[1mStep[0m  [28/42], [94mLoss[0m : 2.46499
[1mStep[0m  [32/42], [94mLoss[0m : 2.53090
[1mStep[0m  [36/42], [94mLoss[0m : 2.32855
[1mStep[0m  [40/42], [94mLoss[0m : 2.67061

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.532, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48866
[1mStep[0m  [4/42], [94mLoss[0m : 2.28536
[1mStep[0m  [8/42], [94mLoss[0m : 2.42414
[1mStep[0m  [12/42], [94mLoss[0m : 2.25308
[1mStep[0m  [16/42], [94mLoss[0m : 2.26590
[1mStep[0m  [20/42], [94mLoss[0m : 2.08765
[1mStep[0m  [24/42], [94mLoss[0m : 2.38717
[1mStep[0m  [28/42], [94mLoss[0m : 2.12007
[1mStep[0m  [32/42], [94mLoss[0m : 2.41841
[1mStep[0m  [36/42], [94mLoss[0m : 2.43686
[1mStep[0m  [40/42], [94mLoss[0m : 2.23194

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.355, [92mTest[0m: 2.510, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40208
[1mStep[0m  [4/42], [94mLoss[0m : 2.14992
[1mStep[0m  [8/42], [94mLoss[0m : 2.40044
[1mStep[0m  [12/42], [94mLoss[0m : 2.46151
[1mStep[0m  [16/42], [94mLoss[0m : 2.25132
[1mStep[0m  [20/42], [94mLoss[0m : 2.63469
[1mStep[0m  [24/42], [94mLoss[0m : 2.33882
[1mStep[0m  [28/42], [94mLoss[0m : 2.16967
[1mStep[0m  [32/42], [94mLoss[0m : 2.23540
[1mStep[0m  [36/42], [94mLoss[0m : 2.35381
[1mStep[0m  [40/42], [94mLoss[0m : 2.10173

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.315, [92mTest[0m: 2.455, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.12546
[1mStep[0m  [4/42], [94mLoss[0m : 2.12807
[1mStep[0m  [8/42], [94mLoss[0m : 2.29728
[1mStep[0m  [12/42], [94mLoss[0m : 2.30863
[1mStep[0m  [16/42], [94mLoss[0m : 2.28575
[1mStep[0m  [20/42], [94mLoss[0m : 2.23055
[1mStep[0m  [24/42], [94mLoss[0m : 2.40110
[1mStep[0m  [28/42], [94mLoss[0m : 2.43417
[1mStep[0m  [32/42], [94mLoss[0m : 2.54272
[1mStep[0m  [36/42], [94mLoss[0m : 2.35668
[1mStep[0m  [40/42], [94mLoss[0m : 2.36360

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.280, [92mTest[0m: 2.589, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.20520
[1mStep[0m  [4/42], [94mLoss[0m : 2.42306
[1mStep[0m  [8/42], [94mLoss[0m : 2.27417
[1mStep[0m  [12/42], [94mLoss[0m : 2.38743
[1mStep[0m  [16/42], [94mLoss[0m : 2.33592
[1mStep[0m  [20/42], [94mLoss[0m : 2.26579
[1mStep[0m  [24/42], [94mLoss[0m : 2.34796
[1mStep[0m  [28/42], [94mLoss[0m : 2.01441
[1mStep[0m  [32/42], [94mLoss[0m : 2.02861
[1mStep[0m  [36/42], [94mLoss[0m : 2.12665
[1mStep[0m  [40/42], [94mLoss[0m : 2.12645

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.241, [92mTest[0m: 2.402, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27933
[1mStep[0m  [4/42], [94mLoss[0m : 2.05646
[1mStep[0m  [8/42], [94mLoss[0m : 2.16581
[1mStep[0m  [12/42], [94mLoss[0m : 2.20000
[1mStep[0m  [16/42], [94mLoss[0m : 2.01018
[1mStep[0m  [20/42], [94mLoss[0m : 2.20643
[1mStep[0m  [24/42], [94mLoss[0m : 2.14296
[1mStep[0m  [28/42], [94mLoss[0m : 2.41454
[1mStep[0m  [32/42], [94mLoss[0m : 2.03653
[1mStep[0m  [36/42], [94mLoss[0m : 2.22583
[1mStep[0m  [40/42], [94mLoss[0m : 2.26199

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.208, [92mTest[0m: 2.421, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.07161
[1mStep[0m  [4/42], [94mLoss[0m : 2.47518
[1mStep[0m  [8/42], [94mLoss[0m : 2.36862
[1mStep[0m  [12/42], [94mLoss[0m : 2.03667
[1mStep[0m  [16/42], [94mLoss[0m : 2.24213
[1mStep[0m  [20/42], [94mLoss[0m : 2.12582
[1mStep[0m  [24/42], [94mLoss[0m : 2.19871
[1mStep[0m  [28/42], [94mLoss[0m : 2.01931
[1mStep[0m  [32/42], [94mLoss[0m : 2.10990
[1mStep[0m  [36/42], [94mLoss[0m : 2.34253
[1mStep[0m  [40/42], [94mLoss[0m : 2.13048

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.167, [92mTest[0m: 2.509, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.06576
[1mStep[0m  [4/42], [94mLoss[0m : 1.95431
[1mStep[0m  [8/42], [94mLoss[0m : 2.07820
[1mStep[0m  [12/42], [94mLoss[0m : 2.12560
[1mStep[0m  [16/42], [94mLoss[0m : 2.10997
[1mStep[0m  [20/42], [94mLoss[0m : 2.00368
[1mStep[0m  [24/42], [94mLoss[0m : 2.27383
[1mStep[0m  [28/42], [94mLoss[0m : 2.03969
[1mStep[0m  [32/42], [94mLoss[0m : 2.31705
[1mStep[0m  [36/42], [94mLoss[0m : 2.24089
[1mStep[0m  [40/42], [94mLoss[0m : 2.29445

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.127, [92mTest[0m: 2.447, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.11913
[1mStep[0m  [4/42], [94mLoss[0m : 2.16101
[1mStep[0m  [8/42], [94mLoss[0m : 1.87952
[1mStep[0m  [12/42], [94mLoss[0m : 2.04842
[1mStep[0m  [16/42], [94mLoss[0m : 2.01004
[1mStep[0m  [20/42], [94mLoss[0m : 2.12465
[1mStep[0m  [24/42], [94mLoss[0m : 2.13717
[1mStep[0m  [28/42], [94mLoss[0m : 2.15954
[1mStep[0m  [32/42], [94mLoss[0m : 1.91518
[1mStep[0m  [36/42], [94mLoss[0m : 1.89082
[1mStep[0m  [40/42], [94mLoss[0m : 2.19426

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.089, [92mTest[0m: 2.439, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.18466
[1mStep[0m  [4/42], [94mLoss[0m : 2.03992
[1mStep[0m  [8/42], [94mLoss[0m : 2.02408
[1mStep[0m  [12/42], [94mLoss[0m : 2.06534
[1mStep[0m  [16/42], [94mLoss[0m : 2.13790
[1mStep[0m  [20/42], [94mLoss[0m : 1.96911
[1mStep[0m  [24/42], [94mLoss[0m : 2.06287
[1mStep[0m  [28/42], [94mLoss[0m : 2.05275
[1mStep[0m  [32/42], [94mLoss[0m : 2.03547
[1mStep[0m  [36/42], [94mLoss[0m : 2.14508
[1mStep[0m  [40/42], [94mLoss[0m : 2.03547

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.064, [92mTest[0m: 2.497, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.04092
[1mStep[0m  [4/42], [94mLoss[0m : 2.13495
[1mStep[0m  [8/42], [94mLoss[0m : 2.05043
[1mStep[0m  [12/42], [94mLoss[0m : 1.80958
[1mStep[0m  [16/42], [94mLoss[0m : 2.19593
[1mStep[0m  [20/42], [94mLoss[0m : 1.84068
[1mStep[0m  [24/42], [94mLoss[0m : 1.94300
[1mStep[0m  [28/42], [94mLoss[0m : 1.91275
[1mStep[0m  [32/42], [94mLoss[0m : 2.10630
[1mStep[0m  [36/42], [94mLoss[0m : 1.90020
[1mStep[0m  [40/42], [94mLoss[0m : 1.89635

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.021, [92mTest[0m: 2.439, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.81095
[1mStep[0m  [4/42], [94mLoss[0m : 2.05421
[1mStep[0m  [8/42], [94mLoss[0m : 2.02930
[1mStep[0m  [12/42], [94mLoss[0m : 1.93818
[1mStep[0m  [16/42], [94mLoss[0m : 1.88657
[1mStep[0m  [20/42], [94mLoss[0m : 2.00165
[1mStep[0m  [24/42], [94mLoss[0m : 1.93744
[1mStep[0m  [28/42], [94mLoss[0m : 1.98595
[1mStep[0m  [32/42], [94mLoss[0m : 1.85995
[1mStep[0m  [36/42], [94mLoss[0m : 1.90143
[1mStep[0m  [40/42], [94mLoss[0m : 1.73825

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.973, [92mTest[0m: 2.468, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.88669
[1mStep[0m  [4/42], [94mLoss[0m : 1.86335
[1mStep[0m  [8/42], [94mLoss[0m : 2.18016
[1mStep[0m  [12/42], [94mLoss[0m : 1.96166
[1mStep[0m  [16/42], [94mLoss[0m : 1.93334
[1mStep[0m  [20/42], [94mLoss[0m : 2.00263
[1mStep[0m  [24/42], [94mLoss[0m : 1.96308
[1mStep[0m  [28/42], [94mLoss[0m : 1.93182
[1mStep[0m  [32/42], [94mLoss[0m : 2.09136
[1mStep[0m  [36/42], [94mLoss[0m : 1.89350
[1mStep[0m  [40/42], [94mLoss[0m : 2.04822

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.949, [92mTest[0m: 2.489, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.02426
[1mStep[0m  [4/42], [94mLoss[0m : 1.87674
[1mStep[0m  [8/42], [94mLoss[0m : 1.78700
[1mStep[0m  [12/42], [94mLoss[0m : 2.00810
[1mStep[0m  [16/42], [94mLoss[0m : 1.83520
[1mStep[0m  [20/42], [94mLoss[0m : 1.80065
[1mStep[0m  [24/42], [94mLoss[0m : 1.92188
[1mStep[0m  [28/42], [94mLoss[0m : 2.00550
[1mStep[0m  [32/42], [94mLoss[0m : 1.95153
[1mStep[0m  [36/42], [94mLoss[0m : 2.08251
[1mStep[0m  [40/42], [94mLoss[0m : 2.05065

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.937, [92mTest[0m: 2.585, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.09675
[1mStep[0m  [4/42], [94mLoss[0m : 1.64041
[1mStep[0m  [8/42], [94mLoss[0m : 1.90175
[1mStep[0m  [12/42], [94mLoss[0m : 1.70175
[1mStep[0m  [16/42], [94mLoss[0m : 1.96926
[1mStep[0m  [20/42], [94mLoss[0m : 1.99162
[1mStep[0m  [24/42], [94mLoss[0m : 2.09108
[1mStep[0m  [28/42], [94mLoss[0m : 1.94730
[1mStep[0m  [32/42], [94mLoss[0m : 1.97438
[1mStep[0m  [36/42], [94mLoss[0m : 1.87037
[1mStep[0m  [40/42], [94mLoss[0m : 1.93165

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.902, [92mTest[0m: 2.562, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.86372
[1mStep[0m  [4/42], [94mLoss[0m : 1.79358
[1mStep[0m  [8/42], [94mLoss[0m : 1.85327
[1mStep[0m  [12/42], [94mLoss[0m : 1.95867
[1mStep[0m  [16/42], [94mLoss[0m : 1.72136
[1mStep[0m  [20/42], [94mLoss[0m : 2.02449
[1mStep[0m  [24/42], [94mLoss[0m : 1.96985
[1mStep[0m  [28/42], [94mLoss[0m : 1.88130
[1mStep[0m  [32/42], [94mLoss[0m : 1.80202
[1mStep[0m  [36/42], [94mLoss[0m : 1.84652
[1mStep[0m  [40/42], [94mLoss[0m : 1.91865

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.851, [92mTest[0m: 2.522, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.93316
[1mStep[0m  [4/42], [94mLoss[0m : 1.93176
[1mStep[0m  [8/42], [94mLoss[0m : 1.80711
[1mStep[0m  [12/42], [94mLoss[0m : 1.84434
[1mStep[0m  [16/42], [94mLoss[0m : 1.98025
[1mStep[0m  [20/42], [94mLoss[0m : 1.95219
[1mStep[0m  [24/42], [94mLoss[0m : 1.96326
[1mStep[0m  [28/42], [94mLoss[0m : 1.94486
[1mStep[0m  [32/42], [94mLoss[0m : 1.87279
[1mStep[0m  [36/42], [94mLoss[0m : 1.83597
[1mStep[0m  [40/42], [94mLoss[0m : 1.95467

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.859, [92mTest[0m: 2.534, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.72786
[1mStep[0m  [4/42], [94mLoss[0m : 1.77544
[1mStep[0m  [8/42], [94mLoss[0m : 1.73465
[1mStep[0m  [12/42], [94mLoss[0m : 1.70021
[1mStep[0m  [16/42], [94mLoss[0m : 1.76719
[1mStep[0m  [20/42], [94mLoss[0m : 1.74932
[1mStep[0m  [24/42], [94mLoss[0m : 1.81570
[1mStep[0m  [28/42], [94mLoss[0m : 1.94588
[1mStep[0m  [32/42], [94mLoss[0m : 1.81714
[1mStep[0m  [36/42], [94mLoss[0m : 1.72881
[1mStep[0m  [40/42], [94mLoss[0m : 1.77058

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.805, [92mTest[0m: 2.464, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.82356
[1mStep[0m  [4/42], [94mLoss[0m : 1.78214
[1mStep[0m  [8/42], [94mLoss[0m : 1.69212
[1mStep[0m  [12/42], [94mLoss[0m : 1.95051
[1mStep[0m  [16/42], [94mLoss[0m : 1.60232
[1mStep[0m  [20/42], [94mLoss[0m : 1.88517
[1mStep[0m  [24/42], [94mLoss[0m : 1.82851
[1mStep[0m  [28/42], [94mLoss[0m : 1.82822
[1mStep[0m  [32/42], [94mLoss[0m : 1.91483
[1mStep[0m  [36/42], [94mLoss[0m : 1.98359
[1mStep[0m  [40/42], [94mLoss[0m : 1.93922

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.798, [92mTest[0m: 2.506, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.83615
[1mStep[0m  [4/42], [94mLoss[0m : 1.79647
[1mStep[0m  [8/42], [94mLoss[0m : 1.71141
[1mStep[0m  [12/42], [94mLoss[0m : 1.67198
[1mStep[0m  [16/42], [94mLoss[0m : 1.82641
[1mStep[0m  [20/42], [94mLoss[0m : 1.93580
[1mStep[0m  [24/42], [94mLoss[0m : 1.71777
[1mStep[0m  [28/42], [94mLoss[0m : 1.73539
[1mStep[0m  [32/42], [94mLoss[0m : 1.63487
[1mStep[0m  [36/42], [94mLoss[0m : 1.80266
[1mStep[0m  [40/42], [94mLoss[0m : 1.68358

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.771, [92mTest[0m: 2.526, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.72598
[1mStep[0m  [4/42], [94mLoss[0m : 1.65749
[1mStep[0m  [8/42], [94mLoss[0m : 1.89106
[1mStep[0m  [12/42], [94mLoss[0m : 1.79698
[1mStep[0m  [16/42], [94mLoss[0m : 1.69889
[1mStep[0m  [20/42], [94mLoss[0m : 1.80509
[1mStep[0m  [24/42], [94mLoss[0m : 1.70613
[1mStep[0m  [28/42], [94mLoss[0m : 1.81680
[1mStep[0m  [32/42], [94mLoss[0m : 1.65447
[1mStep[0m  [36/42], [94mLoss[0m : 1.65655
[1mStep[0m  [40/42], [94mLoss[0m : 2.04026

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.731, [92mTest[0m: 2.474, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.87058
[1mStep[0m  [4/42], [94mLoss[0m : 1.67531
[1mStep[0m  [8/42], [94mLoss[0m : 1.60799
[1mStep[0m  [12/42], [94mLoss[0m : 1.78510
[1mStep[0m  [16/42], [94mLoss[0m : 1.84065
[1mStep[0m  [20/42], [94mLoss[0m : 1.62223
[1mStep[0m  [24/42], [94mLoss[0m : 1.76499
[1mStep[0m  [28/42], [94mLoss[0m : 1.71803
[1mStep[0m  [32/42], [94mLoss[0m : 1.78971
[1mStep[0m  [36/42], [94mLoss[0m : 1.65281
[1mStep[0m  [40/42], [94mLoss[0m : 1.66620

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.718, [92mTest[0m: 2.554, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.75681
[1mStep[0m  [4/42], [94mLoss[0m : 1.78898
[1mStep[0m  [8/42], [94mLoss[0m : 1.66479
[1mStep[0m  [12/42], [94mLoss[0m : 1.67398
[1mStep[0m  [16/42], [94mLoss[0m : 1.68102
[1mStep[0m  [20/42], [94mLoss[0m : 1.66588
[1mStep[0m  [24/42], [94mLoss[0m : 1.72043
[1mStep[0m  [28/42], [94mLoss[0m : 1.54271
[1mStep[0m  [32/42], [94mLoss[0m : 1.63195
[1mStep[0m  [36/42], [94mLoss[0m : 1.68201
[1mStep[0m  [40/42], [94mLoss[0m : 1.68727

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.674, [92mTest[0m: 2.592, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.67010
[1mStep[0m  [4/42], [94mLoss[0m : 1.54568
[1mStep[0m  [8/42], [94mLoss[0m : 1.78185
[1mStep[0m  [12/42], [94mLoss[0m : 1.67001
[1mStep[0m  [16/42], [94mLoss[0m : 1.81748
[1mStep[0m  [20/42], [94mLoss[0m : 1.69201
[1mStep[0m  [24/42], [94mLoss[0m : 1.63165
[1mStep[0m  [28/42], [94mLoss[0m : 1.81837
[1mStep[0m  [32/42], [94mLoss[0m : 1.57900
[1mStep[0m  [36/42], [94mLoss[0m : 1.75476
[1mStep[0m  [40/42], [94mLoss[0m : 1.64822

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.692, [92mTest[0m: 2.617, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.67416
[1mStep[0m  [4/42], [94mLoss[0m : 1.78937
[1mStep[0m  [8/42], [94mLoss[0m : 1.64422
[1mStep[0m  [12/42], [94mLoss[0m : 1.66650
[1mStep[0m  [16/42], [94mLoss[0m : 1.74807
[1mStep[0m  [20/42], [94mLoss[0m : 1.61988
[1mStep[0m  [24/42], [94mLoss[0m : 1.62236
[1mStep[0m  [28/42], [94mLoss[0m : 1.52279
[1mStep[0m  [32/42], [94mLoss[0m : 1.72542
[1mStep[0m  [36/42], [94mLoss[0m : 1.67076
[1mStep[0m  [40/42], [94mLoss[0m : 1.60138

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.653, [92mTest[0m: 2.585, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.63731
[1mStep[0m  [4/42], [94mLoss[0m : 1.65126
[1mStep[0m  [8/42], [94mLoss[0m : 1.69453
[1mStep[0m  [12/42], [94mLoss[0m : 1.69076
[1mStep[0m  [16/42], [94mLoss[0m : 1.59721
[1mStep[0m  [20/42], [94mLoss[0m : 1.60149
[1mStep[0m  [24/42], [94mLoss[0m : 1.73400
[1mStep[0m  [28/42], [94mLoss[0m : 1.53712
[1mStep[0m  [32/42], [94mLoss[0m : 1.87564
[1mStep[0m  [36/42], [94mLoss[0m : 1.67795
[1mStep[0m  [40/42], [94mLoss[0m : 1.66367

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.643, [92mTest[0m: 2.673, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.534
====================================

Phase 2 - Evaluation MAE:  2.5342288187571933
MAE score P1      2.340895
MAE score P2      2.534229
loss               1.64259
learning_rate     0.007525
batch_size             256
hidden_sizes         [100]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.1
weight_decay         0.001
Name: 8, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 10.81266
[1mStep[0m  [8/84], [94mLoss[0m : 9.90380
[1mStep[0m  [16/84], [94mLoss[0m : 7.50697
[1mStep[0m  [24/84], [94mLoss[0m : 6.04029
[1mStep[0m  [32/84], [94mLoss[0m : 4.69211
[1mStep[0m  [40/84], [94mLoss[0m : 3.62649
[1mStep[0m  [48/84], [94mLoss[0m : 3.48024
[1mStep[0m  [56/84], [94mLoss[0m : 2.54323
[1mStep[0m  [64/84], [94mLoss[0m : 2.40896
[1mStep[0m  [72/84], [94mLoss[0m : 2.73824
[1mStep[0m  [80/84], [94mLoss[0m : 2.84161

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.923, [92mTest[0m: 10.851, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.88179
[1mStep[0m  [8/84], [94mLoss[0m : 2.40589
[1mStep[0m  [16/84], [94mLoss[0m : 2.91862
[1mStep[0m  [24/84], [94mLoss[0m : 2.65564
[1mStep[0m  [32/84], [94mLoss[0m : 2.61410
[1mStep[0m  [40/84], [94mLoss[0m : 2.36208
[1mStep[0m  [48/84], [94mLoss[0m : 2.56003
[1mStep[0m  [56/84], [94mLoss[0m : 2.54228
[1mStep[0m  [64/84], [94mLoss[0m : 2.57985
[1mStep[0m  [72/84], [94mLoss[0m : 2.53948
[1mStep[0m  [80/84], [94mLoss[0m : 2.85144

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.569, [92mTest[0m: 2.433, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59897
[1mStep[0m  [8/84], [94mLoss[0m : 2.47785
[1mStep[0m  [16/84], [94mLoss[0m : 2.46100
[1mStep[0m  [24/84], [94mLoss[0m : 2.94715
[1mStep[0m  [32/84], [94mLoss[0m : 2.41633
[1mStep[0m  [40/84], [94mLoss[0m : 2.61087
[1mStep[0m  [48/84], [94mLoss[0m : 2.77796
[1mStep[0m  [56/84], [94mLoss[0m : 2.47250
[1mStep[0m  [64/84], [94mLoss[0m : 2.67470
[1mStep[0m  [72/84], [94mLoss[0m : 2.40652
[1mStep[0m  [80/84], [94mLoss[0m : 2.36655

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.540, [92mTest[0m: 2.363, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52927
[1mStep[0m  [8/84], [94mLoss[0m : 2.88198
[1mStep[0m  [16/84], [94mLoss[0m : 2.55870
[1mStep[0m  [24/84], [94mLoss[0m : 2.69294
[1mStep[0m  [32/84], [94mLoss[0m : 2.33461
[1mStep[0m  [40/84], [94mLoss[0m : 2.78505
[1mStep[0m  [48/84], [94mLoss[0m : 2.67000
[1mStep[0m  [56/84], [94mLoss[0m : 2.54223
[1mStep[0m  [64/84], [94mLoss[0m : 2.30443
[1mStep[0m  [72/84], [94mLoss[0m : 2.31967
[1mStep[0m  [80/84], [94mLoss[0m : 2.36979

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.522, [92mTest[0m: 2.351, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.07816
[1mStep[0m  [8/84], [94mLoss[0m : 2.57997
[1mStep[0m  [16/84], [94mLoss[0m : 2.76972
[1mStep[0m  [24/84], [94mLoss[0m : 2.50769
[1mStep[0m  [32/84], [94mLoss[0m : 2.42586
[1mStep[0m  [40/84], [94mLoss[0m : 2.43678
[1mStep[0m  [48/84], [94mLoss[0m : 2.45575
[1mStep[0m  [56/84], [94mLoss[0m : 2.57975
[1mStep[0m  [64/84], [94mLoss[0m : 2.65233
[1mStep[0m  [72/84], [94mLoss[0m : 2.61912
[1mStep[0m  [80/84], [94mLoss[0m : 2.41807

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.352, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.69136
[1mStep[0m  [8/84], [94mLoss[0m : 2.45564
[1mStep[0m  [16/84], [94mLoss[0m : 2.03694
[1mStep[0m  [24/84], [94mLoss[0m : 2.34084
[1mStep[0m  [32/84], [94mLoss[0m : 2.31563
[1mStep[0m  [40/84], [94mLoss[0m : 2.59928
[1mStep[0m  [48/84], [94mLoss[0m : 2.47843
[1mStep[0m  [56/84], [94mLoss[0m : 2.30751
[1mStep[0m  [64/84], [94mLoss[0m : 2.65680
[1mStep[0m  [72/84], [94mLoss[0m : 2.31220
[1mStep[0m  [80/84], [94mLoss[0m : 2.60206

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.357, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45266
[1mStep[0m  [8/84], [94mLoss[0m : 2.31782
[1mStep[0m  [16/84], [94mLoss[0m : 2.32882
[1mStep[0m  [24/84], [94mLoss[0m : 2.39179
[1mStep[0m  [32/84], [94mLoss[0m : 2.32230
[1mStep[0m  [40/84], [94mLoss[0m : 2.24371
[1mStep[0m  [48/84], [94mLoss[0m : 2.22927
[1mStep[0m  [56/84], [94mLoss[0m : 2.37871
[1mStep[0m  [64/84], [94mLoss[0m : 2.16870
[1mStep[0m  [72/84], [94mLoss[0m : 2.63518
[1mStep[0m  [80/84], [94mLoss[0m : 2.50228

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.341, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.19705
[1mStep[0m  [8/84], [94mLoss[0m : 2.68007
[1mStep[0m  [16/84], [94mLoss[0m : 2.34763
[1mStep[0m  [24/84], [94mLoss[0m : 2.47056
[1mStep[0m  [32/84], [94mLoss[0m : 2.33403
[1mStep[0m  [40/84], [94mLoss[0m : 2.85516
[1mStep[0m  [48/84], [94mLoss[0m : 2.16052
[1mStep[0m  [56/84], [94mLoss[0m : 2.71060
[1mStep[0m  [64/84], [94mLoss[0m : 2.65837
[1mStep[0m  [72/84], [94mLoss[0m : 2.44922
[1mStep[0m  [80/84], [94mLoss[0m : 2.49109

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.334, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43073
[1mStep[0m  [8/84], [94mLoss[0m : 2.42905
[1mStep[0m  [16/84], [94mLoss[0m : 2.32279
[1mStep[0m  [24/84], [94mLoss[0m : 2.43460
[1mStep[0m  [32/84], [94mLoss[0m : 2.72177
[1mStep[0m  [40/84], [94mLoss[0m : 2.32762
[1mStep[0m  [48/84], [94mLoss[0m : 2.45158
[1mStep[0m  [56/84], [94mLoss[0m : 2.76238
[1mStep[0m  [64/84], [94mLoss[0m : 2.52370
[1mStep[0m  [72/84], [94mLoss[0m : 2.65714
[1mStep[0m  [80/84], [94mLoss[0m : 2.50804

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.334, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55159
[1mStep[0m  [8/84], [94mLoss[0m : 2.26158
[1mStep[0m  [16/84], [94mLoss[0m : 2.54881
[1mStep[0m  [24/84], [94mLoss[0m : 2.35950
[1mStep[0m  [32/84], [94mLoss[0m : 2.65117
[1mStep[0m  [40/84], [94mLoss[0m : 2.68322
[1mStep[0m  [48/84], [94mLoss[0m : 2.29194
[1mStep[0m  [56/84], [94mLoss[0m : 2.71778
[1mStep[0m  [64/84], [94mLoss[0m : 2.60643
[1mStep[0m  [72/84], [94mLoss[0m : 2.58439
[1mStep[0m  [80/84], [94mLoss[0m : 2.27993

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.337, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.29524
[1mStep[0m  [8/84], [94mLoss[0m : 2.56941
[1mStep[0m  [16/84], [94mLoss[0m : 2.10285
[1mStep[0m  [24/84], [94mLoss[0m : 2.33103
[1mStep[0m  [32/84], [94mLoss[0m : 2.47922
[1mStep[0m  [40/84], [94mLoss[0m : 2.63624
[1mStep[0m  [48/84], [94mLoss[0m : 2.65803
[1mStep[0m  [56/84], [94mLoss[0m : 2.45085
[1mStep[0m  [64/84], [94mLoss[0m : 2.74512
[1mStep[0m  [72/84], [94mLoss[0m : 2.24461
[1mStep[0m  [80/84], [94mLoss[0m : 2.20919

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.328, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.85092
[1mStep[0m  [8/84], [94mLoss[0m : 2.72609
[1mStep[0m  [16/84], [94mLoss[0m : 2.44324
[1mStep[0m  [24/84], [94mLoss[0m : 2.37896
[1mStep[0m  [32/84], [94mLoss[0m : 2.36989
[1mStep[0m  [40/84], [94mLoss[0m : 2.36539
[1mStep[0m  [48/84], [94mLoss[0m : 2.64727
[1mStep[0m  [56/84], [94mLoss[0m : 2.47697
[1mStep[0m  [64/84], [94mLoss[0m : 2.59516
[1mStep[0m  [72/84], [94mLoss[0m : 2.46098
[1mStep[0m  [80/84], [94mLoss[0m : 2.70384

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.322, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.64470
[1mStep[0m  [8/84], [94mLoss[0m : 2.32939
[1mStep[0m  [16/84], [94mLoss[0m : 2.38578
[1mStep[0m  [24/84], [94mLoss[0m : 2.34639
[1mStep[0m  [32/84], [94mLoss[0m : 2.52921
[1mStep[0m  [40/84], [94mLoss[0m : 2.71738
[1mStep[0m  [48/84], [94mLoss[0m : 2.70379
[1mStep[0m  [56/84], [94mLoss[0m : 2.74634
[1mStep[0m  [64/84], [94mLoss[0m : 2.80440
[1mStep[0m  [72/84], [94mLoss[0m : 2.43349
[1mStep[0m  [80/84], [94mLoss[0m : 2.30709

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.332, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49348
[1mStep[0m  [8/84], [94mLoss[0m : 2.69404
[1mStep[0m  [16/84], [94mLoss[0m : 2.68645
[1mStep[0m  [24/84], [94mLoss[0m : 2.56016
[1mStep[0m  [32/84], [94mLoss[0m : 2.20248
[1mStep[0m  [40/84], [94mLoss[0m : 2.41072
[1mStep[0m  [48/84], [94mLoss[0m : 2.26617
[1mStep[0m  [56/84], [94mLoss[0m : 2.38062
[1mStep[0m  [64/84], [94mLoss[0m : 2.37429
[1mStep[0m  [72/84], [94mLoss[0m : 2.56949
[1mStep[0m  [80/84], [94mLoss[0m : 2.51796

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.328, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48896
[1mStep[0m  [8/84], [94mLoss[0m : 2.39183
[1mStep[0m  [16/84], [94mLoss[0m : 2.63327
[1mStep[0m  [24/84], [94mLoss[0m : 2.61488
[1mStep[0m  [32/84], [94mLoss[0m : 2.80137
[1mStep[0m  [40/84], [94mLoss[0m : 2.50550
[1mStep[0m  [48/84], [94mLoss[0m : 2.29423
[1mStep[0m  [56/84], [94mLoss[0m : 2.60533
[1mStep[0m  [64/84], [94mLoss[0m : 2.38983
[1mStep[0m  [72/84], [94mLoss[0m : 2.14702
[1mStep[0m  [80/84], [94mLoss[0m : 2.53431

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.329, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46407
[1mStep[0m  [8/84], [94mLoss[0m : 2.68485
[1mStep[0m  [16/84], [94mLoss[0m : 2.53991
[1mStep[0m  [24/84], [94mLoss[0m : 2.72725
[1mStep[0m  [32/84], [94mLoss[0m : 2.33807
[1mStep[0m  [40/84], [94mLoss[0m : 2.69626
[1mStep[0m  [48/84], [94mLoss[0m : 2.60452
[1mStep[0m  [56/84], [94mLoss[0m : 2.59986
[1mStep[0m  [64/84], [94mLoss[0m : 2.50505
[1mStep[0m  [72/84], [94mLoss[0m : 2.46521
[1mStep[0m  [80/84], [94mLoss[0m : 2.59004

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.334, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.69346
[1mStep[0m  [8/84], [94mLoss[0m : 2.54185
[1mStep[0m  [16/84], [94mLoss[0m : 2.45045
[1mStep[0m  [24/84], [94mLoss[0m : 2.60801
[1mStep[0m  [32/84], [94mLoss[0m : 2.61740
[1mStep[0m  [40/84], [94mLoss[0m : 2.72172
[1mStep[0m  [48/84], [94mLoss[0m : 2.54970
[1mStep[0m  [56/84], [94mLoss[0m : 2.33702
[1mStep[0m  [64/84], [94mLoss[0m : 2.37176
[1mStep[0m  [72/84], [94mLoss[0m : 2.34556
[1mStep[0m  [80/84], [94mLoss[0m : 2.48491

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.344, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43387
[1mStep[0m  [8/84], [94mLoss[0m : 2.43648
[1mStep[0m  [16/84], [94mLoss[0m : 2.46096
[1mStep[0m  [24/84], [94mLoss[0m : 2.60495
[1mStep[0m  [32/84], [94mLoss[0m : 2.24891
[1mStep[0m  [40/84], [94mLoss[0m : 2.83800
[1mStep[0m  [48/84], [94mLoss[0m : 2.49824
[1mStep[0m  [56/84], [94mLoss[0m : 2.54720
[1mStep[0m  [64/84], [94mLoss[0m : 2.42632
[1mStep[0m  [72/84], [94mLoss[0m : 2.82068
[1mStep[0m  [80/84], [94mLoss[0m : 2.27587

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.329, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35546
[1mStep[0m  [8/84], [94mLoss[0m : 2.41876
[1mStep[0m  [16/84], [94mLoss[0m : 2.61829
[1mStep[0m  [24/84], [94mLoss[0m : 2.37715
[1mStep[0m  [32/84], [94mLoss[0m : 2.62619
[1mStep[0m  [40/84], [94mLoss[0m : 2.52380
[1mStep[0m  [48/84], [94mLoss[0m : 2.28865
[1mStep[0m  [56/84], [94mLoss[0m : 2.37892
[1mStep[0m  [64/84], [94mLoss[0m : 2.75532
[1mStep[0m  [72/84], [94mLoss[0m : 2.64451
[1mStep[0m  [80/84], [94mLoss[0m : 2.76422

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.330, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.18195
[1mStep[0m  [8/84], [94mLoss[0m : 2.67352
[1mStep[0m  [16/84], [94mLoss[0m : 2.46175
[1mStep[0m  [24/84], [94mLoss[0m : 2.61735
[1mStep[0m  [32/84], [94mLoss[0m : 2.22415
[1mStep[0m  [40/84], [94mLoss[0m : 2.44342
[1mStep[0m  [48/84], [94mLoss[0m : 2.64344
[1mStep[0m  [56/84], [94mLoss[0m : 2.24353
[1mStep[0m  [64/84], [94mLoss[0m : 2.61870
[1mStep[0m  [72/84], [94mLoss[0m : 2.48044
[1mStep[0m  [80/84], [94mLoss[0m : 2.48947

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.333, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44525
[1mStep[0m  [8/84], [94mLoss[0m : 2.56050
[1mStep[0m  [16/84], [94mLoss[0m : 2.55722
[1mStep[0m  [24/84], [94mLoss[0m : 2.44126
[1mStep[0m  [32/84], [94mLoss[0m : 2.50093
[1mStep[0m  [40/84], [94mLoss[0m : 2.33424
[1mStep[0m  [48/84], [94mLoss[0m : 2.75084
[1mStep[0m  [56/84], [94mLoss[0m : 2.41335
[1mStep[0m  [64/84], [94mLoss[0m : 2.31806
[1mStep[0m  [72/84], [94mLoss[0m : 2.60671
[1mStep[0m  [80/84], [94mLoss[0m : 2.62780

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.332, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44532
[1mStep[0m  [8/84], [94mLoss[0m : 2.40523
[1mStep[0m  [16/84], [94mLoss[0m : 2.50464
[1mStep[0m  [24/84], [94mLoss[0m : 2.30162
[1mStep[0m  [32/84], [94mLoss[0m : 2.21818
[1mStep[0m  [40/84], [94mLoss[0m : 2.34729
[1mStep[0m  [48/84], [94mLoss[0m : 2.39292
[1mStep[0m  [56/84], [94mLoss[0m : 2.36591
[1mStep[0m  [64/84], [94mLoss[0m : 2.38891
[1mStep[0m  [72/84], [94mLoss[0m : 2.78901
[1mStep[0m  [80/84], [94mLoss[0m : 2.47710

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.331, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.58789
[1mStep[0m  [8/84], [94mLoss[0m : 2.90373
[1mStep[0m  [16/84], [94mLoss[0m : 2.73536
[1mStep[0m  [24/84], [94mLoss[0m : 2.26743
[1mStep[0m  [32/84], [94mLoss[0m : 2.71870
[1mStep[0m  [40/84], [94mLoss[0m : 2.54345
[1mStep[0m  [48/84], [94mLoss[0m : 2.93901
[1mStep[0m  [56/84], [94mLoss[0m : 2.91649
[1mStep[0m  [64/84], [94mLoss[0m : 2.20391
[1mStep[0m  [72/84], [94mLoss[0m : 2.37989
[1mStep[0m  [80/84], [94mLoss[0m : 2.46345

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.338, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.57705
[1mStep[0m  [8/84], [94mLoss[0m : 2.23025
[1mStep[0m  [16/84], [94mLoss[0m : 2.61176
[1mStep[0m  [24/84], [94mLoss[0m : 2.22859
[1mStep[0m  [32/84], [94mLoss[0m : 2.69716
[1mStep[0m  [40/84], [94mLoss[0m : 2.33944
[1mStep[0m  [48/84], [94mLoss[0m : 2.56963
[1mStep[0m  [56/84], [94mLoss[0m : 2.45356
[1mStep[0m  [64/84], [94mLoss[0m : 2.62867
[1mStep[0m  [72/84], [94mLoss[0m : 2.65700
[1mStep[0m  [80/84], [94mLoss[0m : 2.38747

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.326, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41767
[1mStep[0m  [8/84], [94mLoss[0m : 2.62355
[1mStep[0m  [16/84], [94mLoss[0m : 2.46434
[1mStep[0m  [24/84], [94mLoss[0m : 2.35903
[1mStep[0m  [32/84], [94mLoss[0m : 2.33641
[1mStep[0m  [40/84], [94mLoss[0m : 3.02489
[1mStep[0m  [48/84], [94mLoss[0m : 2.32216
[1mStep[0m  [56/84], [94mLoss[0m : 2.68390
[1mStep[0m  [64/84], [94mLoss[0m : 2.33921
[1mStep[0m  [72/84], [94mLoss[0m : 2.77102
[1mStep[0m  [80/84], [94mLoss[0m : 2.57811

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.320, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.34949
[1mStep[0m  [8/84], [94mLoss[0m : 2.32447
[1mStep[0m  [16/84], [94mLoss[0m : 2.55589
[1mStep[0m  [24/84], [94mLoss[0m : 2.26622
[1mStep[0m  [32/84], [94mLoss[0m : 2.28801
[1mStep[0m  [40/84], [94mLoss[0m : 2.51680
[1mStep[0m  [48/84], [94mLoss[0m : 2.60802
[1mStep[0m  [56/84], [94mLoss[0m : 2.55006
[1mStep[0m  [64/84], [94mLoss[0m : 2.33521
[1mStep[0m  [72/84], [94mLoss[0m : 2.22758
[1mStep[0m  [80/84], [94mLoss[0m : 2.87562

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.332, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49419
[1mStep[0m  [8/84], [94mLoss[0m : 2.42762
[1mStep[0m  [16/84], [94mLoss[0m : 2.56884
[1mStep[0m  [24/84], [94mLoss[0m : 2.50695
[1mStep[0m  [32/84], [94mLoss[0m : 2.48188
[1mStep[0m  [40/84], [94mLoss[0m : 2.50900
[1mStep[0m  [48/84], [94mLoss[0m : 2.40246
[1mStep[0m  [56/84], [94mLoss[0m : 2.44871
[1mStep[0m  [64/84], [94mLoss[0m : 2.35557
[1mStep[0m  [72/84], [94mLoss[0m : 2.58985
[1mStep[0m  [80/84], [94mLoss[0m : 2.48762

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.327, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36011
[1mStep[0m  [8/84], [94mLoss[0m : 2.38577
[1mStep[0m  [16/84], [94mLoss[0m : 2.51366
[1mStep[0m  [24/84], [94mLoss[0m : 2.74606
[1mStep[0m  [32/84], [94mLoss[0m : 2.39472
[1mStep[0m  [40/84], [94mLoss[0m : 2.81106
[1mStep[0m  [48/84], [94mLoss[0m : 2.35975
[1mStep[0m  [56/84], [94mLoss[0m : 2.26128
[1mStep[0m  [64/84], [94mLoss[0m : 2.67189
[1mStep[0m  [72/84], [94mLoss[0m : 2.50148
[1mStep[0m  [80/84], [94mLoss[0m : 2.35976

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.333, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.81946
[1mStep[0m  [8/84], [94mLoss[0m : 2.41769
[1mStep[0m  [16/84], [94mLoss[0m : 2.59453
[1mStep[0m  [24/84], [94mLoss[0m : 2.14860
[1mStep[0m  [32/84], [94mLoss[0m : 2.80289
[1mStep[0m  [40/84], [94mLoss[0m : 2.47570
[1mStep[0m  [48/84], [94mLoss[0m : 2.51547
[1mStep[0m  [56/84], [94mLoss[0m : 2.23677
[1mStep[0m  [64/84], [94mLoss[0m : 2.62196
[1mStep[0m  [72/84], [94mLoss[0m : 2.18988
[1mStep[0m  [80/84], [94mLoss[0m : 2.14169

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.328, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45770
[1mStep[0m  [8/84], [94mLoss[0m : 2.42361
[1mStep[0m  [16/84], [94mLoss[0m : 2.28947
[1mStep[0m  [24/84], [94mLoss[0m : 2.39106
[1mStep[0m  [32/84], [94mLoss[0m : 2.00822
[1mStep[0m  [40/84], [94mLoss[0m : 2.81974
[1mStep[0m  [48/84], [94mLoss[0m : 2.43138
[1mStep[0m  [56/84], [94mLoss[0m : 2.40086
[1mStep[0m  [64/84], [94mLoss[0m : 2.39067
[1mStep[0m  [72/84], [94mLoss[0m : 2.63614
[1mStep[0m  [80/84], [94mLoss[0m : 2.62476

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.327, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.327
====================================

Phase 1 - Evaluation MAE:  2.3274461797305515
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 2.58563
[1mStep[0m  [8/84], [94mLoss[0m : 2.35365
[1mStep[0m  [16/84], [94mLoss[0m : 2.35346
[1mStep[0m  [24/84], [94mLoss[0m : 2.73406
[1mStep[0m  [32/84], [94mLoss[0m : 2.72013
[1mStep[0m  [40/84], [94mLoss[0m : 2.44805
[1mStep[0m  [48/84], [94mLoss[0m : 2.51481
[1mStep[0m  [56/84], [94mLoss[0m : 2.53110
[1mStep[0m  [64/84], [94mLoss[0m : 2.35883
[1mStep[0m  [72/84], [94mLoss[0m : 2.49466
[1mStep[0m  [80/84], [94mLoss[0m : 2.22755

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.312, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44393
[1mStep[0m  [8/84], [94mLoss[0m : 2.10954
[1mStep[0m  [16/84], [94mLoss[0m : 2.66399
[1mStep[0m  [24/84], [94mLoss[0m : 2.63788
[1mStep[0m  [32/84], [94mLoss[0m : 2.60900
[1mStep[0m  [40/84], [94mLoss[0m : 2.55243
[1mStep[0m  [48/84], [94mLoss[0m : 2.49834
[1mStep[0m  [56/84], [94mLoss[0m : 2.34170
[1mStep[0m  [64/84], [94mLoss[0m : 2.60431
[1mStep[0m  [72/84], [94mLoss[0m : 2.84992
[1mStep[0m  [80/84], [94mLoss[0m : 2.40723

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.616, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47661
[1mStep[0m  [8/84], [94mLoss[0m : 2.16874
[1mStep[0m  [16/84], [94mLoss[0m : 2.37745
[1mStep[0m  [24/84], [94mLoss[0m : 2.26715
[1mStep[0m  [32/84], [94mLoss[0m : 2.18082
[1mStep[0m  [40/84], [94mLoss[0m : 2.32154
[1mStep[0m  [48/84], [94mLoss[0m : 2.18582
[1mStep[0m  [56/84], [94mLoss[0m : 2.50687
[1mStep[0m  [64/84], [94mLoss[0m : 2.44637
[1mStep[0m  [72/84], [94mLoss[0m : 2.24828
[1mStep[0m  [80/84], [94mLoss[0m : 2.24229

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.360, [92mTest[0m: 2.513, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53037
[1mStep[0m  [8/84], [94mLoss[0m : 2.39040
[1mStep[0m  [16/84], [94mLoss[0m : 2.12419
[1mStep[0m  [24/84], [94mLoss[0m : 2.25719
[1mStep[0m  [32/84], [94mLoss[0m : 2.37205
[1mStep[0m  [40/84], [94mLoss[0m : 2.45257
[1mStep[0m  [48/84], [94mLoss[0m : 2.28714
[1mStep[0m  [56/84], [94mLoss[0m : 2.15524
[1mStep[0m  [64/84], [94mLoss[0m : 2.05261
[1mStep[0m  [72/84], [94mLoss[0m : 2.32352
[1mStep[0m  [80/84], [94mLoss[0m : 2.18961

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.307, [92mTest[0m: 2.411, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.09298
[1mStep[0m  [8/84], [94mLoss[0m : 2.28657
[1mStep[0m  [16/84], [94mLoss[0m : 2.09804
[1mStep[0m  [24/84], [94mLoss[0m : 2.35696
[1mStep[0m  [32/84], [94mLoss[0m : 2.16072
[1mStep[0m  [40/84], [94mLoss[0m : 2.37402
[1mStep[0m  [48/84], [94mLoss[0m : 2.40884
[1mStep[0m  [56/84], [94mLoss[0m : 2.40604
[1mStep[0m  [64/84], [94mLoss[0m : 2.17669
[1mStep[0m  [72/84], [94mLoss[0m : 2.33681
[1mStep[0m  [80/84], [94mLoss[0m : 2.14269

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.253, [92mTest[0m: 2.413, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26818
[1mStep[0m  [8/84], [94mLoss[0m : 2.12330
[1mStep[0m  [16/84], [94mLoss[0m : 2.09976
[1mStep[0m  [24/84], [94mLoss[0m : 2.09707
[1mStep[0m  [32/84], [94mLoss[0m : 2.08702
[1mStep[0m  [40/84], [94mLoss[0m : 2.32434
[1mStep[0m  [48/84], [94mLoss[0m : 2.31247
[1mStep[0m  [56/84], [94mLoss[0m : 2.43570
[1mStep[0m  [64/84], [94mLoss[0m : 2.27672
[1mStep[0m  [72/84], [94mLoss[0m : 2.50933
[1mStep[0m  [80/84], [94mLoss[0m : 2.15468

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.206, [92mTest[0m: 2.399, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.11199
[1mStep[0m  [8/84], [94mLoss[0m : 2.54191
[1mStep[0m  [16/84], [94mLoss[0m : 2.00109
[1mStep[0m  [24/84], [94mLoss[0m : 1.96458
[1mStep[0m  [32/84], [94mLoss[0m : 2.07924
[1mStep[0m  [40/84], [94mLoss[0m : 2.00554
[1mStep[0m  [48/84], [94mLoss[0m : 1.98630
[1mStep[0m  [56/84], [94mLoss[0m : 2.12222
[1mStep[0m  [64/84], [94mLoss[0m : 2.37658
[1mStep[0m  [72/84], [94mLoss[0m : 2.14675
[1mStep[0m  [80/84], [94mLoss[0m : 2.54330

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.132, [92mTest[0m: 2.478, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.22579
[1mStep[0m  [8/84], [94mLoss[0m : 2.08296
[1mStep[0m  [16/84], [94mLoss[0m : 1.82742
[1mStep[0m  [24/84], [94mLoss[0m : 2.15023
[1mStep[0m  [32/84], [94mLoss[0m : 2.33875
[1mStep[0m  [40/84], [94mLoss[0m : 2.12085
[1mStep[0m  [48/84], [94mLoss[0m : 2.07617
[1mStep[0m  [56/84], [94mLoss[0m : 1.91671
[1mStep[0m  [64/84], [94mLoss[0m : 2.19048
[1mStep[0m  [72/84], [94mLoss[0m : 2.09776
[1mStep[0m  [80/84], [94mLoss[0m : 1.73642

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.082, [92mTest[0m: 2.376, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.94352
[1mStep[0m  [8/84], [94mLoss[0m : 1.91845
[1mStep[0m  [16/84], [94mLoss[0m : 1.93890
[1mStep[0m  [24/84], [94mLoss[0m : 2.29088
[1mStep[0m  [32/84], [94mLoss[0m : 1.90104
[1mStep[0m  [40/84], [94mLoss[0m : 2.32631
[1mStep[0m  [48/84], [94mLoss[0m : 1.80868
[1mStep[0m  [56/84], [94mLoss[0m : 2.11496
[1mStep[0m  [64/84], [94mLoss[0m : 2.32805
[1mStep[0m  [72/84], [94mLoss[0m : 2.04682
[1mStep[0m  [80/84], [94mLoss[0m : 2.13259

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.044, [92mTest[0m: 2.442, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.72553
[1mStep[0m  [8/84], [94mLoss[0m : 2.13415
[1mStep[0m  [16/84], [94mLoss[0m : 2.11947
[1mStep[0m  [24/84], [94mLoss[0m : 2.12800
[1mStep[0m  [32/84], [94mLoss[0m : 1.79926
[1mStep[0m  [40/84], [94mLoss[0m : 1.95236
[1mStep[0m  [48/84], [94mLoss[0m : 1.81551
[1mStep[0m  [56/84], [94mLoss[0m : 2.15454
[1mStep[0m  [64/84], [94mLoss[0m : 1.78774
[1mStep[0m  [72/84], [94mLoss[0m : 2.23706
[1mStep[0m  [80/84], [94mLoss[0m : 2.25073

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.978, [92mTest[0m: 2.385, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.89128
[1mStep[0m  [8/84], [94mLoss[0m : 1.79506
[1mStep[0m  [16/84], [94mLoss[0m : 1.95732
[1mStep[0m  [24/84], [94mLoss[0m : 1.72827
[1mStep[0m  [32/84], [94mLoss[0m : 2.01581
[1mStep[0m  [40/84], [94mLoss[0m : 1.67614
[1mStep[0m  [48/84], [94mLoss[0m : 1.80197
[1mStep[0m  [56/84], [94mLoss[0m : 1.94503
[1mStep[0m  [64/84], [94mLoss[0m : 1.68104
[1mStep[0m  [72/84], [94mLoss[0m : 2.04385
[1mStep[0m  [80/84], [94mLoss[0m : 2.31431

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.930, [92mTest[0m: 2.534, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.92076
[1mStep[0m  [8/84], [94mLoss[0m : 1.84739
[1mStep[0m  [16/84], [94mLoss[0m : 1.75252
[1mStep[0m  [24/84], [94mLoss[0m : 1.56091
[1mStep[0m  [32/84], [94mLoss[0m : 2.01519
[1mStep[0m  [40/84], [94mLoss[0m : 1.90505
[1mStep[0m  [48/84], [94mLoss[0m : 1.83901
[1mStep[0m  [56/84], [94mLoss[0m : 1.81061
[1mStep[0m  [64/84], [94mLoss[0m : 2.02589
[1mStep[0m  [72/84], [94mLoss[0m : 1.90026
[1mStep[0m  [80/84], [94mLoss[0m : 1.92192

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.895, [92mTest[0m: 2.532, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.64181
[1mStep[0m  [8/84], [94mLoss[0m : 1.71986
[1mStep[0m  [16/84], [94mLoss[0m : 1.71512
[1mStep[0m  [24/84], [94mLoss[0m : 1.82981
[1mStep[0m  [32/84], [94mLoss[0m : 1.73627
[1mStep[0m  [40/84], [94mLoss[0m : 1.74483
[1mStep[0m  [48/84], [94mLoss[0m : 2.04556
[1mStep[0m  [56/84], [94mLoss[0m : 1.88983
[1mStep[0m  [64/84], [94mLoss[0m : 2.16568
[1mStep[0m  [72/84], [94mLoss[0m : 1.94819
[1mStep[0m  [80/84], [94mLoss[0m : 1.82601

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.857, [92mTest[0m: 2.438, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.80941
[1mStep[0m  [8/84], [94mLoss[0m : 1.69463
[1mStep[0m  [16/84], [94mLoss[0m : 1.73086
[1mStep[0m  [24/84], [94mLoss[0m : 1.77301
[1mStep[0m  [32/84], [94mLoss[0m : 1.84261
[1mStep[0m  [40/84], [94mLoss[0m : 1.70768
[1mStep[0m  [48/84], [94mLoss[0m : 1.69175
[1mStep[0m  [56/84], [94mLoss[0m : 2.14028
[1mStep[0m  [64/84], [94mLoss[0m : 1.74594
[1mStep[0m  [72/84], [94mLoss[0m : 1.91224
[1mStep[0m  [80/84], [94mLoss[0m : 1.89138

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.818, [92mTest[0m: 2.481, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.69746
[1mStep[0m  [8/84], [94mLoss[0m : 1.63587
[1mStep[0m  [16/84], [94mLoss[0m : 1.70914
[1mStep[0m  [24/84], [94mLoss[0m : 2.06881
[1mStep[0m  [32/84], [94mLoss[0m : 1.79419
[1mStep[0m  [40/84], [94mLoss[0m : 1.84813
[1mStep[0m  [48/84], [94mLoss[0m : 1.63897
[1mStep[0m  [56/84], [94mLoss[0m : 1.75974
[1mStep[0m  [64/84], [94mLoss[0m : 1.71176
[1mStep[0m  [72/84], [94mLoss[0m : 1.76746
[1mStep[0m  [80/84], [94mLoss[0m : 1.66209

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.772, [92mTest[0m: 2.429, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.83751
[1mStep[0m  [8/84], [94mLoss[0m : 1.73238
[1mStep[0m  [16/84], [94mLoss[0m : 1.79752
[1mStep[0m  [24/84], [94mLoss[0m : 1.63132
[1mStep[0m  [32/84], [94mLoss[0m : 1.82510
[1mStep[0m  [40/84], [94mLoss[0m : 1.79678
[1mStep[0m  [48/84], [94mLoss[0m : 1.66074
[1mStep[0m  [56/84], [94mLoss[0m : 1.68744
[1mStep[0m  [64/84], [94mLoss[0m : 1.60530
[1mStep[0m  [72/84], [94mLoss[0m : 1.79098
[1mStep[0m  [80/84], [94mLoss[0m : 1.65781

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.760, [92mTest[0m: 2.519, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.61972
[1mStep[0m  [8/84], [94mLoss[0m : 1.46890
[1mStep[0m  [16/84], [94mLoss[0m : 1.74115
[1mStep[0m  [24/84], [94mLoss[0m : 1.82839
[1mStep[0m  [32/84], [94mLoss[0m : 1.67590
[1mStep[0m  [40/84], [94mLoss[0m : 1.84692
[1mStep[0m  [48/84], [94mLoss[0m : 1.46551
[1mStep[0m  [56/84], [94mLoss[0m : 1.70884
[1mStep[0m  [64/84], [94mLoss[0m : 1.92430
[1mStep[0m  [72/84], [94mLoss[0m : 1.81337
[1mStep[0m  [80/84], [94mLoss[0m : 1.75627

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.699, [92mTest[0m: 2.504, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.62046
[1mStep[0m  [8/84], [94mLoss[0m : 1.55281
[1mStep[0m  [16/84], [94mLoss[0m : 1.46823
[1mStep[0m  [24/84], [94mLoss[0m : 2.04742
[1mStep[0m  [32/84], [94mLoss[0m : 1.88897
[1mStep[0m  [40/84], [94mLoss[0m : 1.75702
[1mStep[0m  [48/84], [94mLoss[0m : 1.62221
[1mStep[0m  [56/84], [94mLoss[0m : 1.58560
[1mStep[0m  [64/84], [94mLoss[0m : 1.86657
[1mStep[0m  [72/84], [94mLoss[0m : 1.64984
[1mStep[0m  [80/84], [94mLoss[0m : 1.72729

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.681, [92mTest[0m: 2.496, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.53824
[1mStep[0m  [8/84], [94mLoss[0m : 1.41992
[1mStep[0m  [16/84], [94mLoss[0m : 1.66557
[1mStep[0m  [24/84], [94mLoss[0m : 1.64685
[1mStep[0m  [32/84], [94mLoss[0m : 1.55771
[1mStep[0m  [40/84], [94mLoss[0m : 1.70113
[1mStep[0m  [48/84], [94mLoss[0m : 1.60979
[1mStep[0m  [56/84], [94mLoss[0m : 1.71157
[1mStep[0m  [64/84], [94mLoss[0m : 1.54572
[1mStep[0m  [72/84], [94mLoss[0m : 1.84045
[1mStep[0m  [80/84], [94mLoss[0m : 1.68669

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.624, [92mTest[0m: 2.506, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.45407
[1mStep[0m  [8/84], [94mLoss[0m : 1.49391
[1mStep[0m  [16/84], [94mLoss[0m : 1.49136
[1mStep[0m  [24/84], [94mLoss[0m : 1.64346
[1mStep[0m  [32/84], [94mLoss[0m : 1.78685
[1mStep[0m  [40/84], [94mLoss[0m : 1.75634
[1mStep[0m  [48/84], [94mLoss[0m : 1.50381
[1mStep[0m  [56/84], [94mLoss[0m : 1.51442
[1mStep[0m  [64/84], [94mLoss[0m : 1.54709
[1mStep[0m  [72/84], [94mLoss[0m : 1.76908
[1mStep[0m  [80/84], [94mLoss[0m : 1.54030

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.632, [92mTest[0m: 2.488, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.55466
[1mStep[0m  [8/84], [94mLoss[0m : 1.74454
[1mStep[0m  [16/84], [94mLoss[0m : 1.84770
[1mStep[0m  [24/84], [94mLoss[0m : 1.73593
[1mStep[0m  [32/84], [94mLoss[0m : 1.63386
[1mStep[0m  [40/84], [94mLoss[0m : 1.52683
[1mStep[0m  [48/84], [94mLoss[0m : 1.70211
[1mStep[0m  [56/84], [94mLoss[0m : 1.63938
[1mStep[0m  [64/84], [94mLoss[0m : 1.44738
[1mStep[0m  [72/84], [94mLoss[0m : 1.60720
[1mStep[0m  [80/84], [94mLoss[0m : 1.56392

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.575, [92mTest[0m: 2.631, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.51069
[1mStep[0m  [8/84], [94mLoss[0m : 1.51693
[1mStep[0m  [16/84], [94mLoss[0m : 1.40628
[1mStep[0m  [24/84], [94mLoss[0m : 1.45378
[1mStep[0m  [32/84], [94mLoss[0m : 1.49756
[1mStep[0m  [40/84], [94mLoss[0m : 1.85218
[1mStep[0m  [48/84], [94mLoss[0m : 1.56977
[1mStep[0m  [56/84], [94mLoss[0m : 1.65034
[1mStep[0m  [64/84], [94mLoss[0m : 1.54139
[1mStep[0m  [72/84], [94mLoss[0m : 1.68658
[1mStep[0m  [80/84], [94mLoss[0m : 1.53244

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.545, [92mTest[0m: 2.501, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.45429
[1mStep[0m  [8/84], [94mLoss[0m : 1.46865
[1mStep[0m  [16/84], [94mLoss[0m : 1.50804
[1mStep[0m  [24/84], [94mLoss[0m : 1.89217
[1mStep[0m  [32/84], [94mLoss[0m : 1.56654
[1mStep[0m  [40/84], [94mLoss[0m : 1.61827
[1mStep[0m  [48/84], [94mLoss[0m : 1.32164
[1mStep[0m  [56/84], [94mLoss[0m : 1.55735
[1mStep[0m  [64/84], [94mLoss[0m : 1.45835
[1mStep[0m  [72/84], [94mLoss[0m : 1.60968
[1mStep[0m  [80/84], [94mLoss[0m : 1.52523

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.531, [92mTest[0m: 2.502, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.39143
[1mStep[0m  [8/84], [94mLoss[0m : 1.29797
[1mStep[0m  [16/84], [94mLoss[0m : 1.54040
[1mStep[0m  [24/84], [94mLoss[0m : 1.45945
[1mStep[0m  [32/84], [94mLoss[0m : 1.41534
[1mStep[0m  [40/84], [94mLoss[0m : 1.47747
[1mStep[0m  [48/84], [94mLoss[0m : 1.46058
[1mStep[0m  [56/84], [94mLoss[0m : 1.71601
[1mStep[0m  [64/84], [94mLoss[0m : 1.42164
[1mStep[0m  [72/84], [94mLoss[0m : 1.61518
[1mStep[0m  [80/84], [94mLoss[0m : 1.49205

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.495, [92mTest[0m: 2.486, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.46583
[1mStep[0m  [8/84], [94mLoss[0m : 1.49886
[1mStep[0m  [16/84], [94mLoss[0m : 1.57481
[1mStep[0m  [24/84], [94mLoss[0m : 1.66674
[1mStep[0m  [32/84], [94mLoss[0m : 1.53430
[1mStep[0m  [40/84], [94mLoss[0m : 1.62731
[1mStep[0m  [48/84], [94mLoss[0m : 1.30653
[1mStep[0m  [56/84], [94mLoss[0m : 1.59581
[1mStep[0m  [64/84], [94mLoss[0m : 1.63593
[1mStep[0m  [72/84], [94mLoss[0m : 1.51472
[1mStep[0m  [80/84], [94mLoss[0m : 1.47063

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.485, [92mTest[0m: 2.522, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.30308
[1mStep[0m  [8/84], [94mLoss[0m : 1.44216
[1mStep[0m  [16/84], [94mLoss[0m : 1.15097
[1mStep[0m  [24/84], [94mLoss[0m : 1.66558
[1mStep[0m  [32/84], [94mLoss[0m : 1.40930
[1mStep[0m  [40/84], [94mLoss[0m : 1.45770
[1mStep[0m  [48/84], [94mLoss[0m : 1.70062
[1mStep[0m  [56/84], [94mLoss[0m : 1.35379
[1mStep[0m  [64/84], [94mLoss[0m : 1.62501
[1mStep[0m  [72/84], [94mLoss[0m : 1.42053
[1mStep[0m  [80/84], [94mLoss[0m : 1.48708

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.477, [92mTest[0m: 2.566, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.55671
[1mStep[0m  [8/84], [94mLoss[0m : 1.45626
[1mStep[0m  [16/84], [94mLoss[0m : 1.48975
[1mStep[0m  [24/84], [94mLoss[0m : 1.48985
[1mStep[0m  [32/84], [94mLoss[0m : 1.53778
[1mStep[0m  [40/84], [94mLoss[0m : 1.42984
[1mStep[0m  [48/84], [94mLoss[0m : 1.33909
[1mStep[0m  [56/84], [94mLoss[0m : 1.52632
[1mStep[0m  [64/84], [94mLoss[0m : 1.42821
[1mStep[0m  [72/84], [94mLoss[0m : 1.40082
[1mStep[0m  [80/84], [94mLoss[0m : 1.71800

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.446, [92mTest[0m: 2.524, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.33823
[1mStep[0m  [8/84], [94mLoss[0m : 1.43532
[1mStep[0m  [16/84], [94mLoss[0m : 1.40953
[1mStep[0m  [24/84], [94mLoss[0m : 1.46058
[1mStep[0m  [32/84], [94mLoss[0m : 1.54225
[1mStep[0m  [40/84], [94mLoss[0m : 1.50916
[1mStep[0m  [48/84], [94mLoss[0m : 1.76401
[1mStep[0m  [56/84], [94mLoss[0m : 1.42117
[1mStep[0m  [64/84], [94mLoss[0m : 1.51950
[1mStep[0m  [72/84], [94mLoss[0m : 1.61760
[1mStep[0m  [80/84], [94mLoss[0m : 1.18427

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.432, [92mTest[0m: 2.524, [96mlr[0m: 0.006772500000000002
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 27 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.529
====================================

Phase 2 - Evaluation MAE:  2.5285870092255727
MAE score P1       2.327446
MAE score P2       2.528587
loss                1.43232
learning_rate      0.007525
batch_size              128
hidden_sizes      [200, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping         True
dropout                 0.2
momentum                0.5
weight_decay         0.0001
Name: 9, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 10.49688
[1mStep[0m  [8/84], [94mLoss[0m : 10.77728
[1mStep[0m  [16/84], [94mLoss[0m : 10.72681
[1mStep[0m  [24/84], [94mLoss[0m : 10.89073
[1mStep[0m  [32/84], [94mLoss[0m : 11.14035
[1mStep[0m  [40/84], [94mLoss[0m : 10.17130
[1mStep[0m  [48/84], [94mLoss[0m : 10.58917
[1mStep[0m  [56/84], [94mLoss[0m : 9.99951
[1mStep[0m  [64/84], [94mLoss[0m : 10.63772
[1mStep[0m  [72/84], [94mLoss[0m : 9.67793
[1mStep[0m  [80/84], [94mLoss[0m : 9.99134

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.361, [92mTest[0m: 10.834, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.37140
[1mStep[0m  [8/84], [94mLoss[0m : 9.67155
[1mStep[0m  [16/84], [94mLoss[0m : 8.99606
[1mStep[0m  [24/84], [94mLoss[0m : 9.42942
[1mStep[0m  [32/84], [94mLoss[0m : 9.25056
[1mStep[0m  [40/84], [94mLoss[0m : 9.18692
[1mStep[0m  [48/84], [94mLoss[0m : 9.05825
[1mStep[0m  [56/84], [94mLoss[0m : 9.20055
[1mStep[0m  [64/84], [94mLoss[0m : 8.68604
[1mStep[0m  [72/84], [94mLoss[0m : 8.12147
[1mStep[0m  [80/84], [94mLoss[0m : 8.16723

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 8.943, [92mTest[0m: 9.277, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.89419
[1mStep[0m  [8/84], [94mLoss[0m : 8.09128
[1mStep[0m  [16/84], [94mLoss[0m : 7.63113
[1mStep[0m  [24/84], [94mLoss[0m : 7.23176
[1mStep[0m  [32/84], [94mLoss[0m : 7.49567
[1mStep[0m  [40/84], [94mLoss[0m : 7.83116
[1mStep[0m  [48/84], [94mLoss[0m : 7.34821
[1mStep[0m  [56/84], [94mLoss[0m : 7.03834
[1mStep[0m  [64/84], [94mLoss[0m : 6.86855
[1mStep[0m  [72/84], [94mLoss[0m : 6.67750
[1mStep[0m  [80/84], [94mLoss[0m : 6.21732

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 7.324, [92mTest[0m: 7.384, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.81622
[1mStep[0m  [8/84], [94mLoss[0m : 6.15637
[1mStep[0m  [16/84], [94mLoss[0m : 6.29540
[1mStep[0m  [24/84], [94mLoss[0m : 6.51480
[1mStep[0m  [32/84], [94mLoss[0m : 5.69877
[1mStep[0m  [40/84], [94mLoss[0m : 6.23369
[1mStep[0m  [48/84], [94mLoss[0m : 5.32581
[1mStep[0m  [56/84], [94mLoss[0m : 5.79956
[1mStep[0m  [64/84], [94mLoss[0m : 5.86779
[1mStep[0m  [72/84], [94mLoss[0m : 5.11168
[1mStep[0m  [80/84], [94mLoss[0m : 5.01122

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 5.938, [92mTest[0m: 5.524, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.10179
[1mStep[0m  [8/84], [94mLoss[0m : 4.75408
[1mStep[0m  [16/84], [94mLoss[0m : 4.73180
[1mStep[0m  [24/84], [94mLoss[0m : 4.55911
[1mStep[0m  [32/84], [94mLoss[0m : 4.77784
[1mStep[0m  [40/84], [94mLoss[0m : 4.76442
[1mStep[0m  [48/84], [94mLoss[0m : 4.06947
[1mStep[0m  [56/84], [94mLoss[0m : 4.33691
[1mStep[0m  [64/84], [94mLoss[0m : 3.90462
[1mStep[0m  [72/84], [94mLoss[0m : 3.22647
[1mStep[0m  [80/84], [94mLoss[0m : 3.53945

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 4.445, [92mTest[0m: 4.238, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.10965
[1mStep[0m  [8/84], [94mLoss[0m : 3.49907
[1mStep[0m  [16/84], [94mLoss[0m : 3.41737
[1mStep[0m  [24/84], [94mLoss[0m : 2.79713
[1mStep[0m  [32/84], [94mLoss[0m : 2.89025
[1mStep[0m  [40/84], [94mLoss[0m : 3.15668
[1mStep[0m  [48/84], [94mLoss[0m : 3.15752
[1mStep[0m  [56/84], [94mLoss[0m : 3.11827
[1mStep[0m  [64/84], [94mLoss[0m : 2.76621
[1mStep[0m  [72/84], [94mLoss[0m : 2.41308
[1mStep[0m  [80/84], [94mLoss[0m : 3.05088

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 3.084, [92mTest[0m: 2.814, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.58187
[1mStep[0m  [8/84], [94mLoss[0m : 2.49119
[1mStep[0m  [16/84], [94mLoss[0m : 2.68709
[1mStep[0m  [24/84], [94mLoss[0m : 2.83297
[1mStep[0m  [32/84], [94mLoss[0m : 2.80715
[1mStep[0m  [40/84], [94mLoss[0m : 2.94958
[1mStep[0m  [48/84], [94mLoss[0m : 2.97817
[1mStep[0m  [56/84], [94mLoss[0m : 2.64436
[1mStep[0m  [64/84], [94mLoss[0m : 2.50266
[1mStep[0m  [72/84], [94mLoss[0m : 2.66361
[1mStep[0m  [80/84], [94mLoss[0m : 2.76499

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.734, [92mTest[0m: 2.398, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.64163
[1mStep[0m  [8/84], [94mLoss[0m : 2.77112
[1mStep[0m  [16/84], [94mLoss[0m : 2.92121
[1mStep[0m  [24/84], [94mLoss[0m : 2.81304
[1mStep[0m  [32/84], [94mLoss[0m : 2.20406
[1mStep[0m  [40/84], [94mLoss[0m : 2.88829
[1mStep[0m  [48/84], [94mLoss[0m : 2.36613
[1mStep[0m  [56/84], [94mLoss[0m : 2.56116
[1mStep[0m  [64/84], [94mLoss[0m : 2.44737
[1mStep[0m  [72/84], [94mLoss[0m : 2.64981
[1mStep[0m  [80/84], [94mLoss[0m : 2.42009

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.658, [92mTest[0m: 2.416, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59499
[1mStep[0m  [8/84], [94mLoss[0m : 2.87898
[1mStep[0m  [16/84], [94mLoss[0m : 2.68279
[1mStep[0m  [24/84], [94mLoss[0m : 2.53936
[1mStep[0m  [32/84], [94mLoss[0m : 2.41539
[1mStep[0m  [40/84], [94mLoss[0m : 2.75141
[1mStep[0m  [48/84], [94mLoss[0m : 2.95146
[1mStep[0m  [56/84], [94mLoss[0m : 2.76821
[1mStep[0m  [64/84], [94mLoss[0m : 2.47548
[1mStep[0m  [72/84], [94mLoss[0m : 2.45143
[1mStep[0m  [80/84], [94mLoss[0m : 2.50751

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.637, [92mTest[0m: 2.392, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.63064
[1mStep[0m  [8/84], [94mLoss[0m : 2.54082
[1mStep[0m  [16/84], [94mLoss[0m : 2.47763
[1mStep[0m  [24/84], [94mLoss[0m : 2.67251
[1mStep[0m  [32/84], [94mLoss[0m : 2.87486
[1mStep[0m  [40/84], [94mLoss[0m : 2.45839
[1mStep[0m  [48/84], [94mLoss[0m : 2.46247
[1mStep[0m  [56/84], [94mLoss[0m : 2.96300
[1mStep[0m  [64/84], [94mLoss[0m : 2.82481
[1mStep[0m  [72/84], [94mLoss[0m : 2.42001
[1mStep[0m  [80/84], [94mLoss[0m : 2.33407

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.617, [92mTest[0m: 2.369, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.68011
[1mStep[0m  [8/84], [94mLoss[0m : 2.55723
[1mStep[0m  [16/84], [94mLoss[0m : 2.48365
[1mStep[0m  [24/84], [94mLoss[0m : 2.62236
[1mStep[0m  [32/84], [94mLoss[0m : 2.58181
[1mStep[0m  [40/84], [94mLoss[0m : 2.46339
[1mStep[0m  [48/84], [94mLoss[0m : 2.61227
[1mStep[0m  [56/84], [94mLoss[0m : 2.60203
[1mStep[0m  [64/84], [94mLoss[0m : 2.34056
[1mStep[0m  [72/84], [94mLoss[0m : 2.59685
[1mStep[0m  [80/84], [94mLoss[0m : 2.55772

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.604, [92mTest[0m: 2.377, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46194
[1mStep[0m  [8/84], [94mLoss[0m : 2.28772
[1mStep[0m  [16/84], [94mLoss[0m : 2.88948
[1mStep[0m  [24/84], [94mLoss[0m : 2.42005
[1mStep[0m  [32/84], [94mLoss[0m : 2.30602
[1mStep[0m  [40/84], [94mLoss[0m : 2.64026
[1mStep[0m  [48/84], [94mLoss[0m : 2.63202
[1mStep[0m  [56/84], [94mLoss[0m : 2.81231
[1mStep[0m  [64/84], [94mLoss[0m : 2.50506
[1mStep[0m  [72/84], [94mLoss[0m : 2.44149
[1mStep[0m  [80/84], [94mLoss[0m : 2.32051

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.583, [92mTest[0m: 2.368, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.57280
[1mStep[0m  [8/84], [94mLoss[0m : 2.55594
[1mStep[0m  [16/84], [94mLoss[0m : 2.69691
[1mStep[0m  [24/84], [94mLoss[0m : 2.69334
[1mStep[0m  [32/84], [94mLoss[0m : 3.08029
[1mStep[0m  [40/84], [94mLoss[0m : 2.95065
[1mStep[0m  [48/84], [94mLoss[0m : 2.55420
[1mStep[0m  [56/84], [94mLoss[0m : 2.68178
[1mStep[0m  [64/84], [94mLoss[0m : 2.52595
[1mStep[0m  [72/84], [94mLoss[0m : 2.47752
[1mStep[0m  [80/84], [94mLoss[0m : 2.53368

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.582, [92mTest[0m: 2.391, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30395
[1mStep[0m  [8/84], [94mLoss[0m : 2.75031
[1mStep[0m  [16/84], [94mLoss[0m : 2.21110
[1mStep[0m  [24/84], [94mLoss[0m : 2.43692
[1mStep[0m  [32/84], [94mLoss[0m : 2.80436
[1mStep[0m  [40/84], [94mLoss[0m : 2.81259
[1mStep[0m  [48/84], [94mLoss[0m : 2.59560
[1mStep[0m  [56/84], [94mLoss[0m : 2.84697
[1mStep[0m  [64/84], [94mLoss[0m : 2.09198
[1mStep[0m  [72/84], [94mLoss[0m : 2.80128
[1mStep[0m  [80/84], [94mLoss[0m : 2.51604

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.555, [92mTest[0m: 2.381, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30351
[1mStep[0m  [8/84], [94mLoss[0m : 2.72349
[1mStep[0m  [16/84], [94mLoss[0m : 2.55085
[1mStep[0m  [24/84], [94mLoss[0m : 2.18019
[1mStep[0m  [32/84], [94mLoss[0m : 2.65196
[1mStep[0m  [40/84], [94mLoss[0m : 2.51523
[1mStep[0m  [48/84], [94mLoss[0m : 2.84790
[1mStep[0m  [56/84], [94mLoss[0m : 2.47784
[1mStep[0m  [64/84], [94mLoss[0m : 2.79783
[1mStep[0m  [72/84], [94mLoss[0m : 2.50555
[1mStep[0m  [80/84], [94mLoss[0m : 2.75742

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.573, [92mTest[0m: 2.413, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46642
[1mStep[0m  [8/84], [94mLoss[0m : 2.93590
[1mStep[0m  [16/84], [94mLoss[0m : 2.67162
[1mStep[0m  [24/84], [94mLoss[0m : 2.33621
[1mStep[0m  [32/84], [94mLoss[0m : 2.49642
[1mStep[0m  [40/84], [94mLoss[0m : 2.66314
[1mStep[0m  [48/84], [94mLoss[0m : 2.43764
[1mStep[0m  [56/84], [94mLoss[0m : 2.37710
[1mStep[0m  [64/84], [94mLoss[0m : 2.29932
[1mStep[0m  [72/84], [94mLoss[0m : 2.60159
[1mStep[0m  [80/84], [94mLoss[0m : 2.96738

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.568, [92mTest[0m: 2.371, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.04966
[1mStep[0m  [8/84], [94mLoss[0m : 2.78006
[1mStep[0m  [16/84], [94mLoss[0m : 2.70977
[1mStep[0m  [24/84], [94mLoss[0m : 2.48039
[1mStep[0m  [32/84], [94mLoss[0m : 2.62644
[1mStep[0m  [40/84], [94mLoss[0m : 2.56785
[1mStep[0m  [48/84], [94mLoss[0m : 2.50118
[1mStep[0m  [56/84], [94mLoss[0m : 2.43975
[1mStep[0m  [64/84], [94mLoss[0m : 2.37133
[1mStep[0m  [72/84], [94mLoss[0m : 2.43726
[1mStep[0m  [80/84], [94mLoss[0m : 2.49999

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.529, [92mTest[0m: 2.339, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27031
[1mStep[0m  [8/84], [94mLoss[0m : 2.70678
[1mStep[0m  [16/84], [94mLoss[0m : 2.33325
[1mStep[0m  [24/84], [94mLoss[0m : 2.71661
[1mStep[0m  [32/84], [94mLoss[0m : 2.80193
[1mStep[0m  [40/84], [94mLoss[0m : 2.62410
[1mStep[0m  [48/84], [94mLoss[0m : 2.58581
[1mStep[0m  [56/84], [94mLoss[0m : 2.14327
[1mStep[0m  [64/84], [94mLoss[0m : 2.41564
[1mStep[0m  [72/84], [94mLoss[0m : 2.48596
[1mStep[0m  [80/84], [94mLoss[0m : 2.51151

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.537, [92mTest[0m: 2.371, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.65645
[1mStep[0m  [8/84], [94mLoss[0m : 2.75179
[1mStep[0m  [16/84], [94mLoss[0m : 2.50238
[1mStep[0m  [24/84], [94mLoss[0m : 2.36230
[1mStep[0m  [32/84], [94mLoss[0m : 2.03472
[1mStep[0m  [40/84], [94mLoss[0m : 2.57004
[1mStep[0m  [48/84], [94mLoss[0m : 2.41977
[1mStep[0m  [56/84], [94mLoss[0m : 2.56870
[1mStep[0m  [64/84], [94mLoss[0m : 2.85338
[1mStep[0m  [72/84], [94mLoss[0m : 2.32897
[1mStep[0m  [80/84], [94mLoss[0m : 2.69431

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.341, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48510
[1mStep[0m  [8/84], [94mLoss[0m : 2.56086
[1mStep[0m  [16/84], [94mLoss[0m : 2.59997
[1mStep[0m  [24/84], [94mLoss[0m : 2.35936
[1mStep[0m  [32/84], [94mLoss[0m : 2.25936
[1mStep[0m  [40/84], [94mLoss[0m : 2.33194
[1mStep[0m  [48/84], [94mLoss[0m : 2.33640
[1mStep[0m  [56/84], [94mLoss[0m : 2.26895
[1mStep[0m  [64/84], [94mLoss[0m : 2.58207
[1mStep[0m  [72/84], [94mLoss[0m : 2.29856
[1mStep[0m  [80/84], [94mLoss[0m : 2.53114

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.361, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37389
[1mStep[0m  [8/84], [94mLoss[0m : 2.61127
[1mStep[0m  [16/84], [94mLoss[0m : 2.87584
[1mStep[0m  [24/84], [94mLoss[0m : 2.41890
[1mStep[0m  [32/84], [94mLoss[0m : 2.37427
[1mStep[0m  [40/84], [94mLoss[0m : 2.82778
[1mStep[0m  [48/84], [94mLoss[0m : 2.09576
[1mStep[0m  [56/84], [94mLoss[0m : 2.46305
[1mStep[0m  [64/84], [94mLoss[0m : 2.55746
[1mStep[0m  [72/84], [94mLoss[0m : 2.62646
[1mStep[0m  [80/84], [94mLoss[0m : 2.71807

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.337, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36360
[1mStep[0m  [8/84], [94mLoss[0m : 2.18316
[1mStep[0m  [16/84], [94mLoss[0m : 2.50957
[1mStep[0m  [24/84], [94mLoss[0m : 2.47477
[1mStep[0m  [32/84], [94mLoss[0m : 2.37862
[1mStep[0m  [40/84], [94mLoss[0m : 2.41895
[1mStep[0m  [48/84], [94mLoss[0m : 2.29643
[1mStep[0m  [56/84], [94mLoss[0m : 2.59339
[1mStep[0m  [64/84], [94mLoss[0m : 2.64870
[1mStep[0m  [72/84], [94mLoss[0m : 2.25158
[1mStep[0m  [80/84], [94mLoss[0m : 2.84738

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.348, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54132
[1mStep[0m  [8/84], [94mLoss[0m : 2.44995
[1mStep[0m  [16/84], [94mLoss[0m : 2.41346
[1mStep[0m  [24/84], [94mLoss[0m : 2.34959
[1mStep[0m  [32/84], [94mLoss[0m : 2.28673
[1mStep[0m  [40/84], [94mLoss[0m : 2.39277
[1mStep[0m  [48/84], [94mLoss[0m : 2.34437
[1mStep[0m  [56/84], [94mLoss[0m : 2.78981
[1mStep[0m  [64/84], [94mLoss[0m : 2.67845
[1mStep[0m  [72/84], [94mLoss[0m : 2.26979
[1mStep[0m  [80/84], [94mLoss[0m : 2.37560

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.358, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.63183
[1mStep[0m  [8/84], [94mLoss[0m : 2.49830
[1mStep[0m  [16/84], [94mLoss[0m : 2.17802
[1mStep[0m  [24/84], [94mLoss[0m : 2.55800
[1mStep[0m  [32/84], [94mLoss[0m : 2.12856
[1mStep[0m  [40/84], [94mLoss[0m : 2.23798
[1mStep[0m  [48/84], [94mLoss[0m : 2.35586
[1mStep[0m  [56/84], [94mLoss[0m : 2.64485
[1mStep[0m  [64/84], [94mLoss[0m : 2.43538
[1mStep[0m  [72/84], [94mLoss[0m : 2.57547
[1mStep[0m  [80/84], [94mLoss[0m : 2.60754

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.336, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.25234
[1mStep[0m  [8/84], [94mLoss[0m : 2.66067
[1mStep[0m  [16/84], [94mLoss[0m : 2.13465
[1mStep[0m  [24/84], [94mLoss[0m : 2.52621
[1mStep[0m  [32/84], [94mLoss[0m : 2.22719
[1mStep[0m  [40/84], [94mLoss[0m : 2.47131
[1mStep[0m  [48/84], [94mLoss[0m : 2.44052
[1mStep[0m  [56/84], [94mLoss[0m : 2.25020
[1mStep[0m  [64/84], [94mLoss[0m : 2.48412
[1mStep[0m  [72/84], [94mLoss[0m : 2.38060
[1mStep[0m  [80/84], [94mLoss[0m : 2.86328

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.345, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.25476
[1mStep[0m  [8/84], [94mLoss[0m : 2.43199
[1mStep[0m  [16/84], [94mLoss[0m : 2.48599
[1mStep[0m  [24/84], [94mLoss[0m : 2.91962
[1mStep[0m  [32/84], [94mLoss[0m : 2.60342
[1mStep[0m  [40/84], [94mLoss[0m : 2.73284
[1mStep[0m  [48/84], [94mLoss[0m : 2.38931
[1mStep[0m  [56/84], [94mLoss[0m : 2.46555
[1mStep[0m  [64/84], [94mLoss[0m : 2.36325
[1mStep[0m  [72/84], [94mLoss[0m : 2.17993
[1mStep[0m  [80/84], [94mLoss[0m : 2.36382

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.356, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45412
[1mStep[0m  [8/84], [94mLoss[0m : 2.50247
[1mStep[0m  [16/84], [94mLoss[0m : 2.38310
[1mStep[0m  [24/84], [94mLoss[0m : 2.37576
[1mStep[0m  [32/84], [94mLoss[0m : 2.32099
[1mStep[0m  [40/84], [94mLoss[0m : 2.91087
[1mStep[0m  [48/84], [94mLoss[0m : 2.46036
[1mStep[0m  [56/84], [94mLoss[0m : 2.50476
[1mStep[0m  [64/84], [94mLoss[0m : 2.42825
[1mStep[0m  [72/84], [94mLoss[0m : 2.36375
[1mStep[0m  [80/84], [94mLoss[0m : 2.60107

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.358, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35120
[1mStep[0m  [8/84], [94mLoss[0m : 2.32796
[1mStep[0m  [16/84], [94mLoss[0m : 2.46962
[1mStep[0m  [24/84], [94mLoss[0m : 2.79859
[1mStep[0m  [32/84], [94mLoss[0m : 2.41283
[1mStep[0m  [40/84], [94mLoss[0m : 2.36994
[1mStep[0m  [48/84], [94mLoss[0m : 2.58406
[1mStep[0m  [56/84], [94mLoss[0m : 2.46368
[1mStep[0m  [64/84], [94mLoss[0m : 2.47483
[1mStep[0m  [72/84], [94mLoss[0m : 2.40624
[1mStep[0m  [80/84], [94mLoss[0m : 2.42803

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.335, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.22619
[1mStep[0m  [8/84], [94mLoss[0m : 2.31201
[1mStep[0m  [16/84], [94mLoss[0m : 2.54705
[1mStep[0m  [24/84], [94mLoss[0m : 2.39013
[1mStep[0m  [32/84], [94mLoss[0m : 2.75480
[1mStep[0m  [40/84], [94mLoss[0m : 2.22558
[1mStep[0m  [48/84], [94mLoss[0m : 2.28404
[1mStep[0m  [56/84], [94mLoss[0m : 2.49368
[1mStep[0m  [64/84], [94mLoss[0m : 2.61689
[1mStep[0m  [72/84], [94mLoss[0m : 2.42432
[1mStep[0m  [80/84], [94mLoss[0m : 2.73887

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.356, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54812
[1mStep[0m  [8/84], [94mLoss[0m : 2.66372
[1mStep[0m  [16/84], [94mLoss[0m : 2.69396
[1mStep[0m  [24/84], [94mLoss[0m : 2.09955
[1mStep[0m  [32/84], [94mLoss[0m : 2.67243
[1mStep[0m  [40/84], [94mLoss[0m : 2.11626
[1mStep[0m  [48/84], [94mLoss[0m : 2.30120
[1mStep[0m  [56/84], [94mLoss[0m : 2.27996
[1mStep[0m  [64/84], [94mLoss[0m : 2.35969
[1mStep[0m  [72/84], [94mLoss[0m : 2.46080
[1mStep[0m  [80/84], [94mLoss[0m : 2.48086

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.334, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.340
====================================

Phase 1 - Evaluation MAE:  2.3403914655957903
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 2.51725
[1mStep[0m  [8/84], [94mLoss[0m : 2.64041
[1mStep[0m  [16/84], [94mLoss[0m : 2.70977
[1mStep[0m  [24/84], [94mLoss[0m : 2.27416
[1mStep[0m  [32/84], [94mLoss[0m : 2.41090
[1mStep[0m  [40/84], [94mLoss[0m : 2.72672
[1mStep[0m  [48/84], [94mLoss[0m : 2.51333
[1mStep[0m  [56/84], [94mLoss[0m : 2.73214
[1mStep[0m  [64/84], [94mLoss[0m : 2.42995
[1mStep[0m  [72/84], [94mLoss[0m : 2.49080
[1mStep[0m  [80/84], [94mLoss[0m : 2.71536

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.336, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27584
[1mStep[0m  [8/84], [94mLoss[0m : 2.47813
[1mStep[0m  [16/84], [94mLoss[0m : 2.27699
[1mStep[0m  [24/84], [94mLoss[0m : 2.37081
[1mStep[0m  [32/84], [94mLoss[0m : 2.33878
[1mStep[0m  [40/84], [94mLoss[0m : 2.17959
[1mStep[0m  [48/84], [94mLoss[0m : 2.44360
[1mStep[0m  [56/84], [94mLoss[0m : 2.36795
[1mStep[0m  [64/84], [94mLoss[0m : 2.82541
[1mStep[0m  [72/84], [94mLoss[0m : 2.44841
[1mStep[0m  [80/84], [94mLoss[0m : 2.83566

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.378, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.08702
[1mStep[0m  [8/84], [94mLoss[0m : 2.26625
[1mStep[0m  [16/84], [94mLoss[0m : 2.70028
[1mStep[0m  [24/84], [94mLoss[0m : 2.11208
[1mStep[0m  [32/84], [94mLoss[0m : 2.52035
[1mStep[0m  [40/84], [94mLoss[0m : 2.54285
[1mStep[0m  [48/84], [94mLoss[0m : 2.50142
[1mStep[0m  [56/84], [94mLoss[0m : 2.37359
[1mStep[0m  [64/84], [94mLoss[0m : 2.46074
[1mStep[0m  [72/84], [94mLoss[0m : 2.37214
[1mStep[0m  [80/84], [94mLoss[0m : 2.40088

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.403, [92mTest[0m: 2.402, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48089
[1mStep[0m  [8/84], [94mLoss[0m : 2.29037
[1mStep[0m  [16/84], [94mLoss[0m : 2.16578
[1mStep[0m  [24/84], [94mLoss[0m : 2.25914
[1mStep[0m  [32/84], [94mLoss[0m : 2.58060
[1mStep[0m  [40/84], [94mLoss[0m : 2.48707
[1mStep[0m  [48/84], [94mLoss[0m : 2.46538
[1mStep[0m  [56/84], [94mLoss[0m : 2.10955
[1mStep[0m  [64/84], [94mLoss[0m : 2.51244
[1mStep[0m  [72/84], [94mLoss[0m : 2.36715
[1mStep[0m  [80/84], [94mLoss[0m : 2.31769

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.369, [92mTest[0m: 2.438, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37915
[1mStep[0m  [8/84], [94mLoss[0m : 2.33444
[1mStep[0m  [16/84], [94mLoss[0m : 2.64924
[1mStep[0m  [24/84], [94mLoss[0m : 2.42411
[1mStep[0m  [32/84], [94mLoss[0m : 2.25742
[1mStep[0m  [40/84], [94mLoss[0m : 2.18871
[1mStep[0m  [48/84], [94mLoss[0m : 2.01475
[1mStep[0m  [56/84], [94mLoss[0m : 2.54385
[1mStep[0m  [64/84], [94mLoss[0m : 2.44150
[1mStep[0m  [72/84], [94mLoss[0m : 2.27699
[1mStep[0m  [80/84], [94mLoss[0m : 2.33171

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.322, [92mTest[0m: 2.411, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.95907
[1mStep[0m  [8/84], [94mLoss[0m : 2.39355
[1mStep[0m  [16/84], [94mLoss[0m : 2.22599
[1mStep[0m  [24/84], [94mLoss[0m : 2.41959
[1mStep[0m  [32/84], [94mLoss[0m : 2.40418
[1mStep[0m  [40/84], [94mLoss[0m : 2.27616
[1mStep[0m  [48/84], [94mLoss[0m : 2.29818
[1mStep[0m  [56/84], [94mLoss[0m : 2.24003
[1mStep[0m  [64/84], [94mLoss[0m : 2.10115
[1mStep[0m  [72/84], [94mLoss[0m : 2.41365
[1mStep[0m  [80/84], [94mLoss[0m : 2.12815

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.277, [92mTest[0m: 2.508, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.86215
[1mStep[0m  [8/84], [94mLoss[0m : 2.23144
[1mStep[0m  [16/84], [94mLoss[0m : 2.10581
[1mStep[0m  [24/84], [94mLoss[0m : 2.09190
[1mStep[0m  [32/84], [94mLoss[0m : 2.29026
[1mStep[0m  [40/84], [94mLoss[0m : 2.14640
[1mStep[0m  [48/84], [94mLoss[0m : 2.23610
[1mStep[0m  [56/84], [94mLoss[0m : 2.32783
[1mStep[0m  [64/84], [94mLoss[0m : 2.24000
[1mStep[0m  [72/84], [94mLoss[0m : 2.02307
[1mStep[0m  [80/84], [94mLoss[0m : 2.00684

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.200, [92mTest[0m: 2.428, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.18271
[1mStep[0m  [8/84], [94mLoss[0m : 2.11493
[1mStep[0m  [16/84], [94mLoss[0m : 1.94608
[1mStep[0m  [24/84], [94mLoss[0m : 2.17993
[1mStep[0m  [32/84], [94mLoss[0m : 1.92724
[1mStep[0m  [40/84], [94mLoss[0m : 1.93980
[1mStep[0m  [48/84], [94mLoss[0m : 2.09026
[1mStep[0m  [56/84], [94mLoss[0m : 2.29454
[1mStep[0m  [64/84], [94mLoss[0m : 2.23924
[1mStep[0m  [72/84], [94mLoss[0m : 2.23915
[1mStep[0m  [80/84], [94mLoss[0m : 2.28682

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.171, [92mTest[0m: 2.446, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.12678
[1mStep[0m  [8/84], [94mLoss[0m : 1.93411
[1mStep[0m  [16/84], [94mLoss[0m : 1.96071
[1mStep[0m  [24/84], [94mLoss[0m : 2.00825
[1mStep[0m  [32/84], [94mLoss[0m : 2.02089
[1mStep[0m  [40/84], [94mLoss[0m : 2.13382
[1mStep[0m  [48/84], [94mLoss[0m : 2.06675
[1mStep[0m  [56/84], [94mLoss[0m : 2.20335
[1mStep[0m  [64/84], [94mLoss[0m : 2.06681
[1mStep[0m  [72/84], [94mLoss[0m : 2.17279
[1mStep[0m  [80/84], [94mLoss[0m : 1.89259

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.104, [92mTest[0m: 2.539, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.16669
[1mStep[0m  [8/84], [94mLoss[0m : 1.67792
[1mStep[0m  [16/84], [94mLoss[0m : 1.87967
[1mStep[0m  [24/84], [94mLoss[0m : 1.74011
[1mStep[0m  [32/84], [94mLoss[0m : 2.15901
[1mStep[0m  [40/84], [94mLoss[0m : 2.09645
[1mStep[0m  [48/84], [94mLoss[0m : 2.67602
[1mStep[0m  [56/84], [94mLoss[0m : 2.06592
[1mStep[0m  [64/84], [94mLoss[0m : 1.94136
[1mStep[0m  [72/84], [94mLoss[0m : 1.87052
[1mStep[0m  [80/84], [94mLoss[0m : 2.15416

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.069, [92mTest[0m: 2.433, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.62218
[1mStep[0m  [8/84], [94mLoss[0m : 1.73273
[1mStep[0m  [16/84], [94mLoss[0m : 2.04261
[1mStep[0m  [24/84], [94mLoss[0m : 2.12262
[1mStep[0m  [32/84], [94mLoss[0m : 1.87311
[1mStep[0m  [40/84], [94mLoss[0m : 1.95708
[1mStep[0m  [48/84], [94mLoss[0m : 1.85975
[1mStep[0m  [56/84], [94mLoss[0m : 2.09811
[1mStep[0m  [64/84], [94mLoss[0m : 1.76438
[1mStep[0m  [72/84], [94mLoss[0m : 2.00019
[1mStep[0m  [80/84], [94mLoss[0m : 2.14699

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.021, [92mTest[0m: 2.483, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.98825
[1mStep[0m  [8/84], [94mLoss[0m : 2.02604
[1mStep[0m  [16/84], [94mLoss[0m : 1.54363
[1mStep[0m  [24/84], [94mLoss[0m : 2.11704
[1mStep[0m  [32/84], [94mLoss[0m : 1.80549
[1mStep[0m  [40/84], [94mLoss[0m : 2.03842
[1mStep[0m  [48/84], [94mLoss[0m : 2.07654
[1mStep[0m  [56/84], [94mLoss[0m : 2.29279
[1mStep[0m  [64/84], [94mLoss[0m : 2.02069
[1mStep[0m  [72/84], [94mLoss[0m : 2.28278
[1mStep[0m  [80/84], [94mLoss[0m : 1.91750

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.999, [92mTest[0m: 2.533, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.97852
[1mStep[0m  [8/84], [94mLoss[0m : 1.89153
[1mStep[0m  [16/84], [94mLoss[0m : 2.09790
[1mStep[0m  [24/84], [94mLoss[0m : 2.11241
[1mStep[0m  [32/84], [94mLoss[0m : 2.16984
[1mStep[0m  [40/84], [94mLoss[0m : 2.00585
[1mStep[0m  [48/84], [94mLoss[0m : 2.23923
[1mStep[0m  [56/84], [94mLoss[0m : 1.87195
[1mStep[0m  [64/84], [94mLoss[0m : 2.02400
[1mStep[0m  [72/84], [94mLoss[0m : 1.99792
[1mStep[0m  [80/84], [94mLoss[0m : 1.71085

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.948, [92mTest[0m: 2.436, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.99759
[1mStep[0m  [8/84], [94mLoss[0m : 1.80170
[1mStep[0m  [16/84], [94mLoss[0m : 1.91032
[1mStep[0m  [24/84], [94mLoss[0m : 1.56359
[1mStep[0m  [32/84], [94mLoss[0m : 1.72226
[1mStep[0m  [40/84], [94mLoss[0m : 1.91783
[1mStep[0m  [48/84], [94mLoss[0m : 1.99301
[1mStep[0m  [56/84], [94mLoss[0m : 1.71625
[1mStep[0m  [64/84], [94mLoss[0m : 1.94258
[1mStep[0m  [72/84], [94mLoss[0m : 2.05897
[1mStep[0m  [80/84], [94mLoss[0m : 1.77552

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.916, [92mTest[0m: 2.591, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.84768
[1mStep[0m  [8/84], [94mLoss[0m : 1.91304
[1mStep[0m  [16/84], [94mLoss[0m : 1.47142
[1mStep[0m  [24/84], [94mLoss[0m : 1.86341
[1mStep[0m  [32/84], [94mLoss[0m : 1.92112
[1mStep[0m  [40/84], [94mLoss[0m : 1.95671
[1mStep[0m  [48/84], [94mLoss[0m : 1.62985
[1mStep[0m  [56/84], [94mLoss[0m : 1.91085
[1mStep[0m  [64/84], [94mLoss[0m : 1.82498
[1mStep[0m  [72/84], [94mLoss[0m : 1.99694
[1mStep[0m  [80/84], [94mLoss[0m : 1.97030

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.856, [92mTest[0m: 2.447, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.67682
[1mStep[0m  [8/84], [94mLoss[0m : 1.96635
[1mStep[0m  [16/84], [94mLoss[0m : 1.70489
[1mStep[0m  [24/84], [94mLoss[0m : 1.88118
[1mStep[0m  [32/84], [94mLoss[0m : 1.95524
[1mStep[0m  [40/84], [94mLoss[0m : 1.91485
[1mStep[0m  [48/84], [94mLoss[0m : 1.75169
[1mStep[0m  [56/84], [94mLoss[0m : 1.82111
[1mStep[0m  [64/84], [94mLoss[0m : 2.03733
[1mStep[0m  [72/84], [94mLoss[0m : 2.13489
[1mStep[0m  [80/84], [94mLoss[0m : 1.73778

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.840, [92mTest[0m: 2.429, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.53904
[1mStep[0m  [8/84], [94mLoss[0m : 1.63370
[1mStep[0m  [16/84], [94mLoss[0m : 1.63562
[1mStep[0m  [24/84], [94mLoss[0m : 2.00347
[1mStep[0m  [32/84], [94mLoss[0m : 1.84493
[1mStep[0m  [40/84], [94mLoss[0m : 1.57077
[1mStep[0m  [48/84], [94mLoss[0m : 1.81277
[1mStep[0m  [56/84], [94mLoss[0m : 1.55178
[1mStep[0m  [64/84], [94mLoss[0m : 1.86603
[1mStep[0m  [72/84], [94mLoss[0m : 2.00007
[1mStep[0m  [80/84], [94mLoss[0m : 1.73764

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.809, [92mTest[0m: 2.424, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.93319
[1mStep[0m  [8/84], [94mLoss[0m : 1.67236
[1mStep[0m  [16/84], [94mLoss[0m : 2.21827
[1mStep[0m  [24/84], [94mLoss[0m : 1.84690
[1mStep[0m  [32/84], [94mLoss[0m : 1.55358
[1mStep[0m  [40/84], [94mLoss[0m : 1.68827
[1mStep[0m  [48/84], [94mLoss[0m : 1.76996
[1mStep[0m  [56/84], [94mLoss[0m : 1.71381
[1mStep[0m  [64/84], [94mLoss[0m : 1.49849
[1mStep[0m  [72/84], [94mLoss[0m : 1.76638
[1mStep[0m  [80/84], [94mLoss[0m : 1.77514

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.768, [92mTest[0m: 2.503, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.77410
[1mStep[0m  [8/84], [94mLoss[0m : 1.68489
[1mStep[0m  [16/84], [94mLoss[0m : 1.76037
[1mStep[0m  [24/84], [94mLoss[0m : 1.63893
[1mStep[0m  [32/84], [94mLoss[0m : 1.97397
[1mStep[0m  [40/84], [94mLoss[0m : 1.58251
[1mStep[0m  [48/84], [94mLoss[0m : 1.60693
[1mStep[0m  [56/84], [94mLoss[0m : 1.67009
[1mStep[0m  [64/84], [94mLoss[0m : 1.67455
[1mStep[0m  [72/84], [94mLoss[0m : 1.54369
[1mStep[0m  [80/84], [94mLoss[0m : 1.87912

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.733, [92mTest[0m: 2.489, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.57589
[1mStep[0m  [8/84], [94mLoss[0m : 1.56385
[1mStep[0m  [16/84], [94mLoss[0m : 1.78369
[1mStep[0m  [24/84], [94mLoss[0m : 1.53393
[1mStep[0m  [32/84], [94mLoss[0m : 1.81072
[1mStep[0m  [40/84], [94mLoss[0m : 1.75544
[1mStep[0m  [48/84], [94mLoss[0m : 1.78518
[1mStep[0m  [56/84], [94mLoss[0m : 1.73295
[1mStep[0m  [64/84], [94mLoss[0m : 1.53960
[1mStep[0m  [72/84], [94mLoss[0m : 1.92311
[1mStep[0m  [80/84], [94mLoss[0m : 1.59234

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.724, [92mTest[0m: 2.424, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.77011
[1mStep[0m  [8/84], [94mLoss[0m : 1.50410
[1mStep[0m  [16/84], [94mLoss[0m : 1.70174
[1mStep[0m  [24/84], [94mLoss[0m : 1.55905
[1mStep[0m  [32/84], [94mLoss[0m : 1.70436
[1mStep[0m  [40/84], [94mLoss[0m : 1.60517
[1mStep[0m  [48/84], [94mLoss[0m : 1.56564
[1mStep[0m  [56/84], [94mLoss[0m : 1.72836
[1mStep[0m  [64/84], [94mLoss[0m : 1.28940
[1mStep[0m  [72/84], [94mLoss[0m : 1.62245
[1mStep[0m  [80/84], [94mLoss[0m : 1.89342

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.663, [92mTest[0m: 2.512, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.70193
[1mStep[0m  [8/84], [94mLoss[0m : 1.46949
[1mStep[0m  [16/84], [94mLoss[0m : 1.81969
[1mStep[0m  [24/84], [94mLoss[0m : 1.76749
[1mStep[0m  [32/84], [94mLoss[0m : 1.68670
[1mStep[0m  [40/84], [94mLoss[0m : 1.62144
[1mStep[0m  [48/84], [94mLoss[0m : 1.77981
[1mStep[0m  [56/84], [94mLoss[0m : 1.89065
[1mStep[0m  [64/84], [94mLoss[0m : 1.47651
[1mStep[0m  [72/84], [94mLoss[0m : 1.55536
[1mStep[0m  [80/84], [94mLoss[0m : 1.43578

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.632, [92mTest[0m: 2.462, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.47681
[1mStep[0m  [8/84], [94mLoss[0m : 1.52567
[1mStep[0m  [16/84], [94mLoss[0m : 1.43783
[1mStep[0m  [24/84], [94mLoss[0m : 1.48788
[1mStep[0m  [32/84], [94mLoss[0m : 1.46621
[1mStep[0m  [40/84], [94mLoss[0m : 1.49700
[1mStep[0m  [48/84], [94mLoss[0m : 1.85555
[1mStep[0m  [56/84], [94mLoss[0m : 1.74763
[1mStep[0m  [64/84], [94mLoss[0m : 1.56299
[1mStep[0m  [72/84], [94mLoss[0m : 1.77601
[1mStep[0m  [80/84], [94mLoss[0m : 1.56458

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.606, [92mTest[0m: 2.448, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.45539
[1mStep[0m  [8/84], [94mLoss[0m : 1.56845
[1mStep[0m  [16/84], [94mLoss[0m : 1.65582
[1mStep[0m  [24/84], [94mLoss[0m : 1.69433
[1mStep[0m  [32/84], [94mLoss[0m : 1.49552
[1mStep[0m  [40/84], [94mLoss[0m : 1.67738
[1mStep[0m  [48/84], [94mLoss[0m : 1.75676
[1mStep[0m  [56/84], [94mLoss[0m : 1.40764
[1mStep[0m  [64/84], [94mLoss[0m : 1.36638
[1mStep[0m  [72/84], [94mLoss[0m : 1.71753
[1mStep[0m  [80/84], [94mLoss[0m : 1.69207

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.594, [92mTest[0m: 2.470, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.56081
[1mStep[0m  [8/84], [94mLoss[0m : 1.57549
[1mStep[0m  [16/84], [94mLoss[0m : 1.55894
[1mStep[0m  [24/84], [94mLoss[0m : 1.60224
[1mStep[0m  [32/84], [94mLoss[0m : 1.57914
[1mStep[0m  [40/84], [94mLoss[0m : 1.50053
[1mStep[0m  [48/84], [94mLoss[0m : 1.65268
[1mStep[0m  [56/84], [94mLoss[0m : 1.73306
[1mStep[0m  [64/84], [94mLoss[0m : 1.70329
[1mStep[0m  [72/84], [94mLoss[0m : 1.51250
[1mStep[0m  [80/84], [94mLoss[0m : 1.65392

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.567, [92mTest[0m: 2.540, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.44203
[1mStep[0m  [8/84], [94mLoss[0m : 1.47403
[1mStep[0m  [16/84], [94mLoss[0m : 1.55185
[1mStep[0m  [24/84], [94mLoss[0m : 1.42494
[1mStep[0m  [32/84], [94mLoss[0m : 1.72350
[1mStep[0m  [40/84], [94mLoss[0m : 1.46143
[1mStep[0m  [48/84], [94mLoss[0m : 1.63613
[1mStep[0m  [56/84], [94mLoss[0m : 1.43760
[1mStep[0m  [64/84], [94mLoss[0m : 1.74430
[1mStep[0m  [72/84], [94mLoss[0m : 1.63898
[1mStep[0m  [80/84], [94mLoss[0m : 1.70516

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.551, [92mTest[0m: 2.523, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.45417
[1mStep[0m  [8/84], [94mLoss[0m : 1.54501
[1mStep[0m  [16/84], [94mLoss[0m : 1.53943
[1mStep[0m  [24/84], [94mLoss[0m : 1.61337
[1mStep[0m  [32/84], [94mLoss[0m : 1.46807
[1mStep[0m  [40/84], [94mLoss[0m : 1.78524
[1mStep[0m  [48/84], [94mLoss[0m : 1.72688
[1mStep[0m  [56/84], [94mLoss[0m : 1.35594
[1mStep[0m  [64/84], [94mLoss[0m : 1.51127
[1mStep[0m  [72/84], [94mLoss[0m : 1.64272
[1mStep[0m  [80/84], [94mLoss[0m : 1.71694

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.536, [92mTest[0m: 2.588, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.44818
[1mStep[0m  [8/84], [94mLoss[0m : 1.51128
[1mStep[0m  [16/84], [94mLoss[0m : 1.66883
[1mStep[0m  [24/84], [94mLoss[0m : 1.57448
[1mStep[0m  [32/84], [94mLoss[0m : 1.36053
[1mStep[0m  [40/84], [94mLoss[0m : 1.47121
[1mStep[0m  [48/84], [94mLoss[0m : 1.51818
[1mStep[0m  [56/84], [94mLoss[0m : 1.53419
[1mStep[0m  [64/84], [94mLoss[0m : 1.39700
[1mStep[0m  [72/84], [94mLoss[0m : 1.46064
[1mStep[0m  [80/84], [94mLoss[0m : 1.42143

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.498, [92mTest[0m: 2.464, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.28998
[1mStep[0m  [8/84], [94mLoss[0m : 1.45584
[1mStep[0m  [16/84], [94mLoss[0m : 1.47243
[1mStep[0m  [24/84], [94mLoss[0m : 1.38205
[1mStep[0m  [32/84], [94mLoss[0m : 1.49172
[1mStep[0m  [40/84], [94mLoss[0m : 1.30979
[1mStep[0m  [48/84], [94mLoss[0m : 1.65620
[1mStep[0m  [56/84], [94mLoss[0m : 1.49160
[1mStep[0m  [64/84], [94mLoss[0m : 1.47572
[1mStep[0m  [72/84], [94mLoss[0m : 1.62443
[1mStep[0m  [80/84], [94mLoss[0m : 1.54243

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.495, [92mTest[0m: 2.522, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.47500
[1mStep[0m  [8/84], [94mLoss[0m : 1.52833
[1mStep[0m  [16/84], [94mLoss[0m : 1.51409
[1mStep[0m  [24/84], [94mLoss[0m : 1.67513
[1mStep[0m  [32/84], [94mLoss[0m : 1.52450
[1mStep[0m  [40/84], [94mLoss[0m : 1.41880
[1mStep[0m  [48/84], [94mLoss[0m : 1.58501
[1mStep[0m  [56/84], [94mLoss[0m : 1.35632
[1mStep[0m  [64/84], [94mLoss[0m : 1.60053
[1mStep[0m  [72/84], [94mLoss[0m : 1.51489
[1mStep[0m  [80/84], [94mLoss[0m : 1.48784

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.472, [92mTest[0m: 2.464, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.462
====================================

Phase 2 - Evaluation MAE:  2.461949586868286
MAE score P1      2.340391
MAE score P2       2.46195
loss              1.471654
learning_rate     0.007525
batch_size             128
hidden_sizes         [250]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.1
weight_decay          0.01
Name: 10, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 11.48011
[1mStep[0m  [8/84], [94mLoss[0m : 8.07007
[1mStep[0m  [16/84], [94mLoss[0m : 3.43286
[1mStep[0m  [24/84], [94mLoss[0m : 2.82727
[1mStep[0m  [32/84], [94mLoss[0m : 3.11337
[1mStep[0m  [40/84], [94mLoss[0m : 2.77510
[1mStep[0m  [48/84], [94mLoss[0m : 3.04558
[1mStep[0m  [56/84], [94mLoss[0m : 2.62621
[1mStep[0m  [64/84], [94mLoss[0m : 2.66627
[1mStep[0m  [72/84], [94mLoss[0m : 2.80312
[1mStep[0m  [80/84], [94mLoss[0m : 2.65027

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.842, [92mTest[0m: 11.043, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.71605
[1mStep[0m  [8/84], [94mLoss[0m : 2.36988
[1mStep[0m  [16/84], [94mLoss[0m : 2.53945
[1mStep[0m  [24/84], [94mLoss[0m : 2.49073
[1mStep[0m  [32/84], [94mLoss[0m : 2.61833
[1mStep[0m  [40/84], [94mLoss[0m : 2.77295
[1mStep[0m  [48/84], [94mLoss[0m : 2.77201
[1mStep[0m  [56/84], [94mLoss[0m : 2.73897
[1mStep[0m  [64/84], [94mLoss[0m : 2.64316
[1mStep[0m  [72/84], [94mLoss[0m : 2.45902
[1mStep[0m  [80/84], [94mLoss[0m : 2.75791

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.635, [92mTest[0m: 2.379, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.82153
[1mStep[0m  [8/84], [94mLoss[0m : 2.29317
[1mStep[0m  [16/84], [94mLoss[0m : 2.52626
[1mStep[0m  [24/84], [94mLoss[0m : 2.87494
[1mStep[0m  [32/84], [94mLoss[0m : 2.95018
[1mStep[0m  [40/84], [94mLoss[0m : 2.82207
[1mStep[0m  [48/84], [94mLoss[0m : 2.50263
[1mStep[0m  [56/84], [94mLoss[0m : 2.71304
[1mStep[0m  [64/84], [94mLoss[0m : 2.67407
[1mStep[0m  [72/84], [94mLoss[0m : 2.40075
[1mStep[0m  [80/84], [94mLoss[0m : 2.77367

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.630, [92mTest[0m: 2.356, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43274
[1mStep[0m  [8/84], [94mLoss[0m : 2.78365
[1mStep[0m  [16/84], [94mLoss[0m : 2.65212
[1mStep[0m  [24/84], [94mLoss[0m : 3.20168
[1mStep[0m  [32/84], [94mLoss[0m : 2.43319
[1mStep[0m  [40/84], [94mLoss[0m : 2.53114
[1mStep[0m  [48/84], [94mLoss[0m : 2.48308
[1mStep[0m  [56/84], [94mLoss[0m : 2.77772
[1mStep[0m  [64/84], [94mLoss[0m : 2.70883
[1mStep[0m  [72/84], [94mLoss[0m : 2.81457
[1mStep[0m  [80/84], [94mLoss[0m : 2.35981

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.615, [92mTest[0m: 2.348, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.77276
[1mStep[0m  [8/84], [94mLoss[0m : 2.34818
[1mStep[0m  [16/84], [94mLoss[0m : 2.66523
[1mStep[0m  [24/84], [94mLoss[0m : 2.58340
[1mStep[0m  [32/84], [94mLoss[0m : 2.67100
[1mStep[0m  [40/84], [94mLoss[0m : 2.48231
[1mStep[0m  [48/84], [94mLoss[0m : 2.63370
[1mStep[0m  [56/84], [94mLoss[0m : 2.38032
[1mStep[0m  [64/84], [94mLoss[0m : 2.66437
[1mStep[0m  [72/84], [94mLoss[0m : 2.63646
[1mStep[0m  [80/84], [94mLoss[0m : 2.36474

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.583, [92mTest[0m: 2.350, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41676
[1mStep[0m  [8/84], [94mLoss[0m : 2.57034
[1mStep[0m  [16/84], [94mLoss[0m : 2.25819
[1mStep[0m  [24/84], [94mLoss[0m : 2.29748
[1mStep[0m  [32/84], [94mLoss[0m : 2.46214
[1mStep[0m  [40/84], [94mLoss[0m : 2.75435
[1mStep[0m  [48/84], [94mLoss[0m : 2.67452
[1mStep[0m  [56/84], [94mLoss[0m : 2.53574
[1mStep[0m  [64/84], [94mLoss[0m : 2.51275
[1mStep[0m  [72/84], [94mLoss[0m : 2.47534
[1mStep[0m  [80/84], [94mLoss[0m : 2.67231

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.552, [92mTest[0m: 2.336, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.70099
[1mStep[0m  [8/84], [94mLoss[0m : 2.20734
[1mStep[0m  [16/84], [94mLoss[0m : 2.30052
[1mStep[0m  [24/84], [94mLoss[0m : 2.76214
[1mStep[0m  [32/84], [94mLoss[0m : 2.54741
[1mStep[0m  [40/84], [94mLoss[0m : 2.53295
[1mStep[0m  [48/84], [94mLoss[0m : 2.70134
[1mStep[0m  [56/84], [94mLoss[0m : 2.79818
[1mStep[0m  [64/84], [94mLoss[0m : 2.54369
[1mStep[0m  [72/84], [94mLoss[0m : 2.76968
[1mStep[0m  [80/84], [94mLoss[0m : 2.28134

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.562, [92mTest[0m: 2.340, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.58103
[1mStep[0m  [8/84], [94mLoss[0m : 2.75652
[1mStep[0m  [16/84], [94mLoss[0m : 2.63935
[1mStep[0m  [24/84], [94mLoss[0m : 2.45267
[1mStep[0m  [32/84], [94mLoss[0m : 2.43461
[1mStep[0m  [40/84], [94mLoss[0m : 2.69239
[1mStep[0m  [48/84], [94mLoss[0m : 2.74127
[1mStep[0m  [56/84], [94mLoss[0m : 2.59117
[1mStep[0m  [64/84], [94mLoss[0m : 2.35055
[1mStep[0m  [72/84], [94mLoss[0m : 2.44907
[1mStep[0m  [80/84], [94mLoss[0m : 2.31676

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.554, [92mTest[0m: 2.337, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.82920
[1mStep[0m  [8/84], [94mLoss[0m : 2.48510
[1mStep[0m  [16/84], [94mLoss[0m : 2.55893
[1mStep[0m  [24/84], [94mLoss[0m : 2.88272
[1mStep[0m  [32/84], [94mLoss[0m : 2.80725
[1mStep[0m  [40/84], [94mLoss[0m : 2.45713
[1mStep[0m  [48/84], [94mLoss[0m : 2.32274
[1mStep[0m  [56/84], [94mLoss[0m : 2.34425
[1mStep[0m  [64/84], [94mLoss[0m : 2.73931
[1mStep[0m  [72/84], [94mLoss[0m : 2.54500
[1mStep[0m  [80/84], [94mLoss[0m : 2.31341

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.549, [92mTest[0m: 2.343, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50315
[1mStep[0m  [8/84], [94mLoss[0m : 2.76914
[1mStep[0m  [16/84], [94mLoss[0m : 2.50242
[1mStep[0m  [24/84], [94mLoss[0m : 2.25260
[1mStep[0m  [32/84], [94mLoss[0m : 2.34582
[1mStep[0m  [40/84], [94mLoss[0m : 2.54504
[1mStep[0m  [48/84], [94mLoss[0m : 2.73186
[1mStep[0m  [56/84], [94mLoss[0m : 2.59780
[1mStep[0m  [64/84], [94mLoss[0m : 2.74826
[1mStep[0m  [72/84], [94mLoss[0m : 2.56721
[1mStep[0m  [80/84], [94mLoss[0m : 2.63135

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.345, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.24231
[1mStep[0m  [8/84], [94mLoss[0m : 2.55258
[1mStep[0m  [16/84], [94mLoss[0m : 2.50166
[1mStep[0m  [24/84], [94mLoss[0m : 2.64353
[1mStep[0m  [32/84], [94mLoss[0m : 2.52050
[1mStep[0m  [40/84], [94mLoss[0m : 2.34038
[1mStep[0m  [48/84], [94mLoss[0m : 2.82092
[1mStep[0m  [56/84], [94mLoss[0m : 2.53891
[1mStep[0m  [64/84], [94mLoss[0m : 2.52184
[1mStep[0m  [72/84], [94mLoss[0m : 2.62805
[1mStep[0m  [80/84], [94mLoss[0m : 2.57393

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.530, [92mTest[0m: 2.340, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.79696
[1mStep[0m  [8/84], [94mLoss[0m : 2.60661
[1mStep[0m  [16/84], [94mLoss[0m : 2.50409
[1mStep[0m  [24/84], [94mLoss[0m : 2.63888
[1mStep[0m  [32/84], [94mLoss[0m : 2.71103
[1mStep[0m  [40/84], [94mLoss[0m : 2.29918
[1mStep[0m  [48/84], [94mLoss[0m : 2.40861
[1mStep[0m  [56/84], [94mLoss[0m : 2.55156
[1mStep[0m  [64/84], [94mLoss[0m : 2.23456
[1mStep[0m  [72/84], [94mLoss[0m : 2.62152
[1mStep[0m  [80/84], [94mLoss[0m : 2.46529

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.337, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50219
[1mStep[0m  [8/84], [94mLoss[0m : 2.47607
[1mStep[0m  [16/84], [94mLoss[0m : 2.39211
[1mStep[0m  [24/84], [94mLoss[0m : 2.54336
[1mStep[0m  [32/84], [94mLoss[0m : 2.54542
[1mStep[0m  [40/84], [94mLoss[0m : 2.49608
[1mStep[0m  [48/84], [94mLoss[0m : 2.59774
[1mStep[0m  [56/84], [94mLoss[0m : 2.37861
[1mStep[0m  [64/84], [94mLoss[0m : 2.65777
[1mStep[0m  [72/84], [94mLoss[0m : 2.39069
[1mStep[0m  [80/84], [94mLoss[0m : 2.73837

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.514, [92mTest[0m: 2.329, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.61068
[1mStep[0m  [8/84], [94mLoss[0m : 2.71337
[1mStep[0m  [16/84], [94mLoss[0m : 2.18600
[1mStep[0m  [24/84], [94mLoss[0m : 2.64334
[1mStep[0m  [32/84], [94mLoss[0m : 2.40494
[1mStep[0m  [40/84], [94mLoss[0m : 2.58305
[1mStep[0m  [48/84], [94mLoss[0m : 2.35225
[1mStep[0m  [56/84], [94mLoss[0m : 2.68005
[1mStep[0m  [64/84], [94mLoss[0m : 2.41227
[1mStep[0m  [72/84], [94mLoss[0m : 2.74767
[1mStep[0m  [80/84], [94mLoss[0m : 2.45018

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.517, [92mTest[0m: 2.347, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.68151
[1mStep[0m  [8/84], [94mLoss[0m : 2.75327
[1mStep[0m  [16/84], [94mLoss[0m : 2.71791
[1mStep[0m  [24/84], [94mLoss[0m : 2.32784
[1mStep[0m  [32/84], [94mLoss[0m : 2.80068
[1mStep[0m  [40/84], [94mLoss[0m : 2.40497
[1mStep[0m  [48/84], [94mLoss[0m : 2.39785
[1mStep[0m  [56/84], [94mLoss[0m : 2.75432
[1mStep[0m  [64/84], [94mLoss[0m : 2.51835
[1mStep[0m  [72/84], [94mLoss[0m : 2.63755
[1mStep[0m  [80/84], [94mLoss[0m : 2.24705

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.343, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.17201
[1mStep[0m  [8/84], [94mLoss[0m : 2.48534
[1mStep[0m  [16/84], [94mLoss[0m : 2.34020
[1mStep[0m  [24/84], [94mLoss[0m : 2.30003
[1mStep[0m  [32/84], [94mLoss[0m : 2.61662
[1mStep[0m  [40/84], [94mLoss[0m : 2.81706
[1mStep[0m  [48/84], [94mLoss[0m : 2.31034
[1mStep[0m  [56/84], [94mLoss[0m : 2.59812
[1mStep[0m  [64/84], [94mLoss[0m : 2.41326
[1mStep[0m  [72/84], [94mLoss[0m : 2.58642
[1mStep[0m  [80/84], [94mLoss[0m : 2.84478

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.332, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.39029
[1mStep[0m  [8/84], [94mLoss[0m : 2.42194
[1mStep[0m  [16/84], [94mLoss[0m : 2.45656
[1mStep[0m  [24/84], [94mLoss[0m : 2.54863
[1mStep[0m  [32/84], [94mLoss[0m : 2.55106
[1mStep[0m  [40/84], [94mLoss[0m : 2.45759
[1mStep[0m  [48/84], [94mLoss[0m : 2.33785
[1mStep[0m  [56/84], [94mLoss[0m : 2.69966
[1mStep[0m  [64/84], [94mLoss[0m : 2.13636
[1mStep[0m  [72/84], [94mLoss[0m : 2.52283
[1mStep[0m  [80/84], [94mLoss[0m : 2.54940

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.326, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43046
[1mStep[0m  [8/84], [94mLoss[0m : 2.36880
[1mStep[0m  [16/84], [94mLoss[0m : 2.61245
[1mStep[0m  [24/84], [94mLoss[0m : 2.32999
[1mStep[0m  [32/84], [94mLoss[0m : 2.35018
[1mStep[0m  [40/84], [94mLoss[0m : 2.42519
[1mStep[0m  [48/84], [94mLoss[0m : 2.67925
[1mStep[0m  [56/84], [94mLoss[0m : 2.61789
[1mStep[0m  [64/84], [94mLoss[0m : 2.96307
[1mStep[0m  [72/84], [94mLoss[0m : 2.40748
[1mStep[0m  [80/84], [94mLoss[0m : 2.74790

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.328, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60708
[1mStep[0m  [8/84], [94mLoss[0m : 2.73928
[1mStep[0m  [16/84], [94mLoss[0m : 2.42035
[1mStep[0m  [24/84], [94mLoss[0m : 2.37362
[1mStep[0m  [32/84], [94mLoss[0m : 2.29187
[1mStep[0m  [40/84], [94mLoss[0m : 2.49852
[1mStep[0m  [48/84], [94mLoss[0m : 2.27996
[1mStep[0m  [56/84], [94mLoss[0m : 2.48487
[1mStep[0m  [64/84], [94mLoss[0m : 2.41390
[1mStep[0m  [72/84], [94mLoss[0m : 2.71999
[1mStep[0m  [80/84], [94mLoss[0m : 2.45859

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.333, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.66579
[1mStep[0m  [8/84], [94mLoss[0m : 2.25128
[1mStep[0m  [16/84], [94mLoss[0m : 2.60933
[1mStep[0m  [24/84], [94mLoss[0m : 2.38631
[1mStep[0m  [32/84], [94mLoss[0m : 2.55134
[1mStep[0m  [40/84], [94mLoss[0m : 2.44075
[1mStep[0m  [48/84], [94mLoss[0m : 2.23466
[1mStep[0m  [56/84], [94mLoss[0m : 2.58757
[1mStep[0m  [64/84], [94mLoss[0m : 2.45750
[1mStep[0m  [72/84], [94mLoss[0m : 2.47358
[1mStep[0m  [80/84], [94mLoss[0m : 2.14423

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.344, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.77655
[1mStep[0m  [8/84], [94mLoss[0m : 2.58251
[1mStep[0m  [16/84], [94mLoss[0m : 2.58285
[1mStep[0m  [24/84], [94mLoss[0m : 2.39700
[1mStep[0m  [32/84], [94mLoss[0m : 2.50483
[1mStep[0m  [40/84], [94mLoss[0m : 2.20216
[1mStep[0m  [48/84], [94mLoss[0m : 2.83298
[1mStep[0m  [56/84], [94mLoss[0m : 2.53627
[1mStep[0m  [64/84], [94mLoss[0m : 2.64695
[1mStep[0m  [72/84], [94mLoss[0m : 2.70899
[1mStep[0m  [80/84], [94mLoss[0m : 2.72819

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.334, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.68854
[1mStep[0m  [8/84], [94mLoss[0m : 2.40200
[1mStep[0m  [16/84], [94mLoss[0m : 2.51310
[1mStep[0m  [24/84], [94mLoss[0m : 2.39180
[1mStep[0m  [32/84], [94mLoss[0m : 2.37911
[1mStep[0m  [40/84], [94mLoss[0m : 2.66994
[1mStep[0m  [48/84], [94mLoss[0m : 2.39210
[1mStep[0m  [56/84], [94mLoss[0m : 2.59426
[1mStep[0m  [64/84], [94mLoss[0m : 2.51003
[1mStep[0m  [72/84], [94mLoss[0m : 2.57333
[1mStep[0m  [80/84], [94mLoss[0m : 2.41044

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.328, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.06958
[1mStep[0m  [8/84], [94mLoss[0m : 2.54024
[1mStep[0m  [16/84], [94mLoss[0m : 2.40200
[1mStep[0m  [24/84], [94mLoss[0m : 2.60921
[1mStep[0m  [32/84], [94mLoss[0m : 2.48755
[1mStep[0m  [40/84], [94mLoss[0m : 2.63793
[1mStep[0m  [48/84], [94mLoss[0m : 2.23645
[1mStep[0m  [56/84], [94mLoss[0m : 2.37094
[1mStep[0m  [64/84], [94mLoss[0m : 2.30141
[1mStep[0m  [72/84], [94mLoss[0m : 2.72692
[1mStep[0m  [80/84], [94mLoss[0m : 2.39348

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.323, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50758
[1mStep[0m  [8/84], [94mLoss[0m : 2.56932
[1mStep[0m  [16/84], [94mLoss[0m : 2.46786
[1mStep[0m  [24/84], [94mLoss[0m : 2.65767
[1mStep[0m  [32/84], [94mLoss[0m : 2.61909
[1mStep[0m  [40/84], [94mLoss[0m : 2.41422
[1mStep[0m  [48/84], [94mLoss[0m : 2.44193
[1mStep[0m  [56/84], [94mLoss[0m : 2.52358
[1mStep[0m  [64/84], [94mLoss[0m : 2.44403
[1mStep[0m  [72/84], [94mLoss[0m : 2.33067
[1mStep[0m  [80/84], [94mLoss[0m : 2.58165

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.335, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.38720
[1mStep[0m  [8/84], [94mLoss[0m : 2.42653
[1mStep[0m  [16/84], [94mLoss[0m : 2.76728
[1mStep[0m  [24/84], [94mLoss[0m : 2.07968
[1mStep[0m  [32/84], [94mLoss[0m : 2.67674
[1mStep[0m  [40/84], [94mLoss[0m : 2.50148
[1mStep[0m  [48/84], [94mLoss[0m : 2.54911
[1mStep[0m  [56/84], [94mLoss[0m : 2.32638
[1mStep[0m  [64/84], [94mLoss[0m : 2.78039
[1mStep[0m  [72/84], [94mLoss[0m : 2.34656
[1mStep[0m  [80/84], [94mLoss[0m : 2.66802

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.334, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26607
[1mStep[0m  [8/84], [94mLoss[0m : 2.46017
[1mStep[0m  [16/84], [94mLoss[0m : 2.56883
[1mStep[0m  [24/84], [94mLoss[0m : 2.60585
[1mStep[0m  [32/84], [94mLoss[0m : 2.62636
[1mStep[0m  [40/84], [94mLoss[0m : 2.67290
[1mStep[0m  [48/84], [94mLoss[0m : 2.36422
[1mStep[0m  [56/84], [94mLoss[0m : 2.44151
[1mStep[0m  [64/84], [94mLoss[0m : 2.21167
[1mStep[0m  [72/84], [94mLoss[0m : 2.49389
[1mStep[0m  [80/84], [94mLoss[0m : 2.64417

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.331, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42211
[1mStep[0m  [8/84], [94mLoss[0m : 2.51164
[1mStep[0m  [16/84], [94mLoss[0m : 2.86904
[1mStep[0m  [24/84], [94mLoss[0m : 2.58972
[1mStep[0m  [32/84], [94mLoss[0m : 2.56331
[1mStep[0m  [40/84], [94mLoss[0m : 2.41258
[1mStep[0m  [48/84], [94mLoss[0m : 2.41537
[1mStep[0m  [56/84], [94mLoss[0m : 2.78458
[1mStep[0m  [64/84], [94mLoss[0m : 2.29526
[1mStep[0m  [72/84], [94mLoss[0m : 2.20762
[1mStep[0m  [80/84], [94mLoss[0m : 2.65776

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.325, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46874
[1mStep[0m  [8/84], [94mLoss[0m : 2.66739
[1mStep[0m  [16/84], [94mLoss[0m : 2.38109
[1mStep[0m  [24/84], [94mLoss[0m : 2.36701
[1mStep[0m  [32/84], [94mLoss[0m : 2.36634
[1mStep[0m  [40/84], [94mLoss[0m : 2.57581
[1mStep[0m  [48/84], [94mLoss[0m : 2.01803
[1mStep[0m  [56/84], [94mLoss[0m : 2.70802
[1mStep[0m  [64/84], [94mLoss[0m : 2.45869
[1mStep[0m  [72/84], [94mLoss[0m : 2.45790
[1mStep[0m  [80/84], [94mLoss[0m : 2.41446

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.324, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27678
[1mStep[0m  [8/84], [94mLoss[0m : 2.37205
[1mStep[0m  [16/84], [94mLoss[0m : 2.31315
[1mStep[0m  [24/84], [94mLoss[0m : 2.62226
[1mStep[0m  [32/84], [94mLoss[0m : 2.35661
[1mStep[0m  [40/84], [94mLoss[0m : 2.38826
[1mStep[0m  [48/84], [94mLoss[0m : 1.91735
[1mStep[0m  [56/84], [94mLoss[0m : 2.74109
[1mStep[0m  [64/84], [94mLoss[0m : 2.59754
[1mStep[0m  [72/84], [94mLoss[0m : 2.14576
[1mStep[0m  [80/84], [94mLoss[0m : 2.29078

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.334, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.56503
[1mStep[0m  [8/84], [94mLoss[0m : 2.27776
[1mStep[0m  [16/84], [94mLoss[0m : 2.81259
[1mStep[0m  [24/84], [94mLoss[0m : 2.59715
[1mStep[0m  [32/84], [94mLoss[0m : 2.30825
[1mStep[0m  [40/84], [94mLoss[0m : 2.37292
[1mStep[0m  [48/84], [94mLoss[0m : 2.44582
[1mStep[0m  [56/84], [94mLoss[0m : 2.68124
[1mStep[0m  [64/84], [94mLoss[0m : 2.68047
[1mStep[0m  [72/84], [94mLoss[0m : 2.40928
[1mStep[0m  [80/84], [94mLoss[0m : 2.40555

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.327, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.330
====================================

Phase 1 - Evaluation MAE:  2.330002495220729
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 2.40869
[1mStep[0m  [8/84], [94mLoss[0m : 2.55264
[1mStep[0m  [16/84], [94mLoss[0m : 2.65956
[1mStep[0m  [24/84], [94mLoss[0m : 2.42908
[1mStep[0m  [32/84], [94mLoss[0m : 2.16393
[1mStep[0m  [40/84], [94mLoss[0m : 2.39020
[1mStep[0m  [48/84], [94mLoss[0m : 2.49825
[1mStep[0m  [56/84], [94mLoss[0m : 2.47287
[1mStep[0m  [64/84], [94mLoss[0m : 2.35161
[1mStep[0m  [72/84], [94mLoss[0m : 2.69114
[1mStep[0m  [80/84], [94mLoss[0m : 2.69804

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.328, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.21281
[1mStep[0m  [8/84], [94mLoss[0m : 2.39804
[1mStep[0m  [16/84], [94mLoss[0m : 2.58809
[1mStep[0m  [24/84], [94mLoss[0m : 2.46837
[1mStep[0m  [32/84], [94mLoss[0m : 2.52514
[1mStep[0m  [40/84], [94mLoss[0m : 2.51523
[1mStep[0m  [48/84], [94mLoss[0m : 2.29805
[1mStep[0m  [56/84], [94mLoss[0m : 2.34589
[1mStep[0m  [64/84], [94mLoss[0m : 2.35141
[1mStep[0m  [72/84], [94mLoss[0m : 2.33249
[1mStep[0m  [80/84], [94mLoss[0m : 2.27346

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.367, [92mTest[0m: 2.394, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.57733
[1mStep[0m  [8/84], [94mLoss[0m : 2.20688
[1mStep[0m  [16/84], [94mLoss[0m : 2.14899
[1mStep[0m  [24/84], [94mLoss[0m : 2.19823
[1mStep[0m  [32/84], [94mLoss[0m : 2.64171
[1mStep[0m  [40/84], [94mLoss[0m : 2.10785
[1mStep[0m  [48/84], [94mLoss[0m : 2.25894
[1mStep[0m  [56/84], [94mLoss[0m : 2.36463
[1mStep[0m  [64/84], [94mLoss[0m : 2.03101
[1mStep[0m  [72/84], [94mLoss[0m : 2.58858
[1mStep[0m  [80/84], [94mLoss[0m : 2.57009

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.293, [92mTest[0m: 2.447, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.83620
[1mStep[0m  [8/84], [94mLoss[0m : 2.12760
[1mStep[0m  [16/84], [94mLoss[0m : 2.23096
[1mStep[0m  [24/84], [94mLoss[0m : 2.35334
[1mStep[0m  [32/84], [94mLoss[0m : 2.39734
[1mStep[0m  [40/84], [94mLoss[0m : 2.09364
[1mStep[0m  [48/84], [94mLoss[0m : 2.02701
[1mStep[0m  [56/84], [94mLoss[0m : 2.13350
[1mStep[0m  [64/84], [94mLoss[0m : 2.22500
[1mStep[0m  [72/84], [94mLoss[0m : 2.22201
[1mStep[0m  [80/84], [94mLoss[0m : 2.18541

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.210, [92mTest[0m: 2.369, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.02069
[1mStep[0m  [8/84], [94mLoss[0m : 2.00910
[1mStep[0m  [16/84], [94mLoss[0m : 2.27417
[1mStep[0m  [24/84], [94mLoss[0m : 2.30886
[1mStep[0m  [32/84], [94mLoss[0m : 2.48694
[1mStep[0m  [40/84], [94mLoss[0m : 2.21644
[1mStep[0m  [48/84], [94mLoss[0m : 2.31681
[1mStep[0m  [56/84], [94mLoss[0m : 2.48260
[1mStep[0m  [64/84], [94mLoss[0m : 2.38432
[1mStep[0m  [72/84], [94mLoss[0m : 2.10110
[1mStep[0m  [80/84], [94mLoss[0m : 2.19571

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.157, [92mTest[0m: 2.405, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.23654
[1mStep[0m  [8/84], [94mLoss[0m : 2.27025
[1mStep[0m  [16/84], [94mLoss[0m : 1.78061
[1mStep[0m  [24/84], [94mLoss[0m : 2.10060
[1mStep[0m  [32/84], [94mLoss[0m : 2.43115
[1mStep[0m  [40/84], [94mLoss[0m : 2.27220
[1mStep[0m  [48/84], [94mLoss[0m : 2.08401
[1mStep[0m  [56/84], [94mLoss[0m : 2.16999
[1mStep[0m  [64/84], [94mLoss[0m : 1.84532
[1mStep[0m  [72/84], [94mLoss[0m : 1.83999
[1mStep[0m  [80/84], [94mLoss[0m : 2.00771

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.107, [92mTest[0m: 2.453, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.15475
[1mStep[0m  [8/84], [94mLoss[0m : 1.97564
[1mStep[0m  [16/84], [94mLoss[0m : 2.16828
[1mStep[0m  [24/84], [94mLoss[0m : 1.87776
[1mStep[0m  [32/84], [94mLoss[0m : 2.23675
[1mStep[0m  [40/84], [94mLoss[0m : 2.43488
[1mStep[0m  [48/84], [94mLoss[0m : 1.99405
[1mStep[0m  [56/84], [94mLoss[0m : 1.87617
[1mStep[0m  [64/84], [94mLoss[0m : 1.92814
[1mStep[0m  [72/84], [94mLoss[0m : 2.04725
[1mStep[0m  [80/84], [94mLoss[0m : 2.46586

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.050, [92mTest[0m: 2.388, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.84734
[1mStep[0m  [8/84], [94mLoss[0m : 1.97676
[1mStep[0m  [16/84], [94mLoss[0m : 1.82428
[1mStep[0m  [24/84], [94mLoss[0m : 1.78498
[1mStep[0m  [32/84], [94mLoss[0m : 1.93996
[1mStep[0m  [40/84], [94mLoss[0m : 2.00441
[1mStep[0m  [48/84], [94mLoss[0m : 1.71218
[1mStep[0m  [56/84], [94mLoss[0m : 1.98596
[1mStep[0m  [64/84], [94mLoss[0m : 2.07869
[1mStep[0m  [72/84], [94mLoss[0m : 1.73683
[1mStep[0m  [80/84], [94mLoss[0m : 2.01578

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.004, [92mTest[0m: 2.413, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.79301
[1mStep[0m  [8/84], [94mLoss[0m : 1.81412
[1mStep[0m  [16/84], [94mLoss[0m : 1.87989
[1mStep[0m  [24/84], [94mLoss[0m : 2.16928
[1mStep[0m  [32/84], [94mLoss[0m : 1.89306
[1mStep[0m  [40/84], [94mLoss[0m : 1.71671
[1mStep[0m  [48/84], [94mLoss[0m : 2.06491
[1mStep[0m  [56/84], [94mLoss[0m : 1.74097
[1mStep[0m  [64/84], [94mLoss[0m : 1.83366
[1mStep[0m  [72/84], [94mLoss[0m : 1.97758
[1mStep[0m  [80/84], [94mLoss[0m : 2.06762

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.936, [92mTest[0m: 2.417, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.63657
[1mStep[0m  [8/84], [94mLoss[0m : 1.74839
[1mStep[0m  [16/84], [94mLoss[0m : 1.89806
[1mStep[0m  [24/84], [94mLoss[0m : 1.75605
[1mStep[0m  [32/84], [94mLoss[0m : 1.73603
[1mStep[0m  [40/84], [94mLoss[0m : 1.91341
[1mStep[0m  [48/84], [94mLoss[0m : 1.90657
[1mStep[0m  [56/84], [94mLoss[0m : 2.04354
[1mStep[0m  [64/84], [94mLoss[0m : 2.17729
[1mStep[0m  [72/84], [94mLoss[0m : 2.16411
[1mStep[0m  [80/84], [94mLoss[0m : 1.97363

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.900, [92mTest[0m: 2.472, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.85937
[1mStep[0m  [8/84], [94mLoss[0m : 1.83219
[1mStep[0m  [16/84], [94mLoss[0m : 1.71452
[1mStep[0m  [24/84], [94mLoss[0m : 1.81205
[1mStep[0m  [32/84], [94mLoss[0m : 1.81308
[1mStep[0m  [40/84], [94mLoss[0m : 1.70614
[1mStep[0m  [48/84], [94mLoss[0m : 1.73656
[1mStep[0m  [56/84], [94mLoss[0m : 1.76664
[1mStep[0m  [64/84], [94mLoss[0m : 1.95720
[1mStep[0m  [72/84], [94mLoss[0m : 1.91906
[1mStep[0m  [80/84], [94mLoss[0m : 1.95326

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.846, [92mTest[0m: 2.466, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.76281
[1mStep[0m  [8/84], [94mLoss[0m : 1.86797
[1mStep[0m  [16/84], [94mLoss[0m : 1.70101
[1mStep[0m  [24/84], [94mLoss[0m : 2.01023
[1mStep[0m  [32/84], [94mLoss[0m : 1.75785
[1mStep[0m  [40/84], [94mLoss[0m : 1.65839
[1mStep[0m  [48/84], [94mLoss[0m : 1.97313
[1mStep[0m  [56/84], [94mLoss[0m : 1.69297
[1mStep[0m  [64/84], [94mLoss[0m : 1.89055
[1mStep[0m  [72/84], [94mLoss[0m : 1.79436
[1mStep[0m  [80/84], [94mLoss[0m : 1.81242

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.811, [92mTest[0m: 2.524, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.48047
[1mStep[0m  [8/84], [94mLoss[0m : 1.59597
[1mStep[0m  [16/84], [94mLoss[0m : 1.63137
[1mStep[0m  [24/84], [94mLoss[0m : 1.71913
[1mStep[0m  [32/84], [94mLoss[0m : 1.81388
[1mStep[0m  [40/84], [94mLoss[0m : 1.97089
[1mStep[0m  [48/84], [94mLoss[0m : 1.69672
[1mStep[0m  [56/84], [94mLoss[0m : 1.89818
[1mStep[0m  [64/84], [94mLoss[0m : 1.81308
[1mStep[0m  [72/84], [94mLoss[0m : 1.92642
[1mStep[0m  [80/84], [94mLoss[0m : 1.74302

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.767, [92mTest[0m: 2.512, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.52510
[1mStep[0m  [8/84], [94mLoss[0m : 1.89162
[1mStep[0m  [16/84], [94mLoss[0m : 1.73005
[1mStep[0m  [24/84], [94mLoss[0m : 1.55490
[1mStep[0m  [32/84], [94mLoss[0m : 1.66596
[1mStep[0m  [40/84], [94mLoss[0m : 1.66626
[1mStep[0m  [48/84], [94mLoss[0m : 1.79151
[1mStep[0m  [56/84], [94mLoss[0m : 1.68248
[1mStep[0m  [64/84], [94mLoss[0m : 1.74071
[1mStep[0m  [72/84], [94mLoss[0m : 1.87898
[1mStep[0m  [80/84], [94mLoss[0m : 1.90579

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.730, [92mTest[0m: 2.487, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.54412
[1mStep[0m  [8/84], [94mLoss[0m : 1.80219
[1mStep[0m  [16/84], [94mLoss[0m : 1.66523
[1mStep[0m  [24/84], [94mLoss[0m : 1.56330
[1mStep[0m  [32/84], [94mLoss[0m : 1.40488
[1mStep[0m  [40/84], [94mLoss[0m : 1.67092
[1mStep[0m  [48/84], [94mLoss[0m : 1.62238
[1mStep[0m  [56/84], [94mLoss[0m : 1.94429
[1mStep[0m  [64/84], [94mLoss[0m : 1.61394
[1mStep[0m  [72/84], [94mLoss[0m : 1.75448
[1mStep[0m  [80/84], [94mLoss[0m : 2.12776

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.707, [92mTest[0m: 2.506, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.53151
[1mStep[0m  [8/84], [94mLoss[0m : 1.45132
[1mStep[0m  [16/84], [94mLoss[0m : 1.70400
[1mStep[0m  [24/84], [94mLoss[0m : 1.66430
[1mStep[0m  [32/84], [94mLoss[0m : 1.66783
[1mStep[0m  [40/84], [94mLoss[0m : 1.76497
[1mStep[0m  [48/84], [94mLoss[0m : 1.60603
[1mStep[0m  [56/84], [94mLoss[0m : 1.79861
[1mStep[0m  [64/84], [94mLoss[0m : 1.73070
[1mStep[0m  [72/84], [94mLoss[0m : 1.67255
[1mStep[0m  [80/84], [94mLoss[0m : 1.68608

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.680, [92mTest[0m: 2.472, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.50921
[1mStep[0m  [8/84], [94mLoss[0m : 1.73839
[1mStep[0m  [16/84], [94mLoss[0m : 1.64886
[1mStep[0m  [24/84], [94mLoss[0m : 1.91726
[1mStep[0m  [32/84], [94mLoss[0m : 1.66846
[1mStep[0m  [40/84], [94mLoss[0m : 1.66608
[1mStep[0m  [48/84], [94mLoss[0m : 1.83792
[1mStep[0m  [56/84], [94mLoss[0m : 1.80527
[1mStep[0m  [64/84], [94mLoss[0m : 1.51816
[1mStep[0m  [72/84], [94mLoss[0m : 1.86023
[1mStep[0m  [80/84], [94mLoss[0m : 1.71730

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.691, [92mTest[0m: 2.508, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.55652
[1mStep[0m  [8/84], [94mLoss[0m : 1.72171
[1mStep[0m  [16/84], [94mLoss[0m : 1.79683
[1mStep[0m  [24/84], [94mLoss[0m : 1.47208
[1mStep[0m  [32/84], [94mLoss[0m : 1.61994
[1mStep[0m  [40/84], [94mLoss[0m : 1.46019
[1mStep[0m  [48/84], [94mLoss[0m : 1.47594
[1mStep[0m  [56/84], [94mLoss[0m : 1.91031
[1mStep[0m  [64/84], [94mLoss[0m : 1.73456
[1mStep[0m  [72/84], [94mLoss[0m : 1.46147
[1mStep[0m  [80/84], [94mLoss[0m : 1.53457

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.616, [92mTest[0m: 2.502, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.48325
[1mStep[0m  [8/84], [94mLoss[0m : 1.48029
[1mStep[0m  [16/84], [94mLoss[0m : 1.49302
[1mStep[0m  [24/84], [94mLoss[0m : 1.64561
[1mStep[0m  [32/84], [94mLoss[0m : 1.54108
[1mStep[0m  [40/84], [94mLoss[0m : 1.85302
[1mStep[0m  [48/84], [94mLoss[0m : 1.58421
[1mStep[0m  [56/84], [94mLoss[0m : 1.66188
[1mStep[0m  [64/84], [94mLoss[0m : 1.59510
[1mStep[0m  [72/84], [94mLoss[0m : 1.74862
[1mStep[0m  [80/84], [94mLoss[0m : 1.92692

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.603, [92mTest[0m: 2.549, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.63380
[1mStep[0m  [8/84], [94mLoss[0m : 1.54590
[1mStep[0m  [16/84], [94mLoss[0m : 1.63064
[1mStep[0m  [24/84], [94mLoss[0m : 1.63199
[1mStep[0m  [32/84], [94mLoss[0m : 1.48973
[1mStep[0m  [40/84], [94mLoss[0m : 1.64603
[1mStep[0m  [48/84], [94mLoss[0m : 1.73505
[1mStep[0m  [56/84], [94mLoss[0m : 1.69884
[1mStep[0m  [64/84], [94mLoss[0m : 1.38353
[1mStep[0m  [72/84], [94mLoss[0m : 1.47478
[1mStep[0m  [80/84], [94mLoss[0m : 1.58905

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.592, [92mTest[0m: 2.578, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.46516
[1mStep[0m  [8/84], [94mLoss[0m : 1.64082
[1mStep[0m  [16/84], [94mLoss[0m : 1.73890
[1mStep[0m  [24/84], [94mLoss[0m : 1.30822
[1mStep[0m  [32/84], [94mLoss[0m : 1.54341
[1mStep[0m  [40/84], [94mLoss[0m : 1.65372
[1mStep[0m  [48/84], [94mLoss[0m : 1.48935
[1mStep[0m  [56/84], [94mLoss[0m : 1.38232
[1mStep[0m  [64/84], [94mLoss[0m : 1.70894
[1mStep[0m  [72/84], [94mLoss[0m : 1.66184
[1mStep[0m  [80/84], [94mLoss[0m : 1.38980

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.548, [92mTest[0m: 2.514, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.30137
[1mStep[0m  [8/84], [94mLoss[0m : 1.48860
[1mStep[0m  [16/84], [94mLoss[0m : 1.58620
[1mStep[0m  [24/84], [94mLoss[0m : 1.45225
[1mStep[0m  [32/84], [94mLoss[0m : 1.56899
[1mStep[0m  [40/84], [94mLoss[0m : 1.62747
[1mStep[0m  [48/84], [94mLoss[0m : 1.57695
[1mStep[0m  [56/84], [94mLoss[0m : 1.36934
[1mStep[0m  [64/84], [94mLoss[0m : 1.67134
[1mStep[0m  [72/84], [94mLoss[0m : 1.55956
[1mStep[0m  [80/84], [94mLoss[0m : 1.79771

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.504, [92mTest[0m: 2.544, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.69142
[1mStep[0m  [8/84], [94mLoss[0m : 1.44938
[1mStep[0m  [16/84], [94mLoss[0m : 1.60095
[1mStep[0m  [24/84], [94mLoss[0m : 1.60267
[1mStep[0m  [32/84], [94mLoss[0m : 1.50147
[1mStep[0m  [40/84], [94mLoss[0m : 1.55460
[1mStep[0m  [48/84], [94mLoss[0m : 1.59492
[1mStep[0m  [56/84], [94mLoss[0m : 1.49432
[1mStep[0m  [64/84], [94mLoss[0m : 1.36900
[1mStep[0m  [72/84], [94mLoss[0m : 1.53830
[1mStep[0m  [80/84], [94mLoss[0m : 1.53100

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.506, [92mTest[0m: 2.542, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.58303
[1mStep[0m  [8/84], [94mLoss[0m : 1.42306
[1mStep[0m  [16/84], [94mLoss[0m : 1.29485
[1mStep[0m  [24/84], [94mLoss[0m : 1.66870
[1mStep[0m  [32/84], [94mLoss[0m : 1.24294
[1mStep[0m  [40/84], [94mLoss[0m : 1.57086
[1mStep[0m  [48/84], [94mLoss[0m : 1.23843
[1mStep[0m  [56/84], [94mLoss[0m : 1.58243
[1mStep[0m  [64/84], [94mLoss[0m : 1.46319
[1mStep[0m  [72/84], [94mLoss[0m : 1.49912
[1mStep[0m  [80/84], [94mLoss[0m : 1.57234

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.476, [92mTest[0m: 2.520, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.51969
[1mStep[0m  [8/84], [94mLoss[0m : 1.57021
[1mStep[0m  [16/84], [94mLoss[0m : 1.39412
[1mStep[0m  [24/84], [94mLoss[0m : 1.53600
[1mStep[0m  [32/84], [94mLoss[0m : 1.45661
[1mStep[0m  [40/84], [94mLoss[0m : 1.47712
[1mStep[0m  [48/84], [94mLoss[0m : 1.46332
[1mStep[0m  [56/84], [94mLoss[0m : 1.65717
[1mStep[0m  [64/84], [94mLoss[0m : 1.56767
[1mStep[0m  [72/84], [94mLoss[0m : 1.34546
[1mStep[0m  [80/84], [94mLoss[0m : 1.52315

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.448, [92mTest[0m: 2.527, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.38489
[1mStep[0m  [8/84], [94mLoss[0m : 1.26510
[1mStep[0m  [16/84], [94mLoss[0m : 1.50327
[1mStep[0m  [24/84], [94mLoss[0m : 1.36538
[1mStep[0m  [32/84], [94mLoss[0m : 1.44117
[1mStep[0m  [40/84], [94mLoss[0m : 1.34591
[1mStep[0m  [48/84], [94mLoss[0m : 1.48739
[1mStep[0m  [56/84], [94mLoss[0m : 1.40092
[1mStep[0m  [64/84], [94mLoss[0m : 1.51411
[1mStep[0m  [72/84], [94mLoss[0m : 1.60477
[1mStep[0m  [80/84], [94mLoss[0m : 1.61939

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.435, [92mTest[0m: 2.524, [96mlr[0m: 0.006772500000000002
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 25 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.538
====================================

Phase 2 - Evaluation MAE:  2.5381924510002136
MAE score P1       2.330002
MAE score P2       2.538192
loss               1.435355
learning_rate      0.007525
batch_size              128
hidden_sizes      [250, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping         True
dropout                 0.4
momentum                0.9
weight_decay         0.0001
Name: 11, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 10.48214
[1mStep[0m  [8/84], [94mLoss[0m : 7.62021
[1mStep[0m  [16/84], [94mLoss[0m : 3.16146
[1mStep[0m  [24/84], [94mLoss[0m : 3.12053
[1mStep[0m  [32/84], [94mLoss[0m : 3.27537
[1mStep[0m  [40/84], [94mLoss[0m : 2.44692
[1mStep[0m  [48/84], [94mLoss[0m : 2.58474
[1mStep[0m  [56/84], [94mLoss[0m : 2.59946
[1mStep[0m  [64/84], [94mLoss[0m : 2.69440
[1mStep[0m  [72/84], [94mLoss[0m : 2.69082
[1mStep[0m  [80/84], [94mLoss[0m : 2.42157

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.729, [92mTest[0m: 10.547, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.98406
[1mStep[0m  [8/84], [94mLoss[0m : 2.73854
[1mStep[0m  [16/84], [94mLoss[0m : 3.01956
[1mStep[0m  [24/84], [94mLoss[0m : 2.79773
[1mStep[0m  [32/84], [94mLoss[0m : 2.90812
[1mStep[0m  [40/84], [94mLoss[0m : 2.56389
[1mStep[0m  [48/84], [94mLoss[0m : 2.56425
[1mStep[0m  [56/84], [94mLoss[0m : 2.21215
[1mStep[0m  [64/84], [94mLoss[0m : 2.60562
[1mStep[0m  [72/84], [94mLoss[0m : 2.37455
[1mStep[0m  [80/84], [94mLoss[0m : 2.81827

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.679, [92mTest[0m: 2.370, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.77382
[1mStep[0m  [8/84], [94mLoss[0m : 2.67485
[1mStep[0m  [16/84], [94mLoss[0m : 2.84800
[1mStep[0m  [24/84], [94mLoss[0m : 2.73570
[1mStep[0m  [32/84], [94mLoss[0m : 2.78411
[1mStep[0m  [40/84], [94mLoss[0m : 2.32782
[1mStep[0m  [48/84], [94mLoss[0m : 2.65851
[1mStep[0m  [56/84], [94mLoss[0m : 2.75743
[1mStep[0m  [64/84], [94mLoss[0m : 2.44337
[1mStep[0m  [72/84], [94mLoss[0m : 2.42131
[1mStep[0m  [80/84], [94mLoss[0m : 2.55131

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.603, [92mTest[0m: 2.345, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67556
[1mStep[0m  [8/84], [94mLoss[0m : 2.56610
[1mStep[0m  [16/84], [94mLoss[0m : 2.37401
[1mStep[0m  [24/84], [94mLoss[0m : 2.58672
[1mStep[0m  [32/84], [94mLoss[0m : 2.67955
[1mStep[0m  [40/84], [94mLoss[0m : 2.66758
[1mStep[0m  [48/84], [94mLoss[0m : 2.60239
[1mStep[0m  [56/84], [94mLoss[0m : 2.53483
[1mStep[0m  [64/84], [94mLoss[0m : 2.52140
[1mStep[0m  [72/84], [94mLoss[0m : 2.44753
[1mStep[0m  [80/84], [94mLoss[0m : 2.43806

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.601, [92mTest[0m: 2.366, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.58249
[1mStep[0m  [8/84], [94mLoss[0m : 2.56547
[1mStep[0m  [16/84], [94mLoss[0m : 2.41058
[1mStep[0m  [24/84], [94mLoss[0m : 2.75170
[1mStep[0m  [32/84], [94mLoss[0m : 2.79868
[1mStep[0m  [40/84], [94mLoss[0m : 2.42075
[1mStep[0m  [48/84], [94mLoss[0m : 2.49073
[1mStep[0m  [56/84], [94mLoss[0m : 2.56480
[1mStep[0m  [64/84], [94mLoss[0m : 2.60819
[1mStep[0m  [72/84], [94mLoss[0m : 2.85976
[1mStep[0m  [80/84], [94mLoss[0m : 2.18989

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.590, [92mTest[0m: 2.344, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54035
[1mStep[0m  [8/84], [94mLoss[0m : 2.48369
[1mStep[0m  [16/84], [94mLoss[0m : 2.60269
[1mStep[0m  [24/84], [94mLoss[0m : 2.40051
[1mStep[0m  [32/84], [94mLoss[0m : 2.60208
[1mStep[0m  [40/84], [94mLoss[0m : 2.91079
[1mStep[0m  [48/84], [94mLoss[0m : 2.63471
[1mStep[0m  [56/84], [94mLoss[0m : 3.10233
[1mStep[0m  [64/84], [94mLoss[0m : 2.87012
[1mStep[0m  [72/84], [94mLoss[0m : 2.48083
[1mStep[0m  [80/84], [94mLoss[0m : 2.53598

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.595, [92mTest[0m: 2.347, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.38360
[1mStep[0m  [8/84], [94mLoss[0m : 2.58079
[1mStep[0m  [16/84], [94mLoss[0m : 2.32029
[1mStep[0m  [24/84], [94mLoss[0m : 2.81100
[1mStep[0m  [32/84], [94mLoss[0m : 2.78679
[1mStep[0m  [40/84], [94mLoss[0m : 2.65748
[1mStep[0m  [48/84], [94mLoss[0m : 2.47027
[1mStep[0m  [56/84], [94mLoss[0m : 2.73223
[1mStep[0m  [64/84], [94mLoss[0m : 2.23211
[1mStep[0m  [72/84], [94mLoss[0m : 2.85755
[1mStep[0m  [80/84], [94mLoss[0m : 2.65064

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.557, [92mTest[0m: 2.350, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.62693
[1mStep[0m  [8/84], [94mLoss[0m : 2.57088
[1mStep[0m  [16/84], [94mLoss[0m : 2.57255
[1mStep[0m  [24/84], [94mLoss[0m : 2.44356
[1mStep[0m  [32/84], [94mLoss[0m : 3.04561
[1mStep[0m  [40/84], [94mLoss[0m : 2.24478
[1mStep[0m  [48/84], [94mLoss[0m : 2.66260
[1mStep[0m  [56/84], [94mLoss[0m : 2.55611
[1mStep[0m  [64/84], [94mLoss[0m : 2.14188
[1mStep[0m  [72/84], [94mLoss[0m : 2.62911
[1mStep[0m  [80/84], [94mLoss[0m : 2.55818

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.548, [92mTest[0m: 2.341, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.79468
[1mStep[0m  [8/84], [94mLoss[0m : 2.56427
[1mStep[0m  [16/84], [94mLoss[0m : 2.44324
[1mStep[0m  [24/84], [94mLoss[0m : 2.60859
[1mStep[0m  [32/84], [94mLoss[0m : 2.67191
[1mStep[0m  [40/84], [94mLoss[0m : 2.49788
[1mStep[0m  [48/84], [94mLoss[0m : 2.80943
[1mStep[0m  [56/84], [94mLoss[0m : 2.19546
[1mStep[0m  [64/84], [94mLoss[0m : 2.52581
[1mStep[0m  [72/84], [94mLoss[0m : 2.46887
[1mStep[0m  [80/84], [94mLoss[0m : 2.58876

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.328, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49246
[1mStep[0m  [8/84], [94mLoss[0m : 2.12454
[1mStep[0m  [16/84], [94mLoss[0m : 2.51027
[1mStep[0m  [24/84], [94mLoss[0m : 2.52301
[1mStep[0m  [32/84], [94mLoss[0m : 2.84427
[1mStep[0m  [40/84], [94mLoss[0m : 2.55194
[1mStep[0m  [48/84], [94mLoss[0m : 2.64395
[1mStep[0m  [56/84], [94mLoss[0m : 2.48970
[1mStep[0m  [64/84], [94mLoss[0m : 2.60229
[1mStep[0m  [72/84], [94mLoss[0m : 2.60934
[1mStep[0m  [80/84], [94mLoss[0m : 2.48830

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.547, [92mTest[0m: 2.338, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26318
[1mStep[0m  [8/84], [94mLoss[0m : 2.64500
[1mStep[0m  [16/84], [94mLoss[0m : 2.58488
[1mStep[0m  [24/84], [94mLoss[0m : 2.91928
[1mStep[0m  [32/84], [94mLoss[0m : 2.68768
[1mStep[0m  [40/84], [94mLoss[0m : 2.41454
[1mStep[0m  [48/84], [94mLoss[0m : 2.70314
[1mStep[0m  [56/84], [94mLoss[0m : 2.50959
[1mStep[0m  [64/84], [94mLoss[0m : 2.29132
[1mStep[0m  [72/84], [94mLoss[0m : 3.04157
[1mStep[0m  [80/84], [94mLoss[0m : 2.34838

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.534, [92mTest[0m: 2.340, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.29364
[1mStep[0m  [8/84], [94mLoss[0m : 2.42880
[1mStep[0m  [16/84], [94mLoss[0m : 2.74834
[1mStep[0m  [24/84], [94mLoss[0m : 2.55170
[1mStep[0m  [32/84], [94mLoss[0m : 2.57986
[1mStep[0m  [40/84], [94mLoss[0m : 2.79275
[1mStep[0m  [48/84], [94mLoss[0m : 2.05013
[1mStep[0m  [56/84], [94mLoss[0m : 2.35905
[1mStep[0m  [64/84], [94mLoss[0m : 2.42064
[1mStep[0m  [72/84], [94mLoss[0m : 2.81357
[1mStep[0m  [80/84], [94mLoss[0m : 2.35911

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.334, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.70609
[1mStep[0m  [8/84], [94mLoss[0m : 2.43431
[1mStep[0m  [16/84], [94mLoss[0m : 2.53944
[1mStep[0m  [24/84], [94mLoss[0m : 2.35739
[1mStep[0m  [32/84], [94mLoss[0m : 2.40293
[1mStep[0m  [40/84], [94mLoss[0m : 2.64933
[1mStep[0m  [48/84], [94mLoss[0m : 2.57560
[1mStep[0m  [56/84], [94mLoss[0m : 2.99776
[1mStep[0m  [64/84], [94mLoss[0m : 2.65443
[1mStep[0m  [72/84], [94mLoss[0m : 2.52244
[1mStep[0m  [80/84], [94mLoss[0m : 2.34329

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.526, [92mTest[0m: 2.337, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.69553
[1mStep[0m  [8/84], [94mLoss[0m : 2.53795
[1mStep[0m  [16/84], [94mLoss[0m : 2.40205
[1mStep[0m  [24/84], [94mLoss[0m : 2.51943
[1mStep[0m  [32/84], [94mLoss[0m : 2.31754
[1mStep[0m  [40/84], [94mLoss[0m : 3.11217
[1mStep[0m  [48/84], [94mLoss[0m : 2.44788
[1mStep[0m  [56/84], [94mLoss[0m : 2.53076
[1mStep[0m  [64/84], [94mLoss[0m : 2.35709
[1mStep[0m  [72/84], [94mLoss[0m : 2.63679
[1mStep[0m  [80/84], [94mLoss[0m : 2.43430

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.522, [92mTest[0m: 2.336, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44069
[1mStep[0m  [8/84], [94mLoss[0m : 2.43144
[1mStep[0m  [16/84], [94mLoss[0m : 2.74576
[1mStep[0m  [24/84], [94mLoss[0m : 2.54401
[1mStep[0m  [32/84], [94mLoss[0m : 2.55498
[1mStep[0m  [40/84], [94mLoss[0m : 2.75171
[1mStep[0m  [48/84], [94mLoss[0m : 2.47896
[1mStep[0m  [56/84], [94mLoss[0m : 2.36367
[1mStep[0m  [64/84], [94mLoss[0m : 2.69760
[1mStep[0m  [72/84], [94mLoss[0m : 2.27010
[1mStep[0m  [80/84], [94mLoss[0m : 2.62722

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.326, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35249
[1mStep[0m  [8/84], [94mLoss[0m : 2.51938
[1mStep[0m  [16/84], [94mLoss[0m : 2.83913
[1mStep[0m  [24/84], [94mLoss[0m : 2.47095
[1mStep[0m  [32/84], [94mLoss[0m : 2.42784
[1mStep[0m  [40/84], [94mLoss[0m : 2.37519
[1mStep[0m  [48/84], [94mLoss[0m : 2.70543
[1mStep[0m  [56/84], [94mLoss[0m : 2.90747
[1mStep[0m  [64/84], [94mLoss[0m : 2.59179
[1mStep[0m  [72/84], [94mLoss[0m : 2.44426
[1mStep[0m  [80/84], [94mLoss[0m : 2.31968

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.338, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43939
[1mStep[0m  [8/84], [94mLoss[0m : 2.53316
[1mStep[0m  [16/84], [94mLoss[0m : 2.35396
[1mStep[0m  [24/84], [94mLoss[0m : 2.33261
[1mStep[0m  [32/84], [94mLoss[0m : 2.24425
[1mStep[0m  [40/84], [94mLoss[0m : 2.71647
[1mStep[0m  [48/84], [94mLoss[0m : 2.53712
[1mStep[0m  [56/84], [94mLoss[0m : 2.46114
[1mStep[0m  [64/84], [94mLoss[0m : 2.36683
[1mStep[0m  [72/84], [94mLoss[0m : 2.71050
[1mStep[0m  [80/84], [94mLoss[0m : 1.97261

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.329, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47204
[1mStep[0m  [8/84], [94mLoss[0m : 2.55206
[1mStep[0m  [16/84], [94mLoss[0m : 2.25776
[1mStep[0m  [24/84], [94mLoss[0m : 2.47274
[1mStep[0m  [32/84], [94mLoss[0m : 2.45412
[1mStep[0m  [40/84], [94mLoss[0m : 2.54631
[1mStep[0m  [48/84], [94mLoss[0m : 2.52840
[1mStep[0m  [56/84], [94mLoss[0m : 2.35477
[1mStep[0m  [64/84], [94mLoss[0m : 2.45834
[1mStep[0m  [72/84], [94mLoss[0m : 2.54694
[1mStep[0m  [80/84], [94mLoss[0m : 2.67056

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.327, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.34290
[1mStep[0m  [8/84], [94mLoss[0m : 2.41669
[1mStep[0m  [16/84], [94mLoss[0m : 2.54927
[1mStep[0m  [24/84], [94mLoss[0m : 2.10569
[1mStep[0m  [32/84], [94mLoss[0m : 2.77485
[1mStep[0m  [40/84], [94mLoss[0m : 2.60275
[1mStep[0m  [48/84], [94mLoss[0m : 2.57791
[1mStep[0m  [56/84], [94mLoss[0m : 2.61364
[1mStep[0m  [64/84], [94mLoss[0m : 2.35890
[1mStep[0m  [72/84], [94mLoss[0m : 2.61919
[1mStep[0m  [80/84], [94mLoss[0m : 2.31269

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.335, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33541
[1mStep[0m  [8/84], [94mLoss[0m : 2.54241
[1mStep[0m  [16/84], [94mLoss[0m : 2.34593
[1mStep[0m  [24/84], [94mLoss[0m : 2.23445
[1mStep[0m  [32/84], [94mLoss[0m : 2.74914
[1mStep[0m  [40/84], [94mLoss[0m : 2.65518
[1mStep[0m  [48/84], [94mLoss[0m : 2.74583
[1mStep[0m  [56/84], [94mLoss[0m : 2.28738
[1mStep[0m  [64/84], [94mLoss[0m : 2.56358
[1mStep[0m  [72/84], [94mLoss[0m : 2.36454
[1mStep[0m  [80/84], [94mLoss[0m : 2.49281

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.335, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47693
[1mStep[0m  [8/84], [94mLoss[0m : 2.55407
[1mStep[0m  [16/84], [94mLoss[0m : 2.82989
[1mStep[0m  [24/84], [94mLoss[0m : 2.50514
[1mStep[0m  [32/84], [94mLoss[0m : 2.34835
[1mStep[0m  [40/84], [94mLoss[0m : 1.99551
[1mStep[0m  [48/84], [94mLoss[0m : 2.69017
[1mStep[0m  [56/84], [94mLoss[0m : 2.65856
[1mStep[0m  [64/84], [94mLoss[0m : 2.43056
[1mStep[0m  [72/84], [94mLoss[0m : 2.49310
[1mStep[0m  [80/84], [94mLoss[0m : 2.25761

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.329, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37143
[1mStep[0m  [8/84], [94mLoss[0m : 2.59712
[1mStep[0m  [16/84], [94mLoss[0m : 2.21226
[1mStep[0m  [24/84], [94mLoss[0m : 2.38994
[1mStep[0m  [32/84], [94mLoss[0m : 2.73908
[1mStep[0m  [40/84], [94mLoss[0m : 2.54397
[1mStep[0m  [48/84], [94mLoss[0m : 2.66003
[1mStep[0m  [56/84], [94mLoss[0m : 2.82864
[1mStep[0m  [64/84], [94mLoss[0m : 2.36855
[1mStep[0m  [72/84], [94mLoss[0m : 2.35807
[1mStep[0m  [80/84], [94mLoss[0m : 2.41698

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.328, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.24959
[1mStep[0m  [8/84], [94mLoss[0m : 2.56109
[1mStep[0m  [16/84], [94mLoss[0m : 2.29483
[1mStep[0m  [24/84], [94mLoss[0m : 2.14926
[1mStep[0m  [32/84], [94mLoss[0m : 2.47216
[1mStep[0m  [40/84], [94mLoss[0m : 2.19570
[1mStep[0m  [48/84], [94mLoss[0m : 2.21752
[1mStep[0m  [56/84], [94mLoss[0m : 2.57319
[1mStep[0m  [64/84], [94mLoss[0m : 2.74714
[1mStep[0m  [72/84], [94mLoss[0m : 2.57276
[1mStep[0m  [80/84], [94mLoss[0m : 2.62184

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.324, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67473
[1mStep[0m  [8/84], [94mLoss[0m : 2.31122
[1mStep[0m  [16/84], [94mLoss[0m : 2.40460
[1mStep[0m  [24/84], [94mLoss[0m : 2.36433
[1mStep[0m  [32/84], [94mLoss[0m : 2.53921
[1mStep[0m  [40/84], [94mLoss[0m : 2.37738
[1mStep[0m  [48/84], [94mLoss[0m : 2.51750
[1mStep[0m  [56/84], [94mLoss[0m : 2.47728
[1mStep[0m  [64/84], [94mLoss[0m : 2.20819
[1mStep[0m  [72/84], [94mLoss[0m : 2.39263
[1mStep[0m  [80/84], [94mLoss[0m : 2.17726

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.331, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.22596
[1mStep[0m  [8/84], [94mLoss[0m : 2.26328
[1mStep[0m  [16/84], [94mLoss[0m : 2.46151
[1mStep[0m  [24/84], [94mLoss[0m : 2.03393
[1mStep[0m  [32/84], [94mLoss[0m : 2.42229
[1mStep[0m  [40/84], [94mLoss[0m : 2.29641
[1mStep[0m  [48/84], [94mLoss[0m : 2.31833
[1mStep[0m  [56/84], [94mLoss[0m : 2.56856
[1mStep[0m  [64/84], [94mLoss[0m : 2.15948
[1mStep[0m  [72/84], [94mLoss[0m : 2.31819
[1mStep[0m  [80/84], [94mLoss[0m : 2.53818

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.332, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49820
[1mStep[0m  [8/84], [94mLoss[0m : 2.26482
[1mStep[0m  [16/84], [94mLoss[0m : 2.43510
[1mStep[0m  [24/84], [94mLoss[0m : 2.50694
[1mStep[0m  [32/84], [94mLoss[0m : 2.24119
[1mStep[0m  [40/84], [94mLoss[0m : 2.43448
[1mStep[0m  [48/84], [94mLoss[0m : 2.25835
[1mStep[0m  [56/84], [94mLoss[0m : 2.23620
[1mStep[0m  [64/84], [94mLoss[0m : 2.74522
[1mStep[0m  [72/84], [94mLoss[0m : 2.33650
[1mStep[0m  [80/84], [94mLoss[0m : 2.52202

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.332, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.09476
[1mStep[0m  [8/84], [94mLoss[0m : 2.23462
[1mStep[0m  [16/84], [94mLoss[0m : 2.45053
[1mStep[0m  [24/84], [94mLoss[0m : 2.68583
[1mStep[0m  [32/84], [94mLoss[0m : 2.50672
[1mStep[0m  [40/84], [94mLoss[0m : 2.51041
[1mStep[0m  [48/84], [94mLoss[0m : 2.23760
[1mStep[0m  [56/84], [94mLoss[0m : 2.62753
[1mStep[0m  [64/84], [94mLoss[0m : 2.57791
[1mStep[0m  [72/84], [94mLoss[0m : 2.80629
[1mStep[0m  [80/84], [94mLoss[0m : 2.35176

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.327, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45051
[1mStep[0m  [8/84], [94mLoss[0m : 2.44954
[1mStep[0m  [16/84], [94mLoss[0m : 2.10444
[1mStep[0m  [24/84], [94mLoss[0m : 2.13893
[1mStep[0m  [32/84], [94mLoss[0m : 2.44337
[1mStep[0m  [40/84], [94mLoss[0m : 2.62138
[1mStep[0m  [48/84], [94mLoss[0m : 2.54288
[1mStep[0m  [56/84], [94mLoss[0m : 2.54709
[1mStep[0m  [64/84], [94mLoss[0m : 2.71833
[1mStep[0m  [72/84], [94mLoss[0m : 2.71196
[1mStep[0m  [80/84], [94mLoss[0m : 2.26565

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.336, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.57798
[1mStep[0m  [8/84], [94mLoss[0m : 2.50769
[1mStep[0m  [16/84], [94mLoss[0m : 2.51218
[1mStep[0m  [24/84], [94mLoss[0m : 2.09617
[1mStep[0m  [32/84], [94mLoss[0m : 2.36198
[1mStep[0m  [40/84], [94mLoss[0m : 2.42650
[1mStep[0m  [48/84], [94mLoss[0m : 2.40660
[1mStep[0m  [56/84], [94mLoss[0m : 2.64493
[1mStep[0m  [64/84], [94mLoss[0m : 2.76909
[1mStep[0m  [72/84], [94mLoss[0m : 2.65800
[1mStep[0m  [80/84], [94mLoss[0m : 2.69539

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.332, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.32648
[1mStep[0m  [8/84], [94mLoss[0m : 2.29656
[1mStep[0m  [16/84], [94mLoss[0m : 2.38980
[1mStep[0m  [24/84], [94mLoss[0m : 2.56267
[1mStep[0m  [32/84], [94mLoss[0m : 2.18598
[1mStep[0m  [40/84], [94mLoss[0m : 2.27076
[1mStep[0m  [48/84], [94mLoss[0m : 2.63444
[1mStep[0m  [56/84], [94mLoss[0m : 2.28444
[1mStep[0m  [64/84], [94mLoss[0m : 2.23543
[1mStep[0m  [72/84], [94mLoss[0m : 2.50776
[1mStep[0m  [80/84], [94mLoss[0m : 2.28117

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.331, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.328
====================================

Phase 1 - Evaluation MAE:  2.328434092657907
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 2.59581
[1mStep[0m  [8/84], [94mLoss[0m : 2.73286
[1mStep[0m  [16/84], [94mLoss[0m : 2.45929
[1mStep[0m  [24/84], [94mLoss[0m : 2.26442
[1mStep[0m  [32/84], [94mLoss[0m : 2.59172
[1mStep[0m  [40/84], [94mLoss[0m : 2.26910
[1mStep[0m  [48/84], [94mLoss[0m : 2.75510
[1mStep[0m  [56/84], [94mLoss[0m : 2.66982
[1mStep[0m  [64/84], [94mLoss[0m : 2.21397
[1mStep[0m  [72/84], [94mLoss[0m : 2.32421
[1mStep[0m  [80/84], [94mLoss[0m : 2.52748

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.321, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.38190
[1mStep[0m  [8/84], [94mLoss[0m : 2.59277
[1mStep[0m  [16/84], [94mLoss[0m : 2.34901
[1mStep[0m  [24/84], [94mLoss[0m : 2.43141
[1mStep[0m  [32/84], [94mLoss[0m : 2.36795
[1mStep[0m  [40/84], [94mLoss[0m : 2.07495
[1mStep[0m  [48/84], [94mLoss[0m : 2.24589
[1mStep[0m  [56/84], [94mLoss[0m : 2.20654
[1mStep[0m  [64/84], [94mLoss[0m : 2.17366
[1mStep[0m  [72/84], [94mLoss[0m : 2.25558
[1mStep[0m  [80/84], [94mLoss[0m : 2.43944

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.377, [92mTest[0m: 2.392, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37866
[1mStep[0m  [8/84], [94mLoss[0m : 2.29580
[1mStep[0m  [16/84], [94mLoss[0m : 2.60768
[1mStep[0m  [24/84], [94mLoss[0m : 2.43530
[1mStep[0m  [32/84], [94mLoss[0m : 2.09348
[1mStep[0m  [40/84], [94mLoss[0m : 2.25331
[1mStep[0m  [48/84], [94mLoss[0m : 2.18866
[1mStep[0m  [56/84], [94mLoss[0m : 2.31655
[1mStep[0m  [64/84], [94mLoss[0m : 2.34716
[1mStep[0m  [72/84], [94mLoss[0m : 2.28059
[1mStep[0m  [80/84], [94mLoss[0m : 2.28026

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.304, [92mTest[0m: 2.350, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50573
[1mStep[0m  [8/84], [94mLoss[0m : 2.14048
[1mStep[0m  [16/84], [94mLoss[0m : 2.12769
[1mStep[0m  [24/84], [94mLoss[0m : 2.40317
[1mStep[0m  [32/84], [94mLoss[0m : 2.20992
[1mStep[0m  [40/84], [94mLoss[0m : 2.09869
[1mStep[0m  [48/84], [94mLoss[0m : 2.27093
[1mStep[0m  [56/84], [94mLoss[0m : 2.66792
[1mStep[0m  [64/84], [94mLoss[0m : 2.15746
[1mStep[0m  [72/84], [94mLoss[0m : 2.47764
[1mStep[0m  [80/84], [94mLoss[0m : 2.23772

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.262, [92mTest[0m: 2.342, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.05623
[1mStep[0m  [8/84], [94mLoss[0m : 1.97449
[1mStep[0m  [16/84], [94mLoss[0m : 2.27446
[1mStep[0m  [24/84], [94mLoss[0m : 2.34474
[1mStep[0m  [32/84], [94mLoss[0m : 1.97343
[1mStep[0m  [40/84], [94mLoss[0m : 2.25978
[1mStep[0m  [48/84], [94mLoss[0m : 1.99078
[1mStep[0m  [56/84], [94mLoss[0m : 2.43960
[1mStep[0m  [64/84], [94mLoss[0m : 2.01390
[1mStep[0m  [72/84], [94mLoss[0m : 2.30635
[1mStep[0m  [80/84], [94mLoss[0m : 2.25757

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.168, [92mTest[0m: 2.377, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.90825
[1mStep[0m  [8/84], [94mLoss[0m : 2.29627
[1mStep[0m  [16/84], [94mLoss[0m : 2.00926
[1mStep[0m  [24/84], [94mLoss[0m : 2.23630
[1mStep[0m  [32/84], [94mLoss[0m : 1.83910
[1mStep[0m  [40/84], [94mLoss[0m : 2.12504
[1mStep[0m  [48/84], [94mLoss[0m : 2.05273
[1mStep[0m  [56/84], [94mLoss[0m : 2.24490
[1mStep[0m  [64/84], [94mLoss[0m : 1.99264
[1mStep[0m  [72/84], [94mLoss[0m : 2.19504
[1mStep[0m  [80/84], [94mLoss[0m : 2.05078

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.132, [92mTest[0m: 2.434, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.94730
[1mStep[0m  [8/84], [94mLoss[0m : 1.82149
[1mStep[0m  [16/84], [94mLoss[0m : 2.25221
[1mStep[0m  [24/84], [94mLoss[0m : 2.06706
[1mStep[0m  [32/84], [94mLoss[0m : 2.08831
[1mStep[0m  [40/84], [94mLoss[0m : 2.14230
[1mStep[0m  [48/84], [94mLoss[0m : 2.22282
[1mStep[0m  [56/84], [94mLoss[0m : 2.66557
[1mStep[0m  [64/84], [94mLoss[0m : 1.80946
[1mStep[0m  [72/84], [94mLoss[0m : 1.99174
[1mStep[0m  [80/84], [94mLoss[0m : 1.94802

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.044, [92mTest[0m: 2.416, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.88593
[1mStep[0m  [8/84], [94mLoss[0m : 2.12472
[1mStep[0m  [16/84], [94mLoss[0m : 2.00042
[1mStep[0m  [24/84], [94mLoss[0m : 1.92068
[1mStep[0m  [32/84], [94mLoss[0m : 2.13644
[1mStep[0m  [40/84], [94mLoss[0m : 2.13008
[1mStep[0m  [48/84], [94mLoss[0m : 2.16579
[1mStep[0m  [56/84], [94mLoss[0m : 2.11870
[1mStep[0m  [64/84], [94mLoss[0m : 2.29440
[1mStep[0m  [72/84], [94mLoss[0m : 2.19743
[1mStep[0m  [80/84], [94mLoss[0m : 2.15145

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.028, [92mTest[0m: 2.432, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.00147
[1mStep[0m  [8/84], [94mLoss[0m : 2.05591
[1mStep[0m  [16/84], [94mLoss[0m : 2.01892
[1mStep[0m  [24/84], [94mLoss[0m : 1.67835
[1mStep[0m  [32/84], [94mLoss[0m : 1.82124
[1mStep[0m  [40/84], [94mLoss[0m : 2.06463
[1mStep[0m  [48/84], [94mLoss[0m : 2.34324
[1mStep[0m  [56/84], [94mLoss[0m : 2.22248
[1mStep[0m  [64/84], [94mLoss[0m : 1.96785
[1mStep[0m  [72/84], [94mLoss[0m : 2.19490
[1mStep[0m  [80/84], [94mLoss[0m : 1.79927

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.990, [92mTest[0m: 2.401, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.80182
[1mStep[0m  [8/84], [94mLoss[0m : 2.03407
[1mStep[0m  [16/84], [94mLoss[0m : 1.91411
[1mStep[0m  [24/84], [94mLoss[0m : 2.07760
[1mStep[0m  [32/84], [94mLoss[0m : 1.97294
[1mStep[0m  [40/84], [94mLoss[0m : 1.91416
[1mStep[0m  [48/84], [94mLoss[0m : 2.07931
[1mStep[0m  [56/84], [94mLoss[0m : 1.85192
[1mStep[0m  [64/84], [94mLoss[0m : 2.07989
[1mStep[0m  [72/84], [94mLoss[0m : 1.99941
[1mStep[0m  [80/84], [94mLoss[0m : 1.97453

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.913, [92mTest[0m: 2.442, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.77077
[1mStep[0m  [8/84], [94mLoss[0m : 2.01165
[1mStep[0m  [16/84], [94mLoss[0m : 1.91985
[1mStep[0m  [24/84], [94mLoss[0m : 2.00746
[1mStep[0m  [32/84], [94mLoss[0m : 1.82455
[1mStep[0m  [40/84], [94mLoss[0m : 1.80923
[1mStep[0m  [48/84], [94mLoss[0m : 1.90068
[1mStep[0m  [56/84], [94mLoss[0m : 2.06681
[1mStep[0m  [64/84], [94mLoss[0m : 1.93736
[1mStep[0m  [72/84], [94mLoss[0m : 2.35583
[1mStep[0m  [80/84], [94mLoss[0m : 1.97403

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.898, [92mTest[0m: 2.418, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.89994
[1mStep[0m  [8/84], [94mLoss[0m : 1.86180
[1mStep[0m  [16/84], [94mLoss[0m : 1.68359
[1mStep[0m  [24/84], [94mLoss[0m : 1.98239
[1mStep[0m  [32/84], [94mLoss[0m : 1.94964
[1mStep[0m  [40/84], [94mLoss[0m : 1.75983
[1mStep[0m  [48/84], [94mLoss[0m : 1.89611
[1mStep[0m  [56/84], [94mLoss[0m : 1.89245
[1mStep[0m  [64/84], [94mLoss[0m : 1.91214
[1mStep[0m  [72/84], [94mLoss[0m : 1.91294
[1mStep[0m  [80/84], [94mLoss[0m : 1.94596

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.868, [92mTest[0m: 2.462, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.76826
[1mStep[0m  [8/84], [94mLoss[0m : 1.97037
[1mStep[0m  [16/84], [94mLoss[0m : 1.80533
[1mStep[0m  [24/84], [94mLoss[0m : 1.88089
[1mStep[0m  [32/84], [94mLoss[0m : 1.73514
[1mStep[0m  [40/84], [94mLoss[0m : 1.95194
[1mStep[0m  [48/84], [94mLoss[0m : 1.69107
[1mStep[0m  [56/84], [94mLoss[0m : 1.54444
[1mStep[0m  [64/84], [94mLoss[0m : 1.92284
[1mStep[0m  [72/84], [94mLoss[0m : 1.95850
[1mStep[0m  [80/84], [94mLoss[0m : 1.86877

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.811, [92mTest[0m: 2.509, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.50890
[1mStep[0m  [8/84], [94mLoss[0m : 1.91786
[1mStep[0m  [16/84], [94mLoss[0m : 1.83524
[1mStep[0m  [24/84], [94mLoss[0m : 1.96255
[1mStep[0m  [32/84], [94mLoss[0m : 1.74276
[1mStep[0m  [40/84], [94mLoss[0m : 1.92917
[1mStep[0m  [48/84], [94mLoss[0m : 1.63294
[1mStep[0m  [56/84], [94mLoss[0m : 1.59176
[1mStep[0m  [64/84], [94mLoss[0m : 1.66714
[1mStep[0m  [72/84], [94mLoss[0m : 1.82146
[1mStep[0m  [80/84], [94mLoss[0m : 1.85808

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.796, [92mTest[0m: 2.437, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.50039
[1mStep[0m  [8/84], [94mLoss[0m : 1.72821
[1mStep[0m  [16/84], [94mLoss[0m : 1.44535
[1mStep[0m  [24/84], [94mLoss[0m : 1.83560
[1mStep[0m  [32/84], [94mLoss[0m : 1.61167
[1mStep[0m  [40/84], [94mLoss[0m : 1.77748
[1mStep[0m  [48/84], [94mLoss[0m : 1.98566
[1mStep[0m  [56/84], [94mLoss[0m : 1.61433
[1mStep[0m  [64/84], [94mLoss[0m : 1.78918
[1mStep[0m  [72/84], [94mLoss[0m : 1.71258
[1mStep[0m  [80/84], [94mLoss[0m : 1.86023

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.755, [92mTest[0m: 2.498, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.72052
[1mStep[0m  [8/84], [94mLoss[0m : 1.71156
[1mStep[0m  [16/84], [94mLoss[0m : 1.73473
[1mStep[0m  [24/84], [94mLoss[0m : 1.85858
[1mStep[0m  [32/84], [94mLoss[0m : 1.72196
[1mStep[0m  [40/84], [94mLoss[0m : 1.53622
[1mStep[0m  [48/84], [94mLoss[0m : 2.01408
[1mStep[0m  [56/84], [94mLoss[0m : 1.92101
[1mStep[0m  [64/84], [94mLoss[0m : 1.60243
[1mStep[0m  [72/84], [94mLoss[0m : 1.87802
[1mStep[0m  [80/84], [94mLoss[0m : 1.59177

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.740, [92mTest[0m: 2.503, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.66557
[1mStep[0m  [8/84], [94mLoss[0m : 1.95129
[1mStep[0m  [16/84], [94mLoss[0m : 1.53836
[1mStep[0m  [24/84], [94mLoss[0m : 1.75998
[1mStep[0m  [32/84], [94mLoss[0m : 1.89180
[1mStep[0m  [40/84], [94mLoss[0m : 1.77040
[1mStep[0m  [48/84], [94mLoss[0m : 1.72307
[1mStep[0m  [56/84], [94mLoss[0m : 1.82227
[1mStep[0m  [64/84], [94mLoss[0m : 1.53735
[1mStep[0m  [72/84], [94mLoss[0m : 1.64786
[1mStep[0m  [80/84], [94mLoss[0m : 1.91578

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.714, [92mTest[0m: 2.473, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.52715
[1mStep[0m  [8/84], [94mLoss[0m : 1.75186
[1mStep[0m  [16/84], [94mLoss[0m : 1.66584
[1mStep[0m  [24/84], [94mLoss[0m : 1.59949
[1mStep[0m  [32/84], [94mLoss[0m : 1.79282
[1mStep[0m  [40/84], [94mLoss[0m : 1.43093
[1mStep[0m  [48/84], [94mLoss[0m : 1.91497
[1mStep[0m  [56/84], [94mLoss[0m : 1.74156
[1mStep[0m  [64/84], [94mLoss[0m : 1.77044
[1mStep[0m  [72/84], [94mLoss[0m : 1.80978
[1mStep[0m  [80/84], [94mLoss[0m : 1.62566

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.664, [92mTest[0m: 2.516, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.77679
[1mStep[0m  [8/84], [94mLoss[0m : 1.67459
[1mStep[0m  [16/84], [94mLoss[0m : 1.33382
[1mStep[0m  [24/84], [94mLoss[0m : 1.45943
[1mStep[0m  [32/84], [94mLoss[0m : 1.54512
[1mStep[0m  [40/84], [94mLoss[0m : 1.70842
[1mStep[0m  [48/84], [94mLoss[0m : 1.76169
[1mStep[0m  [56/84], [94mLoss[0m : 1.95031
[1mStep[0m  [64/84], [94mLoss[0m : 1.58496
[1mStep[0m  [72/84], [94mLoss[0m : 1.60164
[1mStep[0m  [80/84], [94mLoss[0m : 1.77237

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.644, [92mTest[0m: 2.527, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.64195
[1mStep[0m  [8/84], [94mLoss[0m : 1.38922
[1mStep[0m  [16/84], [94mLoss[0m : 1.70781
[1mStep[0m  [24/84], [94mLoss[0m : 1.43761
[1mStep[0m  [32/84], [94mLoss[0m : 1.64275
[1mStep[0m  [40/84], [94mLoss[0m : 1.66135
[1mStep[0m  [48/84], [94mLoss[0m : 1.51804
[1mStep[0m  [56/84], [94mLoss[0m : 1.57809
[1mStep[0m  [64/84], [94mLoss[0m : 1.60042
[1mStep[0m  [72/84], [94mLoss[0m : 1.75563
[1mStep[0m  [80/84], [94mLoss[0m : 1.70687

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.642, [92mTest[0m: 2.513, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.60453
[1mStep[0m  [8/84], [94mLoss[0m : 1.52135
[1mStep[0m  [16/84], [94mLoss[0m : 1.56313
[1mStep[0m  [24/84], [94mLoss[0m : 1.51473
[1mStep[0m  [32/84], [94mLoss[0m : 1.60481
[1mStep[0m  [40/84], [94mLoss[0m : 1.58538
[1mStep[0m  [48/84], [94mLoss[0m : 1.46643
[1mStep[0m  [56/84], [94mLoss[0m : 1.68696
[1mStep[0m  [64/84], [94mLoss[0m : 1.64063
[1mStep[0m  [72/84], [94mLoss[0m : 1.54074
[1mStep[0m  [80/84], [94mLoss[0m : 1.57975

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.594, [92mTest[0m: 2.471, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.53830
[1mStep[0m  [8/84], [94mLoss[0m : 1.67531
[1mStep[0m  [16/84], [94mLoss[0m : 1.40878
[1mStep[0m  [24/84], [94mLoss[0m : 1.52707
[1mStep[0m  [32/84], [94mLoss[0m : 1.54898
[1mStep[0m  [40/84], [94mLoss[0m : 1.53678
[1mStep[0m  [48/84], [94mLoss[0m : 1.75240
[1mStep[0m  [56/84], [94mLoss[0m : 1.47643
[1mStep[0m  [64/84], [94mLoss[0m : 1.41210
[1mStep[0m  [72/84], [94mLoss[0m : 1.41935
[1mStep[0m  [80/84], [94mLoss[0m : 1.53141

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.564, [92mTest[0m: 2.473, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.25681
[1mStep[0m  [8/84], [94mLoss[0m : 1.54361
[1mStep[0m  [16/84], [94mLoss[0m : 1.59864
[1mStep[0m  [24/84], [94mLoss[0m : 1.68530
[1mStep[0m  [32/84], [94mLoss[0m : 1.57227
[1mStep[0m  [40/84], [94mLoss[0m : 1.49242
[1mStep[0m  [48/84], [94mLoss[0m : 1.55177
[1mStep[0m  [56/84], [94mLoss[0m : 1.49589
[1mStep[0m  [64/84], [94mLoss[0m : 1.49547
[1mStep[0m  [72/84], [94mLoss[0m : 1.62224
[1mStep[0m  [80/84], [94mLoss[0m : 1.44141

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.552, [92mTest[0m: 2.542, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.58879
[1mStep[0m  [8/84], [94mLoss[0m : 1.70640
[1mStep[0m  [16/84], [94mLoss[0m : 1.33647
[1mStep[0m  [24/84], [94mLoss[0m : 1.42579
[1mStep[0m  [32/84], [94mLoss[0m : 1.34016
[1mStep[0m  [40/84], [94mLoss[0m : 1.57036
[1mStep[0m  [48/84], [94mLoss[0m : 1.31231
[1mStep[0m  [56/84], [94mLoss[0m : 1.48388
[1mStep[0m  [64/84], [94mLoss[0m : 1.47357
[1mStep[0m  [72/84], [94mLoss[0m : 1.47451
[1mStep[0m  [80/84], [94mLoss[0m : 1.37724

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.521, [92mTest[0m: 2.572, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.56295
[1mStep[0m  [8/84], [94mLoss[0m : 1.29329
[1mStep[0m  [16/84], [94mLoss[0m : 1.53136
[1mStep[0m  [24/84], [94mLoss[0m : 1.54040
[1mStep[0m  [32/84], [94mLoss[0m : 1.41845
[1mStep[0m  [40/84], [94mLoss[0m : 1.64461
[1mStep[0m  [48/84], [94mLoss[0m : 1.53548
[1mStep[0m  [56/84], [94mLoss[0m : 1.59216
[1mStep[0m  [64/84], [94mLoss[0m : 1.56327
[1mStep[0m  [72/84], [94mLoss[0m : 1.73207
[1mStep[0m  [80/84], [94mLoss[0m : 1.61539

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.511, [92mTest[0m: 2.531, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.32708
[1mStep[0m  [8/84], [94mLoss[0m : 1.46338
[1mStep[0m  [16/84], [94mLoss[0m : 1.44737
[1mStep[0m  [24/84], [94mLoss[0m : 1.34791
[1mStep[0m  [32/84], [94mLoss[0m : 1.61015
[1mStep[0m  [40/84], [94mLoss[0m : 1.40364
[1mStep[0m  [48/84], [94mLoss[0m : 1.68122
[1mStep[0m  [56/84], [94mLoss[0m : 1.39547
[1mStep[0m  [64/84], [94mLoss[0m : 1.57333
[1mStep[0m  [72/84], [94mLoss[0m : 1.48302
[1mStep[0m  [80/84], [94mLoss[0m : 1.56739

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.479, [92mTest[0m: 2.516, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.54410
[1mStep[0m  [8/84], [94mLoss[0m : 1.66554
[1mStep[0m  [16/84], [94mLoss[0m : 1.41262
[1mStep[0m  [24/84], [94mLoss[0m : 1.31724
[1mStep[0m  [32/84], [94mLoss[0m : 1.32760
[1mStep[0m  [40/84], [94mLoss[0m : 1.35943
[1mStep[0m  [48/84], [94mLoss[0m : 1.46036
[1mStep[0m  [56/84], [94mLoss[0m : 1.19537
[1mStep[0m  [64/84], [94mLoss[0m : 1.58388
[1mStep[0m  [72/84], [94mLoss[0m : 1.57144
[1mStep[0m  [80/84], [94mLoss[0m : 1.52521

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.473, [92mTest[0m: 2.510, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.35631
[1mStep[0m  [8/84], [94mLoss[0m : 1.33902
[1mStep[0m  [16/84], [94mLoss[0m : 1.35752
[1mStep[0m  [24/84], [94mLoss[0m : 1.33857
[1mStep[0m  [32/84], [94mLoss[0m : 1.32240
[1mStep[0m  [40/84], [94mLoss[0m : 1.42310
[1mStep[0m  [48/84], [94mLoss[0m : 1.49926
[1mStep[0m  [56/84], [94mLoss[0m : 1.66115
[1mStep[0m  [64/84], [94mLoss[0m : 1.57035
[1mStep[0m  [72/84], [94mLoss[0m : 1.48516
[1mStep[0m  [80/84], [94mLoss[0m : 1.53880

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.468, [92mTest[0m: 2.502, [96mlr[0m: 0.006772500000000002
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 27 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.475
====================================

Phase 2 - Evaluation MAE:  2.475254237651825
MAE score P1       2.328434
MAE score P2       2.475254
loss                  1.468
learning_rate      0.007525
batch_size              128
hidden_sizes      [250, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping         True
dropout                 0.4
momentum                0.9
weight_decay          0.001
Name: 12, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 10.88619
[1mStep[0m  [4/42], [94mLoss[0m : 10.68000
[1mStep[0m  [8/42], [94mLoss[0m : 9.31425
[1mStep[0m  [12/42], [94mLoss[0m : 8.61248
[1mStep[0m  [16/42], [94mLoss[0m : 6.98032
[1mStep[0m  [20/42], [94mLoss[0m : 6.76091
[1mStep[0m  [24/42], [94mLoss[0m : 5.01073
[1mStep[0m  [28/42], [94mLoss[0m : 5.03478
[1mStep[0m  [32/42], [94mLoss[0m : 4.03324
[1mStep[0m  [36/42], [94mLoss[0m : 3.22350
[1mStep[0m  [40/42], [94mLoss[0m : 3.15809

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.654, [92mTest[0m: 10.945, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.15378
[1mStep[0m  [4/42], [94mLoss[0m : 2.90197
[1mStep[0m  [8/42], [94mLoss[0m : 3.13124
[1mStep[0m  [12/42], [94mLoss[0m : 2.82561
[1mStep[0m  [16/42], [94mLoss[0m : 2.60782
[1mStep[0m  [20/42], [94mLoss[0m : 2.73442
[1mStep[0m  [24/42], [94mLoss[0m : 2.60575
[1mStep[0m  [28/42], [94mLoss[0m : 2.63050
[1mStep[0m  [32/42], [94mLoss[0m : 2.54613
[1mStep[0m  [36/42], [94mLoss[0m : 2.58423
[1mStep[0m  [40/42], [94mLoss[0m : 2.90083

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.762, [92mTest[0m: 3.863, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64779
[1mStep[0m  [4/42], [94mLoss[0m : 2.42222
[1mStep[0m  [8/42], [94mLoss[0m : 2.87293
[1mStep[0m  [12/42], [94mLoss[0m : 2.52746
[1mStep[0m  [16/42], [94mLoss[0m : 2.81932
[1mStep[0m  [20/42], [94mLoss[0m : 2.54262
[1mStep[0m  [24/42], [94mLoss[0m : 2.85885
[1mStep[0m  [28/42], [94mLoss[0m : 2.51240
[1mStep[0m  [32/42], [94mLoss[0m : 2.98090
[1mStep[0m  [36/42], [94mLoss[0m : 2.64256
[1mStep[0m  [40/42], [94mLoss[0m : 2.32763

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.635, [92mTest[0m: 2.639, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.86827
[1mStep[0m  [4/42], [94mLoss[0m : 2.75765
[1mStep[0m  [8/42], [94mLoss[0m : 2.71770
[1mStep[0m  [12/42], [94mLoss[0m : 2.52182
[1mStep[0m  [16/42], [94mLoss[0m : 2.71164
[1mStep[0m  [20/42], [94mLoss[0m : 2.63911
[1mStep[0m  [24/42], [94mLoss[0m : 2.65026
[1mStep[0m  [28/42], [94mLoss[0m : 2.72736
[1mStep[0m  [32/42], [94mLoss[0m : 2.42312
[1mStep[0m  [36/42], [94mLoss[0m : 2.86246
[1mStep[0m  [40/42], [94mLoss[0m : 2.47527

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.609, [92mTest[0m: 2.622, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62213
[1mStep[0m  [4/42], [94mLoss[0m : 2.63164
[1mStep[0m  [8/42], [94mLoss[0m : 2.54858
[1mStep[0m  [12/42], [94mLoss[0m : 2.31242
[1mStep[0m  [16/42], [94mLoss[0m : 2.52161
[1mStep[0m  [20/42], [94mLoss[0m : 2.55547
[1mStep[0m  [24/42], [94mLoss[0m : 2.55736
[1mStep[0m  [28/42], [94mLoss[0m : 2.54687
[1mStep[0m  [32/42], [94mLoss[0m : 2.62182
[1mStep[0m  [36/42], [94mLoss[0m : 2.60446
[1mStep[0m  [40/42], [94mLoss[0m : 2.34652

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.570, [92mTest[0m: 2.453, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46774
[1mStep[0m  [4/42], [94mLoss[0m : 2.26122
[1mStep[0m  [8/42], [94mLoss[0m : 2.58246
[1mStep[0m  [12/42], [94mLoss[0m : 2.66713
[1mStep[0m  [16/42], [94mLoss[0m : 2.48843
[1mStep[0m  [20/42], [94mLoss[0m : 2.52306
[1mStep[0m  [24/42], [94mLoss[0m : 2.78573
[1mStep[0m  [28/42], [94mLoss[0m : 2.76207
[1mStep[0m  [32/42], [94mLoss[0m : 2.58917
[1mStep[0m  [36/42], [94mLoss[0m : 2.47669
[1mStep[0m  [40/42], [94mLoss[0m : 2.41731

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.547, [92mTest[0m: 2.483, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62141
[1mStep[0m  [4/42], [94mLoss[0m : 2.42971
[1mStep[0m  [8/42], [94mLoss[0m : 2.70521
[1mStep[0m  [12/42], [94mLoss[0m : 2.79873
[1mStep[0m  [16/42], [94mLoss[0m : 2.25614
[1mStep[0m  [20/42], [94mLoss[0m : 2.42560
[1mStep[0m  [24/42], [94mLoss[0m : 2.47045
[1mStep[0m  [28/42], [94mLoss[0m : 2.51601
[1mStep[0m  [32/42], [94mLoss[0m : 2.59652
[1mStep[0m  [36/42], [94mLoss[0m : 2.69016
[1mStep[0m  [40/42], [94mLoss[0m : 2.57928

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.542, [92mTest[0m: 2.489, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43006
[1mStep[0m  [4/42], [94mLoss[0m : 2.58238
[1mStep[0m  [8/42], [94mLoss[0m : 2.53729
[1mStep[0m  [12/42], [94mLoss[0m : 2.55636
[1mStep[0m  [16/42], [94mLoss[0m : 2.30240
[1mStep[0m  [20/42], [94mLoss[0m : 2.61802
[1mStep[0m  [24/42], [94mLoss[0m : 2.58787
[1mStep[0m  [28/42], [94mLoss[0m : 2.53837
[1mStep[0m  [32/42], [94mLoss[0m : 2.43909
[1mStep[0m  [36/42], [94mLoss[0m : 2.65490
[1mStep[0m  [40/42], [94mLoss[0m : 2.55028

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.534, [92mTest[0m: 2.448, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.68581
[1mStep[0m  [4/42], [94mLoss[0m : 2.33596
[1mStep[0m  [8/42], [94mLoss[0m : 2.36259
[1mStep[0m  [12/42], [94mLoss[0m : 2.41254
[1mStep[0m  [16/42], [94mLoss[0m : 2.42601
[1mStep[0m  [20/42], [94mLoss[0m : 2.50401
[1mStep[0m  [24/42], [94mLoss[0m : 2.52356
[1mStep[0m  [28/42], [94mLoss[0m : 2.43154
[1mStep[0m  [32/42], [94mLoss[0m : 2.43068
[1mStep[0m  [36/42], [94mLoss[0m : 2.57625
[1mStep[0m  [40/42], [94mLoss[0m : 2.52477

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.530, [92mTest[0m: 2.487, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52669
[1mStep[0m  [4/42], [94mLoss[0m : 2.34034
[1mStep[0m  [8/42], [94mLoss[0m : 2.53465
[1mStep[0m  [12/42], [94mLoss[0m : 2.43165
[1mStep[0m  [16/42], [94mLoss[0m : 2.33050
[1mStep[0m  [20/42], [94mLoss[0m : 2.52150
[1mStep[0m  [24/42], [94mLoss[0m : 2.53247
[1mStep[0m  [28/42], [94mLoss[0m : 2.60591
[1mStep[0m  [32/42], [94mLoss[0m : 2.47257
[1mStep[0m  [36/42], [94mLoss[0m : 2.59517
[1mStep[0m  [40/42], [94mLoss[0m : 2.39272

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.394, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.66006
[1mStep[0m  [4/42], [94mLoss[0m : 2.61764
[1mStep[0m  [8/42], [94mLoss[0m : 2.62511
[1mStep[0m  [12/42], [94mLoss[0m : 2.34189
[1mStep[0m  [16/42], [94mLoss[0m : 2.63332
[1mStep[0m  [20/42], [94mLoss[0m : 2.51022
[1mStep[0m  [24/42], [94mLoss[0m : 2.36863
[1mStep[0m  [28/42], [94mLoss[0m : 2.56836
[1mStep[0m  [32/42], [94mLoss[0m : 2.76670
[1mStep[0m  [36/42], [94mLoss[0m : 2.57489
[1mStep[0m  [40/42], [94mLoss[0m : 2.42031

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.389, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56397
[1mStep[0m  [4/42], [94mLoss[0m : 2.40380
[1mStep[0m  [8/42], [94mLoss[0m : 2.73367
[1mStep[0m  [12/42], [94mLoss[0m : 2.58336
[1mStep[0m  [16/42], [94mLoss[0m : 2.48957
[1mStep[0m  [20/42], [94mLoss[0m : 2.48635
[1mStep[0m  [24/42], [94mLoss[0m : 2.59696
[1mStep[0m  [28/42], [94mLoss[0m : 2.20483
[1mStep[0m  [32/42], [94mLoss[0m : 2.68115
[1mStep[0m  [36/42], [94mLoss[0m : 2.54403
[1mStep[0m  [40/42], [94mLoss[0m : 2.55916

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.354, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51551
[1mStep[0m  [4/42], [94mLoss[0m : 2.20538
[1mStep[0m  [8/42], [94mLoss[0m : 2.43748
[1mStep[0m  [12/42], [94mLoss[0m : 2.20144
[1mStep[0m  [16/42], [94mLoss[0m : 2.54300
[1mStep[0m  [20/42], [94mLoss[0m : 2.49114
[1mStep[0m  [24/42], [94mLoss[0m : 2.41831
[1mStep[0m  [28/42], [94mLoss[0m : 2.59620
[1mStep[0m  [32/42], [94mLoss[0m : 2.36651
[1mStep[0m  [36/42], [94mLoss[0m : 2.41305
[1mStep[0m  [40/42], [94mLoss[0m : 2.40004

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.372, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39431
[1mStep[0m  [4/42], [94mLoss[0m : 2.45298
[1mStep[0m  [8/42], [94mLoss[0m : 2.50839
[1mStep[0m  [12/42], [94mLoss[0m : 2.73153
[1mStep[0m  [16/42], [94mLoss[0m : 2.27325
[1mStep[0m  [20/42], [94mLoss[0m : 2.64127
[1mStep[0m  [24/42], [94mLoss[0m : 2.49330
[1mStep[0m  [28/42], [94mLoss[0m : 2.49161
[1mStep[0m  [32/42], [94mLoss[0m : 2.49585
[1mStep[0m  [36/42], [94mLoss[0m : 2.52537
[1mStep[0m  [40/42], [94mLoss[0m : 2.47113

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.374, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54112
[1mStep[0m  [4/42], [94mLoss[0m : 2.43114
[1mStep[0m  [8/42], [94mLoss[0m : 2.63127
[1mStep[0m  [12/42], [94mLoss[0m : 2.48815
[1mStep[0m  [16/42], [94mLoss[0m : 2.47162
[1mStep[0m  [20/42], [94mLoss[0m : 2.34120
[1mStep[0m  [24/42], [94mLoss[0m : 2.46252
[1mStep[0m  [28/42], [94mLoss[0m : 2.34901
[1mStep[0m  [32/42], [94mLoss[0m : 2.65133
[1mStep[0m  [36/42], [94mLoss[0m : 2.52870
[1mStep[0m  [40/42], [94mLoss[0m : 2.59045

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.360, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56429
[1mStep[0m  [4/42], [94mLoss[0m : 2.47100
[1mStep[0m  [8/42], [94mLoss[0m : 2.26708
[1mStep[0m  [12/42], [94mLoss[0m : 2.54707
[1mStep[0m  [16/42], [94mLoss[0m : 2.66278
[1mStep[0m  [20/42], [94mLoss[0m : 2.36291
[1mStep[0m  [24/42], [94mLoss[0m : 2.24744
[1mStep[0m  [28/42], [94mLoss[0m : 2.74794
[1mStep[0m  [32/42], [94mLoss[0m : 2.50157
[1mStep[0m  [36/42], [94mLoss[0m : 2.54149
[1mStep[0m  [40/42], [94mLoss[0m : 2.64889

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.359, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40508
[1mStep[0m  [4/42], [94mLoss[0m : 2.34760
[1mStep[0m  [8/42], [94mLoss[0m : 2.41502
[1mStep[0m  [12/42], [94mLoss[0m : 2.40741
[1mStep[0m  [16/42], [94mLoss[0m : 2.53404
[1mStep[0m  [20/42], [94mLoss[0m : 2.36708
[1mStep[0m  [24/42], [94mLoss[0m : 2.52947
[1mStep[0m  [28/42], [94mLoss[0m : 2.41336
[1mStep[0m  [32/42], [94mLoss[0m : 2.63225
[1mStep[0m  [36/42], [94mLoss[0m : 2.41980
[1mStep[0m  [40/42], [94mLoss[0m : 2.37489

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.366, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50049
[1mStep[0m  [4/42], [94mLoss[0m : 2.51486
[1mStep[0m  [8/42], [94mLoss[0m : 2.42479
[1mStep[0m  [12/42], [94mLoss[0m : 2.58133
[1mStep[0m  [16/42], [94mLoss[0m : 2.39289
[1mStep[0m  [20/42], [94mLoss[0m : 2.41964
[1mStep[0m  [24/42], [94mLoss[0m : 2.55051
[1mStep[0m  [28/42], [94mLoss[0m : 2.63863
[1mStep[0m  [32/42], [94mLoss[0m : 2.31993
[1mStep[0m  [36/42], [94mLoss[0m : 2.54728
[1mStep[0m  [40/42], [94mLoss[0m : 2.46246

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.346, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.72435
[1mStep[0m  [4/42], [94mLoss[0m : 2.60003
[1mStep[0m  [8/42], [94mLoss[0m : 2.48461
[1mStep[0m  [12/42], [94mLoss[0m : 2.47344
[1mStep[0m  [16/42], [94mLoss[0m : 2.41563
[1mStep[0m  [20/42], [94mLoss[0m : 2.64710
[1mStep[0m  [24/42], [94mLoss[0m : 2.47753
[1mStep[0m  [28/42], [94mLoss[0m : 2.63594
[1mStep[0m  [32/42], [94mLoss[0m : 2.48951
[1mStep[0m  [36/42], [94mLoss[0m : 2.51119
[1mStep[0m  [40/42], [94mLoss[0m : 2.41873

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.363, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48097
[1mStep[0m  [4/42], [94mLoss[0m : 2.41145
[1mStep[0m  [8/42], [94mLoss[0m : 2.57308
[1mStep[0m  [12/42], [94mLoss[0m : 2.48950
[1mStep[0m  [16/42], [94mLoss[0m : 2.41904
[1mStep[0m  [20/42], [94mLoss[0m : 2.40995
[1mStep[0m  [24/42], [94mLoss[0m : 2.33258
[1mStep[0m  [28/42], [94mLoss[0m : 2.35043
[1mStep[0m  [32/42], [94mLoss[0m : 2.47958
[1mStep[0m  [36/42], [94mLoss[0m : 2.57816
[1mStep[0m  [40/42], [94mLoss[0m : 2.34982

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.385, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29964
[1mStep[0m  [4/42], [94mLoss[0m : 2.34016
[1mStep[0m  [8/42], [94mLoss[0m : 2.63342
[1mStep[0m  [12/42], [94mLoss[0m : 2.46312
[1mStep[0m  [16/42], [94mLoss[0m : 2.43293
[1mStep[0m  [20/42], [94mLoss[0m : 2.33278
[1mStep[0m  [24/42], [94mLoss[0m : 2.47432
[1mStep[0m  [28/42], [94mLoss[0m : 2.49294
[1mStep[0m  [32/42], [94mLoss[0m : 2.26482
[1mStep[0m  [36/42], [94mLoss[0m : 2.63299
[1mStep[0m  [40/42], [94mLoss[0m : 2.39997

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.337, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.63338
[1mStep[0m  [4/42], [94mLoss[0m : 2.30851
[1mStep[0m  [8/42], [94mLoss[0m : 2.28572
[1mStep[0m  [12/42], [94mLoss[0m : 2.41155
[1mStep[0m  [16/42], [94mLoss[0m : 2.46674
[1mStep[0m  [20/42], [94mLoss[0m : 2.26579
[1mStep[0m  [24/42], [94mLoss[0m : 2.28657
[1mStep[0m  [28/42], [94mLoss[0m : 2.34775
[1mStep[0m  [32/42], [94mLoss[0m : 2.56138
[1mStep[0m  [36/42], [94mLoss[0m : 2.53514
[1mStep[0m  [40/42], [94mLoss[0m : 2.44505

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.378, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36508
[1mStep[0m  [4/42], [94mLoss[0m : 2.41417
[1mStep[0m  [8/42], [94mLoss[0m : 2.55005
[1mStep[0m  [12/42], [94mLoss[0m : 2.49566
[1mStep[0m  [16/42], [94mLoss[0m : 2.26974
[1mStep[0m  [20/42], [94mLoss[0m : 2.54566
[1mStep[0m  [24/42], [94mLoss[0m : 2.34765
[1mStep[0m  [28/42], [94mLoss[0m : 2.58766
[1mStep[0m  [32/42], [94mLoss[0m : 2.23536
[1mStep[0m  [36/42], [94mLoss[0m : 2.38556
[1mStep[0m  [40/42], [94mLoss[0m : 2.58775

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.328, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58854
[1mStep[0m  [4/42], [94mLoss[0m : 2.54685
[1mStep[0m  [8/42], [94mLoss[0m : 2.34578
[1mStep[0m  [12/42], [94mLoss[0m : 2.41449
[1mStep[0m  [16/42], [94mLoss[0m : 2.59451
[1mStep[0m  [20/42], [94mLoss[0m : 2.44017
[1mStep[0m  [24/42], [94mLoss[0m : 2.38176
[1mStep[0m  [28/42], [94mLoss[0m : 2.33038
[1mStep[0m  [32/42], [94mLoss[0m : 2.45928
[1mStep[0m  [36/42], [94mLoss[0m : 2.29784
[1mStep[0m  [40/42], [94mLoss[0m : 2.57419

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.334, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51952
[1mStep[0m  [4/42], [94mLoss[0m : 2.30385
[1mStep[0m  [8/42], [94mLoss[0m : 2.28354
[1mStep[0m  [12/42], [94mLoss[0m : 2.33795
[1mStep[0m  [16/42], [94mLoss[0m : 2.52291
[1mStep[0m  [20/42], [94mLoss[0m : 2.58014
[1mStep[0m  [24/42], [94mLoss[0m : 2.44459
[1mStep[0m  [28/42], [94mLoss[0m : 2.53720
[1mStep[0m  [32/42], [94mLoss[0m : 2.51591
[1mStep[0m  [36/42], [94mLoss[0m : 2.42537
[1mStep[0m  [40/42], [94mLoss[0m : 2.34034

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.308, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46316
[1mStep[0m  [4/42], [94mLoss[0m : 2.27712
[1mStep[0m  [8/42], [94mLoss[0m : 2.49131
[1mStep[0m  [12/42], [94mLoss[0m : 2.39521
[1mStep[0m  [16/42], [94mLoss[0m : 2.20875
[1mStep[0m  [20/42], [94mLoss[0m : 2.59701
[1mStep[0m  [24/42], [94mLoss[0m : 2.44446
[1mStep[0m  [28/42], [94mLoss[0m : 2.57321
[1mStep[0m  [32/42], [94mLoss[0m : 2.49709
[1mStep[0m  [36/42], [94mLoss[0m : 2.51121
[1mStep[0m  [40/42], [94mLoss[0m : 2.28252

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.320, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38911
[1mStep[0m  [4/42], [94mLoss[0m : 2.30593
[1mStep[0m  [8/42], [94mLoss[0m : 2.32967
[1mStep[0m  [12/42], [94mLoss[0m : 2.36704
[1mStep[0m  [16/42], [94mLoss[0m : 2.20637
[1mStep[0m  [20/42], [94mLoss[0m : 2.37583
[1mStep[0m  [24/42], [94mLoss[0m : 2.45561
[1mStep[0m  [28/42], [94mLoss[0m : 2.57427
[1mStep[0m  [32/42], [94mLoss[0m : 2.58995
[1mStep[0m  [36/42], [94mLoss[0m : 2.48018
[1mStep[0m  [40/42], [94mLoss[0m : 2.38345

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.343, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45489
[1mStep[0m  [4/42], [94mLoss[0m : 2.46652
[1mStep[0m  [8/42], [94mLoss[0m : 2.52325
[1mStep[0m  [12/42], [94mLoss[0m : 2.63320
[1mStep[0m  [16/42], [94mLoss[0m : 2.45250
[1mStep[0m  [20/42], [94mLoss[0m : 2.44656
[1mStep[0m  [24/42], [94mLoss[0m : 2.48038
[1mStep[0m  [28/42], [94mLoss[0m : 2.31931
[1mStep[0m  [32/42], [94mLoss[0m : 2.55064
[1mStep[0m  [36/42], [94mLoss[0m : 2.28952
[1mStep[0m  [40/42], [94mLoss[0m : 2.28726

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.418, [92mTest[0m: 2.315, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28977
[1mStep[0m  [4/42], [94mLoss[0m : 2.46285
[1mStep[0m  [8/42], [94mLoss[0m : 2.33810
[1mStep[0m  [12/42], [94mLoss[0m : 2.37347
[1mStep[0m  [16/42], [94mLoss[0m : 2.29061
[1mStep[0m  [20/42], [94mLoss[0m : 2.27370
[1mStep[0m  [24/42], [94mLoss[0m : 2.64021
[1mStep[0m  [28/42], [94mLoss[0m : 2.63819
[1mStep[0m  [32/42], [94mLoss[0m : 2.26490
[1mStep[0m  [36/42], [94mLoss[0m : 2.34348
[1mStep[0m  [40/42], [94mLoss[0m : 2.20937

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.368, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47202
[1mStep[0m  [4/42], [94mLoss[0m : 2.34080
[1mStep[0m  [8/42], [94mLoss[0m : 2.33513
[1mStep[0m  [12/42], [94mLoss[0m : 2.37006
[1mStep[0m  [16/42], [94mLoss[0m : 2.45483
[1mStep[0m  [20/42], [94mLoss[0m : 2.34400
[1mStep[0m  [24/42], [94mLoss[0m : 2.51577
[1mStep[0m  [28/42], [94mLoss[0m : 2.53329
[1mStep[0m  [32/42], [94mLoss[0m : 2.57745
[1mStep[0m  [36/42], [94mLoss[0m : 2.23818
[1mStep[0m  [40/42], [94mLoss[0m : 2.60707

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.327, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.328
====================================

Phase 1 - Evaluation MAE:  2.3284365279333934
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 2.34228
[1mStep[0m  [4/42], [94mLoss[0m : 2.56928
[1mStep[0m  [8/42], [94mLoss[0m : 2.51584
[1mStep[0m  [12/42], [94mLoss[0m : 2.34153
[1mStep[0m  [16/42], [94mLoss[0m : 2.41005
[1mStep[0m  [20/42], [94mLoss[0m : 2.47710
[1mStep[0m  [24/42], [94mLoss[0m : 2.66020
[1mStep[0m  [28/42], [94mLoss[0m : 2.33953
[1mStep[0m  [32/42], [94mLoss[0m : 2.55772
[1mStep[0m  [36/42], [94mLoss[0m : 2.44176
[1mStep[0m  [40/42], [94mLoss[0m : 2.57939

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.336, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54990
[1mStep[0m  [4/42], [94mLoss[0m : 2.58440
[1mStep[0m  [8/42], [94mLoss[0m : 2.35259
[1mStep[0m  [12/42], [94mLoss[0m : 2.28539
[1mStep[0m  [16/42], [94mLoss[0m : 2.51528
[1mStep[0m  [20/42], [94mLoss[0m : 2.29694
[1mStep[0m  [24/42], [94mLoss[0m : 2.63625
[1mStep[0m  [28/42], [94mLoss[0m : 2.13925
[1mStep[0m  [32/42], [94mLoss[0m : 2.43088
[1mStep[0m  [36/42], [94mLoss[0m : 2.39970
[1mStep[0m  [40/42], [94mLoss[0m : 2.60040

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.481, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31150
[1mStep[0m  [4/42], [94mLoss[0m : 2.40642
[1mStep[0m  [8/42], [94mLoss[0m : 2.27294
[1mStep[0m  [12/42], [94mLoss[0m : 2.35534
[1mStep[0m  [16/42], [94mLoss[0m : 2.36430
[1mStep[0m  [20/42], [94mLoss[0m : 2.14528
[1mStep[0m  [24/42], [94mLoss[0m : 2.45377
[1mStep[0m  [28/42], [94mLoss[0m : 2.40989
[1mStep[0m  [32/42], [94mLoss[0m : 2.47344
[1mStep[0m  [36/42], [94mLoss[0m : 2.42113
[1mStep[0m  [40/42], [94mLoss[0m : 2.43986

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.386, [92mTest[0m: 2.417, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24980
[1mStep[0m  [4/42], [94mLoss[0m : 2.41802
[1mStep[0m  [8/42], [94mLoss[0m : 2.36342
[1mStep[0m  [12/42], [94mLoss[0m : 2.38276
[1mStep[0m  [16/42], [94mLoss[0m : 2.40368
[1mStep[0m  [20/42], [94mLoss[0m : 2.55467
[1mStep[0m  [24/42], [94mLoss[0m : 2.32771
[1mStep[0m  [28/42], [94mLoss[0m : 2.37793
[1mStep[0m  [32/42], [94mLoss[0m : 2.36318
[1mStep[0m  [36/42], [94mLoss[0m : 2.41992
[1mStep[0m  [40/42], [94mLoss[0m : 2.08612

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.345, [92mTest[0m: 2.510, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26134
[1mStep[0m  [4/42], [94mLoss[0m : 2.43684
[1mStep[0m  [8/42], [94mLoss[0m : 2.23688
[1mStep[0m  [12/42], [94mLoss[0m : 2.32438
[1mStep[0m  [16/42], [94mLoss[0m : 2.28936
[1mStep[0m  [20/42], [94mLoss[0m : 2.25326
[1mStep[0m  [24/42], [94mLoss[0m : 2.33664
[1mStep[0m  [28/42], [94mLoss[0m : 2.01692
[1mStep[0m  [32/42], [94mLoss[0m : 2.34708
[1mStep[0m  [36/42], [94mLoss[0m : 2.26050
[1mStep[0m  [40/42], [94mLoss[0m : 2.37769

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.276, [92mTest[0m: 2.430, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.00225
[1mStep[0m  [4/42], [94mLoss[0m : 2.12508
[1mStep[0m  [8/42], [94mLoss[0m : 2.00806
[1mStep[0m  [12/42], [94mLoss[0m : 2.13636
[1mStep[0m  [16/42], [94mLoss[0m : 2.26621
[1mStep[0m  [20/42], [94mLoss[0m : 2.38866
[1mStep[0m  [24/42], [94mLoss[0m : 2.26570
[1mStep[0m  [28/42], [94mLoss[0m : 2.22475
[1mStep[0m  [32/42], [94mLoss[0m : 2.13486
[1mStep[0m  [36/42], [94mLoss[0m : 2.42786
[1mStep[0m  [40/42], [94mLoss[0m : 2.21952

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.195, [92mTest[0m: 2.365, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.19597
[1mStep[0m  [4/42], [94mLoss[0m : 2.18756
[1mStep[0m  [8/42], [94mLoss[0m : 2.32292
[1mStep[0m  [12/42], [94mLoss[0m : 2.02088
[1mStep[0m  [16/42], [94mLoss[0m : 2.17179
[1mStep[0m  [20/42], [94mLoss[0m : 2.14062
[1mStep[0m  [24/42], [94mLoss[0m : 2.00766
[1mStep[0m  [28/42], [94mLoss[0m : 2.13398
[1mStep[0m  [32/42], [94mLoss[0m : 2.22560
[1mStep[0m  [36/42], [94mLoss[0m : 2.28035
[1mStep[0m  [40/42], [94mLoss[0m : 2.10837

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.176, [92mTest[0m: 2.433, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.22180
[1mStep[0m  [4/42], [94mLoss[0m : 2.15592
[1mStep[0m  [8/42], [94mLoss[0m : 2.04631
[1mStep[0m  [12/42], [94mLoss[0m : 1.97861
[1mStep[0m  [16/42], [94mLoss[0m : 2.06588
[1mStep[0m  [20/42], [94mLoss[0m : 2.18624
[1mStep[0m  [24/42], [94mLoss[0m : 2.02172
[1mStep[0m  [28/42], [94mLoss[0m : 1.85333
[1mStep[0m  [32/42], [94mLoss[0m : 2.23449
[1mStep[0m  [36/42], [94mLoss[0m : 2.15365
[1mStep[0m  [40/42], [94mLoss[0m : 2.29371

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.115, [92mTest[0m: 2.415, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34611
[1mStep[0m  [4/42], [94mLoss[0m : 2.03792
[1mStep[0m  [8/42], [94mLoss[0m : 2.04586
[1mStep[0m  [12/42], [94mLoss[0m : 2.22835
[1mStep[0m  [16/42], [94mLoss[0m : 2.15631
[1mStep[0m  [20/42], [94mLoss[0m : 2.05931
[1mStep[0m  [24/42], [94mLoss[0m : 2.00188
[1mStep[0m  [28/42], [94mLoss[0m : 2.02811
[1mStep[0m  [32/42], [94mLoss[0m : 2.00362
[1mStep[0m  [36/42], [94mLoss[0m : 2.07881
[1mStep[0m  [40/42], [94mLoss[0m : 2.07880

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.081, [92mTest[0m: 2.437, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.17292
[1mStep[0m  [4/42], [94mLoss[0m : 1.86408
[1mStep[0m  [8/42], [94mLoss[0m : 2.02610
[1mStep[0m  [12/42], [94mLoss[0m : 1.96748
[1mStep[0m  [16/42], [94mLoss[0m : 2.21435
[1mStep[0m  [20/42], [94mLoss[0m : 2.12059
[1mStep[0m  [24/42], [94mLoss[0m : 2.08953
[1mStep[0m  [28/42], [94mLoss[0m : 1.74679
[1mStep[0m  [32/42], [94mLoss[0m : 2.10249
[1mStep[0m  [36/42], [94mLoss[0m : 1.97783
[1mStep[0m  [40/42], [94mLoss[0m : 2.02345

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.028, [92mTest[0m: 2.400, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.73079
[1mStep[0m  [4/42], [94mLoss[0m : 1.65380
[1mStep[0m  [8/42], [94mLoss[0m : 1.94399
[1mStep[0m  [12/42], [94mLoss[0m : 2.03367
[1mStep[0m  [16/42], [94mLoss[0m : 1.91147
[1mStep[0m  [20/42], [94mLoss[0m : 2.04905
[1mStep[0m  [24/42], [94mLoss[0m : 2.12546
[1mStep[0m  [28/42], [94mLoss[0m : 2.08460
[1mStep[0m  [32/42], [94mLoss[0m : 2.07585
[1mStep[0m  [36/42], [94mLoss[0m : 1.95875
[1mStep[0m  [40/42], [94mLoss[0m : 2.19298

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.974, [92mTest[0m: 2.416, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.86958
[1mStep[0m  [4/42], [94mLoss[0m : 1.95269
[1mStep[0m  [8/42], [94mLoss[0m : 2.17226
[1mStep[0m  [12/42], [94mLoss[0m : 1.99763
[1mStep[0m  [16/42], [94mLoss[0m : 2.11549
[1mStep[0m  [20/42], [94mLoss[0m : 2.06870
[1mStep[0m  [24/42], [94mLoss[0m : 1.89502
[1mStep[0m  [28/42], [94mLoss[0m : 1.96661
[1mStep[0m  [32/42], [94mLoss[0m : 1.77357
[1mStep[0m  [36/42], [94mLoss[0m : 1.76116
[1mStep[0m  [40/42], [94mLoss[0m : 2.03395

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.911, [92mTest[0m: 2.417, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.93503
[1mStep[0m  [4/42], [94mLoss[0m : 1.78644
[1mStep[0m  [8/42], [94mLoss[0m : 1.81022
[1mStep[0m  [12/42], [94mLoss[0m : 1.91707
[1mStep[0m  [16/42], [94mLoss[0m : 1.73606
[1mStep[0m  [20/42], [94mLoss[0m : 1.72725
[1mStep[0m  [24/42], [94mLoss[0m : 2.10485
[1mStep[0m  [28/42], [94mLoss[0m : 1.83721
[1mStep[0m  [32/42], [94mLoss[0m : 1.87792
[1mStep[0m  [36/42], [94mLoss[0m : 1.83463
[1mStep[0m  [40/42], [94mLoss[0m : 2.15685

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.888, [92mTest[0m: 2.463, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.80492
[1mStep[0m  [4/42], [94mLoss[0m : 2.02316
[1mStep[0m  [8/42], [94mLoss[0m : 1.88423
[1mStep[0m  [12/42], [94mLoss[0m : 1.66161
[1mStep[0m  [16/42], [94mLoss[0m : 1.84734
[1mStep[0m  [20/42], [94mLoss[0m : 1.86965
[1mStep[0m  [24/42], [94mLoss[0m : 1.71675
[1mStep[0m  [28/42], [94mLoss[0m : 1.86468
[1mStep[0m  [32/42], [94mLoss[0m : 1.91570
[1mStep[0m  [36/42], [94mLoss[0m : 1.86215
[1mStep[0m  [40/42], [94mLoss[0m : 2.00232

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.868, [92mTest[0m: 2.465, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.82758
[1mStep[0m  [4/42], [94mLoss[0m : 1.91042
[1mStep[0m  [8/42], [94mLoss[0m : 1.84404
[1mStep[0m  [12/42], [94mLoss[0m : 1.85246
[1mStep[0m  [16/42], [94mLoss[0m : 1.86271
[1mStep[0m  [20/42], [94mLoss[0m : 1.79839
[1mStep[0m  [24/42], [94mLoss[0m : 1.94203
[1mStep[0m  [28/42], [94mLoss[0m : 1.95561
[1mStep[0m  [32/42], [94mLoss[0m : 1.74002
[1mStep[0m  [36/42], [94mLoss[0m : 1.65150
[1mStep[0m  [40/42], [94mLoss[0m : 1.89631

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.838, [92mTest[0m: 2.463, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.91885
[1mStep[0m  [4/42], [94mLoss[0m : 1.83853
[1mStep[0m  [8/42], [94mLoss[0m : 1.98244
[1mStep[0m  [12/42], [94mLoss[0m : 1.76177
[1mStep[0m  [16/42], [94mLoss[0m : 1.58958
[1mStep[0m  [20/42], [94mLoss[0m : 1.61052
[1mStep[0m  [24/42], [94mLoss[0m : 1.78853
[1mStep[0m  [28/42], [94mLoss[0m : 1.89005
[1mStep[0m  [32/42], [94mLoss[0m : 1.96496
[1mStep[0m  [36/42], [94mLoss[0m : 1.79599
[1mStep[0m  [40/42], [94mLoss[0m : 1.80486

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.794, [92mTest[0m: 2.454, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.67252
[1mStep[0m  [4/42], [94mLoss[0m : 1.73067
[1mStep[0m  [8/42], [94mLoss[0m : 1.89090
[1mStep[0m  [12/42], [94mLoss[0m : 1.85669
[1mStep[0m  [16/42], [94mLoss[0m : 1.68486
[1mStep[0m  [20/42], [94mLoss[0m : 1.70039
[1mStep[0m  [24/42], [94mLoss[0m : 1.81652
[1mStep[0m  [28/42], [94mLoss[0m : 1.73728
[1mStep[0m  [32/42], [94mLoss[0m : 1.81264
[1mStep[0m  [36/42], [94mLoss[0m : 1.78537
[1mStep[0m  [40/42], [94mLoss[0m : 1.73400

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.752, [92mTest[0m: 2.508, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.59739
[1mStep[0m  [4/42], [94mLoss[0m : 1.82129
[1mStep[0m  [8/42], [94mLoss[0m : 1.74390
[1mStep[0m  [12/42], [94mLoss[0m : 1.84127
[1mStep[0m  [16/42], [94mLoss[0m : 1.86697
[1mStep[0m  [20/42], [94mLoss[0m : 1.79983
[1mStep[0m  [24/42], [94mLoss[0m : 1.73074
[1mStep[0m  [28/42], [94mLoss[0m : 1.63669
[1mStep[0m  [32/42], [94mLoss[0m : 1.62448
[1mStep[0m  [36/42], [94mLoss[0m : 1.64114
[1mStep[0m  [40/42], [94mLoss[0m : 1.72312

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.736, [92mTest[0m: 2.464, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.73200
[1mStep[0m  [4/42], [94mLoss[0m : 1.70369
[1mStep[0m  [8/42], [94mLoss[0m : 1.73844
[1mStep[0m  [12/42], [94mLoss[0m : 1.63975
[1mStep[0m  [16/42], [94mLoss[0m : 1.84651
[1mStep[0m  [20/42], [94mLoss[0m : 1.87289
[1mStep[0m  [24/42], [94mLoss[0m : 1.65880
[1mStep[0m  [28/42], [94mLoss[0m : 1.75181
[1mStep[0m  [32/42], [94mLoss[0m : 1.81958
[1mStep[0m  [36/42], [94mLoss[0m : 1.54004
[1mStep[0m  [40/42], [94mLoss[0m : 1.69317

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.709, [92mTest[0m: 2.474, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.74844
[1mStep[0m  [4/42], [94mLoss[0m : 1.69872
[1mStep[0m  [8/42], [94mLoss[0m : 1.67420
[1mStep[0m  [12/42], [94mLoss[0m : 1.60012
[1mStep[0m  [16/42], [94mLoss[0m : 1.55709
[1mStep[0m  [20/42], [94mLoss[0m : 1.64984
[1mStep[0m  [24/42], [94mLoss[0m : 1.67102
[1mStep[0m  [28/42], [94mLoss[0m : 1.64388
[1mStep[0m  [32/42], [94mLoss[0m : 1.54225
[1mStep[0m  [36/42], [94mLoss[0m : 1.77875
[1mStep[0m  [40/42], [94mLoss[0m : 1.65475

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.683, [92mTest[0m: 2.469, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.76442
[1mStep[0m  [4/42], [94mLoss[0m : 1.80693
[1mStep[0m  [8/42], [94mLoss[0m : 1.47690
[1mStep[0m  [12/42], [94mLoss[0m : 1.59730
[1mStep[0m  [16/42], [94mLoss[0m : 1.64656
[1mStep[0m  [20/42], [94mLoss[0m : 1.61658
[1mStep[0m  [24/42], [94mLoss[0m : 1.57923
[1mStep[0m  [28/42], [94mLoss[0m : 1.64322
[1mStep[0m  [32/42], [94mLoss[0m : 1.61777
[1mStep[0m  [36/42], [94mLoss[0m : 1.53630
[1mStep[0m  [40/42], [94mLoss[0m : 1.72490

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.647, [92mTest[0m: 2.464, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.52974
[1mStep[0m  [4/42], [94mLoss[0m : 1.57180
[1mStep[0m  [8/42], [94mLoss[0m : 1.67192
[1mStep[0m  [12/42], [94mLoss[0m : 1.48396
[1mStep[0m  [16/42], [94mLoss[0m : 1.62835
[1mStep[0m  [20/42], [94mLoss[0m : 1.50628
[1mStep[0m  [24/42], [94mLoss[0m : 1.56485
[1mStep[0m  [28/42], [94mLoss[0m : 1.64095
[1mStep[0m  [32/42], [94mLoss[0m : 1.57108
[1mStep[0m  [36/42], [94mLoss[0m : 1.64076
[1mStep[0m  [40/42], [94mLoss[0m : 1.56732

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.601, [92mTest[0m: 2.482, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.69045
[1mStep[0m  [4/42], [94mLoss[0m : 1.62099
[1mStep[0m  [8/42], [94mLoss[0m : 1.69605
[1mStep[0m  [12/42], [94mLoss[0m : 1.70905
[1mStep[0m  [16/42], [94mLoss[0m : 1.62301
[1mStep[0m  [20/42], [94mLoss[0m : 1.67992
[1mStep[0m  [24/42], [94mLoss[0m : 1.61356
[1mStep[0m  [28/42], [94mLoss[0m : 1.45512
[1mStep[0m  [32/42], [94mLoss[0m : 1.59534
[1mStep[0m  [36/42], [94mLoss[0m : 1.69435
[1mStep[0m  [40/42], [94mLoss[0m : 1.70954

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.586, [92mTest[0m: 2.485, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.60933
[1mStep[0m  [4/42], [94mLoss[0m : 1.62753
[1mStep[0m  [8/42], [94mLoss[0m : 1.49672
[1mStep[0m  [12/42], [94mLoss[0m : 1.66017
[1mStep[0m  [16/42], [94mLoss[0m : 1.61297
[1mStep[0m  [20/42], [94mLoss[0m : 1.62848
[1mStep[0m  [24/42], [94mLoss[0m : 1.58851
[1mStep[0m  [28/42], [94mLoss[0m : 1.64167
[1mStep[0m  [32/42], [94mLoss[0m : 1.46792
[1mStep[0m  [36/42], [94mLoss[0m : 1.63087
[1mStep[0m  [40/42], [94mLoss[0m : 1.50717

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.563, [92mTest[0m: 2.480, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.55117
[1mStep[0m  [4/42], [94mLoss[0m : 1.48319
[1mStep[0m  [8/42], [94mLoss[0m : 1.62826
[1mStep[0m  [12/42], [94mLoss[0m : 1.54163
[1mStep[0m  [16/42], [94mLoss[0m : 1.44651
[1mStep[0m  [20/42], [94mLoss[0m : 1.71347
[1mStep[0m  [24/42], [94mLoss[0m : 1.60617
[1mStep[0m  [28/42], [94mLoss[0m : 1.52130
[1mStep[0m  [32/42], [94mLoss[0m : 1.55374
[1mStep[0m  [36/42], [94mLoss[0m : 1.46568
[1mStep[0m  [40/42], [94mLoss[0m : 1.51142

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.540, [92mTest[0m: 2.500, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.43479
[1mStep[0m  [4/42], [94mLoss[0m : 1.57182
[1mStep[0m  [8/42], [94mLoss[0m : 1.39823
[1mStep[0m  [12/42], [94mLoss[0m : 1.43721
[1mStep[0m  [16/42], [94mLoss[0m : 1.52443
[1mStep[0m  [20/42], [94mLoss[0m : 1.68173
[1mStep[0m  [24/42], [94mLoss[0m : 1.55413
[1mStep[0m  [28/42], [94mLoss[0m : 1.49257
[1mStep[0m  [32/42], [94mLoss[0m : 1.57412
[1mStep[0m  [36/42], [94mLoss[0m : 1.46022
[1mStep[0m  [40/42], [94mLoss[0m : 1.52169

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.517, [92mTest[0m: 2.522, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.38911
[1mStep[0m  [4/42], [94mLoss[0m : 1.54717
[1mStep[0m  [8/42], [94mLoss[0m : 1.48865
[1mStep[0m  [12/42], [94mLoss[0m : 1.54482
[1mStep[0m  [16/42], [94mLoss[0m : 1.45128
[1mStep[0m  [20/42], [94mLoss[0m : 1.42086
[1mStep[0m  [24/42], [94mLoss[0m : 1.49210
[1mStep[0m  [28/42], [94mLoss[0m : 1.60234
[1mStep[0m  [32/42], [94mLoss[0m : 1.65999
[1mStep[0m  [36/42], [94mLoss[0m : 1.44314
[1mStep[0m  [40/42], [94mLoss[0m : 1.56657

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.506, [92mTest[0m: 2.521, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.33192
[1mStep[0m  [4/42], [94mLoss[0m : 1.49967
[1mStep[0m  [8/42], [94mLoss[0m : 1.60590
[1mStep[0m  [12/42], [94mLoss[0m : 1.64165
[1mStep[0m  [16/42], [94mLoss[0m : 1.40346
[1mStep[0m  [20/42], [94mLoss[0m : 1.45069
[1mStep[0m  [24/42], [94mLoss[0m : 1.59212
[1mStep[0m  [28/42], [94mLoss[0m : 1.70707
[1mStep[0m  [32/42], [94mLoss[0m : 1.39078
[1mStep[0m  [36/42], [94mLoss[0m : 1.38640
[1mStep[0m  [40/42], [94mLoss[0m : 1.42304

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.481, [92mTest[0m: 2.459, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.45210
[1mStep[0m  [4/42], [94mLoss[0m : 1.32791
[1mStep[0m  [8/42], [94mLoss[0m : 1.48859
[1mStep[0m  [12/42], [94mLoss[0m : 1.52248
[1mStep[0m  [16/42], [94mLoss[0m : 1.50365
[1mStep[0m  [20/42], [94mLoss[0m : 1.54778
[1mStep[0m  [24/42], [94mLoss[0m : 1.60965
[1mStep[0m  [28/42], [94mLoss[0m : 1.39128
[1mStep[0m  [32/42], [94mLoss[0m : 1.46675
[1mStep[0m  [36/42], [94mLoss[0m : 1.42306
[1mStep[0m  [40/42], [94mLoss[0m : 1.52973

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.471, [92mTest[0m: 2.495, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.52483
[1mStep[0m  [4/42], [94mLoss[0m : 1.29995
[1mStep[0m  [8/42], [94mLoss[0m : 1.66267
[1mStep[0m  [12/42], [94mLoss[0m : 1.57482
[1mStep[0m  [16/42], [94mLoss[0m : 1.42496
[1mStep[0m  [20/42], [94mLoss[0m : 1.40004
[1mStep[0m  [24/42], [94mLoss[0m : 1.32248
[1mStep[0m  [28/42], [94mLoss[0m : 1.47990
[1mStep[0m  [32/42], [94mLoss[0m : 1.51399
[1mStep[0m  [36/42], [94mLoss[0m : 1.58699
[1mStep[0m  [40/42], [94mLoss[0m : 1.51001

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.463, [92mTest[0m: 2.472, [96mlr[0m: 0.006772500000000002
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 29 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.479
====================================

Phase 2 - Evaluation MAE:  2.478714500154768
MAE score P1      2.328437
MAE score P2      2.478715
loss               1.46348
learning_rate     0.007525
batch_size             256
hidden_sizes         [100]
epochs                  30
activation            relu
optimizer              sgd
early stopping        True
dropout                0.3
momentum               0.5
weight_decay        0.0001
Name: 13, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 11.03334
[1mStep[0m  [8/84], [94mLoss[0m : 6.70679
[1mStep[0m  [16/84], [94mLoss[0m : 3.72576
[1mStep[0m  [24/84], [94mLoss[0m : 2.29114
[1mStep[0m  [32/84], [94mLoss[0m : 2.60637
[1mStep[0m  [40/84], [94mLoss[0m : 2.85608
[1mStep[0m  [48/84], [94mLoss[0m : 2.63712
[1mStep[0m  [56/84], [94mLoss[0m : 2.35753
[1mStep[0m  [64/84], [94mLoss[0m : 2.63037
[1mStep[0m  [72/84], [94mLoss[0m : 2.50419
[1mStep[0m  [80/84], [94mLoss[0m : 2.39571

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.523, [92mTest[0m: 11.002, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.85951
[1mStep[0m  [8/84], [94mLoss[0m : 2.70546
[1mStep[0m  [16/84], [94mLoss[0m : 2.34531
[1mStep[0m  [24/84], [94mLoss[0m : 2.20319
[1mStep[0m  [32/84], [94mLoss[0m : 2.54510
[1mStep[0m  [40/84], [94mLoss[0m : 2.56712
[1mStep[0m  [48/84], [94mLoss[0m : 2.62466
[1mStep[0m  [56/84], [94mLoss[0m : 2.50422
[1mStep[0m  [64/84], [94mLoss[0m : 2.51558
[1mStep[0m  [72/84], [94mLoss[0m : 2.66171
[1mStep[0m  [80/84], [94mLoss[0m : 2.45817

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.462, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30895
[1mStep[0m  [8/84], [94mLoss[0m : 2.42331
[1mStep[0m  [16/84], [94mLoss[0m : 2.48992
[1mStep[0m  [24/84], [94mLoss[0m : 2.48825
[1mStep[0m  [32/84], [94mLoss[0m : 2.13294
[1mStep[0m  [40/84], [94mLoss[0m : 2.21744
[1mStep[0m  [48/84], [94mLoss[0m : 2.50730
[1mStep[0m  [56/84], [94mLoss[0m : 2.39984
[1mStep[0m  [64/84], [94mLoss[0m : 2.37799
[1mStep[0m  [72/84], [94mLoss[0m : 2.36030
[1mStep[0m  [80/84], [94mLoss[0m : 2.70005

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.407, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.56808
[1mStep[0m  [8/84], [94mLoss[0m : 2.32355
[1mStep[0m  [16/84], [94mLoss[0m : 2.75360
[1mStep[0m  [24/84], [94mLoss[0m : 2.17135
[1mStep[0m  [32/84], [94mLoss[0m : 2.46454
[1mStep[0m  [40/84], [94mLoss[0m : 2.58518
[1mStep[0m  [48/84], [94mLoss[0m : 2.66806
[1mStep[0m  [56/84], [94mLoss[0m : 2.68086
[1mStep[0m  [64/84], [94mLoss[0m : 2.42298
[1mStep[0m  [72/84], [94mLoss[0m : 2.74693
[1mStep[0m  [80/84], [94mLoss[0m : 2.42906

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.405, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27425
[1mStep[0m  [8/84], [94mLoss[0m : 2.49848
[1mStep[0m  [16/84], [94mLoss[0m : 2.35149
[1mStep[0m  [24/84], [94mLoss[0m : 2.72470
[1mStep[0m  [32/84], [94mLoss[0m : 2.54493
[1mStep[0m  [40/84], [94mLoss[0m : 2.17411
[1mStep[0m  [48/84], [94mLoss[0m : 2.62886
[1mStep[0m  [56/84], [94mLoss[0m : 2.01618
[1mStep[0m  [64/84], [94mLoss[0m : 2.34734
[1mStep[0m  [72/84], [94mLoss[0m : 2.46327
[1mStep[0m  [80/84], [94mLoss[0m : 2.24240

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.372, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45834
[1mStep[0m  [8/84], [94mLoss[0m : 2.21119
[1mStep[0m  [16/84], [94mLoss[0m : 2.18699
[1mStep[0m  [24/84], [94mLoss[0m : 2.55663
[1mStep[0m  [32/84], [94mLoss[0m : 2.57176
[1mStep[0m  [40/84], [94mLoss[0m : 2.42283
[1mStep[0m  [48/84], [94mLoss[0m : 2.73801
[1mStep[0m  [56/84], [94mLoss[0m : 2.56119
[1mStep[0m  [64/84], [94mLoss[0m : 2.61144
[1mStep[0m  [72/84], [94mLoss[0m : 2.43560
[1mStep[0m  [80/84], [94mLoss[0m : 2.66904

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.349, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52165
[1mStep[0m  [8/84], [94mLoss[0m : 2.32019
[1mStep[0m  [16/84], [94mLoss[0m : 2.47460
[1mStep[0m  [24/84], [94mLoss[0m : 2.51094
[1mStep[0m  [32/84], [94mLoss[0m : 2.64128
[1mStep[0m  [40/84], [94mLoss[0m : 2.72020
[1mStep[0m  [48/84], [94mLoss[0m : 2.14804
[1mStep[0m  [56/84], [94mLoss[0m : 2.58447
[1mStep[0m  [64/84], [94mLoss[0m : 2.46197
[1mStep[0m  [72/84], [94mLoss[0m : 2.32211
[1mStep[0m  [80/84], [94mLoss[0m : 2.56520

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.354, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37557
[1mStep[0m  [8/84], [94mLoss[0m : 2.29956
[1mStep[0m  [16/84], [94mLoss[0m : 2.40990
[1mStep[0m  [24/84], [94mLoss[0m : 2.20696
[1mStep[0m  [32/84], [94mLoss[0m : 2.45540
[1mStep[0m  [40/84], [94mLoss[0m : 2.40636
[1mStep[0m  [48/84], [94mLoss[0m : 2.29195
[1mStep[0m  [56/84], [94mLoss[0m : 2.49651
[1mStep[0m  [64/84], [94mLoss[0m : 2.36647
[1mStep[0m  [72/84], [94mLoss[0m : 2.43792
[1mStep[0m  [80/84], [94mLoss[0m : 2.83893

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.339, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37731
[1mStep[0m  [8/84], [94mLoss[0m : 2.35990
[1mStep[0m  [16/84], [94mLoss[0m : 2.44027
[1mStep[0m  [24/84], [94mLoss[0m : 2.69491
[1mStep[0m  [32/84], [94mLoss[0m : 2.88508
[1mStep[0m  [40/84], [94mLoss[0m : 2.37775
[1mStep[0m  [48/84], [94mLoss[0m : 2.50106
[1mStep[0m  [56/84], [94mLoss[0m : 2.38810
[1mStep[0m  [64/84], [94mLoss[0m : 2.25645
[1mStep[0m  [72/84], [94mLoss[0m : 2.39383
[1mStep[0m  [80/84], [94mLoss[0m : 2.36515

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.341, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43628
[1mStep[0m  [8/84], [94mLoss[0m : 2.15088
[1mStep[0m  [16/84], [94mLoss[0m : 2.34366
[1mStep[0m  [24/84], [94mLoss[0m : 2.56809
[1mStep[0m  [32/84], [94mLoss[0m : 2.13468
[1mStep[0m  [40/84], [94mLoss[0m : 2.49413
[1mStep[0m  [48/84], [94mLoss[0m : 2.32265
[1mStep[0m  [56/84], [94mLoss[0m : 2.46648
[1mStep[0m  [64/84], [94mLoss[0m : 2.34679
[1mStep[0m  [72/84], [94mLoss[0m : 2.44974
[1mStep[0m  [80/84], [94mLoss[0m : 2.22548

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.342, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33534
[1mStep[0m  [8/84], [94mLoss[0m : 2.53652
[1mStep[0m  [16/84], [94mLoss[0m : 2.39299
[1mStep[0m  [24/84], [94mLoss[0m : 2.27429
[1mStep[0m  [32/84], [94mLoss[0m : 2.50017
[1mStep[0m  [40/84], [94mLoss[0m : 2.49171
[1mStep[0m  [48/84], [94mLoss[0m : 2.35105
[1mStep[0m  [56/84], [94mLoss[0m : 2.20599
[1mStep[0m  [64/84], [94mLoss[0m : 2.21892
[1mStep[0m  [72/84], [94mLoss[0m : 2.44901
[1mStep[0m  [80/84], [94mLoss[0m : 2.34591

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.336, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.65570
[1mStep[0m  [8/84], [94mLoss[0m : 2.26740
[1mStep[0m  [16/84], [94mLoss[0m : 2.47073
[1mStep[0m  [24/84], [94mLoss[0m : 2.24894
[1mStep[0m  [32/84], [94mLoss[0m : 2.41656
[1mStep[0m  [40/84], [94mLoss[0m : 2.47063
[1mStep[0m  [48/84], [94mLoss[0m : 2.49840
[1mStep[0m  [56/84], [94mLoss[0m : 2.47823
[1mStep[0m  [64/84], [94mLoss[0m : 2.27031
[1mStep[0m  [72/84], [94mLoss[0m : 2.42074
[1mStep[0m  [80/84], [94mLoss[0m : 2.57008

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.418, [92mTest[0m: 2.331, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47377
[1mStep[0m  [8/84], [94mLoss[0m : 2.59011
[1mStep[0m  [16/84], [94mLoss[0m : 2.33900
[1mStep[0m  [24/84], [94mLoss[0m : 2.27661
[1mStep[0m  [32/84], [94mLoss[0m : 2.28738
[1mStep[0m  [40/84], [94mLoss[0m : 2.36294
[1mStep[0m  [48/84], [94mLoss[0m : 2.14715
[1mStep[0m  [56/84], [94mLoss[0m : 2.02569
[1mStep[0m  [64/84], [94mLoss[0m : 2.32583
[1mStep[0m  [72/84], [94mLoss[0m : 2.27800
[1mStep[0m  [80/84], [94mLoss[0m : 2.17267

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.325, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59003
[1mStep[0m  [8/84], [94mLoss[0m : 2.51024
[1mStep[0m  [16/84], [94mLoss[0m : 2.25896
[1mStep[0m  [24/84], [94mLoss[0m : 2.34653
[1mStep[0m  [32/84], [94mLoss[0m : 2.32040
[1mStep[0m  [40/84], [94mLoss[0m : 2.20481
[1mStep[0m  [48/84], [94mLoss[0m : 2.32598
[1mStep[0m  [56/84], [94mLoss[0m : 2.49070
[1mStep[0m  [64/84], [94mLoss[0m : 2.52729
[1mStep[0m  [72/84], [94mLoss[0m : 2.61357
[1mStep[0m  [80/84], [94mLoss[0m : 2.26384

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.333, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37953
[1mStep[0m  [8/84], [94mLoss[0m : 2.92433
[1mStep[0m  [16/84], [94mLoss[0m : 2.29521
[1mStep[0m  [24/84], [94mLoss[0m : 2.36058
[1mStep[0m  [32/84], [94mLoss[0m : 2.28627
[1mStep[0m  [40/84], [94mLoss[0m : 2.16687
[1mStep[0m  [48/84], [94mLoss[0m : 2.55951
[1mStep[0m  [56/84], [94mLoss[0m : 2.32383
[1mStep[0m  [64/84], [94mLoss[0m : 2.41153
[1mStep[0m  [72/84], [94mLoss[0m : 2.34980
[1mStep[0m  [80/84], [94mLoss[0m : 2.15708

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.331, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.57123
[1mStep[0m  [8/84], [94mLoss[0m : 2.45039
[1mStep[0m  [16/84], [94mLoss[0m : 2.66347
[1mStep[0m  [24/84], [94mLoss[0m : 2.52228
[1mStep[0m  [32/84], [94mLoss[0m : 2.40259
[1mStep[0m  [40/84], [94mLoss[0m : 1.98819
[1mStep[0m  [48/84], [94mLoss[0m : 2.40442
[1mStep[0m  [56/84], [94mLoss[0m : 2.65918
[1mStep[0m  [64/84], [94mLoss[0m : 2.12917
[1mStep[0m  [72/84], [94mLoss[0m : 2.23118
[1mStep[0m  [80/84], [94mLoss[0m : 2.22149

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.332, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31730
[1mStep[0m  [8/84], [94mLoss[0m : 2.79708
[1mStep[0m  [16/84], [94mLoss[0m : 2.31236
[1mStep[0m  [24/84], [94mLoss[0m : 2.35967
[1mStep[0m  [32/84], [94mLoss[0m : 2.42300
[1mStep[0m  [40/84], [94mLoss[0m : 2.64212
[1mStep[0m  [48/84], [94mLoss[0m : 2.49071
[1mStep[0m  [56/84], [94mLoss[0m : 2.43401
[1mStep[0m  [64/84], [94mLoss[0m : 2.63094
[1mStep[0m  [72/84], [94mLoss[0m : 2.55988
[1mStep[0m  [80/84], [94mLoss[0m : 2.39229

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.339, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37584
[1mStep[0m  [8/84], [94mLoss[0m : 2.37870
[1mStep[0m  [16/84], [94mLoss[0m : 2.46397
[1mStep[0m  [24/84], [94mLoss[0m : 2.47995
[1mStep[0m  [32/84], [94mLoss[0m : 2.66358
[1mStep[0m  [40/84], [94mLoss[0m : 2.72896
[1mStep[0m  [48/84], [94mLoss[0m : 2.26526
[1mStep[0m  [56/84], [94mLoss[0m : 2.74161
[1mStep[0m  [64/84], [94mLoss[0m : 2.13009
[1mStep[0m  [72/84], [94mLoss[0m : 2.21568
[1mStep[0m  [80/84], [94mLoss[0m : 2.34587

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.330, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.76713
[1mStep[0m  [8/84], [94mLoss[0m : 2.24180
[1mStep[0m  [16/84], [94mLoss[0m : 2.53177
[1mStep[0m  [24/84], [94mLoss[0m : 2.20741
[1mStep[0m  [32/84], [94mLoss[0m : 2.46571
[1mStep[0m  [40/84], [94mLoss[0m : 2.21182
[1mStep[0m  [48/84], [94mLoss[0m : 2.32122
[1mStep[0m  [56/84], [94mLoss[0m : 2.50149
[1mStep[0m  [64/84], [94mLoss[0m : 2.46046
[1mStep[0m  [72/84], [94mLoss[0m : 2.38357
[1mStep[0m  [80/84], [94mLoss[0m : 2.09724

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.327, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41654
[1mStep[0m  [8/84], [94mLoss[0m : 2.16391
[1mStep[0m  [16/84], [94mLoss[0m : 2.63610
[1mStep[0m  [24/84], [94mLoss[0m : 2.11976
[1mStep[0m  [32/84], [94mLoss[0m : 2.21186
[1mStep[0m  [40/84], [94mLoss[0m : 2.43548
[1mStep[0m  [48/84], [94mLoss[0m : 2.46230
[1mStep[0m  [56/84], [94mLoss[0m : 2.17416
[1mStep[0m  [64/84], [94mLoss[0m : 2.38052
[1mStep[0m  [72/84], [94mLoss[0m : 2.22297
[1mStep[0m  [80/84], [94mLoss[0m : 2.61423

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.322, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.09046
[1mStep[0m  [8/84], [94mLoss[0m : 2.55473
[1mStep[0m  [16/84], [94mLoss[0m : 2.71587
[1mStep[0m  [24/84], [94mLoss[0m : 2.13711
[1mStep[0m  [32/84], [94mLoss[0m : 2.57792
[1mStep[0m  [40/84], [94mLoss[0m : 2.30296
[1mStep[0m  [48/84], [94mLoss[0m : 2.20857
[1mStep[0m  [56/84], [94mLoss[0m : 2.20034
[1mStep[0m  [64/84], [94mLoss[0m : 2.42712
[1mStep[0m  [72/84], [94mLoss[0m : 2.28813
[1mStep[0m  [80/84], [94mLoss[0m : 2.17122

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.328, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26814
[1mStep[0m  [8/84], [94mLoss[0m : 2.33363
[1mStep[0m  [16/84], [94mLoss[0m : 2.64899
[1mStep[0m  [24/84], [94mLoss[0m : 2.47035
[1mStep[0m  [32/84], [94mLoss[0m : 2.49874
[1mStep[0m  [40/84], [94mLoss[0m : 2.29229
[1mStep[0m  [48/84], [94mLoss[0m : 2.61905
[1mStep[0m  [56/84], [94mLoss[0m : 2.41894
[1mStep[0m  [64/84], [94mLoss[0m : 2.47989
[1mStep[0m  [72/84], [94mLoss[0m : 2.35508
[1mStep[0m  [80/84], [94mLoss[0m : 2.34028

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.332, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43085
[1mStep[0m  [8/84], [94mLoss[0m : 2.01514
[1mStep[0m  [16/84], [94mLoss[0m : 2.70256
[1mStep[0m  [24/84], [94mLoss[0m : 2.54152
[1mStep[0m  [32/84], [94mLoss[0m : 2.50191
[1mStep[0m  [40/84], [94mLoss[0m : 2.50997
[1mStep[0m  [48/84], [94mLoss[0m : 2.38065
[1mStep[0m  [56/84], [94mLoss[0m : 2.55541
[1mStep[0m  [64/84], [94mLoss[0m : 2.31886
[1mStep[0m  [72/84], [94mLoss[0m : 2.66278
[1mStep[0m  [80/84], [94mLoss[0m : 2.39028

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.325, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.03100
[1mStep[0m  [8/84], [94mLoss[0m : 2.34357
[1mStep[0m  [16/84], [94mLoss[0m : 2.37313
[1mStep[0m  [24/84], [94mLoss[0m : 2.43378
[1mStep[0m  [32/84], [94mLoss[0m : 2.37724
[1mStep[0m  [40/84], [94mLoss[0m : 2.53881
[1mStep[0m  [48/84], [94mLoss[0m : 2.60189
[1mStep[0m  [56/84], [94mLoss[0m : 2.20209
[1mStep[0m  [64/84], [94mLoss[0m : 2.21225
[1mStep[0m  [72/84], [94mLoss[0m : 2.19866
[1mStep[0m  [80/84], [94mLoss[0m : 2.46596

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.325, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53243
[1mStep[0m  [8/84], [94mLoss[0m : 2.02477
[1mStep[0m  [16/84], [94mLoss[0m : 2.26288
[1mStep[0m  [24/84], [94mLoss[0m : 2.53297
[1mStep[0m  [32/84], [94mLoss[0m : 2.23039
[1mStep[0m  [40/84], [94mLoss[0m : 2.41944
[1mStep[0m  [48/84], [94mLoss[0m : 2.36227
[1mStep[0m  [56/84], [94mLoss[0m : 2.18738
[1mStep[0m  [64/84], [94mLoss[0m : 2.22001
[1mStep[0m  [72/84], [94mLoss[0m : 2.36322
[1mStep[0m  [80/84], [94mLoss[0m : 2.22535

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.333, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48538
[1mStep[0m  [8/84], [94mLoss[0m : 2.30720
[1mStep[0m  [16/84], [94mLoss[0m : 2.08291
[1mStep[0m  [24/84], [94mLoss[0m : 2.20997
[1mStep[0m  [32/84], [94mLoss[0m : 2.32702
[1mStep[0m  [40/84], [94mLoss[0m : 2.24250
[1mStep[0m  [48/84], [94mLoss[0m : 2.59111
[1mStep[0m  [56/84], [94mLoss[0m : 2.40894
[1mStep[0m  [64/84], [94mLoss[0m : 2.52777
[1mStep[0m  [72/84], [94mLoss[0m : 2.62766
[1mStep[0m  [80/84], [94mLoss[0m : 2.45577

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.322, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.28249
[1mStep[0m  [8/84], [94mLoss[0m : 2.21040
[1mStep[0m  [16/84], [94mLoss[0m : 2.32509
[1mStep[0m  [24/84], [94mLoss[0m : 2.32900
[1mStep[0m  [32/84], [94mLoss[0m : 2.29107
[1mStep[0m  [40/84], [94mLoss[0m : 2.58226
[1mStep[0m  [48/84], [94mLoss[0m : 2.26324
[1mStep[0m  [56/84], [94mLoss[0m : 2.45299
[1mStep[0m  [64/84], [94mLoss[0m : 2.36023
[1mStep[0m  [72/84], [94mLoss[0m : 2.34065
[1mStep[0m  [80/84], [94mLoss[0m : 2.42335

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.403, [92mTest[0m: 2.330, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37742
[1mStep[0m  [8/84], [94mLoss[0m : 2.59864
[1mStep[0m  [16/84], [94mLoss[0m : 2.54470
[1mStep[0m  [24/84], [94mLoss[0m : 2.56425
[1mStep[0m  [32/84], [94mLoss[0m : 2.24240
[1mStep[0m  [40/84], [94mLoss[0m : 2.54037
[1mStep[0m  [48/84], [94mLoss[0m : 2.37189
[1mStep[0m  [56/84], [94mLoss[0m : 2.21057
[1mStep[0m  [64/84], [94mLoss[0m : 2.37168
[1mStep[0m  [72/84], [94mLoss[0m : 2.11322
[1mStep[0m  [80/84], [94mLoss[0m : 2.40349

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.333, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33266
[1mStep[0m  [8/84], [94mLoss[0m : 2.24034
[1mStep[0m  [16/84], [94mLoss[0m : 2.49824
[1mStep[0m  [24/84], [94mLoss[0m : 2.21676
[1mStep[0m  [32/84], [94mLoss[0m : 2.54348
[1mStep[0m  [40/84], [94mLoss[0m : 2.44145
[1mStep[0m  [48/84], [94mLoss[0m : 2.03348
[1mStep[0m  [56/84], [94mLoss[0m : 2.47293
[1mStep[0m  [64/84], [94mLoss[0m : 2.34037
[1mStep[0m  [72/84], [94mLoss[0m : 2.43515
[1mStep[0m  [80/84], [94mLoss[0m : 2.69539

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.331, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55418
[1mStep[0m  [8/84], [94mLoss[0m : 2.45863
[1mStep[0m  [16/84], [94mLoss[0m : 2.20308
[1mStep[0m  [24/84], [94mLoss[0m : 2.72133
[1mStep[0m  [32/84], [94mLoss[0m : 2.41950
[1mStep[0m  [40/84], [94mLoss[0m : 2.58100
[1mStep[0m  [48/84], [94mLoss[0m : 2.58811
[1mStep[0m  [56/84], [94mLoss[0m : 2.55644
[1mStep[0m  [64/84], [94mLoss[0m : 2.62250
[1mStep[0m  [72/84], [94mLoss[0m : 2.34066
[1mStep[0m  [80/84], [94mLoss[0m : 2.57338

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.325, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.326
====================================

Phase 1 - Evaluation MAE:  2.3262873717716763
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 2.50173
[1mStep[0m  [8/84], [94mLoss[0m : 2.44025
[1mStep[0m  [16/84], [94mLoss[0m : 2.64522
[1mStep[0m  [24/84], [94mLoss[0m : 2.42149
[1mStep[0m  [32/84], [94mLoss[0m : 2.33888
[1mStep[0m  [40/84], [94mLoss[0m : 2.26658
[1mStep[0m  [48/84], [94mLoss[0m : 2.32321
[1mStep[0m  [56/84], [94mLoss[0m : 2.66973
[1mStep[0m  [64/84], [94mLoss[0m : 2.19848
[1mStep[0m  [72/84], [94mLoss[0m : 2.55964
[1mStep[0m  [80/84], [94mLoss[0m : 2.24304

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.418, [92mTest[0m: 2.330, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30502
[1mStep[0m  [8/84], [94mLoss[0m : 2.38498
[1mStep[0m  [16/84], [94mLoss[0m : 2.54739
[1mStep[0m  [24/84], [94mLoss[0m : 2.32761
[1mStep[0m  [32/84], [94mLoss[0m : 2.42901
[1mStep[0m  [40/84], [94mLoss[0m : 2.59712
[1mStep[0m  [48/84], [94mLoss[0m : 2.37090
[1mStep[0m  [56/84], [94mLoss[0m : 2.16374
[1mStep[0m  [64/84], [94mLoss[0m : 2.21890
[1mStep[0m  [72/84], [94mLoss[0m : 2.10967
[1mStep[0m  [80/84], [94mLoss[0m : 2.23531

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.373, [92mTest[0m: 2.399, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.51360
[1mStep[0m  [8/84], [94mLoss[0m : 2.49842
[1mStep[0m  [16/84], [94mLoss[0m : 2.04118
[1mStep[0m  [24/84], [94mLoss[0m : 1.98458
[1mStep[0m  [32/84], [94mLoss[0m : 2.62607
[1mStep[0m  [40/84], [94mLoss[0m : 2.30342
[1mStep[0m  [48/84], [94mLoss[0m : 2.57774
[1mStep[0m  [56/84], [94mLoss[0m : 2.38912
[1mStep[0m  [64/84], [94mLoss[0m : 2.21358
[1mStep[0m  [72/84], [94mLoss[0m : 2.47300
[1mStep[0m  [80/84], [94mLoss[0m : 2.39552

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.338, [92mTest[0m: 2.526, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42146
[1mStep[0m  [8/84], [94mLoss[0m : 2.17474
[1mStep[0m  [16/84], [94mLoss[0m : 2.53111
[1mStep[0m  [24/84], [94mLoss[0m : 2.02796
[1mStep[0m  [32/84], [94mLoss[0m : 2.02894
[1mStep[0m  [40/84], [94mLoss[0m : 2.49949
[1mStep[0m  [48/84], [94mLoss[0m : 2.17856
[1mStep[0m  [56/84], [94mLoss[0m : 2.38265
[1mStep[0m  [64/84], [94mLoss[0m : 2.10002
[1mStep[0m  [72/84], [94mLoss[0m : 2.35962
[1mStep[0m  [80/84], [94mLoss[0m : 2.43220

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.292, [92mTest[0m: 2.580, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.07007
[1mStep[0m  [8/84], [94mLoss[0m : 2.10716
[1mStep[0m  [16/84], [94mLoss[0m : 2.28144
[1mStep[0m  [24/84], [94mLoss[0m : 2.49095
[1mStep[0m  [32/84], [94mLoss[0m : 2.26808
[1mStep[0m  [40/84], [94mLoss[0m : 2.29644
[1mStep[0m  [48/84], [94mLoss[0m : 2.56119
[1mStep[0m  [56/84], [94mLoss[0m : 2.11357
[1mStep[0m  [64/84], [94mLoss[0m : 2.13676
[1mStep[0m  [72/84], [94mLoss[0m : 2.16882
[1mStep[0m  [80/84], [94mLoss[0m : 2.39780

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.246, [92mTest[0m: 2.513, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30332
[1mStep[0m  [8/84], [94mLoss[0m : 2.27945
[1mStep[0m  [16/84], [94mLoss[0m : 2.47949
[1mStep[0m  [24/84], [94mLoss[0m : 2.35663
[1mStep[0m  [32/84], [94mLoss[0m : 2.24791
[1mStep[0m  [40/84], [94mLoss[0m : 1.98888
[1mStep[0m  [48/84], [94mLoss[0m : 2.19394
[1mStep[0m  [56/84], [94mLoss[0m : 2.56766
[1mStep[0m  [64/84], [94mLoss[0m : 2.20353
[1mStep[0m  [72/84], [94mLoss[0m : 2.04725
[1mStep[0m  [80/84], [94mLoss[0m : 2.38292

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.207, [92mTest[0m: 2.512, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33921
[1mStep[0m  [8/84], [94mLoss[0m : 2.22453
[1mStep[0m  [16/84], [94mLoss[0m : 1.99870
[1mStep[0m  [24/84], [94mLoss[0m : 2.21599
[1mStep[0m  [32/84], [94mLoss[0m : 2.37622
[1mStep[0m  [40/84], [94mLoss[0m : 2.07872
[1mStep[0m  [48/84], [94mLoss[0m : 2.27368
[1mStep[0m  [56/84], [94mLoss[0m : 2.04203
[1mStep[0m  [64/84], [94mLoss[0m : 2.29866
[1mStep[0m  [72/84], [94mLoss[0m : 2.02607
[1mStep[0m  [80/84], [94mLoss[0m : 2.11462

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.151, [92mTest[0m: 2.517, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.11345
[1mStep[0m  [8/84], [94mLoss[0m : 1.82268
[1mStep[0m  [16/84], [94mLoss[0m : 2.07111
[1mStep[0m  [24/84], [94mLoss[0m : 2.16305
[1mStep[0m  [32/84], [94mLoss[0m : 2.17941
[1mStep[0m  [40/84], [94mLoss[0m : 1.97175
[1mStep[0m  [48/84], [94mLoss[0m : 2.15056
[1mStep[0m  [56/84], [94mLoss[0m : 2.25709
[1mStep[0m  [64/84], [94mLoss[0m : 2.40384
[1mStep[0m  [72/84], [94mLoss[0m : 2.15788
[1mStep[0m  [80/84], [94mLoss[0m : 2.13085

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.106, [92mTest[0m: 2.439, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.94818
[1mStep[0m  [8/84], [94mLoss[0m : 2.12198
[1mStep[0m  [16/84], [94mLoss[0m : 2.09722
[1mStep[0m  [24/84], [94mLoss[0m : 2.03200
[1mStep[0m  [32/84], [94mLoss[0m : 1.97962
[1mStep[0m  [40/84], [94mLoss[0m : 2.17836
[1mStep[0m  [48/84], [94mLoss[0m : 2.48316
[1mStep[0m  [56/84], [94mLoss[0m : 2.18342
[1mStep[0m  [64/84], [94mLoss[0m : 1.88598
[1mStep[0m  [72/84], [94mLoss[0m : 2.11616
[1mStep[0m  [80/84], [94mLoss[0m : 2.01994

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.063, [92mTest[0m: 2.442, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.82847
[1mStep[0m  [8/84], [94mLoss[0m : 1.87691
[1mStep[0m  [16/84], [94mLoss[0m : 2.15144
[1mStep[0m  [24/84], [94mLoss[0m : 2.15960
[1mStep[0m  [32/84], [94mLoss[0m : 2.02805
[1mStep[0m  [40/84], [94mLoss[0m : 2.15754
[1mStep[0m  [48/84], [94mLoss[0m : 1.73963
[1mStep[0m  [56/84], [94mLoss[0m : 2.27174
[1mStep[0m  [64/84], [94mLoss[0m : 1.99488
[1mStep[0m  [72/84], [94mLoss[0m : 1.91657
[1mStep[0m  [80/84], [94mLoss[0m : 1.89309

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.007, [92mTest[0m: 2.508, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.02390
[1mStep[0m  [8/84], [94mLoss[0m : 1.95757
[1mStep[0m  [16/84], [94mLoss[0m : 1.92785
[1mStep[0m  [24/84], [94mLoss[0m : 2.02904
[1mStep[0m  [32/84], [94mLoss[0m : 1.71991
[1mStep[0m  [40/84], [94mLoss[0m : 2.00560
[1mStep[0m  [48/84], [94mLoss[0m : 2.13672
[1mStep[0m  [56/84], [94mLoss[0m : 2.02032
[1mStep[0m  [64/84], [94mLoss[0m : 1.95743
[1mStep[0m  [72/84], [94mLoss[0m : 2.22105
[1mStep[0m  [80/84], [94mLoss[0m : 1.88809

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.963, [92mTest[0m: 2.439, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.97602
[1mStep[0m  [8/84], [94mLoss[0m : 1.93718
[1mStep[0m  [16/84], [94mLoss[0m : 2.05997
[1mStep[0m  [24/84], [94mLoss[0m : 1.72785
[1mStep[0m  [32/84], [94mLoss[0m : 1.88920
[1mStep[0m  [40/84], [94mLoss[0m : 2.05616
[1mStep[0m  [48/84], [94mLoss[0m : 1.96986
[1mStep[0m  [56/84], [94mLoss[0m : 1.87679
[1mStep[0m  [64/84], [94mLoss[0m : 2.32360
[1mStep[0m  [72/84], [94mLoss[0m : 1.71473
[1mStep[0m  [80/84], [94mLoss[0m : 1.74837

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.905, [92mTest[0m: 2.454, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.79641
[1mStep[0m  [8/84], [94mLoss[0m : 1.85255
[1mStep[0m  [16/84], [94mLoss[0m : 1.76662
[1mStep[0m  [24/84], [94mLoss[0m : 1.70373
[1mStep[0m  [32/84], [94mLoss[0m : 1.70302
[1mStep[0m  [40/84], [94mLoss[0m : 1.94289
[1mStep[0m  [48/84], [94mLoss[0m : 2.27439
[1mStep[0m  [56/84], [94mLoss[0m : 1.59919
[1mStep[0m  [64/84], [94mLoss[0m : 1.83381
[1mStep[0m  [72/84], [94mLoss[0m : 1.69749
[1mStep[0m  [80/84], [94mLoss[0m : 2.28889

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.872, [92mTest[0m: 2.402, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.69014
[1mStep[0m  [8/84], [94mLoss[0m : 1.68262
[1mStep[0m  [16/84], [94mLoss[0m : 1.58878
[1mStep[0m  [24/84], [94mLoss[0m : 1.72679
[1mStep[0m  [32/84], [94mLoss[0m : 1.91471
[1mStep[0m  [40/84], [94mLoss[0m : 1.86277
[1mStep[0m  [48/84], [94mLoss[0m : 1.86754
[1mStep[0m  [56/84], [94mLoss[0m : 1.87511
[1mStep[0m  [64/84], [94mLoss[0m : 1.89035
[1mStep[0m  [72/84], [94mLoss[0m : 1.81690
[1mStep[0m  [80/84], [94mLoss[0m : 1.81326

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.833, [92mTest[0m: 2.472, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.75182
[1mStep[0m  [8/84], [94mLoss[0m : 1.83494
[1mStep[0m  [16/84], [94mLoss[0m : 1.82047
[1mStep[0m  [24/84], [94mLoss[0m : 1.91693
[1mStep[0m  [32/84], [94mLoss[0m : 1.69631
[1mStep[0m  [40/84], [94mLoss[0m : 1.69022
[1mStep[0m  [48/84], [94mLoss[0m : 1.91866
[1mStep[0m  [56/84], [94mLoss[0m : 1.97845
[1mStep[0m  [64/84], [94mLoss[0m : 1.73034
[1mStep[0m  [72/84], [94mLoss[0m : 1.66142
[1mStep[0m  [80/84], [94mLoss[0m : 1.79250

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.789, [92mTest[0m: 2.426, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.56597
[1mStep[0m  [8/84], [94mLoss[0m : 1.90868
[1mStep[0m  [16/84], [94mLoss[0m : 1.50312
[1mStep[0m  [24/84], [94mLoss[0m : 1.84876
[1mStep[0m  [32/84], [94mLoss[0m : 1.50101
[1mStep[0m  [40/84], [94mLoss[0m : 1.69782
[1mStep[0m  [48/84], [94mLoss[0m : 1.77451
[1mStep[0m  [56/84], [94mLoss[0m : 1.79604
[1mStep[0m  [64/84], [94mLoss[0m : 1.64090
[1mStep[0m  [72/84], [94mLoss[0m : 1.82405
[1mStep[0m  [80/84], [94mLoss[0m : 1.64247

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.749, [92mTest[0m: 2.438, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.77382
[1mStep[0m  [8/84], [94mLoss[0m : 1.46388
[1mStep[0m  [16/84], [94mLoss[0m : 1.97969
[1mStep[0m  [24/84], [94mLoss[0m : 1.63142
[1mStep[0m  [32/84], [94mLoss[0m : 1.55713
[1mStep[0m  [40/84], [94mLoss[0m : 1.74580
[1mStep[0m  [48/84], [94mLoss[0m : 1.74420
[1mStep[0m  [56/84], [94mLoss[0m : 1.78393
[1mStep[0m  [64/84], [94mLoss[0m : 1.56859
[1mStep[0m  [72/84], [94mLoss[0m : 1.84736
[1mStep[0m  [80/84], [94mLoss[0m : 1.59435

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.704, [92mTest[0m: 2.613, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.45055
[1mStep[0m  [8/84], [94mLoss[0m : 1.82815
[1mStep[0m  [16/84], [94mLoss[0m : 1.61714
[1mStep[0m  [24/84], [94mLoss[0m : 1.36835
[1mStep[0m  [32/84], [94mLoss[0m : 1.77289
[1mStep[0m  [40/84], [94mLoss[0m : 1.81705
[1mStep[0m  [48/84], [94mLoss[0m : 1.62219
[1mStep[0m  [56/84], [94mLoss[0m : 1.55479
[1mStep[0m  [64/84], [94mLoss[0m : 1.91179
[1mStep[0m  [72/84], [94mLoss[0m : 1.60505
[1mStep[0m  [80/84], [94mLoss[0m : 1.60455

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.664, [92mTest[0m: 2.504, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.85873
[1mStep[0m  [8/84], [94mLoss[0m : 1.84620
[1mStep[0m  [16/84], [94mLoss[0m : 1.73815
[1mStep[0m  [24/84], [94mLoss[0m : 1.53568
[1mStep[0m  [32/84], [94mLoss[0m : 1.58816
[1mStep[0m  [40/84], [94mLoss[0m : 1.46891
[1mStep[0m  [48/84], [94mLoss[0m : 1.69070
[1mStep[0m  [56/84], [94mLoss[0m : 1.67353
[1mStep[0m  [64/84], [94mLoss[0m : 1.68535
[1mStep[0m  [72/84], [94mLoss[0m : 1.66222
[1mStep[0m  [80/84], [94mLoss[0m : 1.72648

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.638, [92mTest[0m: 2.483, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.67862
[1mStep[0m  [8/84], [94mLoss[0m : 1.61087
[1mStep[0m  [16/84], [94mLoss[0m : 1.57327
[1mStep[0m  [24/84], [94mLoss[0m : 1.71205
[1mStep[0m  [32/84], [94mLoss[0m : 1.93948
[1mStep[0m  [40/84], [94mLoss[0m : 1.36708
[1mStep[0m  [48/84], [94mLoss[0m : 1.48268
[1mStep[0m  [56/84], [94mLoss[0m : 1.41249
[1mStep[0m  [64/84], [94mLoss[0m : 1.65233
[1mStep[0m  [72/84], [94mLoss[0m : 1.53550
[1mStep[0m  [80/84], [94mLoss[0m : 1.70018

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.608, [92mTest[0m: 2.826, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.67061
[1mStep[0m  [8/84], [94mLoss[0m : 1.65304
[1mStep[0m  [16/84], [94mLoss[0m : 1.63394
[1mStep[0m  [24/84], [94mLoss[0m : 1.65578
[1mStep[0m  [32/84], [94mLoss[0m : 1.56652
[1mStep[0m  [40/84], [94mLoss[0m : 1.43635
[1mStep[0m  [48/84], [94mLoss[0m : 1.52504
[1mStep[0m  [56/84], [94mLoss[0m : 1.47268
[1mStep[0m  [64/84], [94mLoss[0m : 1.56431
[1mStep[0m  [72/84], [94mLoss[0m : 1.54535
[1mStep[0m  [80/84], [94mLoss[0m : 1.63370

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.557, [92mTest[0m: 2.507, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.43043
[1mStep[0m  [8/84], [94mLoss[0m : 1.39593
[1mStep[0m  [16/84], [94mLoss[0m : 1.60476
[1mStep[0m  [24/84], [94mLoss[0m : 1.53082
[1mStep[0m  [32/84], [94mLoss[0m : 1.51178
[1mStep[0m  [40/84], [94mLoss[0m : 1.44433
[1mStep[0m  [48/84], [94mLoss[0m : 1.87053
[1mStep[0m  [56/84], [94mLoss[0m : 1.52398
[1mStep[0m  [64/84], [94mLoss[0m : 1.32788
[1mStep[0m  [72/84], [94mLoss[0m : 1.31920
[1mStep[0m  [80/84], [94mLoss[0m : 1.44286

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.525, [92mTest[0m: 2.500, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.59265
[1mStep[0m  [8/84], [94mLoss[0m : 1.45583
[1mStep[0m  [16/84], [94mLoss[0m : 1.58842
[1mStep[0m  [24/84], [94mLoss[0m : 1.49537
[1mStep[0m  [32/84], [94mLoss[0m : 1.58523
[1mStep[0m  [40/84], [94mLoss[0m : 1.64943
[1mStep[0m  [48/84], [94mLoss[0m : 1.45291
[1mStep[0m  [56/84], [94mLoss[0m : 1.29370
[1mStep[0m  [64/84], [94mLoss[0m : 1.45836
[1mStep[0m  [72/84], [94mLoss[0m : 1.42284
[1mStep[0m  [80/84], [94mLoss[0m : 1.50758

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.490, [92mTest[0m: 2.532, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.35433
[1mStep[0m  [8/84], [94mLoss[0m : 1.21854
[1mStep[0m  [16/84], [94mLoss[0m : 1.44930
[1mStep[0m  [24/84], [94mLoss[0m : 1.63329
[1mStep[0m  [32/84], [94mLoss[0m : 1.19711
[1mStep[0m  [40/84], [94mLoss[0m : 1.37997
[1mStep[0m  [48/84], [94mLoss[0m : 1.35636
[1mStep[0m  [56/84], [94mLoss[0m : 1.49923
[1mStep[0m  [64/84], [94mLoss[0m : 1.55401
[1mStep[0m  [72/84], [94mLoss[0m : 1.71938
[1mStep[0m  [80/84], [94mLoss[0m : 1.41780

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.476, [92mTest[0m: 2.504, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.35173
[1mStep[0m  [8/84], [94mLoss[0m : 1.32846
[1mStep[0m  [16/84], [94mLoss[0m : 1.44753
[1mStep[0m  [24/84], [94mLoss[0m : 1.58464
[1mStep[0m  [32/84], [94mLoss[0m : 1.30538
[1mStep[0m  [40/84], [94mLoss[0m : 1.32908
[1mStep[0m  [48/84], [94mLoss[0m : 1.32263
[1mStep[0m  [56/84], [94mLoss[0m : 1.50570
[1mStep[0m  [64/84], [94mLoss[0m : 1.49613
[1mStep[0m  [72/84], [94mLoss[0m : 1.48324
[1mStep[0m  [80/84], [94mLoss[0m : 1.40199

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.455, [92mTest[0m: 2.447, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.55420
[1mStep[0m  [8/84], [94mLoss[0m : 1.24958
[1mStep[0m  [16/84], [94mLoss[0m : 1.45191
[1mStep[0m  [24/84], [94mLoss[0m : 1.40560
[1mStep[0m  [32/84], [94mLoss[0m : 1.36074
[1mStep[0m  [40/84], [94mLoss[0m : 1.34385
[1mStep[0m  [48/84], [94mLoss[0m : 1.34424
[1mStep[0m  [56/84], [94mLoss[0m : 1.43522
[1mStep[0m  [64/84], [94mLoss[0m : 1.55002
[1mStep[0m  [72/84], [94mLoss[0m : 1.62268
[1mStep[0m  [80/84], [94mLoss[0m : 1.66562

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.443, [92mTest[0m: 2.502, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.45008
[1mStep[0m  [8/84], [94mLoss[0m : 1.29512
[1mStep[0m  [16/84], [94mLoss[0m : 1.41681
[1mStep[0m  [24/84], [94mLoss[0m : 1.31109
[1mStep[0m  [32/84], [94mLoss[0m : 1.52217
[1mStep[0m  [40/84], [94mLoss[0m : 1.34919
[1mStep[0m  [48/84], [94mLoss[0m : 1.41515
[1mStep[0m  [56/84], [94mLoss[0m : 1.45407
[1mStep[0m  [64/84], [94mLoss[0m : 1.43290
[1mStep[0m  [72/84], [94mLoss[0m : 1.28527
[1mStep[0m  [80/84], [94mLoss[0m : 1.12663

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.410, [92mTest[0m: 2.560, [96mlr[0m: 0.006772500000000002
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 26 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.495
====================================

Phase 2 - Evaluation MAE:  2.494740801198142
MAE score P1      2.326287
MAE score P2      2.494741
loss              1.409645
learning_rate     0.007525
batch_size             128
hidden_sizes         [250]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping        True
dropout                0.2
momentum               0.1
weight_decay        0.0001
Name: 14, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 9.78823
[1mStep[0m  [8/84], [94mLoss[0m : 4.97724
[1mStep[0m  [16/84], [94mLoss[0m : 3.41457
[1mStep[0m  [24/84], [94mLoss[0m : 2.98706
[1mStep[0m  [32/84], [94mLoss[0m : 2.90811
[1mStep[0m  [40/84], [94mLoss[0m : 2.41432
[1mStep[0m  [48/84], [94mLoss[0m : 2.79749
[1mStep[0m  [56/84], [94mLoss[0m : 2.63917
[1mStep[0m  [64/84], [94mLoss[0m : 2.46475
[1mStep[0m  [72/84], [94mLoss[0m : 2.69803
[1mStep[0m  [80/84], [94mLoss[0m : 2.51119

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.338, [92mTest[0m: 10.385, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50915
[1mStep[0m  [8/84], [94mLoss[0m : 2.58671
[1mStep[0m  [16/84], [94mLoss[0m : 2.63683
[1mStep[0m  [24/84], [94mLoss[0m : 2.58112
[1mStep[0m  [32/84], [94mLoss[0m : 2.62862
[1mStep[0m  [40/84], [94mLoss[0m : 2.30438
[1mStep[0m  [48/84], [94mLoss[0m : 2.45163
[1mStep[0m  [56/84], [94mLoss[0m : 2.51972
[1mStep[0m  [64/84], [94mLoss[0m : 2.59155
[1mStep[0m  [72/84], [94mLoss[0m : 2.48193
[1mStep[0m  [80/84], [94mLoss[0m : 2.53303

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.531, [92mTest[0m: 2.356, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.64696
[1mStep[0m  [8/84], [94mLoss[0m : 2.11036
[1mStep[0m  [16/84], [94mLoss[0m : 2.28459
[1mStep[0m  [24/84], [94mLoss[0m : 2.45802
[1mStep[0m  [32/84], [94mLoss[0m : 2.53731
[1mStep[0m  [40/84], [94mLoss[0m : 2.66064
[1mStep[0m  [48/84], [94mLoss[0m : 2.56452
[1mStep[0m  [56/84], [94mLoss[0m : 2.60480
[1mStep[0m  [64/84], [94mLoss[0m : 2.77656
[1mStep[0m  [72/84], [94mLoss[0m : 2.53612
[1mStep[0m  [80/84], [94mLoss[0m : 2.25126

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.345, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45913
[1mStep[0m  [8/84], [94mLoss[0m : 2.49764
[1mStep[0m  [16/84], [94mLoss[0m : 2.72701
[1mStep[0m  [24/84], [94mLoss[0m : 2.24148
[1mStep[0m  [32/84], [94mLoss[0m : 2.56653
[1mStep[0m  [40/84], [94mLoss[0m : 2.51459
[1mStep[0m  [48/84], [94mLoss[0m : 2.17899
[1mStep[0m  [56/84], [94mLoss[0m : 2.48627
[1mStep[0m  [64/84], [94mLoss[0m : 2.52528
[1mStep[0m  [72/84], [94mLoss[0m : 2.59470
[1mStep[0m  [80/84], [94mLoss[0m : 2.77715

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.338, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49011
[1mStep[0m  [8/84], [94mLoss[0m : 2.55388
[1mStep[0m  [16/84], [94mLoss[0m : 2.53341
[1mStep[0m  [24/84], [94mLoss[0m : 2.66043
[1mStep[0m  [32/84], [94mLoss[0m : 2.44714
[1mStep[0m  [40/84], [94mLoss[0m : 2.41414
[1mStep[0m  [48/84], [94mLoss[0m : 2.22160
[1mStep[0m  [56/84], [94mLoss[0m : 2.57410
[1mStep[0m  [64/84], [94mLoss[0m : 2.53720
[1mStep[0m  [72/84], [94mLoss[0m : 2.47833
[1mStep[0m  [80/84], [94mLoss[0m : 2.53761

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.342, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.34534
[1mStep[0m  [8/84], [94mLoss[0m : 2.74791
[1mStep[0m  [16/84], [94mLoss[0m : 2.57383
[1mStep[0m  [24/84], [94mLoss[0m : 2.39858
[1mStep[0m  [32/84], [94mLoss[0m : 2.53372
[1mStep[0m  [40/84], [94mLoss[0m : 2.23850
[1mStep[0m  [48/84], [94mLoss[0m : 2.32022
[1mStep[0m  [56/84], [94mLoss[0m : 2.39931
[1mStep[0m  [64/84], [94mLoss[0m : 2.64436
[1mStep[0m  [72/84], [94mLoss[0m : 2.34776
[1mStep[0m  [80/84], [94mLoss[0m : 2.32787

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.325, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.57586
[1mStep[0m  [8/84], [94mLoss[0m : 2.47241
[1mStep[0m  [16/84], [94mLoss[0m : 2.24629
[1mStep[0m  [24/84], [94mLoss[0m : 2.37230
[1mStep[0m  [32/84], [94mLoss[0m : 2.29322
[1mStep[0m  [40/84], [94mLoss[0m : 2.36174
[1mStep[0m  [48/84], [94mLoss[0m : 2.57636
[1mStep[0m  [56/84], [94mLoss[0m : 2.34491
[1mStep[0m  [64/84], [94mLoss[0m : 2.64267
[1mStep[0m  [72/84], [94mLoss[0m : 2.19598
[1mStep[0m  [80/84], [94mLoss[0m : 2.34856

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.367, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.88775
[1mStep[0m  [8/84], [94mLoss[0m : 2.62745
[1mStep[0m  [16/84], [94mLoss[0m : 2.67894
[1mStep[0m  [24/84], [94mLoss[0m : 2.42264
[1mStep[0m  [32/84], [94mLoss[0m : 2.44561
[1mStep[0m  [40/84], [94mLoss[0m : 2.44131
[1mStep[0m  [48/84], [94mLoss[0m : 2.62466
[1mStep[0m  [56/84], [94mLoss[0m : 2.34862
[1mStep[0m  [64/84], [94mLoss[0m : 2.37367
[1mStep[0m  [72/84], [94mLoss[0m : 2.70521
[1mStep[0m  [80/84], [94mLoss[0m : 2.83071

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.336, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.25241
[1mStep[0m  [8/84], [94mLoss[0m : 2.41212
[1mStep[0m  [16/84], [94mLoss[0m : 2.49700
[1mStep[0m  [24/84], [94mLoss[0m : 2.47763
[1mStep[0m  [32/84], [94mLoss[0m : 2.57821
[1mStep[0m  [40/84], [94mLoss[0m : 2.62492
[1mStep[0m  [48/84], [94mLoss[0m : 2.73231
[1mStep[0m  [56/84], [94mLoss[0m : 2.34414
[1mStep[0m  [64/84], [94mLoss[0m : 2.50217
[1mStep[0m  [72/84], [94mLoss[0m : 2.39377
[1mStep[0m  [80/84], [94mLoss[0m : 2.57887

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.333, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.05637
[1mStep[0m  [8/84], [94mLoss[0m : 2.38340
[1mStep[0m  [16/84], [94mLoss[0m : 2.81270
[1mStep[0m  [24/84], [94mLoss[0m : 2.19430
[1mStep[0m  [32/84], [94mLoss[0m : 2.30625
[1mStep[0m  [40/84], [94mLoss[0m : 2.25699
[1mStep[0m  [48/84], [94mLoss[0m : 2.66851
[1mStep[0m  [56/84], [94mLoss[0m : 2.60371
[1mStep[0m  [64/84], [94mLoss[0m : 2.70288
[1mStep[0m  [72/84], [94mLoss[0m : 2.60121
[1mStep[0m  [80/84], [94mLoss[0m : 2.57791

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.332, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.64394
[1mStep[0m  [8/84], [94mLoss[0m : 2.05818
[1mStep[0m  [16/84], [94mLoss[0m : 2.38268
[1mStep[0m  [24/84], [94mLoss[0m : 2.56194
[1mStep[0m  [32/84], [94mLoss[0m : 2.23168
[1mStep[0m  [40/84], [94mLoss[0m : 2.60039
[1mStep[0m  [48/84], [94mLoss[0m : 2.20846
[1mStep[0m  [56/84], [94mLoss[0m : 2.67683
[1mStep[0m  [64/84], [94mLoss[0m : 2.36182
[1mStep[0m  [72/84], [94mLoss[0m : 2.31728
[1mStep[0m  [80/84], [94mLoss[0m : 2.46558

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.330, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.76032
[1mStep[0m  [8/84], [94mLoss[0m : 2.37017
[1mStep[0m  [16/84], [94mLoss[0m : 2.42239
[1mStep[0m  [24/84], [94mLoss[0m : 2.43910
[1mStep[0m  [32/84], [94mLoss[0m : 2.28418
[1mStep[0m  [40/84], [94mLoss[0m : 2.30035
[1mStep[0m  [48/84], [94mLoss[0m : 2.35987
[1mStep[0m  [56/84], [94mLoss[0m : 2.41379
[1mStep[0m  [64/84], [94mLoss[0m : 2.44433
[1mStep[0m  [72/84], [94mLoss[0m : 2.39547
[1mStep[0m  [80/84], [94mLoss[0m : 2.60162

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.332, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67854
[1mStep[0m  [8/84], [94mLoss[0m : 2.36777
[1mStep[0m  [16/84], [94mLoss[0m : 2.26368
[1mStep[0m  [24/84], [94mLoss[0m : 2.61711
[1mStep[0m  [32/84], [94mLoss[0m : 2.52795
[1mStep[0m  [40/84], [94mLoss[0m : 2.64191
[1mStep[0m  [48/84], [94mLoss[0m : 2.70282
[1mStep[0m  [56/84], [94mLoss[0m : 2.22265
[1mStep[0m  [64/84], [94mLoss[0m : 2.28567
[1mStep[0m  [72/84], [94mLoss[0m : 2.54436
[1mStep[0m  [80/84], [94mLoss[0m : 2.51283

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.339, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.63079
[1mStep[0m  [8/84], [94mLoss[0m : 2.22719
[1mStep[0m  [16/84], [94mLoss[0m : 2.57267
[1mStep[0m  [24/84], [94mLoss[0m : 2.33014
[1mStep[0m  [32/84], [94mLoss[0m : 2.30503
[1mStep[0m  [40/84], [94mLoss[0m : 2.31038
[1mStep[0m  [48/84], [94mLoss[0m : 2.35326
[1mStep[0m  [56/84], [94mLoss[0m : 2.31143
[1mStep[0m  [64/84], [94mLoss[0m : 2.53981
[1mStep[0m  [72/84], [94mLoss[0m : 2.52864
[1mStep[0m  [80/84], [94mLoss[0m : 2.71212

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.330, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42194
[1mStep[0m  [8/84], [94mLoss[0m : 2.36950
[1mStep[0m  [16/84], [94mLoss[0m : 2.00933
[1mStep[0m  [24/84], [94mLoss[0m : 2.14517
[1mStep[0m  [32/84], [94mLoss[0m : 2.47922
[1mStep[0m  [40/84], [94mLoss[0m : 2.67587
[1mStep[0m  [48/84], [94mLoss[0m : 2.76782
[1mStep[0m  [56/84], [94mLoss[0m : 2.32238
[1mStep[0m  [64/84], [94mLoss[0m : 2.54553
[1mStep[0m  [72/84], [94mLoss[0m : 2.57928
[1mStep[0m  [80/84], [94mLoss[0m : 2.78141

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.331, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31653
[1mStep[0m  [8/84], [94mLoss[0m : 2.65565
[1mStep[0m  [16/84], [94mLoss[0m : 2.68948
[1mStep[0m  [24/84], [94mLoss[0m : 2.38996
[1mStep[0m  [32/84], [94mLoss[0m : 2.55122
[1mStep[0m  [40/84], [94mLoss[0m : 2.35912
[1mStep[0m  [48/84], [94mLoss[0m : 2.61514
[1mStep[0m  [56/84], [94mLoss[0m : 2.65885
[1mStep[0m  [64/84], [94mLoss[0m : 2.36985
[1mStep[0m  [72/84], [94mLoss[0m : 2.32647
[1mStep[0m  [80/84], [94mLoss[0m : 2.21779

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.335, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40024
[1mStep[0m  [8/84], [94mLoss[0m : 2.41052
[1mStep[0m  [16/84], [94mLoss[0m : 2.53954
[1mStep[0m  [24/84], [94mLoss[0m : 2.49770
[1mStep[0m  [32/84], [94mLoss[0m : 2.31799
[1mStep[0m  [40/84], [94mLoss[0m : 2.29992
[1mStep[0m  [48/84], [94mLoss[0m : 2.34118
[1mStep[0m  [56/84], [94mLoss[0m : 2.71235
[1mStep[0m  [64/84], [94mLoss[0m : 2.48153
[1mStep[0m  [72/84], [94mLoss[0m : 2.35138
[1mStep[0m  [80/84], [94mLoss[0m : 2.50093

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.331, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35306
[1mStep[0m  [8/84], [94mLoss[0m : 2.29722
[1mStep[0m  [16/84], [94mLoss[0m : 2.51756
[1mStep[0m  [24/84], [94mLoss[0m : 2.61135
[1mStep[0m  [32/84], [94mLoss[0m : 2.37697
[1mStep[0m  [40/84], [94mLoss[0m : 2.37944
[1mStep[0m  [48/84], [94mLoss[0m : 2.29513
[1mStep[0m  [56/84], [94mLoss[0m : 2.47800
[1mStep[0m  [64/84], [94mLoss[0m : 2.34069
[1mStep[0m  [72/84], [94mLoss[0m : 2.27633
[1mStep[0m  [80/84], [94mLoss[0m : 2.41059

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.327, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.70941
[1mStep[0m  [8/84], [94mLoss[0m : 2.43896
[1mStep[0m  [16/84], [94mLoss[0m : 2.05181
[1mStep[0m  [24/84], [94mLoss[0m : 2.44254
[1mStep[0m  [32/84], [94mLoss[0m : 2.22934
[1mStep[0m  [40/84], [94mLoss[0m : 2.60363
[1mStep[0m  [48/84], [94mLoss[0m : 2.37687
[1mStep[0m  [56/84], [94mLoss[0m : 2.37063
[1mStep[0m  [64/84], [94mLoss[0m : 2.74605
[1mStep[0m  [72/84], [94mLoss[0m : 2.50096
[1mStep[0m  [80/84], [94mLoss[0m : 2.25444

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.341, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.17099
[1mStep[0m  [8/84], [94mLoss[0m : 2.56795
[1mStep[0m  [16/84], [94mLoss[0m : 2.46970
[1mStep[0m  [24/84], [94mLoss[0m : 2.13464
[1mStep[0m  [32/84], [94mLoss[0m : 2.22831
[1mStep[0m  [40/84], [94mLoss[0m : 2.37597
[1mStep[0m  [48/84], [94mLoss[0m : 2.54002
[1mStep[0m  [56/84], [94mLoss[0m : 2.56508
[1mStep[0m  [64/84], [94mLoss[0m : 2.36039
[1mStep[0m  [72/84], [94mLoss[0m : 2.26271
[1mStep[0m  [80/84], [94mLoss[0m : 2.62075

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.338, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.78337
[1mStep[0m  [8/84], [94mLoss[0m : 2.20724
[1mStep[0m  [16/84], [94mLoss[0m : 2.71576
[1mStep[0m  [24/84], [94mLoss[0m : 2.52930
[1mStep[0m  [32/84], [94mLoss[0m : 2.34751
[1mStep[0m  [40/84], [94mLoss[0m : 2.56558
[1mStep[0m  [48/84], [94mLoss[0m : 2.21992
[1mStep[0m  [56/84], [94mLoss[0m : 2.53125
[1mStep[0m  [64/84], [94mLoss[0m : 2.33229
[1mStep[0m  [72/84], [94mLoss[0m : 2.76642
[1mStep[0m  [80/84], [94mLoss[0m : 2.56640

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.324, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.23170
[1mStep[0m  [8/84], [94mLoss[0m : 2.38100
[1mStep[0m  [16/84], [94mLoss[0m : 2.29853
[1mStep[0m  [24/84], [94mLoss[0m : 2.67573
[1mStep[0m  [32/84], [94mLoss[0m : 2.54408
[1mStep[0m  [40/84], [94mLoss[0m : 2.45404
[1mStep[0m  [48/84], [94mLoss[0m : 2.30408
[1mStep[0m  [56/84], [94mLoss[0m : 2.64910
[1mStep[0m  [64/84], [94mLoss[0m : 2.37996
[1mStep[0m  [72/84], [94mLoss[0m : 2.66434
[1mStep[0m  [80/84], [94mLoss[0m : 2.47344

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.330, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54784
[1mStep[0m  [8/84], [94mLoss[0m : 2.34521
[1mStep[0m  [16/84], [94mLoss[0m : 2.54566
[1mStep[0m  [24/84], [94mLoss[0m : 2.71080
[1mStep[0m  [32/84], [94mLoss[0m : 2.39406
[1mStep[0m  [40/84], [94mLoss[0m : 2.67156
[1mStep[0m  [48/84], [94mLoss[0m : 2.68357
[1mStep[0m  [56/84], [94mLoss[0m : 2.08220
[1mStep[0m  [64/84], [94mLoss[0m : 2.26939
[1mStep[0m  [72/84], [94mLoss[0m : 2.57855
[1mStep[0m  [80/84], [94mLoss[0m : 2.20571

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.337, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59526
[1mStep[0m  [8/84], [94mLoss[0m : 2.50321
[1mStep[0m  [16/84], [94mLoss[0m : 2.39843
[1mStep[0m  [24/84], [94mLoss[0m : 2.24365
[1mStep[0m  [32/84], [94mLoss[0m : 2.37846
[1mStep[0m  [40/84], [94mLoss[0m : 2.57145
[1mStep[0m  [48/84], [94mLoss[0m : 2.35527
[1mStep[0m  [56/84], [94mLoss[0m : 2.56785
[1mStep[0m  [64/84], [94mLoss[0m : 2.28496
[1mStep[0m  [72/84], [94mLoss[0m : 2.48140
[1mStep[0m  [80/84], [94mLoss[0m : 2.67917

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.323, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.32237
[1mStep[0m  [8/84], [94mLoss[0m : 2.39361
[1mStep[0m  [16/84], [94mLoss[0m : 2.55953
[1mStep[0m  [24/84], [94mLoss[0m : 2.22832
[1mStep[0m  [32/84], [94mLoss[0m : 2.76395
[1mStep[0m  [40/84], [94mLoss[0m : 2.68717
[1mStep[0m  [48/84], [94mLoss[0m : 2.58866
[1mStep[0m  [56/84], [94mLoss[0m : 2.48354
[1mStep[0m  [64/84], [94mLoss[0m : 2.52620
[1mStep[0m  [72/84], [94mLoss[0m : 2.53925
[1mStep[0m  [80/84], [94mLoss[0m : 2.37303

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.329, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.77966
[1mStep[0m  [8/84], [94mLoss[0m : 2.43341
[1mStep[0m  [16/84], [94mLoss[0m : 2.31105
[1mStep[0m  [24/84], [94mLoss[0m : 2.33921
[1mStep[0m  [32/84], [94mLoss[0m : 2.55541
[1mStep[0m  [40/84], [94mLoss[0m : 2.35636
[1mStep[0m  [48/84], [94mLoss[0m : 2.59006
[1mStep[0m  [56/84], [94mLoss[0m : 2.57999
[1mStep[0m  [64/84], [94mLoss[0m : 2.65193
[1mStep[0m  [72/84], [94mLoss[0m : 2.51929
[1mStep[0m  [80/84], [94mLoss[0m : 2.54446

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.329, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44484
[1mStep[0m  [8/84], [94mLoss[0m : 2.29755
[1mStep[0m  [16/84], [94mLoss[0m : 2.47588
[1mStep[0m  [24/84], [94mLoss[0m : 2.39953
[1mStep[0m  [32/84], [94mLoss[0m : 2.38876
[1mStep[0m  [40/84], [94mLoss[0m : 2.57899
[1mStep[0m  [48/84], [94mLoss[0m : 2.38857
[1mStep[0m  [56/84], [94mLoss[0m : 2.29665
[1mStep[0m  [64/84], [94mLoss[0m : 2.16650
[1mStep[0m  [72/84], [94mLoss[0m : 2.43186
[1mStep[0m  [80/84], [94mLoss[0m : 2.62558

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.336, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.68417
[1mStep[0m  [8/84], [94mLoss[0m : 2.54882
[1mStep[0m  [16/84], [94mLoss[0m : 2.15664
[1mStep[0m  [24/84], [94mLoss[0m : 2.38459
[1mStep[0m  [32/84], [94mLoss[0m : 2.05398
[1mStep[0m  [40/84], [94mLoss[0m : 2.46220
[1mStep[0m  [48/84], [94mLoss[0m : 2.60207
[1mStep[0m  [56/84], [94mLoss[0m : 2.32650
[1mStep[0m  [64/84], [94mLoss[0m : 2.17340
[1mStep[0m  [72/84], [94mLoss[0m : 2.29649
[1mStep[0m  [80/84], [94mLoss[0m : 2.59661

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.358, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.58962
[1mStep[0m  [8/84], [94mLoss[0m : 2.38464
[1mStep[0m  [16/84], [94mLoss[0m : 2.39112
[1mStep[0m  [24/84], [94mLoss[0m : 2.37945
[1mStep[0m  [32/84], [94mLoss[0m : 2.47264
[1mStep[0m  [40/84], [94mLoss[0m : 2.28771
[1mStep[0m  [48/84], [94mLoss[0m : 2.32915
[1mStep[0m  [56/84], [94mLoss[0m : 2.50285
[1mStep[0m  [64/84], [94mLoss[0m : 2.32489
[1mStep[0m  [72/84], [94mLoss[0m : 2.44220
[1mStep[0m  [80/84], [94mLoss[0m : 2.16943

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.347, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37195
[1mStep[0m  [8/84], [94mLoss[0m : 2.16791
[1mStep[0m  [16/84], [94mLoss[0m : 2.53622
[1mStep[0m  [24/84], [94mLoss[0m : 2.32637
[1mStep[0m  [32/84], [94mLoss[0m : 2.24260
[1mStep[0m  [40/84], [94mLoss[0m : 2.69406
[1mStep[0m  [48/84], [94mLoss[0m : 2.65858
[1mStep[0m  [56/84], [94mLoss[0m : 2.39496
[1mStep[0m  [64/84], [94mLoss[0m : 2.76542
[1mStep[0m  [72/84], [94mLoss[0m : 2.49096
[1mStep[0m  [80/84], [94mLoss[0m : 2.65646

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.325, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.326
====================================

Phase 1 - Evaluation MAE:  2.326081625052861
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 2.32149
[1mStep[0m  [8/84], [94mLoss[0m : 2.31025
[1mStep[0m  [16/84], [94mLoss[0m : 2.67287
[1mStep[0m  [24/84], [94mLoss[0m : 2.52199
[1mStep[0m  [32/84], [94mLoss[0m : 2.51875
[1mStep[0m  [40/84], [94mLoss[0m : 2.56066
[1mStep[0m  [48/84], [94mLoss[0m : 2.60659
[1mStep[0m  [56/84], [94mLoss[0m : 2.28722
[1mStep[0m  [64/84], [94mLoss[0m : 2.58784
[1mStep[0m  [72/84], [94mLoss[0m : 2.67457
[1mStep[0m  [80/84], [94mLoss[0m : 2.32129

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.322, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.32803
[1mStep[0m  [8/84], [94mLoss[0m : 2.54170
[1mStep[0m  [16/84], [94mLoss[0m : 2.18326
[1mStep[0m  [24/84], [94mLoss[0m : 2.29660
[1mStep[0m  [32/84], [94mLoss[0m : 2.43010
[1mStep[0m  [40/84], [94mLoss[0m : 2.44292
[1mStep[0m  [48/84], [94mLoss[0m : 2.58360
[1mStep[0m  [56/84], [94mLoss[0m : 2.33818
[1mStep[0m  [64/84], [94mLoss[0m : 2.11117
[1mStep[0m  [72/84], [94mLoss[0m : 2.32255
[1mStep[0m  [80/84], [94mLoss[0m : 2.17905

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.354, [92mTest[0m: 2.363, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33751
[1mStep[0m  [8/84], [94mLoss[0m : 2.32148
[1mStep[0m  [16/84], [94mLoss[0m : 2.14719
[1mStep[0m  [24/84], [94mLoss[0m : 2.32867
[1mStep[0m  [32/84], [94mLoss[0m : 2.18402
[1mStep[0m  [40/84], [94mLoss[0m : 2.23438
[1mStep[0m  [48/84], [94mLoss[0m : 2.39657
[1mStep[0m  [56/84], [94mLoss[0m : 2.09273
[1mStep[0m  [64/84], [94mLoss[0m : 1.99621
[1mStep[0m  [72/84], [94mLoss[0m : 2.07024
[1mStep[0m  [80/84], [94mLoss[0m : 2.15253

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.272, [92mTest[0m: 2.321, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.10562
[1mStep[0m  [8/84], [94mLoss[0m : 2.34890
[1mStep[0m  [16/84], [94mLoss[0m : 2.07502
[1mStep[0m  [24/84], [94mLoss[0m : 1.92884
[1mStep[0m  [32/84], [94mLoss[0m : 2.12345
[1mStep[0m  [40/84], [94mLoss[0m : 2.47048
[1mStep[0m  [48/84], [94mLoss[0m : 1.91319
[1mStep[0m  [56/84], [94mLoss[0m : 2.17233
[1mStep[0m  [64/84], [94mLoss[0m : 2.04670
[1mStep[0m  [72/84], [94mLoss[0m : 2.13364
[1mStep[0m  [80/84], [94mLoss[0m : 2.14908

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.178, [92mTest[0m: 2.377, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.06930
[1mStep[0m  [8/84], [94mLoss[0m : 1.97681
[1mStep[0m  [16/84], [94mLoss[0m : 2.10612
[1mStep[0m  [24/84], [94mLoss[0m : 2.32021
[1mStep[0m  [32/84], [94mLoss[0m : 2.10937
[1mStep[0m  [40/84], [94mLoss[0m : 2.31465
[1mStep[0m  [48/84], [94mLoss[0m : 2.19216
[1mStep[0m  [56/84], [94mLoss[0m : 2.18431
[1mStep[0m  [64/84], [94mLoss[0m : 2.27381
[1mStep[0m  [72/84], [94mLoss[0m : 2.38269
[1mStep[0m  [80/84], [94mLoss[0m : 2.03128

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.139, [92mTest[0m: 2.350, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.02484
[1mStep[0m  [8/84], [94mLoss[0m : 1.80963
[1mStep[0m  [16/84], [94mLoss[0m : 1.98667
[1mStep[0m  [24/84], [94mLoss[0m : 1.77108
[1mStep[0m  [32/84], [94mLoss[0m : 1.93045
[1mStep[0m  [40/84], [94mLoss[0m : 2.10455
[1mStep[0m  [48/84], [94mLoss[0m : 2.21108
[1mStep[0m  [56/84], [94mLoss[0m : 2.10458
[1mStep[0m  [64/84], [94mLoss[0m : 2.36859
[1mStep[0m  [72/84], [94mLoss[0m : 1.92949
[1mStep[0m  [80/84], [94mLoss[0m : 2.29566

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.067, [92mTest[0m: 2.395, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.06675
[1mStep[0m  [8/84], [94mLoss[0m : 2.04570
[1mStep[0m  [16/84], [94mLoss[0m : 1.97384
[1mStep[0m  [24/84], [94mLoss[0m : 1.95733
[1mStep[0m  [32/84], [94mLoss[0m : 1.86420
[1mStep[0m  [40/84], [94mLoss[0m : 1.76759
[1mStep[0m  [48/84], [94mLoss[0m : 1.92484
[1mStep[0m  [56/84], [94mLoss[0m : 1.95954
[1mStep[0m  [64/84], [94mLoss[0m : 1.86894
[1mStep[0m  [72/84], [94mLoss[0m : 1.87439
[1mStep[0m  [80/84], [94mLoss[0m : 2.34937

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.990, [92mTest[0m: 2.443, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.79481
[1mStep[0m  [8/84], [94mLoss[0m : 2.10692
[1mStep[0m  [16/84], [94mLoss[0m : 1.81468
[1mStep[0m  [24/84], [94mLoss[0m : 1.91468
[1mStep[0m  [32/84], [94mLoss[0m : 2.01832
[1mStep[0m  [40/84], [94mLoss[0m : 1.86716
[1mStep[0m  [48/84], [94mLoss[0m : 1.99685
[1mStep[0m  [56/84], [94mLoss[0m : 1.90564
[1mStep[0m  [64/84], [94mLoss[0m : 2.03038
[1mStep[0m  [72/84], [94mLoss[0m : 1.92356
[1mStep[0m  [80/84], [94mLoss[0m : 1.78512

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.964, [92mTest[0m: 2.411, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.89818
[1mStep[0m  [8/84], [94mLoss[0m : 2.06114
[1mStep[0m  [16/84], [94mLoss[0m : 2.01582
[1mStep[0m  [24/84], [94mLoss[0m : 1.72843
[1mStep[0m  [32/84], [94mLoss[0m : 1.60210
[1mStep[0m  [40/84], [94mLoss[0m : 2.09935
[1mStep[0m  [48/84], [94mLoss[0m : 2.00478
[1mStep[0m  [56/84], [94mLoss[0m : 2.14416
[1mStep[0m  [64/84], [94mLoss[0m : 2.01797
[1mStep[0m  [72/84], [94mLoss[0m : 1.92393
[1mStep[0m  [80/84], [94mLoss[0m : 1.74146

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.907, [92mTest[0m: 2.448, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.70213
[1mStep[0m  [8/84], [94mLoss[0m : 1.70057
[1mStep[0m  [16/84], [94mLoss[0m : 1.94516
[1mStep[0m  [24/84], [94mLoss[0m : 1.62431
[1mStep[0m  [32/84], [94mLoss[0m : 1.78479
[1mStep[0m  [40/84], [94mLoss[0m : 1.73124
[1mStep[0m  [48/84], [94mLoss[0m : 1.91410
[1mStep[0m  [56/84], [94mLoss[0m : 2.11041
[1mStep[0m  [64/84], [94mLoss[0m : 1.84409
[1mStep[0m  [72/84], [94mLoss[0m : 1.77422
[1mStep[0m  [80/84], [94mLoss[0m : 1.88524

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.858, [92mTest[0m: 2.433, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.68577
[1mStep[0m  [8/84], [94mLoss[0m : 1.81576
[1mStep[0m  [16/84], [94mLoss[0m : 1.48176
[1mStep[0m  [24/84], [94mLoss[0m : 1.84490
[1mStep[0m  [32/84], [94mLoss[0m : 1.76149
[1mStep[0m  [40/84], [94mLoss[0m : 2.02089
[1mStep[0m  [48/84], [94mLoss[0m : 1.94720
[1mStep[0m  [56/84], [94mLoss[0m : 1.84116
[1mStep[0m  [64/84], [94mLoss[0m : 1.73824
[1mStep[0m  [72/84], [94mLoss[0m : 1.76906
[1mStep[0m  [80/84], [94mLoss[0m : 1.87229

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.816, [92mTest[0m: 2.458, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.67610
[1mStep[0m  [8/84], [94mLoss[0m : 1.53957
[1mStep[0m  [16/84], [94mLoss[0m : 1.74876
[1mStep[0m  [24/84], [94mLoss[0m : 1.74651
[1mStep[0m  [32/84], [94mLoss[0m : 1.77781
[1mStep[0m  [40/84], [94mLoss[0m : 1.76707
[1mStep[0m  [48/84], [94mLoss[0m : 1.74409
[1mStep[0m  [56/84], [94mLoss[0m : 1.95290
[1mStep[0m  [64/84], [94mLoss[0m : 1.77459
[1mStep[0m  [72/84], [94mLoss[0m : 1.70595
[1mStep[0m  [80/84], [94mLoss[0m : 2.21483

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.785, [92mTest[0m: 2.478, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.76179
[1mStep[0m  [8/84], [94mLoss[0m : 1.65911
[1mStep[0m  [16/84], [94mLoss[0m : 1.70867
[1mStep[0m  [24/84], [94mLoss[0m : 1.71278
[1mStep[0m  [32/84], [94mLoss[0m : 1.70092
[1mStep[0m  [40/84], [94mLoss[0m : 1.94358
[1mStep[0m  [48/84], [94mLoss[0m : 1.91105
[1mStep[0m  [56/84], [94mLoss[0m : 2.11027
[1mStep[0m  [64/84], [94mLoss[0m : 1.60363
[1mStep[0m  [72/84], [94mLoss[0m : 1.93243
[1mStep[0m  [80/84], [94mLoss[0m : 1.90976

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.734, [92mTest[0m: 2.478, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.60657
[1mStep[0m  [8/84], [94mLoss[0m : 1.64139
[1mStep[0m  [16/84], [94mLoss[0m : 1.79336
[1mStep[0m  [24/84], [94mLoss[0m : 1.57167
[1mStep[0m  [32/84], [94mLoss[0m : 1.65874
[1mStep[0m  [40/84], [94mLoss[0m : 1.83846
[1mStep[0m  [48/84], [94mLoss[0m : 1.87679
[1mStep[0m  [56/84], [94mLoss[0m : 1.52285
[1mStep[0m  [64/84], [94mLoss[0m : 1.59778
[1mStep[0m  [72/84], [94mLoss[0m : 1.78240
[1mStep[0m  [80/84], [94mLoss[0m : 1.54337

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.698, [92mTest[0m: 2.478, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.60989
[1mStep[0m  [8/84], [94mLoss[0m : 1.59997
[1mStep[0m  [16/84], [94mLoss[0m : 1.68028
[1mStep[0m  [24/84], [94mLoss[0m : 1.57812
[1mStep[0m  [32/84], [94mLoss[0m : 1.70017
[1mStep[0m  [40/84], [94mLoss[0m : 1.58069
[1mStep[0m  [48/84], [94mLoss[0m : 1.68423
[1mStep[0m  [56/84], [94mLoss[0m : 1.61558
[1mStep[0m  [64/84], [94mLoss[0m : 1.47819
[1mStep[0m  [72/84], [94mLoss[0m : 1.68432
[1mStep[0m  [80/84], [94mLoss[0m : 1.53274

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.664, [92mTest[0m: 2.516, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.58792
[1mStep[0m  [8/84], [94mLoss[0m : 1.62224
[1mStep[0m  [16/84], [94mLoss[0m : 1.63560
[1mStep[0m  [24/84], [94mLoss[0m : 1.63969
[1mStep[0m  [32/84], [94mLoss[0m : 1.56149
[1mStep[0m  [40/84], [94mLoss[0m : 1.58455
[1mStep[0m  [48/84], [94mLoss[0m : 1.54248
[1mStep[0m  [56/84], [94mLoss[0m : 1.59122
[1mStep[0m  [64/84], [94mLoss[0m : 1.56217
[1mStep[0m  [72/84], [94mLoss[0m : 1.67361
[1mStep[0m  [80/84], [94mLoss[0m : 1.65235

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.636, [92mTest[0m: 2.498, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.90520
[1mStep[0m  [8/84], [94mLoss[0m : 1.58481
[1mStep[0m  [16/84], [94mLoss[0m : 1.39849
[1mStep[0m  [24/84], [94mLoss[0m : 1.66769
[1mStep[0m  [32/84], [94mLoss[0m : 1.58496
[1mStep[0m  [40/84], [94mLoss[0m : 1.56726
[1mStep[0m  [48/84], [94mLoss[0m : 1.39091
[1mStep[0m  [56/84], [94mLoss[0m : 1.50798
[1mStep[0m  [64/84], [94mLoss[0m : 1.60904
[1mStep[0m  [72/84], [94mLoss[0m : 1.49100
[1mStep[0m  [80/84], [94mLoss[0m : 1.46800

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.601, [92mTest[0m: 2.501, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.56458
[1mStep[0m  [8/84], [94mLoss[0m : 1.64210
[1mStep[0m  [16/84], [94mLoss[0m : 1.60331
[1mStep[0m  [24/84], [94mLoss[0m : 1.60452
[1mStep[0m  [32/84], [94mLoss[0m : 1.45215
[1mStep[0m  [40/84], [94mLoss[0m : 1.87434
[1mStep[0m  [48/84], [94mLoss[0m : 1.56260
[1mStep[0m  [56/84], [94mLoss[0m : 1.75890
[1mStep[0m  [64/84], [94mLoss[0m : 1.79155
[1mStep[0m  [72/84], [94mLoss[0m : 1.83982
[1mStep[0m  [80/84], [94mLoss[0m : 1.64362

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.579, [92mTest[0m: 2.548, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.45391
[1mStep[0m  [8/84], [94mLoss[0m : 1.47133
[1mStep[0m  [16/84], [94mLoss[0m : 1.52375
[1mStep[0m  [24/84], [94mLoss[0m : 1.45791
[1mStep[0m  [32/84], [94mLoss[0m : 1.45087
[1mStep[0m  [40/84], [94mLoss[0m : 1.62019
[1mStep[0m  [48/84], [94mLoss[0m : 1.42067
[1mStep[0m  [56/84], [94mLoss[0m : 1.66962
[1mStep[0m  [64/84], [94mLoss[0m : 1.53473
[1mStep[0m  [72/84], [94mLoss[0m : 1.38590
[1mStep[0m  [80/84], [94mLoss[0m : 1.93131

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.540, [92mTest[0m: 2.505, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.46456
[1mStep[0m  [8/84], [94mLoss[0m : 1.40490
[1mStep[0m  [16/84], [94mLoss[0m : 1.45779
[1mStep[0m  [24/84], [94mLoss[0m : 1.49880
[1mStep[0m  [32/84], [94mLoss[0m : 1.52269
[1mStep[0m  [40/84], [94mLoss[0m : 1.38911
[1mStep[0m  [48/84], [94mLoss[0m : 1.58378
[1mStep[0m  [56/84], [94mLoss[0m : 1.51748
[1mStep[0m  [64/84], [94mLoss[0m : 1.65683
[1mStep[0m  [72/84], [94mLoss[0m : 1.73361
[1mStep[0m  [80/84], [94mLoss[0m : 1.38228

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.538, [92mTest[0m: 2.550, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.31384
[1mStep[0m  [8/84], [94mLoss[0m : 1.43648
[1mStep[0m  [16/84], [94mLoss[0m : 1.49967
[1mStep[0m  [24/84], [94mLoss[0m : 1.53806
[1mStep[0m  [32/84], [94mLoss[0m : 1.50451
[1mStep[0m  [40/84], [94mLoss[0m : 1.57113
[1mStep[0m  [48/84], [94mLoss[0m : 1.31739
[1mStep[0m  [56/84], [94mLoss[0m : 1.57858
[1mStep[0m  [64/84], [94mLoss[0m : 1.32410
[1mStep[0m  [72/84], [94mLoss[0m : 1.56997
[1mStep[0m  [80/84], [94mLoss[0m : 1.68623

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.463, [92mTest[0m: 2.538, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.45554
[1mStep[0m  [8/84], [94mLoss[0m : 1.33279
[1mStep[0m  [16/84], [94mLoss[0m : 1.44525
[1mStep[0m  [24/84], [94mLoss[0m : 1.29040
[1mStep[0m  [32/84], [94mLoss[0m : 1.34433
[1mStep[0m  [40/84], [94mLoss[0m : 1.63332
[1mStep[0m  [48/84], [94mLoss[0m : 1.30309
[1mStep[0m  [56/84], [94mLoss[0m : 1.37466
[1mStep[0m  [64/84], [94mLoss[0m : 1.43530
[1mStep[0m  [72/84], [94mLoss[0m : 1.54494
[1mStep[0m  [80/84], [94mLoss[0m : 1.41951

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.464, [92mTest[0m: 2.466, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.40163
[1mStep[0m  [8/84], [94mLoss[0m : 1.43657
[1mStep[0m  [16/84], [94mLoss[0m : 1.25290
[1mStep[0m  [24/84], [94mLoss[0m : 1.46688
[1mStep[0m  [32/84], [94mLoss[0m : 1.37247
[1mStep[0m  [40/84], [94mLoss[0m : 1.41185
[1mStep[0m  [48/84], [94mLoss[0m : 1.23856
[1mStep[0m  [56/84], [94mLoss[0m : 1.50986
[1mStep[0m  [64/84], [94mLoss[0m : 1.34097
[1mStep[0m  [72/84], [94mLoss[0m : 1.66176
[1mStep[0m  [80/84], [94mLoss[0m : 1.63171

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.443, [92mTest[0m: 2.507, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.35527
[1mStep[0m  [8/84], [94mLoss[0m : 1.45441
[1mStep[0m  [16/84], [94mLoss[0m : 1.53102
[1mStep[0m  [24/84], [94mLoss[0m : 1.37173
[1mStep[0m  [32/84], [94mLoss[0m : 1.19692
[1mStep[0m  [40/84], [94mLoss[0m : 1.18253
[1mStep[0m  [48/84], [94mLoss[0m : 1.33103
[1mStep[0m  [56/84], [94mLoss[0m : 1.25891
[1mStep[0m  [64/84], [94mLoss[0m : 1.20372
[1mStep[0m  [72/84], [94mLoss[0m : 1.33001
[1mStep[0m  [80/84], [94mLoss[0m : 1.40871

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.419, [92mTest[0m: 2.501, [96mlr[0m: 0.006772500000000002
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 23 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.482
====================================

Phase 2 - Evaluation MAE:  2.481600420815604
MAE score P1        2.326082
MAE score P2          2.4816
loss                 1.41876
learning_rate       0.007525
batch_size               128
hidden_sizes      [300, 100]
epochs                    30
activation           sigmoid
optimizer                sgd
early stopping          True
dropout                  0.3
momentum                 0.9
weight_decay           0.001
Name: 15, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 10.79818
[1mStep[0m  [4/42], [94mLoss[0m : 11.10543
[1mStep[0m  [8/42], [94mLoss[0m : 10.73538
[1mStep[0m  [12/42], [94mLoss[0m : 10.64987
[1mStep[0m  [16/42], [94mLoss[0m : 10.23723
[1mStep[0m  [20/42], [94mLoss[0m : 10.46426
[1mStep[0m  [24/42], [94mLoss[0m : 10.55552
[1mStep[0m  [28/42], [94mLoss[0m : 10.60699
[1mStep[0m  [32/42], [94mLoss[0m : 10.02193
[1mStep[0m  [36/42], [94mLoss[0m : 9.95619
[1mStep[0m  [40/42], [94mLoss[0m : 10.11185

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.396, [92mTest[0m: 10.904, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.45914
[1mStep[0m  [4/42], [94mLoss[0m : 9.73042
[1mStep[0m  [8/42], [94mLoss[0m : 9.27025
[1mStep[0m  [12/42], [94mLoss[0m : 9.48540
[1mStep[0m  [16/42], [94mLoss[0m : 9.01307
[1mStep[0m  [20/42], [94mLoss[0m : 9.56480
[1mStep[0m  [24/42], [94mLoss[0m : 9.17109
[1mStep[0m  [28/42], [94mLoss[0m : 9.07238
[1mStep[0m  [32/42], [94mLoss[0m : 8.54169
[1mStep[0m  [36/42], [94mLoss[0m : 8.86364
[1mStep[0m  [40/42], [94mLoss[0m : 8.35385

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.135, [92mTest[0m: 9.554, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.72703
[1mStep[0m  [4/42], [94mLoss[0m : 8.03199
[1mStep[0m  [8/42], [94mLoss[0m : 8.06192
[1mStep[0m  [12/42], [94mLoss[0m : 7.92377
[1mStep[0m  [16/42], [94mLoss[0m : 8.28566
[1mStep[0m  [20/42], [94mLoss[0m : 7.92200
[1mStep[0m  [24/42], [94mLoss[0m : 7.94286
[1mStep[0m  [28/42], [94mLoss[0m : 7.26191
[1mStep[0m  [32/42], [94mLoss[0m : 7.43405
[1mStep[0m  [36/42], [94mLoss[0m : 7.18103
[1mStep[0m  [40/42], [94mLoss[0m : 7.04330

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 7.713, [92mTest[0m: 8.014, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.17458
[1mStep[0m  [4/42], [94mLoss[0m : 7.02771
[1mStep[0m  [8/42], [94mLoss[0m : 7.12203
[1mStep[0m  [12/42], [94mLoss[0m : 6.60892
[1mStep[0m  [16/42], [94mLoss[0m : 6.72503
[1mStep[0m  [20/42], [94mLoss[0m : 6.37459
[1mStep[0m  [24/42], [94mLoss[0m : 6.46002
[1mStep[0m  [28/42], [94mLoss[0m : 6.29752
[1mStep[0m  [32/42], [94mLoss[0m : 6.16490
[1mStep[0m  [36/42], [94mLoss[0m : 5.54506
[1mStep[0m  [40/42], [94mLoss[0m : 6.13385

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 6.472, [92mTest[0m: 6.201, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.90181
[1mStep[0m  [4/42], [94mLoss[0m : 5.42429
[1mStep[0m  [8/42], [94mLoss[0m : 5.49896
[1mStep[0m  [12/42], [94mLoss[0m : 5.29025
[1mStep[0m  [16/42], [94mLoss[0m : 5.52708
[1mStep[0m  [20/42], [94mLoss[0m : 4.79107
[1mStep[0m  [24/42], [94mLoss[0m : 5.22979
[1mStep[0m  [28/42], [94mLoss[0m : 5.19140
[1mStep[0m  [32/42], [94mLoss[0m : 4.88977
[1mStep[0m  [36/42], [94mLoss[0m : 4.57056
[1mStep[0m  [40/42], [94mLoss[0m : 4.63043

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 5.183, [92mTest[0m: 4.817, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.38868
[1mStep[0m  [4/42], [94mLoss[0m : 4.37756
[1mStep[0m  [8/42], [94mLoss[0m : 3.97490
[1mStep[0m  [12/42], [94mLoss[0m : 4.11357
[1mStep[0m  [16/42], [94mLoss[0m : 3.67633
[1mStep[0m  [20/42], [94mLoss[0m : 3.78607
[1mStep[0m  [24/42], [94mLoss[0m : 3.80849
[1mStep[0m  [28/42], [94mLoss[0m : 3.43982
[1mStep[0m  [32/42], [94mLoss[0m : 3.37821
[1mStep[0m  [36/42], [94mLoss[0m : 3.25679
[1mStep[0m  [40/42], [94mLoss[0m : 3.18928

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 3.711, [92mTest[0m: 3.318, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.20664
[1mStep[0m  [4/42], [94mLoss[0m : 2.82356
[1mStep[0m  [8/42], [94mLoss[0m : 2.92404
[1mStep[0m  [12/42], [94mLoss[0m : 3.02076
[1mStep[0m  [16/42], [94mLoss[0m : 2.98894
[1mStep[0m  [20/42], [94mLoss[0m : 2.65150
[1mStep[0m  [24/42], [94mLoss[0m : 2.41345
[1mStep[0m  [28/42], [94mLoss[0m : 2.70725
[1mStep[0m  [32/42], [94mLoss[0m : 2.64446
[1mStep[0m  [36/42], [94mLoss[0m : 2.51299
[1mStep[0m  [40/42], [94mLoss[0m : 2.80663

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.771, [92mTest[0m: 2.524, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59865
[1mStep[0m  [4/42], [94mLoss[0m : 2.52573
[1mStep[0m  [8/42], [94mLoss[0m : 2.72976
[1mStep[0m  [12/42], [94mLoss[0m : 2.53780
[1mStep[0m  [16/42], [94mLoss[0m : 2.57956
[1mStep[0m  [20/42], [94mLoss[0m : 2.38345
[1mStep[0m  [24/42], [94mLoss[0m : 2.74341
[1mStep[0m  [28/42], [94mLoss[0m : 2.54352
[1mStep[0m  [32/42], [94mLoss[0m : 2.79195
[1mStep[0m  [36/42], [94mLoss[0m : 2.65987
[1mStep[0m  [40/42], [94mLoss[0m : 2.60103

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.626, [92mTest[0m: 2.440, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42007
[1mStep[0m  [4/42], [94mLoss[0m : 2.57197
[1mStep[0m  [8/42], [94mLoss[0m : 2.62075
[1mStep[0m  [12/42], [94mLoss[0m : 2.59258
[1mStep[0m  [16/42], [94mLoss[0m : 2.46484
[1mStep[0m  [20/42], [94mLoss[0m : 2.30284
[1mStep[0m  [24/42], [94mLoss[0m : 2.44018
[1mStep[0m  [28/42], [94mLoss[0m : 2.79999
[1mStep[0m  [32/42], [94mLoss[0m : 2.66980
[1mStep[0m  [36/42], [94mLoss[0m : 2.44001
[1mStep[0m  [40/42], [94mLoss[0m : 2.76602

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.571, [92mTest[0m: 2.425, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44623
[1mStep[0m  [4/42], [94mLoss[0m : 2.67591
[1mStep[0m  [8/42], [94mLoss[0m : 2.36476
[1mStep[0m  [12/42], [94mLoss[0m : 2.57559
[1mStep[0m  [16/42], [94mLoss[0m : 2.34588
[1mStep[0m  [20/42], [94mLoss[0m : 2.41935
[1mStep[0m  [24/42], [94mLoss[0m : 2.55229
[1mStep[0m  [28/42], [94mLoss[0m : 2.61914
[1mStep[0m  [32/42], [94mLoss[0m : 2.36965
[1mStep[0m  [36/42], [94mLoss[0m : 2.73855
[1mStep[0m  [40/42], [94mLoss[0m : 2.39073

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.386, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28971
[1mStep[0m  [4/42], [94mLoss[0m : 2.83687
[1mStep[0m  [8/42], [94mLoss[0m : 2.77009
[1mStep[0m  [12/42], [94mLoss[0m : 2.67525
[1mStep[0m  [16/42], [94mLoss[0m : 2.35057
[1mStep[0m  [20/42], [94mLoss[0m : 2.52782
[1mStep[0m  [24/42], [94mLoss[0m : 2.45198
[1mStep[0m  [28/42], [94mLoss[0m : 2.41620
[1mStep[0m  [32/42], [94mLoss[0m : 2.50907
[1mStep[0m  [36/42], [94mLoss[0m : 2.56093
[1mStep[0m  [40/42], [94mLoss[0m : 2.53279

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.531, [92mTest[0m: 2.456, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32868
[1mStep[0m  [4/42], [94mLoss[0m : 2.73252
[1mStep[0m  [8/42], [94mLoss[0m : 2.56838
[1mStep[0m  [12/42], [94mLoss[0m : 2.37968
[1mStep[0m  [16/42], [94mLoss[0m : 2.40422
[1mStep[0m  [20/42], [94mLoss[0m : 2.51897
[1mStep[0m  [24/42], [94mLoss[0m : 2.53577
[1mStep[0m  [28/42], [94mLoss[0m : 2.61186
[1mStep[0m  [32/42], [94mLoss[0m : 2.56788
[1mStep[0m  [36/42], [94mLoss[0m : 2.32438
[1mStep[0m  [40/42], [94mLoss[0m : 2.43884

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.353, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50577
[1mStep[0m  [4/42], [94mLoss[0m : 2.63308
[1mStep[0m  [8/42], [94mLoss[0m : 2.73885
[1mStep[0m  [12/42], [94mLoss[0m : 2.43734
[1mStep[0m  [16/42], [94mLoss[0m : 2.75826
[1mStep[0m  [20/42], [94mLoss[0m : 2.41066
[1mStep[0m  [24/42], [94mLoss[0m : 2.79737
[1mStep[0m  [28/42], [94mLoss[0m : 2.53916
[1mStep[0m  [32/42], [94mLoss[0m : 2.28596
[1mStep[0m  [36/42], [94mLoss[0m : 2.58765
[1mStep[0m  [40/42], [94mLoss[0m : 2.71693

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.360, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56003
[1mStep[0m  [4/42], [94mLoss[0m : 2.62177
[1mStep[0m  [8/42], [94mLoss[0m : 2.37215
[1mStep[0m  [12/42], [94mLoss[0m : 2.57220
[1mStep[0m  [16/42], [94mLoss[0m : 2.40807
[1mStep[0m  [20/42], [94mLoss[0m : 2.43694
[1mStep[0m  [24/42], [94mLoss[0m : 2.53834
[1mStep[0m  [28/42], [94mLoss[0m : 2.42944
[1mStep[0m  [32/42], [94mLoss[0m : 2.42975
[1mStep[0m  [36/42], [94mLoss[0m : 2.61080
[1mStep[0m  [40/42], [94mLoss[0m : 2.52011

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.361, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39523
[1mStep[0m  [4/42], [94mLoss[0m : 2.52505
[1mStep[0m  [8/42], [94mLoss[0m : 2.38814
[1mStep[0m  [12/42], [94mLoss[0m : 2.52208
[1mStep[0m  [16/42], [94mLoss[0m : 2.64413
[1mStep[0m  [20/42], [94mLoss[0m : 2.38457
[1mStep[0m  [24/42], [94mLoss[0m : 2.38367
[1mStep[0m  [28/42], [94mLoss[0m : 2.46928
[1mStep[0m  [32/42], [94mLoss[0m : 2.52870
[1mStep[0m  [36/42], [94mLoss[0m : 2.52632
[1mStep[0m  [40/42], [94mLoss[0m : 2.53377

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.354, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47331
[1mStep[0m  [4/42], [94mLoss[0m : 2.35268
[1mStep[0m  [8/42], [94mLoss[0m : 2.52327
[1mStep[0m  [12/42], [94mLoss[0m : 2.55227
[1mStep[0m  [16/42], [94mLoss[0m : 2.36110
[1mStep[0m  [20/42], [94mLoss[0m : 2.49113
[1mStep[0m  [24/42], [94mLoss[0m : 2.41866
[1mStep[0m  [28/42], [94mLoss[0m : 2.43735
[1mStep[0m  [32/42], [94mLoss[0m : 2.27747
[1mStep[0m  [36/42], [94mLoss[0m : 2.61839
[1mStep[0m  [40/42], [94mLoss[0m : 2.45186

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.442, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39103
[1mStep[0m  [4/42], [94mLoss[0m : 2.58678
[1mStep[0m  [8/42], [94mLoss[0m : 2.39105
[1mStep[0m  [12/42], [94mLoss[0m : 2.37039
[1mStep[0m  [16/42], [94mLoss[0m : 2.54310
[1mStep[0m  [20/42], [94mLoss[0m : 2.46819
[1mStep[0m  [24/42], [94mLoss[0m : 2.61434
[1mStep[0m  [28/42], [94mLoss[0m : 2.72888
[1mStep[0m  [32/42], [94mLoss[0m : 2.28270
[1mStep[0m  [36/42], [94mLoss[0m : 2.58469
[1mStep[0m  [40/42], [94mLoss[0m : 2.38944

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.375, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44157
[1mStep[0m  [4/42], [94mLoss[0m : 2.37263
[1mStep[0m  [8/42], [94mLoss[0m : 2.50882
[1mStep[0m  [12/42], [94mLoss[0m : 2.34848
[1mStep[0m  [16/42], [94mLoss[0m : 2.44265
[1mStep[0m  [20/42], [94mLoss[0m : 2.70113
[1mStep[0m  [24/42], [94mLoss[0m : 2.48255
[1mStep[0m  [28/42], [94mLoss[0m : 2.62776
[1mStep[0m  [32/42], [94mLoss[0m : 2.65017
[1mStep[0m  [36/42], [94mLoss[0m : 2.55258
[1mStep[0m  [40/42], [94mLoss[0m : 2.41686

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.365, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41706
[1mStep[0m  [4/42], [94mLoss[0m : 2.39487
[1mStep[0m  [8/42], [94mLoss[0m : 2.44258
[1mStep[0m  [12/42], [94mLoss[0m : 2.56460
[1mStep[0m  [16/42], [94mLoss[0m : 2.37426
[1mStep[0m  [20/42], [94mLoss[0m : 2.67844
[1mStep[0m  [24/42], [94mLoss[0m : 2.41869
[1mStep[0m  [28/42], [94mLoss[0m : 2.33987
[1mStep[0m  [32/42], [94mLoss[0m : 2.43633
[1mStep[0m  [36/42], [94mLoss[0m : 2.46926
[1mStep[0m  [40/42], [94mLoss[0m : 2.50154

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.364, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51700
[1mStep[0m  [4/42], [94mLoss[0m : 2.36663
[1mStep[0m  [8/42], [94mLoss[0m : 2.65371
[1mStep[0m  [12/42], [94mLoss[0m : 2.64216
[1mStep[0m  [16/42], [94mLoss[0m : 2.37637
[1mStep[0m  [20/42], [94mLoss[0m : 2.56375
[1mStep[0m  [24/42], [94mLoss[0m : 2.34742
[1mStep[0m  [28/42], [94mLoss[0m : 2.40844
[1mStep[0m  [32/42], [94mLoss[0m : 2.48277
[1mStep[0m  [36/42], [94mLoss[0m : 2.20886
[1mStep[0m  [40/42], [94mLoss[0m : 2.36806

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.342, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.65159
[1mStep[0m  [4/42], [94mLoss[0m : 2.42930
[1mStep[0m  [8/42], [94mLoss[0m : 2.12769
[1mStep[0m  [12/42], [94mLoss[0m : 2.67663
[1mStep[0m  [16/42], [94mLoss[0m : 2.51257
[1mStep[0m  [20/42], [94mLoss[0m : 2.34062
[1mStep[0m  [24/42], [94mLoss[0m : 2.36498
[1mStep[0m  [28/42], [94mLoss[0m : 2.51252
[1mStep[0m  [32/42], [94mLoss[0m : 2.25375
[1mStep[0m  [36/42], [94mLoss[0m : 2.49034
[1mStep[0m  [40/42], [94mLoss[0m : 2.36261

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.392, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48245
[1mStep[0m  [4/42], [94mLoss[0m : 2.36546
[1mStep[0m  [8/42], [94mLoss[0m : 2.52659
[1mStep[0m  [12/42], [94mLoss[0m : 2.41838
[1mStep[0m  [16/42], [94mLoss[0m : 2.50408
[1mStep[0m  [20/42], [94mLoss[0m : 2.52303
[1mStep[0m  [24/42], [94mLoss[0m : 2.39899
[1mStep[0m  [28/42], [94mLoss[0m : 2.28986
[1mStep[0m  [32/42], [94mLoss[0m : 2.34728
[1mStep[0m  [36/42], [94mLoss[0m : 2.19869
[1mStep[0m  [40/42], [94mLoss[0m : 2.36392

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.374, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34917
[1mStep[0m  [4/42], [94mLoss[0m : 2.32945
[1mStep[0m  [8/42], [94mLoss[0m : 2.44326
[1mStep[0m  [12/42], [94mLoss[0m : 2.27127
[1mStep[0m  [16/42], [94mLoss[0m : 2.43576
[1mStep[0m  [20/42], [94mLoss[0m : 2.30265
[1mStep[0m  [24/42], [94mLoss[0m : 2.36781
[1mStep[0m  [28/42], [94mLoss[0m : 2.45375
[1mStep[0m  [32/42], [94mLoss[0m : 2.47926
[1mStep[0m  [36/42], [94mLoss[0m : 2.40465
[1mStep[0m  [40/42], [94mLoss[0m : 2.23023

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.368, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46930
[1mStep[0m  [4/42], [94mLoss[0m : 2.34184
[1mStep[0m  [8/42], [94mLoss[0m : 2.59141
[1mStep[0m  [12/42], [94mLoss[0m : 2.33899
[1mStep[0m  [16/42], [94mLoss[0m : 2.37061
[1mStep[0m  [20/42], [94mLoss[0m : 2.48676
[1mStep[0m  [24/42], [94mLoss[0m : 2.39902
[1mStep[0m  [28/42], [94mLoss[0m : 2.68035
[1mStep[0m  [32/42], [94mLoss[0m : 2.34133
[1mStep[0m  [36/42], [94mLoss[0m : 2.45404
[1mStep[0m  [40/42], [94mLoss[0m : 2.58307

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.333, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51545
[1mStep[0m  [4/42], [94mLoss[0m : 2.52083
[1mStep[0m  [8/42], [94mLoss[0m : 2.66501
[1mStep[0m  [12/42], [94mLoss[0m : 2.53139
[1mStep[0m  [16/42], [94mLoss[0m : 2.48649
[1mStep[0m  [20/42], [94mLoss[0m : 2.58406
[1mStep[0m  [24/42], [94mLoss[0m : 2.32968
[1mStep[0m  [28/42], [94mLoss[0m : 2.58612
[1mStep[0m  [32/42], [94mLoss[0m : 2.33794
[1mStep[0m  [36/42], [94mLoss[0m : 2.54786
[1mStep[0m  [40/42], [94mLoss[0m : 2.28406

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.360, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50412
[1mStep[0m  [4/42], [94mLoss[0m : 2.53640
[1mStep[0m  [8/42], [94mLoss[0m : 2.52571
[1mStep[0m  [12/42], [94mLoss[0m : 2.31843
[1mStep[0m  [16/42], [94mLoss[0m : 2.37907
[1mStep[0m  [20/42], [94mLoss[0m : 2.52658
[1mStep[0m  [24/42], [94mLoss[0m : 2.40670
[1mStep[0m  [28/42], [94mLoss[0m : 2.48380
[1mStep[0m  [32/42], [94mLoss[0m : 2.24992
[1mStep[0m  [36/42], [94mLoss[0m : 2.28119
[1mStep[0m  [40/42], [94mLoss[0m : 2.54361

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.349, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41478
[1mStep[0m  [4/42], [94mLoss[0m : 2.32992
[1mStep[0m  [8/42], [94mLoss[0m : 2.53562
[1mStep[0m  [12/42], [94mLoss[0m : 2.30127
[1mStep[0m  [16/42], [94mLoss[0m : 2.45139
[1mStep[0m  [20/42], [94mLoss[0m : 2.24909
[1mStep[0m  [24/42], [94mLoss[0m : 2.42459
[1mStep[0m  [28/42], [94mLoss[0m : 2.36565
[1mStep[0m  [32/42], [94mLoss[0m : 2.49265
[1mStep[0m  [36/42], [94mLoss[0m : 2.38596
[1mStep[0m  [40/42], [94mLoss[0m : 2.35504

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.336, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.25663
[1mStep[0m  [4/42], [94mLoss[0m : 2.36819
[1mStep[0m  [8/42], [94mLoss[0m : 2.37066
[1mStep[0m  [12/42], [94mLoss[0m : 2.40186
[1mStep[0m  [16/42], [94mLoss[0m : 2.44903
[1mStep[0m  [20/42], [94mLoss[0m : 2.36963
[1mStep[0m  [24/42], [94mLoss[0m : 2.47921
[1mStep[0m  [28/42], [94mLoss[0m : 2.32008
[1mStep[0m  [32/42], [94mLoss[0m : 2.54806
[1mStep[0m  [36/42], [94mLoss[0m : 2.42613
[1mStep[0m  [40/42], [94mLoss[0m : 2.47046

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.327, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33654
[1mStep[0m  [4/42], [94mLoss[0m : 2.35734
[1mStep[0m  [8/42], [94mLoss[0m : 2.39144
[1mStep[0m  [12/42], [94mLoss[0m : 2.32753
[1mStep[0m  [16/42], [94mLoss[0m : 2.45844
[1mStep[0m  [20/42], [94mLoss[0m : 2.51558
[1mStep[0m  [24/42], [94mLoss[0m : 2.41503
[1mStep[0m  [28/42], [94mLoss[0m : 2.52857
[1mStep[0m  [32/42], [94mLoss[0m : 2.26829
[1mStep[0m  [36/42], [94mLoss[0m : 2.42437
[1mStep[0m  [40/42], [94mLoss[0m : 2.43385

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.326, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53680
[1mStep[0m  [4/42], [94mLoss[0m : 2.40253
[1mStep[0m  [8/42], [94mLoss[0m : 2.25695
[1mStep[0m  [12/42], [94mLoss[0m : 2.28168
[1mStep[0m  [16/42], [94mLoss[0m : 2.62903
[1mStep[0m  [20/42], [94mLoss[0m : 2.48338
[1mStep[0m  [24/42], [94mLoss[0m : 2.42683
[1mStep[0m  [28/42], [94mLoss[0m : 2.56617
[1mStep[0m  [32/42], [94mLoss[0m : 2.46783
[1mStep[0m  [36/42], [94mLoss[0m : 2.43389
[1mStep[0m  [40/42], [94mLoss[0m : 2.28330

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.315, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.361
====================================

Phase 1 - Evaluation MAE:  2.361106038093567
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 2.48496
[1mStep[0m  [4/42], [94mLoss[0m : 2.64439
[1mStep[0m  [8/42], [94mLoss[0m : 2.57835
[1mStep[0m  [12/42], [94mLoss[0m : 2.46759
[1mStep[0m  [16/42], [94mLoss[0m : 2.53709
[1mStep[0m  [20/42], [94mLoss[0m : 2.36377
[1mStep[0m  [24/42], [94mLoss[0m : 2.41342
[1mStep[0m  [28/42], [94mLoss[0m : 2.26790
[1mStep[0m  [32/42], [94mLoss[0m : 2.53822
[1mStep[0m  [36/42], [94mLoss[0m : 2.22464
[1mStep[0m  [40/42], [94mLoss[0m : 2.48072

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.363, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50812
[1mStep[0m  [4/42], [94mLoss[0m : 2.35541
[1mStep[0m  [8/42], [94mLoss[0m : 2.27415
[1mStep[0m  [12/42], [94mLoss[0m : 2.23081
[1mStep[0m  [16/42], [94mLoss[0m : 2.51179
[1mStep[0m  [20/42], [94mLoss[0m : 2.26570
[1mStep[0m  [24/42], [94mLoss[0m : 2.37446
[1mStep[0m  [28/42], [94mLoss[0m : 2.44815
[1mStep[0m  [32/42], [94mLoss[0m : 2.32610
[1mStep[0m  [36/42], [94mLoss[0m : 2.39511
[1mStep[0m  [40/42], [94mLoss[0m : 2.46922

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.324, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27961
[1mStep[0m  [4/42], [94mLoss[0m : 2.32562
[1mStep[0m  [8/42], [94mLoss[0m : 2.53020
[1mStep[0m  [12/42], [94mLoss[0m : 2.20735
[1mStep[0m  [16/42], [94mLoss[0m : 2.29468
[1mStep[0m  [20/42], [94mLoss[0m : 2.47038
[1mStep[0m  [24/42], [94mLoss[0m : 2.38618
[1mStep[0m  [28/42], [94mLoss[0m : 2.37625
[1mStep[0m  [32/42], [94mLoss[0m : 2.22677
[1mStep[0m  [36/42], [94mLoss[0m : 2.54494
[1mStep[0m  [40/42], [94mLoss[0m : 2.51244

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.378, [92mTest[0m: 2.421, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26847
[1mStep[0m  [4/42], [94mLoss[0m : 2.11319
[1mStep[0m  [8/42], [94mLoss[0m : 2.47671
[1mStep[0m  [12/42], [94mLoss[0m : 2.34468
[1mStep[0m  [16/42], [94mLoss[0m : 2.45623
[1mStep[0m  [20/42], [94mLoss[0m : 2.47370
[1mStep[0m  [24/42], [94mLoss[0m : 2.30299
[1mStep[0m  [28/42], [94mLoss[0m : 2.40931
[1mStep[0m  [32/42], [94mLoss[0m : 2.24238
[1mStep[0m  [36/42], [94mLoss[0m : 2.19369
[1mStep[0m  [40/42], [94mLoss[0m : 2.22726

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.327, [92mTest[0m: 2.340, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37108
[1mStep[0m  [4/42], [94mLoss[0m : 2.51269
[1mStep[0m  [8/42], [94mLoss[0m : 2.30945
[1mStep[0m  [12/42], [94mLoss[0m : 2.01702
[1mStep[0m  [16/42], [94mLoss[0m : 2.23163
[1mStep[0m  [20/42], [94mLoss[0m : 2.54271
[1mStep[0m  [24/42], [94mLoss[0m : 2.03385
[1mStep[0m  [28/42], [94mLoss[0m : 2.27850
[1mStep[0m  [32/42], [94mLoss[0m : 2.29408
[1mStep[0m  [36/42], [94mLoss[0m : 2.30190
[1mStep[0m  [40/42], [94mLoss[0m : 2.15295

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.268, [92mTest[0m: 2.350, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.10448
[1mStep[0m  [4/42], [94mLoss[0m : 2.17704
[1mStep[0m  [8/42], [94mLoss[0m : 2.16711
[1mStep[0m  [12/42], [94mLoss[0m : 2.28687
[1mStep[0m  [16/42], [94mLoss[0m : 2.22373
[1mStep[0m  [20/42], [94mLoss[0m : 2.21406
[1mStep[0m  [24/42], [94mLoss[0m : 1.88648
[1mStep[0m  [28/42], [94mLoss[0m : 2.30230
[1mStep[0m  [32/42], [94mLoss[0m : 2.19890
[1mStep[0m  [36/42], [94mLoss[0m : 2.15879
[1mStep[0m  [40/42], [94mLoss[0m : 2.33336

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.203, [92mTest[0m: 2.410, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.13867
[1mStep[0m  [4/42], [94mLoss[0m : 2.13348
[1mStep[0m  [8/42], [94mLoss[0m : 2.16547
[1mStep[0m  [12/42], [94mLoss[0m : 2.13747
[1mStep[0m  [16/42], [94mLoss[0m : 2.10210
[1mStep[0m  [20/42], [94mLoss[0m : 2.04200
[1mStep[0m  [24/42], [94mLoss[0m : 2.11302
[1mStep[0m  [28/42], [94mLoss[0m : 2.10300
[1mStep[0m  [32/42], [94mLoss[0m : 2.07136
[1mStep[0m  [36/42], [94mLoss[0m : 2.06546
[1mStep[0m  [40/42], [94mLoss[0m : 2.26221

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.170, [92mTest[0m: 2.465, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.05953
[1mStep[0m  [4/42], [94mLoss[0m : 2.14426
[1mStep[0m  [8/42], [94mLoss[0m : 2.04171
[1mStep[0m  [12/42], [94mLoss[0m : 2.17218
[1mStep[0m  [16/42], [94mLoss[0m : 2.04266
[1mStep[0m  [20/42], [94mLoss[0m : 1.85470
[1mStep[0m  [24/42], [94mLoss[0m : 2.17061
[1mStep[0m  [28/42], [94mLoss[0m : 2.17499
[1mStep[0m  [32/42], [94mLoss[0m : 2.15882
[1mStep[0m  [36/42], [94mLoss[0m : 2.06582
[1mStep[0m  [40/42], [94mLoss[0m : 2.14103

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.105, [92mTest[0m: 2.413, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.84589
[1mStep[0m  [4/42], [94mLoss[0m : 1.95111
[1mStep[0m  [8/42], [94mLoss[0m : 2.03402
[1mStep[0m  [12/42], [94mLoss[0m : 2.03276
[1mStep[0m  [16/42], [94mLoss[0m : 2.11909
[1mStep[0m  [20/42], [94mLoss[0m : 2.15508
[1mStep[0m  [24/42], [94mLoss[0m : 1.96465
[1mStep[0m  [28/42], [94mLoss[0m : 2.11487
[1mStep[0m  [32/42], [94mLoss[0m : 1.94842
[1mStep[0m  [36/42], [94mLoss[0m : 2.20957
[1mStep[0m  [40/42], [94mLoss[0m : 2.00661

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.054, [92mTest[0m: 2.419, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.06279
[1mStep[0m  [4/42], [94mLoss[0m : 2.02176
[1mStep[0m  [8/42], [94mLoss[0m : 2.14817
[1mStep[0m  [12/42], [94mLoss[0m : 1.92185
[1mStep[0m  [16/42], [94mLoss[0m : 2.10452
[1mStep[0m  [20/42], [94mLoss[0m : 2.04249
[1mStep[0m  [24/42], [94mLoss[0m : 1.80634
[1mStep[0m  [28/42], [94mLoss[0m : 2.26891
[1mStep[0m  [32/42], [94mLoss[0m : 1.91453
[1mStep[0m  [36/42], [94mLoss[0m : 2.21664
[1mStep[0m  [40/42], [94mLoss[0m : 2.12891

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.009, [92mTest[0m: 2.463, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.97144
[1mStep[0m  [4/42], [94mLoss[0m : 1.98788
[1mStep[0m  [8/42], [94mLoss[0m : 2.27286
[1mStep[0m  [12/42], [94mLoss[0m : 2.02692
[1mStep[0m  [16/42], [94mLoss[0m : 1.90681
[1mStep[0m  [20/42], [94mLoss[0m : 2.01417
[1mStep[0m  [24/42], [94mLoss[0m : 2.00860
[1mStep[0m  [28/42], [94mLoss[0m : 1.83263
[1mStep[0m  [32/42], [94mLoss[0m : 1.99159
[1mStep[0m  [36/42], [94mLoss[0m : 2.07338
[1mStep[0m  [40/42], [94mLoss[0m : 2.00358

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.969, [92mTest[0m: 2.557, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.92534
[1mStep[0m  [4/42], [94mLoss[0m : 1.96820
[1mStep[0m  [8/42], [94mLoss[0m : 1.87605
[1mStep[0m  [12/42], [94mLoss[0m : 1.92056
[1mStep[0m  [16/42], [94mLoss[0m : 1.95061
[1mStep[0m  [20/42], [94mLoss[0m : 1.88447
[1mStep[0m  [24/42], [94mLoss[0m : 1.82099
[1mStep[0m  [28/42], [94mLoss[0m : 1.98768
[1mStep[0m  [32/42], [94mLoss[0m : 1.66844
[1mStep[0m  [36/42], [94mLoss[0m : 1.86228
[1mStep[0m  [40/42], [94mLoss[0m : 2.02235

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.905, [92mTest[0m: 2.480, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.90139
[1mStep[0m  [4/42], [94mLoss[0m : 1.76749
[1mStep[0m  [8/42], [94mLoss[0m : 1.61174
[1mStep[0m  [12/42], [94mLoss[0m : 1.78274
[1mStep[0m  [16/42], [94mLoss[0m : 1.69048
[1mStep[0m  [20/42], [94mLoss[0m : 1.92148
[1mStep[0m  [24/42], [94mLoss[0m : 1.70193
[1mStep[0m  [28/42], [94mLoss[0m : 1.90687
[1mStep[0m  [32/42], [94mLoss[0m : 1.82792
[1mStep[0m  [36/42], [94mLoss[0m : 1.96554
[1mStep[0m  [40/42], [94mLoss[0m : 1.90933

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.840, [92mTest[0m: 2.461, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.66253
[1mStep[0m  [4/42], [94mLoss[0m : 1.79623
[1mStep[0m  [8/42], [94mLoss[0m : 1.70540
[1mStep[0m  [12/42], [94mLoss[0m : 1.85302
[1mStep[0m  [16/42], [94mLoss[0m : 2.10110
[1mStep[0m  [20/42], [94mLoss[0m : 1.87355
[1mStep[0m  [24/42], [94mLoss[0m : 1.72540
[1mStep[0m  [28/42], [94mLoss[0m : 1.94877
[1mStep[0m  [32/42], [94mLoss[0m : 1.78235
[1mStep[0m  [36/42], [94mLoss[0m : 1.80557
[1mStep[0m  [40/42], [94mLoss[0m : 1.83002

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.837, [92mTest[0m: 2.427, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.70851
[1mStep[0m  [4/42], [94mLoss[0m : 1.75922
[1mStep[0m  [8/42], [94mLoss[0m : 1.74423
[1mStep[0m  [12/42], [94mLoss[0m : 1.64338
[1mStep[0m  [16/42], [94mLoss[0m : 1.79982
[1mStep[0m  [20/42], [94mLoss[0m : 1.80556
[1mStep[0m  [24/42], [94mLoss[0m : 1.78583
[1mStep[0m  [28/42], [94mLoss[0m : 1.70319
[1mStep[0m  [32/42], [94mLoss[0m : 1.69455
[1mStep[0m  [36/42], [94mLoss[0m : 1.67125
[1mStep[0m  [40/42], [94mLoss[0m : 1.79960

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.785, [92mTest[0m: 2.491, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.72332
[1mStep[0m  [4/42], [94mLoss[0m : 1.55710
[1mStep[0m  [8/42], [94mLoss[0m : 1.69251
[1mStep[0m  [12/42], [94mLoss[0m : 1.87545
[1mStep[0m  [16/42], [94mLoss[0m : 1.86277
[1mStep[0m  [20/42], [94mLoss[0m : 1.81055
[1mStep[0m  [24/42], [94mLoss[0m : 1.76725
[1mStep[0m  [28/42], [94mLoss[0m : 1.85317
[1mStep[0m  [32/42], [94mLoss[0m : 1.86233
[1mStep[0m  [36/42], [94mLoss[0m : 1.64736
[1mStep[0m  [40/42], [94mLoss[0m : 1.84960

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.742, [92mTest[0m: 2.473, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.67967
[1mStep[0m  [4/42], [94mLoss[0m : 1.68298
[1mStep[0m  [8/42], [94mLoss[0m : 1.77367
[1mStep[0m  [12/42], [94mLoss[0m : 1.61817
[1mStep[0m  [16/42], [94mLoss[0m : 1.81136
[1mStep[0m  [20/42], [94mLoss[0m : 1.84308
[1mStep[0m  [24/42], [94mLoss[0m : 1.71845
[1mStep[0m  [28/42], [94mLoss[0m : 1.79098
[1mStep[0m  [32/42], [94mLoss[0m : 1.69108
[1mStep[0m  [36/42], [94mLoss[0m : 1.77109
[1mStep[0m  [40/42], [94mLoss[0m : 1.63030

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.711, [92mTest[0m: 2.463, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.72743
[1mStep[0m  [4/42], [94mLoss[0m : 1.73875
[1mStep[0m  [8/42], [94mLoss[0m : 1.62606
[1mStep[0m  [12/42], [94mLoss[0m : 1.69024
[1mStep[0m  [16/42], [94mLoss[0m : 1.69476
[1mStep[0m  [20/42], [94mLoss[0m : 1.60833
[1mStep[0m  [24/42], [94mLoss[0m : 1.73599
[1mStep[0m  [28/42], [94mLoss[0m : 1.77009
[1mStep[0m  [32/42], [94mLoss[0m : 1.81213
[1mStep[0m  [36/42], [94mLoss[0m : 1.71401
[1mStep[0m  [40/42], [94mLoss[0m : 1.71046

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.682, [92mTest[0m: 2.475, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.79205
[1mStep[0m  [4/42], [94mLoss[0m : 1.58167
[1mStep[0m  [8/42], [94mLoss[0m : 1.76138
[1mStep[0m  [12/42], [94mLoss[0m : 1.70175
[1mStep[0m  [16/42], [94mLoss[0m : 1.47053
[1mStep[0m  [20/42], [94mLoss[0m : 1.66582
[1mStep[0m  [24/42], [94mLoss[0m : 1.52920
[1mStep[0m  [28/42], [94mLoss[0m : 1.48983
[1mStep[0m  [32/42], [94mLoss[0m : 1.82861
[1mStep[0m  [36/42], [94mLoss[0m : 1.54882
[1mStep[0m  [40/42], [94mLoss[0m : 1.77183

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.649, [92mTest[0m: 2.517, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.57615
[1mStep[0m  [4/42], [94mLoss[0m : 1.69114
[1mStep[0m  [8/42], [94mLoss[0m : 1.67739
[1mStep[0m  [12/42], [94mLoss[0m : 1.63670
[1mStep[0m  [16/42], [94mLoss[0m : 1.66018
[1mStep[0m  [20/42], [94mLoss[0m : 1.44351
[1mStep[0m  [24/42], [94mLoss[0m : 1.65707
[1mStep[0m  [28/42], [94mLoss[0m : 1.67114
[1mStep[0m  [32/42], [94mLoss[0m : 1.66274
[1mStep[0m  [36/42], [94mLoss[0m : 1.59631
[1mStep[0m  [40/42], [94mLoss[0m : 1.87280

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.602, [92mTest[0m: 2.435, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.71256
[1mStep[0m  [4/42], [94mLoss[0m : 1.45520
[1mStep[0m  [8/42], [94mLoss[0m : 1.64427
[1mStep[0m  [12/42], [94mLoss[0m : 1.70339
[1mStep[0m  [16/42], [94mLoss[0m : 1.50180
[1mStep[0m  [20/42], [94mLoss[0m : 1.55847
[1mStep[0m  [24/42], [94mLoss[0m : 1.48518
[1mStep[0m  [28/42], [94mLoss[0m : 1.55306
[1mStep[0m  [32/42], [94mLoss[0m : 1.60638
[1mStep[0m  [36/42], [94mLoss[0m : 1.49691
[1mStep[0m  [40/42], [94mLoss[0m : 1.47083

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.592, [92mTest[0m: 2.574, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.56108
[1mStep[0m  [4/42], [94mLoss[0m : 1.44405
[1mStep[0m  [8/42], [94mLoss[0m : 1.59257
[1mStep[0m  [12/42], [94mLoss[0m : 1.50194
[1mStep[0m  [16/42], [94mLoss[0m : 1.58645
[1mStep[0m  [20/42], [94mLoss[0m : 1.44782
[1mStep[0m  [24/42], [94mLoss[0m : 1.43411
[1mStep[0m  [28/42], [94mLoss[0m : 1.58308
[1mStep[0m  [32/42], [94mLoss[0m : 1.49611
[1mStep[0m  [36/42], [94mLoss[0m : 1.61730
[1mStep[0m  [40/42], [94mLoss[0m : 1.61043

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.541, [92mTest[0m: 2.545, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.48776
[1mStep[0m  [4/42], [94mLoss[0m : 1.39831
[1mStep[0m  [8/42], [94mLoss[0m : 1.41655
[1mStep[0m  [12/42], [94mLoss[0m : 1.48113
[1mStep[0m  [16/42], [94mLoss[0m : 1.50075
[1mStep[0m  [20/42], [94mLoss[0m : 1.53502
[1mStep[0m  [24/42], [94mLoss[0m : 1.47011
[1mStep[0m  [28/42], [94mLoss[0m : 1.56371
[1mStep[0m  [32/42], [94mLoss[0m : 1.51347
[1mStep[0m  [36/42], [94mLoss[0m : 1.46707
[1mStep[0m  [40/42], [94mLoss[0m : 1.63989

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.517, [92mTest[0m: 2.442, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.35477
[1mStep[0m  [4/42], [94mLoss[0m : 1.36189
[1mStep[0m  [8/42], [94mLoss[0m : 1.41857
[1mStep[0m  [12/42], [94mLoss[0m : 1.51892
[1mStep[0m  [16/42], [94mLoss[0m : 1.49052
[1mStep[0m  [20/42], [94mLoss[0m : 1.39141
[1mStep[0m  [24/42], [94mLoss[0m : 1.50858
[1mStep[0m  [28/42], [94mLoss[0m : 1.45385
[1mStep[0m  [32/42], [94mLoss[0m : 1.51869
[1mStep[0m  [36/42], [94mLoss[0m : 1.52230
[1mStep[0m  [40/42], [94mLoss[0m : 1.46201

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.478, [92mTest[0m: 2.435, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.36921
[1mStep[0m  [4/42], [94mLoss[0m : 1.59713
[1mStep[0m  [8/42], [94mLoss[0m : 1.48925
[1mStep[0m  [12/42], [94mLoss[0m : 1.44538
[1mStep[0m  [16/42], [94mLoss[0m : 1.61606
[1mStep[0m  [20/42], [94mLoss[0m : 1.53076
[1mStep[0m  [24/42], [94mLoss[0m : 1.59563
[1mStep[0m  [28/42], [94mLoss[0m : 1.41413
[1mStep[0m  [32/42], [94mLoss[0m : 1.47477
[1mStep[0m  [36/42], [94mLoss[0m : 1.56859
[1mStep[0m  [40/42], [94mLoss[0m : 1.37724

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.491, [92mTest[0m: 2.471, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.42011
[1mStep[0m  [4/42], [94mLoss[0m : 1.32188
[1mStep[0m  [8/42], [94mLoss[0m : 1.47659
[1mStep[0m  [12/42], [94mLoss[0m : 1.34147
[1mStep[0m  [16/42], [94mLoss[0m : 1.45968
[1mStep[0m  [20/42], [94mLoss[0m : 1.51937
[1mStep[0m  [24/42], [94mLoss[0m : 1.50269
[1mStep[0m  [28/42], [94mLoss[0m : 1.49738
[1mStep[0m  [32/42], [94mLoss[0m : 1.41664
[1mStep[0m  [36/42], [94mLoss[0m : 1.50494
[1mStep[0m  [40/42], [94mLoss[0m : 1.37602

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.437, [92mTest[0m: 2.469, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.45945
[1mStep[0m  [4/42], [94mLoss[0m : 1.45869
[1mStep[0m  [8/42], [94mLoss[0m : 1.41715
[1mStep[0m  [12/42], [94mLoss[0m : 1.38921
[1mStep[0m  [16/42], [94mLoss[0m : 1.56483
[1mStep[0m  [20/42], [94mLoss[0m : 1.40284
[1mStep[0m  [24/42], [94mLoss[0m : 1.44399
[1mStep[0m  [28/42], [94mLoss[0m : 1.55462
[1mStep[0m  [32/42], [94mLoss[0m : 1.47328
[1mStep[0m  [36/42], [94mLoss[0m : 1.44432
[1mStep[0m  [40/42], [94mLoss[0m : 1.29845

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.448, [92mTest[0m: 2.476, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.41680
[1mStep[0m  [4/42], [94mLoss[0m : 1.36226
[1mStep[0m  [8/42], [94mLoss[0m : 1.33152
[1mStep[0m  [12/42], [94mLoss[0m : 1.38074
[1mStep[0m  [16/42], [94mLoss[0m : 1.41168
[1mStep[0m  [20/42], [94mLoss[0m : 1.53526
[1mStep[0m  [24/42], [94mLoss[0m : 1.31505
[1mStep[0m  [28/42], [94mLoss[0m : 1.44108
[1mStep[0m  [32/42], [94mLoss[0m : 1.30708
[1mStep[0m  [36/42], [94mLoss[0m : 1.48599
[1mStep[0m  [40/42], [94mLoss[0m : 1.61562

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.418, [92mTest[0m: 2.545, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.42419
[1mStep[0m  [4/42], [94mLoss[0m : 1.30835
[1mStep[0m  [8/42], [94mLoss[0m : 1.37519
[1mStep[0m  [12/42], [94mLoss[0m : 1.36497
[1mStep[0m  [16/42], [94mLoss[0m : 1.46816
[1mStep[0m  [20/42], [94mLoss[0m : 1.43011
[1mStep[0m  [24/42], [94mLoss[0m : 1.38382
[1mStep[0m  [28/42], [94mLoss[0m : 1.35612
[1mStep[0m  [32/42], [94mLoss[0m : 1.41798
[1mStep[0m  [36/42], [94mLoss[0m : 1.56485
[1mStep[0m  [40/42], [94mLoss[0m : 1.49616

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.386, [92mTest[0m: 2.455, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.34503
[1mStep[0m  [4/42], [94mLoss[0m : 1.35163
[1mStep[0m  [8/42], [94mLoss[0m : 1.28083
[1mStep[0m  [12/42], [94mLoss[0m : 1.48287
[1mStep[0m  [16/42], [94mLoss[0m : 1.30557
[1mStep[0m  [20/42], [94mLoss[0m : 1.38135
[1mStep[0m  [24/42], [94mLoss[0m : 1.38118
[1mStep[0m  [28/42], [94mLoss[0m : 1.35847
[1mStep[0m  [32/42], [94mLoss[0m : 1.36737
[1mStep[0m  [36/42], [94mLoss[0m : 1.26638
[1mStep[0m  [40/42], [94mLoss[0m : 1.23563

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.370, [92mTest[0m: 2.472, [96mlr[0m: 0.006772500000000002
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 29 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.457
====================================

Phase 2 - Evaluation MAE:  2.4567937340055193
MAE score P1      2.361106
MAE score P2      2.456794
loss              1.370247
learning_rate     0.007525
batch_size             256
hidden_sizes         [300]
epochs                  30
activation            tanh
optimizer              sgd
early stopping        True
dropout                0.2
momentum               0.5
weight_decay          0.01
Name: 16, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 10.80725
[1mStep[0m  [4/42], [94mLoss[0m : 10.95145
[1mStep[0m  [8/42], [94mLoss[0m : 10.77167
[1mStep[0m  [12/42], [94mLoss[0m : 11.11743
[1mStep[0m  [16/42], [94mLoss[0m : 10.47348
[1mStep[0m  [20/42], [94mLoss[0m : 10.38855
[1mStep[0m  [24/42], [94mLoss[0m : 10.99434
[1mStep[0m  [28/42], [94mLoss[0m : 10.21306
[1mStep[0m  [32/42], [94mLoss[0m : 10.40407
[1mStep[0m  [36/42], [94mLoss[0m : 10.41483
[1mStep[0m  [40/42], [94mLoss[0m : 10.30159

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.616, [92mTest[0m: 11.085, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.80335
[1mStep[0m  [4/42], [94mLoss[0m : 10.27410
[1mStep[0m  [8/42], [94mLoss[0m : 10.28238
[1mStep[0m  [12/42], [94mLoss[0m : 10.52465
[1mStep[0m  [16/42], [94mLoss[0m : 9.98683
[1mStep[0m  [20/42], [94mLoss[0m : 10.28782
[1mStep[0m  [24/42], [94mLoss[0m : 9.75836
[1mStep[0m  [28/42], [94mLoss[0m : 9.61589
[1mStep[0m  [32/42], [94mLoss[0m : 9.95920
[1mStep[0m  [36/42], [94mLoss[0m : 9.60935
[1mStep[0m  [40/42], [94mLoss[0m : 9.76454

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.967, [92mTest[0m: 10.147, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.36679
[1mStep[0m  [4/42], [94mLoss[0m : 9.59720
[1mStep[0m  [8/42], [94mLoss[0m : 9.54184
[1mStep[0m  [12/42], [94mLoss[0m : 9.30192
[1mStep[0m  [16/42], [94mLoss[0m : 9.21402
[1mStep[0m  [20/42], [94mLoss[0m : 9.36684
[1mStep[0m  [24/42], [94mLoss[0m : 9.03492
[1mStep[0m  [28/42], [94mLoss[0m : 9.12034
[1mStep[0m  [32/42], [94mLoss[0m : 8.85563
[1mStep[0m  [36/42], [94mLoss[0m : 8.80215
[1mStep[0m  [40/42], [94mLoss[0m : 8.98287

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.177, [92mTest[0m: 9.089, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.42028
[1mStep[0m  [4/42], [94mLoss[0m : 8.53461
[1mStep[0m  [8/42], [94mLoss[0m : 8.67676
[1mStep[0m  [12/42], [94mLoss[0m : 8.45580
[1mStep[0m  [16/42], [94mLoss[0m : 8.37169
[1mStep[0m  [20/42], [94mLoss[0m : 8.27993
[1mStep[0m  [24/42], [94mLoss[0m : 8.06730
[1mStep[0m  [28/42], [94mLoss[0m : 8.31824
[1mStep[0m  [32/42], [94mLoss[0m : 8.41165
[1mStep[0m  [36/42], [94mLoss[0m : 8.08379
[1mStep[0m  [40/42], [94mLoss[0m : 7.96373

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 8.341, [92mTest[0m: 8.163, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.27160
[1mStep[0m  [4/42], [94mLoss[0m : 7.68839
[1mStep[0m  [8/42], [94mLoss[0m : 7.40063
[1mStep[0m  [12/42], [94mLoss[0m : 7.48762
[1mStep[0m  [16/42], [94mLoss[0m : 7.50051
[1mStep[0m  [20/42], [94mLoss[0m : 7.30130
[1mStep[0m  [24/42], [94mLoss[0m : 7.31306
[1mStep[0m  [28/42], [94mLoss[0m : 7.46243
[1mStep[0m  [32/42], [94mLoss[0m : 7.31573
[1mStep[0m  [36/42], [94mLoss[0m : 7.38214
[1mStep[0m  [40/42], [94mLoss[0m : 7.61335

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 7.588, [92mTest[0m: 7.236, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.33889
[1mStep[0m  [4/42], [94mLoss[0m : 7.06392
[1mStep[0m  [8/42], [94mLoss[0m : 6.94517
[1mStep[0m  [12/42], [94mLoss[0m : 7.27949
[1mStep[0m  [16/42], [94mLoss[0m : 7.24275
[1mStep[0m  [20/42], [94mLoss[0m : 6.98079
[1mStep[0m  [24/42], [94mLoss[0m : 6.91751
[1mStep[0m  [28/42], [94mLoss[0m : 6.92583
[1mStep[0m  [32/42], [94mLoss[0m : 6.62126
[1mStep[0m  [36/42], [94mLoss[0m : 6.87194
[1mStep[0m  [40/42], [94mLoss[0m : 6.66599

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 6.912, [92mTest[0m: 6.344, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.55961
[1mStep[0m  [4/42], [94mLoss[0m : 6.44011
[1mStep[0m  [8/42], [94mLoss[0m : 6.55376
[1mStep[0m  [12/42], [94mLoss[0m : 6.41632
[1mStep[0m  [16/42], [94mLoss[0m : 6.67959
[1mStep[0m  [20/42], [94mLoss[0m : 6.33132
[1mStep[0m  [24/42], [94mLoss[0m : 6.68526
[1mStep[0m  [28/42], [94mLoss[0m : 6.30770
[1mStep[0m  [32/42], [94mLoss[0m : 6.10806
[1mStep[0m  [36/42], [94mLoss[0m : 5.90826
[1mStep[0m  [40/42], [94mLoss[0m : 5.94063

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 6.283, [92mTest[0m: 5.625, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.78910
[1mStep[0m  [4/42], [94mLoss[0m : 6.00310
[1mStep[0m  [8/42], [94mLoss[0m : 5.64678
[1mStep[0m  [12/42], [94mLoss[0m : 6.04487
[1mStep[0m  [16/42], [94mLoss[0m : 5.86631
[1mStep[0m  [20/42], [94mLoss[0m : 5.33309
[1mStep[0m  [24/42], [94mLoss[0m : 5.10157
[1mStep[0m  [28/42], [94mLoss[0m : 5.34894
[1mStep[0m  [32/42], [94mLoss[0m : 5.49542
[1mStep[0m  [36/42], [94mLoss[0m : 5.29702
[1mStep[0m  [40/42], [94mLoss[0m : 5.39847

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 5.569, [92mTest[0m: 4.883, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.23417
[1mStep[0m  [4/42], [94mLoss[0m : 5.22692
[1mStep[0m  [8/42], [94mLoss[0m : 5.19036
[1mStep[0m  [12/42], [94mLoss[0m : 4.84698
[1mStep[0m  [16/42], [94mLoss[0m : 4.93034
[1mStep[0m  [20/42], [94mLoss[0m : 4.74074
[1mStep[0m  [24/42], [94mLoss[0m : 4.37553
[1mStep[0m  [28/42], [94mLoss[0m : 4.69664
[1mStep[0m  [32/42], [94mLoss[0m : 4.72827
[1mStep[0m  [36/42], [94mLoss[0m : 4.56346
[1mStep[0m  [40/42], [94mLoss[0m : 4.57979

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 4.842, [92mTest[0m: 4.228, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.34269
[1mStep[0m  [4/42], [94mLoss[0m : 4.36736
[1mStep[0m  [8/42], [94mLoss[0m : 4.58738
[1mStep[0m  [12/42], [94mLoss[0m : 4.34991
[1mStep[0m  [16/42], [94mLoss[0m : 4.33938
[1mStep[0m  [20/42], [94mLoss[0m : 4.14072
[1mStep[0m  [24/42], [94mLoss[0m : 4.02529
[1mStep[0m  [28/42], [94mLoss[0m : 3.57470
[1mStep[0m  [32/42], [94mLoss[0m : 3.82458
[1mStep[0m  [36/42], [94mLoss[0m : 3.71854
[1mStep[0m  [40/42], [94mLoss[0m : 3.80787

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 4.028, [92mTest[0m: 3.363, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.72236
[1mStep[0m  [4/42], [94mLoss[0m : 3.82577
[1mStep[0m  [8/42], [94mLoss[0m : 3.67277
[1mStep[0m  [12/42], [94mLoss[0m : 3.62984
[1mStep[0m  [16/42], [94mLoss[0m : 3.05753
[1mStep[0m  [20/42], [94mLoss[0m : 3.10580
[1mStep[0m  [24/42], [94mLoss[0m : 3.02225
[1mStep[0m  [28/42], [94mLoss[0m : 2.96528
[1mStep[0m  [32/42], [94mLoss[0m : 3.23354
[1mStep[0m  [36/42], [94mLoss[0m : 3.28026
[1mStep[0m  [40/42], [94mLoss[0m : 3.12492

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 3.285, [92mTest[0m: 2.855, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.77074
[1mStep[0m  [4/42], [94mLoss[0m : 2.73306
[1mStep[0m  [8/42], [94mLoss[0m : 3.05405
[1mStep[0m  [12/42], [94mLoss[0m : 3.10615
[1mStep[0m  [16/42], [94mLoss[0m : 2.68555
[1mStep[0m  [20/42], [94mLoss[0m : 2.79394
[1mStep[0m  [24/42], [94mLoss[0m : 2.74155
[1mStep[0m  [28/42], [94mLoss[0m : 2.85564
[1mStep[0m  [32/42], [94mLoss[0m : 2.96396
[1mStep[0m  [36/42], [94mLoss[0m : 2.80479
[1mStep[0m  [40/42], [94mLoss[0m : 2.74728

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.848, [92mTest[0m: 2.473, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.97641
[1mStep[0m  [4/42], [94mLoss[0m : 2.64503
[1mStep[0m  [8/42], [94mLoss[0m : 2.85096
[1mStep[0m  [12/42], [94mLoss[0m : 2.66228
[1mStep[0m  [16/42], [94mLoss[0m : 2.55965
[1mStep[0m  [20/42], [94mLoss[0m : 2.56483
[1mStep[0m  [24/42], [94mLoss[0m : 2.76421
[1mStep[0m  [28/42], [94mLoss[0m : 2.65305
[1mStep[0m  [32/42], [94mLoss[0m : 2.75244
[1mStep[0m  [36/42], [94mLoss[0m : 2.68777
[1mStep[0m  [40/42], [94mLoss[0m : 2.59700

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.723, [92mTest[0m: 2.377, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45777
[1mStep[0m  [4/42], [94mLoss[0m : 2.55936
[1mStep[0m  [8/42], [94mLoss[0m : 2.41015
[1mStep[0m  [12/42], [94mLoss[0m : 2.48374
[1mStep[0m  [16/42], [94mLoss[0m : 2.71837
[1mStep[0m  [20/42], [94mLoss[0m : 2.76455
[1mStep[0m  [24/42], [94mLoss[0m : 2.81878
[1mStep[0m  [28/42], [94mLoss[0m : 2.76090
[1mStep[0m  [32/42], [94mLoss[0m : 2.83712
[1mStep[0m  [36/42], [94mLoss[0m : 2.64942
[1mStep[0m  [40/42], [94mLoss[0m : 2.63648

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.628, [92mTest[0m: 2.384, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62749
[1mStep[0m  [4/42], [94mLoss[0m : 2.66296
[1mStep[0m  [8/42], [94mLoss[0m : 2.64235
[1mStep[0m  [12/42], [94mLoss[0m : 2.64770
[1mStep[0m  [16/42], [94mLoss[0m : 2.51859
[1mStep[0m  [20/42], [94mLoss[0m : 2.77437
[1mStep[0m  [24/42], [94mLoss[0m : 3.09194
[1mStep[0m  [28/42], [94mLoss[0m : 2.59176
[1mStep[0m  [32/42], [94mLoss[0m : 2.65409
[1mStep[0m  [36/42], [94mLoss[0m : 2.84540
[1mStep[0m  [40/42], [94mLoss[0m : 2.61077

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.648, [92mTest[0m: 2.453, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59265
[1mStep[0m  [4/42], [94mLoss[0m : 2.48895
[1mStep[0m  [8/42], [94mLoss[0m : 2.61120
[1mStep[0m  [12/42], [94mLoss[0m : 2.68893
[1mStep[0m  [16/42], [94mLoss[0m : 2.56846
[1mStep[0m  [20/42], [94mLoss[0m : 2.46658
[1mStep[0m  [24/42], [94mLoss[0m : 2.50188
[1mStep[0m  [28/42], [94mLoss[0m : 2.54445
[1mStep[0m  [32/42], [94mLoss[0m : 2.76157
[1mStep[0m  [36/42], [94mLoss[0m : 2.68803
[1mStep[0m  [40/42], [94mLoss[0m : 2.63788

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.617, [92mTest[0m: 2.374, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.82447
[1mStep[0m  [4/42], [94mLoss[0m : 2.61196
[1mStep[0m  [8/42], [94mLoss[0m : 2.52901
[1mStep[0m  [12/42], [94mLoss[0m : 2.58536
[1mStep[0m  [16/42], [94mLoss[0m : 2.62616
[1mStep[0m  [20/42], [94mLoss[0m : 2.70747
[1mStep[0m  [24/42], [94mLoss[0m : 2.74368
[1mStep[0m  [28/42], [94mLoss[0m : 2.56926
[1mStep[0m  [32/42], [94mLoss[0m : 2.68271
[1mStep[0m  [36/42], [94mLoss[0m : 2.62413
[1mStep[0m  [40/42], [94mLoss[0m : 2.29893

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.615, [92mTest[0m: 2.392, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64245
[1mStep[0m  [4/42], [94mLoss[0m : 2.63271
[1mStep[0m  [8/42], [94mLoss[0m : 2.70564
[1mStep[0m  [12/42], [94mLoss[0m : 2.53846
[1mStep[0m  [16/42], [94mLoss[0m : 2.47533
[1mStep[0m  [20/42], [94mLoss[0m : 2.75359
[1mStep[0m  [24/42], [94mLoss[0m : 2.44595
[1mStep[0m  [28/42], [94mLoss[0m : 2.64200
[1mStep[0m  [32/42], [94mLoss[0m : 2.58494
[1mStep[0m  [36/42], [94mLoss[0m : 2.83752
[1mStep[0m  [40/42], [94mLoss[0m : 2.62135

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.598, [92mTest[0m: 2.372, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49141
[1mStep[0m  [4/42], [94mLoss[0m : 2.52530
[1mStep[0m  [8/42], [94mLoss[0m : 2.74068
[1mStep[0m  [12/42], [94mLoss[0m : 2.73996
[1mStep[0m  [16/42], [94mLoss[0m : 2.75749
[1mStep[0m  [20/42], [94mLoss[0m : 2.60712
[1mStep[0m  [24/42], [94mLoss[0m : 2.70806
[1mStep[0m  [28/42], [94mLoss[0m : 2.52305
[1mStep[0m  [32/42], [94mLoss[0m : 2.53562
[1mStep[0m  [36/42], [94mLoss[0m : 2.40261
[1mStep[0m  [40/42], [94mLoss[0m : 2.76711

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.598, [92mTest[0m: 2.411, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54410
[1mStep[0m  [4/42], [94mLoss[0m : 2.55469
[1mStep[0m  [8/42], [94mLoss[0m : 2.69941
[1mStep[0m  [12/42], [94mLoss[0m : 2.88274
[1mStep[0m  [16/42], [94mLoss[0m : 2.49238
[1mStep[0m  [20/42], [94mLoss[0m : 2.68669
[1mStep[0m  [24/42], [94mLoss[0m : 2.60410
[1mStep[0m  [28/42], [94mLoss[0m : 2.74920
[1mStep[0m  [32/42], [94mLoss[0m : 2.43108
[1mStep[0m  [36/42], [94mLoss[0m : 2.59343
[1mStep[0m  [40/42], [94mLoss[0m : 2.52890

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.596, [92mTest[0m: 2.389, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64523
[1mStep[0m  [4/42], [94mLoss[0m : 2.65343
[1mStep[0m  [8/42], [94mLoss[0m : 2.52915
[1mStep[0m  [12/42], [94mLoss[0m : 2.55251
[1mStep[0m  [16/42], [94mLoss[0m : 2.77738
[1mStep[0m  [20/42], [94mLoss[0m : 2.32497
[1mStep[0m  [24/42], [94mLoss[0m : 2.53871
[1mStep[0m  [28/42], [94mLoss[0m : 2.62223
[1mStep[0m  [32/42], [94mLoss[0m : 2.49898
[1mStep[0m  [36/42], [94mLoss[0m : 2.74900
[1mStep[0m  [40/42], [94mLoss[0m : 2.65778

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.579, [92mTest[0m: 2.345, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52332
[1mStep[0m  [4/42], [94mLoss[0m : 2.87147
[1mStep[0m  [8/42], [94mLoss[0m : 2.58202
[1mStep[0m  [12/42], [94mLoss[0m : 2.52717
[1mStep[0m  [16/42], [94mLoss[0m : 2.69424
[1mStep[0m  [20/42], [94mLoss[0m : 2.54251
[1mStep[0m  [24/42], [94mLoss[0m : 2.46042
[1mStep[0m  [28/42], [94mLoss[0m : 2.44895
[1mStep[0m  [32/42], [94mLoss[0m : 2.62028
[1mStep[0m  [36/42], [94mLoss[0m : 2.60418
[1mStep[0m  [40/42], [94mLoss[0m : 2.32303

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.574, [92mTest[0m: 2.348, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42577
[1mStep[0m  [4/42], [94mLoss[0m : 2.56370
[1mStep[0m  [8/42], [94mLoss[0m : 2.54395
[1mStep[0m  [12/42], [94mLoss[0m : 2.43087
[1mStep[0m  [16/42], [94mLoss[0m : 2.72438
[1mStep[0m  [20/42], [94mLoss[0m : 2.68327
[1mStep[0m  [24/42], [94mLoss[0m : 2.57119
[1mStep[0m  [28/42], [94mLoss[0m : 2.60967
[1mStep[0m  [32/42], [94mLoss[0m : 2.58380
[1mStep[0m  [36/42], [94mLoss[0m : 2.59168
[1mStep[0m  [40/42], [94mLoss[0m : 2.52046

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.554, [92mTest[0m: 2.368, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49238
[1mStep[0m  [4/42], [94mLoss[0m : 2.75775
[1mStep[0m  [8/42], [94mLoss[0m : 2.36621
[1mStep[0m  [12/42], [94mLoss[0m : 2.61154
[1mStep[0m  [16/42], [94mLoss[0m : 2.69630
[1mStep[0m  [20/42], [94mLoss[0m : 2.79508
[1mStep[0m  [24/42], [94mLoss[0m : 2.67546
[1mStep[0m  [28/42], [94mLoss[0m : 2.38821
[1mStep[0m  [32/42], [94mLoss[0m : 2.52695
[1mStep[0m  [36/42], [94mLoss[0m : 2.58867
[1mStep[0m  [40/42], [94mLoss[0m : 2.53148

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.550, [92mTest[0m: 2.347, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37571
[1mStep[0m  [4/42], [94mLoss[0m : 2.33071
[1mStep[0m  [8/42], [94mLoss[0m : 2.39362
[1mStep[0m  [12/42], [94mLoss[0m : 2.60531
[1mStep[0m  [16/42], [94mLoss[0m : 2.68144
[1mStep[0m  [20/42], [94mLoss[0m : 2.50317
[1mStep[0m  [24/42], [94mLoss[0m : 2.63686
[1mStep[0m  [28/42], [94mLoss[0m : 2.41302
[1mStep[0m  [32/42], [94mLoss[0m : 2.69416
[1mStep[0m  [36/42], [94mLoss[0m : 2.69564
[1mStep[0m  [40/42], [94mLoss[0m : 2.40642

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.539, [92mTest[0m: 2.370, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37706
[1mStep[0m  [4/42], [94mLoss[0m : 2.66587
[1mStep[0m  [8/42], [94mLoss[0m : 2.59554
[1mStep[0m  [12/42], [94mLoss[0m : 2.35083
[1mStep[0m  [16/42], [94mLoss[0m : 2.72197
[1mStep[0m  [20/42], [94mLoss[0m : 2.35435
[1mStep[0m  [24/42], [94mLoss[0m : 2.62518
[1mStep[0m  [28/42], [94mLoss[0m : 2.60921
[1mStep[0m  [32/42], [94mLoss[0m : 2.54278
[1mStep[0m  [36/42], [94mLoss[0m : 2.69881
[1mStep[0m  [40/42], [94mLoss[0m : 2.59990

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.368, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.23474
[1mStep[0m  [4/42], [94mLoss[0m : 2.57217
[1mStep[0m  [8/42], [94mLoss[0m : 2.48537
[1mStep[0m  [12/42], [94mLoss[0m : 2.32885
[1mStep[0m  [16/42], [94mLoss[0m : 2.80531
[1mStep[0m  [20/42], [94mLoss[0m : 2.48846
[1mStep[0m  [24/42], [94mLoss[0m : 2.45674
[1mStep[0m  [28/42], [94mLoss[0m : 2.56841
[1mStep[0m  [32/42], [94mLoss[0m : 2.49483
[1mStep[0m  [36/42], [94mLoss[0m : 2.54847
[1mStep[0m  [40/42], [94mLoss[0m : 2.44014

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.549, [92mTest[0m: 2.359, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43226
[1mStep[0m  [4/42], [94mLoss[0m : 2.65349
[1mStep[0m  [8/42], [94mLoss[0m : 2.67526
[1mStep[0m  [12/42], [94mLoss[0m : 2.57004
[1mStep[0m  [16/42], [94mLoss[0m : 2.55280
[1mStep[0m  [20/42], [94mLoss[0m : 2.20902
[1mStep[0m  [24/42], [94mLoss[0m : 2.57521
[1mStep[0m  [28/42], [94mLoss[0m : 2.63080
[1mStep[0m  [32/42], [94mLoss[0m : 2.48642
[1mStep[0m  [36/42], [94mLoss[0m : 2.19049
[1mStep[0m  [40/42], [94mLoss[0m : 2.86429

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.540, [92mTest[0m: 2.366, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.67765
[1mStep[0m  [4/42], [94mLoss[0m : 2.65882
[1mStep[0m  [8/42], [94mLoss[0m : 2.37439
[1mStep[0m  [12/42], [94mLoss[0m : 2.68439
[1mStep[0m  [16/42], [94mLoss[0m : 2.64538
[1mStep[0m  [20/42], [94mLoss[0m : 2.57759
[1mStep[0m  [24/42], [94mLoss[0m : 2.36078
[1mStep[0m  [28/42], [94mLoss[0m : 2.44557
[1mStep[0m  [32/42], [94mLoss[0m : 2.51861
[1mStep[0m  [36/42], [94mLoss[0m : 2.71630
[1mStep[0m  [40/42], [94mLoss[0m : 2.38141

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.519, [92mTest[0m: 2.354, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36676
[1mStep[0m  [4/42], [94mLoss[0m : 2.40714
[1mStep[0m  [8/42], [94mLoss[0m : 2.30505
[1mStep[0m  [12/42], [94mLoss[0m : 2.77292
[1mStep[0m  [16/42], [94mLoss[0m : 2.51255
[1mStep[0m  [20/42], [94mLoss[0m : 2.59486
[1mStep[0m  [24/42], [94mLoss[0m : 2.44171
[1mStep[0m  [28/42], [94mLoss[0m : 2.60329
[1mStep[0m  [32/42], [94mLoss[0m : 2.40114
[1mStep[0m  [36/42], [94mLoss[0m : 2.50335
[1mStep[0m  [40/42], [94mLoss[0m : 2.42018

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.341, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.362
====================================

Phase 1 - Evaluation MAE:  2.362474645887102
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 2.48252
[1mStep[0m  [4/42], [94mLoss[0m : 2.29705
[1mStep[0m  [8/42], [94mLoss[0m : 2.54628
[1mStep[0m  [12/42], [94mLoss[0m : 2.59336
[1mStep[0m  [16/42], [94mLoss[0m : 2.55617
[1mStep[0m  [20/42], [94mLoss[0m : 2.72444
[1mStep[0m  [24/42], [94mLoss[0m : 2.93811
[1mStep[0m  [28/42], [94mLoss[0m : 2.34102
[1mStep[0m  [32/42], [94mLoss[0m : 2.47896
[1mStep[0m  [36/42], [94mLoss[0m : 2.82613
[1mStep[0m  [40/42], [94mLoss[0m : 2.47804

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.584, [92mTest[0m: 2.362, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46879
[1mStep[0m  [4/42], [94mLoss[0m : 2.46256
[1mStep[0m  [8/42], [94mLoss[0m : 2.60411
[1mStep[0m  [12/42], [94mLoss[0m : 2.34726
[1mStep[0m  [16/42], [94mLoss[0m : 2.66436
[1mStep[0m  [20/42], [94mLoss[0m : 2.42472
[1mStep[0m  [24/42], [94mLoss[0m : 2.65463
[1mStep[0m  [28/42], [94mLoss[0m : 2.72391
[1mStep[0m  [32/42], [94mLoss[0m : 2.48503
[1mStep[0m  [36/42], [94mLoss[0m : 2.38901
[1mStep[0m  [40/42], [94mLoss[0m : 2.63806

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.537, [92mTest[0m: 2.341, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47294
[1mStep[0m  [4/42], [94mLoss[0m : 2.43813
[1mStep[0m  [8/42], [94mLoss[0m : 2.63290
[1mStep[0m  [12/42], [94mLoss[0m : 2.53885
[1mStep[0m  [16/42], [94mLoss[0m : 2.28626
[1mStep[0m  [20/42], [94mLoss[0m : 2.38717
[1mStep[0m  [24/42], [94mLoss[0m : 2.35741
[1mStep[0m  [28/42], [94mLoss[0m : 2.57771
[1mStep[0m  [32/42], [94mLoss[0m : 2.52996
[1mStep[0m  [36/42], [94mLoss[0m : 2.58009
[1mStep[0m  [40/42], [94mLoss[0m : 2.25707

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.487, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53239
[1mStep[0m  [4/42], [94mLoss[0m : 2.47848
[1mStep[0m  [8/42], [94mLoss[0m : 2.70100
[1mStep[0m  [12/42], [94mLoss[0m : 2.34016
[1mStep[0m  [16/42], [94mLoss[0m : 2.44343
[1mStep[0m  [20/42], [94mLoss[0m : 2.77292
[1mStep[0m  [24/42], [94mLoss[0m : 2.53840
[1mStep[0m  [28/42], [94mLoss[0m : 2.75981
[1mStep[0m  [32/42], [94mLoss[0m : 2.58530
[1mStep[0m  [36/42], [94mLoss[0m : 2.37673
[1mStep[0m  [40/42], [94mLoss[0m : 2.40876

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.420, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29534
[1mStep[0m  [4/42], [94mLoss[0m : 2.26385
[1mStep[0m  [8/42], [94mLoss[0m : 2.50422
[1mStep[0m  [12/42], [94mLoss[0m : 2.63265
[1mStep[0m  [16/42], [94mLoss[0m : 2.30130
[1mStep[0m  [20/42], [94mLoss[0m : 2.24541
[1mStep[0m  [24/42], [94mLoss[0m : 2.44129
[1mStep[0m  [28/42], [94mLoss[0m : 2.28738
[1mStep[0m  [32/42], [94mLoss[0m : 2.48448
[1mStep[0m  [36/42], [94mLoss[0m : 2.36887
[1mStep[0m  [40/42], [94mLoss[0m : 2.54158

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.447, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47017
[1mStep[0m  [4/42], [94mLoss[0m : 2.32246
[1mStep[0m  [8/42], [94mLoss[0m : 2.49342
[1mStep[0m  [12/42], [94mLoss[0m : 2.26671
[1mStep[0m  [16/42], [94mLoss[0m : 2.29835
[1mStep[0m  [20/42], [94mLoss[0m : 2.39393
[1mStep[0m  [24/42], [94mLoss[0m : 2.40642
[1mStep[0m  [28/42], [94mLoss[0m : 2.25694
[1mStep[0m  [32/42], [94mLoss[0m : 2.54141
[1mStep[0m  [36/42], [94mLoss[0m : 2.39946
[1mStep[0m  [40/42], [94mLoss[0m : 2.40092

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.484, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26283
[1mStep[0m  [4/42], [94mLoss[0m : 2.33932
[1mStep[0m  [8/42], [94mLoss[0m : 2.50074
[1mStep[0m  [12/42], [94mLoss[0m : 2.59072
[1mStep[0m  [16/42], [94mLoss[0m : 2.28356
[1mStep[0m  [20/42], [94mLoss[0m : 2.41071
[1mStep[0m  [24/42], [94mLoss[0m : 2.38682
[1mStep[0m  [28/42], [94mLoss[0m : 2.50241
[1mStep[0m  [32/42], [94mLoss[0m : 2.42952
[1mStep[0m  [36/42], [94mLoss[0m : 2.45005
[1mStep[0m  [40/42], [94mLoss[0m : 2.43817

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.376, [92mTest[0m: 2.495, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.20801
[1mStep[0m  [4/42], [94mLoss[0m : 2.43594
[1mStep[0m  [8/42], [94mLoss[0m : 2.28897
[1mStep[0m  [12/42], [94mLoss[0m : 2.26465
[1mStep[0m  [16/42], [94mLoss[0m : 2.18128
[1mStep[0m  [20/42], [94mLoss[0m : 2.35376
[1mStep[0m  [24/42], [94mLoss[0m : 2.29777
[1mStep[0m  [28/42], [94mLoss[0m : 2.27010
[1mStep[0m  [32/42], [94mLoss[0m : 2.32450
[1mStep[0m  [36/42], [94mLoss[0m : 2.37394
[1mStep[0m  [40/42], [94mLoss[0m : 2.29996

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.330, [92mTest[0m: 2.531, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50105
[1mStep[0m  [4/42], [94mLoss[0m : 2.44068
[1mStep[0m  [8/42], [94mLoss[0m : 2.25409
[1mStep[0m  [12/42], [94mLoss[0m : 2.23045
[1mStep[0m  [16/42], [94mLoss[0m : 2.23864
[1mStep[0m  [20/42], [94mLoss[0m : 2.52758
[1mStep[0m  [24/42], [94mLoss[0m : 2.45948
[1mStep[0m  [28/42], [94mLoss[0m : 2.28221
[1mStep[0m  [32/42], [94mLoss[0m : 2.36643
[1mStep[0m  [36/42], [94mLoss[0m : 2.31930
[1mStep[0m  [40/42], [94mLoss[0m : 2.35440

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.305, [92mTest[0m: 2.493, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.19450
[1mStep[0m  [4/42], [94mLoss[0m : 2.08570
[1mStep[0m  [8/42], [94mLoss[0m : 2.16692
[1mStep[0m  [12/42], [94mLoss[0m : 2.30107
[1mStep[0m  [16/42], [94mLoss[0m : 2.04396
[1mStep[0m  [20/42], [94mLoss[0m : 2.29823
[1mStep[0m  [24/42], [94mLoss[0m : 2.30591
[1mStep[0m  [28/42], [94mLoss[0m : 2.46946
[1mStep[0m  [32/42], [94mLoss[0m : 2.18568
[1mStep[0m  [36/42], [94mLoss[0m : 2.40918
[1mStep[0m  [40/42], [94mLoss[0m : 2.07344

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.224, [92mTest[0m: 2.508, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.20569
[1mStep[0m  [4/42], [94mLoss[0m : 2.20803
[1mStep[0m  [8/42], [94mLoss[0m : 2.23198
[1mStep[0m  [12/42], [94mLoss[0m : 2.21031
[1mStep[0m  [16/42], [94mLoss[0m : 2.36518
[1mStep[0m  [20/42], [94mLoss[0m : 1.98837
[1mStep[0m  [24/42], [94mLoss[0m : 2.43492
[1mStep[0m  [28/42], [94mLoss[0m : 2.11233
[1mStep[0m  [32/42], [94mLoss[0m : 2.28768
[1mStep[0m  [36/42], [94mLoss[0m : 2.28401
[1mStep[0m  [40/42], [94mLoss[0m : 2.34068

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.222, [92mTest[0m: 2.513, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.17614
[1mStep[0m  [4/42], [94mLoss[0m : 2.38090
[1mStep[0m  [8/42], [94mLoss[0m : 2.21479
[1mStep[0m  [12/42], [94mLoss[0m : 2.05257
[1mStep[0m  [16/42], [94mLoss[0m : 2.04732
[1mStep[0m  [20/42], [94mLoss[0m : 2.24115
[1mStep[0m  [24/42], [94mLoss[0m : 2.25220
[1mStep[0m  [28/42], [94mLoss[0m : 2.21947
[1mStep[0m  [32/42], [94mLoss[0m : 2.08861
[1mStep[0m  [36/42], [94mLoss[0m : 2.16723
[1mStep[0m  [40/42], [94mLoss[0m : 2.26657

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.158, [92mTest[0m: 2.652, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.08932
[1mStep[0m  [4/42], [94mLoss[0m : 2.11563
[1mStep[0m  [8/42], [94mLoss[0m : 2.18992
[1mStep[0m  [12/42], [94mLoss[0m : 2.00947
[1mStep[0m  [16/42], [94mLoss[0m : 2.32037
[1mStep[0m  [20/42], [94mLoss[0m : 1.96075
[1mStep[0m  [24/42], [94mLoss[0m : 2.11957
[1mStep[0m  [28/42], [94mLoss[0m : 2.03168
[1mStep[0m  [32/42], [94mLoss[0m : 2.26300
[1mStep[0m  [36/42], [94mLoss[0m : 2.15111
[1mStep[0m  [40/42], [94mLoss[0m : 2.25756

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.156, [92mTest[0m: 2.491, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.09793
[1mStep[0m  [4/42], [94mLoss[0m : 2.10215
[1mStep[0m  [8/42], [94mLoss[0m : 1.87622
[1mStep[0m  [12/42], [94mLoss[0m : 2.17169
[1mStep[0m  [16/42], [94mLoss[0m : 2.15453
[1mStep[0m  [20/42], [94mLoss[0m : 2.05050
[1mStep[0m  [24/42], [94mLoss[0m : 2.23036
[1mStep[0m  [28/42], [94mLoss[0m : 2.18227
[1mStep[0m  [32/42], [94mLoss[0m : 2.08542
[1mStep[0m  [36/42], [94mLoss[0m : 2.14987
[1mStep[0m  [40/42], [94mLoss[0m : 1.95864

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.098, [92mTest[0m: 2.614, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24245
[1mStep[0m  [4/42], [94mLoss[0m : 2.05250
[1mStep[0m  [8/42], [94mLoss[0m : 2.19227
[1mStep[0m  [12/42], [94mLoss[0m : 1.99051
[1mStep[0m  [16/42], [94mLoss[0m : 2.25821
[1mStep[0m  [20/42], [94mLoss[0m : 2.12666
[1mStep[0m  [24/42], [94mLoss[0m : 2.16869
[1mStep[0m  [28/42], [94mLoss[0m : 1.88644
[1mStep[0m  [32/42], [94mLoss[0m : 1.94216
[1mStep[0m  [36/42], [94mLoss[0m : 2.15343
[1mStep[0m  [40/42], [94mLoss[0m : 2.21700

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.077, [92mTest[0m: 2.678, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.97800
[1mStep[0m  [4/42], [94mLoss[0m : 2.09800
[1mStep[0m  [8/42], [94mLoss[0m : 2.18788
[1mStep[0m  [12/42], [94mLoss[0m : 1.93817
[1mStep[0m  [16/42], [94mLoss[0m : 1.95137
[1mStep[0m  [20/42], [94mLoss[0m : 1.98515
[1mStep[0m  [24/42], [94mLoss[0m : 2.01715
[1mStep[0m  [28/42], [94mLoss[0m : 2.12725
[1mStep[0m  [32/42], [94mLoss[0m : 2.19797
[1mStep[0m  [36/42], [94mLoss[0m : 2.09942
[1mStep[0m  [40/42], [94mLoss[0m : 2.01712

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.034, [92mTest[0m: 2.677, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.13960
[1mStep[0m  [4/42], [94mLoss[0m : 1.94274
[1mStep[0m  [8/42], [94mLoss[0m : 1.98429
[1mStep[0m  [12/42], [94mLoss[0m : 2.03356
[1mStep[0m  [16/42], [94mLoss[0m : 2.10699
[1mStep[0m  [20/42], [94mLoss[0m : 1.88103
[1mStep[0m  [24/42], [94mLoss[0m : 2.03235
[1mStep[0m  [28/42], [94mLoss[0m : 2.41874
[1mStep[0m  [32/42], [94mLoss[0m : 1.86616
[1mStep[0m  [36/42], [94mLoss[0m : 1.95770
[1mStep[0m  [40/42], [94mLoss[0m : 2.08684

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.008, [92mTest[0m: 2.606, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.82067
[1mStep[0m  [4/42], [94mLoss[0m : 1.98407
[1mStep[0m  [8/42], [94mLoss[0m : 1.94453
[1mStep[0m  [12/42], [94mLoss[0m : 1.94570
[1mStep[0m  [16/42], [94mLoss[0m : 2.07814
[1mStep[0m  [20/42], [94mLoss[0m : 1.81288
[1mStep[0m  [24/42], [94mLoss[0m : 2.19873
[1mStep[0m  [28/42], [94mLoss[0m : 1.98838
[1mStep[0m  [32/42], [94mLoss[0m : 1.85872
[1mStep[0m  [36/42], [94mLoss[0m : 1.90665
[1mStep[0m  [40/42], [94mLoss[0m : 1.71897

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.981, [92mTest[0m: 2.437, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.83674
[1mStep[0m  [4/42], [94mLoss[0m : 1.66297
[1mStep[0m  [8/42], [94mLoss[0m : 1.94785
[1mStep[0m  [12/42], [94mLoss[0m : 1.94829
[1mStep[0m  [16/42], [94mLoss[0m : 1.80283
[1mStep[0m  [20/42], [94mLoss[0m : 1.93115
[1mStep[0m  [24/42], [94mLoss[0m : 1.97775
[1mStep[0m  [28/42], [94mLoss[0m : 1.83647
[1mStep[0m  [32/42], [94mLoss[0m : 1.76998
[1mStep[0m  [36/42], [94mLoss[0m : 1.87410
[1mStep[0m  [40/42], [94mLoss[0m : 1.91569

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.922, [92mTest[0m: 2.459, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.73815
[1mStep[0m  [4/42], [94mLoss[0m : 1.85817
[1mStep[0m  [8/42], [94mLoss[0m : 1.91636
[1mStep[0m  [12/42], [94mLoss[0m : 1.85328
[1mStep[0m  [16/42], [94mLoss[0m : 2.06044
[1mStep[0m  [20/42], [94mLoss[0m : 1.75666
[1mStep[0m  [24/42], [94mLoss[0m : 1.87470
[1mStep[0m  [28/42], [94mLoss[0m : 1.79059
[1mStep[0m  [32/42], [94mLoss[0m : 1.94472
[1mStep[0m  [36/42], [94mLoss[0m : 2.02255
[1mStep[0m  [40/42], [94mLoss[0m : 2.04562

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.908, [92mTest[0m: 2.455, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.94863
[1mStep[0m  [4/42], [94mLoss[0m : 1.62833
[1mStep[0m  [8/42], [94mLoss[0m : 1.86037
[1mStep[0m  [12/42], [94mLoss[0m : 1.76154
[1mStep[0m  [16/42], [94mLoss[0m : 2.00982
[1mStep[0m  [20/42], [94mLoss[0m : 1.80666
[1mStep[0m  [24/42], [94mLoss[0m : 1.84920
[1mStep[0m  [28/42], [94mLoss[0m : 1.75768
[1mStep[0m  [32/42], [94mLoss[0m : 2.06551
[1mStep[0m  [36/42], [94mLoss[0m : 1.77870
[1mStep[0m  [40/42], [94mLoss[0m : 1.83712

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.865, [92mTest[0m: 2.434, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.74125
[1mStep[0m  [4/42], [94mLoss[0m : 1.80749
[1mStep[0m  [8/42], [94mLoss[0m : 2.06736
[1mStep[0m  [12/42], [94mLoss[0m : 1.72829
[1mStep[0m  [16/42], [94mLoss[0m : 1.61234
[1mStep[0m  [20/42], [94mLoss[0m : 1.72342
[1mStep[0m  [24/42], [94mLoss[0m : 1.86490
[1mStep[0m  [28/42], [94mLoss[0m : 1.71242
[1mStep[0m  [32/42], [94mLoss[0m : 1.99744
[1mStep[0m  [36/42], [94mLoss[0m : 1.87405
[1mStep[0m  [40/42], [94mLoss[0m : 1.94576

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.831, [92mTest[0m: 2.429, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.87851
[1mStep[0m  [4/42], [94mLoss[0m : 1.84274
[1mStep[0m  [8/42], [94mLoss[0m : 1.72942
[1mStep[0m  [12/42], [94mLoss[0m : 1.65148
[1mStep[0m  [16/42], [94mLoss[0m : 1.84553
[1mStep[0m  [20/42], [94mLoss[0m : 1.93541
[1mStep[0m  [24/42], [94mLoss[0m : 1.83872
[1mStep[0m  [28/42], [94mLoss[0m : 1.87692
[1mStep[0m  [32/42], [94mLoss[0m : 1.67743
[1mStep[0m  [36/42], [94mLoss[0m : 1.73980
[1mStep[0m  [40/42], [94mLoss[0m : 1.81913

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.786, [92mTest[0m: 2.532, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.65037
[1mStep[0m  [4/42], [94mLoss[0m : 1.70047
[1mStep[0m  [8/42], [94mLoss[0m : 1.69685
[1mStep[0m  [12/42], [94mLoss[0m : 1.64153
[1mStep[0m  [16/42], [94mLoss[0m : 1.86321
[1mStep[0m  [20/42], [94mLoss[0m : 1.93445
[1mStep[0m  [24/42], [94mLoss[0m : 1.71070
[1mStep[0m  [28/42], [94mLoss[0m : 2.01102
[1mStep[0m  [32/42], [94mLoss[0m : 1.76622
[1mStep[0m  [36/42], [94mLoss[0m : 1.83171
[1mStep[0m  [40/42], [94mLoss[0m : 1.74063

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.801, [92mTest[0m: 2.517, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.65170
[1mStep[0m  [4/42], [94mLoss[0m : 1.70241
[1mStep[0m  [8/42], [94mLoss[0m : 1.64347
[1mStep[0m  [12/42], [94mLoss[0m : 1.74180
[1mStep[0m  [16/42], [94mLoss[0m : 1.71378
[1mStep[0m  [20/42], [94mLoss[0m : 1.76113
[1mStep[0m  [24/42], [94mLoss[0m : 1.69256
[1mStep[0m  [28/42], [94mLoss[0m : 1.82317
[1mStep[0m  [32/42], [94mLoss[0m : 1.82139
[1mStep[0m  [36/42], [94mLoss[0m : 1.83532
[1mStep[0m  [40/42], [94mLoss[0m : 1.85004

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.774, [92mTest[0m: 2.467, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.68498
[1mStep[0m  [4/42], [94mLoss[0m : 1.66152
[1mStep[0m  [8/42], [94mLoss[0m : 1.68544
[1mStep[0m  [12/42], [94mLoss[0m : 1.70687
[1mStep[0m  [16/42], [94mLoss[0m : 1.70736
[1mStep[0m  [20/42], [94mLoss[0m : 1.86583
[1mStep[0m  [24/42], [94mLoss[0m : 1.71159
[1mStep[0m  [28/42], [94mLoss[0m : 1.58621
[1mStep[0m  [32/42], [94mLoss[0m : 1.66338
[1mStep[0m  [36/42], [94mLoss[0m : 1.83600
[1mStep[0m  [40/42], [94mLoss[0m : 1.90803

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.747, [92mTest[0m: 2.517, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.77497
[1mStep[0m  [4/42], [94mLoss[0m : 1.79491
[1mStep[0m  [8/42], [94mLoss[0m : 1.71069
[1mStep[0m  [12/42], [94mLoss[0m : 1.72789
[1mStep[0m  [16/42], [94mLoss[0m : 1.73616
[1mStep[0m  [20/42], [94mLoss[0m : 1.89667
[1mStep[0m  [24/42], [94mLoss[0m : 1.73192
[1mStep[0m  [28/42], [94mLoss[0m : 1.71625
[1mStep[0m  [32/42], [94mLoss[0m : 1.73035
[1mStep[0m  [36/42], [94mLoss[0m : 1.74050
[1mStep[0m  [40/42], [94mLoss[0m : 1.73977

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.723, [92mTest[0m: 2.502, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.66807
[1mStep[0m  [4/42], [94mLoss[0m : 1.86476
[1mStep[0m  [8/42], [94mLoss[0m : 1.60706
[1mStep[0m  [12/42], [94mLoss[0m : 1.67065
[1mStep[0m  [16/42], [94mLoss[0m : 1.67939
[1mStep[0m  [20/42], [94mLoss[0m : 1.58447
[1mStep[0m  [24/42], [94mLoss[0m : 1.61785
[1mStep[0m  [28/42], [94mLoss[0m : 1.74035
[1mStep[0m  [32/42], [94mLoss[0m : 1.71364
[1mStep[0m  [36/42], [94mLoss[0m : 1.78652
[1mStep[0m  [40/42], [94mLoss[0m : 1.64285

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.704, [92mTest[0m: 2.515, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.66073
[1mStep[0m  [4/42], [94mLoss[0m : 1.72318
[1mStep[0m  [8/42], [94mLoss[0m : 1.90416
[1mStep[0m  [12/42], [94mLoss[0m : 1.57108
[1mStep[0m  [16/42], [94mLoss[0m : 1.62881
[1mStep[0m  [20/42], [94mLoss[0m : 1.66301
[1mStep[0m  [24/42], [94mLoss[0m : 1.52430
[1mStep[0m  [28/42], [94mLoss[0m : 1.49083
[1mStep[0m  [32/42], [94mLoss[0m : 1.63560
[1mStep[0m  [36/42], [94mLoss[0m : 1.72451
[1mStep[0m  [40/42], [94mLoss[0m : 1.64389

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.681, [92mTest[0m: 2.515, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.47162
[1mStep[0m  [4/42], [94mLoss[0m : 1.75856
[1mStep[0m  [8/42], [94mLoss[0m : 1.76690
[1mStep[0m  [12/42], [94mLoss[0m : 1.48629
[1mStep[0m  [16/42], [94mLoss[0m : 1.57196
[1mStep[0m  [20/42], [94mLoss[0m : 1.66174
[1mStep[0m  [24/42], [94mLoss[0m : 1.63333
[1mStep[0m  [28/42], [94mLoss[0m : 1.55271
[1mStep[0m  [32/42], [94mLoss[0m : 1.59964
[1mStep[0m  [36/42], [94mLoss[0m : 1.69093
[1mStep[0m  [40/42], [94mLoss[0m : 1.63858

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.650, [92mTest[0m: 2.519, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.652
====================================

Phase 2 - Evaluation MAE:  2.6517100504466464
MAE score P1      2.362475
MAE score P2       2.65171
loss              1.649687
learning_rate     0.007525
batch_size             256
hidden_sizes         [300]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.1
weight_decay          0.01
Name: 17, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 10.62749
[1mStep[0m  [4/42], [94mLoss[0m : 9.69845
[1mStep[0m  [8/42], [94mLoss[0m : 7.63737
[1mStep[0m  [12/42], [94mLoss[0m : 4.82708
[1mStep[0m  [16/42], [94mLoss[0m : 2.92119
[1mStep[0m  [20/42], [94mLoss[0m : 3.29284
[1mStep[0m  [24/42], [94mLoss[0m : 3.25591
[1mStep[0m  [28/42], [94mLoss[0m : 2.95823
[1mStep[0m  [32/42], [94mLoss[0m : 2.70390
[1mStep[0m  [36/42], [94mLoss[0m : 2.62155
[1mStep[0m  [40/42], [94mLoss[0m : 2.62594

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.749, [92mTest[0m: 10.982, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.96738
[1mStep[0m  [4/42], [94mLoss[0m : 2.70802
[1mStep[0m  [8/42], [94mLoss[0m : 2.59007
[1mStep[0m  [12/42], [94mLoss[0m : 2.65716
[1mStep[0m  [16/42], [94mLoss[0m : 2.67834
[1mStep[0m  [20/42], [94mLoss[0m : 2.33099
[1mStep[0m  [24/42], [94mLoss[0m : 2.87366
[1mStep[0m  [28/42], [94mLoss[0m : 2.68524
[1mStep[0m  [32/42], [94mLoss[0m : 2.50375
[1mStep[0m  [36/42], [94mLoss[0m : 2.61176
[1mStep[0m  [40/42], [94mLoss[0m : 2.55395

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.608, [92mTest[0m: 3.123, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45534
[1mStep[0m  [4/42], [94mLoss[0m : 2.88276
[1mStep[0m  [8/42], [94mLoss[0m : 2.48385
[1mStep[0m  [12/42], [94mLoss[0m : 2.61239
[1mStep[0m  [16/42], [94mLoss[0m : 2.30395
[1mStep[0m  [20/42], [94mLoss[0m : 2.64137
[1mStep[0m  [24/42], [94mLoss[0m : 2.46032
[1mStep[0m  [28/42], [94mLoss[0m : 2.48179
[1mStep[0m  [32/42], [94mLoss[0m : 2.65273
[1mStep[0m  [36/42], [94mLoss[0m : 2.65806
[1mStep[0m  [40/42], [94mLoss[0m : 2.48726

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.407, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42773
[1mStep[0m  [4/42], [94mLoss[0m : 2.27572
[1mStep[0m  [8/42], [94mLoss[0m : 2.49568
[1mStep[0m  [12/42], [94mLoss[0m : 2.82282
[1mStep[0m  [16/42], [94mLoss[0m : 2.40758
[1mStep[0m  [20/42], [94mLoss[0m : 2.43857
[1mStep[0m  [24/42], [94mLoss[0m : 2.31669
[1mStep[0m  [28/42], [94mLoss[0m : 2.50953
[1mStep[0m  [32/42], [94mLoss[0m : 2.40463
[1mStep[0m  [36/42], [94mLoss[0m : 2.63223
[1mStep[0m  [40/42], [94mLoss[0m : 2.39492

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.432, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38118
[1mStep[0m  [4/42], [94mLoss[0m : 2.57154
[1mStep[0m  [8/42], [94mLoss[0m : 2.47947
[1mStep[0m  [12/42], [94mLoss[0m : 2.47387
[1mStep[0m  [16/42], [94mLoss[0m : 2.53050
[1mStep[0m  [20/42], [94mLoss[0m : 2.35344
[1mStep[0m  [24/42], [94mLoss[0m : 2.49970
[1mStep[0m  [28/42], [94mLoss[0m : 2.43020
[1mStep[0m  [32/42], [94mLoss[0m : 2.57746
[1mStep[0m  [36/42], [94mLoss[0m : 2.47808
[1mStep[0m  [40/42], [94mLoss[0m : 2.57723

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.385, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33513
[1mStep[0m  [4/42], [94mLoss[0m : 2.46648
[1mStep[0m  [8/42], [94mLoss[0m : 2.48845
[1mStep[0m  [12/42], [94mLoss[0m : 2.63622
[1mStep[0m  [16/42], [94mLoss[0m : 2.43445
[1mStep[0m  [20/42], [94mLoss[0m : 2.22289
[1mStep[0m  [24/42], [94mLoss[0m : 2.40188
[1mStep[0m  [28/42], [94mLoss[0m : 2.61640
[1mStep[0m  [32/42], [94mLoss[0m : 2.50542
[1mStep[0m  [36/42], [94mLoss[0m : 2.33697
[1mStep[0m  [40/42], [94mLoss[0m : 2.37967

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.331, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45597
[1mStep[0m  [4/42], [94mLoss[0m : 2.25220
[1mStep[0m  [8/42], [94mLoss[0m : 2.46445
[1mStep[0m  [12/42], [94mLoss[0m : 2.25370
[1mStep[0m  [16/42], [94mLoss[0m : 2.45379
[1mStep[0m  [20/42], [94mLoss[0m : 2.58597
[1mStep[0m  [24/42], [94mLoss[0m : 2.45549
[1mStep[0m  [28/42], [94mLoss[0m : 2.24553
[1mStep[0m  [32/42], [94mLoss[0m : 2.33414
[1mStep[0m  [36/42], [94mLoss[0m : 2.48649
[1mStep[0m  [40/42], [94mLoss[0m : 2.72574

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.365, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32177
[1mStep[0m  [4/42], [94mLoss[0m : 2.32567
[1mStep[0m  [8/42], [94mLoss[0m : 2.37465
[1mStep[0m  [12/42], [94mLoss[0m : 2.49591
[1mStep[0m  [16/42], [94mLoss[0m : 2.64994
[1mStep[0m  [20/42], [94mLoss[0m : 2.50280
[1mStep[0m  [24/42], [94mLoss[0m : 2.43371
[1mStep[0m  [28/42], [94mLoss[0m : 2.44012
[1mStep[0m  [32/42], [94mLoss[0m : 2.53176
[1mStep[0m  [36/42], [94mLoss[0m : 2.57793
[1mStep[0m  [40/42], [94mLoss[0m : 2.25200

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.325, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.13212
[1mStep[0m  [4/42], [94mLoss[0m : 2.40573
[1mStep[0m  [8/42], [94mLoss[0m : 2.45992
[1mStep[0m  [12/42], [94mLoss[0m : 2.35192
[1mStep[0m  [16/42], [94mLoss[0m : 2.38084
[1mStep[0m  [20/42], [94mLoss[0m : 2.54279
[1mStep[0m  [24/42], [94mLoss[0m : 2.17403
[1mStep[0m  [28/42], [94mLoss[0m : 2.26902
[1mStep[0m  [32/42], [94mLoss[0m : 2.60250
[1mStep[0m  [36/42], [94mLoss[0m : 2.43340
[1mStep[0m  [40/42], [94mLoss[0m : 2.43925

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.337, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24414
[1mStep[0m  [4/42], [94mLoss[0m : 2.41614
[1mStep[0m  [8/42], [94mLoss[0m : 2.40543
[1mStep[0m  [12/42], [94mLoss[0m : 2.36068
[1mStep[0m  [16/42], [94mLoss[0m : 2.31587
[1mStep[0m  [20/42], [94mLoss[0m : 2.57644
[1mStep[0m  [24/42], [94mLoss[0m : 2.18008
[1mStep[0m  [28/42], [94mLoss[0m : 2.33034
[1mStep[0m  [32/42], [94mLoss[0m : 2.35525
[1mStep[0m  [36/42], [94mLoss[0m : 2.50697
[1mStep[0m  [40/42], [94mLoss[0m : 2.26887

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.385, [92mTest[0m: 2.316, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34554
[1mStep[0m  [4/42], [94mLoss[0m : 2.36055
[1mStep[0m  [8/42], [94mLoss[0m : 2.57076
[1mStep[0m  [12/42], [94mLoss[0m : 2.26636
[1mStep[0m  [16/42], [94mLoss[0m : 2.48748
[1mStep[0m  [20/42], [94mLoss[0m : 2.21277
[1mStep[0m  [24/42], [94mLoss[0m : 2.26348
[1mStep[0m  [28/42], [94mLoss[0m : 2.29546
[1mStep[0m  [32/42], [94mLoss[0m : 2.53445
[1mStep[0m  [36/42], [94mLoss[0m : 2.53085
[1mStep[0m  [40/42], [94mLoss[0m : 2.26238

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.379, [92mTest[0m: 2.318, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57898
[1mStep[0m  [4/42], [94mLoss[0m : 2.56988
[1mStep[0m  [8/42], [94mLoss[0m : 2.48827
[1mStep[0m  [12/42], [94mLoss[0m : 2.59246
[1mStep[0m  [16/42], [94mLoss[0m : 2.10095
[1mStep[0m  [20/42], [94mLoss[0m : 2.37764
[1mStep[0m  [24/42], [94mLoss[0m : 2.23442
[1mStep[0m  [28/42], [94mLoss[0m : 2.29387
[1mStep[0m  [32/42], [94mLoss[0m : 2.22160
[1mStep[0m  [36/42], [94mLoss[0m : 2.40257
[1mStep[0m  [40/42], [94mLoss[0m : 2.30403

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.356, [92mTest[0m: 2.338, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24387
[1mStep[0m  [4/42], [94mLoss[0m : 2.30475
[1mStep[0m  [8/42], [94mLoss[0m : 2.23427
[1mStep[0m  [12/42], [94mLoss[0m : 2.39553
[1mStep[0m  [16/42], [94mLoss[0m : 2.23949
[1mStep[0m  [20/42], [94mLoss[0m : 2.42588
[1mStep[0m  [24/42], [94mLoss[0m : 2.46977
[1mStep[0m  [28/42], [94mLoss[0m : 2.27655
[1mStep[0m  [32/42], [94mLoss[0m : 2.24829
[1mStep[0m  [36/42], [94mLoss[0m : 2.39873
[1mStep[0m  [40/42], [94mLoss[0m : 2.27699

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.353, [92mTest[0m: 2.314, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43930
[1mStep[0m  [4/42], [94mLoss[0m : 2.40594
[1mStep[0m  [8/42], [94mLoss[0m : 2.24500
[1mStep[0m  [12/42], [94mLoss[0m : 2.30669
[1mStep[0m  [16/42], [94mLoss[0m : 2.26919
[1mStep[0m  [20/42], [94mLoss[0m : 2.35792
[1mStep[0m  [24/42], [94mLoss[0m : 2.31357
[1mStep[0m  [28/42], [94mLoss[0m : 2.49742
[1mStep[0m  [32/42], [94mLoss[0m : 2.52215
[1mStep[0m  [36/42], [94mLoss[0m : 2.33881
[1mStep[0m  [40/42], [94mLoss[0m : 2.19543

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.337, [92mTest[0m: 2.312, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21995
[1mStep[0m  [4/42], [94mLoss[0m : 2.31592
[1mStep[0m  [8/42], [94mLoss[0m : 2.04845
[1mStep[0m  [12/42], [94mLoss[0m : 2.25149
[1mStep[0m  [16/42], [94mLoss[0m : 2.38211
[1mStep[0m  [20/42], [94mLoss[0m : 2.34232
[1mStep[0m  [24/42], [94mLoss[0m : 2.28221
[1mStep[0m  [28/42], [94mLoss[0m : 2.41674
[1mStep[0m  [32/42], [94mLoss[0m : 2.35005
[1mStep[0m  [36/42], [94mLoss[0m : 2.33684
[1mStep[0m  [40/42], [94mLoss[0m : 2.56315

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.345, [92mTest[0m: 2.301, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36296
[1mStep[0m  [4/42], [94mLoss[0m : 2.35635
[1mStep[0m  [8/42], [94mLoss[0m : 2.30131
[1mStep[0m  [12/42], [94mLoss[0m : 2.04230
[1mStep[0m  [16/42], [94mLoss[0m : 2.32025
[1mStep[0m  [20/42], [94mLoss[0m : 2.40068
[1mStep[0m  [24/42], [94mLoss[0m : 2.27101
[1mStep[0m  [28/42], [94mLoss[0m : 2.41113
[1mStep[0m  [32/42], [94mLoss[0m : 2.01503
[1mStep[0m  [36/42], [94mLoss[0m : 2.21114
[1mStep[0m  [40/42], [94mLoss[0m : 2.03879

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.338, [92mTest[0m: 2.317, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58274
[1mStep[0m  [4/42], [94mLoss[0m : 2.15488
[1mStep[0m  [8/42], [94mLoss[0m : 2.14932
[1mStep[0m  [12/42], [94mLoss[0m : 2.17709
[1mStep[0m  [16/42], [94mLoss[0m : 2.35003
[1mStep[0m  [20/42], [94mLoss[0m : 2.23968
[1mStep[0m  [24/42], [94mLoss[0m : 2.40203
[1mStep[0m  [28/42], [94mLoss[0m : 2.52607
[1mStep[0m  [32/42], [94mLoss[0m : 2.16718
[1mStep[0m  [36/42], [94mLoss[0m : 2.12349
[1mStep[0m  [40/42], [94mLoss[0m : 2.24006

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.332, [92mTest[0m: 2.332, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39543
[1mStep[0m  [4/42], [94mLoss[0m : 2.43091
[1mStep[0m  [8/42], [94mLoss[0m : 2.25763
[1mStep[0m  [12/42], [94mLoss[0m : 2.35782
[1mStep[0m  [16/42], [94mLoss[0m : 2.34247
[1mStep[0m  [20/42], [94mLoss[0m : 2.45338
[1mStep[0m  [24/42], [94mLoss[0m : 2.21119
[1mStep[0m  [28/42], [94mLoss[0m : 2.35490
[1mStep[0m  [32/42], [94mLoss[0m : 2.32785
[1mStep[0m  [36/42], [94mLoss[0m : 2.39551
[1mStep[0m  [40/42], [94mLoss[0m : 2.38603

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.322, [92mTest[0m: 2.314, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.02525
[1mStep[0m  [4/42], [94mLoss[0m : 2.16745
[1mStep[0m  [8/42], [94mLoss[0m : 2.33317
[1mStep[0m  [12/42], [94mLoss[0m : 2.28896
[1mStep[0m  [16/42], [94mLoss[0m : 2.18853
[1mStep[0m  [20/42], [94mLoss[0m : 2.47117
[1mStep[0m  [24/42], [94mLoss[0m : 2.35058
[1mStep[0m  [28/42], [94mLoss[0m : 2.27370
[1mStep[0m  [32/42], [94mLoss[0m : 2.14367
[1mStep[0m  [36/42], [94mLoss[0m : 2.36159
[1mStep[0m  [40/42], [94mLoss[0m : 2.40273

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.299, [92mTest[0m: 2.290, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39889
[1mStep[0m  [4/42], [94mLoss[0m : 2.24458
[1mStep[0m  [8/42], [94mLoss[0m : 2.37702
[1mStep[0m  [12/42], [94mLoss[0m : 2.39127
[1mStep[0m  [16/42], [94mLoss[0m : 2.21449
[1mStep[0m  [20/42], [94mLoss[0m : 2.35118
[1mStep[0m  [24/42], [94mLoss[0m : 2.25780
[1mStep[0m  [28/42], [94mLoss[0m : 2.22399
[1mStep[0m  [32/42], [94mLoss[0m : 2.33976
[1mStep[0m  [36/42], [94mLoss[0m : 2.30706
[1mStep[0m  [40/42], [94mLoss[0m : 2.46368

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.336, [92mTest[0m: 2.307, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38576
[1mStep[0m  [4/42], [94mLoss[0m : 2.22594
[1mStep[0m  [8/42], [94mLoss[0m : 2.24226
[1mStep[0m  [12/42], [94mLoss[0m : 2.46267
[1mStep[0m  [16/42], [94mLoss[0m : 2.28333
[1mStep[0m  [20/42], [94mLoss[0m : 2.42077
[1mStep[0m  [24/42], [94mLoss[0m : 2.50095
[1mStep[0m  [28/42], [94mLoss[0m : 2.33857
[1mStep[0m  [32/42], [94mLoss[0m : 2.26430
[1mStep[0m  [36/42], [94mLoss[0m : 2.45518
[1mStep[0m  [40/42], [94mLoss[0m : 2.16284

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.308, [92mTest[0m: 2.301, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29210
[1mStep[0m  [4/42], [94mLoss[0m : 2.42254
[1mStep[0m  [8/42], [94mLoss[0m : 2.18037
[1mStep[0m  [12/42], [94mLoss[0m : 2.37840
[1mStep[0m  [16/42], [94mLoss[0m : 2.35673
[1mStep[0m  [20/42], [94mLoss[0m : 2.38197
[1mStep[0m  [24/42], [94mLoss[0m : 2.35282
[1mStep[0m  [28/42], [94mLoss[0m : 2.24181
[1mStep[0m  [32/42], [94mLoss[0m : 2.34393
[1mStep[0m  [36/42], [94mLoss[0m : 2.49616
[1mStep[0m  [40/42], [94mLoss[0m : 2.30916

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.311, [92mTest[0m: 2.302, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24591
[1mStep[0m  [4/42], [94mLoss[0m : 2.31530
[1mStep[0m  [8/42], [94mLoss[0m : 2.42779
[1mStep[0m  [12/42], [94mLoss[0m : 2.28076
[1mStep[0m  [16/42], [94mLoss[0m : 2.22365
[1mStep[0m  [20/42], [94mLoss[0m : 2.35928
[1mStep[0m  [24/42], [94mLoss[0m : 2.52367
[1mStep[0m  [28/42], [94mLoss[0m : 2.48775
[1mStep[0m  [32/42], [94mLoss[0m : 2.44021
[1mStep[0m  [36/42], [94mLoss[0m : 2.22342
[1mStep[0m  [40/42], [94mLoss[0m : 2.39937

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.314, [92mTest[0m: 2.306, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.19787
[1mStep[0m  [4/42], [94mLoss[0m : 2.35078
[1mStep[0m  [8/42], [94mLoss[0m : 2.26398
[1mStep[0m  [12/42], [94mLoss[0m : 2.26893
[1mStep[0m  [16/42], [94mLoss[0m : 2.25016
[1mStep[0m  [20/42], [94mLoss[0m : 2.53976
[1mStep[0m  [24/42], [94mLoss[0m : 2.37974
[1mStep[0m  [28/42], [94mLoss[0m : 2.09615
[1mStep[0m  [32/42], [94mLoss[0m : 2.36176
[1mStep[0m  [36/42], [94mLoss[0m : 2.33916
[1mStep[0m  [40/42], [94mLoss[0m : 2.26137

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.300, [92mTest[0m: 2.293, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34442
[1mStep[0m  [4/42], [94mLoss[0m : 2.36525
[1mStep[0m  [8/42], [94mLoss[0m : 2.19292
[1mStep[0m  [12/42], [94mLoss[0m : 2.18311
[1mStep[0m  [16/42], [94mLoss[0m : 2.17868
[1mStep[0m  [20/42], [94mLoss[0m : 2.12711
[1mStep[0m  [24/42], [94mLoss[0m : 2.21046
[1mStep[0m  [28/42], [94mLoss[0m : 2.28580
[1mStep[0m  [32/42], [94mLoss[0m : 2.34022
[1mStep[0m  [36/42], [94mLoss[0m : 2.10812
[1mStep[0m  [40/42], [94mLoss[0m : 2.42509

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.305, [92mTest[0m: 2.304, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.10699
[1mStep[0m  [4/42], [94mLoss[0m : 2.19718
[1mStep[0m  [8/42], [94mLoss[0m : 2.31328
[1mStep[0m  [12/42], [94mLoss[0m : 2.00476
[1mStep[0m  [16/42], [94mLoss[0m : 2.25232
[1mStep[0m  [20/42], [94mLoss[0m : 2.24112
[1mStep[0m  [24/42], [94mLoss[0m : 2.14975
[1mStep[0m  [28/42], [94mLoss[0m : 2.31376
[1mStep[0m  [32/42], [94mLoss[0m : 2.55789
[1mStep[0m  [36/42], [94mLoss[0m : 2.42787
[1mStep[0m  [40/42], [94mLoss[0m : 2.27288

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.301, [92mTest[0m: 2.281, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21486
[1mStep[0m  [4/42], [94mLoss[0m : 2.31226
[1mStep[0m  [8/42], [94mLoss[0m : 2.33243
[1mStep[0m  [12/42], [94mLoss[0m : 2.29373
[1mStep[0m  [16/42], [94mLoss[0m : 2.36013
[1mStep[0m  [20/42], [94mLoss[0m : 2.34589
[1mStep[0m  [24/42], [94mLoss[0m : 2.33275
[1mStep[0m  [28/42], [94mLoss[0m : 2.37857
[1mStep[0m  [32/42], [94mLoss[0m : 2.44006
[1mStep[0m  [36/42], [94mLoss[0m : 2.20988
[1mStep[0m  [40/42], [94mLoss[0m : 2.36855

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.276, [92mTest[0m: 2.289, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34488
[1mStep[0m  [4/42], [94mLoss[0m : 2.17290
[1mStep[0m  [8/42], [94mLoss[0m : 2.26970
[1mStep[0m  [12/42], [94mLoss[0m : 2.38457
[1mStep[0m  [16/42], [94mLoss[0m : 2.40571
[1mStep[0m  [20/42], [94mLoss[0m : 2.33128
[1mStep[0m  [24/42], [94mLoss[0m : 2.48641
[1mStep[0m  [28/42], [94mLoss[0m : 2.19315
[1mStep[0m  [32/42], [94mLoss[0m : 2.36364
[1mStep[0m  [36/42], [94mLoss[0m : 2.47786
[1mStep[0m  [40/42], [94mLoss[0m : 2.17628

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.276, [92mTest[0m: 2.286, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28166
[1mStep[0m  [4/42], [94mLoss[0m : 2.08741
[1mStep[0m  [8/42], [94mLoss[0m : 2.16753
[1mStep[0m  [12/42], [94mLoss[0m : 2.27460
[1mStep[0m  [16/42], [94mLoss[0m : 2.22253
[1mStep[0m  [20/42], [94mLoss[0m : 2.16630
[1mStep[0m  [24/42], [94mLoss[0m : 2.46503
[1mStep[0m  [28/42], [94mLoss[0m : 2.29702
[1mStep[0m  [32/42], [94mLoss[0m : 2.23624
[1mStep[0m  [36/42], [94mLoss[0m : 2.28331
[1mStep[0m  [40/42], [94mLoss[0m : 2.16759

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.273, [92mTest[0m: 2.289, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.17792
[1mStep[0m  [4/42], [94mLoss[0m : 2.28297
[1mStep[0m  [8/42], [94mLoss[0m : 2.14377
[1mStep[0m  [12/42], [94mLoss[0m : 2.10701
[1mStep[0m  [16/42], [94mLoss[0m : 2.28691
[1mStep[0m  [20/42], [94mLoss[0m : 2.21702
[1mStep[0m  [24/42], [94mLoss[0m : 2.14732
[1mStep[0m  [28/42], [94mLoss[0m : 2.08019
[1mStep[0m  [32/42], [94mLoss[0m : 2.18607
[1mStep[0m  [36/42], [94mLoss[0m : 2.28815
[1mStep[0m  [40/42], [94mLoss[0m : 2.18224

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.280, [92mTest[0m: 2.293, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.293
====================================

Phase 1 - Evaluation MAE:  2.2931152071271623
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 2.23732
[1mStep[0m  [4/42], [94mLoss[0m : 2.44212
[1mStep[0m  [8/42], [94mLoss[0m : 2.39557
[1mStep[0m  [12/42], [94mLoss[0m : 2.47371
[1mStep[0m  [16/42], [94mLoss[0m : 2.66794
[1mStep[0m  [20/42], [94mLoss[0m : 2.58798
[1mStep[0m  [24/42], [94mLoss[0m : 2.48531
[1mStep[0m  [28/42], [94mLoss[0m : 2.33457
[1mStep[0m  [32/42], [94mLoss[0m : 2.53335
[1mStep[0m  [36/42], [94mLoss[0m : 2.47896
[1mStep[0m  [40/42], [94mLoss[0m : 2.78107

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.288, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28229
[1mStep[0m  [4/42], [94mLoss[0m : 2.35420
[1mStep[0m  [8/42], [94mLoss[0m : 2.17120
[1mStep[0m  [12/42], [94mLoss[0m : 2.13366
[1mStep[0m  [16/42], [94mLoss[0m : 2.27447
[1mStep[0m  [20/42], [94mLoss[0m : 2.22615
[1mStep[0m  [24/42], [94mLoss[0m : 2.17432
[1mStep[0m  [28/42], [94mLoss[0m : 2.42645
[1mStep[0m  [32/42], [94mLoss[0m : 2.16063
[1mStep[0m  [36/42], [94mLoss[0m : 2.42836
[1mStep[0m  [40/42], [94mLoss[0m : 2.35852

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.296, [92mTest[0m: 2.346, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.09958
[1mStep[0m  [4/42], [94mLoss[0m : 2.05028
[1mStep[0m  [8/42], [94mLoss[0m : 2.20357
[1mStep[0m  [12/42], [94mLoss[0m : 2.36090
[1mStep[0m  [16/42], [94mLoss[0m : 2.24977
[1mStep[0m  [20/42], [94mLoss[0m : 2.28874
[1mStep[0m  [24/42], [94mLoss[0m : 2.08634
[1mStep[0m  [28/42], [94mLoss[0m : 2.11456
[1mStep[0m  [32/42], [94mLoss[0m : 2.10267
[1mStep[0m  [36/42], [94mLoss[0m : 2.19305
[1mStep[0m  [40/42], [94mLoss[0m : 2.16273

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.164, [92mTest[0m: 2.354, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.01001
[1mStep[0m  [4/42], [94mLoss[0m : 2.07201
[1mStep[0m  [8/42], [94mLoss[0m : 2.18057
[1mStep[0m  [12/42], [94mLoss[0m : 1.93327
[1mStep[0m  [16/42], [94mLoss[0m : 2.28452
[1mStep[0m  [20/42], [94mLoss[0m : 2.15235
[1mStep[0m  [24/42], [94mLoss[0m : 2.09783
[1mStep[0m  [28/42], [94mLoss[0m : 1.92571
[1mStep[0m  [32/42], [94mLoss[0m : 2.16108
[1mStep[0m  [36/42], [94mLoss[0m : 2.21325
[1mStep[0m  [40/42], [94mLoss[0m : 2.31760

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.109, [92mTest[0m: 2.486, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.19034
[1mStep[0m  [4/42], [94mLoss[0m : 2.11931
[1mStep[0m  [8/42], [94mLoss[0m : 1.68836
[1mStep[0m  [12/42], [94mLoss[0m : 2.02303
[1mStep[0m  [16/42], [94mLoss[0m : 2.00480
[1mStep[0m  [20/42], [94mLoss[0m : 1.79550
[1mStep[0m  [24/42], [94mLoss[0m : 1.90744
[1mStep[0m  [28/42], [94mLoss[0m : 1.92623
[1mStep[0m  [32/42], [94mLoss[0m : 2.23118
[1mStep[0m  [36/42], [94mLoss[0m : 2.05584
[1mStep[0m  [40/42], [94mLoss[0m : 2.06385

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 1.990, [92mTest[0m: 2.378, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.84667
[1mStep[0m  [4/42], [94mLoss[0m : 1.67785
[1mStep[0m  [8/42], [94mLoss[0m : 1.73408
[1mStep[0m  [12/42], [94mLoss[0m : 1.96239
[1mStep[0m  [16/42], [94mLoss[0m : 1.92485
[1mStep[0m  [20/42], [94mLoss[0m : 2.06250
[1mStep[0m  [24/42], [94mLoss[0m : 1.92445
[1mStep[0m  [28/42], [94mLoss[0m : 1.98742
[1mStep[0m  [32/42], [94mLoss[0m : 2.04470
[1mStep[0m  [36/42], [94mLoss[0m : 2.00685
[1mStep[0m  [40/42], [94mLoss[0m : 1.91405

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 1.931, [92mTest[0m: 2.388, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.83032
[1mStep[0m  [4/42], [94mLoss[0m : 1.93288
[1mStep[0m  [8/42], [94mLoss[0m : 1.71370
[1mStep[0m  [12/42], [94mLoss[0m : 1.90110
[1mStep[0m  [16/42], [94mLoss[0m : 1.78899
[1mStep[0m  [20/42], [94mLoss[0m : 1.89706
[1mStep[0m  [24/42], [94mLoss[0m : 1.80702
[1mStep[0m  [28/42], [94mLoss[0m : 1.80418
[1mStep[0m  [32/42], [94mLoss[0m : 1.81718
[1mStep[0m  [36/42], [94mLoss[0m : 1.75937
[1mStep[0m  [40/42], [94mLoss[0m : 1.87995

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.844, [92mTest[0m: 2.415, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.61122
[1mStep[0m  [4/42], [94mLoss[0m : 1.66485
[1mStep[0m  [8/42], [94mLoss[0m : 1.63878
[1mStep[0m  [12/42], [94mLoss[0m : 1.91214
[1mStep[0m  [16/42], [94mLoss[0m : 1.82272
[1mStep[0m  [20/42], [94mLoss[0m : 1.66651
[1mStep[0m  [24/42], [94mLoss[0m : 1.77572
[1mStep[0m  [28/42], [94mLoss[0m : 2.00894
[1mStep[0m  [32/42], [94mLoss[0m : 1.61342
[1mStep[0m  [36/42], [94mLoss[0m : 1.88415
[1mStep[0m  [40/42], [94mLoss[0m : 1.87381

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.779, [92mTest[0m: 2.448, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.56392
[1mStep[0m  [4/42], [94mLoss[0m : 1.81266
[1mStep[0m  [8/42], [94mLoss[0m : 1.75777
[1mStep[0m  [12/42], [94mLoss[0m : 1.65687
[1mStep[0m  [16/42], [94mLoss[0m : 1.67114
[1mStep[0m  [20/42], [94mLoss[0m : 1.78751
[1mStep[0m  [24/42], [94mLoss[0m : 1.79582
[1mStep[0m  [28/42], [94mLoss[0m : 1.76309
[1mStep[0m  [32/42], [94mLoss[0m : 1.95807
[1mStep[0m  [36/42], [94mLoss[0m : 1.83542
[1mStep[0m  [40/42], [94mLoss[0m : 1.82302

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.734, [92mTest[0m: 2.428, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.76009
[1mStep[0m  [4/42], [94mLoss[0m : 1.57645
[1mStep[0m  [8/42], [94mLoss[0m : 1.61633
[1mStep[0m  [12/42], [94mLoss[0m : 1.69143
[1mStep[0m  [16/42], [94mLoss[0m : 1.66855
[1mStep[0m  [20/42], [94mLoss[0m : 1.54150
[1mStep[0m  [24/42], [94mLoss[0m : 1.67842
[1mStep[0m  [28/42], [94mLoss[0m : 1.67298
[1mStep[0m  [32/42], [94mLoss[0m : 1.77541
[1mStep[0m  [36/42], [94mLoss[0m : 1.85639
[1mStep[0m  [40/42], [94mLoss[0m : 1.71650

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.677, [92mTest[0m: 2.449, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.54115
[1mStep[0m  [4/42], [94mLoss[0m : 1.59954
[1mStep[0m  [8/42], [94mLoss[0m : 1.68565
[1mStep[0m  [12/42], [94mLoss[0m : 1.51005
[1mStep[0m  [16/42], [94mLoss[0m : 1.72776
[1mStep[0m  [20/42], [94mLoss[0m : 1.55723
[1mStep[0m  [24/42], [94mLoss[0m : 1.76478
[1mStep[0m  [28/42], [94mLoss[0m : 1.82735
[1mStep[0m  [32/42], [94mLoss[0m : 1.64939
[1mStep[0m  [36/42], [94mLoss[0m : 1.59220
[1mStep[0m  [40/42], [94mLoss[0m : 1.77448

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.626, [92mTest[0m: 2.515, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.48099
[1mStep[0m  [4/42], [94mLoss[0m : 1.52099
[1mStep[0m  [8/42], [94mLoss[0m : 1.67168
[1mStep[0m  [12/42], [94mLoss[0m : 1.59182
[1mStep[0m  [16/42], [94mLoss[0m : 1.55465
[1mStep[0m  [20/42], [94mLoss[0m : 1.44462
[1mStep[0m  [24/42], [94mLoss[0m : 1.86537
[1mStep[0m  [28/42], [94mLoss[0m : 1.62019
[1mStep[0m  [32/42], [94mLoss[0m : 1.74154
[1mStep[0m  [36/42], [94mLoss[0m : 1.51312
[1mStep[0m  [40/42], [94mLoss[0m : 1.74635

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.587, [92mTest[0m: 2.441, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.60487
[1mStep[0m  [4/42], [94mLoss[0m : 1.59566
[1mStep[0m  [8/42], [94mLoss[0m : 1.51816
[1mStep[0m  [12/42], [94mLoss[0m : 1.63722
[1mStep[0m  [16/42], [94mLoss[0m : 1.54587
[1mStep[0m  [20/42], [94mLoss[0m : 1.46718
[1mStep[0m  [24/42], [94mLoss[0m : 1.65068
[1mStep[0m  [28/42], [94mLoss[0m : 1.61865
[1mStep[0m  [32/42], [94mLoss[0m : 1.71160
[1mStep[0m  [36/42], [94mLoss[0m : 1.49651
[1mStep[0m  [40/42], [94mLoss[0m : 1.53170

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.546, [92mTest[0m: 2.438, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.48504
[1mStep[0m  [4/42], [94mLoss[0m : 1.24014
[1mStep[0m  [8/42], [94mLoss[0m : 1.49394
[1mStep[0m  [12/42], [94mLoss[0m : 1.54835
[1mStep[0m  [16/42], [94mLoss[0m : 1.35152
[1mStep[0m  [20/42], [94mLoss[0m : 1.53440
[1mStep[0m  [24/42], [94mLoss[0m : 1.55753
[1mStep[0m  [28/42], [94mLoss[0m : 1.59671
[1mStep[0m  [32/42], [94mLoss[0m : 1.49714
[1mStep[0m  [36/42], [94mLoss[0m : 1.49195
[1mStep[0m  [40/42], [94mLoss[0m : 1.42952

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.484, [92mTest[0m: 2.458, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.31543
[1mStep[0m  [4/42], [94mLoss[0m : 1.29524
[1mStep[0m  [8/42], [94mLoss[0m : 1.49549
[1mStep[0m  [12/42], [94mLoss[0m : 1.42314
[1mStep[0m  [16/42], [94mLoss[0m : 1.47040
[1mStep[0m  [20/42], [94mLoss[0m : 1.45517
[1mStep[0m  [24/42], [94mLoss[0m : 1.47915
[1mStep[0m  [28/42], [94mLoss[0m : 1.47968
[1mStep[0m  [32/42], [94mLoss[0m : 1.45912
[1mStep[0m  [36/42], [94mLoss[0m : 1.43030
[1mStep[0m  [40/42], [94mLoss[0m : 1.59958

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.477, [92mTest[0m: 2.448, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.31258
[1mStep[0m  [4/42], [94mLoss[0m : 1.45199
[1mStep[0m  [8/42], [94mLoss[0m : 1.50623
[1mStep[0m  [12/42], [94mLoss[0m : 1.45971
[1mStep[0m  [16/42], [94mLoss[0m : 1.44914
[1mStep[0m  [20/42], [94mLoss[0m : 1.44641
[1mStep[0m  [24/42], [94mLoss[0m : 1.34310
[1mStep[0m  [28/42], [94mLoss[0m : 1.41513
[1mStep[0m  [32/42], [94mLoss[0m : 1.48783
[1mStep[0m  [36/42], [94mLoss[0m : 1.45020
[1mStep[0m  [40/42], [94mLoss[0m : 1.43907

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.440, [92mTest[0m: 2.450, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.47287
[1mStep[0m  [4/42], [94mLoss[0m : 1.43783
[1mStep[0m  [8/42], [94mLoss[0m : 1.42719
[1mStep[0m  [12/42], [94mLoss[0m : 1.25714
[1mStep[0m  [16/42], [94mLoss[0m : 1.49319
[1mStep[0m  [20/42], [94mLoss[0m : 1.22726
[1mStep[0m  [24/42], [94mLoss[0m : 1.35375
[1mStep[0m  [28/42], [94mLoss[0m : 1.49724
[1mStep[0m  [32/42], [94mLoss[0m : 1.36133
[1mStep[0m  [36/42], [94mLoss[0m : 1.39849
[1mStep[0m  [40/42], [94mLoss[0m : 1.44395

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.406, [92mTest[0m: 2.450, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.32106
[1mStep[0m  [4/42], [94mLoss[0m : 1.33155
[1mStep[0m  [8/42], [94mLoss[0m : 1.42853
[1mStep[0m  [12/42], [94mLoss[0m : 1.43062
[1mStep[0m  [16/42], [94mLoss[0m : 1.45238
[1mStep[0m  [20/42], [94mLoss[0m : 1.39780
[1mStep[0m  [24/42], [94mLoss[0m : 1.39013
[1mStep[0m  [28/42], [94mLoss[0m : 1.48889
[1mStep[0m  [32/42], [94mLoss[0m : 1.36844
[1mStep[0m  [36/42], [94mLoss[0m : 1.46029
[1mStep[0m  [40/42], [94mLoss[0m : 1.41425

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.384, [92mTest[0m: 2.466, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.47655
[1mStep[0m  [4/42], [94mLoss[0m : 1.38613
[1mStep[0m  [8/42], [94mLoss[0m : 1.30118
[1mStep[0m  [12/42], [94mLoss[0m : 1.35074
[1mStep[0m  [16/42], [94mLoss[0m : 1.43327
[1mStep[0m  [20/42], [94mLoss[0m : 1.19349
[1mStep[0m  [24/42], [94mLoss[0m : 1.38909
[1mStep[0m  [28/42], [94mLoss[0m : 1.45928
[1mStep[0m  [32/42], [94mLoss[0m : 1.51342
[1mStep[0m  [36/42], [94mLoss[0m : 1.46830
[1mStep[0m  [40/42], [94mLoss[0m : 1.32415

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.353, [92mTest[0m: 2.492, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.55434
[1mStep[0m  [4/42], [94mLoss[0m : 1.25911
[1mStep[0m  [8/42], [94mLoss[0m : 1.32995
[1mStep[0m  [12/42], [94mLoss[0m : 1.31230
[1mStep[0m  [16/42], [94mLoss[0m : 1.32143
[1mStep[0m  [20/42], [94mLoss[0m : 1.40844
[1mStep[0m  [24/42], [94mLoss[0m : 1.45321
[1mStep[0m  [28/42], [94mLoss[0m : 1.38675
[1mStep[0m  [32/42], [94mLoss[0m : 1.34021
[1mStep[0m  [36/42], [94mLoss[0m : 1.31356
[1mStep[0m  [40/42], [94mLoss[0m : 1.29458

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.320, [92mTest[0m: 2.473, [96mlr[0m: 0.006772500000000002
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 19 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.457
====================================

Phase 2 - Evaluation MAE:  2.4567637613841464
MAE score P1      2.293115
MAE score P2      2.456764
loss              1.320459
learning_rate     0.007525
batch_size             256
hidden_sizes         [100]
epochs                  30
activation            relu
optimizer              sgd
early stopping        True
dropout                0.2
momentum               0.9
weight_decay         0.001
Name: 18, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 11.67226
[1mStep[0m  [8/84], [94mLoss[0m : 10.37668
[1mStep[0m  [16/84], [94mLoss[0m : 8.19708
[1mStep[0m  [24/84], [94mLoss[0m : 7.04561
[1mStep[0m  [32/84], [94mLoss[0m : 5.43018
[1mStep[0m  [40/84], [94mLoss[0m : 3.70242
[1mStep[0m  [48/84], [94mLoss[0m : 3.35181
[1mStep[0m  [56/84], [94mLoss[0m : 2.70805
[1mStep[0m  [64/84], [94mLoss[0m : 2.68513
[1mStep[0m  [72/84], [94mLoss[0m : 2.76150
[1mStep[0m  [80/84], [94mLoss[0m : 3.05304

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 5.288, [92mTest[0m: 11.442, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.96532
[1mStep[0m  [8/84], [94mLoss[0m : 3.07955
[1mStep[0m  [16/84], [94mLoss[0m : 2.83187
[1mStep[0m  [24/84], [94mLoss[0m : 2.80939
[1mStep[0m  [32/84], [94mLoss[0m : 2.61634
[1mStep[0m  [40/84], [94mLoss[0m : 2.74993
[1mStep[0m  [48/84], [94mLoss[0m : 2.97691
[1mStep[0m  [56/84], [94mLoss[0m : 2.77619
[1mStep[0m  [64/84], [94mLoss[0m : 2.72875
[1mStep[0m  [72/84], [94mLoss[0m : 2.74130
[1mStep[0m  [80/84], [94mLoss[0m : 2.72001

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.717, [92mTest[0m: 2.487, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.56575
[1mStep[0m  [8/84], [94mLoss[0m : 2.70732
[1mStep[0m  [16/84], [94mLoss[0m : 2.69170
[1mStep[0m  [24/84], [94mLoss[0m : 2.52494
[1mStep[0m  [32/84], [94mLoss[0m : 2.72685
[1mStep[0m  [40/84], [94mLoss[0m : 2.69572
[1mStep[0m  [48/84], [94mLoss[0m : 2.72752
[1mStep[0m  [56/84], [94mLoss[0m : 2.68352
[1mStep[0m  [64/84], [94mLoss[0m : 2.73834
[1mStep[0m  [72/84], [94mLoss[0m : 2.89049
[1mStep[0m  [80/84], [94mLoss[0m : 2.86087

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.676, [92mTest[0m: 2.405, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50307
[1mStep[0m  [8/84], [94mLoss[0m : 2.41025
[1mStep[0m  [16/84], [94mLoss[0m : 2.58999
[1mStep[0m  [24/84], [94mLoss[0m : 2.59463
[1mStep[0m  [32/84], [94mLoss[0m : 2.71040
[1mStep[0m  [40/84], [94mLoss[0m : 2.68441
[1mStep[0m  [48/84], [94mLoss[0m : 2.48532
[1mStep[0m  [56/84], [94mLoss[0m : 2.54166
[1mStep[0m  [64/84], [94mLoss[0m : 2.69511
[1mStep[0m  [72/84], [94mLoss[0m : 2.25855
[1mStep[0m  [80/84], [94mLoss[0m : 2.46131

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.655, [92mTest[0m: 2.391, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.62894
[1mStep[0m  [8/84], [94mLoss[0m : 2.87524
[1mStep[0m  [16/84], [94mLoss[0m : 2.39305
[1mStep[0m  [24/84], [94mLoss[0m : 2.67880
[1mStep[0m  [32/84], [94mLoss[0m : 2.79796
[1mStep[0m  [40/84], [94mLoss[0m : 2.68067
[1mStep[0m  [48/84], [94mLoss[0m : 2.75432
[1mStep[0m  [56/84], [94mLoss[0m : 2.70160
[1mStep[0m  [64/84], [94mLoss[0m : 2.56471
[1mStep[0m  [72/84], [94mLoss[0m : 2.44747
[1mStep[0m  [80/84], [94mLoss[0m : 2.56288

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.664, [92mTest[0m: 2.371, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49080
[1mStep[0m  [8/84], [94mLoss[0m : 2.90268
[1mStep[0m  [16/84], [94mLoss[0m : 2.31263
[1mStep[0m  [24/84], [94mLoss[0m : 2.66855
[1mStep[0m  [32/84], [94mLoss[0m : 2.58971
[1mStep[0m  [40/84], [94mLoss[0m : 2.75895
[1mStep[0m  [48/84], [94mLoss[0m : 2.40051
[1mStep[0m  [56/84], [94mLoss[0m : 2.81215
[1mStep[0m  [64/84], [94mLoss[0m : 2.68560
[1mStep[0m  [72/84], [94mLoss[0m : 2.56017
[1mStep[0m  [80/84], [94mLoss[0m : 2.99822

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.651, [92mTest[0m: 2.369, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.63637
[1mStep[0m  [8/84], [94mLoss[0m : 2.81438
[1mStep[0m  [16/84], [94mLoss[0m : 2.83855
[1mStep[0m  [24/84], [94mLoss[0m : 2.71290
[1mStep[0m  [32/84], [94mLoss[0m : 2.56488
[1mStep[0m  [40/84], [94mLoss[0m : 2.60090
[1mStep[0m  [48/84], [94mLoss[0m : 2.75424
[1mStep[0m  [56/84], [94mLoss[0m : 2.65482
[1mStep[0m  [64/84], [94mLoss[0m : 2.65174
[1mStep[0m  [72/84], [94mLoss[0m : 2.23164
[1mStep[0m  [80/84], [94mLoss[0m : 2.93509

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.625, [92mTest[0m: 2.367, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54446
[1mStep[0m  [8/84], [94mLoss[0m : 2.34008
[1mStep[0m  [16/84], [94mLoss[0m : 2.97852
[1mStep[0m  [24/84], [94mLoss[0m : 2.59832
[1mStep[0m  [32/84], [94mLoss[0m : 2.63178
[1mStep[0m  [40/84], [94mLoss[0m : 2.49488
[1mStep[0m  [48/84], [94mLoss[0m : 2.63756
[1mStep[0m  [56/84], [94mLoss[0m : 2.79790
[1mStep[0m  [64/84], [94mLoss[0m : 2.64335
[1mStep[0m  [72/84], [94mLoss[0m : 2.85797
[1mStep[0m  [80/84], [94mLoss[0m : 2.54794

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.622, [92mTest[0m: 2.355, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.62189
[1mStep[0m  [8/84], [94mLoss[0m : 2.74596
[1mStep[0m  [16/84], [94mLoss[0m : 2.57595
[1mStep[0m  [24/84], [94mLoss[0m : 2.63864
[1mStep[0m  [32/84], [94mLoss[0m : 2.46861
[1mStep[0m  [40/84], [94mLoss[0m : 2.50269
[1mStep[0m  [48/84], [94mLoss[0m : 2.66998
[1mStep[0m  [56/84], [94mLoss[0m : 2.64046
[1mStep[0m  [64/84], [94mLoss[0m : 2.74207
[1mStep[0m  [72/84], [94mLoss[0m : 2.65804
[1mStep[0m  [80/84], [94mLoss[0m : 2.59610

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.622, [92mTest[0m: 2.356, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59767
[1mStep[0m  [8/84], [94mLoss[0m : 2.75516
[1mStep[0m  [16/84], [94mLoss[0m : 2.83921
[1mStep[0m  [24/84], [94mLoss[0m : 2.56314
[1mStep[0m  [32/84], [94mLoss[0m : 2.45241
[1mStep[0m  [40/84], [94mLoss[0m : 2.63185
[1mStep[0m  [48/84], [94mLoss[0m : 2.78906
[1mStep[0m  [56/84], [94mLoss[0m : 2.55871
[1mStep[0m  [64/84], [94mLoss[0m : 2.34459
[1mStep[0m  [72/84], [94mLoss[0m : 2.56377
[1mStep[0m  [80/84], [94mLoss[0m : 2.85087

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.591, [92mTest[0m: 2.341, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42843
[1mStep[0m  [8/84], [94mLoss[0m : 2.58432
[1mStep[0m  [16/84], [94mLoss[0m : 2.70545
[1mStep[0m  [24/84], [94mLoss[0m : 2.83860
[1mStep[0m  [32/84], [94mLoss[0m : 2.59188
[1mStep[0m  [40/84], [94mLoss[0m : 2.51394
[1mStep[0m  [48/84], [94mLoss[0m : 2.81844
[1mStep[0m  [56/84], [94mLoss[0m : 2.30936
[1mStep[0m  [64/84], [94mLoss[0m : 2.53293
[1mStep[0m  [72/84], [94mLoss[0m : 2.46682
[1mStep[0m  [80/84], [94mLoss[0m : 2.80938

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.610, [92mTest[0m: 2.349, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.65753
[1mStep[0m  [8/84], [94mLoss[0m : 2.64828
[1mStep[0m  [16/84], [94mLoss[0m : 2.39610
[1mStep[0m  [24/84], [94mLoss[0m : 2.76207
[1mStep[0m  [32/84], [94mLoss[0m : 2.68721
[1mStep[0m  [40/84], [94mLoss[0m : 2.51542
[1mStep[0m  [48/84], [94mLoss[0m : 2.39883
[1mStep[0m  [56/84], [94mLoss[0m : 2.77255
[1mStep[0m  [64/84], [94mLoss[0m : 2.57905
[1mStep[0m  [72/84], [94mLoss[0m : 2.39173
[1mStep[0m  [80/84], [94mLoss[0m : 2.53563

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.593, [92mTest[0m: 2.352, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.57524
[1mStep[0m  [8/84], [94mLoss[0m : 2.66608
[1mStep[0m  [16/84], [94mLoss[0m : 2.58752
[1mStep[0m  [24/84], [94mLoss[0m : 2.30848
[1mStep[0m  [32/84], [94mLoss[0m : 2.73728
[1mStep[0m  [40/84], [94mLoss[0m : 2.55670
[1mStep[0m  [48/84], [94mLoss[0m : 2.72465
[1mStep[0m  [56/84], [94mLoss[0m : 2.75110
[1mStep[0m  [64/84], [94mLoss[0m : 2.78529
[1mStep[0m  [72/84], [94mLoss[0m : 2.48505
[1mStep[0m  [80/84], [94mLoss[0m : 2.54113

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.594, [92mTest[0m: 2.344, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.91670
[1mStep[0m  [8/84], [94mLoss[0m : 2.77603
[1mStep[0m  [16/84], [94mLoss[0m : 2.75717
[1mStep[0m  [24/84], [94mLoss[0m : 2.70490
[1mStep[0m  [32/84], [94mLoss[0m : 2.62244
[1mStep[0m  [40/84], [94mLoss[0m : 2.47879
[1mStep[0m  [48/84], [94mLoss[0m : 2.49905
[1mStep[0m  [56/84], [94mLoss[0m : 2.51893
[1mStep[0m  [64/84], [94mLoss[0m : 2.70190
[1mStep[0m  [72/84], [94mLoss[0m : 2.85488
[1mStep[0m  [80/84], [94mLoss[0m : 2.60911

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.602, [92mTest[0m: 2.342, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.88640
[1mStep[0m  [8/84], [94mLoss[0m : 2.56275
[1mStep[0m  [16/84], [94mLoss[0m : 2.69268
[1mStep[0m  [24/84], [94mLoss[0m : 2.76338
[1mStep[0m  [32/84], [94mLoss[0m : 2.46344
[1mStep[0m  [40/84], [94mLoss[0m : 2.81874
[1mStep[0m  [48/84], [94mLoss[0m : 2.93633
[1mStep[0m  [56/84], [94mLoss[0m : 2.49106
[1mStep[0m  [64/84], [94mLoss[0m : 2.44401
[1mStep[0m  [72/84], [94mLoss[0m : 2.60350
[1mStep[0m  [80/84], [94mLoss[0m : 2.85399

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.582, [92mTest[0m: 2.339, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.62174
[1mStep[0m  [8/84], [94mLoss[0m : 2.46044
[1mStep[0m  [16/84], [94mLoss[0m : 2.67413
[1mStep[0m  [24/84], [94mLoss[0m : 2.26979
[1mStep[0m  [32/84], [94mLoss[0m : 2.27449
[1mStep[0m  [40/84], [94mLoss[0m : 2.78131
[1mStep[0m  [48/84], [94mLoss[0m : 2.56833
[1mStep[0m  [56/84], [94mLoss[0m : 2.62370
[1mStep[0m  [64/84], [94mLoss[0m : 2.32802
[1mStep[0m  [72/84], [94mLoss[0m : 2.68401
[1mStep[0m  [80/84], [94mLoss[0m : 2.94349

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.613, [92mTest[0m: 2.343, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.72582
[1mStep[0m  [8/84], [94mLoss[0m : 2.56180
[1mStep[0m  [16/84], [94mLoss[0m : 2.71187
[1mStep[0m  [24/84], [94mLoss[0m : 2.59198
[1mStep[0m  [32/84], [94mLoss[0m : 2.44702
[1mStep[0m  [40/84], [94mLoss[0m : 2.76507
[1mStep[0m  [48/84], [94mLoss[0m : 2.84387
[1mStep[0m  [56/84], [94mLoss[0m : 2.92647
[1mStep[0m  [64/84], [94mLoss[0m : 2.12587
[1mStep[0m  [72/84], [94mLoss[0m : 2.97615
[1mStep[0m  [80/84], [94mLoss[0m : 2.56249

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.608, [92mTest[0m: 2.340, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43878
[1mStep[0m  [8/84], [94mLoss[0m : 2.22277
[1mStep[0m  [16/84], [94mLoss[0m : 2.53576
[1mStep[0m  [24/84], [94mLoss[0m : 2.64301
[1mStep[0m  [32/84], [94mLoss[0m : 2.36935
[1mStep[0m  [40/84], [94mLoss[0m : 2.68597
[1mStep[0m  [48/84], [94mLoss[0m : 2.47404
[1mStep[0m  [56/84], [94mLoss[0m : 2.68089
[1mStep[0m  [64/84], [94mLoss[0m : 2.84039
[1mStep[0m  [72/84], [94mLoss[0m : 2.70372
[1mStep[0m  [80/84], [94mLoss[0m : 2.41009

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.587, [92mTest[0m: 2.343, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.61242
[1mStep[0m  [8/84], [94mLoss[0m : 2.56993
[1mStep[0m  [16/84], [94mLoss[0m : 2.56558
[1mStep[0m  [24/84], [94mLoss[0m : 2.22110
[1mStep[0m  [32/84], [94mLoss[0m : 2.56663
[1mStep[0m  [40/84], [94mLoss[0m : 2.44477
[1mStep[0m  [48/84], [94mLoss[0m : 2.86186
[1mStep[0m  [56/84], [94mLoss[0m : 2.81315
[1mStep[0m  [64/84], [94mLoss[0m : 2.60467
[1mStep[0m  [72/84], [94mLoss[0m : 2.48890
[1mStep[0m  [80/84], [94mLoss[0m : 2.54819

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.565, [92mTest[0m: 2.342, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.63016
[1mStep[0m  [8/84], [94mLoss[0m : 2.60626
[1mStep[0m  [16/84], [94mLoss[0m : 2.53899
[1mStep[0m  [24/84], [94mLoss[0m : 2.50199
[1mStep[0m  [32/84], [94mLoss[0m : 2.46703
[1mStep[0m  [40/84], [94mLoss[0m : 2.70669
[1mStep[0m  [48/84], [94mLoss[0m : 2.60017
[1mStep[0m  [56/84], [94mLoss[0m : 2.51738
[1mStep[0m  [64/84], [94mLoss[0m : 2.36839
[1mStep[0m  [72/84], [94mLoss[0m : 2.38735
[1mStep[0m  [80/84], [94mLoss[0m : 2.76531

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.600, [92mTest[0m: 2.330, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.38601
[1mStep[0m  [8/84], [94mLoss[0m : 2.80813
[1mStep[0m  [16/84], [94mLoss[0m : 2.61725
[1mStep[0m  [24/84], [94mLoss[0m : 2.36022
[1mStep[0m  [32/84], [94mLoss[0m : 2.41036
[1mStep[0m  [40/84], [94mLoss[0m : 2.59881
[1mStep[0m  [48/84], [94mLoss[0m : 2.75618
[1mStep[0m  [56/84], [94mLoss[0m : 2.60734
[1mStep[0m  [64/84], [94mLoss[0m : 2.57968
[1mStep[0m  [72/84], [94mLoss[0m : 2.71149
[1mStep[0m  [80/84], [94mLoss[0m : 2.50512

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.582, [92mTest[0m: 2.331, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.69905
[1mStep[0m  [8/84], [94mLoss[0m : 2.71115
[1mStep[0m  [16/84], [94mLoss[0m : 2.74055
[1mStep[0m  [24/84], [94mLoss[0m : 2.76135
[1mStep[0m  [32/84], [94mLoss[0m : 2.35900
[1mStep[0m  [40/84], [94mLoss[0m : 2.71971
[1mStep[0m  [48/84], [94mLoss[0m : 2.42765
[1mStep[0m  [56/84], [94mLoss[0m : 2.53544
[1mStep[0m  [64/84], [94mLoss[0m : 2.61036
[1mStep[0m  [72/84], [94mLoss[0m : 2.72826
[1mStep[0m  [80/84], [94mLoss[0m : 2.82549

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.589, [92mTest[0m: 2.339, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.85260
[1mStep[0m  [8/84], [94mLoss[0m : 2.19356
[1mStep[0m  [16/84], [94mLoss[0m : 2.67417
[1mStep[0m  [24/84], [94mLoss[0m : 2.48477
[1mStep[0m  [32/84], [94mLoss[0m : 2.39589
[1mStep[0m  [40/84], [94mLoss[0m : 2.17253
[1mStep[0m  [48/84], [94mLoss[0m : 2.72352
[1mStep[0m  [56/84], [94mLoss[0m : 2.70066
[1mStep[0m  [64/84], [94mLoss[0m : 2.51007
[1mStep[0m  [72/84], [94mLoss[0m : 2.94470
[1mStep[0m  [80/84], [94mLoss[0m : 2.52642

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.574, [92mTest[0m: 2.337, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.51132
[1mStep[0m  [8/84], [94mLoss[0m : 2.43845
[1mStep[0m  [16/84], [94mLoss[0m : 2.61258
[1mStep[0m  [24/84], [94mLoss[0m : 2.96058
[1mStep[0m  [32/84], [94mLoss[0m : 2.40006
[1mStep[0m  [40/84], [94mLoss[0m : 2.63380
[1mStep[0m  [48/84], [94mLoss[0m : 2.76725
[1mStep[0m  [56/84], [94mLoss[0m : 2.88714
[1mStep[0m  [64/84], [94mLoss[0m : 2.28504
[1mStep[0m  [72/84], [94mLoss[0m : 2.56426
[1mStep[0m  [80/84], [94mLoss[0m : 2.36817

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.569, [92mTest[0m: 2.326, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48720
[1mStep[0m  [8/84], [94mLoss[0m : 2.62795
[1mStep[0m  [16/84], [94mLoss[0m : 2.56645
[1mStep[0m  [24/84], [94mLoss[0m : 2.88776
[1mStep[0m  [32/84], [94mLoss[0m : 2.49246
[1mStep[0m  [40/84], [94mLoss[0m : 2.86720
[1mStep[0m  [48/84], [94mLoss[0m : 2.38898
[1mStep[0m  [56/84], [94mLoss[0m : 2.71453
[1mStep[0m  [64/84], [94mLoss[0m : 2.69491
[1mStep[0m  [72/84], [94mLoss[0m : 2.61915
[1mStep[0m  [80/84], [94mLoss[0m : 2.53921

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.584, [92mTest[0m: 2.335, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43992
[1mStep[0m  [8/84], [94mLoss[0m : 2.54231
[1mStep[0m  [16/84], [94mLoss[0m : 2.45541
[1mStep[0m  [24/84], [94mLoss[0m : 2.32836
[1mStep[0m  [32/84], [94mLoss[0m : 2.33468
[1mStep[0m  [40/84], [94mLoss[0m : 2.45875
[1mStep[0m  [48/84], [94mLoss[0m : 2.43987
[1mStep[0m  [56/84], [94mLoss[0m : 2.47291
[1mStep[0m  [64/84], [94mLoss[0m : 2.62494
[1mStep[0m  [72/84], [94mLoss[0m : 2.62404
[1mStep[0m  [80/84], [94mLoss[0m : 2.46181

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.552, [92mTest[0m: 2.336, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.65749
[1mStep[0m  [8/84], [94mLoss[0m : 2.28471
[1mStep[0m  [16/84], [94mLoss[0m : 2.37543
[1mStep[0m  [24/84], [94mLoss[0m : 2.48612
[1mStep[0m  [32/84], [94mLoss[0m : 2.50525
[1mStep[0m  [40/84], [94mLoss[0m : 2.34629
[1mStep[0m  [48/84], [94mLoss[0m : 2.46362
[1mStep[0m  [56/84], [94mLoss[0m : 2.61273
[1mStep[0m  [64/84], [94mLoss[0m : 2.62359
[1mStep[0m  [72/84], [94mLoss[0m : 2.73430
[1mStep[0m  [80/84], [94mLoss[0m : 2.75553

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.570, [92mTest[0m: 2.330, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.76006
[1mStep[0m  [8/84], [94mLoss[0m : 2.10378
[1mStep[0m  [16/84], [94mLoss[0m : 2.50522
[1mStep[0m  [24/84], [94mLoss[0m : 2.47061
[1mStep[0m  [32/84], [94mLoss[0m : 2.43461
[1mStep[0m  [40/84], [94mLoss[0m : 2.85968
[1mStep[0m  [48/84], [94mLoss[0m : 2.76283
[1mStep[0m  [56/84], [94mLoss[0m : 2.84469
[1mStep[0m  [64/84], [94mLoss[0m : 2.44684
[1mStep[0m  [72/84], [94mLoss[0m : 2.39253
[1mStep[0m  [80/84], [94mLoss[0m : 2.44636

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.550, [92mTest[0m: 2.321, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.58917
[1mStep[0m  [8/84], [94mLoss[0m : 2.35859
[1mStep[0m  [16/84], [94mLoss[0m : 2.40899
[1mStep[0m  [24/84], [94mLoss[0m : 2.68031
[1mStep[0m  [32/84], [94mLoss[0m : 2.37342
[1mStep[0m  [40/84], [94mLoss[0m : 2.55105
[1mStep[0m  [48/84], [94mLoss[0m : 2.51764
[1mStep[0m  [56/84], [94mLoss[0m : 2.71228
[1mStep[0m  [64/84], [94mLoss[0m : 2.30535
[1mStep[0m  [72/84], [94mLoss[0m : 2.58768
[1mStep[0m  [80/84], [94mLoss[0m : 2.41992

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.573, [92mTest[0m: 2.340, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49466
[1mStep[0m  [8/84], [94mLoss[0m : 2.62752
[1mStep[0m  [16/84], [94mLoss[0m : 2.37762
[1mStep[0m  [24/84], [94mLoss[0m : 2.66217
[1mStep[0m  [32/84], [94mLoss[0m : 2.73942
[1mStep[0m  [40/84], [94mLoss[0m : 2.61264
[1mStep[0m  [48/84], [94mLoss[0m : 2.85808
[1mStep[0m  [56/84], [94mLoss[0m : 2.46616
[1mStep[0m  [64/84], [94mLoss[0m : 2.33853
[1mStep[0m  [72/84], [94mLoss[0m : 2.65834
[1mStep[0m  [80/84], [94mLoss[0m : 3.06452

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.571, [92mTest[0m: 2.336, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.327
====================================

Phase 1 - Evaluation MAE:  2.326564473765237
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 2.46742
[1mStep[0m  [8/84], [94mLoss[0m : 2.73036
[1mStep[0m  [16/84], [94mLoss[0m : 2.95460
[1mStep[0m  [24/84], [94mLoss[0m : 2.27087
[1mStep[0m  [32/84], [94mLoss[0m : 2.47046
[1mStep[0m  [40/84], [94mLoss[0m : 2.73038
[1mStep[0m  [48/84], [94mLoss[0m : 2.57235
[1mStep[0m  [56/84], [94mLoss[0m : 2.38498
[1mStep[0m  [64/84], [94mLoss[0m : 2.13213
[1mStep[0m  [72/84], [94mLoss[0m : 2.54698
[1mStep[0m  [80/84], [94mLoss[0m : 2.44098

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.578, [92mTest[0m: 2.326, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45606
[1mStep[0m  [8/84], [94mLoss[0m : 2.48848
[1mStep[0m  [16/84], [94mLoss[0m : 2.41247
[1mStep[0m  [24/84], [94mLoss[0m : 2.66628
[1mStep[0m  [32/84], [94mLoss[0m : 2.92303
[1mStep[0m  [40/84], [94mLoss[0m : 2.35571
[1mStep[0m  [48/84], [94mLoss[0m : 2.68835
[1mStep[0m  [56/84], [94mLoss[0m : 2.44631
[1mStep[0m  [64/84], [94mLoss[0m : 2.57423
[1mStep[0m  [72/84], [94mLoss[0m : 2.68411
[1mStep[0m  [80/84], [94mLoss[0m : 2.45744

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.521, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.56059
[1mStep[0m  [8/84], [94mLoss[0m : 2.47907
[1mStep[0m  [16/84], [94mLoss[0m : 2.81710
[1mStep[0m  [24/84], [94mLoss[0m : 2.18221
[1mStep[0m  [32/84], [94mLoss[0m : 2.36096
[1mStep[0m  [40/84], [94mLoss[0m : 2.74378
[1mStep[0m  [48/84], [94mLoss[0m : 2.67712
[1mStep[0m  [56/84], [94mLoss[0m : 2.72978
[1mStep[0m  [64/84], [94mLoss[0m : 2.40870
[1mStep[0m  [72/84], [94mLoss[0m : 2.34870
[1mStep[0m  [80/84], [94mLoss[0m : 2.14151

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.613, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.24076
[1mStep[0m  [8/84], [94mLoss[0m : 2.46448
[1mStep[0m  [16/84], [94mLoss[0m : 2.76116
[1mStep[0m  [24/84], [94mLoss[0m : 2.48296
[1mStep[0m  [32/84], [94mLoss[0m : 2.52650
[1mStep[0m  [40/84], [94mLoss[0m : 2.32186
[1mStep[0m  [48/84], [94mLoss[0m : 2.62337
[1mStep[0m  [56/84], [94mLoss[0m : 2.50148
[1mStep[0m  [64/84], [94mLoss[0m : 2.59562
[1mStep[0m  [72/84], [94mLoss[0m : 2.49961
[1mStep[0m  [80/84], [94mLoss[0m : 2.21695

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.517, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.23155
[1mStep[0m  [8/84], [94mLoss[0m : 2.23125
[1mStep[0m  [16/84], [94mLoss[0m : 2.59319
[1mStep[0m  [24/84], [94mLoss[0m : 2.44086
[1mStep[0m  [32/84], [94mLoss[0m : 2.36404
[1mStep[0m  [40/84], [94mLoss[0m : 2.58835
[1mStep[0m  [48/84], [94mLoss[0m : 2.56990
[1mStep[0m  [56/84], [94mLoss[0m : 2.57339
[1mStep[0m  [64/84], [94mLoss[0m : 2.67965
[1mStep[0m  [72/84], [94mLoss[0m : 1.93093
[1mStep[0m  [80/84], [94mLoss[0m : 2.71801

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.548, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.64689
[1mStep[0m  [8/84], [94mLoss[0m : 2.29005
[1mStep[0m  [16/84], [94mLoss[0m : 2.49258
[1mStep[0m  [24/84], [94mLoss[0m : 2.24722
[1mStep[0m  [32/84], [94mLoss[0m : 2.16238
[1mStep[0m  [40/84], [94mLoss[0m : 2.11249
[1mStep[0m  [48/84], [94mLoss[0m : 2.30364
[1mStep[0m  [56/84], [94mLoss[0m : 2.37609
[1mStep[0m  [64/84], [94mLoss[0m : 2.38076
[1mStep[0m  [72/84], [94mLoss[0m : 2.41897
[1mStep[0m  [80/84], [94mLoss[0m : 2.49294

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.359, [92mTest[0m: 2.563, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.14482
[1mStep[0m  [8/84], [94mLoss[0m : 2.44354
[1mStep[0m  [16/84], [94mLoss[0m : 2.30309
[1mStep[0m  [24/84], [94mLoss[0m : 2.28681
[1mStep[0m  [32/84], [94mLoss[0m : 2.17458
[1mStep[0m  [40/84], [94mLoss[0m : 2.43954
[1mStep[0m  [48/84], [94mLoss[0m : 2.25342
[1mStep[0m  [56/84], [94mLoss[0m : 2.22962
[1mStep[0m  [64/84], [94mLoss[0m : 2.26856
[1mStep[0m  [72/84], [94mLoss[0m : 2.18994
[1mStep[0m  [80/84], [94mLoss[0m : 2.15091

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.300, [92mTest[0m: 2.670, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41860
[1mStep[0m  [8/84], [94mLoss[0m : 1.89474
[1mStep[0m  [16/84], [94mLoss[0m : 2.26658
[1mStep[0m  [24/84], [94mLoss[0m : 2.70079
[1mStep[0m  [32/84], [94mLoss[0m : 2.16457
[1mStep[0m  [40/84], [94mLoss[0m : 2.28699
[1mStep[0m  [48/84], [94mLoss[0m : 1.93912
[1mStep[0m  [56/84], [94mLoss[0m : 2.04992
[1mStep[0m  [64/84], [94mLoss[0m : 2.36383
[1mStep[0m  [72/84], [94mLoss[0m : 2.29683
[1mStep[0m  [80/84], [94mLoss[0m : 2.08088

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.254, [92mTest[0m: 2.567, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.19945
[1mStep[0m  [8/84], [94mLoss[0m : 2.05310
[1mStep[0m  [16/84], [94mLoss[0m : 2.41100
[1mStep[0m  [24/84], [94mLoss[0m : 2.50134
[1mStep[0m  [32/84], [94mLoss[0m : 2.10055
[1mStep[0m  [40/84], [94mLoss[0m : 2.05247
[1mStep[0m  [48/84], [94mLoss[0m : 2.35905
[1mStep[0m  [56/84], [94mLoss[0m : 2.10219
[1mStep[0m  [64/84], [94mLoss[0m : 2.09429
[1mStep[0m  [72/84], [94mLoss[0m : 2.08875
[1mStep[0m  [80/84], [94mLoss[0m : 2.16834

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.220, [92mTest[0m: 2.386, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40383
[1mStep[0m  [8/84], [94mLoss[0m : 2.21704
[1mStep[0m  [16/84], [94mLoss[0m : 2.19260
[1mStep[0m  [24/84], [94mLoss[0m : 2.11175
[1mStep[0m  [32/84], [94mLoss[0m : 2.57643
[1mStep[0m  [40/84], [94mLoss[0m : 2.32430
[1mStep[0m  [48/84], [94mLoss[0m : 2.24259
[1mStep[0m  [56/84], [94mLoss[0m : 2.01869
[1mStep[0m  [64/84], [94mLoss[0m : 2.13584
[1mStep[0m  [72/84], [94mLoss[0m : 2.20288
[1mStep[0m  [80/84], [94mLoss[0m : 2.00125

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.191, [92mTest[0m: 2.439, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.06264
[1mStep[0m  [8/84], [94mLoss[0m : 2.31571
[1mStep[0m  [16/84], [94mLoss[0m : 1.80566
[1mStep[0m  [24/84], [94mLoss[0m : 2.32867
[1mStep[0m  [32/84], [94mLoss[0m : 2.52857
[1mStep[0m  [40/84], [94mLoss[0m : 1.94233
[1mStep[0m  [48/84], [94mLoss[0m : 2.49767
[1mStep[0m  [56/84], [94mLoss[0m : 1.96907
[1mStep[0m  [64/84], [94mLoss[0m : 2.15653
[1mStep[0m  [72/84], [94mLoss[0m : 2.04095
[1mStep[0m  [80/84], [94mLoss[0m : 2.37112

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.119, [92mTest[0m: 2.531, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.06054
[1mStep[0m  [8/84], [94mLoss[0m : 1.93677
[1mStep[0m  [16/84], [94mLoss[0m : 2.11839
[1mStep[0m  [24/84], [94mLoss[0m : 2.33602
[1mStep[0m  [32/84], [94mLoss[0m : 1.93999
[1mStep[0m  [40/84], [94mLoss[0m : 1.84073
[1mStep[0m  [48/84], [94mLoss[0m : 2.14689
[1mStep[0m  [56/84], [94mLoss[0m : 2.09139
[1mStep[0m  [64/84], [94mLoss[0m : 1.97141
[1mStep[0m  [72/84], [94mLoss[0m : 1.96486
[1mStep[0m  [80/84], [94mLoss[0m : 2.33269

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.102, [92mTest[0m: 2.455, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.06138
[1mStep[0m  [8/84], [94mLoss[0m : 1.95266
[1mStep[0m  [16/84], [94mLoss[0m : 1.97607
[1mStep[0m  [24/84], [94mLoss[0m : 2.04027
[1mStep[0m  [32/84], [94mLoss[0m : 1.88029
[1mStep[0m  [40/84], [94mLoss[0m : 1.93337
[1mStep[0m  [48/84], [94mLoss[0m : 2.10787
[1mStep[0m  [56/84], [94mLoss[0m : 2.08018
[1mStep[0m  [64/84], [94mLoss[0m : 2.12140
[1mStep[0m  [72/84], [94mLoss[0m : 2.11222
[1mStep[0m  [80/84], [94mLoss[0m : 1.88159

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.033, [92mTest[0m: 2.462, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.80746
[1mStep[0m  [8/84], [94mLoss[0m : 1.94136
[1mStep[0m  [16/84], [94mLoss[0m : 1.86860
[1mStep[0m  [24/84], [94mLoss[0m : 1.81037
[1mStep[0m  [32/84], [94mLoss[0m : 2.33181
[1mStep[0m  [40/84], [94mLoss[0m : 1.95040
[1mStep[0m  [48/84], [94mLoss[0m : 2.04726
[1mStep[0m  [56/84], [94mLoss[0m : 1.96570
[1mStep[0m  [64/84], [94mLoss[0m : 1.97397
[1mStep[0m  [72/84], [94mLoss[0m : 2.25942
[1mStep[0m  [80/84], [94mLoss[0m : 1.90183

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.005, [92mTest[0m: 2.502, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.10562
[1mStep[0m  [8/84], [94mLoss[0m : 2.16136
[1mStep[0m  [16/84], [94mLoss[0m : 1.83575
[1mStep[0m  [24/84], [94mLoss[0m : 2.03765
[1mStep[0m  [32/84], [94mLoss[0m : 1.84921
[1mStep[0m  [40/84], [94mLoss[0m : 2.10894
[1mStep[0m  [48/84], [94mLoss[0m : 1.85550
[1mStep[0m  [56/84], [94mLoss[0m : 1.72123
[1mStep[0m  [64/84], [94mLoss[0m : 2.07878
[1mStep[0m  [72/84], [94mLoss[0m : 1.85679
[1mStep[0m  [80/84], [94mLoss[0m : 2.31387

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.988, [92mTest[0m: 2.486, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.11314
[1mStep[0m  [8/84], [94mLoss[0m : 1.87284
[1mStep[0m  [16/84], [94mLoss[0m : 1.87134
[1mStep[0m  [24/84], [94mLoss[0m : 1.93690
[1mStep[0m  [32/84], [94mLoss[0m : 1.88345
[1mStep[0m  [40/84], [94mLoss[0m : 2.04621
[1mStep[0m  [48/84], [94mLoss[0m : 1.68164
[1mStep[0m  [56/84], [94mLoss[0m : 2.05540
[1mStep[0m  [64/84], [94mLoss[0m : 1.85786
[1mStep[0m  [72/84], [94mLoss[0m : 2.01651
[1mStep[0m  [80/84], [94mLoss[0m : 1.89803

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.943, [92mTest[0m: 2.562, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.69869
[1mStep[0m  [8/84], [94mLoss[0m : 1.80920
[1mStep[0m  [16/84], [94mLoss[0m : 1.81498
[1mStep[0m  [24/84], [94mLoss[0m : 1.70031
[1mStep[0m  [32/84], [94mLoss[0m : 1.97431
[1mStep[0m  [40/84], [94mLoss[0m : 1.63756
[1mStep[0m  [48/84], [94mLoss[0m : 1.63593
[1mStep[0m  [56/84], [94mLoss[0m : 1.96612
[1mStep[0m  [64/84], [94mLoss[0m : 2.05556
[1mStep[0m  [72/84], [94mLoss[0m : 1.81323
[1mStep[0m  [80/84], [94mLoss[0m : 2.12276

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.901, [92mTest[0m: 2.477, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.80688
[1mStep[0m  [8/84], [94mLoss[0m : 1.67263
[1mStep[0m  [16/84], [94mLoss[0m : 2.01866
[1mStep[0m  [24/84], [94mLoss[0m : 1.90599
[1mStep[0m  [32/84], [94mLoss[0m : 2.03371
[1mStep[0m  [40/84], [94mLoss[0m : 2.01203
[1mStep[0m  [48/84], [94mLoss[0m : 1.86369
[1mStep[0m  [56/84], [94mLoss[0m : 1.87671
[1mStep[0m  [64/84], [94mLoss[0m : 2.04340
[1mStep[0m  [72/84], [94mLoss[0m : 1.99403
[1mStep[0m  [80/84], [94mLoss[0m : 1.95623

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.886, [92mTest[0m: 2.534, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.83073
[1mStep[0m  [8/84], [94mLoss[0m : 2.01304
[1mStep[0m  [16/84], [94mLoss[0m : 1.65847
[1mStep[0m  [24/84], [94mLoss[0m : 1.62136
[1mStep[0m  [32/84], [94mLoss[0m : 1.51926
[1mStep[0m  [40/84], [94mLoss[0m : 1.99015
[1mStep[0m  [48/84], [94mLoss[0m : 1.62006
[1mStep[0m  [56/84], [94mLoss[0m : 1.76496
[1mStep[0m  [64/84], [94mLoss[0m : 1.73654
[1mStep[0m  [72/84], [94mLoss[0m : 1.91359
[1mStep[0m  [80/84], [94mLoss[0m : 2.06386

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.854, [92mTest[0m: 2.526, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.88266
[1mStep[0m  [8/84], [94mLoss[0m : 2.00500
[1mStep[0m  [16/84], [94mLoss[0m : 1.80093
[1mStep[0m  [24/84], [94mLoss[0m : 1.73032
[1mStep[0m  [32/84], [94mLoss[0m : 1.42810
[1mStep[0m  [40/84], [94mLoss[0m : 1.89534
[1mStep[0m  [48/84], [94mLoss[0m : 1.79626
[1mStep[0m  [56/84], [94mLoss[0m : 2.02057
[1mStep[0m  [64/84], [94mLoss[0m : 1.71492
[1mStep[0m  [72/84], [94mLoss[0m : 1.60254
[1mStep[0m  [80/84], [94mLoss[0m : 1.99253

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.821, [92mTest[0m: 2.560, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.72259
[1mStep[0m  [8/84], [94mLoss[0m : 1.48262
[1mStep[0m  [16/84], [94mLoss[0m : 1.62612
[1mStep[0m  [24/84], [94mLoss[0m : 1.91222
[1mStep[0m  [32/84], [94mLoss[0m : 1.65268
[1mStep[0m  [40/84], [94mLoss[0m : 1.58303
[1mStep[0m  [48/84], [94mLoss[0m : 1.75588
[1mStep[0m  [56/84], [94mLoss[0m : 1.84167
[1mStep[0m  [64/84], [94mLoss[0m : 1.66199
[1mStep[0m  [72/84], [94mLoss[0m : 1.87342
[1mStep[0m  [80/84], [94mLoss[0m : 1.70379

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.760, [92mTest[0m: 2.490, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.55584
[1mStep[0m  [8/84], [94mLoss[0m : 1.67497
[1mStep[0m  [16/84], [94mLoss[0m : 1.70182
[1mStep[0m  [24/84], [94mLoss[0m : 1.48287
[1mStep[0m  [32/84], [94mLoss[0m : 1.55104
[1mStep[0m  [40/84], [94mLoss[0m : 1.59035
[1mStep[0m  [48/84], [94mLoss[0m : 1.87584
[1mStep[0m  [56/84], [94mLoss[0m : 1.71071
[1mStep[0m  [64/84], [94mLoss[0m : 1.50094
[1mStep[0m  [72/84], [94mLoss[0m : 1.69638
[1mStep[0m  [80/84], [94mLoss[0m : 1.61317

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.749, [92mTest[0m: 2.536, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.45359
[1mStep[0m  [8/84], [94mLoss[0m : 1.68678
[1mStep[0m  [16/84], [94mLoss[0m : 1.60638
[1mStep[0m  [24/84], [94mLoss[0m : 1.56942
[1mStep[0m  [32/84], [94mLoss[0m : 1.57736
[1mStep[0m  [40/84], [94mLoss[0m : 1.87379
[1mStep[0m  [48/84], [94mLoss[0m : 1.93004
[1mStep[0m  [56/84], [94mLoss[0m : 1.83734
[1mStep[0m  [64/84], [94mLoss[0m : 1.79974
[1mStep[0m  [72/84], [94mLoss[0m : 1.56628
[1mStep[0m  [80/84], [94mLoss[0m : 2.16857

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.723, [92mTest[0m: 2.483, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.56973
[1mStep[0m  [8/84], [94mLoss[0m : 1.80378
[1mStep[0m  [16/84], [94mLoss[0m : 1.71340
[1mStep[0m  [24/84], [94mLoss[0m : 1.61327
[1mStep[0m  [32/84], [94mLoss[0m : 1.51536
[1mStep[0m  [40/84], [94mLoss[0m : 1.87862
[1mStep[0m  [48/84], [94mLoss[0m : 1.91121
[1mStep[0m  [56/84], [94mLoss[0m : 1.68970
[1mStep[0m  [64/84], [94mLoss[0m : 1.42145
[1mStep[0m  [72/84], [94mLoss[0m : 1.85688
[1mStep[0m  [80/84], [94mLoss[0m : 1.77796

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.707, [92mTest[0m: 2.507, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.67448
[1mStep[0m  [8/84], [94mLoss[0m : 1.79499
[1mStep[0m  [16/84], [94mLoss[0m : 1.45848
[1mStep[0m  [24/84], [94mLoss[0m : 1.85574
[1mStep[0m  [32/84], [94mLoss[0m : 1.58965
[1mStep[0m  [40/84], [94mLoss[0m : 1.74754
[1mStep[0m  [48/84], [94mLoss[0m : 2.06935
[1mStep[0m  [56/84], [94mLoss[0m : 1.83389
[1mStep[0m  [64/84], [94mLoss[0m : 1.58866
[1mStep[0m  [72/84], [94mLoss[0m : 1.63454
[1mStep[0m  [80/84], [94mLoss[0m : 1.69203

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.676, [92mTest[0m: 2.499, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.47585
[1mStep[0m  [8/84], [94mLoss[0m : 1.69095
[1mStep[0m  [16/84], [94mLoss[0m : 1.59965
[1mStep[0m  [24/84], [94mLoss[0m : 1.80198
[1mStep[0m  [32/84], [94mLoss[0m : 1.60842
[1mStep[0m  [40/84], [94mLoss[0m : 1.84750
[1mStep[0m  [48/84], [94mLoss[0m : 1.90924
[1mStep[0m  [56/84], [94mLoss[0m : 1.59917
[1mStep[0m  [64/84], [94mLoss[0m : 1.75346
[1mStep[0m  [72/84], [94mLoss[0m : 1.98652
[1mStep[0m  [80/84], [94mLoss[0m : 1.71471

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.678, [92mTest[0m: 2.554, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.67366
[1mStep[0m  [8/84], [94mLoss[0m : 1.91007
[1mStep[0m  [16/84], [94mLoss[0m : 1.60747
[1mStep[0m  [24/84], [94mLoss[0m : 1.57821
[1mStep[0m  [32/84], [94mLoss[0m : 1.61269
[1mStep[0m  [40/84], [94mLoss[0m : 1.88069
[1mStep[0m  [48/84], [94mLoss[0m : 1.46767
[1mStep[0m  [56/84], [94mLoss[0m : 1.50652
[1mStep[0m  [64/84], [94mLoss[0m : 1.73306
[1mStep[0m  [72/84], [94mLoss[0m : 1.62985
[1mStep[0m  [80/84], [94mLoss[0m : 1.80352

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.640, [92mTest[0m: 2.562, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.50383
[1mStep[0m  [8/84], [94mLoss[0m : 1.72825
[1mStep[0m  [16/84], [94mLoss[0m : 1.75100
[1mStep[0m  [24/84], [94mLoss[0m : 1.94759
[1mStep[0m  [32/84], [94mLoss[0m : 1.56158
[1mStep[0m  [40/84], [94mLoss[0m : 1.65445
[1mStep[0m  [48/84], [94mLoss[0m : 1.52520
[1mStep[0m  [56/84], [94mLoss[0m : 1.81434
[1mStep[0m  [64/84], [94mLoss[0m : 1.42529
[1mStep[0m  [72/84], [94mLoss[0m : 1.59829
[1mStep[0m  [80/84], [94mLoss[0m : 1.93682

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.636, [92mTest[0m: 2.527, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.89112
[1mStep[0m  [8/84], [94mLoss[0m : 1.60004
[1mStep[0m  [16/84], [94mLoss[0m : 1.53848
[1mStep[0m  [24/84], [94mLoss[0m : 1.78132
[1mStep[0m  [32/84], [94mLoss[0m : 1.71266
[1mStep[0m  [40/84], [94mLoss[0m : 1.47290
[1mStep[0m  [48/84], [94mLoss[0m : 1.64121
[1mStep[0m  [56/84], [94mLoss[0m : 1.49001
[1mStep[0m  [64/84], [94mLoss[0m : 1.64882
[1mStep[0m  [72/84], [94mLoss[0m : 1.64373
[1mStep[0m  [80/84], [94mLoss[0m : 1.66254

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.623, [92mTest[0m: 2.576, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.62281
[1mStep[0m  [8/84], [94mLoss[0m : 1.49307
[1mStep[0m  [16/84], [94mLoss[0m : 1.49727
[1mStep[0m  [24/84], [94mLoss[0m : 1.57897
[1mStep[0m  [32/84], [94mLoss[0m : 1.72470
[1mStep[0m  [40/84], [94mLoss[0m : 1.61957
[1mStep[0m  [48/84], [94mLoss[0m : 1.60903
[1mStep[0m  [56/84], [94mLoss[0m : 1.40654
[1mStep[0m  [64/84], [94mLoss[0m : 1.75288
[1mStep[0m  [72/84], [94mLoss[0m : 1.76300
[1mStep[0m  [80/84], [94mLoss[0m : 1.88424

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.605, [92mTest[0m: 2.575, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.520
====================================

Phase 2 - Evaluation MAE:  2.519588530063629
MAE score P1       2.326564
MAE score P2       2.519589
loss               1.604868
learning_rate      0.007525
batch_size              128
hidden_sizes      [200, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.5
weight_decay         0.0001
Name: 19, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 11.29296
[1mStep[0m  [8/84], [94mLoss[0m : 9.88758
[1mStep[0m  [16/84], [94mLoss[0m : 10.82910
[1mStep[0m  [24/84], [94mLoss[0m : 9.86440
[1mStep[0m  [32/84], [94mLoss[0m : 10.00992
[1mStep[0m  [40/84], [94mLoss[0m : 9.76911
[1mStep[0m  [48/84], [94mLoss[0m : 9.37659
[1mStep[0m  [56/84], [94mLoss[0m : 9.61286
[1mStep[0m  [64/84], [94mLoss[0m : 8.95435
[1mStep[0m  [72/84], [94mLoss[0m : 8.68962
[1mStep[0m  [80/84], [94mLoss[0m : 8.76796

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.777, [92mTest[0m: 10.865, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.38037
[1mStep[0m  [8/84], [94mLoss[0m : 8.10332
[1mStep[0m  [16/84], [94mLoss[0m : 8.53752
[1mStep[0m  [24/84], [94mLoss[0m : 7.75053
[1mStep[0m  [32/84], [94mLoss[0m : 7.54264
[1mStep[0m  [40/84], [94mLoss[0m : 6.91950
[1mStep[0m  [48/84], [94mLoss[0m : 7.10773
[1mStep[0m  [56/84], [94mLoss[0m : 6.30631
[1mStep[0m  [64/84], [94mLoss[0m : 6.23359
[1mStep[0m  [72/84], [94mLoss[0m : 6.62492
[1mStep[0m  [80/84], [94mLoss[0m : 5.80367

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 7.075, [92mTest[0m: 8.059, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.07238
[1mStep[0m  [8/84], [94mLoss[0m : 5.70957
[1mStep[0m  [16/84], [94mLoss[0m : 5.45421
[1mStep[0m  [24/84], [94mLoss[0m : 5.06227
[1mStep[0m  [32/84], [94mLoss[0m : 4.67559
[1mStep[0m  [40/84], [94mLoss[0m : 4.74598
[1mStep[0m  [48/84], [94mLoss[0m : 4.50756
[1mStep[0m  [56/84], [94mLoss[0m : 4.14210
[1mStep[0m  [64/84], [94mLoss[0m : 3.62569
[1mStep[0m  [72/84], [94mLoss[0m : 3.46397
[1mStep[0m  [80/84], [94mLoss[0m : 3.48802

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 4.583, [92mTest[0m: 4.688, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.82374
[1mStep[0m  [8/84], [94mLoss[0m : 3.06474
[1mStep[0m  [16/84], [94mLoss[0m : 2.82281
[1mStep[0m  [24/84], [94mLoss[0m : 3.17395
[1mStep[0m  [32/84], [94mLoss[0m : 2.84514
[1mStep[0m  [40/84], [94mLoss[0m : 3.00058
[1mStep[0m  [48/84], [94mLoss[0m : 3.13166
[1mStep[0m  [56/84], [94mLoss[0m : 2.63101
[1mStep[0m  [64/84], [94mLoss[0m : 2.74002
[1mStep[0m  [72/84], [94mLoss[0m : 2.94514
[1mStep[0m  [80/84], [94mLoss[0m : 2.67034

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.869, [92mTest[0m: 2.584, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53281
[1mStep[0m  [8/84], [94mLoss[0m : 2.69953
[1mStep[0m  [16/84], [94mLoss[0m : 2.79153
[1mStep[0m  [24/84], [94mLoss[0m : 2.77766
[1mStep[0m  [32/84], [94mLoss[0m : 2.36488
[1mStep[0m  [40/84], [94mLoss[0m : 2.98026
[1mStep[0m  [48/84], [94mLoss[0m : 2.81416
[1mStep[0m  [56/84], [94mLoss[0m : 2.69636
[1mStep[0m  [64/84], [94mLoss[0m : 2.64036
[1mStep[0m  [72/84], [94mLoss[0m : 2.58193
[1mStep[0m  [80/84], [94mLoss[0m : 2.54635

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.687, [92mTest[0m: 2.403, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.75896
[1mStep[0m  [8/84], [94mLoss[0m : 2.35042
[1mStep[0m  [16/84], [94mLoss[0m : 2.83623
[1mStep[0m  [24/84], [94mLoss[0m : 2.71186
[1mStep[0m  [32/84], [94mLoss[0m : 2.62813
[1mStep[0m  [40/84], [94mLoss[0m : 2.44638
[1mStep[0m  [48/84], [94mLoss[0m : 2.51724
[1mStep[0m  [56/84], [94mLoss[0m : 2.29779
[1mStep[0m  [64/84], [94mLoss[0m : 2.79651
[1mStep[0m  [72/84], [94mLoss[0m : 2.69988
[1mStep[0m  [80/84], [94mLoss[0m : 2.88555

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.646, [92mTest[0m: 2.466, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.65759
[1mStep[0m  [8/84], [94mLoss[0m : 2.99874
[1mStep[0m  [16/84], [94mLoss[0m : 2.75013
[1mStep[0m  [24/84], [94mLoss[0m : 2.52947
[1mStep[0m  [32/84], [94mLoss[0m : 2.40871
[1mStep[0m  [40/84], [94mLoss[0m : 2.65496
[1mStep[0m  [48/84], [94mLoss[0m : 2.67624
[1mStep[0m  [56/84], [94mLoss[0m : 2.38511
[1mStep[0m  [64/84], [94mLoss[0m : 2.84780
[1mStep[0m  [72/84], [94mLoss[0m : 2.54560
[1mStep[0m  [80/84], [94mLoss[0m : 2.48683

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.640, [92mTest[0m: 2.403, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37018
[1mStep[0m  [8/84], [94mLoss[0m : 2.55887
[1mStep[0m  [16/84], [94mLoss[0m : 2.61024
[1mStep[0m  [24/84], [94mLoss[0m : 2.51074
[1mStep[0m  [32/84], [94mLoss[0m : 2.54435
[1mStep[0m  [40/84], [94mLoss[0m : 2.80089
[1mStep[0m  [48/84], [94mLoss[0m : 2.33656
[1mStep[0m  [56/84], [94mLoss[0m : 2.61455
[1mStep[0m  [64/84], [94mLoss[0m : 2.28772
[1mStep[0m  [72/84], [94mLoss[0m : 2.67362
[1mStep[0m  [80/84], [94mLoss[0m : 2.51500

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.575, [92mTest[0m: 2.424, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44958
[1mStep[0m  [8/84], [94mLoss[0m : 2.30150
[1mStep[0m  [16/84], [94mLoss[0m : 2.48686
[1mStep[0m  [24/84], [94mLoss[0m : 2.24969
[1mStep[0m  [32/84], [94mLoss[0m : 2.43262
[1mStep[0m  [40/84], [94mLoss[0m : 2.61296
[1mStep[0m  [48/84], [94mLoss[0m : 2.53712
[1mStep[0m  [56/84], [94mLoss[0m : 2.50802
[1mStep[0m  [64/84], [94mLoss[0m : 2.37641
[1mStep[0m  [72/84], [94mLoss[0m : 2.51524
[1mStep[0m  [80/84], [94mLoss[0m : 2.45303

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.581, [92mTest[0m: 2.377, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.89892
[1mStep[0m  [8/84], [94mLoss[0m : 2.30202
[1mStep[0m  [16/84], [94mLoss[0m : 2.60133
[1mStep[0m  [24/84], [94mLoss[0m : 2.56007
[1mStep[0m  [32/84], [94mLoss[0m : 2.49285
[1mStep[0m  [40/84], [94mLoss[0m : 2.58148
[1mStep[0m  [48/84], [94mLoss[0m : 2.79453
[1mStep[0m  [56/84], [94mLoss[0m : 2.38294
[1mStep[0m  [64/84], [94mLoss[0m : 2.56000
[1mStep[0m  [72/84], [94mLoss[0m : 2.65477
[1mStep[0m  [80/84], [94mLoss[0m : 2.63009

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.561, [92mTest[0m: 2.366, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36523
[1mStep[0m  [8/84], [94mLoss[0m : 2.59358
[1mStep[0m  [16/84], [94mLoss[0m : 2.63121
[1mStep[0m  [24/84], [94mLoss[0m : 2.53684
[1mStep[0m  [32/84], [94mLoss[0m : 2.18344
[1mStep[0m  [40/84], [94mLoss[0m : 2.33277
[1mStep[0m  [48/84], [94mLoss[0m : 2.83788
[1mStep[0m  [56/84], [94mLoss[0m : 2.62566
[1mStep[0m  [64/84], [94mLoss[0m : 2.53283
[1mStep[0m  [72/84], [94mLoss[0m : 2.52152
[1mStep[0m  [80/84], [94mLoss[0m : 2.15713

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.538, [92mTest[0m: 2.348, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.71091
[1mStep[0m  [8/84], [94mLoss[0m : 2.16215
[1mStep[0m  [16/84], [94mLoss[0m : 2.70700
[1mStep[0m  [24/84], [94mLoss[0m : 2.41757
[1mStep[0m  [32/84], [94mLoss[0m : 2.68212
[1mStep[0m  [40/84], [94mLoss[0m : 2.62585
[1mStep[0m  [48/84], [94mLoss[0m : 2.51064
[1mStep[0m  [56/84], [94mLoss[0m : 2.61272
[1mStep[0m  [64/84], [94mLoss[0m : 3.02188
[1mStep[0m  [72/84], [94mLoss[0m : 2.45559
[1mStep[0m  [80/84], [94mLoss[0m : 2.56182

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.534, [92mTest[0m: 2.403, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53688
[1mStep[0m  [8/84], [94mLoss[0m : 2.84971
[1mStep[0m  [16/84], [94mLoss[0m : 2.64719
[1mStep[0m  [24/84], [94mLoss[0m : 2.50462
[1mStep[0m  [32/84], [94mLoss[0m : 2.67229
[1mStep[0m  [40/84], [94mLoss[0m : 2.27784
[1mStep[0m  [48/84], [94mLoss[0m : 2.63469
[1mStep[0m  [56/84], [94mLoss[0m : 2.58680
[1mStep[0m  [64/84], [94mLoss[0m : 2.45572
[1mStep[0m  [72/84], [94mLoss[0m : 2.94491
[1mStep[0m  [80/84], [94mLoss[0m : 2.38054

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.368, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49675
[1mStep[0m  [8/84], [94mLoss[0m : 2.71992
[1mStep[0m  [16/84], [94mLoss[0m : 2.76096
[1mStep[0m  [24/84], [94mLoss[0m : 2.44568
[1mStep[0m  [32/84], [94mLoss[0m : 2.70973
[1mStep[0m  [40/84], [94mLoss[0m : 2.32526
[1mStep[0m  [48/84], [94mLoss[0m : 2.30761
[1mStep[0m  [56/84], [94mLoss[0m : 2.98244
[1mStep[0m  [64/84], [94mLoss[0m : 2.36090
[1mStep[0m  [72/84], [94mLoss[0m : 2.96477
[1mStep[0m  [80/84], [94mLoss[0m : 2.45712

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.519, [92mTest[0m: 2.376, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.38610
[1mStep[0m  [8/84], [94mLoss[0m : 2.25308
[1mStep[0m  [16/84], [94mLoss[0m : 2.39933
[1mStep[0m  [24/84], [94mLoss[0m : 2.54968
[1mStep[0m  [32/84], [94mLoss[0m : 2.17710
[1mStep[0m  [40/84], [94mLoss[0m : 2.41755
[1mStep[0m  [48/84], [94mLoss[0m : 2.28480
[1mStep[0m  [56/84], [94mLoss[0m : 2.42010
[1mStep[0m  [64/84], [94mLoss[0m : 2.31819
[1mStep[0m  [72/84], [94mLoss[0m : 2.27439
[1mStep[0m  [80/84], [94mLoss[0m : 2.34084

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.357, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30542
[1mStep[0m  [8/84], [94mLoss[0m : 2.42804
[1mStep[0m  [16/84], [94mLoss[0m : 2.39247
[1mStep[0m  [24/84], [94mLoss[0m : 2.53753
[1mStep[0m  [32/84], [94mLoss[0m : 2.43188
[1mStep[0m  [40/84], [94mLoss[0m : 2.50382
[1mStep[0m  [48/84], [94mLoss[0m : 2.33348
[1mStep[0m  [56/84], [94mLoss[0m : 2.17492
[1mStep[0m  [64/84], [94mLoss[0m : 2.46689
[1mStep[0m  [72/84], [94mLoss[0m : 2.43946
[1mStep[0m  [80/84], [94mLoss[0m : 2.23331

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.353, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42400
[1mStep[0m  [8/84], [94mLoss[0m : 2.56210
[1mStep[0m  [16/84], [94mLoss[0m : 2.49694
[1mStep[0m  [24/84], [94mLoss[0m : 2.28887
[1mStep[0m  [32/84], [94mLoss[0m : 2.54215
[1mStep[0m  [40/84], [94mLoss[0m : 2.54005
[1mStep[0m  [48/84], [94mLoss[0m : 2.28604
[1mStep[0m  [56/84], [94mLoss[0m : 2.47625
[1mStep[0m  [64/84], [94mLoss[0m : 2.60883
[1mStep[0m  [72/84], [94mLoss[0m : 2.42792
[1mStep[0m  [80/84], [94mLoss[0m : 2.48212

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.357, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.63516
[1mStep[0m  [8/84], [94mLoss[0m : 2.63249
[1mStep[0m  [16/84], [94mLoss[0m : 2.34003
[1mStep[0m  [24/84], [94mLoss[0m : 2.47541
[1mStep[0m  [32/84], [94mLoss[0m : 2.18299
[1mStep[0m  [40/84], [94mLoss[0m : 2.57505
[1mStep[0m  [48/84], [94mLoss[0m : 2.37443
[1mStep[0m  [56/84], [94mLoss[0m : 2.55618
[1mStep[0m  [64/84], [94mLoss[0m : 2.25358
[1mStep[0m  [72/84], [94mLoss[0m : 2.55318
[1mStep[0m  [80/84], [94mLoss[0m : 2.41202

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.355, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36507
[1mStep[0m  [8/84], [94mLoss[0m : 2.50915
[1mStep[0m  [16/84], [94mLoss[0m : 2.25420
[1mStep[0m  [24/84], [94mLoss[0m : 2.29533
[1mStep[0m  [32/84], [94mLoss[0m : 2.67128
[1mStep[0m  [40/84], [94mLoss[0m : 2.35122
[1mStep[0m  [48/84], [94mLoss[0m : 2.63580
[1mStep[0m  [56/84], [94mLoss[0m : 2.54727
[1mStep[0m  [64/84], [94mLoss[0m : 2.30441
[1mStep[0m  [72/84], [94mLoss[0m : 2.37481
[1mStep[0m  [80/84], [94mLoss[0m : 2.45770

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.376, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.62626
[1mStep[0m  [8/84], [94mLoss[0m : 2.26994
[1mStep[0m  [16/84], [94mLoss[0m : 2.05006
[1mStep[0m  [24/84], [94mLoss[0m : 2.41791
[1mStep[0m  [32/84], [94mLoss[0m : 2.41153
[1mStep[0m  [40/84], [94mLoss[0m : 2.57099
[1mStep[0m  [48/84], [94mLoss[0m : 2.11880
[1mStep[0m  [56/84], [94mLoss[0m : 2.49433
[1mStep[0m  [64/84], [94mLoss[0m : 2.62186
[1mStep[0m  [72/84], [94mLoss[0m : 2.33084
[1mStep[0m  [80/84], [94mLoss[0m : 2.50647

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.347, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44916
[1mStep[0m  [8/84], [94mLoss[0m : 2.15376
[1mStep[0m  [16/84], [94mLoss[0m : 2.75210
[1mStep[0m  [24/84], [94mLoss[0m : 2.41520
[1mStep[0m  [32/84], [94mLoss[0m : 2.57539
[1mStep[0m  [40/84], [94mLoss[0m : 2.40102
[1mStep[0m  [48/84], [94mLoss[0m : 2.54048
[1mStep[0m  [56/84], [94mLoss[0m : 2.25450
[1mStep[0m  [64/84], [94mLoss[0m : 2.18784
[1mStep[0m  [72/84], [94mLoss[0m : 2.39104
[1mStep[0m  [80/84], [94mLoss[0m : 2.51023

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.354, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.69438
[1mStep[0m  [8/84], [94mLoss[0m : 2.36622
[1mStep[0m  [16/84], [94mLoss[0m : 2.74609
[1mStep[0m  [24/84], [94mLoss[0m : 2.46484
[1mStep[0m  [32/84], [94mLoss[0m : 2.19433
[1mStep[0m  [40/84], [94mLoss[0m : 2.71898
[1mStep[0m  [48/84], [94mLoss[0m : 2.70793
[1mStep[0m  [56/84], [94mLoss[0m : 2.05622
[1mStep[0m  [64/84], [94mLoss[0m : 2.37022
[1mStep[0m  [72/84], [94mLoss[0m : 2.44959
[1mStep[0m  [80/84], [94mLoss[0m : 2.18589

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.356, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.34655
[1mStep[0m  [8/84], [94mLoss[0m : 2.29511
[1mStep[0m  [16/84], [94mLoss[0m : 2.45597
[1mStep[0m  [24/84], [94mLoss[0m : 2.46634
[1mStep[0m  [32/84], [94mLoss[0m : 2.66311
[1mStep[0m  [40/84], [94mLoss[0m : 2.84299
[1mStep[0m  [48/84], [94mLoss[0m : 2.50853
[1mStep[0m  [56/84], [94mLoss[0m : 2.32207
[1mStep[0m  [64/84], [94mLoss[0m : 2.40792
[1mStep[0m  [72/84], [94mLoss[0m : 2.65212
[1mStep[0m  [80/84], [94mLoss[0m : 2.69829

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.336, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.22746
[1mStep[0m  [8/84], [94mLoss[0m : 2.38428
[1mStep[0m  [16/84], [94mLoss[0m : 2.15429
[1mStep[0m  [24/84], [94mLoss[0m : 2.27144
[1mStep[0m  [32/84], [94mLoss[0m : 2.43160
[1mStep[0m  [40/84], [94mLoss[0m : 2.23453
[1mStep[0m  [48/84], [94mLoss[0m : 2.50166
[1mStep[0m  [56/84], [94mLoss[0m : 2.63244
[1mStep[0m  [64/84], [94mLoss[0m : 2.28058
[1mStep[0m  [72/84], [94mLoss[0m : 2.59864
[1mStep[0m  [80/84], [94mLoss[0m : 2.61789

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.346, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.15201
[1mStep[0m  [8/84], [94mLoss[0m : 2.61686
[1mStep[0m  [16/84], [94mLoss[0m : 2.25046
[1mStep[0m  [24/84], [94mLoss[0m : 2.61425
[1mStep[0m  [32/84], [94mLoss[0m : 2.46820
[1mStep[0m  [40/84], [94mLoss[0m : 2.40639
[1mStep[0m  [48/84], [94mLoss[0m : 2.26424
[1mStep[0m  [56/84], [94mLoss[0m : 2.14636
[1mStep[0m  [64/84], [94mLoss[0m : 2.58500
[1mStep[0m  [72/84], [94mLoss[0m : 2.42194
[1mStep[0m  [80/84], [94mLoss[0m : 2.42449

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.344, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.00944
[1mStep[0m  [8/84], [94mLoss[0m : 2.45935
[1mStep[0m  [16/84], [94mLoss[0m : 2.58233
[1mStep[0m  [24/84], [94mLoss[0m : 2.75933
[1mStep[0m  [32/84], [94mLoss[0m : 2.33064
[1mStep[0m  [40/84], [94mLoss[0m : 2.33024
[1mStep[0m  [48/84], [94mLoss[0m : 2.08967
[1mStep[0m  [56/84], [94mLoss[0m : 2.51673
[1mStep[0m  [64/84], [94mLoss[0m : 2.35621
[1mStep[0m  [72/84], [94mLoss[0m : 2.40065
[1mStep[0m  [80/84], [94mLoss[0m : 2.25601

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.351, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30925
[1mStep[0m  [8/84], [94mLoss[0m : 2.13905
[1mStep[0m  [16/84], [94mLoss[0m : 2.45965
[1mStep[0m  [24/84], [94mLoss[0m : 2.61842
[1mStep[0m  [32/84], [94mLoss[0m : 2.63709
[1mStep[0m  [40/84], [94mLoss[0m : 2.49843
[1mStep[0m  [48/84], [94mLoss[0m : 2.67382
[1mStep[0m  [56/84], [94mLoss[0m : 2.02868
[1mStep[0m  [64/84], [94mLoss[0m : 2.63838
[1mStep[0m  [72/84], [94mLoss[0m : 2.11754
[1mStep[0m  [80/84], [94mLoss[0m : 2.49668

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.355, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.22519
[1mStep[0m  [8/84], [94mLoss[0m : 2.52024
[1mStep[0m  [16/84], [94mLoss[0m : 2.33521
[1mStep[0m  [24/84], [94mLoss[0m : 2.56328
[1mStep[0m  [32/84], [94mLoss[0m : 2.21763
[1mStep[0m  [40/84], [94mLoss[0m : 2.44424
[1mStep[0m  [48/84], [94mLoss[0m : 2.56266
[1mStep[0m  [56/84], [94mLoss[0m : 2.55110
[1mStep[0m  [64/84], [94mLoss[0m : 2.25389
[1mStep[0m  [72/84], [94mLoss[0m : 2.13635
[1mStep[0m  [80/84], [94mLoss[0m : 2.36423

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.347, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46182
[1mStep[0m  [8/84], [94mLoss[0m : 2.71819
[1mStep[0m  [16/84], [94mLoss[0m : 2.24726
[1mStep[0m  [24/84], [94mLoss[0m : 2.44615
[1mStep[0m  [32/84], [94mLoss[0m : 2.22874
[1mStep[0m  [40/84], [94mLoss[0m : 2.46586
[1mStep[0m  [48/84], [94mLoss[0m : 2.31196
[1mStep[0m  [56/84], [94mLoss[0m : 2.55301
[1mStep[0m  [64/84], [94mLoss[0m : 2.39433
[1mStep[0m  [72/84], [94mLoss[0m : 2.43095
[1mStep[0m  [80/84], [94mLoss[0m : 2.27666

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.401, [92mTest[0m: 2.354, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41676
[1mStep[0m  [8/84], [94mLoss[0m : 2.43979
[1mStep[0m  [16/84], [94mLoss[0m : 2.32815
[1mStep[0m  [24/84], [94mLoss[0m : 2.84318
[1mStep[0m  [32/84], [94mLoss[0m : 2.32095
[1mStep[0m  [40/84], [94mLoss[0m : 2.75989
[1mStep[0m  [48/84], [94mLoss[0m : 2.28519
[1mStep[0m  [56/84], [94mLoss[0m : 2.32887
[1mStep[0m  [64/84], [94mLoss[0m : 2.53133
[1mStep[0m  [72/84], [94mLoss[0m : 2.37685
[1mStep[0m  [80/84], [94mLoss[0m : 2.56759

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.363, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.351
====================================

Phase 1 - Evaluation MAE:  2.3510647330965315
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 2.57122
[1mStep[0m  [8/84], [94mLoss[0m : 2.62920
[1mStep[0m  [16/84], [94mLoss[0m : 2.53769
[1mStep[0m  [24/84], [94mLoss[0m : 2.72377
[1mStep[0m  [32/84], [94mLoss[0m : 2.27244
[1mStep[0m  [40/84], [94mLoss[0m : 2.65036
[1mStep[0m  [48/84], [94mLoss[0m : 2.46510
[1mStep[0m  [56/84], [94mLoss[0m : 2.43161
[1mStep[0m  [64/84], [94mLoss[0m : 2.22887
[1mStep[0m  [72/84], [94mLoss[0m : 2.66827
[1mStep[0m  [80/84], [94mLoss[0m : 2.49729

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.352, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.08647
[1mStep[0m  [8/84], [94mLoss[0m : 2.54914
[1mStep[0m  [16/84], [94mLoss[0m : 2.35289
[1mStep[0m  [24/84], [94mLoss[0m : 2.41593
[1mStep[0m  [32/84], [94mLoss[0m : 2.21912
[1mStep[0m  [40/84], [94mLoss[0m : 2.61947
[1mStep[0m  [48/84], [94mLoss[0m : 2.62831
[1mStep[0m  [56/84], [94mLoss[0m : 2.55004
[1mStep[0m  [64/84], [94mLoss[0m : 2.33634
[1mStep[0m  [72/84], [94mLoss[0m : 2.40253
[1mStep[0m  [80/84], [94mLoss[0m : 2.42849

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.436, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.25675
[1mStep[0m  [8/84], [94mLoss[0m : 2.43437
[1mStep[0m  [16/84], [94mLoss[0m : 2.34859
[1mStep[0m  [24/84], [94mLoss[0m : 2.47791
[1mStep[0m  [32/84], [94mLoss[0m : 2.31316
[1mStep[0m  [40/84], [94mLoss[0m : 2.46807
[1mStep[0m  [48/84], [94mLoss[0m : 2.40531
[1mStep[0m  [56/84], [94mLoss[0m : 2.58097
[1mStep[0m  [64/84], [94mLoss[0m : 2.34365
[1mStep[0m  [72/84], [94mLoss[0m : 2.28241
[1mStep[0m  [80/84], [94mLoss[0m : 2.13447

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.343, [92mTest[0m: 2.452, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.10718
[1mStep[0m  [8/84], [94mLoss[0m : 2.07684
[1mStep[0m  [16/84], [94mLoss[0m : 2.22093
[1mStep[0m  [24/84], [94mLoss[0m : 2.32854
[1mStep[0m  [32/84], [94mLoss[0m : 2.49294
[1mStep[0m  [40/84], [94mLoss[0m : 2.26131
[1mStep[0m  [48/84], [94mLoss[0m : 1.99621
[1mStep[0m  [56/84], [94mLoss[0m : 2.24278
[1mStep[0m  [64/84], [94mLoss[0m : 2.19501
[1mStep[0m  [72/84], [94mLoss[0m : 2.21362
[1mStep[0m  [80/84], [94mLoss[0m : 2.26034

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.277, [92mTest[0m: 2.440, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.11093
[1mStep[0m  [8/84], [94mLoss[0m : 1.98063
[1mStep[0m  [16/84], [94mLoss[0m : 2.11843
[1mStep[0m  [24/84], [94mLoss[0m : 2.18297
[1mStep[0m  [32/84], [94mLoss[0m : 2.24330
[1mStep[0m  [40/84], [94mLoss[0m : 2.10711
[1mStep[0m  [48/84], [94mLoss[0m : 1.92508
[1mStep[0m  [56/84], [94mLoss[0m : 1.90903
[1mStep[0m  [64/84], [94mLoss[0m : 2.46613
[1mStep[0m  [72/84], [94mLoss[0m : 2.15663
[1mStep[0m  [80/84], [94mLoss[0m : 2.28076

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.196, [92mTest[0m: 2.392, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.14082
[1mStep[0m  [8/84], [94mLoss[0m : 1.87162
[1mStep[0m  [16/84], [94mLoss[0m : 2.01687
[1mStep[0m  [24/84], [94mLoss[0m : 2.20911
[1mStep[0m  [32/84], [94mLoss[0m : 2.25467
[1mStep[0m  [40/84], [94mLoss[0m : 1.96843
[1mStep[0m  [48/84], [94mLoss[0m : 2.30961
[1mStep[0m  [56/84], [94mLoss[0m : 2.12638
[1mStep[0m  [64/84], [94mLoss[0m : 2.27713
[1mStep[0m  [72/84], [94mLoss[0m : 2.15099
[1mStep[0m  [80/84], [94mLoss[0m : 2.08829

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.150, [92mTest[0m: 2.414, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.23713
[1mStep[0m  [8/84], [94mLoss[0m : 1.88808
[1mStep[0m  [16/84], [94mLoss[0m : 1.96816
[1mStep[0m  [24/84], [94mLoss[0m : 2.27882
[1mStep[0m  [32/84], [94mLoss[0m : 1.87021
[1mStep[0m  [40/84], [94mLoss[0m : 2.10328
[1mStep[0m  [48/84], [94mLoss[0m : 2.10978
[1mStep[0m  [56/84], [94mLoss[0m : 1.85980
[1mStep[0m  [64/84], [94mLoss[0m : 2.10137
[1mStep[0m  [72/84], [94mLoss[0m : 2.29386
[1mStep[0m  [80/84], [94mLoss[0m : 2.09714

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.091, [92mTest[0m: 2.496, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.19154
[1mStep[0m  [8/84], [94mLoss[0m : 1.81438
[1mStep[0m  [16/84], [94mLoss[0m : 2.10289
[1mStep[0m  [24/84], [94mLoss[0m : 1.82320
[1mStep[0m  [32/84], [94mLoss[0m : 1.76257
[1mStep[0m  [40/84], [94mLoss[0m : 1.99968
[1mStep[0m  [48/84], [94mLoss[0m : 1.96960
[1mStep[0m  [56/84], [94mLoss[0m : 2.00697
[1mStep[0m  [64/84], [94mLoss[0m : 2.19236
[1mStep[0m  [72/84], [94mLoss[0m : 2.25727
[1mStep[0m  [80/84], [94mLoss[0m : 1.98644

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.032, [92mTest[0m: 2.521, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.99158
[1mStep[0m  [8/84], [94mLoss[0m : 1.73623
[1mStep[0m  [16/84], [94mLoss[0m : 2.11014
[1mStep[0m  [24/84], [94mLoss[0m : 2.24685
[1mStep[0m  [32/84], [94mLoss[0m : 1.94023
[1mStep[0m  [40/84], [94mLoss[0m : 2.10524
[1mStep[0m  [48/84], [94mLoss[0m : 1.98384
[1mStep[0m  [56/84], [94mLoss[0m : 2.00442
[1mStep[0m  [64/84], [94mLoss[0m : 2.15174
[1mStep[0m  [72/84], [94mLoss[0m : 1.94124
[1mStep[0m  [80/84], [94mLoss[0m : 1.92078

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.985, [92mTest[0m: 2.416, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.65487
[1mStep[0m  [8/84], [94mLoss[0m : 1.92991
[1mStep[0m  [16/84], [94mLoss[0m : 1.48485
[1mStep[0m  [24/84], [94mLoss[0m : 1.86371
[1mStep[0m  [32/84], [94mLoss[0m : 2.13995
[1mStep[0m  [40/84], [94mLoss[0m : 1.61593
[1mStep[0m  [48/84], [94mLoss[0m : 1.72201
[1mStep[0m  [56/84], [94mLoss[0m : 2.08392
[1mStep[0m  [64/84], [94mLoss[0m : 2.39877
[1mStep[0m  [72/84], [94mLoss[0m : 1.97954
[1mStep[0m  [80/84], [94mLoss[0m : 2.19302

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.902, [92mTest[0m: 2.427, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.02388
[1mStep[0m  [8/84], [94mLoss[0m : 1.74895
[1mStep[0m  [16/84], [94mLoss[0m : 1.98653
[1mStep[0m  [24/84], [94mLoss[0m : 1.98496
[1mStep[0m  [32/84], [94mLoss[0m : 1.95501
[1mStep[0m  [40/84], [94mLoss[0m : 1.93055
[1mStep[0m  [48/84], [94mLoss[0m : 1.99749
[1mStep[0m  [56/84], [94mLoss[0m : 1.99604
[1mStep[0m  [64/84], [94mLoss[0m : 2.03074
[1mStep[0m  [72/84], [94mLoss[0m : 1.99522
[1mStep[0m  [80/84], [94mLoss[0m : 2.06426

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.872, [92mTest[0m: 2.560, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.99622
[1mStep[0m  [8/84], [94mLoss[0m : 1.61722
[1mStep[0m  [16/84], [94mLoss[0m : 1.59193
[1mStep[0m  [24/84], [94mLoss[0m : 1.78167
[1mStep[0m  [32/84], [94mLoss[0m : 1.82024
[1mStep[0m  [40/84], [94mLoss[0m : 1.71267
[1mStep[0m  [48/84], [94mLoss[0m : 1.98427
[1mStep[0m  [56/84], [94mLoss[0m : 1.64686
[1mStep[0m  [64/84], [94mLoss[0m : 1.58843
[1mStep[0m  [72/84], [94mLoss[0m : 2.05797
[1mStep[0m  [80/84], [94mLoss[0m : 2.22261

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.822, [92mTest[0m: 2.495, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.77362
[1mStep[0m  [8/84], [94mLoss[0m : 1.84951
[1mStep[0m  [16/84], [94mLoss[0m : 1.69640
[1mStep[0m  [24/84], [94mLoss[0m : 1.61383
[1mStep[0m  [32/84], [94mLoss[0m : 1.81478
[1mStep[0m  [40/84], [94mLoss[0m : 1.57616
[1mStep[0m  [48/84], [94mLoss[0m : 1.73401
[1mStep[0m  [56/84], [94mLoss[0m : 1.95155
[1mStep[0m  [64/84], [94mLoss[0m : 2.02790
[1mStep[0m  [72/84], [94mLoss[0m : 2.11074
[1mStep[0m  [80/84], [94mLoss[0m : 1.87477

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.803, [92mTest[0m: 2.480, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.96889
[1mStep[0m  [8/84], [94mLoss[0m : 1.73033
[1mStep[0m  [16/84], [94mLoss[0m : 1.72391
[1mStep[0m  [24/84], [94mLoss[0m : 1.72267
[1mStep[0m  [32/84], [94mLoss[0m : 1.87274
[1mStep[0m  [40/84], [94mLoss[0m : 1.65747
[1mStep[0m  [48/84], [94mLoss[0m : 1.80189
[1mStep[0m  [56/84], [94mLoss[0m : 1.63803
[1mStep[0m  [64/84], [94mLoss[0m : 1.69018
[1mStep[0m  [72/84], [94mLoss[0m : 1.89338
[1mStep[0m  [80/84], [94mLoss[0m : 1.82814

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.738, [92mTest[0m: 2.549, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.60422
[1mStep[0m  [8/84], [94mLoss[0m : 1.87758
[1mStep[0m  [16/84], [94mLoss[0m : 1.51815
[1mStep[0m  [24/84], [94mLoss[0m : 1.58930
[1mStep[0m  [32/84], [94mLoss[0m : 1.79460
[1mStep[0m  [40/84], [94mLoss[0m : 1.78111
[1mStep[0m  [48/84], [94mLoss[0m : 1.67543
[1mStep[0m  [56/84], [94mLoss[0m : 1.48909
[1mStep[0m  [64/84], [94mLoss[0m : 2.00571
[1mStep[0m  [72/84], [94mLoss[0m : 1.62423
[1mStep[0m  [80/84], [94mLoss[0m : 1.63691

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.713, [92mTest[0m: 2.485, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.79470
[1mStep[0m  [8/84], [94mLoss[0m : 1.55757
[1mStep[0m  [16/84], [94mLoss[0m : 1.65912
[1mStep[0m  [24/84], [94mLoss[0m : 1.69772
[1mStep[0m  [32/84], [94mLoss[0m : 1.64007
[1mStep[0m  [40/84], [94mLoss[0m : 1.63663
[1mStep[0m  [48/84], [94mLoss[0m : 1.58227
[1mStep[0m  [56/84], [94mLoss[0m : 1.58183
[1mStep[0m  [64/84], [94mLoss[0m : 1.78497
[1mStep[0m  [72/84], [94mLoss[0m : 1.76047
[1mStep[0m  [80/84], [94mLoss[0m : 1.61414

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.684, [92mTest[0m: 2.438, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.62081
[1mStep[0m  [8/84], [94mLoss[0m : 1.65553
[1mStep[0m  [16/84], [94mLoss[0m : 1.48468
[1mStep[0m  [24/84], [94mLoss[0m : 1.89653
[1mStep[0m  [32/84], [94mLoss[0m : 1.58457
[1mStep[0m  [40/84], [94mLoss[0m : 1.64339
[1mStep[0m  [48/84], [94mLoss[0m : 1.70488
[1mStep[0m  [56/84], [94mLoss[0m : 1.49356
[1mStep[0m  [64/84], [94mLoss[0m : 1.73462
[1mStep[0m  [72/84], [94mLoss[0m : 1.69192
[1mStep[0m  [80/84], [94mLoss[0m : 1.63258

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.623, [92mTest[0m: 2.538, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.42870
[1mStep[0m  [8/84], [94mLoss[0m : 1.51092
[1mStep[0m  [16/84], [94mLoss[0m : 1.44595
[1mStep[0m  [24/84], [94mLoss[0m : 1.47670
[1mStep[0m  [32/84], [94mLoss[0m : 1.48864
[1mStep[0m  [40/84], [94mLoss[0m : 1.52741
[1mStep[0m  [48/84], [94mLoss[0m : 1.43477
[1mStep[0m  [56/84], [94mLoss[0m : 1.58040
[1mStep[0m  [64/84], [94mLoss[0m : 1.62379
[1mStep[0m  [72/84], [94mLoss[0m : 1.85627
[1mStep[0m  [80/84], [94mLoss[0m : 1.55802

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.597, [92mTest[0m: 2.470, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.54584
[1mStep[0m  [8/84], [94mLoss[0m : 1.54522
[1mStep[0m  [16/84], [94mLoss[0m : 2.02258
[1mStep[0m  [24/84], [94mLoss[0m : 1.55816
[1mStep[0m  [32/84], [94mLoss[0m : 1.35770
[1mStep[0m  [40/84], [94mLoss[0m : 1.52664
[1mStep[0m  [48/84], [94mLoss[0m : 1.49873
[1mStep[0m  [56/84], [94mLoss[0m : 1.51609
[1mStep[0m  [64/84], [94mLoss[0m : 1.66266
[1mStep[0m  [72/84], [94mLoss[0m : 1.41951
[1mStep[0m  [80/84], [94mLoss[0m : 1.43608

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.562, [92mTest[0m: 2.513, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.28770
[1mStep[0m  [8/84], [94mLoss[0m : 1.52431
[1mStep[0m  [16/84], [94mLoss[0m : 1.53866
[1mStep[0m  [24/84], [94mLoss[0m : 1.52300
[1mStep[0m  [32/84], [94mLoss[0m : 1.51905
[1mStep[0m  [40/84], [94mLoss[0m : 1.58071
[1mStep[0m  [48/84], [94mLoss[0m : 1.57741
[1mStep[0m  [56/84], [94mLoss[0m : 1.62497
[1mStep[0m  [64/84], [94mLoss[0m : 1.33274
[1mStep[0m  [72/84], [94mLoss[0m : 1.81098
[1mStep[0m  [80/84], [94mLoss[0m : 1.46084

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.537, [92mTest[0m: 2.497, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.35412
[1mStep[0m  [8/84], [94mLoss[0m : 1.38136
[1mStep[0m  [16/84], [94mLoss[0m : 1.54881
[1mStep[0m  [24/84], [94mLoss[0m : 1.38733
[1mStep[0m  [32/84], [94mLoss[0m : 1.43430
[1mStep[0m  [40/84], [94mLoss[0m : 1.51482
[1mStep[0m  [48/84], [94mLoss[0m : 1.60075
[1mStep[0m  [56/84], [94mLoss[0m : 1.48888
[1mStep[0m  [64/84], [94mLoss[0m : 1.62364
[1mStep[0m  [72/84], [94mLoss[0m : 1.57563
[1mStep[0m  [80/84], [94mLoss[0m : 1.51577

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.504, [92mTest[0m: 2.548, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.48852
[1mStep[0m  [8/84], [94mLoss[0m : 1.33097
[1mStep[0m  [16/84], [94mLoss[0m : 1.53541
[1mStep[0m  [24/84], [94mLoss[0m : 1.54241
[1mStep[0m  [32/84], [94mLoss[0m : 1.52885
[1mStep[0m  [40/84], [94mLoss[0m : 1.63217
[1mStep[0m  [48/84], [94mLoss[0m : 1.53799
[1mStep[0m  [56/84], [94mLoss[0m : 1.41362
[1mStep[0m  [64/84], [94mLoss[0m : 1.39385
[1mStep[0m  [72/84], [94mLoss[0m : 1.54859
[1mStep[0m  [80/84], [94mLoss[0m : 1.53089

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.464, [92mTest[0m: 2.525, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.44835
[1mStep[0m  [8/84], [94mLoss[0m : 1.36734
[1mStep[0m  [16/84], [94mLoss[0m : 1.43899
[1mStep[0m  [24/84], [94mLoss[0m : 1.53302
[1mStep[0m  [32/84], [94mLoss[0m : 1.52085
[1mStep[0m  [40/84], [94mLoss[0m : 1.65554
[1mStep[0m  [48/84], [94mLoss[0m : 1.53865
[1mStep[0m  [56/84], [94mLoss[0m : 1.52801
[1mStep[0m  [64/84], [94mLoss[0m : 1.71548
[1mStep[0m  [72/84], [94mLoss[0m : 1.41596
[1mStep[0m  [80/84], [94mLoss[0m : 1.21099

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.459, [92mTest[0m: 2.584, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.54127
[1mStep[0m  [8/84], [94mLoss[0m : 1.42618
[1mStep[0m  [16/84], [94mLoss[0m : 1.50415
[1mStep[0m  [24/84], [94mLoss[0m : 1.41239
[1mStep[0m  [32/84], [94mLoss[0m : 1.67711
[1mStep[0m  [40/84], [94mLoss[0m : 1.65568
[1mStep[0m  [48/84], [94mLoss[0m : 1.21790
[1mStep[0m  [56/84], [94mLoss[0m : 1.36224
[1mStep[0m  [64/84], [94mLoss[0m : 1.50398
[1mStep[0m  [72/84], [94mLoss[0m : 1.35849
[1mStep[0m  [80/84], [94mLoss[0m : 1.32497

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.419, [92mTest[0m: 2.479, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.29412
[1mStep[0m  [8/84], [94mLoss[0m : 1.33078
[1mStep[0m  [16/84], [94mLoss[0m : 1.21802
[1mStep[0m  [24/84], [94mLoss[0m : 1.48987
[1mStep[0m  [32/84], [94mLoss[0m : 1.34597
[1mStep[0m  [40/84], [94mLoss[0m : 1.35209
[1mStep[0m  [48/84], [94mLoss[0m : 1.32822
[1mStep[0m  [56/84], [94mLoss[0m : 1.61149
[1mStep[0m  [64/84], [94mLoss[0m : 1.30846
[1mStep[0m  [72/84], [94mLoss[0m : 1.40791
[1mStep[0m  [80/84], [94mLoss[0m : 1.28920

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.403, [92mTest[0m: 2.532, [96mlr[0m: 0.006772500000000002
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 24 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.508
====================================

Phase 2 - Evaluation MAE:  2.5080782004765103
MAE score P1      2.351065
MAE score P2      2.508078
loss              1.402559
learning_rate     0.007525
batch_size             128
hidden_sizes         [250]
epochs                  30
activation            tanh
optimizer              sgd
early stopping        True
dropout                0.3
momentum               0.5
weight_decay        0.0001
Name: 20, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 10.67693
[1mStep[0m  [4/42], [94mLoss[0m : 10.48825
[1mStep[0m  [8/42], [94mLoss[0m : 10.23813
[1mStep[0m  [12/42], [94mLoss[0m : 9.59119
[1mStep[0m  [16/42], [94mLoss[0m : 8.52076
[1mStep[0m  [20/42], [94mLoss[0m : 8.15686
[1mStep[0m  [24/42], [94mLoss[0m : 7.86811
[1mStep[0m  [28/42], [94mLoss[0m : 7.12130
[1mStep[0m  [32/42], [94mLoss[0m : 6.79180
[1mStep[0m  [36/42], [94mLoss[0m : 6.09455
[1mStep[0m  [40/42], [94mLoss[0m : 5.34263

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 8.253, [92mTest[0m: 10.890, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.01034
[1mStep[0m  [4/42], [94mLoss[0m : 4.86062
[1mStep[0m  [8/42], [94mLoss[0m : 4.87408
[1mStep[0m  [12/42], [94mLoss[0m : 4.19135
[1mStep[0m  [16/42], [94mLoss[0m : 3.84123
[1mStep[0m  [20/42], [94mLoss[0m : 3.16924
[1mStep[0m  [24/42], [94mLoss[0m : 3.40612
[1mStep[0m  [28/42], [94mLoss[0m : 3.48910
[1mStep[0m  [32/42], [94mLoss[0m : 3.14030
[1mStep[0m  [36/42], [94mLoss[0m : 2.74885
[1mStep[0m  [40/42], [94mLoss[0m : 3.16763

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.766, [92mTest[0m: 6.587, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.27648
[1mStep[0m  [4/42], [94mLoss[0m : 2.90912
[1mStep[0m  [8/42], [94mLoss[0m : 2.74101
[1mStep[0m  [12/42], [94mLoss[0m : 2.87329
[1mStep[0m  [16/42], [94mLoss[0m : 2.77732
[1mStep[0m  [20/42], [94mLoss[0m : 2.67495
[1mStep[0m  [24/42], [94mLoss[0m : 2.68201
[1mStep[0m  [28/42], [94mLoss[0m : 2.55734
[1mStep[0m  [32/42], [94mLoss[0m : 2.77801
[1mStep[0m  [36/42], [94mLoss[0m : 2.61681
[1mStep[0m  [40/42], [94mLoss[0m : 2.76405

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.686, [92mTest[0m: 3.184, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39713
[1mStep[0m  [4/42], [94mLoss[0m : 2.70154
[1mStep[0m  [8/42], [94mLoss[0m : 2.36579
[1mStep[0m  [12/42], [94mLoss[0m : 2.47898
[1mStep[0m  [16/42], [94mLoss[0m : 2.61215
[1mStep[0m  [20/42], [94mLoss[0m : 2.73406
[1mStep[0m  [24/42], [94mLoss[0m : 2.35549
[1mStep[0m  [28/42], [94mLoss[0m : 2.48548
[1mStep[0m  [32/42], [94mLoss[0m : 2.53281
[1mStep[0m  [36/42], [94mLoss[0m : 2.63633
[1mStep[0m  [40/42], [94mLoss[0m : 2.45389

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.591, [92mTest[0m: 2.661, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.65027
[1mStep[0m  [4/42], [94mLoss[0m : 2.40333
[1mStep[0m  [8/42], [94mLoss[0m : 2.54763
[1mStep[0m  [12/42], [94mLoss[0m : 2.57290
[1mStep[0m  [16/42], [94mLoss[0m : 2.42611
[1mStep[0m  [20/42], [94mLoss[0m : 2.46774
[1mStep[0m  [24/42], [94mLoss[0m : 2.53947
[1mStep[0m  [28/42], [94mLoss[0m : 2.78279
[1mStep[0m  [32/42], [94mLoss[0m : 2.59793
[1mStep[0m  [36/42], [94mLoss[0m : 2.66411
[1mStep[0m  [40/42], [94mLoss[0m : 2.81651

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.562, [92mTest[0m: 2.528, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47861
[1mStep[0m  [4/42], [94mLoss[0m : 2.72837
[1mStep[0m  [8/42], [94mLoss[0m : 2.45993
[1mStep[0m  [12/42], [94mLoss[0m : 2.42494
[1mStep[0m  [16/42], [94mLoss[0m : 2.76329
[1mStep[0m  [20/42], [94mLoss[0m : 2.52805
[1mStep[0m  [24/42], [94mLoss[0m : 2.45825
[1mStep[0m  [28/42], [94mLoss[0m : 2.63279
[1mStep[0m  [32/42], [94mLoss[0m : 2.62780
[1mStep[0m  [36/42], [94mLoss[0m : 2.50103
[1mStep[0m  [40/42], [94mLoss[0m : 2.66231

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.547, [92mTest[0m: 2.525, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41334
[1mStep[0m  [4/42], [94mLoss[0m : 2.39265
[1mStep[0m  [8/42], [94mLoss[0m : 2.57493
[1mStep[0m  [12/42], [94mLoss[0m : 2.41676
[1mStep[0m  [16/42], [94mLoss[0m : 2.51462
[1mStep[0m  [20/42], [94mLoss[0m : 2.44720
[1mStep[0m  [24/42], [94mLoss[0m : 2.59506
[1mStep[0m  [28/42], [94mLoss[0m : 2.47925
[1mStep[0m  [32/42], [94mLoss[0m : 2.36750
[1mStep[0m  [36/42], [94mLoss[0m : 2.66385
[1mStep[0m  [40/42], [94mLoss[0m : 2.50168

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.488, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57706
[1mStep[0m  [4/42], [94mLoss[0m : 2.43383
[1mStep[0m  [8/42], [94mLoss[0m : 2.67173
[1mStep[0m  [12/42], [94mLoss[0m : 2.30338
[1mStep[0m  [16/42], [94mLoss[0m : 2.55173
[1mStep[0m  [20/42], [94mLoss[0m : 2.44093
[1mStep[0m  [24/42], [94mLoss[0m : 2.53541
[1mStep[0m  [28/42], [94mLoss[0m : 2.30385
[1mStep[0m  [32/42], [94mLoss[0m : 2.37197
[1mStep[0m  [36/42], [94mLoss[0m : 2.64113
[1mStep[0m  [40/42], [94mLoss[0m : 2.34486

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.486, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48420
[1mStep[0m  [4/42], [94mLoss[0m : 2.39570
[1mStep[0m  [8/42], [94mLoss[0m : 2.46799
[1mStep[0m  [12/42], [94mLoss[0m : 2.59720
[1mStep[0m  [16/42], [94mLoss[0m : 2.24624
[1mStep[0m  [20/42], [94mLoss[0m : 2.67790
[1mStep[0m  [24/42], [94mLoss[0m : 2.26666
[1mStep[0m  [28/42], [94mLoss[0m : 2.43387
[1mStep[0m  [32/42], [94mLoss[0m : 2.25266
[1mStep[0m  [36/42], [94mLoss[0m : 2.38949
[1mStep[0m  [40/42], [94mLoss[0m : 2.47195

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.426, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43789
[1mStep[0m  [4/42], [94mLoss[0m : 2.58049
[1mStep[0m  [8/42], [94mLoss[0m : 2.53436
[1mStep[0m  [12/42], [94mLoss[0m : 2.74642
[1mStep[0m  [16/42], [94mLoss[0m : 2.43071
[1mStep[0m  [20/42], [94mLoss[0m : 2.44223
[1mStep[0m  [24/42], [94mLoss[0m : 2.62533
[1mStep[0m  [28/42], [94mLoss[0m : 2.61954
[1mStep[0m  [32/42], [94mLoss[0m : 2.35035
[1mStep[0m  [36/42], [94mLoss[0m : 2.48391
[1mStep[0m  [40/42], [94mLoss[0m : 2.52988

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.481, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45784
[1mStep[0m  [4/42], [94mLoss[0m : 2.69887
[1mStep[0m  [8/42], [94mLoss[0m : 2.32488
[1mStep[0m  [12/42], [94mLoss[0m : 2.44852
[1mStep[0m  [16/42], [94mLoss[0m : 2.34164
[1mStep[0m  [20/42], [94mLoss[0m : 2.55217
[1mStep[0m  [24/42], [94mLoss[0m : 2.62819
[1mStep[0m  [28/42], [94mLoss[0m : 2.33164
[1mStep[0m  [32/42], [94mLoss[0m : 2.42843
[1mStep[0m  [36/42], [94mLoss[0m : 2.52867
[1mStep[0m  [40/42], [94mLoss[0m : 2.40425

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.406, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.75015
[1mStep[0m  [4/42], [94mLoss[0m : 2.43863
[1mStep[0m  [8/42], [94mLoss[0m : 2.27349
[1mStep[0m  [12/42], [94mLoss[0m : 2.16278
[1mStep[0m  [16/42], [94mLoss[0m : 2.63430
[1mStep[0m  [20/42], [94mLoss[0m : 2.54662
[1mStep[0m  [24/42], [94mLoss[0m : 2.41731
[1mStep[0m  [28/42], [94mLoss[0m : 2.50550
[1mStep[0m  [32/42], [94mLoss[0m : 2.22286
[1mStep[0m  [36/42], [94mLoss[0m : 2.38508
[1mStep[0m  [40/42], [94mLoss[0m : 2.50838

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.416, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40175
[1mStep[0m  [4/42], [94mLoss[0m : 2.38363
[1mStep[0m  [8/42], [94mLoss[0m : 2.45854
[1mStep[0m  [12/42], [94mLoss[0m : 2.43331
[1mStep[0m  [16/42], [94mLoss[0m : 2.31111
[1mStep[0m  [20/42], [94mLoss[0m : 2.57923
[1mStep[0m  [24/42], [94mLoss[0m : 2.59687
[1mStep[0m  [28/42], [94mLoss[0m : 2.84440
[1mStep[0m  [32/42], [94mLoss[0m : 2.53072
[1mStep[0m  [36/42], [94mLoss[0m : 2.45870
[1mStep[0m  [40/42], [94mLoss[0m : 2.40325

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.400, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.25144
[1mStep[0m  [4/42], [94mLoss[0m : 2.52845
[1mStep[0m  [8/42], [94mLoss[0m : 2.46221
[1mStep[0m  [12/42], [94mLoss[0m : 2.69220
[1mStep[0m  [16/42], [94mLoss[0m : 2.43711
[1mStep[0m  [20/42], [94mLoss[0m : 2.51009
[1mStep[0m  [24/42], [94mLoss[0m : 2.30280
[1mStep[0m  [28/42], [94mLoss[0m : 2.46571
[1mStep[0m  [32/42], [94mLoss[0m : 2.45858
[1mStep[0m  [36/42], [94mLoss[0m : 2.42983
[1mStep[0m  [40/42], [94mLoss[0m : 2.52967

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.476, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40783
[1mStep[0m  [4/42], [94mLoss[0m : 2.30199
[1mStep[0m  [8/42], [94mLoss[0m : 2.31722
[1mStep[0m  [12/42], [94mLoss[0m : 2.44288
[1mStep[0m  [16/42], [94mLoss[0m : 2.55682
[1mStep[0m  [20/42], [94mLoss[0m : 2.50301
[1mStep[0m  [24/42], [94mLoss[0m : 2.48330
[1mStep[0m  [28/42], [94mLoss[0m : 2.54232
[1mStep[0m  [32/42], [94mLoss[0m : 2.40149
[1mStep[0m  [36/42], [94mLoss[0m : 2.49611
[1mStep[0m  [40/42], [94mLoss[0m : 2.59746

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.487, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54712
[1mStep[0m  [4/42], [94mLoss[0m : 2.43564
[1mStep[0m  [8/42], [94mLoss[0m : 2.65337
[1mStep[0m  [12/42], [94mLoss[0m : 2.53176
[1mStep[0m  [16/42], [94mLoss[0m : 2.52671
[1mStep[0m  [20/42], [94mLoss[0m : 2.28284
[1mStep[0m  [24/42], [94mLoss[0m : 2.24759
[1mStep[0m  [28/42], [94mLoss[0m : 2.37239
[1mStep[0m  [32/42], [94mLoss[0m : 2.75561
[1mStep[0m  [36/42], [94mLoss[0m : 2.42237
[1mStep[0m  [40/42], [94mLoss[0m : 2.51055

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.456, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33145
[1mStep[0m  [4/42], [94mLoss[0m : 2.40486
[1mStep[0m  [8/42], [94mLoss[0m : 2.49024
[1mStep[0m  [12/42], [94mLoss[0m : 2.37896
[1mStep[0m  [16/42], [94mLoss[0m : 2.45103
[1mStep[0m  [20/42], [94mLoss[0m : 2.26838
[1mStep[0m  [24/42], [94mLoss[0m : 2.51461
[1mStep[0m  [28/42], [94mLoss[0m : 2.48820
[1mStep[0m  [32/42], [94mLoss[0m : 2.36439
[1mStep[0m  [36/42], [94mLoss[0m : 2.34809
[1mStep[0m  [40/42], [94mLoss[0m : 2.37310

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.408, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24080
[1mStep[0m  [4/42], [94mLoss[0m : 2.39406
[1mStep[0m  [8/42], [94mLoss[0m : 2.49583
[1mStep[0m  [12/42], [94mLoss[0m : 2.69750
[1mStep[0m  [16/42], [94mLoss[0m : 2.19853
[1mStep[0m  [20/42], [94mLoss[0m : 2.46691
[1mStep[0m  [24/42], [94mLoss[0m : 2.41144
[1mStep[0m  [28/42], [94mLoss[0m : 2.53976
[1mStep[0m  [32/42], [94mLoss[0m : 2.43362
[1mStep[0m  [36/42], [94mLoss[0m : 2.17199
[1mStep[0m  [40/42], [94mLoss[0m : 2.26451

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.400, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49819
[1mStep[0m  [4/42], [94mLoss[0m : 2.38262
[1mStep[0m  [8/42], [94mLoss[0m : 2.42240
[1mStep[0m  [12/42], [94mLoss[0m : 2.35785
[1mStep[0m  [16/42], [94mLoss[0m : 2.44431
[1mStep[0m  [20/42], [94mLoss[0m : 2.65896
[1mStep[0m  [24/42], [94mLoss[0m : 2.41710
[1mStep[0m  [28/42], [94mLoss[0m : 2.44199
[1mStep[0m  [32/42], [94mLoss[0m : 2.44065
[1mStep[0m  [36/42], [94mLoss[0m : 2.42259
[1mStep[0m  [40/42], [94mLoss[0m : 2.25245

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.441, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37754
[1mStep[0m  [4/42], [94mLoss[0m : 2.39978
[1mStep[0m  [8/42], [94mLoss[0m : 2.51566
[1mStep[0m  [12/42], [94mLoss[0m : 2.41691
[1mStep[0m  [16/42], [94mLoss[0m : 2.54934
[1mStep[0m  [20/42], [94mLoss[0m : 2.46808
[1mStep[0m  [24/42], [94mLoss[0m : 2.21653
[1mStep[0m  [28/42], [94mLoss[0m : 2.46381
[1mStep[0m  [32/42], [94mLoss[0m : 2.44946
[1mStep[0m  [36/42], [94mLoss[0m : 2.59030
[1mStep[0m  [40/42], [94mLoss[0m : 2.53792

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.392, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40237
[1mStep[0m  [4/42], [94mLoss[0m : 2.45988
[1mStep[0m  [8/42], [94mLoss[0m : 2.52451
[1mStep[0m  [12/42], [94mLoss[0m : 2.43126
[1mStep[0m  [16/42], [94mLoss[0m : 2.24581
[1mStep[0m  [20/42], [94mLoss[0m : 2.62405
[1mStep[0m  [24/42], [94mLoss[0m : 2.26457
[1mStep[0m  [28/42], [94mLoss[0m : 2.43231
[1mStep[0m  [32/42], [94mLoss[0m : 2.50495
[1mStep[0m  [36/42], [94mLoss[0m : 2.25853
[1mStep[0m  [40/42], [94mLoss[0m : 2.47868

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.353, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.25695
[1mStep[0m  [4/42], [94mLoss[0m : 2.43932
[1mStep[0m  [8/42], [94mLoss[0m : 2.48173
[1mStep[0m  [12/42], [94mLoss[0m : 2.49039
[1mStep[0m  [16/42], [94mLoss[0m : 2.40646
[1mStep[0m  [20/42], [94mLoss[0m : 2.49528
[1mStep[0m  [24/42], [94mLoss[0m : 2.47575
[1mStep[0m  [28/42], [94mLoss[0m : 2.52803
[1mStep[0m  [32/42], [94mLoss[0m : 2.16010
[1mStep[0m  [36/42], [94mLoss[0m : 2.32991
[1mStep[0m  [40/42], [94mLoss[0m : 2.39880

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.394, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37817
[1mStep[0m  [4/42], [94mLoss[0m : 2.61977
[1mStep[0m  [8/42], [94mLoss[0m : 2.45002
[1mStep[0m  [12/42], [94mLoss[0m : 2.21186
[1mStep[0m  [16/42], [94mLoss[0m : 2.40750
[1mStep[0m  [20/42], [94mLoss[0m : 2.55863
[1mStep[0m  [24/42], [94mLoss[0m : 2.38190
[1mStep[0m  [28/42], [94mLoss[0m : 2.37908
[1mStep[0m  [32/42], [94mLoss[0m : 2.36390
[1mStep[0m  [36/42], [94mLoss[0m : 2.41430
[1mStep[0m  [40/42], [94mLoss[0m : 2.35945

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.375, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40113
[1mStep[0m  [4/42], [94mLoss[0m : 2.44197
[1mStep[0m  [8/42], [94mLoss[0m : 2.27950
[1mStep[0m  [12/42], [94mLoss[0m : 2.51479
[1mStep[0m  [16/42], [94mLoss[0m : 2.36120
[1mStep[0m  [20/42], [94mLoss[0m : 2.47485
[1mStep[0m  [24/42], [94mLoss[0m : 2.41220
[1mStep[0m  [28/42], [94mLoss[0m : 2.38896
[1mStep[0m  [32/42], [94mLoss[0m : 2.30512
[1mStep[0m  [36/42], [94mLoss[0m : 2.32289
[1mStep[0m  [40/42], [94mLoss[0m : 2.64217

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.371, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.11658
[1mStep[0m  [4/42], [94mLoss[0m : 2.39329
[1mStep[0m  [8/42], [94mLoss[0m : 2.44516
[1mStep[0m  [12/42], [94mLoss[0m : 2.53135
[1mStep[0m  [16/42], [94mLoss[0m : 2.42002
[1mStep[0m  [20/42], [94mLoss[0m : 2.54779
[1mStep[0m  [24/42], [94mLoss[0m : 2.49562
[1mStep[0m  [28/42], [94mLoss[0m : 2.45694
[1mStep[0m  [32/42], [94mLoss[0m : 2.34714
[1mStep[0m  [36/42], [94mLoss[0m : 2.29603
[1mStep[0m  [40/42], [94mLoss[0m : 2.31904

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.400, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33435
[1mStep[0m  [4/42], [94mLoss[0m : 2.41290
[1mStep[0m  [8/42], [94mLoss[0m : 2.36731
[1mStep[0m  [12/42], [94mLoss[0m : 2.37640
[1mStep[0m  [16/42], [94mLoss[0m : 2.27770
[1mStep[0m  [20/42], [94mLoss[0m : 2.31554
[1mStep[0m  [24/42], [94mLoss[0m : 2.66937
[1mStep[0m  [28/42], [94mLoss[0m : 2.16285
[1mStep[0m  [32/42], [94mLoss[0m : 2.48452
[1mStep[0m  [36/42], [94mLoss[0m : 2.51378
[1mStep[0m  [40/42], [94mLoss[0m : 2.43461

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.403, [92mTest[0m: 2.381, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41920
[1mStep[0m  [4/42], [94mLoss[0m : 2.61214
[1mStep[0m  [8/42], [94mLoss[0m : 2.59912
[1mStep[0m  [12/42], [94mLoss[0m : 2.40350
[1mStep[0m  [16/42], [94mLoss[0m : 2.19025
[1mStep[0m  [20/42], [94mLoss[0m : 2.42673
[1mStep[0m  [24/42], [94mLoss[0m : 2.11804
[1mStep[0m  [28/42], [94mLoss[0m : 2.38175
[1mStep[0m  [32/42], [94mLoss[0m : 2.74985
[1mStep[0m  [36/42], [94mLoss[0m : 2.48353
[1mStep[0m  [40/42], [94mLoss[0m : 2.44222

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.363, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.66024
[1mStep[0m  [4/42], [94mLoss[0m : 2.40613
[1mStep[0m  [8/42], [94mLoss[0m : 2.09631
[1mStep[0m  [12/42], [94mLoss[0m : 2.26536
[1mStep[0m  [16/42], [94mLoss[0m : 2.33390
[1mStep[0m  [20/42], [94mLoss[0m : 2.49750
[1mStep[0m  [24/42], [94mLoss[0m : 2.31887
[1mStep[0m  [28/42], [94mLoss[0m : 2.29144
[1mStep[0m  [32/42], [94mLoss[0m : 2.13863
[1mStep[0m  [36/42], [94mLoss[0m : 2.29725
[1mStep[0m  [40/42], [94mLoss[0m : 2.50585

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.396, [92mTest[0m: 2.375, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58657
[1mStep[0m  [4/42], [94mLoss[0m : 2.14138
[1mStep[0m  [8/42], [94mLoss[0m : 2.31152
[1mStep[0m  [12/42], [94mLoss[0m : 2.29970
[1mStep[0m  [16/42], [94mLoss[0m : 2.60277
[1mStep[0m  [20/42], [94mLoss[0m : 2.37129
[1mStep[0m  [24/42], [94mLoss[0m : 2.59850
[1mStep[0m  [28/42], [94mLoss[0m : 2.30882
[1mStep[0m  [32/42], [94mLoss[0m : 2.34480
[1mStep[0m  [36/42], [94mLoss[0m : 2.18171
[1mStep[0m  [40/42], [94mLoss[0m : 2.48312

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.389, [92mTest[0m: 2.379, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34251
[1mStep[0m  [4/42], [94mLoss[0m : 2.32922
[1mStep[0m  [8/42], [94mLoss[0m : 2.35579
[1mStep[0m  [12/42], [94mLoss[0m : 2.53193
[1mStep[0m  [16/42], [94mLoss[0m : 2.45657
[1mStep[0m  [20/42], [94mLoss[0m : 2.21540
[1mStep[0m  [24/42], [94mLoss[0m : 2.47213
[1mStep[0m  [28/42], [94mLoss[0m : 2.53571
[1mStep[0m  [32/42], [94mLoss[0m : 2.41872
[1mStep[0m  [36/42], [94mLoss[0m : 2.37868
[1mStep[0m  [40/42], [94mLoss[0m : 2.34603

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.384, [92mTest[0m: 2.351, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.366
====================================

Phase 1 - Evaluation MAE:  2.366076946258545
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 2.48353
[1mStep[0m  [4/42], [94mLoss[0m : 2.43591
[1mStep[0m  [8/42], [94mLoss[0m : 2.30227
[1mStep[0m  [12/42], [94mLoss[0m : 2.58227
[1mStep[0m  [16/42], [94mLoss[0m : 2.49539
[1mStep[0m  [20/42], [94mLoss[0m : 2.54994
[1mStep[0m  [24/42], [94mLoss[0m : 2.75244
[1mStep[0m  [28/42], [94mLoss[0m : 2.42931
[1mStep[0m  [32/42], [94mLoss[0m : 2.52260
[1mStep[0m  [36/42], [94mLoss[0m : 2.27208
[1mStep[0m  [40/42], [94mLoss[0m : 2.43943

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.369, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43658
[1mStep[0m  [4/42], [94mLoss[0m : 2.30763
[1mStep[0m  [8/42], [94mLoss[0m : 2.44154
[1mStep[0m  [12/42], [94mLoss[0m : 2.53480
[1mStep[0m  [16/42], [94mLoss[0m : 2.56879
[1mStep[0m  [20/42], [94mLoss[0m : 2.49790
[1mStep[0m  [24/42], [94mLoss[0m : 2.37193
[1mStep[0m  [28/42], [94mLoss[0m : 2.37924
[1mStep[0m  [32/42], [94mLoss[0m : 2.44552
[1mStep[0m  [36/42], [94mLoss[0m : 2.37151
[1mStep[0m  [40/42], [94mLoss[0m : 2.54788

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.415, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.20900
[1mStep[0m  [4/42], [94mLoss[0m : 2.49204
[1mStep[0m  [8/42], [94mLoss[0m : 2.22404
[1mStep[0m  [12/42], [94mLoss[0m : 2.27540
[1mStep[0m  [16/42], [94mLoss[0m : 2.41489
[1mStep[0m  [20/42], [94mLoss[0m : 2.56403
[1mStep[0m  [24/42], [94mLoss[0m : 2.27031
[1mStep[0m  [28/42], [94mLoss[0m : 2.29264
[1mStep[0m  [32/42], [94mLoss[0m : 2.55388
[1mStep[0m  [36/42], [94mLoss[0m : 2.20114
[1mStep[0m  [40/42], [94mLoss[0m : 2.39366

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.386, [92mTest[0m: 2.451, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49716
[1mStep[0m  [4/42], [94mLoss[0m : 2.25111
[1mStep[0m  [8/42], [94mLoss[0m : 2.50411
[1mStep[0m  [12/42], [94mLoss[0m : 2.18407
[1mStep[0m  [16/42], [94mLoss[0m : 2.30088
[1mStep[0m  [20/42], [94mLoss[0m : 2.59518
[1mStep[0m  [24/42], [94mLoss[0m : 2.38223
[1mStep[0m  [28/42], [94mLoss[0m : 2.16767
[1mStep[0m  [32/42], [94mLoss[0m : 2.13939
[1mStep[0m  [36/42], [94mLoss[0m : 2.16980
[1mStep[0m  [40/42], [94mLoss[0m : 2.17199

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.342, [92mTest[0m: 2.381, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28845
[1mStep[0m  [4/42], [94mLoss[0m : 2.35265
[1mStep[0m  [8/42], [94mLoss[0m : 2.16866
[1mStep[0m  [12/42], [94mLoss[0m : 2.34459
[1mStep[0m  [16/42], [94mLoss[0m : 2.33308
[1mStep[0m  [20/42], [94mLoss[0m : 2.50016
[1mStep[0m  [24/42], [94mLoss[0m : 2.29846
[1mStep[0m  [28/42], [94mLoss[0m : 2.21889
[1mStep[0m  [32/42], [94mLoss[0m : 2.16593
[1mStep[0m  [36/42], [94mLoss[0m : 2.37035
[1mStep[0m  [40/42], [94mLoss[0m : 2.33316

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.314, [92mTest[0m: 2.398, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46354
[1mStep[0m  [4/42], [94mLoss[0m : 2.37558
[1mStep[0m  [8/42], [94mLoss[0m : 2.30106
[1mStep[0m  [12/42], [94mLoss[0m : 2.30552
[1mStep[0m  [16/42], [94mLoss[0m : 2.15694
[1mStep[0m  [20/42], [94mLoss[0m : 2.34642
[1mStep[0m  [24/42], [94mLoss[0m : 2.27196
[1mStep[0m  [28/42], [94mLoss[0m : 2.23858
[1mStep[0m  [32/42], [94mLoss[0m : 2.26581
[1mStep[0m  [36/42], [94mLoss[0m : 2.30090
[1mStep[0m  [40/42], [94mLoss[0m : 2.46467

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.289, [92mTest[0m: 2.400, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38381
[1mStep[0m  [4/42], [94mLoss[0m : 2.39855
[1mStep[0m  [8/42], [94mLoss[0m : 2.24563
[1mStep[0m  [12/42], [94mLoss[0m : 2.23610
[1mStep[0m  [16/42], [94mLoss[0m : 2.34374
[1mStep[0m  [20/42], [94mLoss[0m : 2.46783
[1mStep[0m  [24/42], [94mLoss[0m : 2.35898
[1mStep[0m  [28/42], [94mLoss[0m : 2.22139
[1mStep[0m  [32/42], [94mLoss[0m : 2.23074
[1mStep[0m  [36/42], [94mLoss[0m : 2.19337
[1mStep[0m  [40/42], [94mLoss[0m : 2.26592

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.225, [92mTest[0m: 2.403, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.18526
[1mStep[0m  [4/42], [94mLoss[0m : 2.24838
[1mStep[0m  [8/42], [94mLoss[0m : 2.03386
[1mStep[0m  [12/42], [94mLoss[0m : 2.19446
[1mStep[0m  [16/42], [94mLoss[0m : 2.09960
[1mStep[0m  [20/42], [94mLoss[0m : 2.22817
[1mStep[0m  [24/42], [94mLoss[0m : 2.37864
[1mStep[0m  [28/42], [94mLoss[0m : 2.03868
[1mStep[0m  [32/42], [94mLoss[0m : 2.29705
[1mStep[0m  [36/42], [94mLoss[0m : 2.32298
[1mStep[0m  [40/42], [94mLoss[0m : 2.03922

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.193, [92mTest[0m: 2.428, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.10111
[1mStep[0m  [4/42], [94mLoss[0m : 2.20393
[1mStep[0m  [8/42], [94mLoss[0m : 2.18818
[1mStep[0m  [12/42], [94mLoss[0m : 2.08724
[1mStep[0m  [16/42], [94mLoss[0m : 1.93844
[1mStep[0m  [20/42], [94mLoss[0m : 1.95641
[1mStep[0m  [24/42], [94mLoss[0m : 2.08860
[1mStep[0m  [28/42], [94mLoss[0m : 2.11867
[1mStep[0m  [32/42], [94mLoss[0m : 2.07873
[1mStep[0m  [36/42], [94mLoss[0m : 2.10425
[1mStep[0m  [40/42], [94mLoss[0m : 2.16727

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.119, [92mTest[0m: 2.427, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.11533
[1mStep[0m  [4/42], [94mLoss[0m : 1.86749
[1mStep[0m  [8/42], [94mLoss[0m : 2.11266
[1mStep[0m  [12/42], [94mLoss[0m : 1.97434
[1mStep[0m  [16/42], [94mLoss[0m : 2.06324
[1mStep[0m  [20/42], [94mLoss[0m : 2.13755
[1mStep[0m  [24/42], [94mLoss[0m : 2.06989
[1mStep[0m  [28/42], [94mLoss[0m : 2.17827
[1mStep[0m  [32/42], [94mLoss[0m : 2.12571
[1mStep[0m  [36/42], [94mLoss[0m : 2.23306
[1mStep[0m  [40/42], [94mLoss[0m : 2.22359

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.111, [92mTest[0m: 2.449, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.08824
[1mStep[0m  [4/42], [94mLoss[0m : 1.94643
[1mStep[0m  [8/42], [94mLoss[0m : 1.98754
[1mStep[0m  [12/42], [94mLoss[0m : 2.19465
[1mStep[0m  [16/42], [94mLoss[0m : 2.11958
[1mStep[0m  [20/42], [94mLoss[0m : 2.24991
[1mStep[0m  [24/42], [94mLoss[0m : 2.04695
[1mStep[0m  [28/42], [94mLoss[0m : 2.23574
[1mStep[0m  [32/42], [94mLoss[0m : 2.33557
[1mStep[0m  [36/42], [94mLoss[0m : 2.13286
[1mStep[0m  [40/42], [94mLoss[0m : 2.10615

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.072, [92mTest[0m: 2.414, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.18605
[1mStep[0m  [4/42], [94mLoss[0m : 1.98002
[1mStep[0m  [8/42], [94mLoss[0m : 2.23684
[1mStep[0m  [12/42], [94mLoss[0m : 2.15028
[1mStep[0m  [16/42], [94mLoss[0m : 1.98305
[1mStep[0m  [20/42], [94mLoss[0m : 1.85409
[1mStep[0m  [24/42], [94mLoss[0m : 2.13111
[1mStep[0m  [28/42], [94mLoss[0m : 2.17502
[1mStep[0m  [32/42], [94mLoss[0m : 2.19178
[1mStep[0m  [36/42], [94mLoss[0m : 1.94601
[1mStep[0m  [40/42], [94mLoss[0m : 1.88297

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.034, [92mTest[0m: 2.423, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.04771
[1mStep[0m  [4/42], [94mLoss[0m : 2.01480
[1mStep[0m  [8/42], [94mLoss[0m : 2.04119
[1mStep[0m  [12/42], [94mLoss[0m : 1.97255
[1mStep[0m  [16/42], [94mLoss[0m : 1.94175
[1mStep[0m  [20/42], [94mLoss[0m : 1.90350
[1mStep[0m  [24/42], [94mLoss[0m : 2.08383
[1mStep[0m  [28/42], [94mLoss[0m : 2.17397
[1mStep[0m  [32/42], [94mLoss[0m : 1.96266
[1mStep[0m  [36/42], [94mLoss[0m : 1.70962
[1mStep[0m  [40/42], [94mLoss[0m : 1.97582

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.001, [92mTest[0m: 2.470, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.87190
[1mStep[0m  [4/42], [94mLoss[0m : 2.01465
[1mStep[0m  [8/42], [94mLoss[0m : 2.04222
[1mStep[0m  [12/42], [94mLoss[0m : 2.08144
[1mStep[0m  [16/42], [94mLoss[0m : 1.88691
[1mStep[0m  [20/42], [94mLoss[0m : 2.01321
[1mStep[0m  [24/42], [94mLoss[0m : 2.06001
[1mStep[0m  [28/42], [94mLoss[0m : 1.86851
[1mStep[0m  [32/42], [94mLoss[0m : 1.98902
[1mStep[0m  [36/42], [94mLoss[0m : 2.12376
[1mStep[0m  [40/42], [94mLoss[0m : 1.90392

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.969, [92mTest[0m: 2.425, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.99547
[1mStep[0m  [4/42], [94mLoss[0m : 1.88670
[1mStep[0m  [8/42], [94mLoss[0m : 1.89089
[1mStep[0m  [12/42], [94mLoss[0m : 1.93935
[1mStep[0m  [16/42], [94mLoss[0m : 1.95244
[1mStep[0m  [20/42], [94mLoss[0m : 1.90089
[1mStep[0m  [24/42], [94mLoss[0m : 1.93618
[1mStep[0m  [28/42], [94mLoss[0m : 1.82195
[1mStep[0m  [32/42], [94mLoss[0m : 1.96687
[1mStep[0m  [36/42], [94mLoss[0m : 2.34148
[1mStep[0m  [40/42], [94mLoss[0m : 1.82478

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.923, [92mTest[0m: 2.430, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.81530
[1mStep[0m  [4/42], [94mLoss[0m : 1.90887
[1mStep[0m  [8/42], [94mLoss[0m : 1.71330
[1mStep[0m  [12/42], [94mLoss[0m : 2.05593
[1mStep[0m  [16/42], [94mLoss[0m : 2.08633
[1mStep[0m  [20/42], [94mLoss[0m : 1.86556
[1mStep[0m  [24/42], [94mLoss[0m : 1.90142
[1mStep[0m  [28/42], [94mLoss[0m : 1.85890
[1mStep[0m  [32/42], [94mLoss[0m : 1.92489
[1mStep[0m  [36/42], [94mLoss[0m : 1.75078
[1mStep[0m  [40/42], [94mLoss[0m : 1.84668

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.894, [92mTest[0m: 2.436, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.76889
[1mStep[0m  [4/42], [94mLoss[0m : 1.83252
[1mStep[0m  [8/42], [94mLoss[0m : 1.98020
[1mStep[0m  [12/42], [94mLoss[0m : 1.99133
[1mStep[0m  [16/42], [94mLoss[0m : 1.70425
[1mStep[0m  [20/42], [94mLoss[0m : 1.77472
[1mStep[0m  [24/42], [94mLoss[0m : 1.85852
[1mStep[0m  [28/42], [94mLoss[0m : 1.71314
[1mStep[0m  [32/42], [94mLoss[0m : 1.90457
[1mStep[0m  [36/42], [94mLoss[0m : 1.97456
[1mStep[0m  [40/42], [94mLoss[0m : 2.08577

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.846, [92mTest[0m: 2.485, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.88536
[1mStep[0m  [4/42], [94mLoss[0m : 1.81925
[1mStep[0m  [8/42], [94mLoss[0m : 1.70016
[1mStep[0m  [12/42], [94mLoss[0m : 1.90536
[1mStep[0m  [16/42], [94mLoss[0m : 1.67251
[1mStep[0m  [20/42], [94mLoss[0m : 1.78892
[1mStep[0m  [24/42], [94mLoss[0m : 1.90018
[1mStep[0m  [28/42], [94mLoss[0m : 1.82686
[1mStep[0m  [32/42], [94mLoss[0m : 1.80817
[1mStep[0m  [36/42], [94mLoss[0m : 1.83727
[1mStep[0m  [40/42], [94mLoss[0m : 1.73360

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.824, [92mTest[0m: 2.454, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.79581
[1mStep[0m  [4/42], [94mLoss[0m : 1.73262
[1mStep[0m  [8/42], [94mLoss[0m : 1.77841
[1mStep[0m  [12/42], [94mLoss[0m : 1.98453
[1mStep[0m  [16/42], [94mLoss[0m : 1.78422
[1mStep[0m  [20/42], [94mLoss[0m : 1.79531
[1mStep[0m  [24/42], [94mLoss[0m : 1.75763
[1mStep[0m  [28/42], [94mLoss[0m : 1.85092
[1mStep[0m  [32/42], [94mLoss[0m : 1.62275
[1mStep[0m  [36/42], [94mLoss[0m : 1.90510
[1mStep[0m  [40/42], [94mLoss[0m : 1.70020

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.786, [92mTest[0m: 2.465, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.92261
[1mStep[0m  [4/42], [94mLoss[0m : 1.84642
[1mStep[0m  [8/42], [94mLoss[0m : 1.78221
[1mStep[0m  [12/42], [94mLoss[0m : 1.84615
[1mStep[0m  [16/42], [94mLoss[0m : 1.70939
[1mStep[0m  [20/42], [94mLoss[0m : 1.76766
[1mStep[0m  [24/42], [94mLoss[0m : 1.79160
[1mStep[0m  [28/42], [94mLoss[0m : 1.81024
[1mStep[0m  [32/42], [94mLoss[0m : 1.63457
[1mStep[0m  [36/42], [94mLoss[0m : 1.61257
[1mStep[0m  [40/42], [94mLoss[0m : 1.69447

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.762, [92mTest[0m: 2.496, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.76423
[1mStep[0m  [4/42], [94mLoss[0m : 1.76345
[1mStep[0m  [8/42], [94mLoss[0m : 1.68448
[1mStep[0m  [12/42], [94mLoss[0m : 1.69969
[1mStep[0m  [16/42], [94mLoss[0m : 1.89762
[1mStep[0m  [20/42], [94mLoss[0m : 1.74531
[1mStep[0m  [24/42], [94mLoss[0m : 1.58516
[1mStep[0m  [28/42], [94mLoss[0m : 1.79342
[1mStep[0m  [32/42], [94mLoss[0m : 1.78041
[1mStep[0m  [36/42], [94mLoss[0m : 1.61846
[1mStep[0m  [40/42], [94mLoss[0m : 1.69076

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.699, [92mTest[0m: 2.458, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.70780
[1mStep[0m  [4/42], [94mLoss[0m : 1.67055
[1mStep[0m  [8/42], [94mLoss[0m : 1.77587
[1mStep[0m  [12/42], [94mLoss[0m : 1.69028
[1mStep[0m  [16/42], [94mLoss[0m : 1.66189
[1mStep[0m  [20/42], [94mLoss[0m : 1.71384
[1mStep[0m  [24/42], [94mLoss[0m : 1.78661
[1mStep[0m  [28/42], [94mLoss[0m : 1.66267
[1mStep[0m  [32/42], [94mLoss[0m : 1.74735
[1mStep[0m  [36/42], [94mLoss[0m : 1.77433
[1mStep[0m  [40/42], [94mLoss[0m : 1.65619

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.713, [92mTest[0m: 2.491, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.74147
[1mStep[0m  [4/42], [94mLoss[0m : 1.54166
[1mStep[0m  [8/42], [94mLoss[0m : 1.75124
[1mStep[0m  [12/42], [94mLoss[0m : 1.62677
[1mStep[0m  [16/42], [94mLoss[0m : 1.77459
[1mStep[0m  [20/42], [94mLoss[0m : 1.48812
[1mStep[0m  [24/42], [94mLoss[0m : 1.87931
[1mStep[0m  [28/42], [94mLoss[0m : 1.65101
[1mStep[0m  [32/42], [94mLoss[0m : 1.61247
[1mStep[0m  [36/42], [94mLoss[0m : 1.55859
[1mStep[0m  [40/42], [94mLoss[0m : 1.55987

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.671, [92mTest[0m: 2.500, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.52227
[1mStep[0m  [4/42], [94mLoss[0m : 1.76940
[1mStep[0m  [8/42], [94mLoss[0m : 1.71988
[1mStep[0m  [12/42], [94mLoss[0m : 1.83760
[1mStep[0m  [16/42], [94mLoss[0m : 1.58793
[1mStep[0m  [20/42], [94mLoss[0m : 1.66290
[1mStep[0m  [24/42], [94mLoss[0m : 1.70034
[1mStep[0m  [28/42], [94mLoss[0m : 1.66728
[1mStep[0m  [32/42], [94mLoss[0m : 1.56480
[1mStep[0m  [36/42], [94mLoss[0m : 1.62224
[1mStep[0m  [40/42], [94mLoss[0m : 1.66995

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.647, [92mTest[0m: 2.533, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.70893
[1mStep[0m  [4/42], [94mLoss[0m : 1.64174
[1mStep[0m  [8/42], [94mLoss[0m : 1.55242
[1mStep[0m  [12/42], [94mLoss[0m : 1.80929
[1mStep[0m  [16/42], [94mLoss[0m : 1.55375
[1mStep[0m  [20/42], [94mLoss[0m : 1.62723
[1mStep[0m  [24/42], [94mLoss[0m : 1.60114
[1mStep[0m  [28/42], [94mLoss[0m : 1.74192
[1mStep[0m  [32/42], [94mLoss[0m : 1.71580
[1mStep[0m  [36/42], [94mLoss[0m : 1.62142
[1mStep[0m  [40/42], [94mLoss[0m : 1.57170

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.630, [92mTest[0m: 2.508, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.60445
[1mStep[0m  [4/42], [94mLoss[0m : 1.68549
[1mStep[0m  [8/42], [94mLoss[0m : 1.46232
[1mStep[0m  [12/42], [94mLoss[0m : 1.57378
[1mStep[0m  [16/42], [94mLoss[0m : 1.49355
[1mStep[0m  [20/42], [94mLoss[0m : 1.55204
[1mStep[0m  [24/42], [94mLoss[0m : 1.50303
[1mStep[0m  [28/42], [94mLoss[0m : 1.72720
[1mStep[0m  [32/42], [94mLoss[0m : 1.59932
[1mStep[0m  [36/42], [94mLoss[0m : 1.66495
[1mStep[0m  [40/42], [94mLoss[0m : 1.69657

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.601, [92mTest[0m: 2.507, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.45399
[1mStep[0m  [4/42], [94mLoss[0m : 1.48370
[1mStep[0m  [8/42], [94mLoss[0m : 1.45871
[1mStep[0m  [12/42], [94mLoss[0m : 1.57742
[1mStep[0m  [16/42], [94mLoss[0m : 1.37868
[1mStep[0m  [20/42], [94mLoss[0m : 1.55782
[1mStep[0m  [24/42], [94mLoss[0m : 1.44334
[1mStep[0m  [28/42], [94mLoss[0m : 1.67226
[1mStep[0m  [32/42], [94mLoss[0m : 1.55330
[1mStep[0m  [36/42], [94mLoss[0m : 1.66697
[1mStep[0m  [40/42], [94mLoss[0m : 1.45898

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.584, [92mTest[0m: 2.493, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.65989
[1mStep[0m  [4/42], [94mLoss[0m : 1.35103
[1mStep[0m  [8/42], [94mLoss[0m : 1.64830
[1mStep[0m  [12/42], [94mLoss[0m : 1.55464
[1mStep[0m  [16/42], [94mLoss[0m : 1.51704
[1mStep[0m  [20/42], [94mLoss[0m : 1.54224
[1mStep[0m  [24/42], [94mLoss[0m : 1.60419
[1mStep[0m  [28/42], [94mLoss[0m : 1.59653
[1mStep[0m  [32/42], [94mLoss[0m : 1.54685
[1mStep[0m  [36/42], [94mLoss[0m : 1.65566
[1mStep[0m  [40/42], [94mLoss[0m : 1.58820

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.574, [92mTest[0m: 2.478, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.47543
[1mStep[0m  [4/42], [94mLoss[0m : 1.51627
[1mStep[0m  [8/42], [94mLoss[0m : 1.64548
[1mStep[0m  [12/42], [94mLoss[0m : 1.48717
[1mStep[0m  [16/42], [94mLoss[0m : 1.57655
[1mStep[0m  [20/42], [94mLoss[0m : 1.61519
[1mStep[0m  [24/42], [94mLoss[0m : 1.52498
[1mStep[0m  [28/42], [94mLoss[0m : 1.37559
[1mStep[0m  [32/42], [94mLoss[0m : 1.54178
[1mStep[0m  [36/42], [94mLoss[0m : 1.47422
[1mStep[0m  [40/42], [94mLoss[0m : 1.62710

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.539, [92mTest[0m: 2.562, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.57765
[1mStep[0m  [4/42], [94mLoss[0m : 1.52401
[1mStep[0m  [8/42], [94mLoss[0m : 1.44200
[1mStep[0m  [12/42], [94mLoss[0m : 1.45648
[1mStep[0m  [16/42], [94mLoss[0m : 1.49377
[1mStep[0m  [20/42], [94mLoss[0m : 1.55997
[1mStep[0m  [24/42], [94mLoss[0m : 1.53037
[1mStep[0m  [28/42], [94mLoss[0m : 1.56852
[1mStep[0m  [32/42], [94mLoss[0m : 1.73112
[1mStep[0m  [36/42], [94mLoss[0m : 1.43959
[1mStep[0m  [40/42], [94mLoss[0m : 1.66454

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.535, [92mTest[0m: 2.487, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.510
====================================

Phase 2 - Evaluation MAE:  2.5098535673958913
MAE score P1      2.366077
MAE score P2      2.509854
loss              1.534966
learning_rate     0.007525
batch_size             256
hidden_sizes         [100]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.1
weight_decay        0.0001
Name: 21, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 11.32008
[1mStep[0m  [8/84], [94mLoss[0m : 10.65094
[1mStep[0m  [16/84], [94mLoss[0m : 10.16995
[1mStep[0m  [24/84], [94mLoss[0m : 9.66280
[1mStep[0m  [32/84], [94mLoss[0m : 8.28107
[1mStep[0m  [40/84], [94mLoss[0m : 7.69779
[1mStep[0m  [48/84], [94mLoss[0m : 5.97035
[1mStep[0m  [56/84], [94mLoss[0m : 5.08890
[1mStep[0m  [64/84], [94mLoss[0m : 4.49604
[1mStep[0m  [72/84], [94mLoss[0m : 3.36095
[1mStep[0m  [80/84], [94mLoss[0m : 2.62609

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 7.145, [92mTest[0m: 11.006, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.84604
[1mStep[0m  [8/84], [94mLoss[0m : 2.81159
[1mStep[0m  [16/84], [94mLoss[0m : 2.48110
[1mStep[0m  [24/84], [94mLoss[0m : 2.74958
[1mStep[0m  [32/84], [94mLoss[0m : 2.57970
[1mStep[0m  [40/84], [94mLoss[0m : 2.40150
[1mStep[0m  [48/84], [94mLoss[0m : 2.54944
[1mStep[0m  [56/84], [94mLoss[0m : 2.71180
[1mStep[0m  [64/84], [94mLoss[0m : 2.59027
[1mStep[0m  [72/84], [94mLoss[0m : 2.44171
[1mStep[0m  [80/84], [94mLoss[0m : 2.95187

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.693, [92mTest[0m: 2.558, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55485
[1mStep[0m  [8/84], [94mLoss[0m : 2.59473
[1mStep[0m  [16/84], [94mLoss[0m : 2.48319
[1mStep[0m  [24/84], [94mLoss[0m : 2.54512
[1mStep[0m  [32/84], [94mLoss[0m : 2.80466
[1mStep[0m  [40/84], [94mLoss[0m : 2.75258
[1mStep[0m  [48/84], [94mLoss[0m : 2.54201
[1mStep[0m  [56/84], [94mLoss[0m : 2.44307
[1mStep[0m  [64/84], [94mLoss[0m : 2.66274
[1mStep[0m  [72/84], [94mLoss[0m : 2.73163
[1mStep[0m  [80/84], [94mLoss[0m : 2.78629

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.558, [92mTest[0m: 2.419, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41673
[1mStep[0m  [8/84], [94mLoss[0m : 2.21299
[1mStep[0m  [16/84], [94mLoss[0m : 2.48333
[1mStep[0m  [24/84], [94mLoss[0m : 2.63304
[1mStep[0m  [32/84], [94mLoss[0m : 2.34967
[1mStep[0m  [40/84], [94mLoss[0m : 2.53234
[1mStep[0m  [48/84], [94mLoss[0m : 2.43743
[1mStep[0m  [56/84], [94mLoss[0m : 2.71529
[1mStep[0m  [64/84], [94mLoss[0m : 2.43639
[1mStep[0m  [72/84], [94mLoss[0m : 2.67474
[1mStep[0m  [80/84], [94mLoss[0m : 2.67939

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.356, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67490
[1mStep[0m  [8/84], [94mLoss[0m : 2.66126
[1mStep[0m  [16/84], [94mLoss[0m : 2.60608
[1mStep[0m  [24/84], [94mLoss[0m : 2.51436
[1mStep[0m  [32/84], [94mLoss[0m : 2.44592
[1mStep[0m  [40/84], [94mLoss[0m : 2.52238
[1mStep[0m  [48/84], [94mLoss[0m : 2.25866
[1mStep[0m  [56/84], [94mLoss[0m : 2.65560
[1mStep[0m  [64/84], [94mLoss[0m : 2.64619
[1mStep[0m  [72/84], [94mLoss[0m : 2.35288
[1mStep[0m  [80/84], [94mLoss[0m : 2.54716

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.349, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.63706
[1mStep[0m  [8/84], [94mLoss[0m : 2.65186
[1mStep[0m  [16/84], [94mLoss[0m : 2.35633
[1mStep[0m  [24/84], [94mLoss[0m : 2.41289
[1mStep[0m  [32/84], [94mLoss[0m : 2.42034
[1mStep[0m  [40/84], [94mLoss[0m : 2.38405
[1mStep[0m  [48/84], [94mLoss[0m : 2.89094
[1mStep[0m  [56/84], [94mLoss[0m : 2.50740
[1mStep[0m  [64/84], [94mLoss[0m : 2.67284
[1mStep[0m  [72/84], [94mLoss[0m : 2.66524
[1mStep[0m  [80/84], [94mLoss[0m : 2.46783

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.349, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.38497
[1mStep[0m  [8/84], [94mLoss[0m : 2.47552
[1mStep[0m  [16/84], [94mLoss[0m : 2.50653
[1mStep[0m  [24/84], [94mLoss[0m : 2.36068
[1mStep[0m  [32/84], [94mLoss[0m : 2.62170
[1mStep[0m  [40/84], [94mLoss[0m : 2.46721
[1mStep[0m  [48/84], [94mLoss[0m : 2.47064
[1mStep[0m  [56/84], [94mLoss[0m : 2.48599
[1mStep[0m  [64/84], [94mLoss[0m : 2.41840
[1mStep[0m  [72/84], [94mLoss[0m : 2.47934
[1mStep[0m  [80/84], [94mLoss[0m : 2.49842

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.333, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.71013
[1mStep[0m  [8/84], [94mLoss[0m : 2.34510
[1mStep[0m  [16/84], [94mLoss[0m : 2.69503
[1mStep[0m  [24/84], [94mLoss[0m : 2.04140
[1mStep[0m  [32/84], [94mLoss[0m : 2.32310
[1mStep[0m  [40/84], [94mLoss[0m : 2.42618
[1mStep[0m  [48/84], [94mLoss[0m : 2.31949
[1mStep[0m  [56/84], [94mLoss[0m : 2.17616
[1mStep[0m  [64/84], [94mLoss[0m : 2.58987
[1mStep[0m  [72/84], [94mLoss[0m : 2.41945
[1mStep[0m  [80/84], [94mLoss[0m : 2.57281

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.328, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67298
[1mStep[0m  [8/84], [94mLoss[0m : 2.79383
[1mStep[0m  [16/84], [94mLoss[0m : 2.42586
[1mStep[0m  [24/84], [94mLoss[0m : 2.29827
[1mStep[0m  [32/84], [94mLoss[0m : 2.42083
[1mStep[0m  [40/84], [94mLoss[0m : 2.65871
[1mStep[0m  [48/84], [94mLoss[0m : 2.51290
[1mStep[0m  [56/84], [94mLoss[0m : 2.22591
[1mStep[0m  [64/84], [94mLoss[0m : 2.20464
[1mStep[0m  [72/84], [94mLoss[0m : 2.75405
[1mStep[0m  [80/84], [94mLoss[0m : 2.60661

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.343, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.64844
[1mStep[0m  [8/84], [94mLoss[0m : 2.70011
[1mStep[0m  [16/84], [94mLoss[0m : 2.25298
[1mStep[0m  [24/84], [94mLoss[0m : 2.24794
[1mStep[0m  [32/84], [94mLoss[0m : 2.49398
[1mStep[0m  [40/84], [94mLoss[0m : 2.52325
[1mStep[0m  [48/84], [94mLoss[0m : 2.05416
[1mStep[0m  [56/84], [94mLoss[0m : 2.75096
[1mStep[0m  [64/84], [94mLoss[0m : 2.31869
[1mStep[0m  [72/84], [94mLoss[0m : 2.41775
[1mStep[0m  [80/84], [94mLoss[0m : 2.28981

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.343, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.02290
[1mStep[0m  [8/84], [94mLoss[0m : 2.28082
[1mStep[0m  [16/84], [94mLoss[0m : 2.58681
[1mStep[0m  [24/84], [94mLoss[0m : 2.50600
[1mStep[0m  [32/84], [94mLoss[0m : 2.38729
[1mStep[0m  [40/84], [94mLoss[0m : 2.30268
[1mStep[0m  [48/84], [94mLoss[0m : 2.42650
[1mStep[0m  [56/84], [94mLoss[0m : 2.41196
[1mStep[0m  [64/84], [94mLoss[0m : 2.86611
[1mStep[0m  [72/84], [94mLoss[0m : 1.99671
[1mStep[0m  [80/84], [94mLoss[0m : 2.68701

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.320, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.39655
[1mStep[0m  [8/84], [94mLoss[0m : 2.04041
[1mStep[0m  [16/84], [94mLoss[0m : 2.45331
[1mStep[0m  [24/84], [94mLoss[0m : 2.26676
[1mStep[0m  [32/84], [94mLoss[0m : 2.46277
[1mStep[0m  [40/84], [94mLoss[0m : 2.20279
[1mStep[0m  [48/84], [94mLoss[0m : 2.56859
[1mStep[0m  [56/84], [94mLoss[0m : 2.19567
[1mStep[0m  [64/84], [94mLoss[0m : 2.38949
[1mStep[0m  [72/84], [94mLoss[0m : 2.61089
[1mStep[0m  [80/84], [94mLoss[0m : 2.40008

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.321, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45985
[1mStep[0m  [8/84], [94mLoss[0m : 2.42426
[1mStep[0m  [16/84], [94mLoss[0m : 2.06187
[1mStep[0m  [24/84], [94mLoss[0m : 2.27235
[1mStep[0m  [32/84], [94mLoss[0m : 2.24067
[1mStep[0m  [40/84], [94mLoss[0m : 2.18823
[1mStep[0m  [48/84], [94mLoss[0m : 2.46561
[1mStep[0m  [56/84], [94mLoss[0m : 2.33785
[1mStep[0m  [64/84], [94mLoss[0m : 2.73702
[1mStep[0m  [72/84], [94mLoss[0m : 2.49813
[1mStep[0m  [80/84], [94mLoss[0m : 2.45534

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.310, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41393
[1mStep[0m  [8/84], [94mLoss[0m : 2.29462
[1mStep[0m  [16/84], [94mLoss[0m : 2.20754
[1mStep[0m  [24/84], [94mLoss[0m : 2.21547
[1mStep[0m  [32/84], [94mLoss[0m : 2.61205
[1mStep[0m  [40/84], [94mLoss[0m : 2.65224
[1mStep[0m  [48/84], [94mLoss[0m : 2.66615
[1mStep[0m  [56/84], [94mLoss[0m : 2.38782
[1mStep[0m  [64/84], [94mLoss[0m : 2.71397
[1mStep[0m  [72/84], [94mLoss[0m : 2.20211
[1mStep[0m  [80/84], [94mLoss[0m : 2.11582

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.313, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.34307
[1mStep[0m  [8/84], [94mLoss[0m : 2.13680
[1mStep[0m  [16/84], [94mLoss[0m : 2.28343
[1mStep[0m  [24/84], [94mLoss[0m : 2.52981
[1mStep[0m  [32/84], [94mLoss[0m : 2.63445
[1mStep[0m  [40/84], [94mLoss[0m : 2.35914
[1mStep[0m  [48/84], [94mLoss[0m : 2.33551
[1mStep[0m  [56/84], [94mLoss[0m : 2.33564
[1mStep[0m  [64/84], [94mLoss[0m : 2.06884
[1mStep[0m  [72/84], [94mLoss[0m : 2.74763
[1mStep[0m  [80/84], [94mLoss[0m : 2.39009

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.401, [92mTest[0m: 2.318, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37658
[1mStep[0m  [8/84], [94mLoss[0m : 1.94220
[1mStep[0m  [16/84], [94mLoss[0m : 2.27681
[1mStep[0m  [24/84], [94mLoss[0m : 2.46973
[1mStep[0m  [32/84], [94mLoss[0m : 2.21609
[1mStep[0m  [40/84], [94mLoss[0m : 2.08342
[1mStep[0m  [48/84], [94mLoss[0m : 2.55993
[1mStep[0m  [56/84], [94mLoss[0m : 2.42103
[1mStep[0m  [64/84], [94mLoss[0m : 2.64997
[1mStep[0m  [72/84], [94mLoss[0m : 2.11348
[1mStep[0m  [80/84], [94mLoss[0m : 2.34664

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.396, [92mTest[0m: 2.323, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.14428
[1mStep[0m  [8/84], [94mLoss[0m : 2.80973
[1mStep[0m  [16/84], [94mLoss[0m : 2.26934
[1mStep[0m  [24/84], [94mLoss[0m : 2.36874
[1mStep[0m  [32/84], [94mLoss[0m : 2.42353
[1mStep[0m  [40/84], [94mLoss[0m : 2.32437
[1mStep[0m  [48/84], [94mLoss[0m : 2.16831
[1mStep[0m  [56/84], [94mLoss[0m : 2.20953
[1mStep[0m  [64/84], [94mLoss[0m : 2.42902
[1mStep[0m  [72/84], [94mLoss[0m : 2.28363
[1mStep[0m  [80/84], [94mLoss[0m : 2.36372

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.388, [92mTest[0m: 2.325, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53690
[1mStep[0m  [8/84], [94mLoss[0m : 2.33720
[1mStep[0m  [16/84], [94mLoss[0m : 2.11825
[1mStep[0m  [24/84], [94mLoss[0m : 2.74877
[1mStep[0m  [32/84], [94mLoss[0m : 2.28462
[1mStep[0m  [40/84], [94mLoss[0m : 2.45464
[1mStep[0m  [48/84], [94mLoss[0m : 2.50504
[1mStep[0m  [56/84], [94mLoss[0m : 2.28869
[1mStep[0m  [64/84], [94mLoss[0m : 2.49242
[1mStep[0m  [72/84], [94mLoss[0m : 2.29107
[1mStep[0m  [80/84], [94mLoss[0m : 2.64183

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.395, [92mTest[0m: 2.311, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.22192
[1mStep[0m  [8/84], [94mLoss[0m : 2.27205
[1mStep[0m  [16/84], [94mLoss[0m : 2.39715
[1mStep[0m  [24/84], [94mLoss[0m : 2.40063
[1mStep[0m  [32/84], [94mLoss[0m : 2.18441
[1mStep[0m  [40/84], [94mLoss[0m : 2.43374
[1mStep[0m  [48/84], [94mLoss[0m : 2.20972
[1mStep[0m  [56/84], [94mLoss[0m : 2.51181
[1mStep[0m  [64/84], [94mLoss[0m : 2.35819
[1mStep[0m  [72/84], [94mLoss[0m : 2.40646
[1mStep[0m  [80/84], [94mLoss[0m : 2.45781

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.386, [92mTest[0m: 2.310, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.61766
[1mStep[0m  [8/84], [94mLoss[0m : 2.50086
[1mStep[0m  [16/84], [94mLoss[0m : 2.57431
[1mStep[0m  [24/84], [94mLoss[0m : 2.62335
[1mStep[0m  [32/84], [94mLoss[0m : 2.30171
[1mStep[0m  [40/84], [94mLoss[0m : 2.45448
[1mStep[0m  [48/84], [94mLoss[0m : 2.67761
[1mStep[0m  [56/84], [94mLoss[0m : 2.47348
[1mStep[0m  [64/84], [94mLoss[0m : 2.48073
[1mStep[0m  [72/84], [94mLoss[0m : 2.29624
[1mStep[0m  [80/84], [94mLoss[0m : 2.35657

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.368, [92mTest[0m: 2.328, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.82775
[1mStep[0m  [8/84], [94mLoss[0m : 2.40615
[1mStep[0m  [16/84], [94mLoss[0m : 2.14594
[1mStep[0m  [24/84], [94mLoss[0m : 2.28852
[1mStep[0m  [32/84], [94mLoss[0m : 2.06890
[1mStep[0m  [40/84], [94mLoss[0m : 2.18934
[1mStep[0m  [48/84], [94mLoss[0m : 2.34886
[1mStep[0m  [56/84], [94mLoss[0m : 2.42137
[1mStep[0m  [64/84], [94mLoss[0m : 2.57019
[1mStep[0m  [72/84], [94mLoss[0m : 2.22920
[1mStep[0m  [80/84], [94mLoss[0m : 2.42429

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.376, [92mTest[0m: 2.326, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.24851
[1mStep[0m  [8/84], [94mLoss[0m : 2.40171
[1mStep[0m  [16/84], [94mLoss[0m : 2.43286
[1mStep[0m  [24/84], [94mLoss[0m : 2.35081
[1mStep[0m  [32/84], [94mLoss[0m : 2.52891
[1mStep[0m  [40/84], [94mLoss[0m : 2.13265
[1mStep[0m  [48/84], [94mLoss[0m : 2.41829
[1mStep[0m  [56/84], [94mLoss[0m : 2.40035
[1mStep[0m  [64/84], [94mLoss[0m : 2.25113
[1mStep[0m  [72/84], [94mLoss[0m : 2.22131
[1mStep[0m  [80/84], [94mLoss[0m : 2.26484

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.364, [92mTest[0m: 2.317, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.17073
[1mStep[0m  [8/84], [94mLoss[0m : 2.34639
[1mStep[0m  [16/84], [94mLoss[0m : 2.52744
[1mStep[0m  [24/84], [94mLoss[0m : 2.25954
[1mStep[0m  [32/84], [94mLoss[0m : 2.43252
[1mStep[0m  [40/84], [94mLoss[0m : 2.38460
[1mStep[0m  [48/84], [94mLoss[0m : 2.56259
[1mStep[0m  [56/84], [94mLoss[0m : 2.55853
[1mStep[0m  [64/84], [94mLoss[0m : 2.44300
[1mStep[0m  [72/84], [94mLoss[0m : 2.67366
[1mStep[0m  [80/84], [94mLoss[0m : 2.57935

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.369, [92mTest[0m: 2.332, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.17475
[1mStep[0m  [8/84], [94mLoss[0m : 2.42616
[1mStep[0m  [16/84], [94mLoss[0m : 2.63892
[1mStep[0m  [24/84], [94mLoss[0m : 2.30172
[1mStep[0m  [32/84], [94mLoss[0m : 2.30938
[1mStep[0m  [40/84], [94mLoss[0m : 2.53397
[1mStep[0m  [48/84], [94mLoss[0m : 2.10670
[1mStep[0m  [56/84], [94mLoss[0m : 2.37171
[1mStep[0m  [64/84], [94mLoss[0m : 2.35693
[1mStep[0m  [72/84], [94mLoss[0m : 2.30720
[1mStep[0m  [80/84], [94mLoss[0m : 2.41323

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.365, [92mTest[0m: 2.305, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.28380
[1mStep[0m  [8/84], [94mLoss[0m : 2.50430
[1mStep[0m  [16/84], [94mLoss[0m : 2.33080
[1mStep[0m  [24/84], [94mLoss[0m : 2.32171
[1mStep[0m  [32/84], [94mLoss[0m : 2.51246
[1mStep[0m  [40/84], [94mLoss[0m : 2.50060
[1mStep[0m  [48/84], [94mLoss[0m : 2.00257
[1mStep[0m  [56/84], [94mLoss[0m : 2.38257
[1mStep[0m  [64/84], [94mLoss[0m : 2.46708
[1mStep[0m  [72/84], [94mLoss[0m : 2.14079
[1mStep[0m  [80/84], [94mLoss[0m : 2.65827

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.372, [92mTest[0m: 2.324, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49731
[1mStep[0m  [8/84], [94mLoss[0m : 2.23804
[1mStep[0m  [16/84], [94mLoss[0m : 2.42663
[1mStep[0m  [24/84], [94mLoss[0m : 2.24483
[1mStep[0m  [32/84], [94mLoss[0m : 2.16048
[1mStep[0m  [40/84], [94mLoss[0m : 2.53649
[1mStep[0m  [48/84], [94mLoss[0m : 2.34849
[1mStep[0m  [56/84], [94mLoss[0m : 2.40839
[1mStep[0m  [64/84], [94mLoss[0m : 2.30826
[1mStep[0m  [72/84], [94mLoss[0m : 2.30533
[1mStep[0m  [80/84], [94mLoss[0m : 2.27117

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.371, [92mTest[0m: 2.317, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40658
[1mStep[0m  [8/84], [94mLoss[0m : 2.19882
[1mStep[0m  [16/84], [94mLoss[0m : 2.31453
[1mStep[0m  [24/84], [94mLoss[0m : 2.28810
[1mStep[0m  [32/84], [94mLoss[0m : 2.28496
[1mStep[0m  [40/84], [94mLoss[0m : 2.29236
[1mStep[0m  [48/84], [94mLoss[0m : 2.52144
[1mStep[0m  [56/84], [94mLoss[0m : 2.27287
[1mStep[0m  [64/84], [94mLoss[0m : 2.44818
[1mStep[0m  [72/84], [94mLoss[0m : 2.20338
[1mStep[0m  [80/84], [94mLoss[0m : 2.10255

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.365, [92mTest[0m: 2.336, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.17369
[1mStep[0m  [8/84], [94mLoss[0m : 2.20285
[1mStep[0m  [16/84], [94mLoss[0m : 2.53620
[1mStep[0m  [24/84], [94mLoss[0m : 2.24211
[1mStep[0m  [32/84], [94mLoss[0m : 2.46556
[1mStep[0m  [40/84], [94mLoss[0m : 2.30273
[1mStep[0m  [48/84], [94mLoss[0m : 2.39450
[1mStep[0m  [56/84], [94mLoss[0m : 2.34176
[1mStep[0m  [64/84], [94mLoss[0m : 2.67184
[1mStep[0m  [72/84], [94mLoss[0m : 2.32607
[1mStep[0m  [80/84], [94mLoss[0m : 2.32179

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.361, [92mTest[0m: 2.314, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.28065
[1mStep[0m  [8/84], [94mLoss[0m : 2.07205
[1mStep[0m  [16/84], [94mLoss[0m : 2.42491
[1mStep[0m  [24/84], [94mLoss[0m : 2.18863
[1mStep[0m  [32/84], [94mLoss[0m : 2.61912
[1mStep[0m  [40/84], [94mLoss[0m : 2.31549
[1mStep[0m  [48/84], [94mLoss[0m : 2.45295
[1mStep[0m  [56/84], [94mLoss[0m : 2.48022
[1mStep[0m  [64/84], [94mLoss[0m : 2.30944
[1mStep[0m  [72/84], [94mLoss[0m : 2.40002
[1mStep[0m  [80/84], [94mLoss[0m : 2.32935

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.346, [92mTest[0m: 2.319, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.29718
[1mStep[0m  [8/84], [94mLoss[0m : 2.32150
[1mStep[0m  [16/84], [94mLoss[0m : 2.70417
[1mStep[0m  [24/84], [94mLoss[0m : 2.60649
[1mStep[0m  [32/84], [94mLoss[0m : 2.83821
[1mStep[0m  [40/84], [94mLoss[0m : 2.29722
[1mStep[0m  [48/84], [94mLoss[0m : 2.32627
[1mStep[0m  [56/84], [94mLoss[0m : 2.39178
[1mStep[0m  [64/84], [94mLoss[0m : 2.50486
[1mStep[0m  [72/84], [94mLoss[0m : 2.25836
[1mStep[0m  [80/84], [94mLoss[0m : 2.62017

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.334, [92mTest[0m: 2.308, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.314
====================================

Phase 1 - Evaluation MAE:  2.3144612397466386
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 2.35224
[1mStep[0m  [8/84], [94mLoss[0m : 2.54832
[1mStep[0m  [16/84], [94mLoss[0m : 2.50044
[1mStep[0m  [24/84], [94mLoss[0m : 2.60867
[1mStep[0m  [32/84], [94mLoss[0m : 2.56433
[1mStep[0m  [40/84], [94mLoss[0m : 2.70475
[1mStep[0m  [48/84], [94mLoss[0m : 2.56214
[1mStep[0m  [56/84], [94mLoss[0m : 2.57985
[1mStep[0m  [64/84], [94mLoss[0m : 2.48254
[1mStep[0m  [72/84], [94mLoss[0m : 2.04478
[1mStep[0m  [80/84], [94mLoss[0m : 2.58378

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.308, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.24557
[1mStep[0m  [8/84], [94mLoss[0m : 2.43735
[1mStep[0m  [16/84], [94mLoss[0m : 2.21331
[1mStep[0m  [24/84], [94mLoss[0m : 2.63923
[1mStep[0m  [32/84], [94mLoss[0m : 2.21229
[1mStep[0m  [40/84], [94mLoss[0m : 2.38775
[1mStep[0m  [48/84], [94mLoss[0m : 2.34990
[1mStep[0m  [56/84], [94mLoss[0m : 2.14597
[1mStep[0m  [64/84], [94mLoss[0m : 2.68864
[1mStep[0m  [72/84], [94mLoss[0m : 2.32925
[1mStep[0m  [80/84], [94mLoss[0m : 2.07073

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.307, [92mTest[0m: 2.351, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50767
[1mStep[0m  [8/84], [94mLoss[0m : 2.12456
[1mStep[0m  [16/84], [94mLoss[0m : 2.15646
[1mStep[0m  [24/84], [94mLoss[0m : 2.33246
[1mStep[0m  [32/84], [94mLoss[0m : 2.03700
[1mStep[0m  [40/84], [94mLoss[0m : 2.18178
[1mStep[0m  [48/84], [94mLoss[0m : 2.18417
[1mStep[0m  [56/84], [94mLoss[0m : 2.15904
[1mStep[0m  [64/84], [94mLoss[0m : 2.30000
[1mStep[0m  [72/84], [94mLoss[0m : 2.54513
[1mStep[0m  [80/84], [94mLoss[0m : 1.90630

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.214, [92mTest[0m: 2.360, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.02511
[1mStep[0m  [8/84], [94mLoss[0m : 2.10622
[1mStep[0m  [16/84], [94mLoss[0m : 2.20732
[1mStep[0m  [24/84], [94mLoss[0m : 2.04816
[1mStep[0m  [32/84], [94mLoss[0m : 1.87675
[1mStep[0m  [40/84], [94mLoss[0m : 2.23079
[1mStep[0m  [48/84], [94mLoss[0m : 2.10214
[1mStep[0m  [56/84], [94mLoss[0m : 1.86467
[1mStep[0m  [64/84], [94mLoss[0m : 2.43533
[1mStep[0m  [72/84], [94mLoss[0m : 2.41940
[1mStep[0m  [80/84], [94mLoss[0m : 2.02649

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.145, [92mTest[0m: 2.328, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.96195
[1mStep[0m  [8/84], [94mLoss[0m : 2.17950
[1mStep[0m  [16/84], [94mLoss[0m : 2.02413
[1mStep[0m  [24/84], [94mLoss[0m : 2.05268
[1mStep[0m  [32/84], [94mLoss[0m : 1.88755
[1mStep[0m  [40/84], [94mLoss[0m : 2.16791
[1mStep[0m  [48/84], [94mLoss[0m : 1.80872
[1mStep[0m  [56/84], [94mLoss[0m : 2.08659
[1mStep[0m  [64/84], [94mLoss[0m : 1.85885
[1mStep[0m  [72/84], [94mLoss[0m : 2.26268
[1mStep[0m  [80/84], [94mLoss[0m : 1.94317

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.082, [92mTest[0m: 2.377, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.25061
[1mStep[0m  [8/84], [94mLoss[0m : 1.94334
[1mStep[0m  [16/84], [94mLoss[0m : 1.70998
[1mStep[0m  [24/84], [94mLoss[0m : 2.13476
[1mStep[0m  [32/84], [94mLoss[0m : 2.28123
[1mStep[0m  [40/84], [94mLoss[0m : 2.13559
[1mStep[0m  [48/84], [94mLoss[0m : 2.15122
[1mStep[0m  [56/84], [94mLoss[0m : 1.83659
[1mStep[0m  [64/84], [94mLoss[0m : 1.87127
[1mStep[0m  [72/84], [94mLoss[0m : 1.92569
[1mStep[0m  [80/84], [94mLoss[0m : 1.86710

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.005, [92mTest[0m: 2.364, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.90971
[1mStep[0m  [8/84], [94mLoss[0m : 1.85143
[1mStep[0m  [16/84], [94mLoss[0m : 1.82536
[1mStep[0m  [24/84], [94mLoss[0m : 2.19222
[1mStep[0m  [32/84], [94mLoss[0m : 1.93845
[1mStep[0m  [40/84], [94mLoss[0m : 1.71557
[1mStep[0m  [48/84], [94mLoss[0m : 1.92924
[1mStep[0m  [56/84], [94mLoss[0m : 1.93639
[1mStep[0m  [64/84], [94mLoss[0m : 1.80710
[1mStep[0m  [72/84], [94mLoss[0m : 2.10991
[1mStep[0m  [80/84], [94mLoss[0m : 1.94168

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.936, [92mTest[0m: 2.427, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.76928
[1mStep[0m  [8/84], [94mLoss[0m : 2.05829
[1mStep[0m  [16/84], [94mLoss[0m : 1.70412
[1mStep[0m  [24/84], [94mLoss[0m : 1.79962
[1mStep[0m  [32/84], [94mLoss[0m : 1.63431
[1mStep[0m  [40/84], [94mLoss[0m : 1.84913
[1mStep[0m  [48/84], [94mLoss[0m : 2.12785
[1mStep[0m  [56/84], [94mLoss[0m : 2.08435
[1mStep[0m  [64/84], [94mLoss[0m : 1.81848
[1mStep[0m  [72/84], [94mLoss[0m : 1.79372
[1mStep[0m  [80/84], [94mLoss[0m : 1.63282

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.873, [92mTest[0m: 2.444, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.98633
[1mStep[0m  [8/84], [94mLoss[0m : 1.79482
[1mStep[0m  [16/84], [94mLoss[0m : 1.75592
[1mStep[0m  [24/84], [94mLoss[0m : 1.63485
[1mStep[0m  [32/84], [94mLoss[0m : 1.64085
[1mStep[0m  [40/84], [94mLoss[0m : 1.78649
[1mStep[0m  [48/84], [94mLoss[0m : 2.02374
[1mStep[0m  [56/84], [94mLoss[0m : 1.82334
[1mStep[0m  [64/84], [94mLoss[0m : 2.19926
[1mStep[0m  [72/84], [94mLoss[0m : 1.89536
[1mStep[0m  [80/84], [94mLoss[0m : 1.73441

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.823, [92mTest[0m: 2.479, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.06174
[1mStep[0m  [8/84], [94mLoss[0m : 1.69909
[1mStep[0m  [16/84], [94mLoss[0m : 1.65027
[1mStep[0m  [24/84], [94mLoss[0m : 1.72477
[1mStep[0m  [32/84], [94mLoss[0m : 1.82391
[1mStep[0m  [40/84], [94mLoss[0m : 2.06018
[1mStep[0m  [48/84], [94mLoss[0m : 1.61437
[1mStep[0m  [56/84], [94mLoss[0m : 1.94557
[1mStep[0m  [64/84], [94mLoss[0m : 1.85434
[1mStep[0m  [72/84], [94mLoss[0m : 2.10295
[1mStep[0m  [80/84], [94mLoss[0m : 1.73413

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.801, [92mTest[0m: 2.431, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.49893
[1mStep[0m  [8/84], [94mLoss[0m : 1.78447
[1mStep[0m  [16/84], [94mLoss[0m : 1.80004
[1mStep[0m  [24/84], [94mLoss[0m : 1.60350
[1mStep[0m  [32/84], [94mLoss[0m : 1.87722
[1mStep[0m  [40/84], [94mLoss[0m : 1.83100
[1mStep[0m  [48/84], [94mLoss[0m : 1.63606
[1mStep[0m  [56/84], [94mLoss[0m : 1.92638
[1mStep[0m  [64/84], [94mLoss[0m : 1.50451
[1mStep[0m  [72/84], [94mLoss[0m : 1.83635
[1mStep[0m  [80/84], [94mLoss[0m : 1.83017

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.725, [92mTest[0m: 2.462, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.80544
[1mStep[0m  [8/84], [94mLoss[0m : 1.62074
[1mStep[0m  [16/84], [94mLoss[0m : 1.52867
[1mStep[0m  [24/84], [94mLoss[0m : 1.86555
[1mStep[0m  [32/84], [94mLoss[0m : 1.51049
[1mStep[0m  [40/84], [94mLoss[0m : 1.64336
[1mStep[0m  [48/84], [94mLoss[0m : 1.72418
[1mStep[0m  [56/84], [94mLoss[0m : 1.55167
[1mStep[0m  [64/84], [94mLoss[0m : 1.76309
[1mStep[0m  [72/84], [94mLoss[0m : 2.03148
[1mStep[0m  [80/84], [94mLoss[0m : 1.72031

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.710, [92mTest[0m: 2.488, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.43604
[1mStep[0m  [8/84], [94mLoss[0m : 1.52160
[1mStep[0m  [16/84], [94mLoss[0m : 1.59188
[1mStep[0m  [24/84], [94mLoss[0m : 1.67664
[1mStep[0m  [32/84], [94mLoss[0m : 1.81672
[1mStep[0m  [40/84], [94mLoss[0m : 1.50168
[1mStep[0m  [48/84], [94mLoss[0m : 1.56911
[1mStep[0m  [56/84], [94mLoss[0m : 1.70240
[1mStep[0m  [64/84], [94mLoss[0m : 1.69125
[1mStep[0m  [72/84], [94mLoss[0m : 1.66206
[1mStep[0m  [80/84], [94mLoss[0m : 1.50238

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.672, [92mTest[0m: 2.503, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.40709
[1mStep[0m  [8/84], [94mLoss[0m : 1.46165
[1mStep[0m  [16/84], [94mLoss[0m : 1.37420
[1mStep[0m  [24/84], [94mLoss[0m : 1.71296
[1mStep[0m  [32/84], [94mLoss[0m : 1.61743
[1mStep[0m  [40/84], [94mLoss[0m : 1.55703
[1mStep[0m  [48/84], [94mLoss[0m : 1.66429
[1mStep[0m  [56/84], [94mLoss[0m : 1.72041
[1mStep[0m  [64/84], [94mLoss[0m : 1.54885
[1mStep[0m  [72/84], [94mLoss[0m : 1.70529
[1mStep[0m  [80/84], [94mLoss[0m : 2.06533

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.629, [92mTest[0m: 2.494, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.78310
[1mStep[0m  [8/84], [94mLoss[0m : 1.56004
[1mStep[0m  [16/84], [94mLoss[0m : 1.54696
[1mStep[0m  [24/84], [94mLoss[0m : 1.39284
[1mStep[0m  [32/84], [94mLoss[0m : 1.52570
[1mStep[0m  [40/84], [94mLoss[0m : 1.48908
[1mStep[0m  [48/84], [94mLoss[0m : 1.60498
[1mStep[0m  [56/84], [94mLoss[0m : 1.60796
[1mStep[0m  [64/84], [94mLoss[0m : 1.60822
[1mStep[0m  [72/84], [94mLoss[0m : 1.64980
[1mStep[0m  [80/84], [94mLoss[0m : 1.78185

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.587, [92mTest[0m: 2.485, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.58244
[1mStep[0m  [8/84], [94mLoss[0m : 1.72004
[1mStep[0m  [16/84], [94mLoss[0m : 1.56314
[1mStep[0m  [24/84], [94mLoss[0m : 1.45142
[1mStep[0m  [32/84], [94mLoss[0m : 1.57849
[1mStep[0m  [40/84], [94mLoss[0m : 1.74425
[1mStep[0m  [48/84], [94mLoss[0m : 1.55476
[1mStep[0m  [56/84], [94mLoss[0m : 1.56802
[1mStep[0m  [64/84], [94mLoss[0m : 1.88719
[1mStep[0m  [72/84], [94mLoss[0m : 1.34523
[1mStep[0m  [80/84], [94mLoss[0m : 1.79866

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.559, [92mTest[0m: 2.493, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.46413
[1mStep[0m  [8/84], [94mLoss[0m : 1.49314
[1mStep[0m  [16/84], [94mLoss[0m : 1.59350
[1mStep[0m  [24/84], [94mLoss[0m : 1.59298
[1mStep[0m  [32/84], [94mLoss[0m : 1.49452
[1mStep[0m  [40/84], [94mLoss[0m : 1.45477
[1mStep[0m  [48/84], [94mLoss[0m : 1.55871
[1mStep[0m  [56/84], [94mLoss[0m : 1.55466
[1mStep[0m  [64/84], [94mLoss[0m : 1.38001
[1mStep[0m  [72/84], [94mLoss[0m : 1.46757
[1mStep[0m  [80/84], [94mLoss[0m : 1.45694

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.526, [92mTest[0m: 2.492, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.38218
[1mStep[0m  [8/84], [94mLoss[0m : 1.53286
[1mStep[0m  [16/84], [94mLoss[0m : 1.42370
[1mStep[0m  [24/84], [94mLoss[0m : 1.64447
[1mStep[0m  [32/84], [94mLoss[0m : 1.45344
[1mStep[0m  [40/84], [94mLoss[0m : 1.44328
[1mStep[0m  [48/84], [94mLoss[0m : 1.43961
[1mStep[0m  [56/84], [94mLoss[0m : 1.76981
[1mStep[0m  [64/84], [94mLoss[0m : 1.63347
[1mStep[0m  [72/84], [94mLoss[0m : 1.50178
[1mStep[0m  [80/84], [94mLoss[0m : 1.64114

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.517, [92mTest[0m: 2.503, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.49279
[1mStep[0m  [8/84], [94mLoss[0m : 1.45603
[1mStep[0m  [16/84], [94mLoss[0m : 1.43464
[1mStep[0m  [24/84], [94mLoss[0m : 1.65223
[1mStep[0m  [32/84], [94mLoss[0m : 1.40207
[1mStep[0m  [40/84], [94mLoss[0m : 1.64081
[1mStep[0m  [48/84], [94mLoss[0m : 1.36367
[1mStep[0m  [56/84], [94mLoss[0m : 1.35147
[1mStep[0m  [64/84], [94mLoss[0m : 1.34196
[1mStep[0m  [72/84], [94mLoss[0m : 1.40989
[1mStep[0m  [80/84], [94mLoss[0m : 1.43034

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.483, [92mTest[0m: 2.487, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.43955
[1mStep[0m  [8/84], [94mLoss[0m : 1.48130
[1mStep[0m  [16/84], [94mLoss[0m : 1.27432
[1mStep[0m  [24/84], [94mLoss[0m : 1.48231
[1mStep[0m  [32/84], [94mLoss[0m : 1.35072
[1mStep[0m  [40/84], [94mLoss[0m : 1.49963
[1mStep[0m  [48/84], [94mLoss[0m : 1.43006
[1mStep[0m  [56/84], [94mLoss[0m : 1.44040
[1mStep[0m  [64/84], [94mLoss[0m : 1.49470
[1mStep[0m  [72/84], [94mLoss[0m : 1.66179
[1mStep[0m  [80/84], [94mLoss[0m : 1.63092

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.453, [92mTest[0m: 2.524, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.33411
[1mStep[0m  [8/84], [94mLoss[0m : 1.41021
[1mStep[0m  [16/84], [94mLoss[0m : 1.38833
[1mStep[0m  [24/84], [94mLoss[0m : 1.42565
[1mStep[0m  [32/84], [94mLoss[0m : 1.45650
[1mStep[0m  [40/84], [94mLoss[0m : 1.56233
[1mStep[0m  [48/84], [94mLoss[0m : 1.25418
[1mStep[0m  [56/84], [94mLoss[0m : 1.56248
[1mStep[0m  [64/84], [94mLoss[0m : 1.43650
[1mStep[0m  [72/84], [94mLoss[0m : 1.32508
[1mStep[0m  [80/84], [94mLoss[0m : 1.52322

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.417, [92mTest[0m: 2.540, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.28955
[1mStep[0m  [8/84], [94mLoss[0m : 1.35123
[1mStep[0m  [16/84], [94mLoss[0m : 1.26013
[1mStep[0m  [24/84], [94mLoss[0m : 1.34224
[1mStep[0m  [32/84], [94mLoss[0m : 1.34208
[1mStep[0m  [40/84], [94mLoss[0m : 1.28020
[1mStep[0m  [48/84], [94mLoss[0m : 1.22459
[1mStep[0m  [56/84], [94mLoss[0m : 1.32419
[1mStep[0m  [64/84], [94mLoss[0m : 1.37766
[1mStep[0m  [72/84], [94mLoss[0m : 1.45424
[1mStep[0m  [80/84], [94mLoss[0m : 1.51180

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.364, [92mTest[0m: 2.538, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.37364
[1mStep[0m  [8/84], [94mLoss[0m : 1.42073
[1mStep[0m  [16/84], [94mLoss[0m : 1.43492
[1mStep[0m  [24/84], [94mLoss[0m : 1.46331
[1mStep[0m  [32/84], [94mLoss[0m : 1.45208
[1mStep[0m  [40/84], [94mLoss[0m : 1.35009
[1mStep[0m  [48/84], [94mLoss[0m : 1.28552
[1mStep[0m  [56/84], [94mLoss[0m : 1.40614
[1mStep[0m  [64/84], [94mLoss[0m : 1.28620
[1mStep[0m  [72/84], [94mLoss[0m : 1.55288
[1mStep[0m  [80/84], [94mLoss[0m : 1.37293

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.353, [92mTest[0m: 2.554, [96mlr[0m: 0.006772500000000002
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 22 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.548
====================================

Phase 2 - Evaluation MAE:  2.54843556029456
MAE score P1       2.314461
MAE score P2       2.548436
loss               1.353259
learning_rate      0.007525
batch_size              128
hidden_sizes      [250, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping         True
dropout                 0.3
momentum                0.9
weight_decay         0.0001
Name: 22, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 11.35839
[1mStep[0m  [4/42], [94mLoss[0m : 10.70531
[1mStep[0m  [8/42], [94mLoss[0m : 10.76031
[1mStep[0m  [12/42], [94mLoss[0m : 10.73168
[1mStep[0m  [16/42], [94mLoss[0m : 10.45766
[1mStep[0m  [20/42], [94mLoss[0m : 9.98320
[1mStep[0m  [24/42], [94mLoss[0m : 10.30645
[1mStep[0m  [28/42], [94mLoss[0m : 9.95082
[1mStep[0m  [32/42], [94mLoss[0m : 10.03692
[1mStep[0m  [36/42], [94mLoss[0m : 9.61913
[1mStep[0m  [40/42], [94mLoss[0m : 9.68191

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.393, [92mTest[0m: 11.057, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.10787
[1mStep[0m  [4/42], [94mLoss[0m : 9.96306
[1mStep[0m  [8/42], [94mLoss[0m : 9.35078
[1mStep[0m  [12/42], [94mLoss[0m : 9.36106
[1mStep[0m  [16/42], [94mLoss[0m : 9.20640
[1mStep[0m  [20/42], [94mLoss[0m : 9.48431
[1mStep[0m  [24/42], [94mLoss[0m : 9.32583
[1mStep[0m  [28/42], [94mLoss[0m : 8.62090
[1mStep[0m  [32/42], [94mLoss[0m : 9.02591
[1mStep[0m  [36/42], [94mLoss[0m : 8.57000
[1mStep[0m  [40/42], [94mLoss[0m : 8.49423

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.244, [92mTest[0m: 9.796, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.93736
[1mStep[0m  [4/42], [94mLoss[0m : 8.17121
[1mStep[0m  [8/42], [94mLoss[0m : 8.14980
[1mStep[0m  [12/42], [94mLoss[0m : 7.98880
[1mStep[0m  [16/42], [94mLoss[0m : 7.73853
[1mStep[0m  [20/42], [94mLoss[0m : 7.66397
[1mStep[0m  [24/42], [94mLoss[0m : 7.45738
[1mStep[0m  [28/42], [94mLoss[0m : 7.36728
[1mStep[0m  [32/42], [94mLoss[0m : 7.29263
[1mStep[0m  [36/42], [94mLoss[0m : 7.25054
[1mStep[0m  [40/42], [94mLoss[0m : 6.63228

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 7.606, [92mTest[0m: 8.245, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.77457
[1mStep[0m  [4/42], [94mLoss[0m : 6.93021
[1mStep[0m  [8/42], [94mLoss[0m : 6.56602
[1mStep[0m  [12/42], [94mLoss[0m : 6.67444
[1mStep[0m  [16/42], [94mLoss[0m : 6.25682
[1mStep[0m  [20/42], [94mLoss[0m : 6.17228
[1mStep[0m  [24/42], [94mLoss[0m : 6.33358
[1mStep[0m  [28/42], [94mLoss[0m : 5.84627
[1mStep[0m  [32/42], [94mLoss[0m : 6.01162
[1mStep[0m  [36/42], [94mLoss[0m : 5.43283
[1mStep[0m  [40/42], [94mLoss[0m : 5.59730

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 6.222, [92mTest[0m: 6.329, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.62299
[1mStep[0m  [4/42], [94mLoss[0m : 5.72396
[1mStep[0m  [8/42], [94mLoss[0m : 5.27388
[1mStep[0m  [12/42], [94mLoss[0m : 4.96618
[1mStep[0m  [16/42], [94mLoss[0m : 5.48941
[1mStep[0m  [20/42], [94mLoss[0m : 5.02856
[1mStep[0m  [24/42], [94mLoss[0m : 5.04732
[1mStep[0m  [28/42], [94mLoss[0m : 4.79748
[1mStep[0m  [32/42], [94mLoss[0m : 4.74832
[1mStep[0m  [36/42], [94mLoss[0m : 4.83174
[1mStep[0m  [40/42], [94mLoss[0m : 4.66491

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 5.169, [92mTest[0m: 4.724, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.81370
[1mStep[0m  [4/42], [94mLoss[0m : 4.69985
[1mStep[0m  [8/42], [94mLoss[0m : 4.29181
[1mStep[0m  [12/42], [94mLoss[0m : 4.56928
[1mStep[0m  [16/42], [94mLoss[0m : 4.28367
[1mStep[0m  [20/42], [94mLoss[0m : 4.25528
[1mStep[0m  [24/42], [94mLoss[0m : 3.83791
[1mStep[0m  [28/42], [94mLoss[0m : 3.86811
[1mStep[0m  [32/42], [94mLoss[0m : 3.60643
[1mStep[0m  [36/42], [94mLoss[0m : 3.77259
[1mStep[0m  [40/42], [94mLoss[0m : 3.59268

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 4.082, [92mTest[0m: 3.741, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.67086
[1mStep[0m  [4/42], [94mLoss[0m : 3.66443
[1mStep[0m  [8/42], [94mLoss[0m : 3.38985
[1mStep[0m  [12/42], [94mLoss[0m : 3.16125
[1mStep[0m  [16/42], [94mLoss[0m : 2.92786
[1mStep[0m  [20/42], [94mLoss[0m : 3.17617
[1mStep[0m  [24/42], [94mLoss[0m : 2.99369
[1mStep[0m  [28/42], [94mLoss[0m : 3.07931
[1mStep[0m  [32/42], [94mLoss[0m : 2.87458
[1mStep[0m  [36/42], [94mLoss[0m : 2.79693
[1mStep[0m  [40/42], [94mLoss[0m : 2.84831

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 3.090, [92mTest[0m: 2.937, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64947
[1mStep[0m  [4/42], [94mLoss[0m : 3.00585
[1mStep[0m  [8/42], [94mLoss[0m : 2.66261
[1mStep[0m  [12/42], [94mLoss[0m : 2.68021
[1mStep[0m  [16/42], [94mLoss[0m : 2.70867
[1mStep[0m  [20/42], [94mLoss[0m : 2.61723
[1mStep[0m  [24/42], [94mLoss[0m : 2.67366
[1mStep[0m  [28/42], [94mLoss[0m : 2.73650
[1mStep[0m  [32/42], [94mLoss[0m : 2.78348
[1mStep[0m  [36/42], [94mLoss[0m : 2.64311
[1mStep[0m  [40/42], [94mLoss[0m : 2.61209

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.654, [92mTest[0m: 2.478, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55399
[1mStep[0m  [4/42], [94mLoss[0m : 2.56683
[1mStep[0m  [8/42], [94mLoss[0m : 2.69704
[1mStep[0m  [12/42], [94mLoss[0m : 2.63274
[1mStep[0m  [16/42], [94mLoss[0m : 2.63550
[1mStep[0m  [20/42], [94mLoss[0m : 2.66195
[1mStep[0m  [24/42], [94mLoss[0m : 2.47086
[1mStep[0m  [28/42], [94mLoss[0m : 2.57801
[1mStep[0m  [32/42], [94mLoss[0m : 2.77608
[1mStep[0m  [36/42], [94mLoss[0m : 2.54021
[1mStep[0m  [40/42], [94mLoss[0m : 2.25934

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.552, [92mTest[0m: 2.414, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59231
[1mStep[0m  [4/42], [94mLoss[0m : 2.48251
[1mStep[0m  [8/42], [94mLoss[0m : 2.65141
[1mStep[0m  [12/42], [94mLoss[0m : 2.51729
[1mStep[0m  [16/42], [94mLoss[0m : 2.52475
[1mStep[0m  [20/42], [94mLoss[0m : 2.40552
[1mStep[0m  [24/42], [94mLoss[0m : 2.69752
[1mStep[0m  [28/42], [94mLoss[0m : 2.64960
[1mStep[0m  [32/42], [94mLoss[0m : 2.52668
[1mStep[0m  [36/42], [94mLoss[0m : 2.65270
[1mStep[0m  [40/42], [94mLoss[0m : 2.62875

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.360, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64674
[1mStep[0m  [4/42], [94mLoss[0m : 2.53235
[1mStep[0m  [8/42], [94mLoss[0m : 2.58483
[1mStep[0m  [12/42], [94mLoss[0m : 2.48128
[1mStep[0m  [16/42], [94mLoss[0m : 2.45243
[1mStep[0m  [20/42], [94mLoss[0m : 2.65097
[1mStep[0m  [24/42], [94mLoss[0m : 2.65219
[1mStep[0m  [28/42], [94mLoss[0m : 2.56011
[1mStep[0m  [32/42], [94mLoss[0m : 2.46210
[1mStep[0m  [36/42], [94mLoss[0m : 2.78569
[1mStep[0m  [40/42], [94mLoss[0m : 2.59282

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.541, [92mTest[0m: 2.381, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.69339
[1mStep[0m  [4/42], [94mLoss[0m : 2.49138
[1mStep[0m  [8/42], [94mLoss[0m : 2.77473
[1mStep[0m  [12/42], [94mLoss[0m : 2.41797
[1mStep[0m  [16/42], [94mLoss[0m : 2.64679
[1mStep[0m  [20/42], [94mLoss[0m : 2.52369
[1mStep[0m  [24/42], [94mLoss[0m : 2.50462
[1mStep[0m  [28/42], [94mLoss[0m : 2.71084
[1mStep[0m  [32/42], [94mLoss[0m : 2.57549
[1mStep[0m  [36/42], [94mLoss[0m : 2.64828
[1mStep[0m  [40/42], [94mLoss[0m : 2.65575

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.370, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37680
[1mStep[0m  [4/42], [94mLoss[0m : 2.45227
[1mStep[0m  [8/42], [94mLoss[0m : 2.46634
[1mStep[0m  [12/42], [94mLoss[0m : 2.38078
[1mStep[0m  [16/42], [94mLoss[0m : 2.58737
[1mStep[0m  [20/42], [94mLoss[0m : 2.49397
[1mStep[0m  [24/42], [94mLoss[0m : 2.44545
[1mStep[0m  [28/42], [94mLoss[0m : 2.42466
[1mStep[0m  [32/42], [94mLoss[0m : 2.39111
[1mStep[0m  [36/42], [94mLoss[0m : 2.50820
[1mStep[0m  [40/42], [94mLoss[0m : 2.69921

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.542, [92mTest[0m: 2.389, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47354
[1mStep[0m  [4/42], [94mLoss[0m : 2.43022
[1mStep[0m  [8/42], [94mLoss[0m : 2.46700
[1mStep[0m  [12/42], [94mLoss[0m : 2.61770
[1mStep[0m  [16/42], [94mLoss[0m : 2.46203
[1mStep[0m  [20/42], [94mLoss[0m : 2.39170
[1mStep[0m  [24/42], [94mLoss[0m : 2.60529
[1mStep[0m  [28/42], [94mLoss[0m : 2.38198
[1mStep[0m  [32/42], [94mLoss[0m : 2.69037
[1mStep[0m  [36/42], [94mLoss[0m : 2.53535
[1mStep[0m  [40/42], [94mLoss[0m : 2.64291

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.409, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.73764
[1mStep[0m  [4/42], [94mLoss[0m : 2.48813
[1mStep[0m  [8/42], [94mLoss[0m : 2.42106
[1mStep[0m  [12/42], [94mLoss[0m : 2.45294
[1mStep[0m  [16/42], [94mLoss[0m : 2.50205
[1mStep[0m  [20/42], [94mLoss[0m : 2.87555
[1mStep[0m  [24/42], [94mLoss[0m : 2.46358
[1mStep[0m  [28/42], [94mLoss[0m : 2.45925
[1mStep[0m  [32/42], [94mLoss[0m : 2.66161
[1mStep[0m  [36/42], [94mLoss[0m : 2.54032
[1mStep[0m  [40/42], [94mLoss[0m : 2.43425

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.356, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46535
[1mStep[0m  [4/42], [94mLoss[0m : 2.66940
[1mStep[0m  [8/42], [94mLoss[0m : 2.35127
[1mStep[0m  [12/42], [94mLoss[0m : 2.52268
[1mStep[0m  [16/42], [94mLoss[0m : 2.23993
[1mStep[0m  [20/42], [94mLoss[0m : 2.46779
[1mStep[0m  [24/42], [94mLoss[0m : 2.60313
[1mStep[0m  [28/42], [94mLoss[0m : 2.50383
[1mStep[0m  [32/42], [94mLoss[0m : 2.39523
[1mStep[0m  [36/42], [94mLoss[0m : 2.47127
[1mStep[0m  [40/42], [94mLoss[0m : 2.45088

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.370, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48814
[1mStep[0m  [4/42], [94mLoss[0m : 2.53144
[1mStep[0m  [8/42], [94mLoss[0m : 2.31310
[1mStep[0m  [12/42], [94mLoss[0m : 2.45123
[1mStep[0m  [16/42], [94mLoss[0m : 2.46905
[1mStep[0m  [20/42], [94mLoss[0m : 2.33510
[1mStep[0m  [24/42], [94mLoss[0m : 2.39575
[1mStep[0m  [28/42], [94mLoss[0m : 2.56534
[1mStep[0m  [32/42], [94mLoss[0m : 2.39015
[1mStep[0m  [36/42], [94mLoss[0m : 2.47629
[1mStep[0m  [40/42], [94mLoss[0m : 2.67276

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.376, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58482
[1mStep[0m  [4/42], [94mLoss[0m : 2.54437
[1mStep[0m  [8/42], [94mLoss[0m : 2.48243
[1mStep[0m  [12/42], [94mLoss[0m : 2.45932
[1mStep[0m  [16/42], [94mLoss[0m : 2.36906
[1mStep[0m  [20/42], [94mLoss[0m : 2.46046
[1mStep[0m  [24/42], [94mLoss[0m : 2.67486
[1mStep[0m  [28/42], [94mLoss[0m : 2.51631
[1mStep[0m  [32/42], [94mLoss[0m : 2.56080
[1mStep[0m  [36/42], [94mLoss[0m : 2.61818
[1mStep[0m  [40/42], [94mLoss[0m : 2.33827

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.363, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58812
[1mStep[0m  [4/42], [94mLoss[0m : 2.53553
[1mStep[0m  [8/42], [94mLoss[0m : 2.48472
[1mStep[0m  [12/42], [94mLoss[0m : 2.49174
[1mStep[0m  [16/42], [94mLoss[0m : 2.44027
[1mStep[0m  [20/42], [94mLoss[0m : 2.29862
[1mStep[0m  [24/42], [94mLoss[0m : 2.58152
[1mStep[0m  [28/42], [94mLoss[0m : 2.53671
[1mStep[0m  [32/42], [94mLoss[0m : 2.43741
[1mStep[0m  [36/42], [94mLoss[0m : 2.48142
[1mStep[0m  [40/42], [94mLoss[0m : 2.35849

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.354, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64448
[1mStep[0m  [4/42], [94mLoss[0m : 2.48870
[1mStep[0m  [8/42], [94mLoss[0m : 2.42405
[1mStep[0m  [12/42], [94mLoss[0m : 2.35862
[1mStep[0m  [16/42], [94mLoss[0m : 2.47324
[1mStep[0m  [20/42], [94mLoss[0m : 2.25885
[1mStep[0m  [24/42], [94mLoss[0m : 2.43610
[1mStep[0m  [28/42], [94mLoss[0m : 2.55853
[1mStep[0m  [32/42], [94mLoss[0m : 2.26852
[1mStep[0m  [36/42], [94mLoss[0m : 2.39542
[1mStep[0m  [40/42], [94mLoss[0m : 2.55056

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.348, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36857
[1mStep[0m  [4/42], [94mLoss[0m : 2.40009
[1mStep[0m  [8/42], [94mLoss[0m : 2.60011
[1mStep[0m  [12/42], [94mLoss[0m : 2.42832
[1mStep[0m  [16/42], [94mLoss[0m : 2.53875
[1mStep[0m  [20/42], [94mLoss[0m : 2.57498
[1mStep[0m  [24/42], [94mLoss[0m : 2.47687
[1mStep[0m  [28/42], [94mLoss[0m : 2.24656
[1mStep[0m  [32/42], [94mLoss[0m : 2.29523
[1mStep[0m  [36/42], [94mLoss[0m : 2.49582
[1mStep[0m  [40/42], [94mLoss[0m : 2.41038

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.333, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54142
[1mStep[0m  [4/42], [94mLoss[0m : 2.47024
[1mStep[0m  [8/42], [94mLoss[0m : 2.28148
[1mStep[0m  [12/42], [94mLoss[0m : 2.62192
[1mStep[0m  [16/42], [94mLoss[0m : 2.73570
[1mStep[0m  [20/42], [94mLoss[0m : 2.35060
[1mStep[0m  [24/42], [94mLoss[0m : 2.30012
[1mStep[0m  [28/42], [94mLoss[0m : 2.45618
[1mStep[0m  [32/42], [94mLoss[0m : 2.40442
[1mStep[0m  [36/42], [94mLoss[0m : 2.41068
[1mStep[0m  [40/42], [94mLoss[0m : 2.41028

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.347, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45447
[1mStep[0m  [4/42], [94mLoss[0m : 2.26186
[1mStep[0m  [8/42], [94mLoss[0m : 2.64817
[1mStep[0m  [12/42], [94mLoss[0m : 2.36983
[1mStep[0m  [16/42], [94mLoss[0m : 2.60103
[1mStep[0m  [20/42], [94mLoss[0m : 2.41621
[1mStep[0m  [24/42], [94mLoss[0m : 2.43818
[1mStep[0m  [28/42], [94mLoss[0m : 2.45644
[1mStep[0m  [32/42], [94mLoss[0m : 2.43557
[1mStep[0m  [36/42], [94mLoss[0m : 2.37633
[1mStep[0m  [40/42], [94mLoss[0m : 2.55002

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.350, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51420
[1mStep[0m  [4/42], [94mLoss[0m : 2.52290
[1mStep[0m  [8/42], [94mLoss[0m : 2.33556
[1mStep[0m  [12/42], [94mLoss[0m : 2.59281
[1mStep[0m  [16/42], [94mLoss[0m : 2.33348
[1mStep[0m  [20/42], [94mLoss[0m : 2.45000
[1mStep[0m  [24/42], [94mLoss[0m : 2.58166
[1mStep[0m  [28/42], [94mLoss[0m : 2.16994
[1mStep[0m  [32/42], [94mLoss[0m : 2.36081
[1mStep[0m  [36/42], [94mLoss[0m : 2.58836
[1mStep[0m  [40/42], [94mLoss[0m : 2.62012

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.372, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.65423
[1mStep[0m  [4/42], [94mLoss[0m : 2.49040
[1mStep[0m  [8/42], [94mLoss[0m : 2.46001
[1mStep[0m  [12/42], [94mLoss[0m : 2.50158
[1mStep[0m  [16/42], [94mLoss[0m : 2.29860
[1mStep[0m  [20/42], [94mLoss[0m : 2.63128
[1mStep[0m  [24/42], [94mLoss[0m : 2.28099
[1mStep[0m  [28/42], [94mLoss[0m : 2.32223
[1mStep[0m  [32/42], [94mLoss[0m : 2.37473
[1mStep[0m  [36/42], [94mLoss[0m : 2.54673
[1mStep[0m  [40/42], [94mLoss[0m : 2.54157

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.349, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53295
[1mStep[0m  [4/42], [94mLoss[0m : 2.45301
[1mStep[0m  [8/42], [94mLoss[0m : 2.42527
[1mStep[0m  [12/42], [94mLoss[0m : 2.36446
[1mStep[0m  [16/42], [94mLoss[0m : 2.33928
[1mStep[0m  [20/42], [94mLoss[0m : 2.27113
[1mStep[0m  [24/42], [94mLoss[0m : 2.68350
[1mStep[0m  [28/42], [94mLoss[0m : 2.46096
[1mStep[0m  [32/42], [94mLoss[0m : 2.36725
[1mStep[0m  [36/42], [94mLoss[0m : 2.37353
[1mStep[0m  [40/42], [94mLoss[0m : 2.18676

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.342, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.30583
[1mStep[0m  [4/42], [94mLoss[0m : 2.40247
[1mStep[0m  [8/42], [94mLoss[0m : 2.44112
[1mStep[0m  [12/42], [94mLoss[0m : 2.44420
[1mStep[0m  [16/42], [94mLoss[0m : 2.19863
[1mStep[0m  [20/42], [94mLoss[0m : 2.45404
[1mStep[0m  [24/42], [94mLoss[0m : 2.69374
[1mStep[0m  [28/42], [94mLoss[0m : 2.44191
[1mStep[0m  [32/42], [94mLoss[0m : 2.40509
[1mStep[0m  [36/42], [94mLoss[0m : 2.50391
[1mStep[0m  [40/42], [94mLoss[0m : 2.30376

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.353, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38353
[1mStep[0m  [4/42], [94mLoss[0m : 2.39303
[1mStep[0m  [8/42], [94mLoss[0m : 2.40301
[1mStep[0m  [12/42], [94mLoss[0m : 2.47563
[1mStep[0m  [16/42], [94mLoss[0m : 2.23326
[1mStep[0m  [20/42], [94mLoss[0m : 2.75913
[1mStep[0m  [24/42], [94mLoss[0m : 2.44331
[1mStep[0m  [28/42], [94mLoss[0m : 2.35201
[1mStep[0m  [32/42], [94mLoss[0m : 2.66004
[1mStep[0m  [36/42], [94mLoss[0m : 2.42684
[1mStep[0m  [40/42], [94mLoss[0m : 2.49660

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.356, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54922
[1mStep[0m  [4/42], [94mLoss[0m : 2.29537
[1mStep[0m  [8/42], [94mLoss[0m : 2.46240
[1mStep[0m  [12/42], [94mLoss[0m : 2.38867
[1mStep[0m  [16/42], [94mLoss[0m : 2.72014
[1mStep[0m  [20/42], [94mLoss[0m : 2.41461
[1mStep[0m  [24/42], [94mLoss[0m : 2.57067
[1mStep[0m  [28/42], [94mLoss[0m : 2.35477
[1mStep[0m  [32/42], [94mLoss[0m : 2.67998
[1mStep[0m  [36/42], [94mLoss[0m : 2.54616
[1mStep[0m  [40/42], [94mLoss[0m : 2.44592

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.342, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27545
[1mStep[0m  [4/42], [94mLoss[0m : 2.44029
[1mStep[0m  [8/42], [94mLoss[0m : 2.40244
[1mStep[0m  [12/42], [94mLoss[0m : 2.61259
[1mStep[0m  [16/42], [94mLoss[0m : 2.30434
[1mStep[0m  [20/42], [94mLoss[0m : 2.56141
[1mStep[0m  [24/42], [94mLoss[0m : 2.40729
[1mStep[0m  [28/42], [94mLoss[0m : 2.52354
[1mStep[0m  [32/42], [94mLoss[0m : 2.41453
[1mStep[0m  [36/42], [94mLoss[0m : 2.53772
[1mStep[0m  [40/42], [94mLoss[0m : 2.65387

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.338, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.325
====================================

Phase 1 - Evaluation MAE:  2.3251277719225203
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 2.37027
[1mStep[0m  [4/42], [94mLoss[0m : 2.63313
[1mStep[0m  [8/42], [94mLoss[0m : 2.36382
[1mStep[0m  [12/42], [94mLoss[0m : 2.44605
[1mStep[0m  [16/42], [94mLoss[0m : 2.68283
[1mStep[0m  [20/42], [94mLoss[0m : 2.31187
[1mStep[0m  [24/42], [94mLoss[0m : 2.73911
[1mStep[0m  [28/42], [94mLoss[0m : 2.63170
[1mStep[0m  [32/42], [94mLoss[0m : 2.31674
[1mStep[0m  [36/42], [94mLoss[0m : 2.38727
[1mStep[0m  [40/42], [94mLoss[0m : 2.58332

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.323, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.65210
[1mStep[0m  [4/42], [94mLoss[0m : 2.37542
[1mStep[0m  [8/42], [94mLoss[0m : 2.33764
[1mStep[0m  [12/42], [94mLoss[0m : 2.53161
[1mStep[0m  [16/42], [94mLoss[0m : 2.26015
[1mStep[0m  [20/42], [94mLoss[0m : 2.37210
[1mStep[0m  [24/42], [94mLoss[0m : 2.51482
[1mStep[0m  [28/42], [94mLoss[0m : 2.59174
[1mStep[0m  [32/42], [94mLoss[0m : 2.30070
[1mStep[0m  [36/42], [94mLoss[0m : 2.35956
[1mStep[0m  [40/42], [94mLoss[0m : 2.22610

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.362, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39004
[1mStep[0m  [4/42], [94mLoss[0m : 2.28412
[1mStep[0m  [8/42], [94mLoss[0m : 2.58964
[1mStep[0m  [12/42], [94mLoss[0m : 2.38714
[1mStep[0m  [16/42], [94mLoss[0m : 2.31978
[1mStep[0m  [20/42], [94mLoss[0m : 2.28447
[1mStep[0m  [24/42], [94mLoss[0m : 2.37254
[1mStep[0m  [28/42], [94mLoss[0m : 2.48013
[1mStep[0m  [32/42], [94mLoss[0m : 2.29284
[1mStep[0m  [36/42], [94mLoss[0m : 2.30164
[1mStep[0m  [40/42], [94mLoss[0m : 2.40865

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.375, [92mTest[0m: 2.374, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.63802
[1mStep[0m  [4/42], [94mLoss[0m : 2.12090
[1mStep[0m  [8/42], [94mLoss[0m : 2.40350
[1mStep[0m  [12/42], [94mLoss[0m : 2.27097
[1mStep[0m  [16/42], [94mLoss[0m : 2.20488
[1mStep[0m  [20/42], [94mLoss[0m : 2.42169
[1mStep[0m  [24/42], [94mLoss[0m : 2.32845
[1mStep[0m  [28/42], [94mLoss[0m : 2.07316
[1mStep[0m  [32/42], [94mLoss[0m : 2.42658
[1mStep[0m  [36/42], [94mLoss[0m : 2.28446
[1mStep[0m  [40/42], [94mLoss[0m : 2.31569

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.330, [92mTest[0m: 2.383, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29106
[1mStep[0m  [4/42], [94mLoss[0m : 2.33250
[1mStep[0m  [8/42], [94mLoss[0m : 2.39437
[1mStep[0m  [12/42], [94mLoss[0m : 2.04179
[1mStep[0m  [16/42], [94mLoss[0m : 2.16052
[1mStep[0m  [20/42], [94mLoss[0m : 2.15116
[1mStep[0m  [24/42], [94mLoss[0m : 2.29489
[1mStep[0m  [28/42], [94mLoss[0m : 2.13995
[1mStep[0m  [32/42], [94mLoss[0m : 2.41847
[1mStep[0m  [36/42], [94mLoss[0m : 2.29456
[1mStep[0m  [40/42], [94mLoss[0m : 2.40218

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.277, [92mTest[0m: 2.516, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.13826
[1mStep[0m  [4/42], [94mLoss[0m : 2.14308
[1mStep[0m  [8/42], [94mLoss[0m : 2.20032
[1mStep[0m  [12/42], [94mLoss[0m : 2.24373
[1mStep[0m  [16/42], [94mLoss[0m : 2.22403
[1mStep[0m  [20/42], [94mLoss[0m : 2.24164
[1mStep[0m  [24/42], [94mLoss[0m : 2.11008
[1mStep[0m  [28/42], [94mLoss[0m : 2.28338
[1mStep[0m  [32/42], [94mLoss[0m : 2.26339
[1mStep[0m  [36/42], [94mLoss[0m : 2.20553
[1mStep[0m  [40/42], [94mLoss[0m : 2.29765

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.243, [92mTest[0m: 2.409, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.08183
[1mStep[0m  [4/42], [94mLoss[0m : 2.09701
[1mStep[0m  [8/42], [94mLoss[0m : 2.22772
[1mStep[0m  [12/42], [94mLoss[0m : 2.33130
[1mStep[0m  [16/42], [94mLoss[0m : 2.02895
[1mStep[0m  [20/42], [94mLoss[0m : 1.96929
[1mStep[0m  [24/42], [94mLoss[0m : 2.10779
[1mStep[0m  [28/42], [94mLoss[0m : 2.18073
[1mStep[0m  [32/42], [94mLoss[0m : 2.07474
[1mStep[0m  [36/42], [94mLoss[0m : 2.26484
[1mStep[0m  [40/42], [94mLoss[0m : 1.95711

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.180, [92mTest[0m: 2.474, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.20817
[1mStep[0m  [4/42], [94mLoss[0m : 2.08457
[1mStep[0m  [8/42], [94mLoss[0m : 2.18141
[1mStep[0m  [12/42], [94mLoss[0m : 2.22797
[1mStep[0m  [16/42], [94mLoss[0m : 2.18268
[1mStep[0m  [20/42], [94mLoss[0m : 2.11558
[1mStep[0m  [24/42], [94mLoss[0m : 2.08887
[1mStep[0m  [28/42], [94mLoss[0m : 2.12673
[1mStep[0m  [32/42], [94mLoss[0m : 2.04797
[1mStep[0m  [36/42], [94mLoss[0m : 2.12754
[1mStep[0m  [40/42], [94mLoss[0m : 2.09010

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.148, [92mTest[0m: 2.484, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.85172
[1mStep[0m  [4/42], [94mLoss[0m : 2.22209
[1mStep[0m  [8/42], [94mLoss[0m : 2.03615
[1mStep[0m  [12/42], [94mLoss[0m : 1.97817
[1mStep[0m  [16/42], [94mLoss[0m : 2.05145
[1mStep[0m  [20/42], [94mLoss[0m : 2.10535
[1mStep[0m  [24/42], [94mLoss[0m : 2.23818
[1mStep[0m  [28/42], [94mLoss[0m : 1.94894
[1mStep[0m  [32/42], [94mLoss[0m : 2.19052
[1mStep[0m  [36/42], [94mLoss[0m : 2.09961
[1mStep[0m  [40/42], [94mLoss[0m : 1.99484

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.099, [92mTest[0m: 2.436, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.88226
[1mStep[0m  [4/42], [94mLoss[0m : 1.89525
[1mStep[0m  [8/42], [94mLoss[0m : 2.07574
[1mStep[0m  [12/42], [94mLoss[0m : 2.03234
[1mStep[0m  [16/42], [94mLoss[0m : 2.01237
[1mStep[0m  [20/42], [94mLoss[0m : 2.23216
[1mStep[0m  [24/42], [94mLoss[0m : 2.05443
[1mStep[0m  [28/42], [94mLoss[0m : 1.82297
[1mStep[0m  [32/42], [94mLoss[0m : 1.94889
[1mStep[0m  [36/42], [94mLoss[0m : 2.28909
[1mStep[0m  [40/42], [94mLoss[0m : 1.92655

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.030, [92mTest[0m: 2.452, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.95595
[1mStep[0m  [4/42], [94mLoss[0m : 2.03131
[1mStep[0m  [8/42], [94mLoss[0m : 1.99394
[1mStep[0m  [12/42], [94mLoss[0m : 1.91710
[1mStep[0m  [16/42], [94mLoss[0m : 1.98177
[1mStep[0m  [20/42], [94mLoss[0m : 1.82092
[1mStep[0m  [24/42], [94mLoss[0m : 1.79535
[1mStep[0m  [28/42], [94mLoss[0m : 1.93059
[1mStep[0m  [32/42], [94mLoss[0m : 1.97518
[1mStep[0m  [36/42], [94mLoss[0m : 1.82142
[1mStep[0m  [40/42], [94mLoss[0m : 1.94915

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.964, [92mTest[0m: 2.454, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.04285
[1mStep[0m  [4/42], [94mLoss[0m : 1.77197
[1mStep[0m  [8/42], [94mLoss[0m : 1.93000
[1mStep[0m  [12/42], [94mLoss[0m : 2.08002
[1mStep[0m  [16/42], [94mLoss[0m : 1.81306
[1mStep[0m  [20/42], [94mLoss[0m : 1.89192
[1mStep[0m  [24/42], [94mLoss[0m : 1.94331
[1mStep[0m  [28/42], [94mLoss[0m : 1.95114
[1mStep[0m  [32/42], [94mLoss[0m : 1.79905
[1mStep[0m  [36/42], [94mLoss[0m : 1.88843
[1mStep[0m  [40/42], [94mLoss[0m : 1.96245

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.938, [92mTest[0m: 2.418, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.90019
[1mStep[0m  [4/42], [94mLoss[0m : 1.79326
[1mStep[0m  [8/42], [94mLoss[0m : 1.89527
[1mStep[0m  [12/42], [94mLoss[0m : 1.79587
[1mStep[0m  [16/42], [94mLoss[0m : 1.82803
[1mStep[0m  [20/42], [94mLoss[0m : 1.81111
[1mStep[0m  [24/42], [94mLoss[0m : 1.90118
[1mStep[0m  [28/42], [94mLoss[0m : 2.00395
[1mStep[0m  [32/42], [94mLoss[0m : 1.85757
[1mStep[0m  [36/42], [94mLoss[0m : 1.85148
[1mStep[0m  [40/42], [94mLoss[0m : 1.84231

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.903, [92mTest[0m: 2.521, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.86098
[1mStep[0m  [4/42], [94mLoss[0m : 1.84022
[1mStep[0m  [8/42], [94mLoss[0m : 1.70741
[1mStep[0m  [12/42], [94mLoss[0m : 1.80155
[1mStep[0m  [16/42], [94mLoss[0m : 2.03042
[1mStep[0m  [20/42], [94mLoss[0m : 1.75705
[1mStep[0m  [24/42], [94mLoss[0m : 1.78860
[1mStep[0m  [28/42], [94mLoss[0m : 1.89328
[1mStep[0m  [32/42], [94mLoss[0m : 1.71785
[1mStep[0m  [36/42], [94mLoss[0m : 1.91803
[1mStep[0m  [40/42], [94mLoss[0m : 1.80923

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.839, [92mTest[0m: 2.476, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.74358
[1mStep[0m  [4/42], [94mLoss[0m : 1.89629
[1mStep[0m  [8/42], [94mLoss[0m : 1.70503
[1mStep[0m  [12/42], [94mLoss[0m : 1.80010
[1mStep[0m  [16/42], [94mLoss[0m : 1.92401
[1mStep[0m  [20/42], [94mLoss[0m : 1.96339
[1mStep[0m  [24/42], [94mLoss[0m : 1.78628
[1mStep[0m  [28/42], [94mLoss[0m : 1.89987
[1mStep[0m  [32/42], [94mLoss[0m : 1.77417
[1mStep[0m  [36/42], [94mLoss[0m : 1.62259
[1mStep[0m  [40/42], [94mLoss[0m : 1.72337

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.814, [92mTest[0m: 2.443, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.82807
[1mStep[0m  [4/42], [94mLoss[0m : 1.81949
[1mStep[0m  [8/42], [94mLoss[0m : 1.68024
[1mStep[0m  [12/42], [94mLoss[0m : 1.74494
[1mStep[0m  [16/42], [94mLoss[0m : 1.67614
[1mStep[0m  [20/42], [94mLoss[0m : 1.78169
[1mStep[0m  [24/42], [94mLoss[0m : 1.61763
[1mStep[0m  [28/42], [94mLoss[0m : 1.86142
[1mStep[0m  [32/42], [94mLoss[0m : 1.81065
[1mStep[0m  [36/42], [94mLoss[0m : 1.74271
[1mStep[0m  [40/42], [94mLoss[0m : 1.75416

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.775, [92mTest[0m: 2.508, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.64399
[1mStep[0m  [4/42], [94mLoss[0m : 1.81490
[1mStep[0m  [8/42], [94mLoss[0m : 1.78434
[1mStep[0m  [12/42], [94mLoss[0m : 1.59432
[1mStep[0m  [16/42], [94mLoss[0m : 1.82317
[1mStep[0m  [20/42], [94mLoss[0m : 1.80424
[1mStep[0m  [24/42], [94mLoss[0m : 1.60721
[1mStep[0m  [28/42], [94mLoss[0m : 1.79455
[1mStep[0m  [32/42], [94mLoss[0m : 1.85961
[1mStep[0m  [36/42], [94mLoss[0m : 1.74825
[1mStep[0m  [40/42], [94mLoss[0m : 1.79689

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.752, [92mTest[0m: 2.475, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.74274
[1mStep[0m  [4/42], [94mLoss[0m : 1.65355
[1mStep[0m  [8/42], [94mLoss[0m : 1.76481
[1mStep[0m  [12/42], [94mLoss[0m : 1.72601
[1mStep[0m  [16/42], [94mLoss[0m : 1.71636
[1mStep[0m  [20/42], [94mLoss[0m : 1.81674
[1mStep[0m  [24/42], [94mLoss[0m : 1.83862
[1mStep[0m  [28/42], [94mLoss[0m : 1.87738
[1mStep[0m  [32/42], [94mLoss[0m : 1.68309
[1mStep[0m  [36/42], [94mLoss[0m : 1.68141
[1mStep[0m  [40/42], [94mLoss[0m : 1.73051

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.720, [92mTest[0m: 2.493, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.49013
[1mStep[0m  [4/42], [94mLoss[0m : 1.54492
[1mStep[0m  [8/42], [94mLoss[0m : 1.60946
[1mStep[0m  [12/42], [94mLoss[0m : 1.68446
[1mStep[0m  [16/42], [94mLoss[0m : 1.69157
[1mStep[0m  [20/42], [94mLoss[0m : 1.66563
[1mStep[0m  [24/42], [94mLoss[0m : 1.78926
[1mStep[0m  [28/42], [94mLoss[0m : 1.62519
[1mStep[0m  [32/42], [94mLoss[0m : 1.73218
[1mStep[0m  [36/42], [94mLoss[0m : 1.74425
[1mStep[0m  [40/42], [94mLoss[0m : 1.62992

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.683, [92mTest[0m: 2.466, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.70825
[1mStep[0m  [4/42], [94mLoss[0m : 1.61042
[1mStep[0m  [8/42], [94mLoss[0m : 1.54050
[1mStep[0m  [12/42], [94mLoss[0m : 1.59279
[1mStep[0m  [16/42], [94mLoss[0m : 1.74216
[1mStep[0m  [20/42], [94mLoss[0m : 1.61479
[1mStep[0m  [24/42], [94mLoss[0m : 1.66130
[1mStep[0m  [28/42], [94mLoss[0m : 1.62948
[1mStep[0m  [32/42], [94mLoss[0m : 1.57423
[1mStep[0m  [36/42], [94mLoss[0m : 1.64362
[1mStep[0m  [40/42], [94mLoss[0m : 1.80597

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.654, [92mTest[0m: 2.462, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.60720
[1mStep[0m  [4/42], [94mLoss[0m : 1.58790
[1mStep[0m  [8/42], [94mLoss[0m : 1.54530
[1mStep[0m  [12/42], [94mLoss[0m : 1.55201
[1mStep[0m  [16/42], [94mLoss[0m : 1.61643
[1mStep[0m  [20/42], [94mLoss[0m : 1.63953
[1mStep[0m  [24/42], [94mLoss[0m : 1.65834
[1mStep[0m  [28/42], [94mLoss[0m : 1.72556
[1mStep[0m  [32/42], [94mLoss[0m : 1.77539
[1mStep[0m  [36/42], [94mLoss[0m : 1.68933
[1mStep[0m  [40/42], [94mLoss[0m : 1.67064

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.625, [92mTest[0m: 2.568, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.63568
[1mStep[0m  [4/42], [94mLoss[0m : 1.47087
[1mStep[0m  [8/42], [94mLoss[0m : 1.45206
[1mStep[0m  [12/42], [94mLoss[0m : 1.70874
[1mStep[0m  [16/42], [94mLoss[0m : 1.57949
[1mStep[0m  [20/42], [94mLoss[0m : 1.47847
[1mStep[0m  [24/42], [94mLoss[0m : 1.76214
[1mStep[0m  [28/42], [94mLoss[0m : 1.65891
[1mStep[0m  [32/42], [94mLoss[0m : 1.33317
[1mStep[0m  [36/42], [94mLoss[0m : 1.49647
[1mStep[0m  [40/42], [94mLoss[0m : 1.59199

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.587, [92mTest[0m: 2.475, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.66098
[1mStep[0m  [4/42], [94mLoss[0m : 1.56358
[1mStep[0m  [8/42], [94mLoss[0m : 1.51106
[1mStep[0m  [12/42], [94mLoss[0m : 1.57097
[1mStep[0m  [16/42], [94mLoss[0m : 1.60648
[1mStep[0m  [20/42], [94mLoss[0m : 1.60131
[1mStep[0m  [24/42], [94mLoss[0m : 1.52223
[1mStep[0m  [28/42], [94mLoss[0m : 1.66794
[1mStep[0m  [32/42], [94mLoss[0m : 1.71279
[1mStep[0m  [36/42], [94mLoss[0m : 1.58498
[1mStep[0m  [40/42], [94mLoss[0m : 1.56593

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.563, [92mTest[0m: 2.495, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.55384
[1mStep[0m  [4/42], [94mLoss[0m : 1.60792
[1mStep[0m  [8/42], [94mLoss[0m : 1.51343
[1mStep[0m  [12/42], [94mLoss[0m : 1.56575
[1mStep[0m  [16/42], [94mLoss[0m : 1.63639
[1mStep[0m  [20/42], [94mLoss[0m : 1.53216
[1mStep[0m  [24/42], [94mLoss[0m : 1.49066
[1mStep[0m  [28/42], [94mLoss[0m : 1.51106
[1mStep[0m  [32/42], [94mLoss[0m : 1.51438
[1mStep[0m  [36/42], [94mLoss[0m : 1.51701
[1mStep[0m  [40/42], [94mLoss[0m : 1.49753

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.544, [92mTest[0m: 2.440, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.54962
[1mStep[0m  [4/42], [94mLoss[0m : 1.57531
[1mStep[0m  [8/42], [94mLoss[0m : 1.37463
[1mStep[0m  [12/42], [94mLoss[0m : 1.49308
[1mStep[0m  [16/42], [94mLoss[0m : 1.48618
[1mStep[0m  [20/42], [94mLoss[0m : 1.34728
[1mStep[0m  [24/42], [94mLoss[0m : 1.40157
[1mStep[0m  [28/42], [94mLoss[0m : 1.48476
[1mStep[0m  [32/42], [94mLoss[0m : 1.66361
[1mStep[0m  [36/42], [94mLoss[0m : 1.53251
[1mStep[0m  [40/42], [94mLoss[0m : 1.53536

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.500, [92mTest[0m: 2.436, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.45230
[1mStep[0m  [4/42], [94mLoss[0m : 1.38327
[1mStep[0m  [8/42], [94mLoss[0m : 1.60975
[1mStep[0m  [12/42], [94mLoss[0m : 1.45248
[1mStep[0m  [16/42], [94mLoss[0m : 1.54235
[1mStep[0m  [20/42], [94mLoss[0m : 1.58114
[1mStep[0m  [24/42], [94mLoss[0m : 1.53648
[1mStep[0m  [28/42], [94mLoss[0m : 1.46909
[1mStep[0m  [32/42], [94mLoss[0m : 1.52933
[1mStep[0m  [36/42], [94mLoss[0m : 1.54493
[1mStep[0m  [40/42], [94mLoss[0m : 1.53188

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.498, [92mTest[0m: 2.535, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.50532
[1mStep[0m  [4/42], [94mLoss[0m : 1.39939
[1mStep[0m  [8/42], [94mLoss[0m : 1.49507
[1mStep[0m  [12/42], [94mLoss[0m : 1.36280
[1mStep[0m  [16/42], [94mLoss[0m : 1.39974
[1mStep[0m  [20/42], [94mLoss[0m : 1.48265
[1mStep[0m  [24/42], [94mLoss[0m : 1.43001
[1mStep[0m  [28/42], [94mLoss[0m : 1.53060
[1mStep[0m  [32/42], [94mLoss[0m : 1.49406
[1mStep[0m  [36/42], [94mLoss[0m : 1.49775
[1mStep[0m  [40/42], [94mLoss[0m : 1.46293

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.472, [92mTest[0m: 2.463, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.24009
[1mStep[0m  [4/42], [94mLoss[0m : 1.45521
[1mStep[0m  [8/42], [94mLoss[0m : 1.53604
[1mStep[0m  [12/42], [94mLoss[0m : 1.53311
[1mStep[0m  [16/42], [94mLoss[0m : 1.37974
[1mStep[0m  [20/42], [94mLoss[0m : 1.40697
[1mStep[0m  [24/42], [94mLoss[0m : 1.47506
[1mStep[0m  [28/42], [94mLoss[0m : 1.34241
[1mStep[0m  [32/42], [94mLoss[0m : 1.37795
[1mStep[0m  [36/42], [94mLoss[0m : 1.47236
[1mStep[0m  [40/42], [94mLoss[0m : 1.55643

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.455, [92mTest[0m: 2.442, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.30619
[1mStep[0m  [4/42], [94mLoss[0m : 1.41326
[1mStep[0m  [8/42], [94mLoss[0m : 1.39532
[1mStep[0m  [12/42], [94mLoss[0m : 1.37385
[1mStep[0m  [16/42], [94mLoss[0m : 1.42093
[1mStep[0m  [20/42], [94mLoss[0m : 1.39097
[1mStep[0m  [24/42], [94mLoss[0m : 1.40391
[1mStep[0m  [28/42], [94mLoss[0m : 1.29725
[1mStep[0m  [32/42], [94mLoss[0m : 1.63434
[1mStep[0m  [36/42], [94mLoss[0m : 1.43093
[1mStep[0m  [40/42], [94mLoss[0m : 1.36971

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.430, [92mTest[0m: 2.450, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.38362
[1mStep[0m  [4/42], [94mLoss[0m : 1.56523
[1mStep[0m  [8/42], [94mLoss[0m : 1.48725
[1mStep[0m  [12/42], [94mLoss[0m : 1.30275
[1mStep[0m  [16/42], [94mLoss[0m : 1.45928
[1mStep[0m  [20/42], [94mLoss[0m : 1.37711
[1mStep[0m  [24/42], [94mLoss[0m : 1.45049
[1mStep[0m  [28/42], [94mLoss[0m : 1.48639
[1mStep[0m  [32/42], [94mLoss[0m : 1.43339
[1mStep[0m  [36/42], [94mLoss[0m : 1.45723
[1mStep[0m  [40/42], [94mLoss[0m : 1.53243

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.429, [92mTest[0m: 2.444, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.456
====================================

Phase 2 - Evaluation MAE:  2.456220729010446
MAE score P1      2.325128
MAE score P2      2.456221
loss              1.428526
learning_rate     0.007525
batch_size             256
hidden_sizes         [100]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.5
weight_decay        0.0001
Name: 23, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 10.58143
[1mStep[0m  [8/84], [94mLoss[0m : 10.91900
[1mStep[0m  [16/84], [94mLoss[0m : 11.02097
[1mStep[0m  [24/84], [94mLoss[0m : 10.49790
[1mStep[0m  [32/84], [94mLoss[0m : 11.09614
[1mStep[0m  [40/84], [94mLoss[0m : 10.42201
[1mStep[0m  [48/84], [94mLoss[0m : 10.26977
[1mStep[0m  [56/84], [94mLoss[0m : 9.94852
[1mStep[0m  [64/84], [94mLoss[0m : 9.51143
[1mStep[0m  [72/84], [94mLoss[0m : 10.04001
[1mStep[0m  [80/84], [94mLoss[0m : 9.23940

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.370, [92mTest[0m: 10.945, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.15641
[1mStep[0m  [8/84], [94mLoss[0m : 9.90042
[1mStep[0m  [16/84], [94mLoss[0m : 9.51566
[1mStep[0m  [24/84], [94mLoss[0m : 9.44777
[1mStep[0m  [32/84], [94mLoss[0m : 9.09238
[1mStep[0m  [40/84], [94mLoss[0m : 8.91877
[1mStep[0m  [48/84], [94mLoss[0m : 8.42468
[1mStep[0m  [56/84], [94mLoss[0m : 8.59749
[1mStep[0m  [64/84], [94mLoss[0m : 8.85046
[1mStep[0m  [72/84], [94mLoss[0m : 8.69481
[1mStep[0m  [80/84], [94mLoss[0m : 7.95383

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 8.940, [92mTest[0m: 9.433, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.26657
[1mStep[0m  [8/84], [94mLoss[0m : 8.18613
[1mStep[0m  [16/84], [94mLoss[0m : 7.66809
[1mStep[0m  [24/84], [94mLoss[0m : 7.48685
[1mStep[0m  [32/84], [94mLoss[0m : 7.46859
[1mStep[0m  [40/84], [94mLoss[0m : 7.46592
[1mStep[0m  [48/84], [94mLoss[0m : 6.88966
[1mStep[0m  [56/84], [94mLoss[0m : 7.24525
[1mStep[0m  [64/84], [94mLoss[0m : 6.85768
[1mStep[0m  [72/84], [94mLoss[0m : 7.12617
[1mStep[0m  [80/84], [94mLoss[0m : 6.45860

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 7.318, [92mTest[0m: 7.424, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.69335
[1mStep[0m  [8/84], [94mLoss[0m : 6.46430
[1mStep[0m  [16/84], [94mLoss[0m : 6.74784
[1mStep[0m  [24/84], [94mLoss[0m : 6.04864
[1mStep[0m  [32/84], [94mLoss[0m : 5.70281
[1mStep[0m  [40/84], [94mLoss[0m : 6.52414
[1mStep[0m  [48/84], [94mLoss[0m : 6.04788
[1mStep[0m  [56/84], [94mLoss[0m : 5.41088
[1mStep[0m  [64/84], [94mLoss[0m : 4.97818
[1mStep[0m  [72/84], [94mLoss[0m : 5.63457
[1mStep[0m  [80/84], [94mLoss[0m : 5.14210

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 5.924, [92mTest[0m: 5.766, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.34547
[1mStep[0m  [8/84], [94mLoss[0m : 4.84795
[1mStep[0m  [16/84], [94mLoss[0m : 4.66590
[1mStep[0m  [24/84], [94mLoss[0m : 4.39113
[1mStep[0m  [32/84], [94mLoss[0m : 4.84176
[1mStep[0m  [40/84], [94mLoss[0m : 3.95490
[1mStep[0m  [48/84], [94mLoss[0m : 4.04171
[1mStep[0m  [56/84], [94mLoss[0m : 4.02209
[1mStep[0m  [64/84], [94mLoss[0m : 3.56073
[1mStep[0m  [72/84], [94mLoss[0m : 3.24111
[1mStep[0m  [80/84], [94mLoss[0m : 3.33278

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 4.336, [92mTest[0m: 4.195, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.52927
[1mStep[0m  [8/84], [94mLoss[0m : 2.97775
[1mStep[0m  [16/84], [94mLoss[0m : 2.74880
[1mStep[0m  [24/84], [94mLoss[0m : 3.15775
[1mStep[0m  [32/84], [94mLoss[0m : 2.78947
[1mStep[0m  [40/84], [94mLoss[0m : 2.93480
[1mStep[0m  [48/84], [94mLoss[0m : 2.86771
[1mStep[0m  [56/84], [94mLoss[0m : 2.86994
[1mStep[0m  [64/84], [94mLoss[0m : 2.64386
[1mStep[0m  [72/84], [94mLoss[0m : 2.55269
[1mStep[0m  [80/84], [94mLoss[0m : 2.60777

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.979, [92mTest[0m: 2.705, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67515
[1mStep[0m  [8/84], [94mLoss[0m : 2.83926
[1mStep[0m  [16/84], [94mLoss[0m : 2.72378
[1mStep[0m  [24/84], [94mLoss[0m : 2.18201
[1mStep[0m  [32/84], [94mLoss[0m : 2.92128
[1mStep[0m  [40/84], [94mLoss[0m : 2.73769
[1mStep[0m  [48/84], [94mLoss[0m : 2.57407
[1mStep[0m  [56/84], [94mLoss[0m : 2.45714
[1mStep[0m  [64/84], [94mLoss[0m : 2.53479
[1mStep[0m  [72/84], [94mLoss[0m : 2.75854
[1mStep[0m  [80/84], [94mLoss[0m : 2.52881

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.633, [92mTest[0m: 2.397, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.82537
[1mStep[0m  [8/84], [94mLoss[0m : 2.50700
[1mStep[0m  [16/84], [94mLoss[0m : 2.30138
[1mStep[0m  [24/84], [94mLoss[0m : 2.73559
[1mStep[0m  [32/84], [94mLoss[0m : 2.52818
[1mStep[0m  [40/84], [94mLoss[0m : 2.83673
[1mStep[0m  [48/84], [94mLoss[0m : 2.76280
[1mStep[0m  [56/84], [94mLoss[0m : 2.62814
[1mStep[0m  [64/84], [94mLoss[0m : 2.50217
[1mStep[0m  [72/84], [94mLoss[0m : 2.45332
[1mStep[0m  [80/84], [94mLoss[0m : 2.37055

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.572, [92mTest[0m: 2.370, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.74535
[1mStep[0m  [8/84], [94mLoss[0m : 2.15796
[1mStep[0m  [16/84], [94mLoss[0m : 2.19109
[1mStep[0m  [24/84], [94mLoss[0m : 2.54102
[1mStep[0m  [32/84], [94mLoss[0m : 2.63214
[1mStep[0m  [40/84], [94mLoss[0m : 2.76417
[1mStep[0m  [48/84], [94mLoss[0m : 2.61684
[1mStep[0m  [56/84], [94mLoss[0m : 2.22774
[1mStep[0m  [64/84], [94mLoss[0m : 2.79127
[1mStep[0m  [72/84], [94mLoss[0m : 2.91482
[1mStep[0m  [80/84], [94mLoss[0m : 2.84816

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.566, [92mTest[0m: 2.380, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.34476
[1mStep[0m  [8/84], [94mLoss[0m : 2.49291
[1mStep[0m  [16/84], [94mLoss[0m : 3.16713
[1mStep[0m  [24/84], [94mLoss[0m : 2.18629
[1mStep[0m  [32/84], [94mLoss[0m : 2.45256
[1mStep[0m  [40/84], [94mLoss[0m : 2.60352
[1mStep[0m  [48/84], [94mLoss[0m : 2.32453
[1mStep[0m  [56/84], [94mLoss[0m : 2.29032
[1mStep[0m  [64/84], [94mLoss[0m : 2.55502
[1mStep[0m  [72/84], [94mLoss[0m : 2.54460
[1mStep[0m  [80/84], [94mLoss[0m : 2.48015

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.546, [92mTest[0m: 2.369, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40417
[1mStep[0m  [8/84], [94mLoss[0m : 2.59547
[1mStep[0m  [16/84], [94mLoss[0m : 2.59602
[1mStep[0m  [24/84], [94mLoss[0m : 2.56653
[1mStep[0m  [32/84], [94mLoss[0m : 2.43901
[1mStep[0m  [40/84], [94mLoss[0m : 2.57787
[1mStep[0m  [48/84], [94mLoss[0m : 2.41543
[1mStep[0m  [56/84], [94mLoss[0m : 2.25636
[1mStep[0m  [64/84], [94mLoss[0m : 2.41359
[1mStep[0m  [72/84], [94mLoss[0m : 2.24106
[1mStep[0m  [80/84], [94mLoss[0m : 2.59641

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.510, [92mTest[0m: 2.358, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.80367
[1mStep[0m  [8/84], [94mLoss[0m : 2.65661
[1mStep[0m  [16/84], [94mLoss[0m : 2.54040
[1mStep[0m  [24/84], [94mLoss[0m : 2.53828
[1mStep[0m  [32/84], [94mLoss[0m : 2.19987
[1mStep[0m  [40/84], [94mLoss[0m : 2.59364
[1mStep[0m  [48/84], [94mLoss[0m : 2.40072
[1mStep[0m  [56/84], [94mLoss[0m : 2.53771
[1mStep[0m  [64/84], [94mLoss[0m : 2.59055
[1mStep[0m  [72/84], [94mLoss[0m : 2.43897
[1mStep[0m  [80/84], [94mLoss[0m : 2.10463

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.378, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.71974
[1mStep[0m  [8/84], [94mLoss[0m : 2.91213
[1mStep[0m  [16/84], [94mLoss[0m : 2.21870
[1mStep[0m  [24/84], [94mLoss[0m : 2.32194
[1mStep[0m  [32/84], [94mLoss[0m : 2.78110
[1mStep[0m  [40/84], [94mLoss[0m : 2.39131
[1mStep[0m  [48/84], [94mLoss[0m : 2.67125
[1mStep[0m  [56/84], [94mLoss[0m : 2.45210
[1mStep[0m  [64/84], [94mLoss[0m : 2.79819
[1mStep[0m  [72/84], [94mLoss[0m : 2.50592
[1mStep[0m  [80/84], [94mLoss[0m : 2.52178

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.372, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52988
[1mStep[0m  [8/84], [94mLoss[0m : 2.49902
[1mStep[0m  [16/84], [94mLoss[0m : 2.28920
[1mStep[0m  [24/84], [94mLoss[0m : 2.34133
[1mStep[0m  [32/84], [94mLoss[0m : 2.74707
[1mStep[0m  [40/84], [94mLoss[0m : 2.39041
[1mStep[0m  [48/84], [94mLoss[0m : 2.74814
[1mStep[0m  [56/84], [94mLoss[0m : 2.43705
[1mStep[0m  [64/84], [94mLoss[0m : 2.48123
[1mStep[0m  [72/84], [94mLoss[0m : 2.60609
[1mStep[0m  [80/84], [94mLoss[0m : 2.57297

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.400, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41712
[1mStep[0m  [8/84], [94mLoss[0m : 2.54594
[1mStep[0m  [16/84], [94mLoss[0m : 2.39870
[1mStep[0m  [24/84], [94mLoss[0m : 2.36334
[1mStep[0m  [32/84], [94mLoss[0m : 2.53052
[1mStep[0m  [40/84], [94mLoss[0m : 2.50462
[1mStep[0m  [48/84], [94mLoss[0m : 2.76395
[1mStep[0m  [56/84], [94mLoss[0m : 2.20738
[1mStep[0m  [64/84], [94mLoss[0m : 2.68399
[1mStep[0m  [72/84], [94mLoss[0m : 2.34292
[1mStep[0m  [80/84], [94mLoss[0m : 2.54894

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.378, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45989
[1mStep[0m  [8/84], [94mLoss[0m : 2.63543
[1mStep[0m  [16/84], [94mLoss[0m : 2.75167
[1mStep[0m  [24/84], [94mLoss[0m : 2.45413
[1mStep[0m  [32/84], [94mLoss[0m : 2.08666
[1mStep[0m  [40/84], [94mLoss[0m : 2.57941
[1mStep[0m  [48/84], [94mLoss[0m : 2.40596
[1mStep[0m  [56/84], [94mLoss[0m : 2.40329
[1mStep[0m  [64/84], [94mLoss[0m : 2.59915
[1mStep[0m  [72/84], [94mLoss[0m : 2.39869
[1mStep[0m  [80/84], [94mLoss[0m : 2.59621

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.377, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47398
[1mStep[0m  [8/84], [94mLoss[0m : 2.60797
[1mStep[0m  [16/84], [94mLoss[0m : 2.39995
[1mStep[0m  [24/84], [94mLoss[0m : 2.08836
[1mStep[0m  [32/84], [94mLoss[0m : 2.65065
[1mStep[0m  [40/84], [94mLoss[0m : 2.43912
[1mStep[0m  [48/84], [94mLoss[0m : 2.48477
[1mStep[0m  [56/84], [94mLoss[0m : 2.52665
[1mStep[0m  [64/84], [94mLoss[0m : 2.10083
[1mStep[0m  [72/84], [94mLoss[0m : 2.45279
[1mStep[0m  [80/84], [94mLoss[0m : 2.43886

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.365, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46115
[1mStep[0m  [8/84], [94mLoss[0m : 2.47367
[1mStep[0m  [16/84], [94mLoss[0m : 2.71694
[1mStep[0m  [24/84], [94mLoss[0m : 2.61701
[1mStep[0m  [32/84], [94mLoss[0m : 2.31218
[1mStep[0m  [40/84], [94mLoss[0m : 2.38612
[1mStep[0m  [48/84], [94mLoss[0m : 2.55299
[1mStep[0m  [56/84], [94mLoss[0m : 2.33920
[1mStep[0m  [64/84], [94mLoss[0m : 2.74175
[1mStep[0m  [72/84], [94mLoss[0m : 2.44873
[1mStep[0m  [80/84], [94mLoss[0m : 2.32901

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.372, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.56673
[1mStep[0m  [8/84], [94mLoss[0m : 2.51136
[1mStep[0m  [16/84], [94mLoss[0m : 2.39892
[1mStep[0m  [24/84], [94mLoss[0m : 2.50086
[1mStep[0m  [32/84], [94mLoss[0m : 2.33322
[1mStep[0m  [40/84], [94mLoss[0m : 2.28904
[1mStep[0m  [48/84], [94mLoss[0m : 2.35499
[1mStep[0m  [56/84], [94mLoss[0m : 2.60419
[1mStep[0m  [64/84], [94mLoss[0m : 2.54485
[1mStep[0m  [72/84], [94mLoss[0m : 2.36308
[1mStep[0m  [80/84], [94mLoss[0m : 2.33924

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.372, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.19651
[1mStep[0m  [8/84], [94mLoss[0m : 2.39966
[1mStep[0m  [16/84], [94mLoss[0m : 2.35593
[1mStep[0m  [24/84], [94mLoss[0m : 2.55347
[1mStep[0m  [32/84], [94mLoss[0m : 2.75731
[1mStep[0m  [40/84], [94mLoss[0m : 2.57640
[1mStep[0m  [48/84], [94mLoss[0m : 2.07002
[1mStep[0m  [56/84], [94mLoss[0m : 2.64134
[1mStep[0m  [64/84], [94mLoss[0m : 2.72221
[1mStep[0m  [72/84], [94mLoss[0m : 2.29432
[1mStep[0m  [80/84], [94mLoss[0m : 2.23310

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.403, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33831
[1mStep[0m  [8/84], [94mLoss[0m : 2.37257
[1mStep[0m  [16/84], [94mLoss[0m : 2.30887
[1mStep[0m  [24/84], [94mLoss[0m : 2.28392
[1mStep[0m  [32/84], [94mLoss[0m : 2.38612
[1mStep[0m  [40/84], [94mLoss[0m : 2.58227
[1mStep[0m  [48/84], [94mLoss[0m : 2.77504
[1mStep[0m  [56/84], [94mLoss[0m : 2.36956
[1mStep[0m  [64/84], [94mLoss[0m : 2.48693
[1mStep[0m  [72/84], [94mLoss[0m : 2.46666
[1mStep[0m  [80/84], [94mLoss[0m : 2.31731

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.372, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.17157
[1mStep[0m  [8/84], [94mLoss[0m : 2.38693
[1mStep[0m  [16/84], [94mLoss[0m : 2.34443
[1mStep[0m  [24/84], [94mLoss[0m : 2.43741
[1mStep[0m  [32/84], [94mLoss[0m : 2.67825
[1mStep[0m  [40/84], [94mLoss[0m : 2.22347
[1mStep[0m  [48/84], [94mLoss[0m : 2.67462
[1mStep[0m  [56/84], [94mLoss[0m : 2.43015
[1mStep[0m  [64/84], [94mLoss[0m : 2.30989
[1mStep[0m  [72/84], [94mLoss[0m : 2.61617
[1mStep[0m  [80/84], [94mLoss[0m : 2.28546

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.375, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.23071
[1mStep[0m  [8/84], [94mLoss[0m : 2.51247
[1mStep[0m  [16/84], [94mLoss[0m : 2.41933
[1mStep[0m  [24/84], [94mLoss[0m : 2.53483
[1mStep[0m  [32/84], [94mLoss[0m : 2.37217
[1mStep[0m  [40/84], [94mLoss[0m : 2.49338
[1mStep[0m  [48/84], [94mLoss[0m : 2.57057
[1mStep[0m  [56/84], [94mLoss[0m : 2.26705
[1mStep[0m  [64/84], [94mLoss[0m : 2.58919
[1mStep[0m  [72/84], [94mLoss[0m : 2.25876
[1mStep[0m  [80/84], [94mLoss[0m : 2.23942

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.341, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52476
[1mStep[0m  [8/84], [94mLoss[0m : 2.43266
[1mStep[0m  [16/84], [94mLoss[0m : 2.56529
[1mStep[0m  [24/84], [94mLoss[0m : 2.42437
[1mStep[0m  [32/84], [94mLoss[0m : 2.55625
[1mStep[0m  [40/84], [94mLoss[0m : 2.71390
[1mStep[0m  [48/84], [94mLoss[0m : 2.45097
[1mStep[0m  [56/84], [94mLoss[0m : 2.43173
[1mStep[0m  [64/84], [94mLoss[0m : 2.59421
[1mStep[0m  [72/84], [94mLoss[0m : 2.53653
[1mStep[0m  [80/84], [94mLoss[0m : 2.30256

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.367, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37655
[1mStep[0m  [8/84], [94mLoss[0m : 2.60674
[1mStep[0m  [16/84], [94mLoss[0m : 2.44831
[1mStep[0m  [24/84], [94mLoss[0m : 2.15766
[1mStep[0m  [32/84], [94mLoss[0m : 2.20754
[1mStep[0m  [40/84], [94mLoss[0m : 2.32577
[1mStep[0m  [48/84], [94mLoss[0m : 2.26603
[1mStep[0m  [56/84], [94mLoss[0m : 2.40241
[1mStep[0m  [64/84], [94mLoss[0m : 2.41311
[1mStep[0m  [72/84], [94mLoss[0m : 2.47349
[1mStep[0m  [80/84], [94mLoss[0m : 2.69869

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.357, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.10203
[1mStep[0m  [8/84], [94mLoss[0m : 2.63446
[1mStep[0m  [16/84], [94mLoss[0m : 2.34032
[1mStep[0m  [24/84], [94mLoss[0m : 2.54326
[1mStep[0m  [32/84], [94mLoss[0m : 2.37689
[1mStep[0m  [40/84], [94mLoss[0m : 2.51996
[1mStep[0m  [48/84], [94mLoss[0m : 2.23566
[1mStep[0m  [56/84], [94mLoss[0m : 2.34404
[1mStep[0m  [64/84], [94mLoss[0m : 2.31311
[1mStep[0m  [72/84], [94mLoss[0m : 2.36538
[1mStep[0m  [80/84], [94mLoss[0m : 2.52846

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.349, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31697
[1mStep[0m  [8/84], [94mLoss[0m : 2.23555
[1mStep[0m  [16/84], [94mLoss[0m : 2.12532
[1mStep[0m  [24/84], [94mLoss[0m : 2.73264
[1mStep[0m  [32/84], [94mLoss[0m : 2.54311
[1mStep[0m  [40/84], [94mLoss[0m : 2.17948
[1mStep[0m  [48/84], [94mLoss[0m : 2.26996
[1mStep[0m  [56/84], [94mLoss[0m : 2.14404
[1mStep[0m  [64/84], [94mLoss[0m : 2.42010
[1mStep[0m  [72/84], [94mLoss[0m : 2.16717
[1mStep[0m  [80/84], [94mLoss[0m : 2.38806

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.348, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.22773
[1mStep[0m  [8/84], [94mLoss[0m : 2.44631
[1mStep[0m  [16/84], [94mLoss[0m : 2.18165
[1mStep[0m  [24/84], [94mLoss[0m : 2.29870
[1mStep[0m  [32/84], [94mLoss[0m : 2.88221
[1mStep[0m  [40/84], [94mLoss[0m : 2.39404
[1mStep[0m  [48/84], [94mLoss[0m : 2.45474
[1mStep[0m  [56/84], [94mLoss[0m : 2.39118
[1mStep[0m  [64/84], [94mLoss[0m : 2.10700
[1mStep[0m  [72/84], [94mLoss[0m : 2.43371
[1mStep[0m  [80/84], [94mLoss[0m : 2.59287

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.348, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.63243
[1mStep[0m  [8/84], [94mLoss[0m : 2.58832
[1mStep[0m  [16/84], [94mLoss[0m : 2.30359
[1mStep[0m  [24/84], [94mLoss[0m : 2.67521
[1mStep[0m  [32/84], [94mLoss[0m : 2.28978
[1mStep[0m  [40/84], [94mLoss[0m : 2.57610
[1mStep[0m  [48/84], [94mLoss[0m : 2.70230
[1mStep[0m  [56/84], [94mLoss[0m : 2.37393
[1mStep[0m  [64/84], [94mLoss[0m : 2.26201
[1mStep[0m  [72/84], [94mLoss[0m : 2.11518
[1mStep[0m  [80/84], [94mLoss[0m : 2.39393

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.366, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37928
[1mStep[0m  [8/84], [94mLoss[0m : 2.37543
[1mStep[0m  [16/84], [94mLoss[0m : 2.12375
[1mStep[0m  [24/84], [94mLoss[0m : 2.17840
[1mStep[0m  [32/84], [94mLoss[0m : 2.35714
[1mStep[0m  [40/84], [94mLoss[0m : 2.28367
[1mStep[0m  [48/84], [94mLoss[0m : 2.32840
[1mStep[0m  [56/84], [94mLoss[0m : 2.85612
[1mStep[0m  [64/84], [94mLoss[0m : 2.50292
[1mStep[0m  [72/84], [94mLoss[0m : 2.14291
[1mStep[0m  [80/84], [94mLoss[0m : 2.38558

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.358, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.353
====================================

Phase 1 - Evaluation MAE:  2.3525668552943637
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 2.18849
[1mStep[0m  [8/84], [94mLoss[0m : 2.13355
[1mStep[0m  [16/84], [94mLoss[0m : 2.36666
[1mStep[0m  [24/84], [94mLoss[0m : 2.43021
[1mStep[0m  [32/84], [94mLoss[0m : 2.49995
[1mStep[0m  [40/84], [94mLoss[0m : 2.46413
[1mStep[0m  [48/84], [94mLoss[0m : 2.48750
[1mStep[0m  [56/84], [94mLoss[0m : 2.66380
[1mStep[0m  [64/84], [94mLoss[0m : 2.50123
[1mStep[0m  [72/84], [94mLoss[0m : 2.53006
[1mStep[0m  [80/84], [94mLoss[0m : 2.36183

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.364, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50907
[1mStep[0m  [8/84], [94mLoss[0m : 2.34270
[1mStep[0m  [16/84], [94mLoss[0m : 2.45153
[1mStep[0m  [24/84], [94mLoss[0m : 2.28109
[1mStep[0m  [32/84], [94mLoss[0m : 2.60990
[1mStep[0m  [40/84], [94mLoss[0m : 2.41033
[1mStep[0m  [48/84], [94mLoss[0m : 2.53896
[1mStep[0m  [56/84], [94mLoss[0m : 2.36203
[1mStep[0m  [64/84], [94mLoss[0m : 2.47742
[1mStep[0m  [72/84], [94mLoss[0m : 2.38041
[1mStep[0m  [80/84], [94mLoss[0m : 2.33387

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.502, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.02991
[1mStep[0m  [8/84], [94mLoss[0m : 2.84389
[1mStep[0m  [16/84], [94mLoss[0m : 2.17601
[1mStep[0m  [24/84], [94mLoss[0m : 2.42669
[1mStep[0m  [32/84], [94mLoss[0m : 2.61626
[1mStep[0m  [40/84], [94mLoss[0m : 2.39905
[1mStep[0m  [48/84], [94mLoss[0m : 2.35462
[1mStep[0m  [56/84], [94mLoss[0m : 2.54357
[1mStep[0m  [64/84], [94mLoss[0m : 2.26912
[1mStep[0m  [72/84], [94mLoss[0m : 2.50109
[1mStep[0m  [80/84], [94mLoss[0m : 2.35058

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.361, [92mTest[0m: 2.505, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45141
[1mStep[0m  [8/84], [94mLoss[0m : 2.38116
[1mStep[0m  [16/84], [94mLoss[0m : 2.35600
[1mStep[0m  [24/84], [94mLoss[0m : 2.14233
[1mStep[0m  [32/84], [94mLoss[0m : 2.39165
[1mStep[0m  [40/84], [94mLoss[0m : 2.62696
[1mStep[0m  [48/84], [94mLoss[0m : 2.07686
[1mStep[0m  [56/84], [94mLoss[0m : 2.09679
[1mStep[0m  [64/84], [94mLoss[0m : 2.60852
[1mStep[0m  [72/84], [94mLoss[0m : 2.21061
[1mStep[0m  [80/84], [94mLoss[0m : 2.20970

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.315, [92mTest[0m: 2.520, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.12080
[1mStep[0m  [8/84], [94mLoss[0m : 2.56842
[1mStep[0m  [16/84], [94mLoss[0m : 2.21765
[1mStep[0m  [24/84], [94mLoss[0m : 2.53470
[1mStep[0m  [32/84], [94mLoss[0m : 2.52956
[1mStep[0m  [40/84], [94mLoss[0m : 2.06385
[1mStep[0m  [48/84], [94mLoss[0m : 2.11008
[1mStep[0m  [56/84], [94mLoss[0m : 2.21479
[1mStep[0m  [64/84], [94mLoss[0m : 2.63250
[1mStep[0m  [72/84], [94mLoss[0m : 2.22279
[1mStep[0m  [80/84], [94mLoss[0m : 2.41569

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.252, [92mTest[0m: 2.589, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.94280
[1mStep[0m  [8/84], [94mLoss[0m : 2.13894
[1mStep[0m  [16/84], [94mLoss[0m : 2.29877
[1mStep[0m  [24/84], [94mLoss[0m : 1.91713
[1mStep[0m  [32/84], [94mLoss[0m : 2.02521
[1mStep[0m  [40/84], [94mLoss[0m : 2.25647
[1mStep[0m  [48/84], [94mLoss[0m : 2.40259
[1mStep[0m  [56/84], [94mLoss[0m : 2.09650
[1mStep[0m  [64/84], [94mLoss[0m : 2.05675
[1mStep[0m  [72/84], [94mLoss[0m : 2.32497
[1mStep[0m  [80/84], [94mLoss[0m : 1.99952

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.211, [92mTest[0m: 2.560, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.11730
[1mStep[0m  [8/84], [94mLoss[0m : 2.30949
[1mStep[0m  [16/84], [94mLoss[0m : 1.95122
[1mStep[0m  [24/84], [94mLoss[0m : 2.12955
[1mStep[0m  [32/84], [94mLoss[0m : 2.15459
[1mStep[0m  [40/84], [94mLoss[0m : 1.91469
[1mStep[0m  [48/84], [94mLoss[0m : 1.83713
[1mStep[0m  [56/84], [94mLoss[0m : 2.28146
[1mStep[0m  [64/84], [94mLoss[0m : 2.39494
[1mStep[0m  [72/84], [94mLoss[0m : 2.08582
[1mStep[0m  [80/84], [94mLoss[0m : 1.94281

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.153, [92mTest[0m: 2.486, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.00391
[1mStep[0m  [8/84], [94mLoss[0m : 1.99593
[1mStep[0m  [16/84], [94mLoss[0m : 2.06945
[1mStep[0m  [24/84], [94mLoss[0m : 2.13325
[1mStep[0m  [32/84], [94mLoss[0m : 1.96044
[1mStep[0m  [40/84], [94mLoss[0m : 2.47827
[1mStep[0m  [48/84], [94mLoss[0m : 1.97610
[1mStep[0m  [56/84], [94mLoss[0m : 1.99304
[1mStep[0m  [64/84], [94mLoss[0m : 1.96792
[1mStep[0m  [72/84], [94mLoss[0m : 2.13627
[1mStep[0m  [80/84], [94mLoss[0m : 1.97972

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.099, [92mTest[0m: 2.539, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.24340
[1mStep[0m  [8/84], [94mLoss[0m : 1.87237
[1mStep[0m  [16/84], [94mLoss[0m : 2.26054
[1mStep[0m  [24/84], [94mLoss[0m : 2.03604
[1mStep[0m  [32/84], [94mLoss[0m : 2.08176
[1mStep[0m  [40/84], [94mLoss[0m : 2.35249
[1mStep[0m  [48/84], [94mLoss[0m : 2.06826
[1mStep[0m  [56/84], [94mLoss[0m : 2.04782
[1mStep[0m  [64/84], [94mLoss[0m : 1.79966
[1mStep[0m  [72/84], [94mLoss[0m : 2.24402
[1mStep[0m  [80/84], [94mLoss[0m : 2.13735

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.059, [92mTest[0m: 2.454, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.72034
[1mStep[0m  [8/84], [94mLoss[0m : 1.74188
[1mStep[0m  [16/84], [94mLoss[0m : 1.88136
[1mStep[0m  [24/84], [94mLoss[0m : 2.04493
[1mStep[0m  [32/84], [94mLoss[0m : 2.16123
[1mStep[0m  [40/84], [94mLoss[0m : 1.81746
[1mStep[0m  [48/84], [94mLoss[0m : 1.99548
[1mStep[0m  [56/84], [94mLoss[0m : 2.14590
[1mStep[0m  [64/84], [94mLoss[0m : 1.76884
[1mStep[0m  [72/84], [94mLoss[0m : 2.04506
[1mStep[0m  [80/84], [94mLoss[0m : 1.96935

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.008, [92mTest[0m: 2.465, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.12390
[1mStep[0m  [8/84], [94mLoss[0m : 1.87187
[1mStep[0m  [16/84], [94mLoss[0m : 1.71208
[1mStep[0m  [24/84], [94mLoss[0m : 1.91847
[1mStep[0m  [32/84], [94mLoss[0m : 2.12524
[1mStep[0m  [40/84], [94mLoss[0m : 1.87458
[1mStep[0m  [48/84], [94mLoss[0m : 1.77245
[1mStep[0m  [56/84], [94mLoss[0m : 2.04603
[1mStep[0m  [64/84], [94mLoss[0m : 1.93524
[1mStep[0m  [72/84], [94mLoss[0m : 1.97558
[1mStep[0m  [80/84], [94mLoss[0m : 2.07061

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.972, [92mTest[0m: 2.440, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.77512
[1mStep[0m  [8/84], [94mLoss[0m : 1.89793
[1mStep[0m  [16/84], [94mLoss[0m : 1.77259
[1mStep[0m  [24/84], [94mLoss[0m : 1.88580
[1mStep[0m  [32/84], [94mLoss[0m : 1.67789
[1mStep[0m  [40/84], [94mLoss[0m : 2.02649
[1mStep[0m  [48/84], [94mLoss[0m : 1.80808
[1mStep[0m  [56/84], [94mLoss[0m : 1.66227
[1mStep[0m  [64/84], [94mLoss[0m : 1.89605
[1mStep[0m  [72/84], [94mLoss[0m : 1.82186
[1mStep[0m  [80/84], [94mLoss[0m : 2.03522

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.914, [92mTest[0m: 2.539, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.82547
[1mStep[0m  [8/84], [94mLoss[0m : 1.79196
[1mStep[0m  [16/84], [94mLoss[0m : 1.65175
[1mStep[0m  [24/84], [94mLoss[0m : 2.04806
[1mStep[0m  [32/84], [94mLoss[0m : 1.67991
[1mStep[0m  [40/84], [94mLoss[0m : 1.78913
[1mStep[0m  [48/84], [94mLoss[0m : 1.88696
[1mStep[0m  [56/84], [94mLoss[0m : 1.61965
[1mStep[0m  [64/84], [94mLoss[0m : 2.02814
[1mStep[0m  [72/84], [94mLoss[0m : 2.02692
[1mStep[0m  [80/84], [94mLoss[0m : 1.83490

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.872, [92mTest[0m: 2.504, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.88242
[1mStep[0m  [8/84], [94mLoss[0m : 1.75124
[1mStep[0m  [16/84], [94mLoss[0m : 1.80404
[1mStep[0m  [24/84], [94mLoss[0m : 1.84540
[1mStep[0m  [32/84], [94mLoss[0m : 1.79937
[1mStep[0m  [40/84], [94mLoss[0m : 1.76517
[1mStep[0m  [48/84], [94mLoss[0m : 1.90351
[1mStep[0m  [56/84], [94mLoss[0m : 1.96407
[1mStep[0m  [64/84], [94mLoss[0m : 1.74815
[1mStep[0m  [72/84], [94mLoss[0m : 1.82647
[1mStep[0m  [80/84], [94mLoss[0m : 1.90505

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.840, [92mTest[0m: 2.516, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.77586
[1mStep[0m  [8/84], [94mLoss[0m : 1.85210
[1mStep[0m  [16/84], [94mLoss[0m : 1.84026
[1mStep[0m  [24/84], [94mLoss[0m : 1.70800
[1mStep[0m  [32/84], [94mLoss[0m : 1.82141
[1mStep[0m  [40/84], [94mLoss[0m : 1.88841
[1mStep[0m  [48/84], [94mLoss[0m : 1.79441
[1mStep[0m  [56/84], [94mLoss[0m : 1.81388
[1mStep[0m  [64/84], [94mLoss[0m : 1.61032
[1mStep[0m  [72/84], [94mLoss[0m : 1.80232
[1mStep[0m  [80/84], [94mLoss[0m : 1.76236

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.813, [92mTest[0m: 2.447, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.93869
[1mStep[0m  [8/84], [94mLoss[0m : 1.68049
[1mStep[0m  [16/84], [94mLoss[0m : 1.60686
[1mStep[0m  [24/84], [94mLoss[0m : 1.67214
[1mStep[0m  [32/84], [94mLoss[0m : 1.72336
[1mStep[0m  [40/84], [94mLoss[0m : 1.94519
[1mStep[0m  [48/84], [94mLoss[0m : 1.96516
[1mStep[0m  [56/84], [94mLoss[0m : 1.83837
[1mStep[0m  [64/84], [94mLoss[0m : 1.94615
[1mStep[0m  [72/84], [94mLoss[0m : 1.75013
[1mStep[0m  [80/84], [94mLoss[0m : 1.66225

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.793, [92mTest[0m: 2.474, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.84047
[1mStep[0m  [8/84], [94mLoss[0m : 1.75286
[1mStep[0m  [16/84], [94mLoss[0m : 1.52059
[1mStep[0m  [24/84], [94mLoss[0m : 1.64865
[1mStep[0m  [32/84], [94mLoss[0m : 1.54611
[1mStep[0m  [40/84], [94mLoss[0m : 1.96161
[1mStep[0m  [48/84], [94mLoss[0m : 1.65585
[1mStep[0m  [56/84], [94mLoss[0m : 1.82299
[1mStep[0m  [64/84], [94mLoss[0m : 1.76327
[1mStep[0m  [72/84], [94mLoss[0m : 1.63325
[1mStep[0m  [80/84], [94mLoss[0m : 1.51417

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.728, [92mTest[0m: 2.524, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.75772
[1mStep[0m  [8/84], [94mLoss[0m : 1.56627
[1mStep[0m  [16/84], [94mLoss[0m : 1.54367
[1mStep[0m  [24/84], [94mLoss[0m : 1.66699
[1mStep[0m  [32/84], [94mLoss[0m : 1.75116
[1mStep[0m  [40/84], [94mLoss[0m : 1.61617
[1mStep[0m  [48/84], [94mLoss[0m : 1.85367
[1mStep[0m  [56/84], [94mLoss[0m : 1.65602
[1mStep[0m  [64/84], [94mLoss[0m : 1.86994
[1mStep[0m  [72/84], [94mLoss[0m : 1.61157
[1mStep[0m  [80/84], [94mLoss[0m : 1.58693

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.712, [92mTest[0m: 2.533, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.75961
[1mStep[0m  [8/84], [94mLoss[0m : 1.68193
[1mStep[0m  [16/84], [94mLoss[0m : 1.46063
[1mStep[0m  [24/84], [94mLoss[0m : 1.41632
[1mStep[0m  [32/84], [94mLoss[0m : 1.78703
[1mStep[0m  [40/84], [94mLoss[0m : 1.84399
[1mStep[0m  [48/84], [94mLoss[0m : 1.53292
[1mStep[0m  [56/84], [94mLoss[0m : 1.47507
[1mStep[0m  [64/84], [94mLoss[0m : 1.74059
[1mStep[0m  [72/84], [94mLoss[0m : 1.84438
[1mStep[0m  [80/84], [94mLoss[0m : 1.88457

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.674, [92mTest[0m: 2.483, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.57447
[1mStep[0m  [8/84], [94mLoss[0m : 1.69244
[1mStep[0m  [16/84], [94mLoss[0m : 1.91205
[1mStep[0m  [24/84], [94mLoss[0m : 1.71693
[1mStep[0m  [32/84], [94mLoss[0m : 1.78830
[1mStep[0m  [40/84], [94mLoss[0m : 1.80867
[1mStep[0m  [48/84], [94mLoss[0m : 1.43600
[1mStep[0m  [56/84], [94mLoss[0m : 1.76628
[1mStep[0m  [64/84], [94mLoss[0m : 1.92275
[1mStep[0m  [72/84], [94mLoss[0m : 1.44874
[1mStep[0m  [80/84], [94mLoss[0m : 1.83094

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.642, [92mTest[0m: 2.478, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.40511
[1mStep[0m  [8/84], [94mLoss[0m : 1.72256
[1mStep[0m  [16/84], [94mLoss[0m : 1.42809
[1mStep[0m  [24/84], [94mLoss[0m : 1.63241
[1mStep[0m  [32/84], [94mLoss[0m : 1.70687
[1mStep[0m  [40/84], [94mLoss[0m : 1.65244
[1mStep[0m  [48/84], [94mLoss[0m : 1.66429
[1mStep[0m  [56/84], [94mLoss[0m : 1.41464
[1mStep[0m  [64/84], [94mLoss[0m : 1.53592
[1mStep[0m  [72/84], [94mLoss[0m : 1.86993
[1mStep[0m  [80/84], [94mLoss[0m : 1.66442

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.600, [92mTest[0m: 2.463, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.52929
[1mStep[0m  [8/84], [94mLoss[0m : 1.59222
[1mStep[0m  [16/84], [94mLoss[0m : 1.49390
[1mStep[0m  [24/84], [94mLoss[0m : 1.82230
[1mStep[0m  [32/84], [94mLoss[0m : 1.72076
[1mStep[0m  [40/84], [94mLoss[0m : 1.68132
[1mStep[0m  [48/84], [94mLoss[0m : 1.75758
[1mStep[0m  [56/84], [94mLoss[0m : 1.82954
[1mStep[0m  [64/84], [94mLoss[0m : 1.40298
[1mStep[0m  [72/84], [94mLoss[0m : 1.57837
[1mStep[0m  [80/84], [94mLoss[0m : 1.66999

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.562, [92mTest[0m: 2.437, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.63507
[1mStep[0m  [8/84], [94mLoss[0m : 1.42479
[1mStep[0m  [16/84], [94mLoss[0m : 1.55903
[1mStep[0m  [24/84], [94mLoss[0m : 1.51176
[1mStep[0m  [32/84], [94mLoss[0m : 1.58579
[1mStep[0m  [40/84], [94mLoss[0m : 1.59330
[1mStep[0m  [48/84], [94mLoss[0m : 1.48707
[1mStep[0m  [56/84], [94mLoss[0m : 1.47732
[1mStep[0m  [64/84], [94mLoss[0m : 1.37651
[1mStep[0m  [72/84], [94mLoss[0m : 1.67322
[1mStep[0m  [80/84], [94mLoss[0m : 1.43200

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.550, [92mTest[0m: 2.473, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.59025
[1mStep[0m  [8/84], [94mLoss[0m : 1.46942
[1mStep[0m  [16/84], [94mLoss[0m : 1.34952
[1mStep[0m  [24/84], [94mLoss[0m : 1.64368
[1mStep[0m  [32/84], [94mLoss[0m : 1.73775
[1mStep[0m  [40/84], [94mLoss[0m : 1.60738
[1mStep[0m  [48/84], [94mLoss[0m : 1.87404
[1mStep[0m  [56/84], [94mLoss[0m : 1.67789
[1mStep[0m  [64/84], [94mLoss[0m : 1.53053
[1mStep[0m  [72/84], [94mLoss[0m : 1.72184
[1mStep[0m  [80/84], [94mLoss[0m : 1.36383

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.525, [92mTest[0m: 2.430, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.44454
[1mStep[0m  [8/84], [94mLoss[0m : 1.46453
[1mStep[0m  [16/84], [94mLoss[0m : 1.27313
[1mStep[0m  [24/84], [94mLoss[0m : 1.41054
[1mStep[0m  [32/84], [94mLoss[0m : 1.69267
[1mStep[0m  [40/84], [94mLoss[0m : 1.60030
[1mStep[0m  [48/84], [94mLoss[0m : 1.54785
[1mStep[0m  [56/84], [94mLoss[0m : 1.41559
[1mStep[0m  [64/84], [94mLoss[0m : 1.68743
[1mStep[0m  [72/84], [94mLoss[0m : 1.78995
[1mStep[0m  [80/84], [94mLoss[0m : 1.48335

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.513, [92mTest[0m: 2.451, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.47608
[1mStep[0m  [8/84], [94mLoss[0m : 1.68012
[1mStep[0m  [16/84], [94mLoss[0m : 1.37649
[1mStep[0m  [24/84], [94mLoss[0m : 1.51749
[1mStep[0m  [32/84], [94mLoss[0m : 1.38068
[1mStep[0m  [40/84], [94mLoss[0m : 1.45672
[1mStep[0m  [48/84], [94mLoss[0m : 1.67565
[1mStep[0m  [56/84], [94mLoss[0m : 1.71266
[1mStep[0m  [64/84], [94mLoss[0m : 1.47001
[1mStep[0m  [72/84], [94mLoss[0m : 1.60913
[1mStep[0m  [80/84], [94mLoss[0m : 1.55512

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.502, [92mTest[0m: 2.459, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.29573
[1mStep[0m  [8/84], [94mLoss[0m : 1.45088
[1mStep[0m  [16/84], [94mLoss[0m : 1.54317
[1mStep[0m  [24/84], [94mLoss[0m : 1.51434
[1mStep[0m  [32/84], [94mLoss[0m : 1.54547
[1mStep[0m  [40/84], [94mLoss[0m : 1.69816
[1mStep[0m  [48/84], [94mLoss[0m : 1.72827
[1mStep[0m  [56/84], [94mLoss[0m : 1.53576
[1mStep[0m  [64/84], [94mLoss[0m : 1.40655
[1mStep[0m  [72/84], [94mLoss[0m : 1.50045
[1mStep[0m  [80/84], [94mLoss[0m : 1.47468

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.487, [92mTest[0m: 2.419, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.22357
[1mStep[0m  [8/84], [94mLoss[0m : 1.43111
[1mStep[0m  [16/84], [94mLoss[0m : 1.43379
[1mStep[0m  [24/84], [94mLoss[0m : 1.37641
[1mStep[0m  [32/84], [94mLoss[0m : 1.35881
[1mStep[0m  [40/84], [94mLoss[0m : 1.66695
[1mStep[0m  [48/84], [94mLoss[0m : 1.57153
[1mStep[0m  [56/84], [94mLoss[0m : 1.39687
[1mStep[0m  [64/84], [94mLoss[0m : 1.53760
[1mStep[0m  [72/84], [94mLoss[0m : 1.60026
[1mStep[0m  [80/84], [94mLoss[0m : 1.40076

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.453, [92mTest[0m: 2.461, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.40450
[1mStep[0m  [8/84], [94mLoss[0m : 1.48970
[1mStep[0m  [16/84], [94mLoss[0m : 1.56434
[1mStep[0m  [24/84], [94mLoss[0m : 1.52120
[1mStep[0m  [32/84], [94mLoss[0m : 1.37444
[1mStep[0m  [40/84], [94mLoss[0m : 1.38386
[1mStep[0m  [48/84], [94mLoss[0m : 1.55961
[1mStep[0m  [56/84], [94mLoss[0m : 1.45493
[1mStep[0m  [64/84], [94mLoss[0m : 1.57108
[1mStep[0m  [72/84], [94mLoss[0m : 1.25813
[1mStep[0m  [80/84], [94mLoss[0m : 1.48391

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.430, [92mTest[0m: 2.480, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.42499
[1mStep[0m  [8/84], [94mLoss[0m : 1.49562
[1mStep[0m  [16/84], [94mLoss[0m : 1.32508
[1mStep[0m  [24/84], [94mLoss[0m : 1.59767
[1mStep[0m  [32/84], [94mLoss[0m : 1.40911
[1mStep[0m  [40/84], [94mLoss[0m : 1.37589
[1mStep[0m  [48/84], [94mLoss[0m : 1.38229
[1mStep[0m  [56/84], [94mLoss[0m : 1.43506
[1mStep[0m  [64/84], [94mLoss[0m : 1.31919
[1mStep[0m  [72/84], [94mLoss[0m : 1.61445
[1mStep[0m  [80/84], [94mLoss[0m : 1.37316

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.416, [92mTest[0m: 2.454, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.459
====================================

Phase 2 - Evaluation MAE:  2.4587036626679555
MAE score P1      2.352567
MAE score P2      2.458704
loss              1.416199
learning_rate     0.007525
batch_size             128
hidden_sizes         [250]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.1
weight_decay          0.01
Name: 24, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 10.65042
[1mStep[0m  [4/42], [94mLoss[0m : 10.00961
[1mStep[0m  [8/42], [94mLoss[0m : 8.73247
[1mStep[0m  [12/42], [94mLoss[0m : 8.00836
[1mStep[0m  [16/42], [94mLoss[0m : 7.23420
[1mStep[0m  [20/42], [94mLoss[0m : 6.49042
[1mStep[0m  [24/42], [94mLoss[0m : 5.24520
[1mStep[0m  [28/42], [94mLoss[0m : 4.56079
[1mStep[0m  [32/42], [94mLoss[0m : 3.97799
[1mStep[0m  [36/42], [94mLoss[0m : 3.49713
[1mStep[0m  [40/42], [94mLoss[0m : 3.43376

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.490, [92mTest[0m: 10.730, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.93948
[1mStep[0m  [4/42], [94mLoss[0m : 3.08980
[1mStep[0m  [8/42], [94mLoss[0m : 2.89647
[1mStep[0m  [12/42], [94mLoss[0m : 2.98556
[1mStep[0m  [16/42], [94mLoss[0m : 2.60063
[1mStep[0m  [20/42], [94mLoss[0m : 2.70595
[1mStep[0m  [24/42], [94mLoss[0m : 2.76812
[1mStep[0m  [28/42], [94mLoss[0m : 2.59027
[1mStep[0m  [32/42], [94mLoss[0m : 2.70122
[1mStep[0m  [36/42], [94mLoss[0m : 2.84505
[1mStep[0m  [40/42], [94mLoss[0m : 2.89882

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.823, [92mTest[0m: 3.974, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.87572
[1mStep[0m  [4/42], [94mLoss[0m : 2.66691
[1mStep[0m  [8/42], [94mLoss[0m : 2.83794
[1mStep[0m  [12/42], [94mLoss[0m : 2.65578
[1mStep[0m  [16/42], [94mLoss[0m : 2.69849
[1mStep[0m  [20/42], [94mLoss[0m : 2.58927
[1mStep[0m  [24/42], [94mLoss[0m : 2.39802
[1mStep[0m  [28/42], [94mLoss[0m : 2.47201
[1mStep[0m  [32/42], [94mLoss[0m : 2.96079
[1mStep[0m  [36/42], [94mLoss[0m : 2.70867
[1mStep[0m  [40/42], [94mLoss[0m : 2.77279

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.688, [92mTest[0m: 2.766, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.81880
[1mStep[0m  [4/42], [94mLoss[0m : 2.71921
[1mStep[0m  [8/42], [94mLoss[0m : 2.67444
[1mStep[0m  [12/42], [94mLoss[0m : 2.66352
[1mStep[0m  [16/42], [94mLoss[0m : 2.58417
[1mStep[0m  [20/42], [94mLoss[0m : 2.79661
[1mStep[0m  [24/42], [94mLoss[0m : 2.69715
[1mStep[0m  [28/42], [94mLoss[0m : 2.84543
[1mStep[0m  [32/42], [94mLoss[0m : 2.55398
[1mStep[0m  [36/42], [94mLoss[0m : 2.59922
[1mStep[0m  [40/42], [94mLoss[0m : 2.41305

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.692, [92mTest[0m: 2.663, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42793
[1mStep[0m  [4/42], [94mLoss[0m : 2.54958
[1mStep[0m  [8/42], [94mLoss[0m : 2.61283
[1mStep[0m  [12/42], [94mLoss[0m : 2.54695
[1mStep[0m  [16/42], [94mLoss[0m : 2.61775
[1mStep[0m  [20/42], [94mLoss[0m : 2.78707
[1mStep[0m  [24/42], [94mLoss[0m : 2.57310
[1mStep[0m  [28/42], [94mLoss[0m : 2.59357
[1mStep[0m  [32/42], [94mLoss[0m : 2.47845
[1mStep[0m  [36/42], [94mLoss[0m : 2.38830
[1mStep[0m  [40/42], [94mLoss[0m : 2.53706

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.635, [92mTest[0m: 2.497, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.71653
[1mStep[0m  [4/42], [94mLoss[0m : 2.54150
[1mStep[0m  [8/42], [94mLoss[0m : 2.68535
[1mStep[0m  [12/42], [94mLoss[0m : 2.54228
[1mStep[0m  [16/42], [94mLoss[0m : 2.71477
[1mStep[0m  [20/42], [94mLoss[0m : 2.71560
[1mStep[0m  [24/42], [94mLoss[0m : 2.79067
[1mStep[0m  [28/42], [94mLoss[0m : 2.61650
[1mStep[0m  [32/42], [94mLoss[0m : 2.64190
[1mStep[0m  [36/42], [94mLoss[0m : 2.48458
[1mStep[0m  [40/42], [94mLoss[0m : 2.82519

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.631, [92mTest[0m: 2.491, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36782
[1mStep[0m  [4/42], [94mLoss[0m : 2.87665
[1mStep[0m  [8/42], [94mLoss[0m : 2.61783
[1mStep[0m  [12/42], [94mLoss[0m : 2.73626
[1mStep[0m  [16/42], [94mLoss[0m : 2.53380
[1mStep[0m  [20/42], [94mLoss[0m : 2.66510
[1mStep[0m  [24/42], [94mLoss[0m : 2.47156
[1mStep[0m  [28/42], [94mLoss[0m : 2.50853
[1mStep[0m  [32/42], [94mLoss[0m : 2.76330
[1mStep[0m  [36/42], [94mLoss[0m : 2.86927
[1mStep[0m  [40/42], [94mLoss[0m : 2.58861

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.621, [92mTest[0m: 2.441, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57930
[1mStep[0m  [4/42], [94mLoss[0m : 2.74449
[1mStep[0m  [8/42], [94mLoss[0m : 2.63706
[1mStep[0m  [12/42], [94mLoss[0m : 2.60752
[1mStep[0m  [16/42], [94mLoss[0m : 2.83123
[1mStep[0m  [20/42], [94mLoss[0m : 2.83712
[1mStep[0m  [24/42], [94mLoss[0m : 2.59751
[1mStep[0m  [28/42], [94mLoss[0m : 2.41992
[1mStep[0m  [32/42], [94mLoss[0m : 2.59560
[1mStep[0m  [36/42], [94mLoss[0m : 2.79053
[1mStep[0m  [40/42], [94mLoss[0m : 2.64749

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.609, [92mTest[0m: 2.400, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53814
[1mStep[0m  [4/42], [94mLoss[0m : 2.63496
[1mStep[0m  [8/42], [94mLoss[0m : 2.63313
[1mStep[0m  [12/42], [94mLoss[0m : 2.51367
[1mStep[0m  [16/42], [94mLoss[0m : 2.71989
[1mStep[0m  [20/42], [94mLoss[0m : 2.47689
[1mStep[0m  [24/42], [94mLoss[0m : 2.67614
[1mStep[0m  [28/42], [94mLoss[0m : 2.47846
[1mStep[0m  [32/42], [94mLoss[0m : 2.67936
[1mStep[0m  [36/42], [94mLoss[0m : 2.58689
[1mStep[0m  [40/42], [94mLoss[0m : 2.62288

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.569, [92mTest[0m: 2.422, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.75130
[1mStep[0m  [4/42], [94mLoss[0m : 2.71447
[1mStep[0m  [8/42], [94mLoss[0m : 2.63933
[1mStep[0m  [12/42], [94mLoss[0m : 2.36454
[1mStep[0m  [16/42], [94mLoss[0m : 2.72700
[1mStep[0m  [20/42], [94mLoss[0m : 2.58594
[1mStep[0m  [24/42], [94mLoss[0m : 2.57167
[1mStep[0m  [28/42], [94mLoss[0m : 2.48473
[1mStep[0m  [32/42], [94mLoss[0m : 2.59379
[1mStep[0m  [36/42], [94mLoss[0m : 2.51038
[1mStep[0m  [40/42], [94mLoss[0m : 2.48419

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.584, [92mTest[0m: 2.412, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58653
[1mStep[0m  [4/42], [94mLoss[0m : 2.71101
[1mStep[0m  [8/42], [94mLoss[0m : 2.50691
[1mStep[0m  [12/42], [94mLoss[0m : 2.48626
[1mStep[0m  [16/42], [94mLoss[0m : 2.46216
[1mStep[0m  [20/42], [94mLoss[0m : 2.58050
[1mStep[0m  [24/42], [94mLoss[0m : 2.73729
[1mStep[0m  [28/42], [94mLoss[0m : 2.71094
[1mStep[0m  [32/42], [94mLoss[0m : 2.45338
[1mStep[0m  [36/42], [94mLoss[0m : 2.26519
[1mStep[0m  [40/42], [94mLoss[0m : 2.70951

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.595, [92mTest[0m: 2.375, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50445
[1mStep[0m  [4/42], [94mLoss[0m : 2.52794
[1mStep[0m  [8/42], [94mLoss[0m : 2.60282
[1mStep[0m  [12/42], [94mLoss[0m : 2.45308
[1mStep[0m  [16/42], [94mLoss[0m : 2.51367
[1mStep[0m  [20/42], [94mLoss[0m : 2.64312
[1mStep[0m  [24/42], [94mLoss[0m : 2.62404
[1mStep[0m  [28/42], [94mLoss[0m : 2.68184
[1mStep[0m  [32/42], [94mLoss[0m : 2.83115
[1mStep[0m  [36/42], [94mLoss[0m : 2.54103
[1mStep[0m  [40/42], [94mLoss[0m : 2.54252

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.574, [92mTest[0m: 2.379, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.69714
[1mStep[0m  [4/42], [94mLoss[0m : 2.36172
[1mStep[0m  [8/42], [94mLoss[0m : 2.38424
[1mStep[0m  [12/42], [94mLoss[0m : 2.57245
[1mStep[0m  [16/42], [94mLoss[0m : 2.66128
[1mStep[0m  [20/42], [94mLoss[0m : 2.51119
[1mStep[0m  [24/42], [94mLoss[0m : 2.54164
[1mStep[0m  [28/42], [94mLoss[0m : 2.52425
[1mStep[0m  [32/42], [94mLoss[0m : 2.58357
[1mStep[0m  [36/42], [94mLoss[0m : 2.54138
[1mStep[0m  [40/42], [94mLoss[0m : 2.50545

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.547, [92mTest[0m: 2.378, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.65940
[1mStep[0m  [4/42], [94mLoss[0m : 2.53144
[1mStep[0m  [8/42], [94mLoss[0m : 2.44532
[1mStep[0m  [12/42], [94mLoss[0m : 2.65435
[1mStep[0m  [16/42], [94mLoss[0m : 2.61930
[1mStep[0m  [20/42], [94mLoss[0m : 2.44774
[1mStep[0m  [24/42], [94mLoss[0m : 2.72252
[1mStep[0m  [28/42], [94mLoss[0m : 2.35021
[1mStep[0m  [32/42], [94mLoss[0m : 2.48689
[1mStep[0m  [36/42], [94mLoss[0m : 2.57151
[1mStep[0m  [40/42], [94mLoss[0m : 2.71989

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.552, [92mTest[0m: 2.348, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60814
[1mStep[0m  [4/42], [94mLoss[0m : 2.75739
[1mStep[0m  [8/42], [94mLoss[0m : 2.51200
[1mStep[0m  [12/42], [94mLoss[0m : 2.38924
[1mStep[0m  [16/42], [94mLoss[0m : 2.46123
[1mStep[0m  [20/42], [94mLoss[0m : 2.52120
[1mStep[0m  [24/42], [94mLoss[0m : 2.56941
[1mStep[0m  [28/42], [94mLoss[0m : 2.56108
[1mStep[0m  [32/42], [94mLoss[0m : 2.54391
[1mStep[0m  [36/42], [94mLoss[0m : 2.50177
[1mStep[0m  [40/42], [94mLoss[0m : 2.57820

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.575, [92mTest[0m: 2.391, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37930
[1mStep[0m  [4/42], [94mLoss[0m : 2.44390
[1mStep[0m  [8/42], [94mLoss[0m : 2.47704
[1mStep[0m  [12/42], [94mLoss[0m : 2.69838
[1mStep[0m  [16/42], [94mLoss[0m : 2.51108
[1mStep[0m  [20/42], [94mLoss[0m : 2.78768
[1mStep[0m  [24/42], [94mLoss[0m : 2.80057
[1mStep[0m  [28/42], [94mLoss[0m : 2.55260
[1mStep[0m  [32/42], [94mLoss[0m : 2.34791
[1mStep[0m  [36/42], [94mLoss[0m : 2.59083
[1mStep[0m  [40/42], [94mLoss[0m : 2.48138

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.538, [92mTest[0m: 2.315, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48081
[1mStep[0m  [4/42], [94mLoss[0m : 2.44886
[1mStep[0m  [8/42], [94mLoss[0m : 2.48590
[1mStep[0m  [12/42], [94mLoss[0m : 2.54884
[1mStep[0m  [16/42], [94mLoss[0m : 2.57237
[1mStep[0m  [20/42], [94mLoss[0m : 2.49969
[1mStep[0m  [24/42], [94mLoss[0m : 2.60899
[1mStep[0m  [28/42], [94mLoss[0m : 2.45177
[1mStep[0m  [32/42], [94mLoss[0m : 2.39336
[1mStep[0m  [36/42], [94mLoss[0m : 2.69919
[1mStep[0m  [40/42], [94mLoss[0m : 2.52125

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.537, [92mTest[0m: 2.349, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27448
[1mStep[0m  [4/42], [94mLoss[0m : 2.49825
[1mStep[0m  [8/42], [94mLoss[0m : 2.59258
[1mStep[0m  [12/42], [94mLoss[0m : 2.69534
[1mStep[0m  [16/42], [94mLoss[0m : 2.44410
[1mStep[0m  [20/42], [94mLoss[0m : 2.49110
[1mStep[0m  [24/42], [94mLoss[0m : 2.49069
[1mStep[0m  [28/42], [94mLoss[0m : 2.32921
[1mStep[0m  [32/42], [94mLoss[0m : 2.34058
[1mStep[0m  [36/42], [94mLoss[0m : 2.52069
[1mStep[0m  [40/42], [94mLoss[0m : 2.57871

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.378, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50198
[1mStep[0m  [4/42], [94mLoss[0m : 2.24651
[1mStep[0m  [8/42], [94mLoss[0m : 2.37944
[1mStep[0m  [12/42], [94mLoss[0m : 2.52613
[1mStep[0m  [16/42], [94mLoss[0m : 2.52856
[1mStep[0m  [20/42], [94mLoss[0m : 2.57220
[1mStep[0m  [24/42], [94mLoss[0m : 2.43423
[1mStep[0m  [28/42], [94mLoss[0m : 2.34470
[1mStep[0m  [32/42], [94mLoss[0m : 2.54176
[1mStep[0m  [36/42], [94mLoss[0m : 2.37952
[1mStep[0m  [40/42], [94mLoss[0m : 2.69390

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.364, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60082
[1mStep[0m  [4/42], [94mLoss[0m : 2.38502
[1mStep[0m  [8/42], [94mLoss[0m : 2.19001
[1mStep[0m  [12/42], [94mLoss[0m : 2.64881
[1mStep[0m  [16/42], [94mLoss[0m : 2.42403
[1mStep[0m  [20/42], [94mLoss[0m : 2.41003
[1mStep[0m  [24/42], [94mLoss[0m : 2.66655
[1mStep[0m  [28/42], [94mLoss[0m : 2.51514
[1mStep[0m  [32/42], [94mLoss[0m : 2.54808
[1mStep[0m  [36/42], [94mLoss[0m : 2.28746
[1mStep[0m  [40/42], [94mLoss[0m : 2.33889

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.357, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41156
[1mStep[0m  [4/42], [94mLoss[0m : 2.46595
[1mStep[0m  [8/42], [94mLoss[0m : 2.44300
[1mStep[0m  [12/42], [94mLoss[0m : 2.43927
[1mStep[0m  [16/42], [94mLoss[0m : 2.53767
[1mStep[0m  [20/42], [94mLoss[0m : 2.36539
[1mStep[0m  [24/42], [94mLoss[0m : 2.60094
[1mStep[0m  [28/42], [94mLoss[0m : 2.59133
[1mStep[0m  [32/42], [94mLoss[0m : 2.39263
[1mStep[0m  [36/42], [94mLoss[0m : 2.53710
[1mStep[0m  [40/42], [94mLoss[0m : 2.53931

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.323, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59990
[1mStep[0m  [4/42], [94mLoss[0m : 2.46478
[1mStep[0m  [8/42], [94mLoss[0m : 2.41722
[1mStep[0m  [12/42], [94mLoss[0m : 2.44605
[1mStep[0m  [16/42], [94mLoss[0m : 2.40249
[1mStep[0m  [20/42], [94mLoss[0m : 2.57720
[1mStep[0m  [24/42], [94mLoss[0m : 2.26613
[1mStep[0m  [28/42], [94mLoss[0m : 2.37995
[1mStep[0m  [32/42], [94mLoss[0m : 2.67108
[1mStep[0m  [36/42], [94mLoss[0m : 2.35691
[1mStep[0m  [40/42], [94mLoss[0m : 2.46525

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.348, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32978
[1mStep[0m  [4/42], [94mLoss[0m : 2.51657
[1mStep[0m  [8/42], [94mLoss[0m : 2.14105
[1mStep[0m  [12/42], [94mLoss[0m : 2.47092
[1mStep[0m  [16/42], [94mLoss[0m : 2.37901
[1mStep[0m  [20/42], [94mLoss[0m : 2.59858
[1mStep[0m  [24/42], [94mLoss[0m : 2.54618
[1mStep[0m  [28/42], [94mLoss[0m : 2.62749
[1mStep[0m  [32/42], [94mLoss[0m : 2.53566
[1mStep[0m  [36/42], [94mLoss[0m : 2.53437
[1mStep[0m  [40/42], [94mLoss[0m : 2.35409

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.342, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43271
[1mStep[0m  [4/42], [94mLoss[0m : 2.45952
[1mStep[0m  [8/42], [94mLoss[0m : 2.35700
[1mStep[0m  [12/42], [94mLoss[0m : 2.42772
[1mStep[0m  [16/42], [94mLoss[0m : 2.23828
[1mStep[0m  [20/42], [94mLoss[0m : 2.57230
[1mStep[0m  [24/42], [94mLoss[0m : 2.42503
[1mStep[0m  [28/42], [94mLoss[0m : 2.41260
[1mStep[0m  [32/42], [94mLoss[0m : 2.54104
[1mStep[0m  [36/42], [94mLoss[0m : 2.49856
[1mStep[0m  [40/42], [94mLoss[0m : 2.32606

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.338, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52006
[1mStep[0m  [4/42], [94mLoss[0m : 2.52971
[1mStep[0m  [8/42], [94mLoss[0m : 2.48273
[1mStep[0m  [12/42], [94mLoss[0m : 2.55680
[1mStep[0m  [16/42], [94mLoss[0m : 2.25673
[1mStep[0m  [20/42], [94mLoss[0m : 2.57552
[1mStep[0m  [24/42], [94mLoss[0m : 2.69813
[1mStep[0m  [28/42], [94mLoss[0m : 2.40054
[1mStep[0m  [32/42], [94mLoss[0m : 2.47195
[1mStep[0m  [36/42], [94mLoss[0m : 2.43495
[1mStep[0m  [40/42], [94mLoss[0m : 2.88739

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.347, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43835
[1mStep[0m  [4/42], [94mLoss[0m : 2.36895
[1mStep[0m  [8/42], [94mLoss[0m : 2.65762
[1mStep[0m  [12/42], [94mLoss[0m : 2.40649
[1mStep[0m  [16/42], [94mLoss[0m : 2.49711
[1mStep[0m  [20/42], [94mLoss[0m : 2.51121
[1mStep[0m  [24/42], [94mLoss[0m : 2.58299
[1mStep[0m  [28/42], [94mLoss[0m : 2.47123
[1mStep[0m  [32/42], [94mLoss[0m : 2.38130
[1mStep[0m  [36/42], [94mLoss[0m : 2.39334
[1mStep[0m  [40/42], [94mLoss[0m : 2.39557

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.330, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52485
[1mStep[0m  [4/42], [94mLoss[0m : 2.45187
[1mStep[0m  [8/42], [94mLoss[0m : 2.35678
[1mStep[0m  [12/42], [94mLoss[0m : 2.38666
[1mStep[0m  [16/42], [94mLoss[0m : 2.61139
[1mStep[0m  [20/42], [94mLoss[0m : 2.56248
[1mStep[0m  [24/42], [94mLoss[0m : 2.68366
[1mStep[0m  [28/42], [94mLoss[0m : 2.47518
[1mStep[0m  [32/42], [94mLoss[0m : 2.49350
[1mStep[0m  [36/42], [94mLoss[0m : 2.45000
[1mStep[0m  [40/42], [94mLoss[0m : 2.47310

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.323, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54251
[1mStep[0m  [4/42], [94mLoss[0m : 2.28039
[1mStep[0m  [8/42], [94mLoss[0m : 2.52562
[1mStep[0m  [12/42], [94mLoss[0m : 2.29508
[1mStep[0m  [16/42], [94mLoss[0m : 2.40674
[1mStep[0m  [20/42], [94mLoss[0m : 2.33485
[1mStep[0m  [24/42], [94mLoss[0m : 2.60171
[1mStep[0m  [28/42], [94mLoss[0m : 2.40555
[1mStep[0m  [32/42], [94mLoss[0m : 2.58673
[1mStep[0m  [36/42], [94mLoss[0m : 2.47360
[1mStep[0m  [40/42], [94mLoss[0m : 2.72949

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.314, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38794
[1mStep[0m  [4/42], [94mLoss[0m : 2.46399
[1mStep[0m  [8/42], [94mLoss[0m : 2.43988
[1mStep[0m  [12/42], [94mLoss[0m : 2.47903
[1mStep[0m  [16/42], [94mLoss[0m : 2.39720
[1mStep[0m  [20/42], [94mLoss[0m : 2.44653
[1mStep[0m  [24/42], [94mLoss[0m : 2.53042
[1mStep[0m  [28/42], [94mLoss[0m : 2.55782
[1mStep[0m  [32/42], [94mLoss[0m : 2.59137
[1mStep[0m  [36/42], [94mLoss[0m : 2.46650
[1mStep[0m  [40/42], [94mLoss[0m : 2.36649

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.336, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60942
[1mStep[0m  [4/42], [94mLoss[0m : 2.27201
[1mStep[0m  [8/42], [94mLoss[0m : 2.51132
[1mStep[0m  [12/42], [94mLoss[0m : 2.54791
[1mStep[0m  [16/42], [94mLoss[0m : 2.39718
[1mStep[0m  [20/42], [94mLoss[0m : 2.38641
[1mStep[0m  [24/42], [94mLoss[0m : 2.37323
[1mStep[0m  [28/42], [94mLoss[0m : 2.52341
[1mStep[0m  [32/42], [94mLoss[0m : 2.72796
[1mStep[0m  [36/42], [94mLoss[0m : 2.42206
[1mStep[0m  [40/42], [94mLoss[0m : 2.44084

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.310, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.302
====================================

Phase 1 - Evaluation MAE:  2.3022824355534146
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 2.34575
[1mStep[0m  [4/42], [94mLoss[0m : 2.42157
[1mStep[0m  [8/42], [94mLoss[0m : 2.49246
[1mStep[0m  [12/42], [94mLoss[0m : 2.60627
[1mStep[0m  [16/42], [94mLoss[0m : 2.67211
[1mStep[0m  [20/42], [94mLoss[0m : 2.62148
[1mStep[0m  [24/42], [94mLoss[0m : 2.73757
[1mStep[0m  [28/42], [94mLoss[0m : 2.53328
[1mStep[0m  [32/42], [94mLoss[0m : 2.66174
[1mStep[0m  [36/42], [94mLoss[0m : 2.49747
[1mStep[0m  [40/42], [94mLoss[0m : 2.60295

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.576, [92mTest[0m: 2.299, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.72002
[1mStep[0m  [4/42], [94mLoss[0m : 2.62842
[1mStep[0m  [8/42], [94mLoss[0m : 2.71880
[1mStep[0m  [12/42], [94mLoss[0m : 2.66292
[1mStep[0m  [16/42], [94mLoss[0m : 2.45762
[1mStep[0m  [20/42], [94mLoss[0m : 2.53479
[1mStep[0m  [24/42], [94mLoss[0m : 2.63189
[1mStep[0m  [28/42], [94mLoss[0m : 2.28413
[1mStep[0m  [32/42], [94mLoss[0m : 2.41030
[1mStep[0m  [36/42], [94mLoss[0m : 2.56764
[1mStep[0m  [40/42], [94mLoss[0m : 2.42079

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.519, [92mTest[0m: 2.320, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41975
[1mStep[0m  [4/42], [94mLoss[0m : 2.39535
[1mStep[0m  [8/42], [94mLoss[0m : 2.67842
[1mStep[0m  [12/42], [94mLoss[0m : 2.24312
[1mStep[0m  [16/42], [94mLoss[0m : 2.23815
[1mStep[0m  [20/42], [94mLoss[0m : 2.35135
[1mStep[0m  [24/42], [94mLoss[0m : 2.44109
[1mStep[0m  [28/42], [94mLoss[0m : 2.18431
[1mStep[0m  [32/42], [94mLoss[0m : 2.44715
[1mStep[0m  [36/42], [94mLoss[0m : 2.44649
[1mStep[0m  [40/42], [94mLoss[0m : 2.48731

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.352, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32896
[1mStep[0m  [4/42], [94mLoss[0m : 2.39827
[1mStep[0m  [8/42], [94mLoss[0m : 2.31319
[1mStep[0m  [12/42], [94mLoss[0m : 2.44361
[1mStep[0m  [16/42], [94mLoss[0m : 2.48070
[1mStep[0m  [20/42], [94mLoss[0m : 2.31625
[1mStep[0m  [24/42], [94mLoss[0m : 2.07694
[1mStep[0m  [28/42], [94mLoss[0m : 2.52194
[1mStep[0m  [32/42], [94mLoss[0m : 2.41972
[1mStep[0m  [36/42], [94mLoss[0m : 2.68279
[1mStep[0m  [40/42], [94mLoss[0m : 2.48041

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.352, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44737
[1mStep[0m  [4/42], [94mLoss[0m : 2.22232
[1mStep[0m  [8/42], [94mLoss[0m : 2.23542
[1mStep[0m  [12/42], [94mLoss[0m : 2.40598
[1mStep[0m  [16/42], [94mLoss[0m : 2.24637
[1mStep[0m  [20/42], [94mLoss[0m : 2.21804
[1mStep[0m  [24/42], [94mLoss[0m : 2.14407
[1mStep[0m  [28/42], [94mLoss[0m : 2.18027
[1mStep[0m  [32/42], [94mLoss[0m : 2.21928
[1mStep[0m  [36/42], [94mLoss[0m : 2.29469
[1mStep[0m  [40/42], [94mLoss[0m : 2.26360

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.337, [92mTest[0m: 2.353, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62903
[1mStep[0m  [4/42], [94mLoss[0m : 2.38202
[1mStep[0m  [8/42], [94mLoss[0m : 2.31158
[1mStep[0m  [12/42], [94mLoss[0m : 2.30707
[1mStep[0m  [16/42], [94mLoss[0m : 2.16432
[1mStep[0m  [20/42], [94mLoss[0m : 2.38395
[1mStep[0m  [24/42], [94mLoss[0m : 2.36548
[1mStep[0m  [28/42], [94mLoss[0m : 2.12737
[1mStep[0m  [32/42], [94mLoss[0m : 2.18936
[1mStep[0m  [36/42], [94mLoss[0m : 2.22260
[1mStep[0m  [40/42], [94mLoss[0m : 2.19058

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.309, [92mTest[0m: 2.409, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43485
[1mStep[0m  [4/42], [94mLoss[0m : 2.22140
[1mStep[0m  [8/42], [94mLoss[0m : 2.35071
[1mStep[0m  [12/42], [94mLoss[0m : 2.25243
[1mStep[0m  [16/42], [94mLoss[0m : 2.46779
[1mStep[0m  [20/42], [94mLoss[0m : 2.06128
[1mStep[0m  [24/42], [94mLoss[0m : 2.06798
[1mStep[0m  [28/42], [94mLoss[0m : 2.31234
[1mStep[0m  [32/42], [94mLoss[0m : 2.02343
[1mStep[0m  [36/42], [94mLoss[0m : 2.07495
[1mStep[0m  [40/42], [94mLoss[0m : 2.61988

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.249, [92mTest[0m: 2.363, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.23571
[1mStep[0m  [4/42], [94mLoss[0m : 2.06901
[1mStep[0m  [8/42], [94mLoss[0m : 2.05771
[1mStep[0m  [12/42], [94mLoss[0m : 2.24274
[1mStep[0m  [16/42], [94mLoss[0m : 2.11896
[1mStep[0m  [20/42], [94mLoss[0m : 2.22047
[1mStep[0m  [24/42], [94mLoss[0m : 2.11982
[1mStep[0m  [28/42], [94mLoss[0m : 2.35535
[1mStep[0m  [32/42], [94mLoss[0m : 2.29477
[1mStep[0m  [36/42], [94mLoss[0m : 2.16397
[1mStep[0m  [40/42], [94mLoss[0m : 2.10374

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.195, [92mTest[0m: 2.354, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.14413
[1mStep[0m  [4/42], [94mLoss[0m : 2.23144
[1mStep[0m  [8/42], [94mLoss[0m : 2.05612
[1mStep[0m  [12/42], [94mLoss[0m : 2.00817
[1mStep[0m  [16/42], [94mLoss[0m : 2.17401
[1mStep[0m  [20/42], [94mLoss[0m : 2.09983
[1mStep[0m  [24/42], [94mLoss[0m : 2.35204
[1mStep[0m  [28/42], [94mLoss[0m : 2.08824
[1mStep[0m  [32/42], [94mLoss[0m : 2.18179
[1mStep[0m  [36/42], [94mLoss[0m : 2.31865
[1mStep[0m  [40/42], [94mLoss[0m : 2.28581

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.149, [92mTest[0m: 2.387, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.05220
[1mStep[0m  [4/42], [94mLoss[0m : 1.97932
[1mStep[0m  [8/42], [94mLoss[0m : 1.98524
[1mStep[0m  [12/42], [94mLoss[0m : 2.07071
[1mStep[0m  [16/42], [94mLoss[0m : 2.19836
[1mStep[0m  [20/42], [94mLoss[0m : 2.04751
[1mStep[0m  [24/42], [94mLoss[0m : 2.04130
[1mStep[0m  [28/42], [94mLoss[0m : 2.17925
[1mStep[0m  [32/42], [94mLoss[0m : 2.07477
[1mStep[0m  [36/42], [94mLoss[0m : 2.10885
[1mStep[0m  [40/42], [94mLoss[0m : 1.94417

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.114, [92mTest[0m: 2.376, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27182
[1mStep[0m  [4/42], [94mLoss[0m : 2.25516
[1mStep[0m  [8/42], [94mLoss[0m : 2.16828
[1mStep[0m  [12/42], [94mLoss[0m : 2.08216
[1mStep[0m  [16/42], [94mLoss[0m : 1.97974
[1mStep[0m  [20/42], [94mLoss[0m : 2.03999
[1mStep[0m  [24/42], [94mLoss[0m : 2.13064
[1mStep[0m  [28/42], [94mLoss[0m : 1.98726
[1mStep[0m  [32/42], [94mLoss[0m : 2.00536
[1mStep[0m  [36/42], [94mLoss[0m : 2.09631
[1mStep[0m  [40/42], [94mLoss[0m : 2.00435

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.062, [92mTest[0m: 2.413, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.70933
[1mStep[0m  [4/42], [94mLoss[0m : 2.07282
[1mStep[0m  [8/42], [94mLoss[0m : 2.17421
[1mStep[0m  [12/42], [94mLoss[0m : 2.00735
[1mStep[0m  [16/42], [94mLoss[0m : 1.86337
[1mStep[0m  [20/42], [94mLoss[0m : 2.01078
[1mStep[0m  [24/42], [94mLoss[0m : 1.99141
[1mStep[0m  [28/42], [94mLoss[0m : 1.99881
[1mStep[0m  [32/42], [94mLoss[0m : 2.01572
[1mStep[0m  [36/42], [94mLoss[0m : 1.96678
[1mStep[0m  [40/42], [94mLoss[0m : 2.28583

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.022, [92mTest[0m: 2.427, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.74878
[1mStep[0m  [4/42], [94mLoss[0m : 1.91672
[1mStep[0m  [8/42], [94mLoss[0m : 2.03703
[1mStep[0m  [12/42], [94mLoss[0m : 2.01772
[1mStep[0m  [16/42], [94mLoss[0m : 1.91027
[1mStep[0m  [20/42], [94mLoss[0m : 1.94733
[1mStep[0m  [24/42], [94mLoss[0m : 1.94997
[1mStep[0m  [28/42], [94mLoss[0m : 2.11247
[1mStep[0m  [32/42], [94mLoss[0m : 1.99402
[1mStep[0m  [36/42], [94mLoss[0m : 2.13353
[1mStep[0m  [40/42], [94mLoss[0m : 2.16565

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.983, [92mTest[0m: 2.443, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.96605
[1mStep[0m  [4/42], [94mLoss[0m : 1.93115
[1mStep[0m  [8/42], [94mLoss[0m : 1.95995
[1mStep[0m  [12/42], [94mLoss[0m : 1.73766
[1mStep[0m  [16/42], [94mLoss[0m : 2.03898
[1mStep[0m  [20/42], [94mLoss[0m : 2.16701
[1mStep[0m  [24/42], [94mLoss[0m : 1.98044
[1mStep[0m  [28/42], [94mLoss[0m : 1.70472
[1mStep[0m  [32/42], [94mLoss[0m : 1.71980
[1mStep[0m  [36/42], [94mLoss[0m : 2.04900
[1mStep[0m  [40/42], [94mLoss[0m : 1.92573

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.935, [92mTest[0m: 2.457, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.90990
[1mStep[0m  [4/42], [94mLoss[0m : 1.94034
[1mStep[0m  [8/42], [94mLoss[0m : 2.16312
[1mStep[0m  [12/42], [94mLoss[0m : 1.88314
[1mStep[0m  [16/42], [94mLoss[0m : 2.01343
[1mStep[0m  [20/42], [94mLoss[0m : 1.98648
[1mStep[0m  [24/42], [94mLoss[0m : 2.01304
[1mStep[0m  [28/42], [94mLoss[0m : 1.91401
[1mStep[0m  [32/42], [94mLoss[0m : 1.81641
[1mStep[0m  [36/42], [94mLoss[0m : 1.86154
[1mStep[0m  [40/42], [94mLoss[0m : 1.97938

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.914, [92mTest[0m: 2.479, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.74524
[1mStep[0m  [4/42], [94mLoss[0m : 1.72915
[1mStep[0m  [8/42], [94mLoss[0m : 2.04032
[1mStep[0m  [12/42], [94mLoss[0m : 1.80020
[1mStep[0m  [16/42], [94mLoss[0m : 1.75564
[1mStep[0m  [20/42], [94mLoss[0m : 1.84002
[1mStep[0m  [24/42], [94mLoss[0m : 1.94981
[1mStep[0m  [28/42], [94mLoss[0m : 1.71491
[1mStep[0m  [32/42], [94mLoss[0m : 1.91962
[1mStep[0m  [36/42], [94mLoss[0m : 1.78470
[1mStep[0m  [40/42], [94mLoss[0m : 1.81407

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.865, [92mTest[0m: 2.483, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.86587
[1mStep[0m  [4/42], [94mLoss[0m : 1.92205
[1mStep[0m  [8/42], [94mLoss[0m : 1.89786
[1mStep[0m  [12/42], [94mLoss[0m : 1.63013
[1mStep[0m  [16/42], [94mLoss[0m : 1.68951
[1mStep[0m  [20/42], [94mLoss[0m : 1.95303
[1mStep[0m  [24/42], [94mLoss[0m : 2.00097
[1mStep[0m  [28/42], [94mLoss[0m : 1.89688
[1mStep[0m  [32/42], [94mLoss[0m : 2.00169
[1mStep[0m  [36/42], [94mLoss[0m : 1.95220
[1mStep[0m  [40/42], [94mLoss[0m : 1.58357

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.847, [92mTest[0m: 2.449, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.65645
[1mStep[0m  [4/42], [94mLoss[0m : 1.77909
[1mStep[0m  [8/42], [94mLoss[0m : 1.80979
[1mStep[0m  [12/42], [94mLoss[0m : 1.90040
[1mStep[0m  [16/42], [94mLoss[0m : 1.63650
[1mStep[0m  [20/42], [94mLoss[0m : 1.75392
[1mStep[0m  [24/42], [94mLoss[0m : 1.82807
[1mStep[0m  [28/42], [94mLoss[0m : 1.85553
[1mStep[0m  [32/42], [94mLoss[0m : 1.95967
[1mStep[0m  [36/42], [94mLoss[0m : 1.86294
[1mStep[0m  [40/42], [94mLoss[0m : 1.83493

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.830, [92mTest[0m: 2.473, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.77487
[1mStep[0m  [4/42], [94mLoss[0m : 1.52098
[1mStep[0m  [8/42], [94mLoss[0m : 1.50673
[1mStep[0m  [12/42], [94mLoss[0m : 1.88298
[1mStep[0m  [16/42], [94mLoss[0m : 1.92177
[1mStep[0m  [20/42], [94mLoss[0m : 1.85042
[1mStep[0m  [24/42], [94mLoss[0m : 1.69472
[1mStep[0m  [28/42], [94mLoss[0m : 1.78416
[1mStep[0m  [32/42], [94mLoss[0m : 1.78070
[1mStep[0m  [36/42], [94mLoss[0m : 1.71294
[1mStep[0m  [40/42], [94mLoss[0m : 1.68596

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.784, [92mTest[0m: 2.495, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.66862
[1mStep[0m  [4/42], [94mLoss[0m : 1.81810
[1mStep[0m  [8/42], [94mLoss[0m : 1.95025
[1mStep[0m  [12/42], [94mLoss[0m : 1.59955
[1mStep[0m  [16/42], [94mLoss[0m : 1.82369
[1mStep[0m  [20/42], [94mLoss[0m : 1.68433
[1mStep[0m  [24/42], [94mLoss[0m : 1.85219
[1mStep[0m  [28/42], [94mLoss[0m : 1.70775
[1mStep[0m  [32/42], [94mLoss[0m : 2.12956
[1mStep[0m  [36/42], [94mLoss[0m : 1.92769
[1mStep[0m  [40/42], [94mLoss[0m : 1.69767

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.764, [92mTest[0m: 2.470, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.73557
[1mStep[0m  [4/42], [94mLoss[0m : 1.65488
[1mStep[0m  [8/42], [94mLoss[0m : 1.79409
[1mStep[0m  [12/42], [94mLoss[0m : 1.64159
[1mStep[0m  [16/42], [94mLoss[0m : 1.70174
[1mStep[0m  [20/42], [94mLoss[0m : 1.75319
[1mStep[0m  [24/42], [94mLoss[0m : 1.59592
[1mStep[0m  [28/42], [94mLoss[0m : 1.73731
[1mStep[0m  [32/42], [94mLoss[0m : 1.79431
[1mStep[0m  [36/42], [94mLoss[0m : 1.81415
[1mStep[0m  [40/42], [94mLoss[0m : 1.77899

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.727, [92mTest[0m: 2.480, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.70300
[1mStep[0m  [4/42], [94mLoss[0m : 1.66135
[1mStep[0m  [8/42], [94mLoss[0m : 1.58773
[1mStep[0m  [12/42], [94mLoss[0m : 1.61133
[1mStep[0m  [16/42], [94mLoss[0m : 1.70904
[1mStep[0m  [20/42], [94mLoss[0m : 1.76445
[1mStep[0m  [24/42], [94mLoss[0m : 1.70014
[1mStep[0m  [28/42], [94mLoss[0m : 1.69535
[1mStep[0m  [32/42], [94mLoss[0m : 1.55561
[1mStep[0m  [36/42], [94mLoss[0m : 1.63847
[1mStep[0m  [40/42], [94mLoss[0m : 1.63910

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.673, [92mTest[0m: 2.471, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.77146
[1mStep[0m  [4/42], [94mLoss[0m : 1.61330
[1mStep[0m  [8/42], [94mLoss[0m : 1.90502
[1mStep[0m  [12/42], [94mLoss[0m : 1.73207
[1mStep[0m  [16/42], [94mLoss[0m : 1.61195
[1mStep[0m  [20/42], [94mLoss[0m : 1.84516
[1mStep[0m  [24/42], [94mLoss[0m : 1.73766
[1mStep[0m  [28/42], [94mLoss[0m : 1.59157
[1mStep[0m  [32/42], [94mLoss[0m : 1.78940
[1mStep[0m  [36/42], [94mLoss[0m : 1.68877
[1mStep[0m  [40/42], [94mLoss[0m : 1.68140

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.674, [92mTest[0m: 2.463, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.69591
[1mStep[0m  [4/42], [94mLoss[0m : 1.66149
[1mStep[0m  [8/42], [94mLoss[0m : 1.67897
[1mStep[0m  [12/42], [94mLoss[0m : 1.68617
[1mStep[0m  [16/42], [94mLoss[0m : 1.69608
[1mStep[0m  [20/42], [94mLoss[0m : 1.50483
[1mStep[0m  [24/42], [94mLoss[0m : 1.68088
[1mStep[0m  [28/42], [94mLoss[0m : 1.61789
[1mStep[0m  [32/42], [94mLoss[0m : 1.66097
[1mStep[0m  [36/42], [94mLoss[0m : 1.68991
[1mStep[0m  [40/42], [94mLoss[0m : 1.51552

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.648, [92mTest[0m: 2.502, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.52515
[1mStep[0m  [4/42], [94mLoss[0m : 1.48640
[1mStep[0m  [8/42], [94mLoss[0m : 1.58842
[1mStep[0m  [12/42], [94mLoss[0m : 1.67621
[1mStep[0m  [16/42], [94mLoss[0m : 1.51689
[1mStep[0m  [20/42], [94mLoss[0m : 1.73147
[1mStep[0m  [24/42], [94mLoss[0m : 1.73451
[1mStep[0m  [28/42], [94mLoss[0m : 1.68046
[1mStep[0m  [32/42], [94mLoss[0m : 1.69426
[1mStep[0m  [36/42], [94mLoss[0m : 1.71201
[1mStep[0m  [40/42], [94mLoss[0m : 1.59739

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.636, [92mTest[0m: 2.554, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.53834
[1mStep[0m  [4/42], [94mLoss[0m : 1.43706
[1mStep[0m  [8/42], [94mLoss[0m : 1.58114
[1mStep[0m  [12/42], [94mLoss[0m : 1.62022
[1mStep[0m  [16/42], [94mLoss[0m : 1.60870
[1mStep[0m  [20/42], [94mLoss[0m : 1.63540
[1mStep[0m  [24/42], [94mLoss[0m : 1.54852
[1mStep[0m  [28/42], [94mLoss[0m : 1.58729
[1mStep[0m  [32/42], [94mLoss[0m : 1.49587
[1mStep[0m  [36/42], [94mLoss[0m : 1.62929
[1mStep[0m  [40/42], [94mLoss[0m : 1.51089

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.610, [92mTest[0m: 2.450, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.56021
[1mStep[0m  [4/42], [94mLoss[0m : 1.46594
[1mStep[0m  [8/42], [94mLoss[0m : 1.55150
[1mStep[0m  [12/42], [94mLoss[0m : 1.53010
[1mStep[0m  [16/42], [94mLoss[0m : 1.59731
[1mStep[0m  [20/42], [94mLoss[0m : 1.56381
[1mStep[0m  [24/42], [94mLoss[0m : 1.51933
[1mStep[0m  [28/42], [94mLoss[0m : 1.61865
[1mStep[0m  [32/42], [94mLoss[0m : 1.59295
[1mStep[0m  [36/42], [94mLoss[0m : 1.71795
[1mStep[0m  [40/42], [94mLoss[0m : 1.77261

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.585, [92mTest[0m: 2.464, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.63100
[1mStep[0m  [4/42], [94mLoss[0m : 1.52829
[1mStep[0m  [8/42], [94mLoss[0m : 1.52569
[1mStep[0m  [12/42], [94mLoss[0m : 1.64035
[1mStep[0m  [16/42], [94mLoss[0m : 1.67053
[1mStep[0m  [20/42], [94mLoss[0m : 1.67114
[1mStep[0m  [24/42], [94mLoss[0m : 1.53542
[1mStep[0m  [28/42], [94mLoss[0m : 1.61361
[1mStep[0m  [32/42], [94mLoss[0m : 1.69125
[1mStep[0m  [36/42], [94mLoss[0m : 1.50598
[1mStep[0m  [40/42], [94mLoss[0m : 1.63280

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.571, [92mTest[0m: 2.500, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.44780
[1mStep[0m  [4/42], [94mLoss[0m : 1.51778
[1mStep[0m  [8/42], [94mLoss[0m : 1.46972
[1mStep[0m  [12/42], [94mLoss[0m : 1.50266
[1mStep[0m  [16/42], [94mLoss[0m : 1.55489
[1mStep[0m  [20/42], [94mLoss[0m : 1.59709
[1mStep[0m  [24/42], [94mLoss[0m : 1.62966
[1mStep[0m  [28/42], [94mLoss[0m : 1.56923
[1mStep[0m  [32/42], [94mLoss[0m : 1.60044
[1mStep[0m  [36/42], [94mLoss[0m : 1.60844
[1mStep[0m  [40/42], [94mLoss[0m : 1.43865

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.559, [92mTest[0m: 2.496, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.52882
[1mStep[0m  [4/42], [94mLoss[0m : 1.44309
[1mStep[0m  [8/42], [94mLoss[0m : 1.59289
[1mStep[0m  [12/42], [94mLoss[0m : 1.46218
[1mStep[0m  [16/42], [94mLoss[0m : 1.37107
[1mStep[0m  [20/42], [94mLoss[0m : 1.59511
[1mStep[0m  [24/42], [94mLoss[0m : 1.70730
[1mStep[0m  [28/42], [94mLoss[0m : 1.43670
[1mStep[0m  [32/42], [94mLoss[0m : 1.56704
[1mStep[0m  [36/42], [94mLoss[0m : 1.48119
[1mStep[0m  [40/42], [94mLoss[0m : 1.47393

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.525, [92mTest[0m: 2.483, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.483
====================================

Phase 2 - Evaluation MAE:  2.4827970266342163
MAE score P1      2.302282
MAE score P2      2.482797
loss              1.524569
learning_rate     0.007525
batch_size             256
hidden_sizes         [100]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.4
momentum               0.5
weight_decay          0.01
Name: 25, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 10.49992
[1mStep[0m  [4/42], [94mLoss[0m : 9.75476
[1mStep[0m  [8/42], [94mLoss[0m : 8.84042
[1mStep[0m  [12/42], [94mLoss[0m : 7.27562
[1mStep[0m  [16/42], [94mLoss[0m : 6.18729
[1mStep[0m  [20/42], [94mLoss[0m : 4.92427
[1mStep[0m  [24/42], [94mLoss[0m : 4.71008
[1mStep[0m  [28/42], [94mLoss[0m : 3.40983
[1mStep[0m  [32/42], [94mLoss[0m : 3.09559
[1mStep[0m  [36/42], [94mLoss[0m : 3.52686
[1mStep[0m  [40/42], [94mLoss[0m : 2.65416

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 5.743, [92mTest[0m: 11.037, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59351
[1mStep[0m  [4/42], [94mLoss[0m : 2.66512
[1mStep[0m  [8/42], [94mLoss[0m : 2.70685
[1mStep[0m  [12/42], [94mLoss[0m : 2.83431
[1mStep[0m  [16/42], [94mLoss[0m : 2.51460
[1mStep[0m  [20/42], [94mLoss[0m : 2.56813
[1mStep[0m  [24/42], [94mLoss[0m : 2.50666
[1mStep[0m  [28/42], [94mLoss[0m : 2.45110
[1mStep[0m  [32/42], [94mLoss[0m : 2.66679
[1mStep[0m  [36/42], [94mLoss[0m : 2.52401
[1mStep[0m  [40/42], [94mLoss[0m : 2.55385

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.664, [92mTest[0m: 3.468, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34014
[1mStep[0m  [4/42], [94mLoss[0m : 2.65901
[1mStep[0m  [8/42], [94mLoss[0m : 2.47848
[1mStep[0m  [12/42], [94mLoss[0m : 2.54885
[1mStep[0m  [16/42], [94mLoss[0m : 2.58549
[1mStep[0m  [20/42], [94mLoss[0m : 2.64727
[1mStep[0m  [24/42], [94mLoss[0m : 2.63558
[1mStep[0m  [28/42], [94mLoss[0m : 2.49914
[1mStep[0m  [32/42], [94mLoss[0m : 2.33455
[1mStep[0m  [36/42], [94mLoss[0m : 2.38994
[1mStep[0m  [40/42], [94mLoss[0m : 2.42335

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.552, [92mTest[0m: 2.706, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39982
[1mStep[0m  [4/42], [94mLoss[0m : 2.56137
[1mStep[0m  [8/42], [94mLoss[0m : 2.68002
[1mStep[0m  [12/42], [94mLoss[0m : 2.61800
[1mStep[0m  [16/42], [94mLoss[0m : 2.57536
[1mStep[0m  [20/42], [94mLoss[0m : 2.47459
[1mStep[0m  [24/42], [94mLoss[0m : 2.62885
[1mStep[0m  [28/42], [94mLoss[0m : 2.50390
[1mStep[0m  [32/42], [94mLoss[0m : 2.55481
[1mStep[0m  [36/42], [94mLoss[0m : 2.34532
[1mStep[0m  [40/42], [94mLoss[0m : 2.43073

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.559, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49455
[1mStep[0m  [4/42], [94mLoss[0m : 2.58154
[1mStep[0m  [8/42], [94mLoss[0m : 2.54632
[1mStep[0m  [12/42], [94mLoss[0m : 2.36788
[1mStep[0m  [16/42], [94mLoss[0m : 2.39479
[1mStep[0m  [20/42], [94mLoss[0m : 2.57394
[1mStep[0m  [24/42], [94mLoss[0m : 2.49323
[1mStep[0m  [28/42], [94mLoss[0m : 2.45679
[1mStep[0m  [32/42], [94mLoss[0m : 2.44714
[1mStep[0m  [36/42], [94mLoss[0m : 2.32857
[1mStep[0m  [40/42], [94mLoss[0m : 2.45065

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.567, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39446
[1mStep[0m  [4/42], [94mLoss[0m : 2.51029
[1mStep[0m  [8/42], [94mLoss[0m : 2.18601
[1mStep[0m  [12/42], [94mLoss[0m : 2.28569
[1mStep[0m  [16/42], [94mLoss[0m : 2.36921
[1mStep[0m  [20/42], [94mLoss[0m : 2.46043
[1mStep[0m  [24/42], [94mLoss[0m : 2.64616
[1mStep[0m  [28/42], [94mLoss[0m : 2.52526
[1mStep[0m  [32/42], [94mLoss[0m : 2.58206
[1mStep[0m  [36/42], [94mLoss[0m : 2.49866
[1mStep[0m  [40/42], [94mLoss[0m : 2.72716

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.571, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49423
[1mStep[0m  [4/42], [94mLoss[0m : 2.23698
[1mStep[0m  [8/42], [94mLoss[0m : 2.29506
[1mStep[0m  [12/42], [94mLoss[0m : 2.40895
[1mStep[0m  [16/42], [94mLoss[0m : 2.62539
[1mStep[0m  [20/42], [94mLoss[0m : 2.42103
[1mStep[0m  [24/42], [94mLoss[0m : 2.57602
[1mStep[0m  [28/42], [94mLoss[0m : 2.45552
[1mStep[0m  [32/42], [94mLoss[0m : 2.51346
[1mStep[0m  [36/42], [94mLoss[0m : 2.48924
[1mStep[0m  [40/42], [94mLoss[0m : 2.44204

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.571, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39271
[1mStep[0m  [4/42], [94mLoss[0m : 2.50227
[1mStep[0m  [8/42], [94mLoss[0m : 2.39301
[1mStep[0m  [12/42], [94mLoss[0m : 2.66132
[1mStep[0m  [16/42], [94mLoss[0m : 2.40831
[1mStep[0m  [20/42], [94mLoss[0m : 2.44784
[1mStep[0m  [24/42], [94mLoss[0m : 2.47004
[1mStep[0m  [28/42], [94mLoss[0m : 2.45099
[1mStep[0m  [32/42], [94mLoss[0m : 2.49239
[1mStep[0m  [36/42], [94mLoss[0m : 2.51917
[1mStep[0m  [40/42], [94mLoss[0m : 2.62593

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.484, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.66740
[1mStep[0m  [4/42], [94mLoss[0m : 2.52618
[1mStep[0m  [8/42], [94mLoss[0m : 2.46477
[1mStep[0m  [12/42], [94mLoss[0m : 2.31856
[1mStep[0m  [16/42], [94mLoss[0m : 2.60631
[1mStep[0m  [20/42], [94mLoss[0m : 2.54638
[1mStep[0m  [24/42], [94mLoss[0m : 2.29925
[1mStep[0m  [28/42], [94mLoss[0m : 2.39185
[1mStep[0m  [32/42], [94mLoss[0m : 2.59369
[1mStep[0m  [36/42], [94mLoss[0m : 2.16262
[1mStep[0m  [40/42], [94mLoss[0m : 2.57209

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.555, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49694
[1mStep[0m  [4/42], [94mLoss[0m : 2.49492
[1mStep[0m  [8/42], [94mLoss[0m : 2.51903
[1mStep[0m  [12/42], [94mLoss[0m : 2.65928
[1mStep[0m  [16/42], [94mLoss[0m : 2.45116
[1mStep[0m  [20/42], [94mLoss[0m : 2.57681
[1mStep[0m  [24/42], [94mLoss[0m : 2.43931
[1mStep[0m  [28/42], [94mLoss[0m : 2.35968
[1mStep[0m  [32/42], [94mLoss[0m : 2.41988
[1mStep[0m  [36/42], [94mLoss[0m : 2.48421
[1mStep[0m  [40/42], [94mLoss[0m : 2.33219

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.477, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40023
[1mStep[0m  [4/42], [94mLoss[0m : 2.43475
[1mStep[0m  [8/42], [94mLoss[0m : 2.50002
[1mStep[0m  [12/42], [94mLoss[0m : 2.39229
[1mStep[0m  [16/42], [94mLoss[0m : 2.51529
[1mStep[0m  [20/42], [94mLoss[0m : 2.40730
[1mStep[0m  [24/42], [94mLoss[0m : 2.62067
[1mStep[0m  [28/42], [94mLoss[0m : 2.30909
[1mStep[0m  [32/42], [94mLoss[0m : 2.60579
[1mStep[0m  [36/42], [94mLoss[0m : 2.39135
[1mStep[0m  [40/42], [94mLoss[0m : 2.30164

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.529, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52566
[1mStep[0m  [4/42], [94mLoss[0m : 2.46510
[1mStep[0m  [8/42], [94mLoss[0m : 2.60803
[1mStep[0m  [12/42], [94mLoss[0m : 2.34558
[1mStep[0m  [16/42], [94mLoss[0m : 2.51501
[1mStep[0m  [20/42], [94mLoss[0m : 2.42076
[1mStep[0m  [24/42], [94mLoss[0m : 2.24821
[1mStep[0m  [28/42], [94mLoss[0m : 2.27514
[1mStep[0m  [32/42], [94mLoss[0m : 2.63776
[1mStep[0m  [36/42], [94mLoss[0m : 2.27089
[1mStep[0m  [40/42], [94mLoss[0m : 2.45788

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.521, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46192
[1mStep[0m  [4/42], [94mLoss[0m : 2.38315
[1mStep[0m  [8/42], [94mLoss[0m : 2.29060
[1mStep[0m  [12/42], [94mLoss[0m : 2.57191
[1mStep[0m  [16/42], [94mLoss[0m : 2.53674
[1mStep[0m  [20/42], [94mLoss[0m : 2.35199
[1mStep[0m  [24/42], [94mLoss[0m : 2.58732
[1mStep[0m  [28/42], [94mLoss[0m : 2.55953
[1mStep[0m  [32/42], [94mLoss[0m : 2.49626
[1mStep[0m  [36/42], [94mLoss[0m : 2.46906
[1mStep[0m  [40/42], [94mLoss[0m : 2.47737

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.478, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36604
[1mStep[0m  [4/42], [94mLoss[0m : 2.41785
[1mStep[0m  [8/42], [94mLoss[0m : 2.41306
[1mStep[0m  [12/42], [94mLoss[0m : 2.29220
[1mStep[0m  [16/42], [94mLoss[0m : 2.48022
[1mStep[0m  [20/42], [94mLoss[0m : 2.44466
[1mStep[0m  [24/42], [94mLoss[0m : 2.63512
[1mStep[0m  [28/42], [94mLoss[0m : 2.45837
[1mStep[0m  [32/42], [94mLoss[0m : 2.63411
[1mStep[0m  [36/42], [94mLoss[0m : 2.42216
[1mStep[0m  [40/42], [94mLoss[0m : 2.34540

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.510, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42456
[1mStep[0m  [4/42], [94mLoss[0m : 2.69280
[1mStep[0m  [8/42], [94mLoss[0m : 2.36758
[1mStep[0m  [12/42], [94mLoss[0m : 2.43540
[1mStep[0m  [16/42], [94mLoss[0m : 2.28216
[1mStep[0m  [20/42], [94mLoss[0m : 2.52097
[1mStep[0m  [24/42], [94mLoss[0m : 2.56507
[1mStep[0m  [28/42], [94mLoss[0m : 2.54219
[1mStep[0m  [32/42], [94mLoss[0m : 2.41447
[1mStep[0m  [36/42], [94mLoss[0m : 2.34364
[1mStep[0m  [40/42], [94mLoss[0m : 2.65284

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.475, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.22760
[1mStep[0m  [4/42], [94mLoss[0m : 2.49277
[1mStep[0m  [8/42], [94mLoss[0m : 2.26824
[1mStep[0m  [12/42], [94mLoss[0m : 2.55316
[1mStep[0m  [16/42], [94mLoss[0m : 2.48400
[1mStep[0m  [20/42], [94mLoss[0m : 2.44751
[1mStep[0m  [24/42], [94mLoss[0m : 2.45986
[1mStep[0m  [28/42], [94mLoss[0m : 2.42688
[1mStep[0m  [32/42], [94mLoss[0m : 2.53984
[1mStep[0m  [36/42], [94mLoss[0m : 2.46526
[1mStep[0m  [40/42], [94mLoss[0m : 2.43827

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.493, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.73941
[1mStep[0m  [4/42], [94mLoss[0m : 2.21573
[1mStep[0m  [8/42], [94mLoss[0m : 2.27147
[1mStep[0m  [12/42], [94mLoss[0m : 2.48772
[1mStep[0m  [16/42], [94mLoss[0m : 2.50844
[1mStep[0m  [20/42], [94mLoss[0m : 2.48969
[1mStep[0m  [24/42], [94mLoss[0m : 2.36384
[1mStep[0m  [28/42], [94mLoss[0m : 2.26850
[1mStep[0m  [32/42], [94mLoss[0m : 2.46848
[1mStep[0m  [36/42], [94mLoss[0m : 2.36778
[1mStep[0m  [40/42], [94mLoss[0m : 2.41930

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.418, [92mTest[0m: 2.464, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38319
[1mStep[0m  [4/42], [94mLoss[0m : 2.37388
[1mStep[0m  [8/42], [94mLoss[0m : 2.46627
[1mStep[0m  [12/42], [94mLoss[0m : 2.16524
[1mStep[0m  [16/42], [94mLoss[0m : 2.79919
[1mStep[0m  [20/42], [94mLoss[0m : 2.57609
[1mStep[0m  [24/42], [94mLoss[0m : 2.51050
[1mStep[0m  [28/42], [94mLoss[0m : 2.28096
[1mStep[0m  [32/42], [94mLoss[0m : 2.32328
[1mStep[0m  [36/42], [94mLoss[0m : 2.31089
[1mStep[0m  [40/42], [94mLoss[0m : 2.56654

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.479, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47638
[1mStep[0m  [4/42], [94mLoss[0m : 2.44920
[1mStep[0m  [8/42], [94mLoss[0m : 2.54264
[1mStep[0m  [12/42], [94mLoss[0m : 2.30066
[1mStep[0m  [16/42], [94mLoss[0m : 2.52784
[1mStep[0m  [20/42], [94mLoss[0m : 2.38164
[1mStep[0m  [24/42], [94mLoss[0m : 2.26979
[1mStep[0m  [28/42], [94mLoss[0m : 2.43185
[1mStep[0m  [32/42], [94mLoss[0m : 2.46691
[1mStep[0m  [36/42], [94mLoss[0m : 2.42378
[1mStep[0m  [40/42], [94mLoss[0m : 2.26537

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.480, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59976
[1mStep[0m  [4/42], [94mLoss[0m : 2.40583
[1mStep[0m  [8/42], [94mLoss[0m : 2.33350
[1mStep[0m  [12/42], [94mLoss[0m : 2.49527
[1mStep[0m  [16/42], [94mLoss[0m : 2.30687
[1mStep[0m  [20/42], [94mLoss[0m : 2.40603
[1mStep[0m  [24/42], [94mLoss[0m : 2.43682
[1mStep[0m  [28/42], [94mLoss[0m : 2.39359
[1mStep[0m  [32/42], [94mLoss[0m : 2.47011
[1mStep[0m  [36/42], [94mLoss[0m : 2.36586
[1mStep[0m  [40/42], [94mLoss[0m : 2.58914

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.455, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57052
[1mStep[0m  [4/42], [94mLoss[0m : 2.27613
[1mStep[0m  [8/42], [94mLoss[0m : 2.48596
[1mStep[0m  [12/42], [94mLoss[0m : 2.46524
[1mStep[0m  [16/42], [94mLoss[0m : 2.28089
[1mStep[0m  [20/42], [94mLoss[0m : 2.14302
[1mStep[0m  [24/42], [94mLoss[0m : 2.44539
[1mStep[0m  [28/42], [94mLoss[0m : 2.35953
[1mStep[0m  [32/42], [94mLoss[0m : 2.50812
[1mStep[0m  [36/42], [94mLoss[0m : 2.55703
[1mStep[0m  [40/42], [94mLoss[0m : 2.50639

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.461, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34527
[1mStep[0m  [4/42], [94mLoss[0m : 2.38305
[1mStep[0m  [8/42], [94mLoss[0m : 2.29061
[1mStep[0m  [12/42], [94mLoss[0m : 2.61380
[1mStep[0m  [16/42], [94mLoss[0m : 2.28072
[1mStep[0m  [20/42], [94mLoss[0m : 2.38136
[1mStep[0m  [24/42], [94mLoss[0m : 2.39379
[1mStep[0m  [28/42], [94mLoss[0m : 2.24542
[1mStep[0m  [32/42], [94mLoss[0m : 2.51373
[1mStep[0m  [36/42], [94mLoss[0m : 2.52919
[1mStep[0m  [40/42], [94mLoss[0m : 2.43741

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.418, [92mTest[0m: 2.438, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48208
[1mStep[0m  [4/42], [94mLoss[0m : 2.24774
[1mStep[0m  [8/42], [94mLoss[0m : 2.61642
[1mStep[0m  [12/42], [94mLoss[0m : 2.56941
[1mStep[0m  [16/42], [94mLoss[0m : 2.40988
[1mStep[0m  [20/42], [94mLoss[0m : 2.39591
[1mStep[0m  [24/42], [94mLoss[0m : 2.43485
[1mStep[0m  [28/42], [94mLoss[0m : 2.52316
[1mStep[0m  [32/42], [94mLoss[0m : 2.31121
[1mStep[0m  [36/42], [94mLoss[0m : 2.29863
[1mStep[0m  [40/42], [94mLoss[0m : 2.45363

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.462, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26613
[1mStep[0m  [4/42], [94mLoss[0m : 2.43704
[1mStep[0m  [8/42], [94mLoss[0m : 2.43200
[1mStep[0m  [12/42], [94mLoss[0m : 2.40076
[1mStep[0m  [16/42], [94mLoss[0m : 2.30937
[1mStep[0m  [20/42], [94mLoss[0m : 2.38999
[1mStep[0m  [24/42], [94mLoss[0m : 2.35261
[1mStep[0m  [28/42], [94mLoss[0m : 2.39067
[1mStep[0m  [32/42], [94mLoss[0m : 2.49414
[1mStep[0m  [36/42], [94mLoss[0m : 2.43976
[1mStep[0m  [40/42], [94mLoss[0m : 2.41594

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.460, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.30700
[1mStep[0m  [4/42], [94mLoss[0m : 2.43006
[1mStep[0m  [8/42], [94mLoss[0m : 2.54481
[1mStep[0m  [12/42], [94mLoss[0m : 2.40363
[1mStep[0m  [16/42], [94mLoss[0m : 2.28195
[1mStep[0m  [20/42], [94mLoss[0m : 2.45828
[1mStep[0m  [24/42], [94mLoss[0m : 2.69651
[1mStep[0m  [28/42], [94mLoss[0m : 2.35131
[1mStep[0m  [32/42], [94mLoss[0m : 2.32626
[1mStep[0m  [36/42], [94mLoss[0m : 2.34652
[1mStep[0m  [40/42], [94mLoss[0m : 2.38767

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.438, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50226
[1mStep[0m  [4/42], [94mLoss[0m : 2.52181
[1mStep[0m  [8/42], [94mLoss[0m : 2.30686
[1mStep[0m  [12/42], [94mLoss[0m : 2.48698
[1mStep[0m  [16/42], [94mLoss[0m : 2.26259
[1mStep[0m  [20/42], [94mLoss[0m : 2.39425
[1mStep[0m  [24/42], [94mLoss[0m : 2.44466
[1mStep[0m  [28/42], [94mLoss[0m : 2.48555
[1mStep[0m  [32/42], [94mLoss[0m : 2.45696
[1mStep[0m  [36/42], [94mLoss[0m : 2.31428
[1mStep[0m  [40/42], [94mLoss[0m : 2.28550

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.443, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51119
[1mStep[0m  [4/42], [94mLoss[0m : 2.37104
[1mStep[0m  [8/42], [94mLoss[0m : 2.43669
[1mStep[0m  [12/42], [94mLoss[0m : 2.41001
[1mStep[0m  [16/42], [94mLoss[0m : 2.48154
[1mStep[0m  [20/42], [94mLoss[0m : 2.32862
[1mStep[0m  [24/42], [94mLoss[0m : 2.45414
[1mStep[0m  [28/42], [94mLoss[0m : 2.47274
[1mStep[0m  [32/42], [94mLoss[0m : 2.41216
[1mStep[0m  [36/42], [94mLoss[0m : 2.19997
[1mStep[0m  [40/42], [94mLoss[0m : 2.26715

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.448, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46260
[1mStep[0m  [4/42], [94mLoss[0m : 2.42598
[1mStep[0m  [8/42], [94mLoss[0m : 2.54315
[1mStep[0m  [12/42], [94mLoss[0m : 2.32158
[1mStep[0m  [16/42], [94mLoss[0m : 2.43668
[1mStep[0m  [20/42], [94mLoss[0m : 2.35910
[1mStep[0m  [24/42], [94mLoss[0m : 2.31971
[1mStep[0m  [28/42], [94mLoss[0m : 2.38989
[1mStep[0m  [32/42], [94mLoss[0m : 2.33424
[1mStep[0m  [36/42], [94mLoss[0m : 2.55906
[1mStep[0m  [40/42], [94mLoss[0m : 2.33098

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.383, [92mTest[0m: 2.454, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31479
[1mStep[0m  [4/42], [94mLoss[0m : 2.43965
[1mStep[0m  [8/42], [94mLoss[0m : 2.53515
[1mStep[0m  [12/42], [94mLoss[0m : 2.24114
[1mStep[0m  [16/42], [94mLoss[0m : 2.35515
[1mStep[0m  [20/42], [94mLoss[0m : 2.34898
[1mStep[0m  [24/42], [94mLoss[0m : 2.26678
[1mStep[0m  [28/42], [94mLoss[0m : 2.31095
[1mStep[0m  [32/42], [94mLoss[0m : 2.38281
[1mStep[0m  [36/42], [94mLoss[0m : 2.34226
[1mStep[0m  [40/42], [94mLoss[0m : 2.05771

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.368, [92mTest[0m: 2.463, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21410
[1mStep[0m  [4/42], [94mLoss[0m : 2.24612
[1mStep[0m  [8/42], [94mLoss[0m : 2.48814
[1mStep[0m  [12/42], [94mLoss[0m : 2.46255
[1mStep[0m  [16/42], [94mLoss[0m : 2.52248
[1mStep[0m  [20/42], [94mLoss[0m : 2.15063
[1mStep[0m  [24/42], [94mLoss[0m : 2.21910
[1mStep[0m  [28/42], [94mLoss[0m : 2.28928
[1mStep[0m  [32/42], [94mLoss[0m : 2.40418
[1mStep[0m  [36/42], [94mLoss[0m : 2.34483
[1mStep[0m  [40/42], [94mLoss[0m : 2.52813

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.380, [92mTest[0m: 2.486, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.462
====================================

Phase 1 - Evaluation MAE:  2.4615028415407454
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 2.36336
[1mStep[0m  [4/42], [94mLoss[0m : 2.50511
[1mStep[0m  [8/42], [94mLoss[0m : 2.54463
[1mStep[0m  [12/42], [94mLoss[0m : 2.47770
[1mStep[0m  [16/42], [94mLoss[0m : 2.44496
[1mStep[0m  [20/42], [94mLoss[0m : 2.54890
[1mStep[0m  [24/42], [94mLoss[0m : 2.54396
[1mStep[0m  [28/42], [94mLoss[0m : 2.32854
[1mStep[0m  [32/42], [94mLoss[0m : 2.63904
[1mStep[0m  [36/42], [94mLoss[0m : 2.51911
[1mStep[0m  [40/42], [94mLoss[0m : 2.32451

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.459, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58864
[1mStep[0m  [4/42], [94mLoss[0m : 2.47426
[1mStep[0m  [8/42], [94mLoss[0m : 2.30786
[1mStep[0m  [12/42], [94mLoss[0m : 2.35892
[1mStep[0m  [16/42], [94mLoss[0m : 2.47574
[1mStep[0m  [20/42], [94mLoss[0m : 2.27246
[1mStep[0m  [24/42], [94mLoss[0m : 2.25262
[1mStep[0m  [28/42], [94mLoss[0m : 2.43974
[1mStep[0m  [32/42], [94mLoss[0m : 2.29687
[1mStep[0m  [36/42], [94mLoss[0m : 2.26540
[1mStep[0m  [40/42], [94mLoss[0m : 2.35197

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.379, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28270
[1mStep[0m  [4/42], [94mLoss[0m : 2.47382
[1mStep[0m  [8/42], [94mLoss[0m : 2.26757
[1mStep[0m  [12/42], [94mLoss[0m : 2.39614
[1mStep[0m  [16/42], [94mLoss[0m : 2.27789
[1mStep[0m  [20/42], [94mLoss[0m : 2.21622
[1mStep[0m  [24/42], [94mLoss[0m : 2.41564
[1mStep[0m  [28/42], [94mLoss[0m : 2.44740
[1mStep[0m  [32/42], [94mLoss[0m : 2.33173
[1mStep[0m  [36/42], [94mLoss[0m : 2.28364
[1mStep[0m  [40/42], [94mLoss[0m : 2.32130

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.377, [92mTest[0m: 2.500, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43207
[1mStep[0m  [4/42], [94mLoss[0m : 2.36613
[1mStep[0m  [8/42], [94mLoss[0m : 2.40937
[1mStep[0m  [12/42], [94mLoss[0m : 2.18270
[1mStep[0m  [16/42], [94mLoss[0m : 2.38639
[1mStep[0m  [20/42], [94mLoss[0m : 2.23612
[1mStep[0m  [24/42], [94mLoss[0m : 2.45804
[1mStep[0m  [28/42], [94mLoss[0m : 2.20199
[1mStep[0m  [32/42], [94mLoss[0m : 2.16198
[1mStep[0m  [36/42], [94mLoss[0m : 2.28227
[1mStep[0m  [40/42], [94mLoss[0m : 2.39319

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.332, [92mTest[0m: 2.443, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.15530
[1mStep[0m  [4/42], [94mLoss[0m : 2.38168
[1mStep[0m  [8/42], [94mLoss[0m : 2.20048
[1mStep[0m  [12/42], [94mLoss[0m : 2.32672
[1mStep[0m  [16/42], [94mLoss[0m : 2.55860
[1mStep[0m  [20/42], [94mLoss[0m : 2.25754
[1mStep[0m  [24/42], [94mLoss[0m : 2.45336
[1mStep[0m  [28/42], [94mLoss[0m : 2.20853
[1mStep[0m  [32/42], [94mLoss[0m : 2.27012
[1mStep[0m  [36/42], [94mLoss[0m : 2.52327
[1mStep[0m  [40/42], [94mLoss[0m : 2.39341

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.303, [92mTest[0m: 2.351, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21453
[1mStep[0m  [4/42], [94mLoss[0m : 2.27762
[1mStep[0m  [8/42], [94mLoss[0m : 2.18919
[1mStep[0m  [12/42], [94mLoss[0m : 2.13407
[1mStep[0m  [16/42], [94mLoss[0m : 2.33741
[1mStep[0m  [20/42], [94mLoss[0m : 2.19129
[1mStep[0m  [24/42], [94mLoss[0m : 2.29143
[1mStep[0m  [28/42], [94mLoss[0m : 2.20565
[1mStep[0m  [32/42], [94mLoss[0m : 2.28028
[1mStep[0m  [36/42], [94mLoss[0m : 2.15862
[1mStep[0m  [40/42], [94mLoss[0m : 2.34424

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.260, [92mTest[0m: 2.370, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.17462
[1mStep[0m  [4/42], [94mLoss[0m : 2.13671
[1mStep[0m  [8/42], [94mLoss[0m : 2.32152
[1mStep[0m  [12/42], [94mLoss[0m : 2.09019
[1mStep[0m  [16/42], [94mLoss[0m : 2.27172
[1mStep[0m  [20/42], [94mLoss[0m : 2.46591
[1mStep[0m  [24/42], [94mLoss[0m : 2.24325
[1mStep[0m  [28/42], [94mLoss[0m : 2.09331
[1mStep[0m  [32/42], [94mLoss[0m : 2.20558
[1mStep[0m  [36/42], [94mLoss[0m : 2.17856
[1mStep[0m  [40/42], [94mLoss[0m : 2.06882

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.210, [92mTest[0m: 2.399, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.08226
[1mStep[0m  [4/42], [94mLoss[0m : 2.05867
[1mStep[0m  [8/42], [94mLoss[0m : 2.32376
[1mStep[0m  [12/42], [94mLoss[0m : 1.95351
[1mStep[0m  [16/42], [94mLoss[0m : 2.30005
[1mStep[0m  [20/42], [94mLoss[0m : 2.32419
[1mStep[0m  [24/42], [94mLoss[0m : 2.00011
[1mStep[0m  [28/42], [94mLoss[0m : 1.94452
[1mStep[0m  [32/42], [94mLoss[0m : 2.21812
[1mStep[0m  [36/42], [94mLoss[0m : 2.10997
[1mStep[0m  [40/42], [94mLoss[0m : 2.17697

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.176, [92mTest[0m: 2.433, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.18679
[1mStep[0m  [4/42], [94mLoss[0m : 2.09047
[1mStep[0m  [8/42], [94mLoss[0m : 2.15918
[1mStep[0m  [12/42], [94mLoss[0m : 2.20160
[1mStep[0m  [16/42], [94mLoss[0m : 2.16278
[1mStep[0m  [20/42], [94mLoss[0m : 2.06712
[1mStep[0m  [24/42], [94mLoss[0m : 2.25647
[1mStep[0m  [28/42], [94mLoss[0m : 2.19740
[1mStep[0m  [32/42], [94mLoss[0m : 2.11300
[1mStep[0m  [36/42], [94mLoss[0m : 2.05413
[1mStep[0m  [40/42], [94mLoss[0m : 2.25738

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.139, [92mTest[0m: 2.451, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.09914
[1mStep[0m  [4/42], [94mLoss[0m : 2.11612
[1mStep[0m  [8/42], [94mLoss[0m : 2.05908
[1mStep[0m  [12/42], [94mLoss[0m : 2.14645
[1mStep[0m  [16/42], [94mLoss[0m : 2.13803
[1mStep[0m  [20/42], [94mLoss[0m : 1.98369
[1mStep[0m  [24/42], [94mLoss[0m : 2.12212
[1mStep[0m  [28/42], [94mLoss[0m : 2.18137
[1mStep[0m  [32/42], [94mLoss[0m : 2.13766
[1mStep[0m  [36/42], [94mLoss[0m : 1.97868
[1mStep[0m  [40/42], [94mLoss[0m : 2.17888

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.110, [92mTest[0m: 2.613, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.05717
[1mStep[0m  [4/42], [94mLoss[0m : 1.94627
[1mStep[0m  [8/42], [94mLoss[0m : 2.02057
[1mStep[0m  [12/42], [94mLoss[0m : 2.12949
[1mStep[0m  [16/42], [94mLoss[0m : 2.17783
[1mStep[0m  [20/42], [94mLoss[0m : 1.96964
[1mStep[0m  [24/42], [94mLoss[0m : 2.10340
[1mStep[0m  [28/42], [94mLoss[0m : 2.00386
[1mStep[0m  [32/42], [94mLoss[0m : 1.82908
[1mStep[0m  [36/42], [94mLoss[0m : 2.30036
[1mStep[0m  [40/42], [94mLoss[0m : 2.17009

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.065, [92mTest[0m: 2.599, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.98888
[1mStep[0m  [4/42], [94mLoss[0m : 2.09453
[1mStep[0m  [8/42], [94mLoss[0m : 2.05511
[1mStep[0m  [12/42], [94mLoss[0m : 1.96374
[1mStep[0m  [16/42], [94mLoss[0m : 2.15186
[1mStep[0m  [20/42], [94mLoss[0m : 1.99298
[1mStep[0m  [24/42], [94mLoss[0m : 1.76171
[1mStep[0m  [28/42], [94mLoss[0m : 1.98811
[1mStep[0m  [32/42], [94mLoss[0m : 1.79718
[1mStep[0m  [36/42], [94mLoss[0m : 2.08766
[1mStep[0m  [40/42], [94mLoss[0m : 2.01323

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.012, [92mTest[0m: 2.550, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.07828
[1mStep[0m  [4/42], [94mLoss[0m : 1.99035
[1mStep[0m  [8/42], [94mLoss[0m : 1.94239
[1mStep[0m  [12/42], [94mLoss[0m : 2.08721
[1mStep[0m  [16/42], [94mLoss[0m : 2.23233
[1mStep[0m  [20/42], [94mLoss[0m : 2.18477
[1mStep[0m  [24/42], [94mLoss[0m : 1.75345
[1mStep[0m  [28/42], [94mLoss[0m : 1.89856
[1mStep[0m  [32/42], [94mLoss[0m : 1.97645
[1mStep[0m  [36/42], [94mLoss[0m : 1.91603
[1mStep[0m  [40/42], [94mLoss[0m : 1.95185

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.999, [92mTest[0m: 2.663, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.04135
[1mStep[0m  [4/42], [94mLoss[0m : 1.94791
[1mStep[0m  [8/42], [94mLoss[0m : 1.94855
[1mStep[0m  [12/42], [94mLoss[0m : 1.94735
[1mStep[0m  [16/42], [94mLoss[0m : 1.83182
[1mStep[0m  [20/42], [94mLoss[0m : 1.87202
[1mStep[0m  [24/42], [94mLoss[0m : 2.04559
[1mStep[0m  [28/42], [94mLoss[0m : 2.02517
[1mStep[0m  [32/42], [94mLoss[0m : 2.01541
[1mStep[0m  [36/42], [94mLoss[0m : 2.09965
[1mStep[0m  [40/42], [94mLoss[0m : 2.01120

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.949, [92mTest[0m: 2.629, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.95731
[1mStep[0m  [4/42], [94mLoss[0m : 1.77826
[1mStep[0m  [8/42], [94mLoss[0m : 2.06588
[1mStep[0m  [12/42], [94mLoss[0m : 2.12497
[1mStep[0m  [16/42], [94mLoss[0m : 1.75681
[1mStep[0m  [20/42], [94mLoss[0m : 1.87054
[1mStep[0m  [24/42], [94mLoss[0m : 1.86496
[1mStep[0m  [28/42], [94mLoss[0m : 1.82387
[1mStep[0m  [32/42], [94mLoss[0m : 2.03634
[1mStep[0m  [36/42], [94mLoss[0m : 2.09869
[1mStep[0m  [40/42], [94mLoss[0m : 2.01446

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.913, [92mTest[0m: 2.644, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.80345
[1mStep[0m  [4/42], [94mLoss[0m : 1.83116
[1mStep[0m  [8/42], [94mLoss[0m : 2.02395
[1mStep[0m  [12/42], [94mLoss[0m : 1.89349
[1mStep[0m  [16/42], [94mLoss[0m : 1.97860
[1mStep[0m  [20/42], [94mLoss[0m : 1.86202
[1mStep[0m  [24/42], [94mLoss[0m : 1.96733
[1mStep[0m  [28/42], [94mLoss[0m : 1.91291
[1mStep[0m  [32/42], [94mLoss[0m : 1.70953
[1mStep[0m  [36/42], [94mLoss[0m : 1.82754
[1mStep[0m  [40/42], [94mLoss[0m : 1.94724

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.883, [92mTest[0m: 2.620, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.84702
[1mStep[0m  [4/42], [94mLoss[0m : 1.87274
[1mStep[0m  [8/42], [94mLoss[0m : 1.83661
[1mStep[0m  [12/42], [94mLoss[0m : 1.73009
[1mStep[0m  [16/42], [94mLoss[0m : 1.86710
[1mStep[0m  [20/42], [94mLoss[0m : 1.85016
[1mStep[0m  [24/42], [94mLoss[0m : 1.89174
[1mStep[0m  [28/42], [94mLoss[0m : 2.05302
[1mStep[0m  [32/42], [94mLoss[0m : 1.90671
[1mStep[0m  [36/42], [94mLoss[0m : 1.95660
[1mStep[0m  [40/42], [94mLoss[0m : 1.80966

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.866, [92mTest[0m: 2.686, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.84519
[1mStep[0m  [4/42], [94mLoss[0m : 1.64279
[1mStep[0m  [8/42], [94mLoss[0m : 1.99516
[1mStep[0m  [12/42], [94mLoss[0m : 1.97358
[1mStep[0m  [16/42], [94mLoss[0m : 1.75247
[1mStep[0m  [20/42], [94mLoss[0m : 1.84868
[1mStep[0m  [24/42], [94mLoss[0m : 1.97724
[1mStep[0m  [28/42], [94mLoss[0m : 1.82047
[1mStep[0m  [32/42], [94mLoss[0m : 1.88496
[1mStep[0m  [36/42], [94mLoss[0m : 1.83991
[1mStep[0m  [40/42], [94mLoss[0m : 1.73428

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.824, [92mTest[0m: 2.700, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.73896
[1mStep[0m  [4/42], [94mLoss[0m : 1.66787
[1mStep[0m  [8/42], [94mLoss[0m : 1.73584
[1mStep[0m  [12/42], [94mLoss[0m : 2.07036
[1mStep[0m  [16/42], [94mLoss[0m : 1.68406
[1mStep[0m  [20/42], [94mLoss[0m : 1.74553
[1mStep[0m  [24/42], [94mLoss[0m : 1.77059
[1mStep[0m  [28/42], [94mLoss[0m : 1.69787
[1mStep[0m  [32/42], [94mLoss[0m : 1.81334
[1mStep[0m  [36/42], [94mLoss[0m : 1.82201
[1mStep[0m  [40/42], [94mLoss[0m : 1.82029

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.784, [92mTest[0m: 2.635, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.57667
[1mStep[0m  [4/42], [94mLoss[0m : 1.62442
[1mStep[0m  [8/42], [94mLoss[0m : 1.78001
[1mStep[0m  [12/42], [94mLoss[0m : 1.70042
[1mStep[0m  [16/42], [94mLoss[0m : 1.96845
[1mStep[0m  [20/42], [94mLoss[0m : 1.76824
[1mStep[0m  [24/42], [94mLoss[0m : 1.86782
[1mStep[0m  [28/42], [94mLoss[0m : 1.76182
[1mStep[0m  [32/42], [94mLoss[0m : 1.65928
[1mStep[0m  [36/42], [94mLoss[0m : 1.74637
[1mStep[0m  [40/42], [94mLoss[0m : 1.72471

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.757, [92mTest[0m: 2.757, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.84099
[1mStep[0m  [4/42], [94mLoss[0m : 1.75295
[1mStep[0m  [8/42], [94mLoss[0m : 1.86043
[1mStep[0m  [12/42], [94mLoss[0m : 1.52438
[1mStep[0m  [16/42], [94mLoss[0m : 1.63841
[1mStep[0m  [20/42], [94mLoss[0m : 1.62684
[1mStep[0m  [24/42], [94mLoss[0m : 1.77897
[1mStep[0m  [28/42], [94mLoss[0m : 1.66641
[1mStep[0m  [32/42], [94mLoss[0m : 1.64068
[1mStep[0m  [36/42], [94mLoss[0m : 1.63955
[1mStep[0m  [40/42], [94mLoss[0m : 1.77265

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.722, [92mTest[0m: 2.754, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.79878
[1mStep[0m  [4/42], [94mLoss[0m : 1.58229
[1mStep[0m  [8/42], [94mLoss[0m : 1.66421
[1mStep[0m  [12/42], [94mLoss[0m : 1.80259
[1mStep[0m  [16/42], [94mLoss[0m : 1.70362
[1mStep[0m  [20/42], [94mLoss[0m : 1.57098
[1mStep[0m  [24/42], [94mLoss[0m : 1.61331
[1mStep[0m  [28/42], [94mLoss[0m : 1.77059
[1mStep[0m  [32/42], [94mLoss[0m : 1.52176
[1mStep[0m  [36/42], [94mLoss[0m : 1.67575
[1mStep[0m  [40/42], [94mLoss[0m : 1.69653

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.674, [92mTest[0m: 2.906, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.69167
[1mStep[0m  [4/42], [94mLoss[0m : 1.69421
[1mStep[0m  [8/42], [94mLoss[0m : 1.50429
[1mStep[0m  [12/42], [94mLoss[0m : 1.68820
[1mStep[0m  [16/42], [94mLoss[0m : 1.63400
[1mStep[0m  [20/42], [94mLoss[0m : 1.62017
[1mStep[0m  [24/42], [94mLoss[0m : 1.61733
[1mStep[0m  [28/42], [94mLoss[0m : 1.72442
[1mStep[0m  [32/42], [94mLoss[0m : 1.58676
[1mStep[0m  [36/42], [94mLoss[0m : 1.67633
[1mStep[0m  [40/42], [94mLoss[0m : 1.67586

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.661, [92mTest[0m: 2.765, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.64122
[1mStep[0m  [4/42], [94mLoss[0m : 1.62648
[1mStep[0m  [8/42], [94mLoss[0m : 1.69397
[1mStep[0m  [12/42], [94mLoss[0m : 1.56820
[1mStep[0m  [16/42], [94mLoss[0m : 1.57675
[1mStep[0m  [20/42], [94mLoss[0m : 1.71193
[1mStep[0m  [24/42], [94mLoss[0m : 1.65312
[1mStep[0m  [28/42], [94mLoss[0m : 1.63838
[1mStep[0m  [32/42], [94mLoss[0m : 1.53342
[1mStep[0m  [36/42], [94mLoss[0m : 1.61068
[1mStep[0m  [40/42], [94mLoss[0m : 1.55349

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.655, [92mTest[0m: 2.643, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.44767
[1mStep[0m  [4/42], [94mLoss[0m : 1.70956
[1mStep[0m  [8/42], [94mLoss[0m : 1.68388
[1mStep[0m  [12/42], [94mLoss[0m : 1.64305
[1mStep[0m  [16/42], [94mLoss[0m : 1.60570
[1mStep[0m  [20/42], [94mLoss[0m : 1.69684
[1mStep[0m  [24/42], [94mLoss[0m : 1.59609
[1mStep[0m  [28/42], [94mLoss[0m : 1.59252
[1mStep[0m  [32/42], [94mLoss[0m : 1.56166
[1mStep[0m  [36/42], [94mLoss[0m : 1.81463
[1mStep[0m  [40/42], [94mLoss[0m : 1.67695

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.622, [92mTest[0m: 2.747, [96mlr[0m: 0.006772500000000002
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 24 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.768
====================================

Phase 2 - Evaluation MAE:  2.7682616199765886
MAE score P1      2.461503
MAE score P2      2.768262
loss              1.621572
learning_rate     0.007525
batch_size             256
hidden_sizes         [250]
epochs                  30
activation            relu
optimizer              sgd
early stopping        True
dropout                0.3
momentum               0.1
weight_decay        0.0001
Name: 26, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 10.64121
[1mStep[0m  [8/84], [94mLoss[0m : 10.56444
[1mStep[0m  [16/84], [94mLoss[0m : 10.41628
[1mStep[0m  [24/84], [94mLoss[0m : 10.54677
[1mStep[0m  [32/84], [94mLoss[0m : 11.22035
[1mStep[0m  [40/84], [94mLoss[0m : 10.02742
[1mStep[0m  [48/84], [94mLoss[0m : 10.19820
[1mStep[0m  [56/84], [94mLoss[0m : 11.16375
[1mStep[0m  [64/84], [94mLoss[0m : 10.15348
[1mStep[0m  [72/84], [94mLoss[0m : 10.31383
[1mStep[0m  [80/84], [94mLoss[0m : 10.42490

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.566, [92mTest[0m: 10.914, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.32363
[1mStep[0m  [8/84], [94mLoss[0m : 9.66932
[1mStep[0m  [16/84], [94mLoss[0m : 10.02678
[1mStep[0m  [24/84], [94mLoss[0m : 9.87623
[1mStep[0m  [32/84], [94mLoss[0m : 9.33750
[1mStep[0m  [40/84], [94mLoss[0m : 10.09359
[1mStep[0m  [48/84], [94mLoss[0m : 9.78983
[1mStep[0m  [56/84], [94mLoss[0m : 9.24437
[1mStep[0m  [64/84], [94mLoss[0m : 8.97361
[1mStep[0m  [72/84], [94mLoss[0m : 9.45554
[1mStep[0m  [80/84], [94mLoss[0m : 9.01511

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.615, [92mTest[0m: 9.931, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.47641
[1mStep[0m  [8/84], [94mLoss[0m : 8.79688
[1mStep[0m  [16/84], [94mLoss[0m : 9.03341
[1mStep[0m  [24/84], [94mLoss[0m : 8.51598
[1mStep[0m  [32/84], [94mLoss[0m : 8.23262
[1mStep[0m  [40/84], [94mLoss[0m : 8.25557
[1mStep[0m  [48/84], [94mLoss[0m : 8.17271
[1mStep[0m  [56/84], [94mLoss[0m : 7.83892
[1mStep[0m  [64/84], [94mLoss[0m : 7.83829
[1mStep[0m  [72/84], [94mLoss[0m : 7.78652
[1mStep[0m  [80/84], [94mLoss[0m : 7.43398

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 8.313, [92mTest[0m: 8.684, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.54857
[1mStep[0m  [8/84], [94mLoss[0m : 6.93464
[1mStep[0m  [16/84], [94mLoss[0m : 6.89075
[1mStep[0m  [24/84], [94mLoss[0m : 7.10946
[1mStep[0m  [32/84], [94mLoss[0m : 6.95000
[1mStep[0m  [40/84], [94mLoss[0m : 6.88303
[1mStep[0m  [48/84], [94mLoss[0m : 6.67808
[1mStep[0m  [56/84], [94mLoss[0m : 6.74376
[1mStep[0m  [64/84], [94mLoss[0m : 5.84571
[1mStep[0m  [72/84], [94mLoss[0m : 6.03068
[1mStep[0m  [80/84], [94mLoss[0m : 5.83045

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 6.589, [92mTest[0m: 6.622, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.56278
[1mStep[0m  [8/84], [94mLoss[0m : 5.25482
[1mStep[0m  [16/84], [94mLoss[0m : 5.79487
[1mStep[0m  [24/84], [94mLoss[0m : 5.17394
[1mStep[0m  [32/84], [94mLoss[0m : 5.41332
[1mStep[0m  [40/84], [94mLoss[0m : 4.93751
[1mStep[0m  [48/84], [94mLoss[0m : 4.55185
[1mStep[0m  [56/84], [94mLoss[0m : 4.59172
[1mStep[0m  [64/84], [94mLoss[0m : 4.30602
[1mStep[0m  [72/84], [94mLoss[0m : 3.68673
[1mStep[0m  [80/84], [94mLoss[0m : 3.32011

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 4.725, [92mTest[0m: 4.737, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.65446
[1mStep[0m  [8/84], [94mLoss[0m : 3.80723
[1mStep[0m  [16/84], [94mLoss[0m : 3.47830
[1mStep[0m  [24/84], [94mLoss[0m : 3.22061
[1mStep[0m  [32/84], [94mLoss[0m : 3.27235
[1mStep[0m  [40/84], [94mLoss[0m : 3.34278
[1mStep[0m  [48/84], [94mLoss[0m : 3.32481
[1mStep[0m  [56/84], [94mLoss[0m : 2.92846
[1mStep[0m  [64/84], [94mLoss[0m : 3.05076
[1mStep[0m  [72/84], [94mLoss[0m : 3.28237
[1mStep[0m  [80/84], [94mLoss[0m : 2.88808

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 3.206, [92mTest[0m: 3.025, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.16263
[1mStep[0m  [8/84], [94mLoss[0m : 3.09898
[1mStep[0m  [16/84], [94mLoss[0m : 3.04084
[1mStep[0m  [24/84], [94mLoss[0m : 2.75941
[1mStep[0m  [32/84], [94mLoss[0m : 2.70541
[1mStep[0m  [40/84], [94mLoss[0m : 2.46452
[1mStep[0m  [48/84], [94mLoss[0m : 2.77455
[1mStep[0m  [56/84], [94mLoss[0m : 2.61300
[1mStep[0m  [64/84], [94mLoss[0m : 2.70130
[1mStep[0m  [72/84], [94mLoss[0m : 2.70106
[1mStep[0m  [80/84], [94mLoss[0m : 2.73778

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.766, [92mTest[0m: 2.400, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.85174
[1mStep[0m  [8/84], [94mLoss[0m : 2.46989
[1mStep[0m  [16/84], [94mLoss[0m : 2.78235
[1mStep[0m  [24/84], [94mLoss[0m : 2.70704
[1mStep[0m  [32/84], [94mLoss[0m : 2.63459
[1mStep[0m  [40/84], [94mLoss[0m : 2.90365
[1mStep[0m  [48/84], [94mLoss[0m : 2.95527
[1mStep[0m  [56/84], [94mLoss[0m : 2.63173
[1mStep[0m  [64/84], [94mLoss[0m : 2.63812
[1mStep[0m  [72/84], [94mLoss[0m : 2.77938
[1mStep[0m  [80/84], [94mLoss[0m : 2.86684

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.729, [92mTest[0m: 2.388, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35436
[1mStep[0m  [8/84], [94mLoss[0m : 2.46505
[1mStep[0m  [16/84], [94mLoss[0m : 2.88456
[1mStep[0m  [24/84], [94mLoss[0m : 2.76791
[1mStep[0m  [32/84], [94mLoss[0m : 2.48415
[1mStep[0m  [40/84], [94mLoss[0m : 2.58610
[1mStep[0m  [48/84], [94mLoss[0m : 2.52574
[1mStep[0m  [56/84], [94mLoss[0m : 2.48170
[1mStep[0m  [64/84], [94mLoss[0m : 2.44846
[1mStep[0m  [72/84], [94mLoss[0m : 2.64101
[1mStep[0m  [80/84], [94mLoss[0m : 2.53869

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.686, [92mTest[0m: 2.375, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55122
[1mStep[0m  [8/84], [94mLoss[0m : 2.61196
[1mStep[0m  [16/84], [94mLoss[0m : 2.45723
[1mStep[0m  [24/84], [94mLoss[0m : 2.66901
[1mStep[0m  [32/84], [94mLoss[0m : 2.76586
[1mStep[0m  [40/84], [94mLoss[0m : 2.86292
[1mStep[0m  [48/84], [94mLoss[0m : 2.60370
[1mStep[0m  [56/84], [94mLoss[0m : 2.43254
[1mStep[0m  [64/84], [94mLoss[0m : 3.01104
[1mStep[0m  [72/84], [94mLoss[0m : 2.79170
[1mStep[0m  [80/84], [94mLoss[0m : 2.77659

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.648, [92mTest[0m: 2.374, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.06614
[1mStep[0m  [8/84], [94mLoss[0m : 2.86888
[1mStep[0m  [16/84], [94mLoss[0m : 2.95637
[1mStep[0m  [24/84], [94mLoss[0m : 2.84862
[1mStep[0m  [32/84], [94mLoss[0m : 2.83631
[1mStep[0m  [40/84], [94mLoss[0m : 2.57036
[1mStep[0m  [48/84], [94mLoss[0m : 2.96501
[1mStep[0m  [56/84], [94mLoss[0m : 2.45605
[1mStep[0m  [64/84], [94mLoss[0m : 2.72644
[1mStep[0m  [72/84], [94mLoss[0m : 2.80949
[1mStep[0m  [80/84], [94mLoss[0m : 2.47588

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.653, [92mTest[0m: 2.383, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.69667
[1mStep[0m  [8/84], [94mLoss[0m : 2.90201
[1mStep[0m  [16/84], [94mLoss[0m : 2.61418
[1mStep[0m  [24/84], [94mLoss[0m : 2.60282
[1mStep[0m  [32/84], [94mLoss[0m : 2.81304
[1mStep[0m  [40/84], [94mLoss[0m : 2.95956
[1mStep[0m  [48/84], [94mLoss[0m : 2.88801
[1mStep[0m  [56/84], [94mLoss[0m : 2.81767
[1mStep[0m  [64/84], [94mLoss[0m : 2.37548
[1mStep[0m  [72/84], [94mLoss[0m : 2.58576
[1mStep[0m  [80/84], [94mLoss[0m : 2.45823

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.626, [92mTest[0m: 2.360, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.80233
[1mStep[0m  [8/84], [94mLoss[0m : 2.43089
[1mStep[0m  [16/84], [94mLoss[0m : 2.83677
[1mStep[0m  [24/84], [94mLoss[0m : 2.51651
[1mStep[0m  [32/84], [94mLoss[0m : 2.71403
[1mStep[0m  [40/84], [94mLoss[0m : 2.11318
[1mStep[0m  [48/84], [94mLoss[0m : 2.31354
[1mStep[0m  [56/84], [94mLoss[0m : 2.49237
[1mStep[0m  [64/84], [94mLoss[0m : 2.73467
[1mStep[0m  [72/84], [94mLoss[0m : 2.72413
[1mStep[0m  [80/84], [94mLoss[0m : 2.84514

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.624, [92mTest[0m: 2.369, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.88092
[1mStep[0m  [8/84], [94mLoss[0m : 2.43846
[1mStep[0m  [16/84], [94mLoss[0m : 2.18914
[1mStep[0m  [24/84], [94mLoss[0m : 2.50332
[1mStep[0m  [32/84], [94mLoss[0m : 2.39862
[1mStep[0m  [40/84], [94mLoss[0m : 2.27047
[1mStep[0m  [48/84], [94mLoss[0m : 2.19589
[1mStep[0m  [56/84], [94mLoss[0m : 2.81655
[1mStep[0m  [64/84], [94mLoss[0m : 2.52827
[1mStep[0m  [72/84], [94mLoss[0m : 2.60735
[1mStep[0m  [80/84], [94mLoss[0m : 2.65154

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.610, [92mTest[0m: 2.339, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54326
[1mStep[0m  [8/84], [94mLoss[0m : 2.47142
[1mStep[0m  [16/84], [94mLoss[0m : 2.88334
[1mStep[0m  [24/84], [94mLoss[0m : 2.58609
[1mStep[0m  [32/84], [94mLoss[0m : 2.64212
[1mStep[0m  [40/84], [94mLoss[0m : 2.86171
[1mStep[0m  [48/84], [94mLoss[0m : 2.31765
[1mStep[0m  [56/84], [94mLoss[0m : 2.77109
[1mStep[0m  [64/84], [94mLoss[0m : 2.57195
[1mStep[0m  [72/84], [94mLoss[0m : 2.85302
[1mStep[0m  [80/84], [94mLoss[0m : 2.43220

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.612, [92mTest[0m: 2.354, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.69748
[1mStep[0m  [8/84], [94mLoss[0m : 2.61288
[1mStep[0m  [16/84], [94mLoss[0m : 2.76606
[1mStep[0m  [24/84], [94mLoss[0m : 2.55225
[1mStep[0m  [32/84], [94mLoss[0m : 2.53705
[1mStep[0m  [40/84], [94mLoss[0m : 2.68483
[1mStep[0m  [48/84], [94mLoss[0m : 2.51935
[1mStep[0m  [56/84], [94mLoss[0m : 2.82957
[1mStep[0m  [64/84], [94mLoss[0m : 2.57421
[1mStep[0m  [72/84], [94mLoss[0m : 2.66396
[1mStep[0m  [80/84], [94mLoss[0m : 2.50848

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.592, [92mTest[0m: 2.358, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30696
[1mStep[0m  [8/84], [94mLoss[0m : 2.60488
[1mStep[0m  [16/84], [94mLoss[0m : 2.55626
[1mStep[0m  [24/84], [94mLoss[0m : 2.70896
[1mStep[0m  [32/84], [94mLoss[0m : 2.55079
[1mStep[0m  [40/84], [94mLoss[0m : 2.49646
[1mStep[0m  [48/84], [94mLoss[0m : 2.80449
[1mStep[0m  [56/84], [94mLoss[0m : 2.71122
[1mStep[0m  [64/84], [94mLoss[0m : 2.77269
[1mStep[0m  [72/84], [94mLoss[0m : 2.70116
[1mStep[0m  [80/84], [94mLoss[0m : 2.58450

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.586, [92mTest[0m: 2.339, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35071
[1mStep[0m  [8/84], [94mLoss[0m : 2.81192
[1mStep[0m  [16/84], [94mLoss[0m : 2.44221
[1mStep[0m  [24/84], [94mLoss[0m : 2.54397
[1mStep[0m  [32/84], [94mLoss[0m : 2.43261
[1mStep[0m  [40/84], [94mLoss[0m : 2.67851
[1mStep[0m  [48/84], [94mLoss[0m : 2.65114
[1mStep[0m  [56/84], [94mLoss[0m : 2.41009
[1mStep[0m  [64/84], [94mLoss[0m : 2.52975
[1mStep[0m  [72/84], [94mLoss[0m : 2.54600
[1mStep[0m  [80/84], [94mLoss[0m : 2.42314

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.574, [92mTest[0m: 2.344, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.62042
[1mStep[0m  [8/84], [94mLoss[0m : 2.72736
[1mStep[0m  [16/84], [94mLoss[0m : 2.53874
[1mStep[0m  [24/84], [94mLoss[0m : 2.43937
[1mStep[0m  [32/84], [94mLoss[0m : 2.45651
[1mStep[0m  [40/84], [94mLoss[0m : 2.79308
[1mStep[0m  [48/84], [94mLoss[0m : 2.23014
[1mStep[0m  [56/84], [94mLoss[0m : 2.54891
[1mStep[0m  [64/84], [94mLoss[0m : 2.76609
[1mStep[0m  [72/84], [94mLoss[0m : 2.50683
[1mStep[0m  [80/84], [94mLoss[0m : 2.72498

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.562, [92mTest[0m: 2.360, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52026
[1mStep[0m  [8/84], [94mLoss[0m : 2.75166
[1mStep[0m  [16/84], [94mLoss[0m : 2.50627
[1mStep[0m  [24/84], [94mLoss[0m : 2.36836
[1mStep[0m  [32/84], [94mLoss[0m : 2.58402
[1mStep[0m  [40/84], [94mLoss[0m : 2.60964
[1mStep[0m  [48/84], [94mLoss[0m : 2.58877
[1mStep[0m  [56/84], [94mLoss[0m : 2.96205
[1mStep[0m  [64/84], [94mLoss[0m : 2.64013
[1mStep[0m  [72/84], [94mLoss[0m : 2.58594
[1mStep[0m  [80/84], [94mLoss[0m : 2.35152

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.557, [92mTest[0m: 2.336, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44344
[1mStep[0m  [8/84], [94mLoss[0m : 2.45595
[1mStep[0m  [16/84], [94mLoss[0m : 2.62984
[1mStep[0m  [24/84], [94mLoss[0m : 2.28013
[1mStep[0m  [32/84], [94mLoss[0m : 2.70048
[1mStep[0m  [40/84], [94mLoss[0m : 2.64179
[1mStep[0m  [48/84], [94mLoss[0m : 2.50937
[1mStep[0m  [56/84], [94mLoss[0m : 2.47538
[1mStep[0m  [64/84], [94mLoss[0m : 2.68309
[1mStep[0m  [72/84], [94mLoss[0m : 2.62599
[1mStep[0m  [80/84], [94mLoss[0m : 2.80478

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.555, [92mTest[0m: 2.346, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41915
[1mStep[0m  [8/84], [94mLoss[0m : 2.74131
[1mStep[0m  [16/84], [94mLoss[0m : 2.66043
[1mStep[0m  [24/84], [94mLoss[0m : 2.89699
[1mStep[0m  [32/84], [94mLoss[0m : 2.60448
[1mStep[0m  [40/84], [94mLoss[0m : 2.31784
[1mStep[0m  [48/84], [94mLoss[0m : 2.73750
[1mStep[0m  [56/84], [94mLoss[0m : 2.58840
[1mStep[0m  [64/84], [94mLoss[0m : 2.87321
[1mStep[0m  [72/84], [94mLoss[0m : 2.65294
[1mStep[0m  [80/84], [94mLoss[0m : 2.49648

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.529, [92mTest[0m: 2.351, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46811
[1mStep[0m  [8/84], [94mLoss[0m : 2.33706
[1mStep[0m  [16/84], [94mLoss[0m : 2.66621
[1mStep[0m  [24/84], [94mLoss[0m : 2.45175
[1mStep[0m  [32/84], [94mLoss[0m : 2.17785
[1mStep[0m  [40/84], [94mLoss[0m : 2.68184
[1mStep[0m  [48/84], [94mLoss[0m : 2.32489
[1mStep[0m  [56/84], [94mLoss[0m : 2.28870
[1mStep[0m  [64/84], [94mLoss[0m : 2.79075
[1mStep[0m  [72/84], [94mLoss[0m : 2.30041
[1mStep[0m  [80/84], [94mLoss[0m : 2.40315

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.535, [92mTest[0m: 2.354, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.56035
[1mStep[0m  [8/84], [94mLoss[0m : 2.72875
[1mStep[0m  [16/84], [94mLoss[0m : 2.48800
[1mStep[0m  [24/84], [94mLoss[0m : 2.63455
[1mStep[0m  [32/84], [94mLoss[0m : 2.64747
[1mStep[0m  [40/84], [94mLoss[0m : 2.57717
[1mStep[0m  [48/84], [94mLoss[0m : 2.60585
[1mStep[0m  [56/84], [94mLoss[0m : 2.84689
[1mStep[0m  [64/84], [94mLoss[0m : 2.53573
[1mStep[0m  [72/84], [94mLoss[0m : 2.52875
[1mStep[0m  [80/84], [94mLoss[0m : 2.66552

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.537, [92mTest[0m: 2.343, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45979
[1mStep[0m  [8/84], [94mLoss[0m : 2.66124
[1mStep[0m  [16/84], [94mLoss[0m : 2.32094
[1mStep[0m  [24/84], [94mLoss[0m : 2.69499
[1mStep[0m  [32/84], [94mLoss[0m : 2.62657
[1mStep[0m  [40/84], [94mLoss[0m : 2.75965
[1mStep[0m  [48/84], [94mLoss[0m : 2.75629
[1mStep[0m  [56/84], [94mLoss[0m : 2.38617
[1mStep[0m  [64/84], [94mLoss[0m : 2.46997
[1mStep[0m  [72/84], [94mLoss[0m : 2.62832
[1mStep[0m  [80/84], [94mLoss[0m : 2.70271

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.553, [92mTest[0m: 2.342, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.93921
[1mStep[0m  [8/84], [94mLoss[0m : 2.44975
[1mStep[0m  [16/84], [94mLoss[0m : 2.47816
[1mStep[0m  [24/84], [94mLoss[0m : 2.45558
[1mStep[0m  [32/84], [94mLoss[0m : 2.50963
[1mStep[0m  [40/84], [94mLoss[0m : 2.85269
[1mStep[0m  [48/84], [94mLoss[0m : 2.65814
[1mStep[0m  [56/84], [94mLoss[0m : 2.20195
[1mStep[0m  [64/84], [94mLoss[0m : 2.40565
[1mStep[0m  [72/84], [94mLoss[0m : 2.46233
[1mStep[0m  [80/84], [94mLoss[0m : 2.49725

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.339, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45609
[1mStep[0m  [8/84], [94mLoss[0m : 2.57513
[1mStep[0m  [16/84], [94mLoss[0m : 2.59997
[1mStep[0m  [24/84], [94mLoss[0m : 2.54748
[1mStep[0m  [32/84], [94mLoss[0m : 2.47476
[1mStep[0m  [40/84], [94mLoss[0m : 2.53761
[1mStep[0m  [48/84], [94mLoss[0m : 2.53921
[1mStep[0m  [56/84], [94mLoss[0m : 2.37325
[1mStep[0m  [64/84], [94mLoss[0m : 2.77512
[1mStep[0m  [72/84], [94mLoss[0m : 2.82890
[1mStep[0m  [80/84], [94mLoss[0m : 2.85173

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.524, [92mTest[0m: 2.331, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41239
[1mStep[0m  [8/84], [94mLoss[0m : 2.12627
[1mStep[0m  [16/84], [94mLoss[0m : 2.74246
[1mStep[0m  [24/84], [94mLoss[0m : 2.42440
[1mStep[0m  [32/84], [94mLoss[0m : 2.43339
[1mStep[0m  [40/84], [94mLoss[0m : 2.65443
[1mStep[0m  [48/84], [94mLoss[0m : 2.39603
[1mStep[0m  [56/84], [94mLoss[0m : 2.39582
[1mStep[0m  [64/84], [94mLoss[0m : 2.27140
[1mStep[0m  [72/84], [94mLoss[0m : 2.33959
[1mStep[0m  [80/84], [94mLoss[0m : 2.57529

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.340, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43384
[1mStep[0m  [8/84], [94mLoss[0m : 2.49728
[1mStep[0m  [16/84], [94mLoss[0m : 2.76867
[1mStep[0m  [24/84], [94mLoss[0m : 2.51306
[1mStep[0m  [32/84], [94mLoss[0m : 2.35981
[1mStep[0m  [40/84], [94mLoss[0m : 2.48567
[1mStep[0m  [48/84], [94mLoss[0m : 2.48046
[1mStep[0m  [56/84], [94mLoss[0m : 2.47097
[1mStep[0m  [64/84], [94mLoss[0m : 2.55221
[1mStep[0m  [72/84], [94mLoss[0m : 1.98065
[1mStep[0m  [80/84], [94mLoss[0m : 2.48703

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.332, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.25464
[1mStep[0m  [8/84], [94mLoss[0m : 2.44956
[1mStep[0m  [16/84], [94mLoss[0m : 2.36825
[1mStep[0m  [24/84], [94mLoss[0m : 2.19660
[1mStep[0m  [32/84], [94mLoss[0m : 2.35752
[1mStep[0m  [40/84], [94mLoss[0m : 2.76236
[1mStep[0m  [48/84], [94mLoss[0m : 2.38065
[1mStep[0m  [56/84], [94mLoss[0m : 2.55980
[1mStep[0m  [64/84], [94mLoss[0m : 2.69607
[1mStep[0m  [72/84], [94mLoss[0m : 2.35743
[1mStep[0m  [80/84], [94mLoss[0m : 2.57435

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.353, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.339
====================================

Phase 1 - Evaluation MAE:  2.338713688509805
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 2.46711
[1mStep[0m  [8/84], [94mLoss[0m : 2.44561
[1mStep[0m  [16/84], [94mLoss[0m : 2.23325
[1mStep[0m  [24/84], [94mLoss[0m : 2.48227
[1mStep[0m  [32/84], [94mLoss[0m : 2.62241
[1mStep[0m  [40/84], [94mLoss[0m : 2.88750
[1mStep[0m  [48/84], [94mLoss[0m : 2.61944
[1mStep[0m  [56/84], [94mLoss[0m : 2.16871
[1mStep[0m  [64/84], [94mLoss[0m : 2.17237
[1mStep[0m  [72/84], [94mLoss[0m : 2.50143
[1mStep[0m  [80/84], [94mLoss[0m : 2.62173

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.559, [92mTest[0m: 2.340, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59621
[1mStep[0m  [8/84], [94mLoss[0m : 2.38485
[1mStep[0m  [16/84], [94mLoss[0m : 2.61290
[1mStep[0m  [24/84], [94mLoss[0m : 2.25733
[1mStep[0m  [32/84], [94mLoss[0m : 2.46428
[1mStep[0m  [40/84], [94mLoss[0m : 2.39327
[1mStep[0m  [48/84], [94mLoss[0m : 2.84780
[1mStep[0m  [56/84], [94mLoss[0m : 2.74972
[1mStep[0m  [64/84], [94mLoss[0m : 2.45082
[1mStep[0m  [72/84], [94mLoss[0m : 2.19642
[1mStep[0m  [80/84], [94mLoss[0m : 2.56474

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.519, [92mTest[0m: 2.495, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.05706
[1mStep[0m  [8/84], [94mLoss[0m : 2.52754
[1mStep[0m  [16/84], [94mLoss[0m : 2.52554
[1mStep[0m  [24/84], [94mLoss[0m : 2.43891
[1mStep[0m  [32/84], [94mLoss[0m : 2.53756
[1mStep[0m  [40/84], [94mLoss[0m : 2.27196
[1mStep[0m  [48/84], [94mLoss[0m : 2.38821
[1mStep[0m  [56/84], [94mLoss[0m : 2.44595
[1mStep[0m  [64/84], [94mLoss[0m : 2.43330
[1mStep[0m  [72/84], [94mLoss[0m : 2.32613
[1mStep[0m  [80/84], [94mLoss[0m : 2.79934

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.539, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.32589
[1mStep[0m  [8/84], [94mLoss[0m : 2.60423
[1mStep[0m  [16/84], [94mLoss[0m : 2.41129
[1mStep[0m  [24/84], [94mLoss[0m : 2.75273
[1mStep[0m  [32/84], [94mLoss[0m : 2.29479
[1mStep[0m  [40/84], [94mLoss[0m : 2.49429
[1mStep[0m  [48/84], [94mLoss[0m : 2.51704
[1mStep[0m  [56/84], [94mLoss[0m : 2.60796
[1mStep[0m  [64/84], [94mLoss[0m : 2.60709
[1mStep[0m  [72/84], [94mLoss[0m : 2.28886
[1mStep[0m  [80/84], [94mLoss[0m : 2.35265

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.450, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.34041
[1mStep[0m  [8/84], [94mLoss[0m : 2.41389
[1mStep[0m  [16/84], [94mLoss[0m : 2.12020
[1mStep[0m  [24/84], [94mLoss[0m : 2.46241
[1mStep[0m  [32/84], [94mLoss[0m : 2.26796
[1mStep[0m  [40/84], [94mLoss[0m : 2.50748
[1mStep[0m  [48/84], [94mLoss[0m : 2.56777
[1mStep[0m  [56/84], [94mLoss[0m : 2.46362
[1mStep[0m  [64/84], [94mLoss[0m : 2.38877
[1mStep[0m  [72/84], [94mLoss[0m : 2.51051
[1mStep[0m  [80/84], [94mLoss[0m : 2.52742

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.384, [92mTest[0m: 2.443, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31324
[1mStep[0m  [8/84], [94mLoss[0m : 2.27061
[1mStep[0m  [16/84], [94mLoss[0m : 2.29787
[1mStep[0m  [24/84], [94mLoss[0m : 2.47041
[1mStep[0m  [32/84], [94mLoss[0m : 2.36476
[1mStep[0m  [40/84], [94mLoss[0m : 2.08971
[1mStep[0m  [48/84], [94mLoss[0m : 2.11077
[1mStep[0m  [56/84], [94mLoss[0m : 2.36720
[1mStep[0m  [64/84], [94mLoss[0m : 2.33875
[1mStep[0m  [72/84], [94mLoss[0m : 2.21230
[1mStep[0m  [80/84], [94mLoss[0m : 2.12425

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.334, [92mTest[0m: 2.424, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.14447
[1mStep[0m  [8/84], [94mLoss[0m : 2.25523
[1mStep[0m  [16/84], [94mLoss[0m : 2.17763
[1mStep[0m  [24/84], [94mLoss[0m : 2.03815
[1mStep[0m  [32/84], [94mLoss[0m : 2.37850
[1mStep[0m  [40/84], [94mLoss[0m : 2.08645
[1mStep[0m  [48/84], [94mLoss[0m : 2.36072
[1mStep[0m  [56/84], [94mLoss[0m : 2.32024
[1mStep[0m  [64/84], [94mLoss[0m : 2.26281
[1mStep[0m  [72/84], [94mLoss[0m : 2.04896
[1mStep[0m  [80/84], [94mLoss[0m : 2.12721

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.284, [92mTest[0m: 2.377, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.32197
[1mStep[0m  [8/84], [94mLoss[0m : 2.11288
[1mStep[0m  [16/84], [94mLoss[0m : 2.26143
[1mStep[0m  [24/84], [94mLoss[0m : 2.31182
[1mStep[0m  [32/84], [94mLoss[0m : 2.32120
[1mStep[0m  [40/84], [94mLoss[0m : 2.28928
[1mStep[0m  [48/84], [94mLoss[0m : 2.07767
[1mStep[0m  [56/84], [94mLoss[0m : 2.25157
[1mStep[0m  [64/84], [94mLoss[0m : 2.04894
[1mStep[0m  [72/84], [94mLoss[0m : 2.48665
[1mStep[0m  [80/84], [94mLoss[0m : 2.35861

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.263, [92mTest[0m: 2.402, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.02766
[1mStep[0m  [8/84], [94mLoss[0m : 2.24972
[1mStep[0m  [16/84], [94mLoss[0m : 2.61057
[1mStep[0m  [24/84], [94mLoss[0m : 2.23307
[1mStep[0m  [32/84], [94mLoss[0m : 2.26658
[1mStep[0m  [40/84], [94mLoss[0m : 2.21581
[1mStep[0m  [48/84], [94mLoss[0m : 2.20760
[1mStep[0m  [56/84], [94mLoss[0m : 2.17462
[1mStep[0m  [64/84], [94mLoss[0m : 2.17261
[1mStep[0m  [72/84], [94mLoss[0m : 2.32495
[1mStep[0m  [80/84], [94mLoss[0m : 2.30208

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.214, [92mTest[0m: 2.435, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.00425
[1mStep[0m  [8/84], [94mLoss[0m : 2.50192
[1mStep[0m  [16/84], [94mLoss[0m : 2.02035
[1mStep[0m  [24/84], [94mLoss[0m : 1.92803
[1mStep[0m  [32/84], [94mLoss[0m : 2.22024
[1mStep[0m  [40/84], [94mLoss[0m : 2.15185
[1mStep[0m  [48/84], [94mLoss[0m : 1.95200
[1mStep[0m  [56/84], [94mLoss[0m : 2.23868
[1mStep[0m  [64/84], [94mLoss[0m : 2.07257
[1mStep[0m  [72/84], [94mLoss[0m : 2.41948
[1mStep[0m  [80/84], [94mLoss[0m : 2.14196

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.177, [92mTest[0m: 2.434, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.06402
[1mStep[0m  [8/84], [94mLoss[0m : 1.90505
[1mStep[0m  [16/84], [94mLoss[0m : 2.15678
[1mStep[0m  [24/84], [94mLoss[0m : 2.12189
[1mStep[0m  [32/84], [94mLoss[0m : 2.06042
[1mStep[0m  [40/84], [94mLoss[0m : 2.22172
[1mStep[0m  [48/84], [94mLoss[0m : 1.86743
[1mStep[0m  [56/84], [94mLoss[0m : 2.14835
[1mStep[0m  [64/84], [94mLoss[0m : 2.11963
[1mStep[0m  [72/84], [94mLoss[0m : 1.99404
[1mStep[0m  [80/84], [94mLoss[0m : 2.00900

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.105, [92mTest[0m: 2.483, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.02424
[1mStep[0m  [8/84], [94mLoss[0m : 1.94688
[1mStep[0m  [16/84], [94mLoss[0m : 2.25180
[1mStep[0m  [24/84], [94mLoss[0m : 1.98063
[1mStep[0m  [32/84], [94mLoss[0m : 2.00736
[1mStep[0m  [40/84], [94mLoss[0m : 2.03177
[1mStep[0m  [48/84], [94mLoss[0m : 2.15992
[1mStep[0m  [56/84], [94mLoss[0m : 2.22098
[1mStep[0m  [64/84], [94mLoss[0m : 2.03642
[1mStep[0m  [72/84], [94mLoss[0m : 2.23698
[1mStep[0m  [80/84], [94mLoss[0m : 1.90377

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.105, [92mTest[0m: 2.497, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.01319
[1mStep[0m  [8/84], [94mLoss[0m : 1.90020
[1mStep[0m  [16/84], [94mLoss[0m : 1.93247
[1mStep[0m  [24/84], [94mLoss[0m : 1.95771
[1mStep[0m  [32/84], [94mLoss[0m : 1.98254
[1mStep[0m  [40/84], [94mLoss[0m : 2.05173
[1mStep[0m  [48/84], [94mLoss[0m : 2.16495
[1mStep[0m  [56/84], [94mLoss[0m : 2.23870
[1mStep[0m  [64/84], [94mLoss[0m : 2.00519
[1mStep[0m  [72/84], [94mLoss[0m : 2.27236
[1mStep[0m  [80/84], [94mLoss[0m : 2.12504

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.054, [92mTest[0m: 2.432, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.92087
[1mStep[0m  [8/84], [94mLoss[0m : 2.01116
[1mStep[0m  [16/84], [94mLoss[0m : 1.94257
[1mStep[0m  [24/84], [94mLoss[0m : 2.01115
[1mStep[0m  [32/84], [94mLoss[0m : 1.88985
[1mStep[0m  [40/84], [94mLoss[0m : 2.11391
[1mStep[0m  [48/84], [94mLoss[0m : 2.05273
[1mStep[0m  [56/84], [94mLoss[0m : 2.11191
[1mStep[0m  [64/84], [94mLoss[0m : 2.12025
[1mStep[0m  [72/84], [94mLoss[0m : 1.84278
[1mStep[0m  [80/84], [94mLoss[0m : 2.24423

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.005, [92mTest[0m: 2.471, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.91392
[1mStep[0m  [8/84], [94mLoss[0m : 1.70047
[1mStep[0m  [16/84], [94mLoss[0m : 1.98788
[1mStep[0m  [24/84], [94mLoss[0m : 1.87419
[1mStep[0m  [32/84], [94mLoss[0m : 2.08832
[1mStep[0m  [40/84], [94mLoss[0m : 2.13630
[1mStep[0m  [48/84], [94mLoss[0m : 1.94234
[1mStep[0m  [56/84], [94mLoss[0m : 1.87348
[1mStep[0m  [64/84], [94mLoss[0m : 1.95560
[1mStep[0m  [72/84], [94mLoss[0m : 2.07769
[1mStep[0m  [80/84], [94mLoss[0m : 2.19839

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.979, [92mTest[0m: 2.460, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.00304
[1mStep[0m  [8/84], [94mLoss[0m : 2.01881
[1mStep[0m  [16/84], [94mLoss[0m : 1.90374
[1mStep[0m  [24/84], [94mLoss[0m : 1.93572
[1mStep[0m  [32/84], [94mLoss[0m : 2.09446
[1mStep[0m  [40/84], [94mLoss[0m : 1.92631
[1mStep[0m  [48/84], [94mLoss[0m : 1.91407
[1mStep[0m  [56/84], [94mLoss[0m : 1.88741
[1mStep[0m  [64/84], [94mLoss[0m : 1.95477
[1mStep[0m  [72/84], [94mLoss[0m : 1.90425
[1mStep[0m  [80/84], [94mLoss[0m : 2.00974

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.946, [92mTest[0m: 2.474, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.01943
[1mStep[0m  [8/84], [94mLoss[0m : 1.82016
[1mStep[0m  [16/84], [94mLoss[0m : 1.51953
[1mStep[0m  [24/84], [94mLoss[0m : 1.79251
[1mStep[0m  [32/84], [94mLoss[0m : 1.85073
[1mStep[0m  [40/84], [94mLoss[0m : 1.65158
[1mStep[0m  [48/84], [94mLoss[0m : 1.75042
[1mStep[0m  [56/84], [94mLoss[0m : 2.16379
[1mStep[0m  [64/84], [94mLoss[0m : 1.72251
[1mStep[0m  [72/84], [94mLoss[0m : 1.93458
[1mStep[0m  [80/84], [94mLoss[0m : 2.25365

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.895, [92mTest[0m: 2.464, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.79391
[1mStep[0m  [8/84], [94mLoss[0m : 1.91490
[1mStep[0m  [16/84], [94mLoss[0m : 1.72396
[1mStep[0m  [24/84], [94mLoss[0m : 1.92212
[1mStep[0m  [32/84], [94mLoss[0m : 2.02713
[1mStep[0m  [40/84], [94mLoss[0m : 2.00955
[1mStep[0m  [48/84], [94mLoss[0m : 1.96482
[1mStep[0m  [56/84], [94mLoss[0m : 1.87422
[1mStep[0m  [64/84], [94mLoss[0m : 2.31148
[1mStep[0m  [72/84], [94mLoss[0m : 1.91571
[1mStep[0m  [80/84], [94mLoss[0m : 2.08954

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.853, [92mTest[0m: 2.492, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.87001
[1mStep[0m  [8/84], [94mLoss[0m : 1.96478
[1mStep[0m  [16/84], [94mLoss[0m : 2.09207
[1mStep[0m  [24/84], [94mLoss[0m : 1.77359
[1mStep[0m  [32/84], [94mLoss[0m : 1.89409
[1mStep[0m  [40/84], [94mLoss[0m : 1.88037
[1mStep[0m  [48/84], [94mLoss[0m : 1.75661
[1mStep[0m  [56/84], [94mLoss[0m : 1.57993
[1mStep[0m  [64/84], [94mLoss[0m : 1.86804
[1mStep[0m  [72/84], [94mLoss[0m : 2.04276
[1mStep[0m  [80/84], [94mLoss[0m : 2.02412

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.838, [92mTest[0m: 2.488, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.90510
[1mStep[0m  [8/84], [94mLoss[0m : 1.99070
[1mStep[0m  [16/84], [94mLoss[0m : 1.65366
[1mStep[0m  [24/84], [94mLoss[0m : 1.54445
[1mStep[0m  [32/84], [94mLoss[0m : 1.55729
[1mStep[0m  [40/84], [94mLoss[0m : 2.00236
[1mStep[0m  [48/84], [94mLoss[0m : 1.63034
[1mStep[0m  [56/84], [94mLoss[0m : 1.73705
[1mStep[0m  [64/84], [94mLoss[0m : 2.04702
[1mStep[0m  [72/84], [94mLoss[0m : 1.74508
[1mStep[0m  [80/84], [94mLoss[0m : 1.93719

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.793, [92mTest[0m: 2.524, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.82626
[1mStep[0m  [8/84], [94mLoss[0m : 1.67575
[1mStep[0m  [16/84], [94mLoss[0m : 1.80490
[1mStep[0m  [24/84], [94mLoss[0m : 1.72482
[1mStep[0m  [32/84], [94mLoss[0m : 1.76940
[1mStep[0m  [40/84], [94mLoss[0m : 1.69317
[1mStep[0m  [48/84], [94mLoss[0m : 1.53676
[1mStep[0m  [56/84], [94mLoss[0m : 1.83159
[1mStep[0m  [64/84], [94mLoss[0m : 2.16632
[1mStep[0m  [72/84], [94mLoss[0m : 1.71204
[1mStep[0m  [80/84], [94mLoss[0m : 2.06349

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.776, [92mTest[0m: 2.511, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.78718
[1mStep[0m  [8/84], [94mLoss[0m : 1.89590
[1mStep[0m  [16/84], [94mLoss[0m : 1.70661
[1mStep[0m  [24/84], [94mLoss[0m : 1.93463
[1mStep[0m  [32/84], [94mLoss[0m : 1.76550
[1mStep[0m  [40/84], [94mLoss[0m : 1.60892
[1mStep[0m  [48/84], [94mLoss[0m : 1.68329
[1mStep[0m  [56/84], [94mLoss[0m : 1.64546
[1mStep[0m  [64/84], [94mLoss[0m : 1.90338
[1mStep[0m  [72/84], [94mLoss[0m : 1.86260
[1mStep[0m  [80/84], [94mLoss[0m : 1.72818

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.762, [92mTest[0m: 2.488, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.59244
[1mStep[0m  [8/84], [94mLoss[0m : 1.82968
[1mStep[0m  [16/84], [94mLoss[0m : 1.80222
[1mStep[0m  [24/84], [94mLoss[0m : 1.57877
[1mStep[0m  [32/84], [94mLoss[0m : 1.58952
[1mStep[0m  [40/84], [94mLoss[0m : 1.52390
[1mStep[0m  [48/84], [94mLoss[0m : 1.78534
[1mStep[0m  [56/84], [94mLoss[0m : 1.71637
[1mStep[0m  [64/84], [94mLoss[0m : 1.76680
[1mStep[0m  [72/84], [94mLoss[0m : 1.78886
[1mStep[0m  [80/84], [94mLoss[0m : 1.60870

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.712, [92mTest[0m: 2.519, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.50950
[1mStep[0m  [8/84], [94mLoss[0m : 1.83943
[1mStep[0m  [16/84], [94mLoss[0m : 1.47643
[1mStep[0m  [24/84], [94mLoss[0m : 1.37252
[1mStep[0m  [32/84], [94mLoss[0m : 1.55412
[1mStep[0m  [40/84], [94mLoss[0m : 1.50927
[1mStep[0m  [48/84], [94mLoss[0m : 1.68507
[1mStep[0m  [56/84], [94mLoss[0m : 1.63886
[1mStep[0m  [64/84], [94mLoss[0m : 1.64570
[1mStep[0m  [72/84], [94mLoss[0m : 1.64654
[1mStep[0m  [80/84], [94mLoss[0m : 1.58208

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.681, [92mTest[0m: 2.522, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.65710
[1mStep[0m  [8/84], [94mLoss[0m : 1.51034
[1mStep[0m  [16/84], [94mLoss[0m : 2.00668
[1mStep[0m  [24/84], [94mLoss[0m : 1.72764
[1mStep[0m  [32/84], [94mLoss[0m : 1.79237
[1mStep[0m  [40/84], [94mLoss[0m : 1.65487
[1mStep[0m  [48/84], [94mLoss[0m : 1.70532
[1mStep[0m  [56/84], [94mLoss[0m : 1.61444
[1mStep[0m  [64/84], [94mLoss[0m : 1.63230
[1mStep[0m  [72/84], [94mLoss[0m : 1.62979
[1mStep[0m  [80/84], [94mLoss[0m : 1.51325

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.639, [92mTest[0m: 2.509, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.68819
[1mStep[0m  [8/84], [94mLoss[0m : 1.64163
[1mStep[0m  [16/84], [94mLoss[0m : 1.52488
[1mStep[0m  [24/84], [94mLoss[0m : 1.59870
[1mStep[0m  [32/84], [94mLoss[0m : 1.61595
[1mStep[0m  [40/84], [94mLoss[0m : 1.61461
[1mStep[0m  [48/84], [94mLoss[0m : 1.41261
[1mStep[0m  [56/84], [94mLoss[0m : 1.56761
[1mStep[0m  [64/84], [94mLoss[0m : 1.65245
[1mStep[0m  [72/84], [94mLoss[0m : 1.77297
[1mStep[0m  [80/84], [94mLoss[0m : 1.75204

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.642, [92mTest[0m: 2.485, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.32731
[1mStep[0m  [8/84], [94mLoss[0m : 1.37621
[1mStep[0m  [16/84], [94mLoss[0m : 2.00316
[1mStep[0m  [24/84], [94mLoss[0m : 1.60439
[1mStep[0m  [32/84], [94mLoss[0m : 1.52875
[1mStep[0m  [40/84], [94mLoss[0m : 1.77358
[1mStep[0m  [48/84], [94mLoss[0m : 1.76504
[1mStep[0m  [56/84], [94mLoss[0m : 1.86339
[1mStep[0m  [64/84], [94mLoss[0m : 1.53389
[1mStep[0m  [72/84], [94mLoss[0m : 1.66323
[1mStep[0m  [80/84], [94mLoss[0m : 1.89254

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.621, [92mTest[0m: 2.489, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.60174
[1mStep[0m  [8/84], [94mLoss[0m : 1.79417
[1mStep[0m  [16/84], [94mLoss[0m : 1.62722
[1mStep[0m  [24/84], [94mLoss[0m : 1.74579
[1mStep[0m  [32/84], [94mLoss[0m : 1.73662
[1mStep[0m  [40/84], [94mLoss[0m : 1.53321
[1mStep[0m  [48/84], [94mLoss[0m : 1.65086
[1mStep[0m  [56/84], [94mLoss[0m : 1.83752
[1mStep[0m  [64/84], [94mLoss[0m : 1.45192
[1mStep[0m  [72/84], [94mLoss[0m : 1.50654
[1mStep[0m  [80/84], [94mLoss[0m : 1.54366

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.587, [92mTest[0m: 2.497, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.55100
[1mStep[0m  [8/84], [94mLoss[0m : 1.47809
[1mStep[0m  [16/84], [94mLoss[0m : 1.36773
[1mStep[0m  [24/84], [94mLoss[0m : 1.53885
[1mStep[0m  [32/84], [94mLoss[0m : 1.57971
[1mStep[0m  [40/84], [94mLoss[0m : 1.72292
[1mStep[0m  [48/84], [94mLoss[0m : 1.61719
[1mStep[0m  [56/84], [94mLoss[0m : 1.55982
[1mStep[0m  [64/84], [94mLoss[0m : 1.69169
[1mStep[0m  [72/84], [94mLoss[0m : 1.36064
[1mStep[0m  [80/84], [94mLoss[0m : 1.33886

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.579, [92mTest[0m: 2.523, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.84115
[1mStep[0m  [8/84], [94mLoss[0m : 1.63558
[1mStep[0m  [16/84], [94mLoss[0m : 1.49817
[1mStep[0m  [24/84], [94mLoss[0m : 1.76090
[1mStep[0m  [32/84], [94mLoss[0m : 1.73749
[1mStep[0m  [40/84], [94mLoss[0m : 1.39688
[1mStep[0m  [48/84], [94mLoss[0m : 1.69879
[1mStep[0m  [56/84], [94mLoss[0m : 1.49398
[1mStep[0m  [64/84], [94mLoss[0m : 1.38070
[1mStep[0m  [72/84], [94mLoss[0m : 1.57036
[1mStep[0m  [80/84], [94mLoss[0m : 1.49887

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.558, [92mTest[0m: 2.556, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.538
====================================

Phase 2 - Evaluation MAE:  2.5377405967031206
MAE score P1        2.338714
MAE score P2        2.537741
loss                1.557807
learning_rate       0.007525
batch_size               128
hidden_sizes      [300, 100]
epochs                    30
activation              tanh
optimizer                sgd
early stopping         False
dropout                  0.3
momentum                 0.1
weight_decay          0.0001
Name: 27, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 11.07032
[1mStep[0m  [8/84], [94mLoss[0m : 8.28213
[1mStep[0m  [16/84], [94mLoss[0m : 5.03436
[1mStep[0m  [24/84], [94mLoss[0m : 3.52647
[1mStep[0m  [32/84], [94mLoss[0m : 2.74863
[1mStep[0m  [40/84], [94mLoss[0m : 2.57562
[1mStep[0m  [48/84], [94mLoss[0m : 2.44939
[1mStep[0m  [56/84], [94mLoss[0m : 2.76561
[1mStep[0m  [64/84], [94mLoss[0m : 2.70323
[1mStep[0m  [72/84], [94mLoss[0m : 2.45799
[1mStep[0m  [80/84], [94mLoss[0m : 2.55076

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.912, [92mTest[0m: 11.036, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.51200
[1mStep[0m  [8/84], [94mLoss[0m : 2.35641
[1mStep[0m  [16/84], [94mLoss[0m : 2.27775
[1mStep[0m  [24/84], [94mLoss[0m : 2.48240
[1mStep[0m  [32/84], [94mLoss[0m : 2.95692
[1mStep[0m  [40/84], [94mLoss[0m : 2.44436
[1mStep[0m  [48/84], [94mLoss[0m : 2.37041
[1mStep[0m  [56/84], [94mLoss[0m : 2.75970
[1mStep[0m  [64/84], [94mLoss[0m : 2.48338
[1mStep[0m  [72/84], [94mLoss[0m : 2.35298
[1mStep[0m  [80/84], [94mLoss[0m : 2.59425

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.408, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.12095
[1mStep[0m  [8/84], [94mLoss[0m : 2.45493
[1mStep[0m  [16/84], [94mLoss[0m : 2.14984
[1mStep[0m  [24/84], [94mLoss[0m : 2.35268
[1mStep[0m  [32/84], [94mLoss[0m : 2.62211
[1mStep[0m  [40/84], [94mLoss[0m : 2.34147
[1mStep[0m  [48/84], [94mLoss[0m : 2.40251
[1mStep[0m  [56/84], [94mLoss[0m : 2.49066
[1mStep[0m  [64/84], [94mLoss[0m : 2.63047
[1mStep[0m  [72/84], [94mLoss[0m : 2.44198
[1mStep[0m  [80/84], [94mLoss[0m : 2.42278

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.364, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36842
[1mStep[0m  [8/84], [94mLoss[0m : 2.67014
[1mStep[0m  [16/84], [94mLoss[0m : 2.36919
[1mStep[0m  [24/84], [94mLoss[0m : 2.30355
[1mStep[0m  [32/84], [94mLoss[0m : 2.72918
[1mStep[0m  [40/84], [94mLoss[0m : 2.59951
[1mStep[0m  [48/84], [94mLoss[0m : 2.47044
[1mStep[0m  [56/84], [94mLoss[0m : 2.51113
[1mStep[0m  [64/84], [94mLoss[0m : 2.36156
[1mStep[0m  [72/84], [94mLoss[0m : 2.29237
[1mStep[0m  [80/84], [94mLoss[0m : 2.54715

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.342, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.29351
[1mStep[0m  [8/84], [94mLoss[0m : 2.41330
[1mStep[0m  [16/84], [94mLoss[0m : 2.35248
[1mStep[0m  [24/84], [94mLoss[0m : 2.33310
[1mStep[0m  [32/84], [94mLoss[0m : 2.39763
[1mStep[0m  [40/84], [94mLoss[0m : 2.51242
[1mStep[0m  [48/84], [94mLoss[0m : 2.52284
[1mStep[0m  [56/84], [94mLoss[0m : 2.42718
[1mStep[0m  [64/84], [94mLoss[0m : 2.50605
[1mStep[0m  [72/84], [94mLoss[0m : 2.79242
[1mStep[0m  [80/84], [94mLoss[0m : 2.42025

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.336, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.13560
[1mStep[0m  [8/84], [94mLoss[0m : 2.20765
[1mStep[0m  [16/84], [94mLoss[0m : 2.20289
[1mStep[0m  [24/84], [94mLoss[0m : 2.50894
[1mStep[0m  [32/84], [94mLoss[0m : 2.18499
[1mStep[0m  [40/84], [94mLoss[0m : 2.43608
[1mStep[0m  [48/84], [94mLoss[0m : 2.45297
[1mStep[0m  [56/84], [94mLoss[0m : 2.35077
[1mStep[0m  [64/84], [94mLoss[0m : 2.74188
[1mStep[0m  [72/84], [94mLoss[0m : 2.29322
[1mStep[0m  [80/84], [94mLoss[0m : 2.60728

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.330, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53059
[1mStep[0m  [8/84], [94mLoss[0m : 2.04177
[1mStep[0m  [16/84], [94mLoss[0m : 2.23259
[1mStep[0m  [24/84], [94mLoss[0m : 2.74868
[1mStep[0m  [32/84], [94mLoss[0m : 2.55558
[1mStep[0m  [40/84], [94mLoss[0m : 2.59983
[1mStep[0m  [48/84], [94mLoss[0m : 2.25476
[1mStep[0m  [56/84], [94mLoss[0m : 2.56767
[1mStep[0m  [64/84], [94mLoss[0m : 2.25849
[1mStep[0m  [72/84], [94mLoss[0m : 2.24545
[1mStep[0m  [80/84], [94mLoss[0m : 2.53304

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.334, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.32876
[1mStep[0m  [8/84], [94mLoss[0m : 2.47065
[1mStep[0m  [16/84], [94mLoss[0m : 2.37330
[1mStep[0m  [24/84], [94mLoss[0m : 2.41355
[1mStep[0m  [32/84], [94mLoss[0m : 2.54918
[1mStep[0m  [40/84], [94mLoss[0m : 2.43839
[1mStep[0m  [48/84], [94mLoss[0m : 2.31233
[1mStep[0m  [56/84], [94mLoss[0m : 2.09068
[1mStep[0m  [64/84], [94mLoss[0m : 2.08105
[1mStep[0m  [72/84], [94mLoss[0m : 2.38487
[1mStep[0m  [80/84], [94mLoss[0m : 2.52359

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.337, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33616
[1mStep[0m  [8/84], [94mLoss[0m : 2.49845
[1mStep[0m  [16/84], [94mLoss[0m : 2.63332
[1mStep[0m  [24/84], [94mLoss[0m : 2.78558
[1mStep[0m  [32/84], [94mLoss[0m : 2.35585
[1mStep[0m  [40/84], [94mLoss[0m : 2.25897
[1mStep[0m  [48/84], [94mLoss[0m : 2.49744
[1mStep[0m  [56/84], [94mLoss[0m : 2.80153
[1mStep[0m  [64/84], [94mLoss[0m : 2.64991
[1mStep[0m  [72/84], [94mLoss[0m : 2.52955
[1mStep[0m  [80/84], [94mLoss[0m : 2.45917

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.329, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54591
[1mStep[0m  [8/84], [94mLoss[0m : 2.59842
[1mStep[0m  [16/84], [94mLoss[0m : 2.37595
[1mStep[0m  [24/84], [94mLoss[0m : 2.47148
[1mStep[0m  [32/84], [94mLoss[0m : 2.43890
[1mStep[0m  [40/84], [94mLoss[0m : 2.52557
[1mStep[0m  [48/84], [94mLoss[0m : 2.39546
[1mStep[0m  [56/84], [94mLoss[0m : 2.38783
[1mStep[0m  [64/84], [94mLoss[0m : 2.60805
[1mStep[0m  [72/84], [94mLoss[0m : 2.30993
[1mStep[0m  [80/84], [94mLoss[0m : 2.42899

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.331, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42248
[1mStep[0m  [8/84], [94mLoss[0m : 2.66687
[1mStep[0m  [16/84], [94mLoss[0m : 2.37410
[1mStep[0m  [24/84], [94mLoss[0m : 2.71921
[1mStep[0m  [32/84], [94mLoss[0m : 2.63763
[1mStep[0m  [40/84], [94mLoss[0m : 2.52415
[1mStep[0m  [48/84], [94mLoss[0m : 2.48473
[1mStep[0m  [56/84], [94mLoss[0m : 2.38177
[1mStep[0m  [64/84], [94mLoss[0m : 2.37891
[1mStep[0m  [72/84], [94mLoss[0m : 2.41213
[1mStep[0m  [80/84], [94mLoss[0m : 2.84258

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.337, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.28745
[1mStep[0m  [8/84], [94mLoss[0m : 2.51649
[1mStep[0m  [16/84], [94mLoss[0m : 2.45700
[1mStep[0m  [24/84], [94mLoss[0m : 2.35328
[1mStep[0m  [32/84], [94mLoss[0m : 2.17504
[1mStep[0m  [40/84], [94mLoss[0m : 2.51657
[1mStep[0m  [48/84], [94mLoss[0m : 2.53347
[1mStep[0m  [56/84], [94mLoss[0m : 2.16218
[1mStep[0m  [64/84], [94mLoss[0m : 2.26605
[1mStep[0m  [72/84], [94mLoss[0m : 2.03825
[1mStep[0m  [80/84], [94mLoss[0m : 2.54450

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.328, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45784
[1mStep[0m  [8/84], [94mLoss[0m : 2.63863
[1mStep[0m  [16/84], [94mLoss[0m : 2.22007
[1mStep[0m  [24/84], [94mLoss[0m : 2.48019
[1mStep[0m  [32/84], [94mLoss[0m : 2.49376
[1mStep[0m  [40/84], [94mLoss[0m : 2.73236
[1mStep[0m  [48/84], [94mLoss[0m : 2.45349
[1mStep[0m  [56/84], [94mLoss[0m : 2.54341
[1mStep[0m  [64/84], [94mLoss[0m : 2.48798
[1mStep[0m  [72/84], [94mLoss[0m : 2.72740
[1mStep[0m  [80/84], [94mLoss[0m : 2.48353

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.330, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.80667
[1mStep[0m  [8/84], [94mLoss[0m : 2.58342
[1mStep[0m  [16/84], [94mLoss[0m : 2.86441
[1mStep[0m  [24/84], [94mLoss[0m : 2.42394
[1mStep[0m  [32/84], [94mLoss[0m : 2.49640
[1mStep[0m  [40/84], [94mLoss[0m : 2.92527
[1mStep[0m  [48/84], [94mLoss[0m : 2.45188
[1mStep[0m  [56/84], [94mLoss[0m : 2.39366
[1mStep[0m  [64/84], [94mLoss[0m : 2.34762
[1mStep[0m  [72/84], [94mLoss[0m : 2.39783
[1mStep[0m  [80/84], [94mLoss[0m : 2.45698

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.328, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54383
[1mStep[0m  [8/84], [94mLoss[0m : 2.41507
[1mStep[0m  [16/84], [94mLoss[0m : 2.51991
[1mStep[0m  [24/84], [94mLoss[0m : 2.67541
[1mStep[0m  [32/84], [94mLoss[0m : 2.54904
[1mStep[0m  [40/84], [94mLoss[0m : 2.45447
[1mStep[0m  [48/84], [94mLoss[0m : 2.18329
[1mStep[0m  [56/84], [94mLoss[0m : 2.46983
[1mStep[0m  [64/84], [94mLoss[0m : 2.22879
[1mStep[0m  [72/84], [94mLoss[0m : 1.87371
[1mStep[0m  [80/84], [94mLoss[0m : 2.17620

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.324, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37388
[1mStep[0m  [8/84], [94mLoss[0m : 2.63342
[1mStep[0m  [16/84], [94mLoss[0m : 2.38680
[1mStep[0m  [24/84], [94mLoss[0m : 2.14973
[1mStep[0m  [32/84], [94mLoss[0m : 2.39012
[1mStep[0m  [40/84], [94mLoss[0m : 2.47563
[1mStep[0m  [48/84], [94mLoss[0m : 2.38757
[1mStep[0m  [56/84], [94mLoss[0m : 2.35498
[1mStep[0m  [64/84], [94mLoss[0m : 2.40128
[1mStep[0m  [72/84], [94mLoss[0m : 2.52060
[1mStep[0m  [80/84], [94mLoss[0m : 2.29382

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.418, [92mTest[0m: 2.332, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.19916
[1mStep[0m  [8/84], [94mLoss[0m : 2.11005
[1mStep[0m  [16/84], [94mLoss[0m : 2.28057
[1mStep[0m  [24/84], [94mLoss[0m : 2.25287
[1mStep[0m  [32/84], [94mLoss[0m : 2.27816
[1mStep[0m  [40/84], [94mLoss[0m : 2.54087
[1mStep[0m  [48/84], [94mLoss[0m : 2.46873
[1mStep[0m  [56/84], [94mLoss[0m : 2.27766
[1mStep[0m  [64/84], [94mLoss[0m : 2.25958
[1mStep[0m  [72/84], [94mLoss[0m : 2.56161
[1mStep[0m  [80/84], [94mLoss[0m : 2.42544

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.332, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27276
[1mStep[0m  [8/84], [94mLoss[0m : 2.74877
[1mStep[0m  [16/84], [94mLoss[0m : 2.82158
[1mStep[0m  [24/84], [94mLoss[0m : 2.36236
[1mStep[0m  [32/84], [94mLoss[0m : 2.20934
[1mStep[0m  [40/84], [94mLoss[0m : 2.42475
[1mStep[0m  [48/84], [94mLoss[0m : 2.23509
[1mStep[0m  [56/84], [94mLoss[0m : 2.35011
[1mStep[0m  [64/84], [94mLoss[0m : 2.49174
[1mStep[0m  [72/84], [94mLoss[0m : 2.45867
[1mStep[0m  [80/84], [94mLoss[0m : 2.37488

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.321, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.22388
[1mStep[0m  [8/84], [94mLoss[0m : 2.24960
[1mStep[0m  [16/84], [94mLoss[0m : 2.41953
[1mStep[0m  [24/84], [94mLoss[0m : 2.51594
[1mStep[0m  [32/84], [94mLoss[0m : 2.45542
[1mStep[0m  [40/84], [94mLoss[0m : 2.26864
[1mStep[0m  [48/84], [94mLoss[0m : 2.46798
[1mStep[0m  [56/84], [94mLoss[0m : 2.27673
[1mStep[0m  [64/84], [94mLoss[0m : 2.32129
[1mStep[0m  [72/84], [94mLoss[0m : 2.22602
[1mStep[0m  [80/84], [94mLoss[0m : 2.35936

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.336, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44712
[1mStep[0m  [8/84], [94mLoss[0m : 2.19592
[1mStep[0m  [16/84], [94mLoss[0m : 2.42277
[1mStep[0m  [24/84], [94mLoss[0m : 2.30285
[1mStep[0m  [32/84], [94mLoss[0m : 2.64522
[1mStep[0m  [40/84], [94mLoss[0m : 2.54547
[1mStep[0m  [48/84], [94mLoss[0m : 2.58862
[1mStep[0m  [56/84], [94mLoss[0m : 2.46189
[1mStep[0m  [64/84], [94mLoss[0m : 2.45958
[1mStep[0m  [72/84], [94mLoss[0m : 2.37078
[1mStep[0m  [80/84], [94mLoss[0m : 2.59083

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.403, [92mTest[0m: 2.326, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.34364
[1mStep[0m  [8/84], [94mLoss[0m : 2.57709
[1mStep[0m  [16/84], [94mLoss[0m : 2.42758
[1mStep[0m  [24/84], [94mLoss[0m : 2.40180
[1mStep[0m  [32/84], [94mLoss[0m : 2.65718
[1mStep[0m  [40/84], [94mLoss[0m : 2.59281
[1mStep[0m  [48/84], [94mLoss[0m : 2.39878
[1mStep[0m  [56/84], [94mLoss[0m : 2.77353
[1mStep[0m  [64/84], [94mLoss[0m : 2.54572
[1mStep[0m  [72/84], [94mLoss[0m : 2.07817
[1mStep[0m  [80/84], [94mLoss[0m : 2.04184

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.323, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33694
[1mStep[0m  [8/84], [94mLoss[0m : 2.04340
[1mStep[0m  [16/84], [94mLoss[0m : 2.36059
[1mStep[0m  [24/84], [94mLoss[0m : 2.52384
[1mStep[0m  [32/84], [94mLoss[0m : 2.24662
[1mStep[0m  [40/84], [94mLoss[0m : 2.14640
[1mStep[0m  [48/84], [94mLoss[0m : 2.36239
[1mStep[0m  [56/84], [94mLoss[0m : 2.64379
[1mStep[0m  [64/84], [94mLoss[0m : 2.53517
[1mStep[0m  [72/84], [94mLoss[0m : 2.36668
[1mStep[0m  [80/84], [94mLoss[0m : 2.21329

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.325, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44283
[1mStep[0m  [8/84], [94mLoss[0m : 2.65028
[1mStep[0m  [16/84], [94mLoss[0m : 2.63680
[1mStep[0m  [24/84], [94mLoss[0m : 2.64975
[1mStep[0m  [32/84], [94mLoss[0m : 2.69277
[1mStep[0m  [40/84], [94mLoss[0m : 2.75982
[1mStep[0m  [48/84], [94mLoss[0m : 2.68610
[1mStep[0m  [56/84], [94mLoss[0m : 2.17803
[1mStep[0m  [64/84], [94mLoss[0m : 2.48218
[1mStep[0m  [72/84], [94mLoss[0m : 2.34395
[1mStep[0m  [80/84], [94mLoss[0m : 2.32789

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.319, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.73968
[1mStep[0m  [8/84], [94mLoss[0m : 2.12829
[1mStep[0m  [16/84], [94mLoss[0m : 2.40870
[1mStep[0m  [24/84], [94mLoss[0m : 2.36866
[1mStep[0m  [32/84], [94mLoss[0m : 2.79512
[1mStep[0m  [40/84], [94mLoss[0m : 2.48212
[1mStep[0m  [48/84], [94mLoss[0m : 2.50471
[1mStep[0m  [56/84], [94mLoss[0m : 2.39636
[1mStep[0m  [64/84], [94mLoss[0m : 2.42626
[1mStep[0m  [72/84], [94mLoss[0m : 2.53840
[1mStep[0m  [80/84], [94mLoss[0m : 2.52142

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.313, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55111
[1mStep[0m  [8/84], [94mLoss[0m : 2.47498
[1mStep[0m  [16/84], [94mLoss[0m : 2.18821
[1mStep[0m  [24/84], [94mLoss[0m : 2.43136
[1mStep[0m  [32/84], [94mLoss[0m : 2.59980
[1mStep[0m  [40/84], [94mLoss[0m : 2.60799
[1mStep[0m  [48/84], [94mLoss[0m : 2.79058
[1mStep[0m  [56/84], [94mLoss[0m : 2.38381
[1mStep[0m  [64/84], [94mLoss[0m : 2.28796
[1mStep[0m  [72/84], [94mLoss[0m : 2.43753
[1mStep[0m  [80/84], [94mLoss[0m : 2.44846

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.317, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.22114
[1mStep[0m  [8/84], [94mLoss[0m : 2.14811
[1mStep[0m  [16/84], [94mLoss[0m : 2.42475
[1mStep[0m  [24/84], [94mLoss[0m : 2.14789
[1mStep[0m  [32/84], [94mLoss[0m : 2.47174
[1mStep[0m  [40/84], [94mLoss[0m : 2.21916
[1mStep[0m  [48/84], [94mLoss[0m : 2.48466
[1mStep[0m  [56/84], [94mLoss[0m : 2.66982
[1mStep[0m  [64/84], [94mLoss[0m : 2.84865
[1mStep[0m  [72/84], [94mLoss[0m : 2.70202
[1mStep[0m  [80/84], [94mLoss[0m : 2.44580

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.403, [92mTest[0m: 2.331, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42894
[1mStep[0m  [8/84], [94mLoss[0m : 2.10520
[1mStep[0m  [16/84], [94mLoss[0m : 2.51203
[1mStep[0m  [24/84], [94mLoss[0m : 2.27414
[1mStep[0m  [32/84], [94mLoss[0m : 2.09821
[1mStep[0m  [40/84], [94mLoss[0m : 2.18416
[1mStep[0m  [48/84], [94mLoss[0m : 2.42035
[1mStep[0m  [56/84], [94mLoss[0m : 2.20650
[1mStep[0m  [64/84], [94mLoss[0m : 2.30938
[1mStep[0m  [72/84], [94mLoss[0m : 2.18558
[1mStep[0m  [80/84], [94mLoss[0m : 2.27295

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.329, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47476
[1mStep[0m  [8/84], [94mLoss[0m : 2.30828
[1mStep[0m  [16/84], [94mLoss[0m : 2.38127
[1mStep[0m  [24/84], [94mLoss[0m : 2.32799
[1mStep[0m  [32/84], [94mLoss[0m : 2.31563
[1mStep[0m  [40/84], [94mLoss[0m : 2.50481
[1mStep[0m  [48/84], [94mLoss[0m : 2.65139
[1mStep[0m  [56/84], [94mLoss[0m : 2.50509
[1mStep[0m  [64/84], [94mLoss[0m : 2.55433
[1mStep[0m  [72/84], [94mLoss[0m : 2.71728
[1mStep[0m  [80/84], [94mLoss[0m : 2.23005

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.326, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.62765
[1mStep[0m  [8/84], [94mLoss[0m : 2.80915
[1mStep[0m  [16/84], [94mLoss[0m : 2.32707
[1mStep[0m  [24/84], [94mLoss[0m : 2.39740
[1mStep[0m  [32/84], [94mLoss[0m : 2.51030
[1mStep[0m  [40/84], [94mLoss[0m : 2.51713
[1mStep[0m  [48/84], [94mLoss[0m : 2.60365
[1mStep[0m  [56/84], [94mLoss[0m : 2.76332
[1mStep[0m  [64/84], [94mLoss[0m : 2.66545
[1mStep[0m  [72/84], [94mLoss[0m : 2.58710
[1mStep[0m  [80/84], [94mLoss[0m : 2.18312

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.319, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45375
[1mStep[0m  [8/84], [94mLoss[0m : 2.51431
[1mStep[0m  [16/84], [94mLoss[0m : 2.36019
[1mStep[0m  [24/84], [94mLoss[0m : 2.39726
[1mStep[0m  [32/84], [94mLoss[0m : 2.43737
[1mStep[0m  [40/84], [94mLoss[0m : 2.61328
[1mStep[0m  [48/84], [94mLoss[0m : 2.80696
[1mStep[0m  [56/84], [94mLoss[0m : 2.62055
[1mStep[0m  [64/84], [94mLoss[0m : 2.48209
[1mStep[0m  [72/84], [94mLoss[0m : 2.36663
[1mStep[0m  [80/84], [94mLoss[0m : 2.30977

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.418, [92mTest[0m: 2.324, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.321
====================================

Phase 1 - Evaluation MAE:  2.3214037077767506
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 2.23756
[1mStep[0m  [8/84], [94mLoss[0m : 2.39031
[1mStep[0m  [16/84], [94mLoss[0m : 2.40680
[1mStep[0m  [24/84], [94mLoss[0m : 2.39673
[1mStep[0m  [32/84], [94mLoss[0m : 2.28458
[1mStep[0m  [40/84], [94mLoss[0m : 2.36114
[1mStep[0m  [48/84], [94mLoss[0m : 2.37475
[1mStep[0m  [56/84], [94mLoss[0m : 2.39094
[1mStep[0m  [64/84], [94mLoss[0m : 2.63754
[1mStep[0m  [72/84], [94mLoss[0m : 2.59324
[1mStep[0m  [80/84], [94mLoss[0m : 2.13413

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.323, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.64244
[1mStep[0m  [8/84], [94mLoss[0m : 2.22448
[1mStep[0m  [16/84], [94mLoss[0m : 2.22370
[1mStep[0m  [24/84], [94mLoss[0m : 2.26055
[1mStep[0m  [32/84], [94mLoss[0m : 2.30155
[1mStep[0m  [40/84], [94mLoss[0m : 2.42925
[1mStep[0m  [48/84], [94mLoss[0m : 2.63297
[1mStep[0m  [56/84], [94mLoss[0m : 2.57779
[1mStep[0m  [64/84], [94mLoss[0m : 2.46036
[1mStep[0m  [72/84], [94mLoss[0m : 2.26973
[1mStep[0m  [80/84], [94mLoss[0m : 2.42735

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.363, [92mTest[0m: 2.470, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46680
[1mStep[0m  [8/84], [94mLoss[0m : 2.55011
[1mStep[0m  [16/84], [94mLoss[0m : 2.06741
[1mStep[0m  [24/84], [94mLoss[0m : 2.23394
[1mStep[0m  [32/84], [94mLoss[0m : 2.18232
[1mStep[0m  [40/84], [94mLoss[0m : 2.21418
[1mStep[0m  [48/84], [94mLoss[0m : 2.32344
[1mStep[0m  [56/84], [94mLoss[0m : 2.40294
[1mStep[0m  [64/84], [94mLoss[0m : 1.96346
[1mStep[0m  [72/84], [94mLoss[0m : 2.01907
[1mStep[0m  [80/84], [94mLoss[0m : 2.36691

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.285, [92mTest[0m: 2.408, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.15773
[1mStep[0m  [8/84], [94mLoss[0m : 2.30467
[1mStep[0m  [16/84], [94mLoss[0m : 2.16164
[1mStep[0m  [24/84], [94mLoss[0m : 2.32627
[1mStep[0m  [32/84], [94mLoss[0m : 2.49715
[1mStep[0m  [40/84], [94mLoss[0m : 2.17015
[1mStep[0m  [48/84], [94mLoss[0m : 2.11302
[1mStep[0m  [56/84], [94mLoss[0m : 1.85224
[1mStep[0m  [64/84], [94mLoss[0m : 2.42032
[1mStep[0m  [72/84], [94mLoss[0m : 2.00495
[1mStep[0m  [80/84], [94mLoss[0m : 2.56205

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.230, [92mTest[0m: 2.350, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.29963
[1mStep[0m  [8/84], [94mLoss[0m : 2.30485
[1mStep[0m  [16/84], [94mLoss[0m : 1.83064
[1mStep[0m  [24/84], [94mLoss[0m : 2.11978
[1mStep[0m  [32/84], [94mLoss[0m : 2.40522
[1mStep[0m  [40/84], [94mLoss[0m : 2.23880
[1mStep[0m  [48/84], [94mLoss[0m : 2.09551
[1mStep[0m  [56/84], [94mLoss[0m : 1.96341
[1mStep[0m  [64/84], [94mLoss[0m : 2.36016
[1mStep[0m  [72/84], [94mLoss[0m : 1.95367
[1mStep[0m  [80/84], [94mLoss[0m : 1.96277

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.163, [92mTest[0m: 2.385, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.06799
[1mStep[0m  [8/84], [94mLoss[0m : 2.23291
[1mStep[0m  [16/84], [94mLoss[0m : 1.95828
[1mStep[0m  [24/84], [94mLoss[0m : 2.00382
[1mStep[0m  [32/84], [94mLoss[0m : 2.08863
[1mStep[0m  [40/84], [94mLoss[0m : 1.69880
[1mStep[0m  [48/84], [94mLoss[0m : 2.12937
[1mStep[0m  [56/84], [94mLoss[0m : 2.33530
[1mStep[0m  [64/84], [94mLoss[0m : 1.92789
[1mStep[0m  [72/84], [94mLoss[0m : 2.04832
[1mStep[0m  [80/84], [94mLoss[0m : 2.07791

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.090, [92mTest[0m: 2.390, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.90223
[1mStep[0m  [8/84], [94mLoss[0m : 1.89352
[1mStep[0m  [16/84], [94mLoss[0m : 2.02470
[1mStep[0m  [24/84], [94mLoss[0m : 2.07499
[1mStep[0m  [32/84], [94mLoss[0m : 2.18310
[1mStep[0m  [40/84], [94mLoss[0m : 2.11104
[1mStep[0m  [48/84], [94mLoss[0m : 1.87848
[1mStep[0m  [56/84], [94mLoss[0m : 2.08124
[1mStep[0m  [64/84], [94mLoss[0m : 1.67554
[1mStep[0m  [72/84], [94mLoss[0m : 1.93412
[1mStep[0m  [80/84], [94mLoss[0m : 2.08809

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.032, [92mTest[0m: 2.397, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.13697
[1mStep[0m  [8/84], [94mLoss[0m : 1.87889
[1mStep[0m  [16/84], [94mLoss[0m : 1.82977
[1mStep[0m  [24/84], [94mLoss[0m : 1.88375
[1mStep[0m  [32/84], [94mLoss[0m : 1.76589
[1mStep[0m  [40/84], [94mLoss[0m : 2.07712
[1mStep[0m  [48/84], [94mLoss[0m : 2.27467
[1mStep[0m  [56/84], [94mLoss[0m : 2.12954
[1mStep[0m  [64/84], [94mLoss[0m : 2.08918
[1mStep[0m  [72/84], [94mLoss[0m : 1.89905
[1mStep[0m  [80/84], [94mLoss[0m : 1.95271

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.971, [92mTest[0m: 2.372, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.68877
[1mStep[0m  [8/84], [94mLoss[0m : 1.99637
[1mStep[0m  [16/84], [94mLoss[0m : 1.97646
[1mStep[0m  [24/84], [94mLoss[0m : 1.86122
[1mStep[0m  [32/84], [94mLoss[0m : 1.69700
[1mStep[0m  [40/84], [94mLoss[0m : 2.19627
[1mStep[0m  [48/84], [94mLoss[0m : 1.76714
[1mStep[0m  [56/84], [94mLoss[0m : 1.91227
[1mStep[0m  [64/84], [94mLoss[0m : 1.88382
[1mStep[0m  [72/84], [94mLoss[0m : 2.17737
[1mStep[0m  [80/84], [94mLoss[0m : 1.91213

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.916, [92mTest[0m: 2.394, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.90423
[1mStep[0m  [8/84], [94mLoss[0m : 2.06992
[1mStep[0m  [16/84], [94mLoss[0m : 1.85755
[1mStep[0m  [24/84], [94mLoss[0m : 1.69826
[1mStep[0m  [32/84], [94mLoss[0m : 2.07900
[1mStep[0m  [40/84], [94mLoss[0m : 1.74829
[1mStep[0m  [48/84], [94mLoss[0m : 1.63116
[1mStep[0m  [56/84], [94mLoss[0m : 1.85164
[1mStep[0m  [64/84], [94mLoss[0m : 1.61658
[1mStep[0m  [72/84], [94mLoss[0m : 1.61698
[1mStep[0m  [80/84], [94mLoss[0m : 1.90326

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.860, [92mTest[0m: 2.462, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.88349
[1mStep[0m  [8/84], [94mLoss[0m : 1.82263
[1mStep[0m  [16/84], [94mLoss[0m : 1.74532
[1mStep[0m  [24/84], [94mLoss[0m : 1.57057
[1mStep[0m  [32/84], [94mLoss[0m : 1.82475
[1mStep[0m  [40/84], [94mLoss[0m : 1.83560
[1mStep[0m  [48/84], [94mLoss[0m : 1.77027
[1mStep[0m  [56/84], [94mLoss[0m : 1.62834
[1mStep[0m  [64/84], [94mLoss[0m : 1.84910
[1mStep[0m  [72/84], [94mLoss[0m : 1.74278
[1mStep[0m  [80/84], [94mLoss[0m : 2.00015

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.812, [92mTest[0m: 2.443, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.96511
[1mStep[0m  [8/84], [94mLoss[0m : 1.50973
[1mStep[0m  [16/84], [94mLoss[0m : 1.99994
[1mStep[0m  [24/84], [94mLoss[0m : 1.86532
[1mStep[0m  [32/84], [94mLoss[0m : 1.74067
[1mStep[0m  [40/84], [94mLoss[0m : 1.54807
[1mStep[0m  [48/84], [94mLoss[0m : 1.73144
[1mStep[0m  [56/84], [94mLoss[0m : 1.58281
[1mStep[0m  [64/84], [94mLoss[0m : 1.50781
[1mStep[0m  [72/84], [94mLoss[0m : 2.04184
[1mStep[0m  [80/84], [94mLoss[0m : 1.66312

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.769, [92mTest[0m: 2.459, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.64575
[1mStep[0m  [8/84], [94mLoss[0m : 1.34067
[1mStep[0m  [16/84], [94mLoss[0m : 1.67076
[1mStep[0m  [24/84], [94mLoss[0m : 1.83800
[1mStep[0m  [32/84], [94mLoss[0m : 1.79334
[1mStep[0m  [40/84], [94mLoss[0m : 1.69872
[1mStep[0m  [48/84], [94mLoss[0m : 1.77318
[1mStep[0m  [56/84], [94mLoss[0m : 2.00575
[1mStep[0m  [64/84], [94mLoss[0m : 1.58172
[1mStep[0m  [72/84], [94mLoss[0m : 1.73349
[1mStep[0m  [80/84], [94mLoss[0m : 1.59726

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.741, [92mTest[0m: 2.504, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.59785
[1mStep[0m  [8/84], [94mLoss[0m : 1.67034
[1mStep[0m  [16/84], [94mLoss[0m : 1.66174
[1mStep[0m  [24/84], [94mLoss[0m : 1.49305
[1mStep[0m  [32/84], [94mLoss[0m : 1.61020
[1mStep[0m  [40/84], [94mLoss[0m : 1.65557
[1mStep[0m  [48/84], [94mLoss[0m : 1.97162
[1mStep[0m  [56/84], [94mLoss[0m : 1.75933
[1mStep[0m  [64/84], [94mLoss[0m : 1.55214
[1mStep[0m  [72/84], [94mLoss[0m : 1.44583
[1mStep[0m  [80/84], [94mLoss[0m : 1.43211

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.680, [92mTest[0m: 2.451, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.49460
[1mStep[0m  [8/84], [94mLoss[0m : 1.81060
[1mStep[0m  [16/84], [94mLoss[0m : 1.74760
[1mStep[0m  [24/84], [94mLoss[0m : 1.61152
[1mStep[0m  [32/84], [94mLoss[0m : 1.69863
[1mStep[0m  [40/84], [94mLoss[0m : 1.74391
[1mStep[0m  [48/84], [94mLoss[0m : 1.50141
[1mStep[0m  [56/84], [94mLoss[0m : 1.45804
[1mStep[0m  [64/84], [94mLoss[0m : 1.51768
[1mStep[0m  [72/84], [94mLoss[0m : 1.71325
[1mStep[0m  [80/84], [94mLoss[0m : 1.62812

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.656, [92mTest[0m: 2.452, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.45413
[1mStep[0m  [8/84], [94mLoss[0m : 1.85942
[1mStep[0m  [16/84], [94mLoss[0m : 1.33153
[1mStep[0m  [24/84], [94mLoss[0m : 1.67623
[1mStep[0m  [32/84], [94mLoss[0m : 1.81035
[1mStep[0m  [40/84], [94mLoss[0m : 1.54380
[1mStep[0m  [48/84], [94mLoss[0m : 1.50420
[1mStep[0m  [56/84], [94mLoss[0m : 1.47763
[1mStep[0m  [64/84], [94mLoss[0m : 1.66179
[1mStep[0m  [72/84], [94mLoss[0m : 1.64094
[1mStep[0m  [80/84], [94mLoss[0m : 1.69520

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.625, [92mTest[0m: 2.470, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.57023
[1mStep[0m  [8/84], [94mLoss[0m : 1.60458
[1mStep[0m  [16/84], [94mLoss[0m : 1.54387
[1mStep[0m  [24/84], [94mLoss[0m : 1.42435
[1mStep[0m  [32/84], [94mLoss[0m : 1.59166
[1mStep[0m  [40/84], [94mLoss[0m : 1.44986
[1mStep[0m  [48/84], [94mLoss[0m : 1.67702
[1mStep[0m  [56/84], [94mLoss[0m : 1.57821
[1mStep[0m  [64/84], [94mLoss[0m : 1.68259
[1mStep[0m  [72/84], [94mLoss[0m : 1.71729
[1mStep[0m  [80/84], [94mLoss[0m : 1.70143

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.588, [92mTest[0m: 2.459, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.42262
[1mStep[0m  [8/84], [94mLoss[0m : 1.53191
[1mStep[0m  [16/84], [94mLoss[0m : 1.45331
[1mStep[0m  [24/84], [94mLoss[0m : 1.65056
[1mStep[0m  [32/84], [94mLoss[0m : 1.41479
[1mStep[0m  [40/84], [94mLoss[0m : 1.64995
[1mStep[0m  [48/84], [94mLoss[0m : 1.57846
[1mStep[0m  [56/84], [94mLoss[0m : 1.44158
[1mStep[0m  [64/84], [94mLoss[0m : 1.32726
[1mStep[0m  [72/84], [94mLoss[0m : 1.62449
[1mStep[0m  [80/84], [94mLoss[0m : 1.48303

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.553, [92mTest[0m: 2.482, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.31958
[1mStep[0m  [8/84], [94mLoss[0m : 1.50794
[1mStep[0m  [16/84], [94mLoss[0m : 1.54237
[1mStep[0m  [24/84], [94mLoss[0m : 1.78092
[1mStep[0m  [32/84], [94mLoss[0m : 1.42162
[1mStep[0m  [40/84], [94mLoss[0m : 1.56321
[1mStep[0m  [48/84], [94mLoss[0m : 1.45735
[1mStep[0m  [56/84], [94mLoss[0m : 1.42057
[1mStep[0m  [64/84], [94mLoss[0m : 1.51609
[1mStep[0m  [72/84], [94mLoss[0m : 1.63770
[1mStep[0m  [80/84], [94mLoss[0m : 1.53870

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.521, [92mTest[0m: 2.458, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.59773
[1mStep[0m  [8/84], [94mLoss[0m : 1.47906
[1mStep[0m  [16/84], [94mLoss[0m : 1.40886
[1mStep[0m  [24/84], [94mLoss[0m : 1.59693
[1mStep[0m  [32/84], [94mLoss[0m : 1.47713
[1mStep[0m  [40/84], [94mLoss[0m : 1.24146
[1mStep[0m  [48/84], [94mLoss[0m : 1.53354
[1mStep[0m  [56/84], [94mLoss[0m : 1.61634
[1mStep[0m  [64/84], [94mLoss[0m : 1.50608
[1mStep[0m  [72/84], [94mLoss[0m : 1.55260
[1mStep[0m  [80/84], [94mLoss[0m : 1.38215

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.495, [92mTest[0m: 2.461, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.36824
[1mStep[0m  [8/84], [94mLoss[0m : 1.43805
[1mStep[0m  [16/84], [94mLoss[0m : 1.68537
[1mStep[0m  [24/84], [94mLoss[0m : 1.39621
[1mStep[0m  [32/84], [94mLoss[0m : 1.54182
[1mStep[0m  [40/84], [94mLoss[0m : 1.58879
[1mStep[0m  [48/84], [94mLoss[0m : 1.31715
[1mStep[0m  [56/84], [94mLoss[0m : 1.45669
[1mStep[0m  [64/84], [94mLoss[0m : 1.54555
[1mStep[0m  [72/84], [94mLoss[0m : 1.46036
[1mStep[0m  [80/84], [94mLoss[0m : 1.48426

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.463, [92mTest[0m: 2.494, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.37248
[1mStep[0m  [8/84], [94mLoss[0m : 1.37941
[1mStep[0m  [16/84], [94mLoss[0m : 1.53172
[1mStep[0m  [24/84], [94mLoss[0m : 1.29896
[1mStep[0m  [32/84], [94mLoss[0m : 1.20833
[1mStep[0m  [40/84], [94mLoss[0m : 1.31227
[1mStep[0m  [48/84], [94mLoss[0m : 1.39639
[1mStep[0m  [56/84], [94mLoss[0m : 1.56499
[1mStep[0m  [64/84], [94mLoss[0m : 1.38767
[1mStep[0m  [72/84], [94mLoss[0m : 1.50521
[1mStep[0m  [80/84], [94mLoss[0m : 1.48701

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.426, [92mTest[0m: 2.506, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.31505
[1mStep[0m  [8/84], [94mLoss[0m : 1.36225
[1mStep[0m  [16/84], [94mLoss[0m : 1.19882
[1mStep[0m  [24/84], [94mLoss[0m : 1.42779
[1mStep[0m  [32/84], [94mLoss[0m : 1.42386
[1mStep[0m  [40/84], [94mLoss[0m : 1.56579
[1mStep[0m  [48/84], [94mLoss[0m : 1.44086
[1mStep[0m  [56/84], [94mLoss[0m : 1.38092
[1mStep[0m  [64/84], [94mLoss[0m : 1.38084
[1mStep[0m  [72/84], [94mLoss[0m : 1.42958
[1mStep[0m  [80/84], [94mLoss[0m : 1.27620

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.406, [92mTest[0m: 2.514, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.33475
[1mStep[0m  [8/84], [94mLoss[0m : 1.41801
[1mStep[0m  [16/84], [94mLoss[0m : 1.54352
[1mStep[0m  [24/84], [94mLoss[0m : 1.42305
[1mStep[0m  [32/84], [94mLoss[0m : 1.48368
[1mStep[0m  [40/84], [94mLoss[0m : 1.42007
[1mStep[0m  [48/84], [94mLoss[0m : 1.50075
[1mStep[0m  [56/84], [94mLoss[0m : 1.54856
[1mStep[0m  [64/84], [94mLoss[0m : 1.58752
[1mStep[0m  [72/84], [94mLoss[0m : 1.62555
[1mStep[0m  [80/84], [94mLoss[0m : 1.40919

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.396, [92mTest[0m: 2.502, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.49418
[1mStep[0m  [8/84], [94mLoss[0m : 1.30520
[1mStep[0m  [16/84], [94mLoss[0m : 1.39612
[1mStep[0m  [24/84], [94mLoss[0m : 1.40933
[1mStep[0m  [32/84], [94mLoss[0m : 1.29749
[1mStep[0m  [40/84], [94mLoss[0m : 1.20294
[1mStep[0m  [48/84], [94mLoss[0m : 1.33140
[1mStep[0m  [56/84], [94mLoss[0m : 1.57809
[1mStep[0m  [64/84], [94mLoss[0m : 1.60038
[1mStep[0m  [72/84], [94mLoss[0m : 1.48707
[1mStep[0m  [80/84], [94mLoss[0m : 1.31603

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.362, [92mTest[0m: 2.486, [96mlr[0m: 0.006772500000000002
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 24 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.548
====================================

Phase 2 - Evaluation MAE:  2.5477652294295177
MAE score P1      2.321404
MAE score P2      2.547765
loss              1.362283
learning_rate     0.007525
batch_size             128
hidden_sizes         [100]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping        True
dropout                0.2
momentum               0.5
weight_decay         0.001
Name: 28, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 11.71563
[1mStep[0m  [4/42], [94mLoss[0m : 8.97793
[1mStep[0m  [8/42], [94mLoss[0m : 5.82563
[1mStep[0m  [12/42], [94mLoss[0m : 3.01995
[1mStep[0m  [16/42], [94mLoss[0m : 3.61390
[1mStep[0m  [20/42], [94mLoss[0m : 4.29719
[1mStep[0m  [24/42], [94mLoss[0m : 3.31172
[1mStep[0m  [28/42], [94mLoss[0m : 2.74992
[1mStep[0m  [32/42], [94mLoss[0m : 2.60316
[1mStep[0m  [36/42], [94mLoss[0m : 3.02201
[1mStep[0m  [40/42], [94mLoss[0m : 2.72494

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.484, [92mTest[0m: 11.167, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52891
[1mStep[0m  [4/42], [94mLoss[0m : 2.59611
[1mStep[0m  [8/42], [94mLoss[0m : 2.78417
[1mStep[0m  [12/42], [94mLoss[0m : 2.64305
[1mStep[0m  [16/42], [94mLoss[0m : 2.38004
[1mStep[0m  [20/42], [94mLoss[0m : 2.57764
[1mStep[0m  [24/42], [94mLoss[0m : 2.60792
[1mStep[0m  [28/42], [94mLoss[0m : 2.56983
[1mStep[0m  [32/42], [94mLoss[0m : 2.53881
[1mStep[0m  [36/42], [94mLoss[0m : 2.41276
[1mStep[0m  [40/42], [94mLoss[0m : 2.46491

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.596, [92mTest[0m: 2.442, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35864
[1mStep[0m  [4/42], [94mLoss[0m : 2.41582
[1mStep[0m  [8/42], [94mLoss[0m : 2.51361
[1mStep[0m  [12/42], [94mLoss[0m : 2.50002
[1mStep[0m  [16/42], [94mLoss[0m : 2.50196
[1mStep[0m  [20/42], [94mLoss[0m : 2.66040
[1mStep[0m  [24/42], [94mLoss[0m : 2.53727
[1mStep[0m  [28/42], [94mLoss[0m : 2.65260
[1mStep[0m  [32/42], [94mLoss[0m : 2.63104
[1mStep[0m  [36/42], [94mLoss[0m : 2.68531
[1mStep[0m  [40/42], [94mLoss[0m : 2.73411

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.557, [92mTest[0m: 2.364, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58200
[1mStep[0m  [4/42], [94mLoss[0m : 2.49695
[1mStep[0m  [8/42], [94mLoss[0m : 2.37513
[1mStep[0m  [12/42], [94mLoss[0m : 2.75802
[1mStep[0m  [16/42], [94mLoss[0m : 2.65867
[1mStep[0m  [20/42], [94mLoss[0m : 2.55607
[1mStep[0m  [24/42], [94mLoss[0m : 2.30776
[1mStep[0m  [28/42], [94mLoss[0m : 2.49053
[1mStep[0m  [32/42], [94mLoss[0m : 2.45212
[1mStep[0m  [36/42], [94mLoss[0m : 2.66094
[1mStep[0m  [40/42], [94mLoss[0m : 2.79761

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.558, [92mTest[0m: 2.356, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48629
[1mStep[0m  [4/42], [94mLoss[0m : 2.42961
[1mStep[0m  [8/42], [94mLoss[0m : 2.55406
[1mStep[0m  [12/42], [94mLoss[0m : 2.46674
[1mStep[0m  [16/42], [94mLoss[0m : 2.73342
[1mStep[0m  [20/42], [94mLoss[0m : 2.30970
[1mStep[0m  [24/42], [94mLoss[0m : 2.50461
[1mStep[0m  [28/42], [94mLoss[0m : 2.41933
[1mStep[0m  [32/42], [94mLoss[0m : 2.41910
[1mStep[0m  [36/42], [94mLoss[0m : 2.54769
[1mStep[0m  [40/42], [94mLoss[0m : 2.55964

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.537, [92mTest[0m: 2.344, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58906
[1mStep[0m  [4/42], [94mLoss[0m : 2.71450
[1mStep[0m  [8/42], [94mLoss[0m : 2.56619
[1mStep[0m  [12/42], [94mLoss[0m : 2.58642
[1mStep[0m  [16/42], [94mLoss[0m : 2.51380
[1mStep[0m  [20/42], [94mLoss[0m : 2.64988
[1mStep[0m  [24/42], [94mLoss[0m : 2.67729
[1mStep[0m  [28/42], [94mLoss[0m : 2.57336
[1mStep[0m  [32/42], [94mLoss[0m : 2.60207
[1mStep[0m  [36/42], [94mLoss[0m : 2.39423
[1mStep[0m  [40/42], [94mLoss[0m : 2.48098

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.534, [92mTest[0m: 2.350, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.67948
[1mStep[0m  [4/42], [94mLoss[0m : 2.56836
[1mStep[0m  [8/42], [94mLoss[0m : 2.47604
[1mStep[0m  [12/42], [94mLoss[0m : 2.54157
[1mStep[0m  [16/42], [94mLoss[0m : 2.42729
[1mStep[0m  [20/42], [94mLoss[0m : 2.63833
[1mStep[0m  [24/42], [94mLoss[0m : 2.53417
[1mStep[0m  [28/42], [94mLoss[0m : 2.35026
[1mStep[0m  [32/42], [94mLoss[0m : 2.44058
[1mStep[0m  [36/42], [94mLoss[0m : 2.71763
[1mStep[0m  [40/42], [94mLoss[0m : 2.54264

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.360, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37042
[1mStep[0m  [4/42], [94mLoss[0m : 2.44393
[1mStep[0m  [8/42], [94mLoss[0m : 2.55270
[1mStep[0m  [12/42], [94mLoss[0m : 2.61874
[1mStep[0m  [16/42], [94mLoss[0m : 2.73437
[1mStep[0m  [20/42], [94mLoss[0m : 2.42564
[1mStep[0m  [24/42], [94mLoss[0m : 2.55441
[1mStep[0m  [28/42], [94mLoss[0m : 2.70302
[1mStep[0m  [32/42], [94mLoss[0m : 2.57078
[1mStep[0m  [36/42], [94mLoss[0m : 2.29845
[1mStep[0m  [40/42], [94mLoss[0m : 2.43994

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.332, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55709
[1mStep[0m  [4/42], [94mLoss[0m : 2.72552
[1mStep[0m  [8/42], [94mLoss[0m : 2.30841
[1mStep[0m  [12/42], [94mLoss[0m : 2.51705
[1mStep[0m  [16/42], [94mLoss[0m : 2.57547
[1mStep[0m  [20/42], [94mLoss[0m : 2.52526
[1mStep[0m  [24/42], [94mLoss[0m : 2.31746
[1mStep[0m  [28/42], [94mLoss[0m : 2.47044
[1mStep[0m  [32/42], [94mLoss[0m : 2.50436
[1mStep[0m  [36/42], [94mLoss[0m : 2.49716
[1mStep[0m  [40/42], [94mLoss[0m : 2.53507

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.339, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26454
[1mStep[0m  [4/42], [94mLoss[0m : 2.79252
[1mStep[0m  [8/42], [94mLoss[0m : 2.59107
[1mStep[0m  [12/42], [94mLoss[0m : 2.67249
[1mStep[0m  [16/42], [94mLoss[0m : 2.39636
[1mStep[0m  [20/42], [94mLoss[0m : 2.33014
[1mStep[0m  [24/42], [94mLoss[0m : 2.52008
[1mStep[0m  [28/42], [94mLoss[0m : 2.70945
[1mStep[0m  [32/42], [94mLoss[0m : 2.60444
[1mStep[0m  [36/42], [94mLoss[0m : 2.46494
[1mStep[0m  [40/42], [94mLoss[0m : 2.75256

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.538, [92mTest[0m: 2.338, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58426
[1mStep[0m  [4/42], [94mLoss[0m : 2.47744
[1mStep[0m  [8/42], [94mLoss[0m : 2.38146
[1mStep[0m  [12/42], [94mLoss[0m : 2.81651
[1mStep[0m  [16/42], [94mLoss[0m : 2.39201
[1mStep[0m  [20/42], [94mLoss[0m : 2.54508
[1mStep[0m  [24/42], [94mLoss[0m : 2.48652
[1mStep[0m  [28/42], [94mLoss[0m : 2.24935
[1mStep[0m  [32/42], [94mLoss[0m : 2.58902
[1mStep[0m  [36/42], [94mLoss[0m : 2.43167
[1mStep[0m  [40/42], [94mLoss[0m : 2.60613

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.528, [92mTest[0m: 2.334, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29374
[1mStep[0m  [4/42], [94mLoss[0m : 2.49009
[1mStep[0m  [8/42], [94mLoss[0m : 2.44853
[1mStep[0m  [12/42], [94mLoss[0m : 2.66181
[1mStep[0m  [16/42], [94mLoss[0m : 2.45294
[1mStep[0m  [20/42], [94mLoss[0m : 2.41899
[1mStep[0m  [24/42], [94mLoss[0m : 2.25003
[1mStep[0m  [28/42], [94mLoss[0m : 2.26538
[1mStep[0m  [32/42], [94mLoss[0m : 2.67876
[1mStep[0m  [36/42], [94mLoss[0m : 2.35879
[1mStep[0m  [40/42], [94mLoss[0m : 2.45940

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.352, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.78327
[1mStep[0m  [4/42], [94mLoss[0m : 2.49272
[1mStep[0m  [8/42], [94mLoss[0m : 2.55223
[1mStep[0m  [12/42], [94mLoss[0m : 2.45307
[1mStep[0m  [16/42], [94mLoss[0m : 2.49125
[1mStep[0m  [20/42], [94mLoss[0m : 2.48960
[1mStep[0m  [24/42], [94mLoss[0m : 2.64607
[1mStep[0m  [28/42], [94mLoss[0m : 2.42304
[1mStep[0m  [32/42], [94mLoss[0m : 2.63401
[1mStep[0m  [36/42], [94mLoss[0m : 2.52408
[1mStep[0m  [40/42], [94mLoss[0m : 2.33609

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.510, [92mTest[0m: 2.340, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53727
[1mStep[0m  [4/42], [94mLoss[0m : 2.43035
[1mStep[0m  [8/42], [94mLoss[0m : 2.70709
[1mStep[0m  [12/42], [94mLoss[0m : 2.18554
[1mStep[0m  [16/42], [94mLoss[0m : 2.76281
[1mStep[0m  [20/42], [94mLoss[0m : 2.30126
[1mStep[0m  [24/42], [94mLoss[0m : 2.64565
[1mStep[0m  [28/42], [94mLoss[0m : 2.35636
[1mStep[0m  [32/42], [94mLoss[0m : 2.61443
[1mStep[0m  [36/42], [94mLoss[0m : 2.51371
[1mStep[0m  [40/42], [94mLoss[0m : 2.46633

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.325, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45406
[1mStep[0m  [4/42], [94mLoss[0m : 2.63719
[1mStep[0m  [8/42], [94mLoss[0m : 2.51390
[1mStep[0m  [12/42], [94mLoss[0m : 2.48423
[1mStep[0m  [16/42], [94mLoss[0m : 2.16816
[1mStep[0m  [20/42], [94mLoss[0m : 2.49813
[1mStep[0m  [24/42], [94mLoss[0m : 2.51788
[1mStep[0m  [28/42], [94mLoss[0m : 2.53071
[1mStep[0m  [32/42], [94mLoss[0m : 2.64926
[1mStep[0m  [36/42], [94mLoss[0m : 2.56364
[1mStep[0m  [40/42], [94mLoss[0m : 2.45850

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.333, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51345
[1mStep[0m  [4/42], [94mLoss[0m : 2.70094
[1mStep[0m  [8/42], [94mLoss[0m : 2.47510
[1mStep[0m  [12/42], [94mLoss[0m : 2.53919
[1mStep[0m  [16/42], [94mLoss[0m : 2.47282
[1mStep[0m  [20/42], [94mLoss[0m : 2.45608
[1mStep[0m  [24/42], [94mLoss[0m : 2.72418
[1mStep[0m  [28/42], [94mLoss[0m : 2.67258
[1mStep[0m  [32/42], [94mLoss[0m : 2.61933
[1mStep[0m  [36/42], [94mLoss[0m : 2.53432
[1mStep[0m  [40/42], [94mLoss[0m : 2.51173

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.510, [92mTest[0m: 2.334, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48534
[1mStep[0m  [4/42], [94mLoss[0m : 2.57333
[1mStep[0m  [8/42], [94mLoss[0m : 2.59090
[1mStep[0m  [12/42], [94mLoss[0m : 2.46403
[1mStep[0m  [16/42], [94mLoss[0m : 2.59813
[1mStep[0m  [20/42], [94mLoss[0m : 2.51502
[1mStep[0m  [24/42], [94mLoss[0m : 2.62685
[1mStep[0m  [28/42], [94mLoss[0m : 2.39802
[1mStep[0m  [32/42], [94mLoss[0m : 2.68961
[1mStep[0m  [36/42], [94mLoss[0m : 2.72226
[1mStep[0m  [40/42], [94mLoss[0m : 2.43971

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.334, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47470
[1mStep[0m  [4/42], [94mLoss[0m : 2.40478
[1mStep[0m  [8/42], [94mLoss[0m : 2.37601
[1mStep[0m  [12/42], [94mLoss[0m : 2.61620
[1mStep[0m  [16/42], [94mLoss[0m : 2.44647
[1mStep[0m  [20/42], [94mLoss[0m : 2.48415
[1mStep[0m  [24/42], [94mLoss[0m : 2.35978
[1mStep[0m  [28/42], [94mLoss[0m : 2.70098
[1mStep[0m  [32/42], [94mLoss[0m : 2.53153
[1mStep[0m  [36/42], [94mLoss[0m : 2.43934
[1mStep[0m  [40/42], [94mLoss[0m : 2.57517

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.335, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53725
[1mStep[0m  [4/42], [94mLoss[0m : 2.45450
[1mStep[0m  [8/42], [94mLoss[0m : 2.33019
[1mStep[0m  [12/42], [94mLoss[0m : 2.50029
[1mStep[0m  [16/42], [94mLoss[0m : 2.34635
[1mStep[0m  [20/42], [94mLoss[0m : 2.43746
[1mStep[0m  [24/42], [94mLoss[0m : 2.26795
[1mStep[0m  [28/42], [94mLoss[0m : 2.71674
[1mStep[0m  [32/42], [94mLoss[0m : 2.49192
[1mStep[0m  [36/42], [94mLoss[0m : 2.62341
[1mStep[0m  [40/42], [94mLoss[0m : 2.39537

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.338, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44801
[1mStep[0m  [4/42], [94mLoss[0m : 2.35128
[1mStep[0m  [8/42], [94mLoss[0m : 2.46230
[1mStep[0m  [12/42], [94mLoss[0m : 2.61584
[1mStep[0m  [16/42], [94mLoss[0m : 2.58837
[1mStep[0m  [20/42], [94mLoss[0m : 2.46604
[1mStep[0m  [24/42], [94mLoss[0m : 2.41190
[1mStep[0m  [28/42], [94mLoss[0m : 2.65727
[1mStep[0m  [32/42], [94mLoss[0m : 2.22154
[1mStep[0m  [36/42], [94mLoss[0m : 2.54454
[1mStep[0m  [40/42], [94mLoss[0m : 2.44362

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.348, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60051
[1mStep[0m  [4/42], [94mLoss[0m : 2.61818
[1mStep[0m  [8/42], [94mLoss[0m : 2.51764
[1mStep[0m  [12/42], [94mLoss[0m : 2.42192
[1mStep[0m  [16/42], [94mLoss[0m : 2.39660
[1mStep[0m  [20/42], [94mLoss[0m : 2.45222
[1mStep[0m  [24/42], [94mLoss[0m : 2.46208
[1mStep[0m  [28/42], [94mLoss[0m : 2.43927
[1mStep[0m  [32/42], [94mLoss[0m : 2.64578
[1mStep[0m  [36/42], [94mLoss[0m : 2.50853
[1mStep[0m  [40/42], [94mLoss[0m : 2.74009

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.340, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55189
[1mStep[0m  [4/42], [94mLoss[0m : 2.59749
[1mStep[0m  [8/42], [94mLoss[0m : 2.39007
[1mStep[0m  [12/42], [94mLoss[0m : 2.49827
[1mStep[0m  [16/42], [94mLoss[0m : 2.32203
[1mStep[0m  [20/42], [94mLoss[0m : 2.39234
[1mStep[0m  [24/42], [94mLoss[0m : 2.48753
[1mStep[0m  [28/42], [94mLoss[0m : 2.41908
[1mStep[0m  [32/42], [94mLoss[0m : 2.59802
[1mStep[0m  [36/42], [94mLoss[0m : 2.50894
[1mStep[0m  [40/42], [94mLoss[0m : 2.46920

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.338, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54861
[1mStep[0m  [4/42], [94mLoss[0m : 2.41656
[1mStep[0m  [8/42], [94mLoss[0m : 2.45088
[1mStep[0m  [12/42], [94mLoss[0m : 2.58974
[1mStep[0m  [16/42], [94mLoss[0m : 2.73190
[1mStep[0m  [20/42], [94mLoss[0m : 2.51930
[1mStep[0m  [24/42], [94mLoss[0m : 2.46912
[1mStep[0m  [28/42], [94mLoss[0m : 2.60629
[1mStep[0m  [32/42], [94mLoss[0m : 2.52826
[1mStep[0m  [36/42], [94mLoss[0m : 2.29664
[1mStep[0m  [40/42], [94mLoss[0m : 2.42001

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.340, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54047
[1mStep[0m  [4/42], [94mLoss[0m : 2.41124
[1mStep[0m  [8/42], [94mLoss[0m : 2.32071
[1mStep[0m  [12/42], [94mLoss[0m : 2.51045
[1mStep[0m  [16/42], [94mLoss[0m : 2.46851
[1mStep[0m  [20/42], [94mLoss[0m : 2.48592
[1mStep[0m  [24/42], [94mLoss[0m : 2.56492
[1mStep[0m  [28/42], [94mLoss[0m : 2.51314
[1mStep[0m  [32/42], [94mLoss[0m : 2.48503
[1mStep[0m  [36/42], [94mLoss[0m : 2.53129
[1mStep[0m  [40/42], [94mLoss[0m : 2.44311

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.337, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27620
[1mStep[0m  [4/42], [94mLoss[0m : 2.54646
[1mStep[0m  [8/42], [94mLoss[0m : 2.75246
[1mStep[0m  [12/42], [94mLoss[0m : 2.59836
[1mStep[0m  [16/42], [94mLoss[0m : 2.58434
[1mStep[0m  [20/42], [94mLoss[0m : 2.33167
[1mStep[0m  [24/42], [94mLoss[0m : 2.62421
[1mStep[0m  [28/42], [94mLoss[0m : 2.47279
[1mStep[0m  [32/42], [94mLoss[0m : 2.51172
[1mStep[0m  [36/42], [94mLoss[0m : 2.42034
[1mStep[0m  [40/42], [94mLoss[0m : 2.72370

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.341, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52771
[1mStep[0m  [4/42], [94mLoss[0m : 2.53411
[1mStep[0m  [8/42], [94mLoss[0m : 2.75564
[1mStep[0m  [12/42], [94mLoss[0m : 2.64377
[1mStep[0m  [16/42], [94mLoss[0m : 2.43778
[1mStep[0m  [20/42], [94mLoss[0m : 2.85303
[1mStep[0m  [24/42], [94mLoss[0m : 2.25673
[1mStep[0m  [28/42], [94mLoss[0m : 2.43246
[1mStep[0m  [32/42], [94mLoss[0m : 2.73480
[1mStep[0m  [36/42], [94mLoss[0m : 2.61684
[1mStep[0m  [40/42], [94mLoss[0m : 2.21686

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.347, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46962
[1mStep[0m  [4/42], [94mLoss[0m : 2.58690
[1mStep[0m  [8/42], [94mLoss[0m : 2.31593
[1mStep[0m  [12/42], [94mLoss[0m : 2.37636
[1mStep[0m  [16/42], [94mLoss[0m : 2.76719
[1mStep[0m  [20/42], [94mLoss[0m : 2.74359
[1mStep[0m  [24/42], [94mLoss[0m : 2.45299
[1mStep[0m  [28/42], [94mLoss[0m : 2.37493
[1mStep[0m  [32/42], [94mLoss[0m : 2.72662
[1mStep[0m  [36/42], [94mLoss[0m : 2.47133
[1mStep[0m  [40/42], [94mLoss[0m : 2.46904

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.352, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46449
[1mStep[0m  [4/42], [94mLoss[0m : 2.69105
[1mStep[0m  [8/42], [94mLoss[0m : 2.39589
[1mStep[0m  [12/42], [94mLoss[0m : 2.53408
[1mStep[0m  [16/42], [94mLoss[0m : 2.67411
[1mStep[0m  [20/42], [94mLoss[0m : 2.53810
[1mStep[0m  [24/42], [94mLoss[0m : 2.36096
[1mStep[0m  [28/42], [94mLoss[0m : 2.59543
[1mStep[0m  [32/42], [94mLoss[0m : 2.66631
[1mStep[0m  [36/42], [94mLoss[0m : 2.42665
[1mStep[0m  [40/42], [94mLoss[0m : 2.55524

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.515, [92mTest[0m: 2.351, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49586
[1mStep[0m  [4/42], [94mLoss[0m : 2.61191
[1mStep[0m  [8/42], [94mLoss[0m : 2.50972
[1mStep[0m  [12/42], [94mLoss[0m : 2.46293
[1mStep[0m  [16/42], [94mLoss[0m : 2.23225
[1mStep[0m  [20/42], [94mLoss[0m : 2.49755
[1mStep[0m  [24/42], [94mLoss[0m : 2.55976
[1mStep[0m  [28/42], [94mLoss[0m : 2.61046
[1mStep[0m  [32/42], [94mLoss[0m : 2.50131
[1mStep[0m  [36/42], [94mLoss[0m : 2.51351
[1mStep[0m  [40/42], [94mLoss[0m : 2.42311

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.352, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29914
[1mStep[0m  [4/42], [94mLoss[0m : 2.43402
[1mStep[0m  [8/42], [94mLoss[0m : 2.43167
[1mStep[0m  [12/42], [94mLoss[0m : 2.52526
[1mStep[0m  [16/42], [94mLoss[0m : 2.55121
[1mStep[0m  [20/42], [94mLoss[0m : 2.35060
[1mStep[0m  [24/42], [94mLoss[0m : 2.58202
[1mStep[0m  [28/42], [94mLoss[0m : 2.46830
[1mStep[0m  [32/42], [94mLoss[0m : 2.37092
[1mStep[0m  [36/42], [94mLoss[0m : 2.40085
[1mStep[0m  [40/42], [94mLoss[0m : 2.45910

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.350, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.356
====================================

Phase 1 - Evaluation MAE:  2.3556279114314487
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 2.45620
[1mStep[0m  [4/42], [94mLoss[0m : 2.70396
[1mStep[0m  [8/42], [94mLoss[0m : 2.38125
[1mStep[0m  [12/42], [94mLoss[0m : 2.70075
[1mStep[0m  [16/42], [94mLoss[0m : 2.76676
[1mStep[0m  [20/42], [94mLoss[0m : 2.50373
[1mStep[0m  [24/42], [94mLoss[0m : 2.60205
[1mStep[0m  [28/42], [94mLoss[0m : 2.46055
[1mStep[0m  [32/42], [94mLoss[0m : 2.56191
[1mStep[0m  [36/42], [94mLoss[0m : 2.46817
[1mStep[0m  [40/42], [94mLoss[0m : 2.64882

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.549, [92mTest[0m: 2.358, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38821
[1mStep[0m  [4/42], [94mLoss[0m : 2.54184
[1mStep[0m  [8/42], [94mLoss[0m : 2.32693
[1mStep[0m  [12/42], [94mLoss[0m : 2.40372
[1mStep[0m  [16/42], [94mLoss[0m : 2.46023
[1mStep[0m  [20/42], [94mLoss[0m : 2.43923
[1mStep[0m  [24/42], [94mLoss[0m : 2.50851
[1mStep[0m  [28/42], [94mLoss[0m : 2.63803
[1mStep[0m  [32/42], [94mLoss[0m : 2.43623
[1mStep[0m  [36/42], [94mLoss[0m : 2.38247
[1mStep[0m  [40/42], [94mLoss[0m : 2.23774

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.556, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.13361
[1mStep[0m  [4/42], [94mLoss[0m : 2.49679
[1mStep[0m  [8/42], [94mLoss[0m : 2.39381
[1mStep[0m  [12/42], [94mLoss[0m : 2.30816
[1mStep[0m  [16/42], [94mLoss[0m : 2.51255
[1mStep[0m  [20/42], [94mLoss[0m : 2.43121
[1mStep[0m  [24/42], [94mLoss[0m : 2.51819
[1mStep[0m  [28/42], [94mLoss[0m : 2.38241
[1mStep[0m  [32/42], [94mLoss[0m : 2.28575
[1mStep[0m  [36/42], [94mLoss[0m : 2.41222
[1mStep[0m  [40/42], [94mLoss[0m : 2.37584

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.493, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27771
[1mStep[0m  [4/42], [94mLoss[0m : 2.19673
[1mStep[0m  [8/42], [94mLoss[0m : 2.55762
[1mStep[0m  [12/42], [94mLoss[0m : 2.21060
[1mStep[0m  [16/42], [94mLoss[0m : 2.38736
[1mStep[0m  [20/42], [94mLoss[0m : 2.33197
[1mStep[0m  [24/42], [94mLoss[0m : 2.54589
[1mStep[0m  [28/42], [94mLoss[0m : 2.60893
[1mStep[0m  [32/42], [94mLoss[0m : 2.18383
[1mStep[0m  [36/42], [94mLoss[0m : 2.34686
[1mStep[0m  [40/42], [94mLoss[0m : 2.51083

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.345, [92mTest[0m: 2.404, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.25650
[1mStep[0m  [4/42], [94mLoss[0m : 2.22433
[1mStep[0m  [8/42], [94mLoss[0m : 2.17656
[1mStep[0m  [12/42], [94mLoss[0m : 2.25465
[1mStep[0m  [16/42], [94mLoss[0m : 2.29592
[1mStep[0m  [20/42], [94mLoss[0m : 2.16400
[1mStep[0m  [24/42], [94mLoss[0m : 2.26370
[1mStep[0m  [28/42], [94mLoss[0m : 2.24645
[1mStep[0m  [32/42], [94mLoss[0m : 2.46628
[1mStep[0m  [36/42], [94mLoss[0m : 2.34075
[1mStep[0m  [40/42], [94mLoss[0m : 2.16845

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.296, [92mTest[0m: 2.420, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.19242
[1mStep[0m  [4/42], [94mLoss[0m : 2.12108
[1mStep[0m  [8/42], [94mLoss[0m : 2.24689
[1mStep[0m  [12/42], [94mLoss[0m : 2.11198
[1mStep[0m  [16/42], [94mLoss[0m : 2.10450
[1mStep[0m  [20/42], [94mLoss[0m : 2.19159
[1mStep[0m  [24/42], [94mLoss[0m : 2.17541
[1mStep[0m  [28/42], [94mLoss[0m : 2.30299
[1mStep[0m  [32/42], [94mLoss[0m : 2.38295
[1mStep[0m  [36/42], [94mLoss[0m : 2.32284
[1mStep[0m  [40/42], [94mLoss[0m : 2.22398

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.219, [92mTest[0m: 2.436, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.03787
[1mStep[0m  [4/42], [94mLoss[0m : 1.95227
[1mStep[0m  [8/42], [94mLoss[0m : 2.22414
[1mStep[0m  [12/42], [94mLoss[0m : 2.16422
[1mStep[0m  [16/42], [94mLoss[0m : 2.39984
[1mStep[0m  [20/42], [94mLoss[0m : 1.97195
[1mStep[0m  [24/42], [94mLoss[0m : 2.03640
[1mStep[0m  [28/42], [94mLoss[0m : 2.11810
[1mStep[0m  [32/42], [94mLoss[0m : 2.28178
[1mStep[0m  [36/42], [94mLoss[0m : 2.26832
[1mStep[0m  [40/42], [94mLoss[0m : 2.30683

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.202, [92mTest[0m: 2.454, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.05745
[1mStep[0m  [4/42], [94mLoss[0m : 2.15867
[1mStep[0m  [8/42], [94mLoss[0m : 2.09449
[1mStep[0m  [12/42], [94mLoss[0m : 2.15750
[1mStep[0m  [16/42], [94mLoss[0m : 2.10210
[1mStep[0m  [20/42], [94mLoss[0m : 2.17467
[1mStep[0m  [24/42], [94mLoss[0m : 2.16420
[1mStep[0m  [28/42], [94mLoss[0m : 2.33432
[1mStep[0m  [32/42], [94mLoss[0m : 2.09714
[1mStep[0m  [36/42], [94mLoss[0m : 2.49796
[1mStep[0m  [40/42], [94mLoss[0m : 2.27946

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.143, [92mTest[0m: 2.450, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.18054
[1mStep[0m  [4/42], [94mLoss[0m : 2.03146
[1mStep[0m  [8/42], [94mLoss[0m : 2.10846
[1mStep[0m  [12/42], [94mLoss[0m : 2.10074
[1mStep[0m  [16/42], [94mLoss[0m : 2.14834
[1mStep[0m  [20/42], [94mLoss[0m : 2.15077
[1mStep[0m  [24/42], [94mLoss[0m : 2.28517
[1mStep[0m  [28/42], [94mLoss[0m : 2.13599
[1mStep[0m  [32/42], [94mLoss[0m : 2.02820
[1mStep[0m  [36/42], [94mLoss[0m : 2.18694
[1mStep[0m  [40/42], [94mLoss[0m : 2.05233

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.117, [92mTest[0m: 2.411, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.16700
[1mStep[0m  [4/42], [94mLoss[0m : 1.80187
[1mStep[0m  [8/42], [94mLoss[0m : 1.85228
[1mStep[0m  [12/42], [94mLoss[0m : 1.91800
[1mStep[0m  [16/42], [94mLoss[0m : 2.16025
[1mStep[0m  [20/42], [94mLoss[0m : 2.01237
[1mStep[0m  [24/42], [94mLoss[0m : 1.98645
[1mStep[0m  [28/42], [94mLoss[0m : 1.91733
[1mStep[0m  [32/42], [94mLoss[0m : 2.08417
[1mStep[0m  [36/42], [94mLoss[0m : 2.02810
[1mStep[0m  [40/42], [94mLoss[0m : 2.10722

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.069, [92mTest[0m: 2.407, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.91470
[1mStep[0m  [4/42], [94mLoss[0m : 2.00862
[1mStep[0m  [8/42], [94mLoss[0m : 2.15096
[1mStep[0m  [12/42], [94mLoss[0m : 1.98653
[1mStep[0m  [16/42], [94mLoss[0m : 2.15591
[1mStep[0m  [20/42], [94mLoss[0m : 1.99351
[1mStep[0m  [24/42], [94mLoss[0m : 2.16264
[1mStep[0m  [28/42], [94mLoss[0m : 2.25441
[1mStep[0m  [32/42], [94mLoss[0m : 2.14412
[1mStep[0m  [36/42], [94mLoss[0m : 1.88741
[1mStep[0m  [40/42], [94mLoss[0m : 2.04906

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.053, [92mTest[0m: 2.422, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.95214
[1mStep[0m  [4/42], [94mLoss[0m : 2.14399
[1mStep[0m  [8/42], [94mLoss[0m : 1.87064
[1mStep[0m  [12/42], [94mLoss[0m : 1.99608
[1mStep[0m  [16/42], [94mLoss[0m : 2.02247
[1mStep[0m  [20/42], [94mLoss[0m : 1.85849
[1mStep[0m  [24/42], [94mLoss[0m : 1.96141
[1mStep[0m  [28/42], [94mLoss[0m : 2.03428
[1mStep[0m  [32/42], [94mLoss[0m : 2.03327
[1mStep[0m  [36/42], [94mLoss[0m : 2.06997
[1mStep[0m  [40/42], [94mLoss[0m : 1.94427

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.015, [92mTest[0m: 2.461, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.04676
[1mStep[0m  [4/42], [94mLoss[0m : 1.90967
[1mStep[0m  [8/42], [94mLoss[0m : 1.81891
[1mStep[0m  [12/42], [94mLoss[0m : 2.11515
[1mStep[0m  [16/42], [94mLoss[0m : 2.08343
[1mStep[0m  [20/42], [94mLoss[0m : 2.10347
[1mStep[0m  [24/42], [94mLoss[0m : 2.10880
[1mStep[0m  [28/42], [94mLoss[0m : 2.08037
[1mStep[0m  [32/42], [94mLoss[0m : 2.06473
[1mStep[0m  [36/42], [94mLoss[0m : 2.01483
[1mStep[0m  [40/42], [94mLoss[0m : 2.05278

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.024, [92mTest[0m: 2.507, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.82848
[1mStep[0m  [4/42], [94mLoss[0m : 1.92010
[1mStep[0m  [8/42], [94mLoss[0m : 1.89550
[1mStep[0m  [12/42], [94mLoss[0m : 2.01125
[1mStep[0m  [16/42], [94mLoss[0m : 2.08737
[1mStep[0m  [20/42], [94mLoss[0m : 2.03505
[1mStep[0m  [24/42], [94mLoss[0m : 2.08985
[1mStep[0m  [28/42], [94mLoss[0m : 2.00814
[1mStep[0m  [32/42], [94mLoss[0m : 2.00832
[1mStep[0m  [36/42], [94mLoss[0m : 2.03077
[1mStep[0m  [40/42], [94mLoss[0m : 2.01685

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.979, [92mTest[0m: 2.432, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.92661
[1mStep[0m  [4/42], [94mLoss[0m : 1.87054
[1mStep[0m  [8/42], [94mLoss[0m : 1.83372
[1mStep[0m  [12/42], [94mLoss[0m : 1.93178
[1mStep[0m  [16/42], [94mLoss[0m : 1.95614
[1mStep[0m  [20/42], [94mLoss[0m : 2.01290
[1mStep[0m  [24/42], [94mLoss[0m : 2.09227
[1mStep[0m  [28/42], [94mLoss[0m : 2.07110
[1mStep[0m  [32/42], [94mLoss[0m : 2.11047
[1mStep[0m  [36/42], [94mLoss[0m : 1.85983
[1mStep[0m  [40/42], [94mLoss[0m : 1.67387

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.966, [92mTest[0m: 2.480, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.93322
[1mStep[0m  [4/42], [94mLoss[0m : 1.84629
[1mStep[0m  [8/42], [94mLoss[0m : 2.16559
[1mStep[0m  [12/42], [94mLoss[0m : 1.73611
[1mStep[0m  [16/42], [94mLoss[0m : 1.81359
[1mStep[0m  [20/42], [94mLoss[0m : 1.91844
[1mStep[0m  [24/42], [94mLoss[0m : 1.90933
[1mStep[0m  [28/42], [94mLoss[0m : 1.98515
[1mStep[0m  [32/42], [94mLoss[0m : 2.08872
[1mStep[0m  [36/42], [94mLoss[0m : 1.98619
[1mStep[0m  [40/42], [94mLoss[0m : 1.89893

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.946, [92mTest[0m: 2.507, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.70265
[1mStep[0m  [4/42], [94mLoss[0m : 1.73590
[1mStep[0m  [8/42], [94mLoss[0m : 1.93396
[1mStep[0m  [12/42], [94mLoss[0m : 1.81398
[1mStep[0m  [16/42], [94mLoss[0m : 1.84174
[1mStep[0m  [20/42], [94mLoss[0m : 1.95062
[1mStep[0m  [24/42], [94mLoss[0m : 1.94671
[1mStep[0m  [28/42], [94mLoss[0m : 2.16113
[1mStep[0m  [32/42], [94mLoss[0m : 2.07445
[1mStep[0m  [36/42], [94mLoss[0m : 1.86537
[1mStep[0m  [40/42], [94mLoss[0m : 1.92912

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.940, [92mTest[0m: 2.486, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.77498
[1mStep[0m  [4/42], [94mLoss[0m : 1.93459
[1mStep[0m  [8/42], [94mLoss[0m : 1.70260
[1mStep[0m  [12/42], [94mLoss[0m : 1.84016
[1mStep[0m  [16/42], [94mLoss[0m : 2.10376
[1mStep[0m  [20/42], [94mLoss[0m : 1.76136
[1mStep[0m  [24/42], [94mLoss[0m : 2.05164
[1mStep[0m  [28/42], [94mLoss[0m : 2.01502
[1mStep[0m  [32/42], [94mLoss[0m : 1.98531
[1mStep[0m  [36/42], [94mLoss[0m : 2.03331
[1mStep[0m  [40/42], [94mLoss[0m : 1.96668

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.928, [92mTest[0m: 2.488, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.81004
[1mStep[0m  [4/42], [94mLoss[0m : 2.15846
[1mStep[0m  [8/42], [94mLoss[0m : 2.01637
[1mStep[0m  [12/42], [94mLoss[0m : 1.77498
[1mStep[0m  [16/42], [94mLoss[0m : 1.94372
[1mStep[0m  [20/42], [94mLoss[0m : 1.99668
[1mStep[0m  [24/42], [94mLoss[0m : 2.11806
[1mStep[0m  [28/42], [94mLoss[0m : 1.82323
[1mStep[0m  [32/42], [94mLoss[0m : 1.90846
[1mStep[0m  [36/42], [94mLoss[0m : 1.82536
[1mStep[0m  [40/42], [94mLoss[0m : 2.06603

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.932, [92mTest[0m: 2.470, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.95905
[1mStep[0m  [4/42], [94mLoss[0m : 1.85811
[1mStep[0m  [8/42], [94mLoss[0m : 1.83196
[1mStep[0m  [12/42], [94mLoss[0m : 1.65136
[1mStep[0m  [16/42], [94mLoss[0m : 1.81210
[1mStep[0m  [20/42], [94mLoss[0m : 1.91270
[1mStep[0m  [24/42], [94mLoss[0m : 2.00751
[1mStep[0m  [28/42], [94mLoss[0m : 1.93762
[1mStep[0m  [32/42], [94mLoss[0m : 1.90291
[1mStep[0m  [36/42], [94mLoss[0m : 1.99676
[1mStep[0m  [40/42], [94mLoss[0m : 1.78121

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.897, [92mTest[0m: 2.459, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.69037
[1mStep[0m  [4/42], [94mLoss[0m : 1.87289
[1mStep[0m  [8/42], [94mLoss[0m : 1.96792
[1mStep[0m  [12/42], [94mLoss[0m : 1.88517
[1mStep[0m  [16/42], [94mLoss[0m : 2.01763
[1mStep[0m  [20/42], [94mLoss[0m : 1.70164
[1mStep[0m  [24/42], [94mLoss[0m : 1.83458
[1mStep[0m  [28/42], [94mLoss[0m : 1.88933
[1mStep[0m  [32/42], [94mLoss[0m : 1.84087
[1mStep[0m  [36/42], [94mLoss[0m : 1.98336
[1mStep[0m  [40/42], [94mLoss[0m : 1.94450

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.887, [92mTest[0m: 2.456, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.69799
[1mStep[0m  [4/42], [94mLoss[0m : 1.76151
[1mStep[0m  [8/42], [94mLoss[0m : 1.94495
[1mStep[0m  [12/42], [94mLoss[0m : 1.94614
[1mStep[0m  [16/42], [94mLoss[0m : 1.86900
[1mStep[0m  [20/42], [94mLoss[0m : 1.87442
[1mStep[0m  [24/42], [94mLoss[0m : 1.75504
[1mStep[0m  [28/42], [94mLoss[0m : 1.71817
[1mStep[0m  [32/42], [94mLoss[0m : 1.82112
[1mStep[0m  [36/42], [94mLoss[0m : 1.91567
[1mStep[0m  [40/42], [94mLoss[0m : 2.04307

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.865, [92mTest[0m: 2.473, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.73530
[1mStep[0m  [4/42], [94mLoss[0m : 1.83010
[1mStep[0m  [8/42], [94mLoss[0m : 1.86374
[1mStep[0m  [12/42], [94mLoss[0m : 1.78481
[1mStep[0m  [16/42], [94mLoss[0m : 1.82188
[1mStep[0m  [20/42], [94mLoss[0m : 1.85253
[1mStep[0m  [24/42], [94mLoss[0m : 1.90585
[1mStep[0m  [28/42], [94mLoss[0m : 1.68937
[1mStep[0m  [32/42], [94mLoss[0m : 1.91887
[1mStep[0m  [36/42], [94mLoss[0m : 1.82879
[1mStep[0m  [40/42], [94mLoss[0m : 1.95652

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.869, [92mTest[0m: 2.481, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.85248
[1mStep[0m  [4/42], [94mLoss[0m : 1.85943
[1mStep[0m  [8/42], [94mLoss[0m : 1.82523
[1mStep[0m  [12/42], [94mLoss[0m : 1.74441
[1mStep[0m  [16/42], [94mLoss[0m : 1.98098
[1mStep[0m  [20/42], [94mLoss[0m : 1.91987
[1mStep[0m  [24/42], [94mLoss[0m : 1.83494
[1mStep[0m  [28/42], [94mLoss[0m : 1.81830
[1mStep[0m  [32/42], [94mLoss[0m : 1.94883
[1mStep[0m  [36/42], [94mLoss[0m : 1.68435
[1mStep[0m  [40/42], [94mLoss[0m : 1.68341

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.825, [92mTest[0m: 2.469, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.81545
[1mStep[0m  [4/42], [94mLoss[0m : 1.80224
[1mStep[0m  [8/42], [94mLoss[0m : 1.80944
[1mStep[0m  [12/42], [94mLoss[0m : 1.88265
[1mStep[0m  [16/42], [94mLoss[0m : 1.82554
[1mStep[0m  [20/42], [94mLoss[0m : 1.82575
[1mStep[0m  [24/42], [94mLoss[0m : 1.89208
[1mStep[0m  [28/42], [94mLoss[0m : 1.86465
[1mStep[0m  [32/42], [94mLoss[0m : 1.85926
[1mStep[0m  [36/42], [94mLoss[0m : 1.80538
[1mStep[0m  [40/42], [94mLoss[0m : 1.90331

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.844, [92mTest[0m: 2.500, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.68822
[1mStep[0m  [4/42], [94mLoss[0m : 1.77129
[1mStep[0m  [8/42], [94mLoss[0m : 1.77131
[1mStep[0m  [12/42], [94mLoss[0m : 1.71072
[1mStep[0m  [16/42], [94mLoss[0m : 1.86809
[1mStep[0m  [20/42], [94mLoss[0m : 1.86980
[1mStep[0m  [24/42], [94mLoss[0m : 1.84163
[1mStep[0m  [28/42], [94mLoss[0m : 1.76633
[1mStep[0m  [32/42], [94mLoss[0m : 1.83872
[1mStep[0m  [36/42], [94mLoss[0m : 1.67584
[1mStep[0m  [40/42], [94mLoss[0m : 1.91229

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.807, [92mTest[0m: 2.500, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.81661
[1mStep[0m  [4/42], [94mLoss[0m : 1.64955
[1mStep[0m  [8/42], [94mLoss[0m : 1.73414
[1mStep[0m  [12/42], [94mLoss[0m : 1.90527
[1mStep[0m  [16/42], [94mLoss[0m : 1.93361
[1mStep[0m  [20/42], [94mLoss[0m : 1.95958
[1mStep[0m  [24/42], [94mLoss[0m : 1.79785
[1mStep[0m  [28/42], [94mLoss[0m : 1.71613
[1mStep[0m  [32/42], [94mLoss[0m : 1.88914
[1mStep[0m  [36/42], [94mLoss[0m : 1.80338
[1mStep[0m  [40/42], [94mLoss[0m : 1.82566

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.791, [92mTest[0m: 2.448, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.69769
[1mStep[0m  [4/42], [94mLoss[0m : 1.73437
[1mStep[0m  [8/42], [94mLoss[0m : 1.82941
[1mStep[0m  [12/42], [94mLoss[0m : 1.75193
[1mStep[0m  [16/42], [94mLoss[0m : 1.79574
[1mStep[0m  [20/42], [94mLoss[0m : 2.00082
[1mStep[0m  [24/42], [94mLoss[0m : 1.87295
[1mStep[0m  [28/42], [94mLoss[0m : 1.76487
[1mStep[0m  [32/42], [94mLoss[0m : 1.89487
[1mStep[0m  [36/42], [94mLoss[0m : 1.90862
[1mStep[0m  [40/42], [94mLoss[0m : 2.08492

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.817, [92mTest[0m: 2.490, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.74219
[1mStep[0m  [4/42], [94mLoss[0m : 1.71262
[1mStep[0m  [8/42], [94mLoss[0m : 1.74567
[1mStep[0m  [12/42], [94mLoss[0m : 1.75069
[1mStep[0m  [16/42], [94mLoss[0m : 1.92932
[1mStep[0m  [20/42], [94mLoss[0m : 1.71141
[1mStep[0m  [24/42], [94mLoss[0m : 1.80875
[1mStep[0m  [28/42], [94mLoss[0m : 1.74049
[1mStep[0m  [32/42], [94mLoss[0m : 1.84036
[1mStep[0m  [36/42], [94mLoss[0m : 1.89645
[1mStep[0m  [40/42], [94mLoss[0m : 1.83158

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.823, [92mTest[0m: 2.457, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.88290
[1mStep[0m  [4/42], [94mLoss[0m : 1.78556
[1mStep[0m  [8/42], [94mLoss[0m : 1.70324
[1mStep[0m  [12/42], [94mLoss[0m : 1.66553
[1mStep[0m  [16/42], [94mLoss[0m : 1.83071
[1mStep[0m  [20/42], [94mLoss[0m : 1.81054
[1mStep[0m  [24/42], [94mLoss[0m : 1.86190
[1mStep[0m  [28/42], [94mLoss[0m : 1.91259
[1mStep[0m  [32/42], [94mLoss[0m : 1.76064
[1mStep[0m  [36/42], [94mLoss[0m : 1.69871
[1mStep[0m  [40/42], [94mLoss[0m : 1.93024

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.795, [92mTest[0m: 2.485, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.495
====================================

Phase 2 - Evaluation MAE:  2.495220116206578
MAE score P1        2.355628
MAE score P2         2.49522
loss                1.791299
learning_rate       0.007525
batch_size               256
hidden_sizes      [300, 100]
epochs                    30
activation           sigmoid
optimizer                sgd
early stopping         False
dropout                  0.4
momentum                 0.9
weight_decay            0.01
Name: 29, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 10.91223
[1mStep[0m  [4/42], [94mLoss[0m : 9.97523
[1mStep[0m  [8/42], [94mLoss[0m : 7.91388
[1mStep[0m  [12/42], [94mLoss[0m : 6.58890
[1mStep[0m  [16/42], [94mLoss[0m : 5.54054
[1mStep[0m  [20/42], [94mLoss[0m : 4.85531
[1mStep[0m  [24/42], [94mLoss[0m : 3.96364
[1mStep[0m  [28/42], [94mLoss[0m : 3.35472
[1mStep[0m  [32/42], [94mLoss[0m : 3.06257
[1mStep[0m  [36/42], [94mLoss[0m : 2.81247
[1mStep[0m  [40/42], [94mLoss[0m : 2.77697

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 5.458, [92mTest[0m: 10.965, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47730
[1mStep[0m  [4/42], [94mLoss[0m : 2.85215
[1mStep[0m  [8/42], [94mLoss[0m : 2.72678
[1mStep[0m  [12/42], [94mLoss[0m : 2.64157
[1mStep[0m  [16/42], [94mLoss[0m : 2.63012
[1mStep[0m  [20/42], [94mLoss[0m : 2.67527
[1mStep[0m  [24/42], [94mLoss[0m : 2.54863
[1mStep[0m  [28/42], [94mLoss[0m : 2.38225
[1mStep[0m  [32/42], [94mLoss[0m : 2.52303
[1mStep[0m  [36/42], [94mLoss[0m : 2.75650
[1mStep[0m  [40/42], [94mLoss[0m : 2.58914

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.685, [92mTest[0m: 3.370, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57541
[1mStep[0m  [4/42], [94mLoss[0m : 2.61575
[1mStep[0m  [8/42], [94mLoss[0m : 2.71824
[1mStep[0m  [12/42], [94mLoss[0m : 2.63658
[1mStep[0m  [16/42], [94mLoss[0m : 2.50045
[1mStep[0m  [20/42], [94mLoss[0m : 2.47648
[1mStep[0m  [24/42], [94mLoss[0m : 2.55788
[1mStep[0m  [28/42], [94mLoss[0m : 2.40430
[1mStep[0m  [32/42], [94mLoss[0m : 2.84228
[1mStep[0m  [36/42], [94mLoss[0m : 2.68607
[1mStep[0m  [40/42], [94mLoss[0m : 2.65662

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.584, [92mTest[0m: 2.722, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51173
[1mStep[0m  [4/42], [94mLoss[0m : 2.55463
[1mStep[0m  [8/42], [94mLoss[0m : 2.58376
[1mStep[0m  [12/42], [94mLoss[0m : 2.74061
[1mStep[0m  [16/42], [94mLoss[0m : 2.70379
[1mStep[0m  [20/42], [94mLoss[0m : 2.47502
[1mStep[0m  [24/42], [94mLoss[0m : 2.51753
[1mStep[0m  [28/42], [94mLoss[0m : 2.44451
[1mStep[0m  [32/42], [94mLoss[0m : 2.54209
[1mStep[0m  [36/42], [94mLoss[0m : 2.69460
[1mStep[0m  [40/42], [94mLoss[0m : 2.52219

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.571, [92mTest[0m: 2.598, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.67536
[1mStep[0m  [4/42], [94mLoss[0m : 2.68281
[1mStep[0m  [8/42], [94mLoss[0m : 2.71941
[1mStep[0m  [12/42], [94mLoss[0m : 2.56605
[1mStep[0m  [16/42], [94mLoss[0m : 2.52662
[1mStep[0m  [20/42], [94mLoss[0m : 2.60304
[1mStep[0m  [24/42], [94mLoss[0m : 2.59973
[1mStep[0m  [28/42], [94mLoss[0m : 2.36069
[1mStep[0m  [32/42], [94mLoss[0m : 2.84075
[1mStep[0m  [36/42], [94mLoss[0m : 2.72083
[1mStep[0m  [40/42], [94mLoss[0m : 2.30933

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.535, [92mTest[0m: 2.602, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50533
[1mStep[0m  [4/42], [94mLoss[0m : 2.34511
[1mStep[0m  [8/42], [94mLoss[0m : 2.39825
[1mStep[0m  [12/42], [94mLoss[0m : 2.45448
[1mStep[0m  [16/42], [94mLoss[0m : 2.20830
[1mStep[0m  [20/42], [94mLoss[0m : 2.53825
[1mStep[0m  [24/42], [94mLoss[0m : 2.62856
[1mStep[0m  [28/42], [94mLoss[0m : 2.64632
[1mStep[0m  [32/42], [94mLoss[0m : 2.46099
[1mStep[0m  [36/42], [94mLoss[0m : 2.43751
[1mStep[0m  [40/42], [94mLoss[0m : 2.74297

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.588, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48907
[1mStep[0m  [4/42], [94mLoss[0m : 2.64847
[1mStep[0m  [8/42], [94mLoss[0m : 2.51646
[1mStep[0m  [12/42], [94mLoss[0m : 2.32361
[1mStep[0m  [16/42], [94mLoss[0m : 2.54032
[1mStep[0m  [20/42], [94mLoss[0m : 2.50934
[1mStep[0m  [24/42], [94mLoss[0m : 2.63276
[1mStep[0m  [28/42], [94mLoss[0m : 2.47334
[1mStep[0m  [32/42], [94mLoss[0m : 2.48207
[1mStep[0m  [36/42], [94mLoss[0m : 2.44638
[1mStep[0m  [40/42], [94mLoss[0m : 2.63897

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.545, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38665
[1mStep[0m  [4/42], [94mLoss[0m : 2.65264
[1mStep[0m  [8/42], [94mLoss[0m : 2.48411
[1mStep[0m  [12/42], [94mLoss[0m : 2.43162
[1mStep[0m  [16/42], [94mLoss[0m : 2.34155
[1mStep[0m  [20/42], [94mLoss[0m : 2.50991
[1mStep[0m  [24/42], [94mLoss[0m : 2.43371
[1mStep[0m  [28/42], [94mLoss[0m : 2.39030
[1mStep[0m  [32/42], [94mLoss[0m : 2.50517
[1mStep[0m  [36/42], [94mLoss[0m : 2.48534
[1mStep[0m  [40/42], [94mLoss[0m : 2.50988

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.581, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59439
[1mStep[0m  [4/42], [94mLoss[0m : 2.57971
[1mStep[0m  [8/42], [94mLoss[0m : 2.55588
[1mStep[0m  [12/42], [94mLoss[0m : 2.69722
[1mStep[0m  [16/42], [94mLoss[0m : 2.50365
[1mStep[0m  [20/42], [94mLoss[0m : 2.72528
[1mStep[0m  [24/42], [94mLoss[0m : 2.41489
[1mStep[0m  [28/42], [94mLoss[0m : 2.41357
[1mStep[0m  [32/42], [94mLoss[0m : 2.55091
[1mStep[0m  [36/42], [94mLoss[0m : 2.31178
[1mStep[0m  [40/42], [94mLoss[0m : 2.36752

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.547, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29765
[1mStep[0m  [4/42], [94mLoss[0m : 2.71020
[1mStep[0m  [8/42], [94mLoss[0m : 2.66362
[1mStep[0m  [12/42], [94mLoss[0m : 2.52470
[1mStep[0m  [16/42], [94mLoss[0m : 2.44459
[1mStep[0m  [20/42], [94mLoss[0m : 2.56367
[1mStep[0m  [24/42], [94mLoss[0m : 2.26017
[1mStep[0m  [28/42], [94mLoss[0m : 2.68514
[1mStep[0m  [32/42], [94mLoss[0m : 2.50451
[1mStep[0m  [36/42], [94mLoss[0m : 2.54418
[1mStep[0m  [40/42], [94mLoss[0m : 2.55866

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.552, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38097
[1mStep[0m  [4/42], [94mLoss[0m : 2.65111
[1mStep[0m  [8/42], [94mLoss[0m : 2.48907
[1mStep[0m  [12/42], [94mLoss[0m : 2.45982
[1mStep[0m  [16/42], [94mLoss[0m : 2.53622
[1mStep[0m  [20/42], [94mLoss[0m : 2.36257
[1mStep[0m  [24/42], [94mLoss[0m : 2.45538
[1mStep[0m  [28/42], [94mLoss[0m : 2.27142
[1mStep[0m  [32/42], [94mLoss[0m : 2.49958
[1mStep[0m  [36/42], [94mLoss[0m : 2.44232
[1mStep[0m  [40/42], [94mLoss[0m : 2.53220

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.546, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32526
[1mStep[0m  [4/42], [94mLoss[0m : 2.59741
[1mStep[0m  [8/42], [94mLoss[0m : 2.54424
[1mStep[0m  [12/42], [94mLoss[0m : 2.27363
[1mStep[0m  [16/42], [94mLoss[0m : 2.42970
[1mStep[0m  [20/42], [94mLoss[0m : 2.37346
[1mStep[0m  [24/42], [94mLoss[0m : 2.46048
[1mStep[0m  [28/42], [94mLoss[0m : 2.51481
[1mStep[0m  [32/42], [94mLoss[0m : 2.62251
[1mStep[0m  [36/42], [94mLoss[0m : 2.64267
[1mStep[0m  [40/42], [94mLoss[0m : 2.34977

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.549, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.30720
[1mStep[0m  [4/42], [94mLoss[0m : 2.59612
[1mStep[0m  [8/42], [94mLoss[0m : 2.54136
[1mStep[0m  [12/42], [94mLoss[0m : 2.46708
[1mStep[0m  [16/42], [94mLoss[0m : 2.43023
[1mStep[0m  [20/42], [94mLoss[0m : 2.33991
[1mStep[0m  [24/42], [94mLoss[0m : 2.32673
[1mStep[0m  [28/42], [94mLoss[0m : 2.65470
[1mStep[0m  [32/42], [94mLoss[0m : 2.51807
[1mStep[0m  [36/42], [94mLoss[0m : 2.51078
[1mStep[0m  [40/42], [94mLoss[0m : 2.39238

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.568, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44900
[1mStep[0m  [4/42], [94mLoss[0m : 2.58938
[1mStep[0m  [8/42], [94mLoss[0m : 2.58648
[1mStep[0m  [12/42], [94mLoss[0m : 2.71647
[1mStep[0m  [16/42], [94mLoss[0m : 2.53577
[1mStep[0m  [20/42], [94mLoss[0m : 2.42295
[1mStep[0m  [24/42], [94mLoss[0m : 2.40564
[1mStep[0m  [28/42], [94mLoss[0m : 2.39491
[1mStep[0m  [32/42], [94mLoss[0m : 2.30976
[1mStep[0m  [36/42], [94mLoss[0m : 2.45486
[1mStep[0m  [40/42], [94mLoss[0m : 2.36026

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.521, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45621
[1mStep[0m  [4/42], [94mLoss[0m : 2.23705
[1mStep[0m  [8/42], [94mLoss[0m : 2.29135
[1mStep[0m  [12/42], [94mLoss[0m : 2.49461
[1mStep[0m  [16/42], [94mLoss[0m : 2.71382
[1mStep[0m  [20/42], [94mLoss[0m : 2.62094
[1mStep[0m  [24/42], [94mLoss[0m : 2.36884
[1mStep[0m  [28/42], [94mLoss[0m : 2.30135
[1mStep[0m  [32/42], [94mLoss[0m : 2.49010
[1mStep[0m  [36/42], [94mLoss[0m : 2.46592
[1mStep[0m  [40/42], [94mLoss[0m : 2.45483

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.544, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31440
[1mStep[0m  [4/42], [94mLoss[0m : 2.38890
[1mStep[0m  [8/42], [94mLoss[0m : 2.52938
[1mStep[0m  [12/42], [94mLoss[0m : 2.37986
[1mStep[0m  [16/42], [94mLoss[0m : 2.57495
[1mStep[0m  [20/42], [94mLoss[0m : 2.46440
[1mStep[0m  [24/42], [94mLoss[0m : 2.36997
[1mStep[0m  [28/42], [94mLoss[0m : 2.65667
[1mStep[0m  [32/42], [94mLoss[0m : 2.47858
[1mStep[0m  [36/42], [94mLoss[0m : 2.62241
[1mStep[0m  [40/42], [94mLoss[0m : 2.54377

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.503, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.76047
[1mStep[0m  [4/42], [94mLoss[0m : 2.67439
[1mStep[0m  [8/42], [94mLoss[0m : 2.35016
[1mStep[0m  [12/42], [94mLoss[0m : 2.39847
[1mStep[0m  [16/42], [94mLoss[0m : 2.32102
[1mStep[0m  [20/42], [94mLoss[0m : 2.47751
[1mStep[0m  [24/42], [94mLoss[0m : 2.51006
[1mStep[0m  [28/42], [94mLoss[0m : 2.42882
[1mStep[0m  [32/42], [94mLoss[0m : 2.27865
[1mStep[0m  [36/42], [94mLoss[0m : 2.49551
[1mStep[0m  [40/42], [94mLoss[0m : 2.47019

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.477, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38921
[1mStep[0m  [4/42], [94mLoss[0m : 2.16538
[1mStep[0m  [8/42], [94mLoss[0m : 2.56440
[1mStep[0m  [12/42], [94mLoss[0m : 2.40442
[1mStep[0m  [16/42], [94mLoss[0m : 2.34900
[1mStep[0m  [20/42], [94mLoss[0m : 2.19751
[1mStep[0m  [24/42], [94mLoss[0m : 2.35717
[1mStep[0m  [28/42], [94mLoss[0m : 2.39404
[1mStep[0m  [32/42], [94mLoss[0m : 2.55643
[1mStep[0m  [36/42], [94mLoss[0m : 2.55215
[1mStep[0m  [40/42], [94mLoss[0m : 2.40114

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.497, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64067
[1mStep[0m  [4/42], [94mLoss[0m : 2.28347
[1mStep[0m  [8/42], [94mLoss[0m : 2.64529
[1mStep[0m  [12/42], [94mLoss[0m : 2.14593
[1mStep[0m  [16/42], [94mLoss[0m : 2.46721
[1mStep[0m  [20/42], [94mLoss[0m : 2.48840
[1mStep[0m  [24/42], [94mLoss[0m : 2.41264
[1mStep[0m  [28/42], [94mLoss[0m : 2.34006
[1mStep[0m  [32/42], [94mLoss[0m : 2.46368
[1mStep[0m  [36/42], [94mLoss[0m : 2.51279
[1mStep[0m  [40/42], [94mLoss[0m : 2.25642

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.503, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50440
[1mStep[0m  [4/42], [94mLoss[0m : 2.47371
[1mStep[0m  [8/42], [94mLoss[0m : 2.51802
[1mStep[0m  [12/42], [94mLoss[0m : 2.56571
[1mStep[0m  [16/42], [94mLoss[0m : 2.42494
[1mStep[0m  [20/42], [94mLoss[0m : 2.34485
[1mStep[0m  [24/42], [94mLoss[0m : 2.33669
[1mStep[0m  [28/42], [94mLoss[0m : 2.57473
[1mStep[0m  [32/42], [94mLoss[0m : 2.48379
[1mStep[0m  [36/42], [94mLoss[0m : 2.44531
[1mStep[0m  [40/42], [94mLoss[0m : 2.51445

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.522, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40773
[1mStep[0m  [4/42], [94mLoss[0m : 2.42113
[1mStep[0m  [8/42], [94mLoss[0m : 2.45421
[1mStep[0m  [12/42], [94mLoss[0m : 2.66478
[1mStep[0m  [16/42], [94mLoss[0m : 2.33415
[1mStep[0m  [20/42], [94mLoss[0m : 2.33253
[1mStep[0m  [24/42], [94mLoss[0m : 2.33586
[1mStep[0m  [28/42], [94mLoss[0m : 2.58522
[1mStep[0m  [32/42], [94mLoss[0m : 2.46360
[1mStep[0m  [36/42], [94mLoss[0m : 2.25362
[1mStep[0m  [40/42], [94mLoss[0m : 2.57320

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.493, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49117
[1mStep[0m  [4/42], [94mLoss[0m : 2.23109
[1mStep[0m  [8/42], [94mLoss[0m : 2.36641
[1mStep[0m  [12/42], [94mLoss[0m : 2.54063
[1mStep[0m  [16/42], [94mLoss[0m : 2.52626
[1mStep[0m  [20/42], [94mLoss[0m : 2.64710
[1mStep[0m  [24/42], [94mLoss[0m : 2.23816
[1mStep[0m  [28/42], [94mLoss[0m : 2.48049
[1mStep[0m  [32/42], [94mLoss[0m : 2.44758
[1mStep[0m  [36/42], [94mLoss[0m : 2.19913
[1mStep[0m  [40/42], [94mLoss[0m : 2.60198

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.515, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54610
[1mStep[0m  [4/42], [94mLoss[0m : 2.58313
[1mStep[0m  [8/42], [94mLoss[0m : 2.48824
[1mStep[0m  [12/42], [94mLoss[0m : 2.29214
[1mStep[0m  [16/42], [94mLoss[0m : 2.40830
[1mStep[0m  [20/42], [94mLoss[0m : 2.24992
[1mStep[0m  [24/42], [94mLoss[0m : 2.18702
[1mStep[0m  [28/42], [94mLoss[0m : 2.71363
[1mStep[0m  [32/42], [94mLoss[0m : 2.48197
[1mStep[0m  [36/42], [94mLoss[0m : 2.51112
[1mStep[0m  [40/42], [94mLoss[0m : 2.46479

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.495, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54995
[1mStep[0m  [4/42], [94mLoss[0m : 2.26886
[1mStep[0m  [8/42], [94mLoss[0m : 2.60480
[1mStep[0m  [12/42], [94mLoss[0m : 2.34021
[1mStep[0m  [16/42], [94mLoss[0m : 2.44169
[1mStep[0m  [20/42], [94mLoss[0m : 2.31884
[1mStep[0m  [24/42], [94mLoss[0m : 2.40108
[1mStep[0m  [28/42], [94mLoss[0m : 2.71963
[1mStep[0m  [32/42], [94mLoss[0m : 2.55761
[1mStep[0m  [36/42], [94mLoss[0m : 2.54311
[1mStep[0m  [40/42], [94mLoss[0m : 2.30717

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.487, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.30386
[1mStep[0m  [4/42], [94mLoss[0m : 2.56272
[1mStep[0m  [8/42], [94mLoss[0m : 2.46769
[1mStep[0m  [12/42], [94mLoss[0m : 2.48427
[1mStep[0m  [16/42], [94mLoss[0m : 2.38077
[1mStep[0m  [20/42], [94mLoss[0m : 2.72050
[1mStep[0m  [24/42], [94mLoss[0m : 2.35816
[1mStep[0m  [28/42], [94mLoss[0m : 2.42957
[1mStep[0m  [32/42], [94mLoss[0m : 2.28255
[1mStep[0m  [36/42], [94mLoss[0m : 2.40155
[1mStep[0m  [40/42], [94mLoss[0m : 2.38502

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.519, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34853
[1mStep[0m  [4/42], [94mLoss[0m : 2.31969
[1mStep[0m  [8/42], [94mLoss[0m : 2.38973
[1mStep[0m  [12/42], [94mLoss[0m : 2.34683
[1mStep[0m  [16/42], [94mLoss[0m : 2.41292
[1mStep[0m  [20/42], [94mLoss[0m : 2.20633
[1mStep[0m  [24/42], [94mLoss[0m : 2.60666
[1mStep[0m  [28/42], [94mLoss[0m : 2.56846
[1mStep[0m  [32/42], [94mLoss[0m : 2.40947
[1mStep[0m  [36/42], [94mLoss[0m : 2.42115
[1mStep[0m  [40/42], [94mLoss[0m : 2.41897

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.476, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32048
[1mStep[0m  [4/42], [94mLoss[0m : 2.25370
[1mStep[0m  [8/42], [94mLoss[0m : 2.36566
[1mStep[0m  [12/42], [94mLoss[0m : 2.36217
[1mStep[0m  [16/42], [94mLoss[0m : 2.45533
[1mStep[0m  [20/42], [94mLoss[0m : 2.49663
[1mStep[0m  [24/42], [94mLoss[0m : 2.35479
[1mStep[0m  [28/42], [94mLoss[0m : 2.49547
[1mStep[0m  [32/42], [94mLoss[0m : 2.67143
[1mStep[0m  [36/42], [94mLoss[0m : 2.54467
[1mStep[0m  [40/42], [94mLoss[0m : 2.60055

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.497, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39960
[1mStep[0m  [4/42], [94mLoss[0m : 2.41771
[1mStep[0m  [8/42], [94mLoss[0m : 2.32819
[1mStep[0m  [12/42], [94mLoss[0m : 2.39234
[1mStep[0m  [16/42], [94mLoss[0m : 2.47556
[1mStep[0m  [20/42], [94mLoss[0m : 2.68094
[1mStep[0m  [24/42], [94mLoss[0m : 2.13700
[1mStep[0m  [28/42], [94mLoss[0m : 2.33331
[1mStep[0m  [32/42], [94mLoss[0m : 2.49554
[1mStep[0m  [36/42], [94mLoss[0m : 2.44545
[1mStep[0m  [40/42], [94mLoss[0m : 2.60993

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.497, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26248
[1mStep[0m  [4/42], [94mLoss[0m : 2.41506
[1mStep[0m  [8/42], [94mLoss[0m : 2.40532
[1mStep[0m  [12/42], [94mLoss[0m : 2.42095
[1mStep[0m  [16/42], [94mLoss[0m : 2.56615
[1mStep[0m  [20/42], [94mLoss[0m : 2.39246
[1mStep[0m  [24/42], [94mLoss[0m : 2.27698
[1mStep[0m  [28/42], [94mLoss[0m : 2.21060
[1mStep[0m  [32/42], [94mLoss[0m : 2.31947
[1mStep[0m  [36/42], [94mLoss[0m : 2.28810
[1mStep[0m  [40/42], [94mLoss[0m : 2.30520

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.466, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36994
[1mStep[0m  [4/42], [94mLoss[0m : 2.15671
[1mStep[0m  [8/42], [94mLoss[0m : 2.26744
[1mStep[0m  [12/42], [94mLoss[0m : 2.36946
[1mStep[0m  [16/42], [94mLoss[0m : 2.44613
[1mStep[0m  [20/42], [94mLoss[0m : 2.27844
[1mStep[0m  [24/42], [94mLoss[0m : 2.37260
[1mStep[0m  [28/42], [94mLoss[0m : 2.35778
[1mStep[0m  [32/42], [94mLoss[0m : 2.35419
[1mStep[0m  [36/42], [94mLoss[0m : 2.42452
[1mStep[0m  [40/42], [94mLoss[0m : 2.66923

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.467, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.468
====================================

Phase 1 - Evaluation MAE:  2.4678545338766917
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 2.42650
[1mStep[0m  [4/42], [94mLoss[0m : 2.38209
[1mStep[0m  [8/42], [94mLoss[0m : 2.45631
[1mStep[0m  [12/42], [94mLoss[0m : 2.37680
[1mStep[0m  [16/42], [94mLoss[0m : 2.33610
[1mStep[0m  [20/42], [94mLoss[0m : 2.54139
[1mStep[0m  [24/42], [94mLoss[0m : 2.35469
[1mStep[0m  [28/42], [94mLoss[0m : 2.45802
[1mStep[0m  [32/42], [94mLoss[0m : 2.73891
[1mStep[0m  [36/42], [94mLoss[0m : 2.23185
[1mStep[0m  [40/42], [94mLoss[0m : 2.33531

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.472, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56933
[1mStep[0m  [4/42], [94mLoss[0m : 2.41715
[1mStep[0m  [8/42], [94mLoss[0m : 2.40446
[1mStep[0m  [12/42], [94mLoss[0m : 2.49660
[1mStep[0m  [16/42], [94mLoss[0m : 2.46794
[1mStep[0m  [20/42], [94mLoss[0m : 2.44186
[1mStep[0m  [24/42], [94mLoss[0m : 2.30560
[1mStep[0m  [28/42], [94mLoss[0m : 2.39624
[1mStep[0m  [32/42], [94mLoss[0m : 2.32691
[1mStep[0m  [36/42], [94mLoss[0m : 2.42101
[1mStep[0m  [40/42], [94mLoss[0m : 2.38041

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.420, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43356
[1mStep[0m  [4/42], [94mLoss[0m : 2.22951
[1mStep[0m  [8/42], [94mLoss[0m : 2.36395
[1mStep[0m  [12/42], [94mLoss[0m : 2.42128
[1mStep[0m  [16/42], [94mLoss[0m : 2.48195
[1mStep[0m  [20/42], [94mLoss[0m : 2.50990
[1mStep[0m  [24/42], [94mLoss[0m : 2.19141
[1mStep[0m  [28/42], [94mLoss[0m : 2.61134
[1mStep[0m  [32/42], [94mLoss[0m : 2.44140
[1mStep[0m  [36/42], [94mLoss[0m : 2.58955
[1mStep[0m  [40/42], [94mLoss[0m : 2.34553

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.393, [92mTest[0m: 2.394, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41774
[1mStep[0m  [4/42], [94mLoss[0m : 2.26081
[1mStep[0m  [8/42], [94mLoss[0m : 2.32644
[1mStep[0m  [12/42], [94mLoss[0m : 2.50040
[1mStep[0m  [16/42], [94mLoss[0m : 2.36257
[1mStep[0m  [20/42], [94mLoss[0m : 2.37519
[1mStep[0m  [24/42], [94mLoss[0m : 2.52770
[1mStep[0m  [28/42], [94mLoss[0m : 2.02230
[1mStep[0m  [32/42], [94mLoss[0m : 2.34678
[1mStep[0m  [36/42], [94mLoss[0m : 2.30681
[1mStep[0m  [40/42], [94mLoss[0m : 2.51802

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.358, [92mTest[0m: 2.354, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34568
[1mStep[0m  [4/42], [94mLoss[0m : 2.34305
[1mStep[0m  [8/42], [94mLoss[0m : 2.28574
[1mStep[0m  [12/42], [94mLoss[0m : 2.29825
[1mStep[0m  [16/42], [94mLoss[0m : 2.37145
[1mStep[0m  [20/42], [94mLoss[0m : 2.44250
[1mStep[0m  [24/42], [94mLoss[0m : 2.16879
[1mStep[0m  [28/42], [94mLoss[0m : 2.17268
[1mStep[0m  [32/42], [94mLoss[0m : 2.37709
[1mStep[0m  [36/42], [94mLoss[0m : 2.40527
[1mStep[0m  [40/42], [94mLoss[0m : 2.41670

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.319, [92mTest[0m: 2.366, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.19428
[1mStep[0m  [4/42], [94mLoss[0m : 2.43970
[1mStep[0m  [8/42], [94mLoss[0m : 2.53761
[1mStep[0m  [12/42], [94mLoss[0m : 2.26618
[1mStep[0m  [16/42], [94mLoss[0m : 2.45468
[1mStep[0m  [20/42], [94mLoss[0m : 2.14459
[1mStep[0m  [24/42], [94mLoss[0m : 2.36390
[1mStep[0m  [28/42], [94mLoss[0m : 2.28074
[1mStep[0m  [32/42], [94mLoss[0m : 2.26880
[1mStep[0m  [36/42], [94mLoss[0m : 2.37421
[1mStep[0m  [40/42], [94mLoss[0m : 2.43730

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.283, [92mTest[0m: 2.404, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26976
[1mStep[0m  [4/42], [94mLoss[0m : 2.26101
[1mStep[0m  [8/42], [94mLoss[0m : 1.99907
[1mStep[0m  [12/42], [94mLoss[0m : 2.28344
[1mStep[0m  [16/42], [94mLoss[0m : 2.18515
[1mStep[0m  [20/42], [94mLoss[0m : 2.27709
[1mStep[0m  [24/42], [94mLoss[0m : 2.60225
[1mStep[0m  [28/42], [94mLoss[0m : 2.19419
[1mStep[0m  [32/42], [94mLoss[0m : 2.20935
[1mStep[0m  [36/42], [94mLoss[0m : 2.31979
[1mStep[0m  [40/42], [94mLoss[0m : 2.38703

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.268, [92mTest[0m: 2.417, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42558
[1mStep[0m  [4/42], [94mLoss[0m : 2.25800
[1mStep[0m  [8/42], [94mLoss[0m : 2.44120
[1mStep[0m  [12/42], [94mLoss[0m : 2.56453
[1mStep[0m  [16/42], [94mLoss[0m : 2.21877
[1mStep[0m  [20/42], [94mLoss[0m : 2.02647
[1mStep[0m  [24/42], [94mLoss[0m : 2.27265
[1mStep[0m  [28/42], [94mLoss[0m : 2.02514
[1mStep[0m  [32/42], [94mLoss[0m : 2.48200
[1mStep[0m  [36/42], [94mLoss[0m : 2.33741
[1mStep[0m  [40/42], [94mLoss[0m : 2.29024

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.208, [92mTest[0m: 2.360, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.06268
[1mStep[0m  [4/42], [94mLoss[0m : 2.19444
[1mStep[0m  [8/42], [94mLoss[0m : 2.23919
[1mStep[0m  [12/42], [94mLoss[0m : 2.10441
[1mStep[0m  [16/42], [94mLoss[0m : 2.17884
[1mStep[0m  [20/42], [94mLoss[0m : 2.22438
[1mStep[0m  [24/42], [94mLoss[0m : 2.28843
[1mStep[0m  [28/42], [94mLoss[0m : 2.30826
[1mStep[0m  [32/42], [94mLoss[0m : 2.34783
[1mStep[0m  [36/42], [94mLoss[0m : 2.10770
[1mStep[0m  [40/42], [94mLoss[0m : 2.13380

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.170, [92mTest[0m: 2.365, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.04849
[1mStep[0m  [4/42], [94mLoss[0m : 2.27261
[1mStep[0m  [8/42], [94mLoss[0m : 2.02326
[1mStep[0m  [12/42], [94mLoss[0m : 1.92564
[1mStep[0m  [16/42], [94mLoss[0m : 2.44936
[1mStep[0m  [20/42], [94mLoss[0m : 2.28215
[1mStep[0m  [24/42], [94mLoss[0m : 2.03792
[1mStep[0m  [28/42], [94mLoss[0m : 2.36223
[1mStep[0m  [32/42], [94mLoss[0m : 1.92923
[1mStep[0m  [36/42], [94mLoss[0m : 2.00594
[1mStep[0m  [40/42], [94mLoss[0m : 2.13711

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.135, [92mTest[0m: 2.394, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.12490
[1mStep[0m  [4/42], [94mLoss[0m : 1.98477
[1mStep[0m  [8/42], [94mLoss[0m : 2.22526
[1mStep[0m  [12/42], [94mLoss[0m : 2.14413
[1mStep[0m  [16/42], [94mLoss[0m : 2.04703
[1mStep[0m  [20/42], [94mLoss[0m : 2.14182
[1mStep[0m  [24/42], [94mLoss[0m : 1.99349
[1mStep[0m  [28/42], [94mLoss[0m : 1.98195
[1mStep[0m  [32/42], [94mLoss[0m : 2.10827
[1mStep[0m  [36/42], [94mLoss[0m : 2.38459
[1mStep[0m  [40/42], [94mLoss[0m : 2.10995

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.105, [92mTest[0m: 2.395, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.88541
[1mStep[0m  [4/42], [94mLoss[0m : 2.00007
[1mStep[0m  [8/42], [94mLoss[0m : 2.00590
[1mStep[0m  [12/42], [94mLoss[0m : 2.13282
[1mStep[0m  [16/42], [94mLoss[0m : 2.00223
[1mStep[0m  [20/42], [94mLoss[0m : 2.07454
[1mStep[0m  [24/42], [94mLoss[0m : 2.04090
[1mStep[0m  [28/42], [94mLoss[0m : 1.93590
[1mStep[0m  [32/42], [94mLoss[0m : 2.37842
[1mStep[0m  [36/42], [94mLoss[0m : 2.10321
[1mStep[0m  [40/42], [94mLoss[0m : 1.90963

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.070, [92mTest[0m: 2.390, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.97069
[1mStep[0m  [4/42], [94mLoss[0m : 2.03656
[1mStep[0m  [8/42], [94mLoss[0m : 2.23495
[1mStep[0m  [12/42], [94mLoss[0m : 2.13161
[1mStep[0m  [16/42], [94mLoss[0m : 2.05231
[1mStep[0m  [20/42], [94mLoss[0m : 1.90997
[1mStep[0m  [24/42], [94mLoss[0m : 2.11045
[1mStep[0m  [28/42], [94mLoss[0m : 1.96359
[1mStep[0m  [32/42], [94mLoss[0m : 2.15714
[1mStep[0m  [36/42], [94mLoss[0m : 1.97738
[1mStep[0m  [40/42], [94mLoss[0m : 2.00785

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.034, [92mTest[0m: 2.439, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.02996
[1mStep[0m  [4/42], [94mLoss[0m : 1.77143
[1mStep[0m  [8/42], [94mLoss[0m : 1.98389
[1mStep[0m  [12/42], [94mLoss[0m : 2.01743
[1mStep[0m  [16/42], [94mLoss[0m : 1.91222
[1mStep[0m  [20/42], [94mLoss[0m : 1.94110
[1mStep[0m  [24/42], [94mLoss[0m : 2.07386
[1mStep[0m  [28/42], [94mLoss[0m : 1.95092
[1mStep[0m  [32/42], [94mLoss[0m : 1.95469
[1mStep[0m  [36/42], [94mLoss[0m : 1.84314
[1mStep[0m  [40/42], [94mLoss[0m : 2.23962

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.985, [92mTest[0m: 2.438, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.06631
[1mStep[0m  [4/42], [94mLoss[0m : 1.63333
[1mStep[0m  [8/42], [94mLoss[0m : 1.93158
[1mStep[0m  [12/42], [94mLoss[0m : 2.10635
[1mStep[0m  [16/42], [94mLoss[0m : 2.12630
[1mStep[0m  [20/42], [94mLoss[0m : 1.79814
[1mStep[0m  [24/42], [94mLoss[0m : 1.88206
[1mStep[0m  [28/42], [94mLoss[0m : 2.11829
[1mStep[0m  [32/42], [94mLoss[0m : 1.92458
[1mStep[0m  [36/42], [94mLoss[0m : 1.90448
[1mStep[0m  [40/42], [94mLoss[0m : 1.98746

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.943, [92mTest[0m: 2.565, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.80927
[1mStep[0m  [4/42], [94mLoss[0m : 2.01400
[1mStep[0m  [8/42], [94mLoss[0m : 1.98965
[1mStep[0m  [12/42], [94mLoss[0m : 2.06551
[1mStep[0m  [16/42], [94mLoss[0m : 2.05444
[1mStep[0m  [20/42], [94mLoss[0m : 1.73486
[1mStep[0m  [24/42], [94mLoss[0m : 1.85360
[1mStep[0m  [28/42], [94mLoss[0m : 1.90285
[1mStep[0m  [32/42], [94mLoss[0m : 2.00115
[1mStep[0m  [36/42], [94mLoss[0m : 1.93405
[1mStep[0m  [40/42], [94mLoss[0m : 2.02563

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.916, [92mTest[0m: 2.457, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.92080
[1mStep[0m  [4/42], [94mLoss[0m : 1.93650
[1mStep[0m  [8/42], [94mLoss[0m : 1.78580
[1mStep[0m  [12/42], [94mLoss[0m : 1.86261
[1mStep[0m  [16/42], [94mLoss[0m : 1.94823
[1mStep[0m  [20/42], [94mLoss[0m : 1.94526
[1mStep[0m  [24/42], [94mLoss[0m : 1.75981
[1mStep[0m  [28/42], [94mLoss[0m : 2.00279
[1mStep[0m  [32/42], [94mLoss[0m : 1.80963
[1mStep[0m  [36/42], [94mLoss[0m : 1.85418
[1mStep[0m  [40/42], [94mLoss[0m : 1.78692

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.900, [92mTest[0m: 2.484, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.86653
[1mStep[0m  [4/42], [94mLoss[0m : 1.98402
[1mStep[0m  [8/42], [94mLoss[0m : 1.69258
[1mStep[0m  [12/42], [94mLoss[0m : 1.90014
[1mStep[0m  [16/42], [94mLoss[0m : 1.89026
[1mStep[0m  [20/42], [94mLoss[0m : 1.95985
[1mStep[0m  [24/42], [94mLoss[0m : 1.81851
[1mStep[0m  [28/42], [94mLoss[0m : 1.89423
[1mStep[0m  [32/42], [94mLoss[0m : 1.98077
[1mStep[0m  [36/42], [94mLoss[0m : 1.89255
[1mStep[0m  [40/42], [94mLoss[0m : 1.84768

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.864, [92mTest[0m: 2.476, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.83441
[1mStep[0m  [4/42], [94mLoss[0m : 1.78922
[1mStep[0m  [8/42], [94mLoss[0m : 1.94954
[1mStep[0m  [12/42], [94mLoss[0m : 1.82332
[1mStep[0m  [16/42], [94mLoss[0m : 1.87315
[1mStep[0m  [20/42], [94mLoss[0m : 1.81519
[1mStep[0m  [24/42], [94mLoss[0m : 1.85486
[1mStep[0m  [28/42], [94mLoss[0m : 1.98152
[1mStep[0m  [32/42], [94mLoss[0m : 1.89197
[1mStep[0m  [36/42], [94mLoss[0m : 1.89480
[1mStep[0m  [40/42], [94mLoss[0m : 1.87283

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.833, [92mTest[0m: 2.513, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.62912
[1mStep[0m  [4/42], [94mLoss[0m : 1.81493
[1mStep[0m  [8/42], [94mLoss[0m : 1.70687
[1mStep[0m  [12/42], [94mLoss[0m : 1.80060
[1mStep[0m  [16/42], [94mLoss[0m : 1.64893
[1mStep[0m  [20/42], [94mLoss[0m : 1.71311
[1mStep[0m  [24/42], [94mLoss[0m : 1.82810
[1mStep[0m  [28/42], [94mLoss[0m : 1.85250
[1mStep[0m  [32/42], [94mLoss[0m : 1.82601
[1mStep[0m  [36/42], [94mLoss[0m : 1.90754
[1mStep[0m  [40/42], [94mLoss[0m : 1.85578

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.805, [92mTest[0m: 2.487, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.95649
[1mStep[0m  [4/42], [94mLoss[0m : 1.52998
[1mStep[0m  [8/42], [94mLoss[0m : 1.90596
[1mStep[0m  [12/42], [94mLoss[0m : 1.65173
[1mStep[0m  [16/42], [94mLoss[0m : 1.83634
[1mStep[0m  [20/42], [94mLoss[0m : 1.78619
[1mStep[0m  [24/42], [94mLoss[0m : 1.70699
[1mStep[0m  [28/42], [94mLoss[0m : 1.85342
[1mStep[0m  [32/42], [94mLoss[0m : 1.77519
[1mStep[0m  [36/42], [94mLoss[0m : 1.74913
[1mStep[0m  [40/42], [94mLoss[0m : 1.78910

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.754, [92mTest[0m: 2.543, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.95540
[1mStep[0m  [4/42], [94mLoss[0m : 1.72538
[1mStep[0m  [8/42], [94mLoss[0m : 1.75355
[1mStep[0m  [12/42], [94mLoss[0m : 1.76308
[1mStep[0m  [16/42], [94mLoss[0m : 1.78431
[1mStep[0m  [20/42], [94mLoss[0m : 1.68421
[1mStep[0m  [24/42], [94mLoss[0m : 1.60707
[1mStep[0m  [28/42], [94mLoss[0m : 1.72764
[1mStep[0m  [32/42], [94mLoss[0m : 1.86396
[1mStep[0m  [36/42], [94mLoss[0m : 1.98754
[1mStep[0m  [40/42], [94mLoss[0m : 1.70743

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.729, [92mTest[0m: 2.490, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.62517
[1mStep[0m  [4/42], [94mLoss[0m : 1.72181
[1mStep[0m  [8/42], [94mLoss[0m : 1.71866
[1mStep[0m  [12/42], [94mLoss[0m : 1.78483
[1mStep[0m  [16/42], [94mLoss[0m : 1.70013
[1mStep[0m  [20/42], [94mLoss[0m : 1.85715
[1mStep[0m  [24/42], [94mLoss[0m : 1.73063
[1mStep[0m  [28/42], [94mLoss[0m : 1.68991
[1mStep[0m  [32/42], [94mLoss[0m : 1.82551
[1mStep[0m  [36/42], [94mLoss[0m : 1.58766
[1mStep[0m  [40/42], [94mLoss[0m : 1.76474

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.707, [92mTest[0m: 2.638, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.75006
[1mStep[0m  [4/42], [94mLoss[0m : 1.82016
[1mStep[0m  [8/42], [94mLoss[0m : 1.53986
[1mStep[0m  [12/42], [94mLoss[0m : 1.65660
[1mStep[0m  [16/42], [94mLoss[0m : 1.80417
[1mStep[0m  [20/42], [94mLoss[0m : 1.80530
[1mStep[0m  [24/42], [94mLoss[0m : 1.59070
[1mStep[0m  [28/42], [94mLoss[0m : 1.65970
[1mStep[0m  [32/42], [94mLoss[0m : 1.67628
[1mStep[0m  [36/42], [94mLoss[0m : 1.51512
[1mStep[0m  [40/42], [94mLoss[0m : 1.62654

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.659, [92mTest[0m: 2.557, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.71351
[1mStep[0m  [4/42], [94mLoss[0m : 1.64122
[1mStep[0m  [8/42], [94mLoss[0m : 1.52031
[1mStep[0m  [12/42], [94mLoss[0m : 1.57339
[1mStep[0m  [16/42], [94mLoss[0m : 1.85643
[1mStep[0m  [20/42], [94mLoss[0m : 1.51602
[1mStep[0m  [24/42], [94mLoss[0m : 1.89539
[1mStep[0m  [28/42], [94mLoss[0m : 1.61006
[1mStep[0m  [32/42], [94mLoss[0m : 1.46955
[1mStep[0m  [36/42], [94mLoss[0m : 1.41153
[1mStep[0m  [40/42], [94mLoss[0m : 1.70080

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.629, [92mTest[0m: 2.504, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.68340
[1mStep[0m  [4/42], [94mLoss[0m : 1.60443
[1mStep[0m  [8/42], [94mLoss[0m : 1.55802
[1mStep[0m  [12/42], [94mLoss[0m : 1.45257
[1mStep[0m  [16/42], [94mLoss[0m : 1.60362
[1mStep[0m  [20/42], [94mLoss[0m : 1.80770
[1mStep[0m  [24/42], [94mLoss[0m : 1.57364
[1mStep[0m  [28/42], [94mLoss[0m : 1.70169
[1mStep[0m  [32/42], [94mLoss[0m : 1.63959
[1mStep[0m  [36/42], [94mLoss[0m : 1.69341
[1mStep[0m  [40/42], [94mLoss[0m : 1.60296

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.641, [92mTest[0m: 2.503, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.50702
[1mStep[0m  [4/42], [94mLoss[0m : 1.68847
[1mStep[0m  [8/42], [94mLoss[0m : 1.59391
[1mStep[0m  [12/42], [94mLoss[0m : 1.66453
[1mStep[0m  [16/42], [94mLoss[0m : 1.53221
[1mStep[0m  [20/42], [94mLoss[0m : 1.58178
[1mStep[0m  [24/42], [94mLoss[0m : 1.68108
[1mStep[0m  [28/42], [94mLoss[0m : 1.52845
[1mStep[0m  [32/42], [94mLoss[0m : 1.61140
[1mStep[0m  [36/42], [94mLoss[0m : 1.66280
[1mStep[0m  [40/42], [94mLoss[0m : 1.58101

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.614, [92mTest[0m: 2.547, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.62212
[1mStep[0m  [4/42], [94mLoss[0m : 1.60309
[1mStep[0m  [8/42], [94mLoss[0m : 1.64456
[1mStep[0m  [12/42], [94mLoss[0m : 1.67845
[1mStep[0m  [16/42], [94mLoss[0m : 1.71896
[1mStep[0m  [20/42], [94mLoss[0m : 1.29390
[1mStep[0m  [24/42], [94mLoss[0m : 1.57770
[1mStep[0m  [28/42], [94mLoss[0m : 1.61461
[1mStep[0m  [32/42], [94mLoss[0m : 1.59357
[1mStep[0m  [36/42], [94mLoss[0m : 1.56840
[1mStep[0m  [40/42], [94mLoss[0m : 1.57580

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.587, [92mTest[0m: 2.599, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.47120
[1mStep[0m  [4/42], [94mLoss[0m : 1.54972
[1mStep[0m  [8/42], [94mLoss[0m : 1.65618
[1mStep[0m  [12/42], [94mLoss[0m : 1.54778
[1mStep[0m  [16/42], [94mLoss[0m : 1.65910
[1mStep[0m  [20/42], [94mLoss[0m : 1.68064
[1mStep[0m  [24/42], [94mLoss[0m : 1.54796
[1mStep[0m  [28/42], [94mLoss[0m : 1.61156
[1mStep[0m  [32/42], [94mLoss[0m : 1.74722
[1mStep[0m  [36/42], [94mLoss[0m : 1.59753
[1mStep[0m  [40/42], [94mLoss[0m : 1.57987

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.587, [92mTest[0m: 2.542, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.52310
[1mStep[0m  [4/42], [94mLoss[0m : 1.57620
[1mStep[0m  [8/42], [94mLoss[0m : 1.43187
[1mStep[0m  [12/42], [94mLoss[0m : 1.54659
[1mStep[0m  [16/42], [94mLoss[0m : 1.39072
[1mStep[0m  [20/42], [94mLoss[0m : 1.49742
[1mStep[0m  [24/42], [94mLoss[0m : 1.58567
[1mStep[0m  [28/42], [94mLoss[0m : 1.57970
[1mStep[0m  [32/42], [94mLoss[0m : 1.63419
[1mStep[0m  [36/42], [94mLoss[0m : 1.59263
[1mStep[0m  [40/42], [94mLoss[0m : 1.52313

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.561, [92mTest[0m: 2.620, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.633
====================================

Phase 2 - Evaluation MAE:  2.6329756464276994
MAE score P1      2.467855
MAE score P2      2.632976
loss              1.560655
learning_rate     0.007525
batch_size             256
hidden_sizes         [300]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.4
momentum               0.1
weight_decay          0.01
Name: 30, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 11.50834
[1mStep[0m  [4/42], [94mLoss[0m : 10.52579
[1mStep[0m  [8/42], [94mLoss[0m : 10.14347
[1mStep[0m  [12/42], [94mLoss[0m : 9.78819
[1mStep[0m  [16/42], [94mLoss[0m : 9.82002
[1mStep[0m  [20/42], [94mLoss[0m : 8.45573
[1mStep[0m  [24/42], [94mLoss[0m : 7.91912
[1mStep[0m  [28/42], [94mLoss[0m : 7.46067
[1mStep[0m  [32/42], [94mLoss[0m : 7.12822
[1mStep[0m  [36/42], [94mLoss[0m : 6.97133
[1mStep[0m  [40/42], [94mLoss[0m : 6.06455

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 8.686, [92mTest[0m: 11.008, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.82268
[1mStep[0m  [4/42], [94mLoss[0m : 5.38932
[1mStep[0m  [8/42], [94mLoss[0m : 4.86438
[1mStep[0m  [12/42], [94mLoss[0m : 4.33704
[1mStep[0m  [16/42], [94mLoss[0m : 3.92605
[1mStep[0m  [20/42], [94mLoss[0m : 3.82628
[1mStep[0m  [24/42], [94mLoss[0m : 3.41715
[1mStep[0m  [28/42], [94mLoss[0m : 3.52573
[1mStep[0m  [32/42], [94mLoss[0m : 3.31032
[1mStep[0m  [36/42], [94mLoss[0m : 3.09619
[1mStep[0m  [40/42], [94mLoss[0m : 3.06645

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 4.036, [92mTest[0m: 7.902, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.11935
[1mStep[0m  [4/42], [94mLoss[0m : 3.05212
[1mStep[0m  [8/42], [94mLoss[0m : 3.60102
[1mStep[0m  [12/42], [94mLoss[0m : 2.89986
[1mStep[0m  [16/42], [94mLoss[0m : 2.98563
[1mStep[0m  [20/42], [94mLoss[0m : 2.98743
[1mStep[0m  [24/42], [94mLoss[0m : 3.00013
[1mStep[0m  [28/42], [94mLoss[0m : 3.09535
[1mStep[0m  [32/42], [94mLoss[0m : 3.31171
[1mStep[0m  [36/42], [94mLoss[0m : 3.11886
[1mStep[0m  [40/42], [94mLoss[0m : 2.97916

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.060, [92mTest[0m: 3.937, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.12381
[1mStep[0m  [4/42], [94mLoss[0m : 2.77540
[1mStep[0m  [8/42], [94mLoss[0m : 3.13318
[1mStep[0m  [12/42], [94mLoss[0m : 2.96518
[1mStep[0m  [16/42], [94mLoss[0m : 2.98180
[1mStep[0m  [20/42], [94mLoss[0m : 2.98961
[1mStep[0m  [24/42], [94mLoss[0m : 2.99788
[1mStep[0m  [28/42], [94mLoss[0m : 3.00929
[1mStep[0m  [32/42], [94mLoss[0m : 2.94915
[1mStep[0m  [36/42], [94mLoss[0m : 3.31911
[1mStep[0m  [40/42], [94mLoss[0m : 2.91819

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.936, [92mTest[0m: 3.129, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.85668
[1mStep[0m  [4/42], [94mLoss[0m : 2.74297
[1mStep[0m  [8/42], [94mLoss[0m : 2.84468
[1mStep[0m  [12/42], [94mLoss[0m : 2.93561
[1mStep[0m  [16/42], [94mLoss[0m : 2.79758
[1mStep[0m  [20/42], [94mLoss[0m : 3.12719
[1mStep[0m  [24/42], [94mLoss[0m : 2.79637
[1mStep[0m  [28/42], [94mLoss[0m : 2.99372
[1mStep[0m  [32/42], [94mLoss[0m : 2.99072
[1mStep[0m  [36/42], [94mLoss[0m : 2.97454
[1mStep[0m  [40/42], [94mLoss[0m : 2.91780

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.897, [92mTest[0m: 2.685, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.95840
[1mStep[0m  [4/42], [94mLoss[0m : 2.75650
[1mStep[0m  [8/42], [94mLoss[0m : 2.84085
[1mStep[0m  [12/42], [94mLoss[0m : 3.14329
[1mStep[0m  [16/42], [94mLoss[0m : 2.81420
[1mStep[0m  [20/42], [94mLoss[0m : 2.91259
[1mStep[0m  [24/42], [94mLoss[0m : 2.96842
[1mStep[0m  [28/42], [94mLoss[0m : 3.02238
[1mStep[0m  [32/42], [94mLoss[0m : 2.99144
[1mStep[0m  [36/42], [94mLoss[0m : 2.91447
[1mStep[0m  [40/42], [94mLoss[0m : 2.92418

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.875, [92mTest[0m: 2.548, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.12540
[1mStep[0m  [4/42], [94mLoss[0m : 2.65286
[1mStep[0m  [8/42], [94mLoss[0m : 2.76912
[1mStep[0m  [12/42], [94mLoss[0m : 2.81282
[1mStep[0m  [16/42], [94mLoss[0m : 2.66591
[1mStep[0m  [20/42], [94mLoss[0m : 2.82989
[1mStep[0m  [24/42], [94mLoss[0m : 3.00257
[1mStep[0m  [28/42], [94mLoss[0m : 2.59766
[1mStep[0m  [32/42], [94mLoss[0m : 3.07093
[1mStep[0m  [36/42], [94mLoss[0m : 2.85332
[1mStep[0m  [40/42], [94mLoss[0m : 2.67463

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.842, [92mTest[0m: 2.662, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.87598
[1mStep[0m  [4/42], [94mLoss[0m : 2.81804
[1mStep[0m  [8/42], [94mLoss[0m : 2.95176
[1mStep[0m  [12/42], [94mLoss[0m : 3.29007
[1mStep[0m  [16/42], [94mLoss[0m : 2.97885
[1mStep[0m  [20/42], [94mLoss[0m : 3.06843
[1mStep[0m  [24/42], [94mLoss[0m : 2.72475
[1mStep[0m  [28/42], [94mLoss[0m : 2.67019
[1mStep[0m  [32/42], [94mLoss[0m : 2.94509
[1mStep[0m  [36/42], [94mLoss[0m : 2.73859
[1mStep[0m  [40/42], [94mLoss[0m : 2.80098

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.846, [92mTest[0m: 2.503, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.69713
[1mStep[0m  [4/42], [94mLoss[0m : 2.92142
[1mStep[0m  [8/42], [94mLoss[0m : 2.83918
[1mStep[0m  [12/42], [94mLoss[0m : 2.94870
[1mStep[0m  [16/42], [94mLoss[0m : 2.68917
[1mStep[0m  [20/42], [94mLoss[0m : 2.85131
[1mStep[0m  [24/42], [94mLoss[0m : 2.91304
[1mStep[0m  [28/42], [94mLoss[0m : 2.98151
[1mStep[0m  [32/42], [94mLoss[0m : 2.84457
[1mStep[0m  [36/42], [94mLoss[0m : 2.62053
[1mStep[0m  [40/42], [94mLoss[0m : 3.17916

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.811, [92mTest[0m: 2.513, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.92087
[1mStep[0m  [4/42], [94mLoss[0m : 2.66587
[1mStep[0m  [8/42], [94mLoss[0m : 2.76336
[1mStep[0m  [12/42], [94mLoss[0m : 3.02774
[1mStep[0m  [16/42], [94mLoss[0m : 2.82511
[1mStep[0m  [20/42], [94mLoss[0m : 2.67749
[1mStep[0m  [24/42], [94mLoss[0m : 2.86275
[1mStep[0m  [28/42], [94mLoss[0m : 2.52069
[1mStep[0m  [32/42], [94mLoss[0m : 2.99626
[1mStep[0m  [36/42], [94mLoss[0m : 2.76953
[1mStep[0m  [40/42], [94mLoss[0m : 2.65436

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.785, [92mTest[0m: 2.551, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.93582
[1mStep[0m  [4/42], [94mLoss[0m : 2.96043
[1mStep[0m  [8/42], [94mLoss[0m : 2.76878
[1mStep[0m  [12/42], [94mLoss[0m : 2.50771
[1mStep[0m  [16/42], [94mLoss[0m : 2.78578
[1mStep[0m  [20/42], [94mLoss[0m : 2.95951
[1mStep[0m  [24/42], [94mLoss[0m : 2.88672
[1mStep[0m  [28/42], [94mLoss[0m : 2.58264
[1mStep[0m  [32/42], [94mLoss[0m : 2.80590
[1mStep[0m  [36/42], [94mLoss[0m : 2.72264
[1mStep[0m  [40/42], [94mLoss[0m : 2.87966

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.786, [92mTest[0m: 2.441, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.81038
[1mStep[0m  [4/42], [94mLoss[0m : 2.70295
[1mStep[0m  [8/42], [94mLoss[0m : 2.70827
[1mStep[0m  [12/42], [94mLoss[0m : 2.66826
[1mStep[0m  [16/42], [94mLoss[0m : 2.86160
[1mStep[0m  [20/42], [94mLoss[0m : 2.60760
[1mStep[0m  [24/42], [94mLoss[0m : 2.54691
[1mStep[0m  [28/42], [94mLoss[0m : 2.78219
[1mStep[0m  [32/42], [94mLoss[0m : 2.63740
[1mStep[0m  [36/42], [94mLoss[0m : 2.73706
[1mStep[0m  [40/42], [94mLoss[0m : 2.67627

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.780, [92mTest[0m: 2.435, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.00420
[1mStep[0m  [4/42], [94mLoss[0m : 2.64342
[1mStep[0m  [8/42], [94mLoss[0m : 2.78654
[1mStep[0m  [12/42], [94mLoss[0m : 2.81827
[1mStep[0m  [16/42], [94mLoss[0m : 2.55737
[1mStep[0m  [20/42], [94mLoss[0m : 2.55514
[1mStep[0m  [24/42], [94mLoss[0m : 2.90457
[1mStep[0m  [28/42], [94mLoss[0m : 2.76701
[1mStep[0m  [32/42], [94mLoss[0m : 2.70150
[1mStep[0m  [36/42], [94mLoss[0m : 2.80482
[1mStep[0m  [40/42], [94mLoss[0m : 2.47176

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.751, [92mTest[0m: 2.417, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.93450
[1mStep[0m  [4/42], [94mLoss[0m : 2.65252
[1mStep[0m  [8/42], [94mLoss[0m : 2.60426
[1mStep[0m  [12/42], [94mLoss[0m : 2.80340
[1mStep[0m  [16/42], [94mLoss[0m : 3.09851
[1mStep[0m  [20/42], [94mLoss[0m : 2.96581
[1mStep[0m  [24/42], [94mLoss[0m : 2.68893
[1mStep[0m  [28/42], [94mLoss[0m : 2.90284
[1mStep[0m  [32/42], [94mLoss[0m : 2.63145
[1mStep[0m  [36/42], [94mLoss[0m : 2.84184
[1mStep[0m  [40/42], [94mLoss[0m : 2.69616

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.746, [92mTest[0m: 2.439, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62363
[1mStep[0m  [4/42], [94mLoss[0m : 2.77653
[1mStep[0m  [8/42], [94mLoss[0m : 2.64203
[1mStep[0m  [12/42], [94mLoss[0m : 2.98492
[1mStep[0m  [16/42], [94mLoss[0m : 2.71872
[1mStep[0m  [20/42], [94mLoss[0m : 3.02201
[1mStep[0m  [24/42], [94mLoss[0m : 2.80032
[1mStep[0m  [28/42], [94mLoss[0m : 2.80523
[1mStep[0m  [32/42], [94mLoss[0m : 2.99486
[1mStep[0m  [36/42], [94mLoss[0m : 2.69090
[1mStep[0m  [40/42], [94mLoss[0m : 2.82245

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.739, [92mTest[0m: 2.381, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.85669
[1mStep[0m  [4/42], [94mLoss[0m : 2.78274
[1mStep[0m  [8/42], [94mLoss[0m : 2.77657
[1mStep[0m  [12/42], [94mLoss[0m : 2.59072
[1mStep[0m  [16/42], [94mLoss[0m : 2.81976
[1mStep[0m  [20/42], [94mLoss[0m : 2.73898
[1mStep[0m  [24/42], [94mLoss[0m : 2.78518
[1mStep[0m  [28/42], [94mLoss[0m : 2.70111
[1mStep[0m  [32/42], [94mLoss[0m : 2.92773
[1mStep[0m  [36/42], [94mLoss[0m : 2.78098
[1mStep[0m  [40/42], [94mLoss[0m : 2.64886

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.730, [92mTest[0m: 2.371, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.71257
[1mStep[0m  [4/42], [94mLoss[0m : 2.68384
[1mStep[0m  [8/42], [94mLoss[0m : 2.61438
[1mStep[0m  [12/42], [94mLoss[0m : 2.66721
[1mStep[0m  [16/42], [94mLoss[0m : 2.56509
[1mStep[0m  [20/42], [94mLoss[0m : 2.74899
[1mStep[0m  [24/42], [94mLoss[0m : 2.86089
[1mStep[0m  [28/42], [94mLoss[0m : 2.72314
[1mStep[0m  [32/42], [94mLoss[0m : 2.85797
[1mStep[0m  [36/42], [94mLoss[0m : 2.77266
[1mStep[0m  [40/42], [94mLoss[0m : 2.75140

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.693, [92mTest[0m: 2.381, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60603
[1mStep[0m  [4/42], [94mLoss[0m : 3.06250
[1mStep[0m  [8/42], [94mLoss[0m : 2.61730
[1mStep[0m  [12/42], [94mLoss[0m : 2.31974
[1mStep[0m  [16/42], [94mLoss[0m : 2.86279
[1mStep[0m  [20/42], [94mLoss[0m : 2.53500
[1mStep[0m  [24/42], [94mLoss[0m : 2.65441
[1mStep[0m  [28/42], [94mLoss[0m : 2.94559
[1mStep[0m  [32/42], [94mLoss[0m : 2.75179
[1mStep[0m  [36/42], [94mLoss[0m : 2.65852
[1mStep[0m  [40/42], [94mLoss[0m : 2.90159

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.694, [92mTest[0m: 2.368, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.76717
[1mStep[0m  [4/42], [94mLoss[0m : 2.69998
[1mStep[0m  [8/42], [94mLoss[0m : 2.39448
[1mStep[0m  [12/42], [94mLoss[0m : 2.70639
[1mStep[0m  [16/42], [94mLoss[0m : 2.91308
[1mStep[0m  [20/42], [94mLoss[0m : 2.48734
[1mStep[0m  [24/42], [94mLoss[0m : 2.57946
[1mStep[0m  [28/42], [94mLoss[0m : 2.78140
[1mStep[0m  [32/42], [94mLoss[0m : 2.76721
[1mStep[0m  [36/42], [94mLoss[0m : 2.50883
[1mStep[0m  [40/42], [94mLoss[0m : 2.53520

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.677, [92mTest[0m: 2.372, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60811
[1mStep[0m  [4/42], [94mLoss[0m : 2.75209
[1mStep[0m  [8/42], [94mLoss[0m : 2.45666
[1mStep[0m  [12/42], [94mLoss[0m : 2.56580
[1mStep[0m  [16/42], [94mLoss[0m : 2.65757
[1mStep[0m  [20/42], [94mLoss[0m : 2.81379
[1mStep[0m  [24/42], [94mLoss[0m : 2.66882
[1mStep[0m  [28/42], [94mLoss[0m : 2.80541
[1mStep[0m  [32/42], [94mLoss[0m : 2.94732
[1mStep[0m  [36/42], [94mLoss[0m : 2.53583
[1mStep[0m  [40/42], [94mLoss[0m : 2.79780

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.672, [92mTest[0m: 2.372, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.74300
[1mStep[0m  [4/42], [94mLoss[0m : 2.57752
[1mStep[0m  [8/42], [94mLoss[0m : 2.57623
[1mStep[0m  [12/42], [94mLoss[0m : 2.79696
[1mStep[0m  [16/42], [94mLoss[0m : 2.49312
[1mStep[0m  [20/42], [94mLoss[0m : 2.61774
[1mStep[0m  [24/42], [94mLoss[0m : 2.39798
[1mStep[0m  [28/42], [94mLoss[0m : 2.62121
[1mStep[0m  [32/42], [94mLoss[0m : 2.63717
[1mStep[0m  [36/42], [94mLoss[0m : 2.60160
[1mStep[0m  [40/42], [94mLoss[0m : 2.74951

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.669, [92mTest[0m: 2.367, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49289
[1mStep[0m  [4/42], [94mLoss[0m : 2.53851
[1mStep[0m  [8/42], [94mLoss[0m : 2.71672
[1mStep[0m  [12/42], [94mLoss[0m : 2.64698
[1mStep[0m  [16/42], [94mLoss[0m : 2.75408
[1mStep[0m  [20/42], [94mLoss[0m : 2.78031
[1mStep[0m  [24/42], [94mLoss[0m : 2.84487
[1mStep[0m  [28/42], [94mLoss[0m : 2.78120
[1mStep[0m  [32/42], [94mLoss[0m : 2.75393
[1mStep[0m  [36/42], [94mLoss[0m : 2.84932
[1mStep[0m  [40/42], [94mLoss[0m : 2.56630

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.639, [92mTest[0m: 2.386, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.77904
[1mStep[0m  [4/42], [94mLoss[0m : 2.63416
[1mStep[0m  [8/42], [94mLoss[0m : 2.61595
[1mStep[0m  [12/42], [94mLoss[0m : 2.72919
[1mStep[0m  [16/42], [94mLoss[0m : 2.71745
[1mStep[0m  [20/42], [94mLoss[0m : 2.55620
[1mStep[0m  [24/42], [94mLoss[0m : 2.58899
[1mStep[0m  [28/42], [94mLoss[0m : 2.50103
[1mStep[0m  [32/42], [94mLoss[0m : 2.58588
[1mStep[0m  [36/42], [94mLoss[0m : 2.49010
[1mStep[0m  [40/42], [94mLoss[0m : 2.62573

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.661, [92mTest[0m: 2.356, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.78074
[1mStep[0m  [4/42], [94mLoss[0m : 2.42515
[1mStep[0m  [8/42], [94mLoss[0m : 2.72604
[1mStep[0m  [12/42], [94mLoss[0m : 2.90010
[1mStep[0m  [16/42], [94mLoss[0m : 2.76948
[1mStep[0m  [20/42], [94mLoss[0m : 2.60315
[1mStep[0m  [24/42], [94mLoss[0m : 2.81751
[1mStep[0m  [28/42], [94mLoss[0m : 2.86682
[1mStep[0m  [32/42], [94mLoss[0m : 2.58768
[1mStep[0m  [36/42], [94mLoss[0m : 2.40981
[1mStep[0m  [40/42], [94mLoss[0m : 2.51071

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.638, [92mTest[0m: 2.347, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.65957
[1mStep[0m  [4/42], [94mLoss[0m : 2.59590
[1mStep[0m  [8/42], [94mLoss[0m : 2.53647
[1mStep[0m  [12/42], [94mLoss[0m : 2.66045
[1mStep[0m  [16/42], [94mLoss[0m : 2.71963
[1mStep[0m  [20/42], [94mLoss[0m : 2.62492
[1mStep[0m  [24/42], [94mLoss[0m : 2.69696
[1mStep[0m  [28/42], [94mLoss[0m : 2.64387
[1mStep[0m  [32/42], [94mLoss[0m : 2.85383
[1mStep[0m  [36/42], [94mLoss[0m : 2.53082
[1mStep[0m  [40/42], [94mLoss[0m : 2.68043

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.640, [92mTest[0m: 2.361, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.71614
[1mStep[0m  [4/42], [94mLoss[0m : 2.71487
[1mStep[0m  [8/42], [94mLoss[0m : 2.57569
[1mStep[0m  [12/42], [94mLoss[0m : 2.51623
[1mStep[0m  [16/42], [94mLoss[0m : 2.81250
[1mStep[0m  [20/42], [94mLoss[0m : 2.64578
[1mStep[0m  [24/42], [94mLoss[0m : 2.46928
[1mStep[0m  [28/42], [94mLoss[0m : 2.84851
[1mStep[0m  [32/42], [94mLoss[0m : 2.61622
[1mStep[0m  [36/42], [94mLoss[0m : 2.67371
[1mStep[0m  [40/42], [94mLoss[0m : 2.72549

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.639, [92mTest[0m: 2.366, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43365
[1mStep[0m  [4/42], [94mLoss[0m : 2.51072
[1mStep[0m  [8/42], [94mLoss[0m : 2.85322
[1mStep[0m  [12/42], [94mLoss[0m : 2.66074
[1mStep[0m  [16/42], [94mLoss[0m : 2.73139
[1mStep[0m  [20/42], [94mLoss[0m : 2.80290
[1mStep[0m  [24/42], [94mLoss[0m : 2.67382
[1mStep[0m  [28/42], [94mLoss[0m : 2.79986
[1mStep[0m  [32/42], [94mLoss[0m : 2.75686
[1mStep[0m  [36/42], [94mLoss[0m : 2.64645
[1mStep[0m  [40/42], [94mLoss[0m : 2.66928

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.641, [92mTest[0m: 2.342, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61489
[1mStep[0m  [4/42], [94mLoss[0m : 2.65049
[1mStep[0m  [8/42], [94mLoss[0m : 2.57576
[1mStep[0m  [12/42], [94mLoss[0m : 2.40810
[1mStep[0m  [16/42], [94mLoss[0m : 2.59452
[1mStep[0m  [20/42], [94mLoss[0m : 2.61487
[1mStep[0m  [24/42], [94mLoss[0m : 2.41255
[1mStep[0m  [28/42], [94mLoss[0m : 2.94097
[1mStep[0m  [32/42], [94mLoss[0m : 2.71981
[1mStep[0m  [36/42], [94mLoss[0m : 2.73623
[1mStep[0m  [40/42], [94mLoss[0m : 2.63223

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.627, [92mTest[0m: 2.348, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61839
[1mStep[0m  [4/42], [94mLoss[0m : 2.57357
[1mStep[0m  [8/42], [94mLoss[0m : 2.53986
[1mStep[0m  [12/42], [94mLoss[0m : 2.55814
[1mStep[0m  [16/42], [94mLoss[0m : 2.56136
[1mStep[0m  [20/42], [94mLoss[0m : 2.71293
[1mStep[0m  [24/42], [94mLoss[0m : 2.44208
[1mStep[0m  [28/42], [94mLoss[0m : 2.68131
[1mStep[0m  [32/42], [94mLoss[0m : 2.71946
[1mStep[0m  [36/42], [94mLoss[0m : 2.58258
[1mStep[0m  [40/42], [94mLoss[0m : 2.56050

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.608, [92mTest[0m: 2.354, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58755
[1mStep[0m  [4/42], [94mLoss[0m : 2.58724
[1mStep[0m  [8/42], [94mLoss[0m : 2.61004
[1mStep[0m  [12/42], [94mLoss[0m : 2.57153
[1mStep[0m  [16/42], [94mLoss[0m : 2.62001
[1mStep[0m  [20/42], [94mLoss[0m : 2.79597
[1mStep[0m  [24/42], [94mLoss[0m : 2.67817
[1mStep[0m  [28/42], [94mLoss[0m : 2.51117
[1mStep[0m  [32/42], [94mLoss[0m : 2.60237
[1mStep[0m  [36/42], [94mLoss[0m : 2.48596
[1mStep[0m  [40/42], [94mLoss[0m : 2.58405

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.632, [92mTest[0m: 2.362, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.359
====================================

Phase 1 - Evaluation MAE:  2.3594635895320346
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 2.71961
[1mStep[0m  [4/42], [94mLoss[0m : 2.61119
[1mStep[0m  [8/42], [94mLoss[0m : 2.79227
[1mStep[0m  [12/42], [94mLoss[0m : 2.65712
[1mStep[0m  [16/42], [94mLoss[0m : 2.79470
[1mStep[0m  [20/42], [94mLoss[0m : 2.76507
[1mStep[0m  [24/42], [94mLoss[0m : 2.63657
[1mStep[0m  [28/42], [94mLoss[0m : 2.62205
[1mStep[0m  [32/42], [94mLoss[0m : 2.91490
[1mStep[0m  [36/42], [94mLoss[0m : 2.75804
[1mStep[0m  [40/42], [94mLoss[0m : 2.94548

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.727, [92mTest[0m: 2.357, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.63606
[1mStep[0m  [4/42], [94mLoss[0m : 2.90676
[1mStep[0m  [8/42], [94mLoss[0m : 2.39887
[1mStep[0m  [12/42], [94mLoss[0m : 2.57870
[1mStep[0m  [16/42], [94mLoss[0m : 2.70488
[1mStep[0m  [20/42], [94mLoss[0m : 2.60092
[1mStep[0m  [24/42], [94mLoss[0m : 2.65453
[1mStep[0m  [28/42], [94mLoss[0m : 2.63323
[1mStep[0m  [32/42], [94mLoss[0m : 2.82848
[1mStep[0m  [36/42], [94mLoss[0m : 2.77015
[1mStep[0m  [40/42], [94mLoss[0m : 2.57076

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.646, [92mTest[0m: 2.478, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59731
[1mStep[0m  [4/42], [94mLoss[0m : 2.50518
[1mStep[0m  [8/42], [94mLoss[0m : 2.64054
[1mStep[0m  [12/42], [94mLoss[0m : 2.92551
[1mStep[0m  [16/42], [94mLoss[0m : 2.71395
[1mStep[0m  [20/42], [94mLoss[0m : 2.59666
[1mStep[0m  [24/42], [94mLoss[0m : 2.46746
[1mStep[0m  [28/42], [94mLoss[0m : 2.98775
[1mStep[0m  [32/42], [94mLoss[0m : 2.51035
[1mStep[0m  [36/42], [94mLoss[0m : 2.52185
[1mStep[0m  [40/42], [94mLoss[0m : 2.35113

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.615, [92mTest[0m: 2.495, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.79088
[1mStep[0m  [4/42], [94mLoss[0m : 2.36995
[1mStep[0m  [8/42], [94mLoss[0m : 2.79280
[1mStep[0m  [12/42], [94mLoss[0m : 2.49684
[1mStep[0m  [16/42], [94mLoss[0m : 2.62165
[1mStep[0m  [20/42], [94mLoss[0m : 2.59648
[1mStep[0m  [24/42], [94mLoss[0m : 2.56839
[1mStep[0m  [28/42], [94mLoss[0m : 2.40891
[1mStep[0m  [32/42], [94mLoss[0m : 2.41017
[1mStep[0m  [36/42], [94mLoss[0m : 2.50959
[1mStep[0m  [40/42], [94mLoss[0m : 2.62392

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.572, [92mTest[0m: 2.467, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61620
[1mStep[0m  [4/42], [94mLoss[0m : 2.53265
[1mStep[0m  [8/42], [94mLoss[0m : 2.44634
[1mStep[0m  [12/42], [94mLoss[0m : 2.85587
[1mStep[0m  [16/42], [94mLoss[0m : 2.47699
[1mStep[0m  [20/42], [94mLoss[0m : 2.48642
[1mStep[0m  [24/42], [94mLoss[0m : 2.34060
[1mStep[0m  [28/42], [94mLoss[0m : 2.46583
[1mStep[0m  [32/42], [94mLoss[0m : 2.74833
[1mStep[0m  [36/42], [94mLoss[0m : 2.59681
[1mStep[0m  [40/42], [94mLoss[0m : 2.56582

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.562, [92mTest[0m: 2.448, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50900
[1mStep[0m  [4/42], [94mLoss[0m : 2.44398
[1mStep[0m  [8/42], [94mLoss[0m : 2.43548
[1mStep[0m  [12/42], [94mLoss[0m : 2.43787
[1mStep[0m  [16/42], [94mLoss[0m : 2.67712
[1mStep[0m  [20/42], [94mLoss[0m : 2.55628
[1mStep[0m  [24/42], [94mLoss[0m : 2.51852
[1mStep[0m  [28/42], [94mLoss[0m : 2.34127
[1mStep[0m  [32/42], [94mLoss[0m : 2.58926
[1mStep[0m  [36/42], [94mLoss[0m : 2.41359
[1mStep[0m  [40/42], [94mLoss[0m : 2.47396

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.522, [92mTest[0m: 2.414, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40254
[1mStep[0m  [4/42], [94mLoss[0m : 2.54188
[1mStep[0m  [8/42], [94mLoss[0m : 2.48553
[1mStep[0m  [12/42], [94mLoss[0m : 2.41942
[1mStep[0m  [16/42], [94mLoss[0m : 2.81753
[1mStep[0m  [20/42], [94mLoss[0m : 2.40560
[1mStep[0m  [24/42], [94mLoss[0m : 2.32252
[1mStep[0m  [28/42], [94mLoss[0m : 2.44959
[1mStep[0m  [32/42], [94mLoss[0m : 2.46279
[1mStep[0m  [36/42], [94mLoss[0m : 2.78998
[1mStep[0m  [40/42], [94mLoss[0m : 2.46828

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.402, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41381
[1mStep[0m  [4/42], [94mLoss[0m : 2.39195
[1mStep[0m  [8/42], [94mLoss[0m : 2.63961
[1mStep[0m  [12/42], [94mLoss[0m : 2.47137
[1mStep[0m  [16/42], [94mLoss[0m : 2.31180
[1mStep[0m  [20/42], [94mLoss[0m : 2.37340
[1mStep[0m  [24/42], [94mLoss[0m : 2.62777
[1mStep[0m  [28/42], [94mLoss[0m : 2.49728
[1mStep[0m  [32/42], [94mLoss[0m : 2.65457
[1mStep[0m  [36/42], [94mLoss[0m : 2.55718
[1mStep[0m  [40/42], [94mLoss[0m : 2.56992

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.442, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28147
[1mStep[0m  [4/42], [94mLoss[0m : 2.36673
[1mStep[0m  [8/42], [94mLoss[0m : 2.46472
[1mStep[0m  [12/42], [94mLoss[0m : 2.51368
[1mStep[0m  [16/42], [94mLoss[0m : 2.20846
[1mStep[0m  [20/42], [94mLoss[0m : 2.44959
[1mStep[0m  [24/42], [94mLoss[0m : 2.40597
[1mStep[0m  [28/42], [94mLoss[0m : 2.43687
[1mStep[0m  [32/42], [94mLoss[0m : 2.31924
[1mStep[0m  [36/42], [94mLoss[0m : 2.37799
[1mStep[0m  [40/42], [94mLoss[0m : 2.51108

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.395, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.30556
[1mStep[0m  [4/42], [94mLoss[0m : 2.30613
[1mStep[0m  [8/42], [94mLoss[0m : 2.56911
[1mStep[0m  [12/42], [94mLoss[0m : 2.18838
[1mStep[0m  [16/42], [94mLoss[0m : 2.06122
[1mStep[0m  [20/42], [94mLoss[0m : 2.45183
[1mStep[0m  [24/42], [94mLoss[0m : 2.53263
[1mStep[0m  [28/42], [94mLoss[0m : 2.26339
[1mStep[0m  [32/42], [94mLoss[0m : 2.52199
[1mStep[0m  [36/42], [94mLoss[0m : 2.37434
[1mStep[0m  [40/42], [94mLoss[0m : 2.47671

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.380, [92mTest[0m: 2.445, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.11850
[1mStep[0m  [4/42], [94mLoss[0m : 2.41752
[1mStep[0m  [8/42], [94mLoss[0m : 2.40507
[1mStep[0m  [12/42], [94mLoss[0m : 2.27471
[1mStep[0m  [16/42], [94mLoss[0m : 2.60664
[1mStep[0m  [20/42], [94mLoss[0m : 2.13440
[1mStep[0m  [24/42], [94mLoss[0m : 2.22032
[1mStep[0m  [28/42], [94mLoss[0m : 2.23398
[1mStep[0m  [32/42], [94mLoss[0m : 2.28905
[1mStep[0m  [36/42], [94mLoss[0m : 2.23112
[1mStep[0m  [40/42], [94mLoss[0m : 2.13705

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.346, [92mTest[0m: 2.448, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29416
[1mStep[0m  [4/42], [94mLoss[0m : 2.14179
[1mStep[0m  [8/42], [94mLoss[0m : 2.56591
[1mStep[0m  [12/42], [94mLoss[0m : 2.26090
[1mStep[0m  [16/42], [94mLoss[0m : 2.35819
[1mStep[0m  [20/42], [94mLoss[0m : 2.39004
[1mStep[0m  [24/42], [94mLoss[0m : 2.34823
[1mStep[0m  [28/42], [94mLoss[0m : 2.43150
[1mStep[0m  [32/42], [94mLoss[0m : 2.26390
[1mStep[0m  [36/42], [94mLoss[0m : 2.26158
[1mStep[0m  [40/42], [94mLoss[0m : 2.12077

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.325, [92mTest[0m: 2.404, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.25186
[1mStep[0m  [4/42], [94mLoss[0m : 2.19595
[1mStep[0m  [8/42], [94mLoss[0m : 2.25946
[1mStep[0m  [12/42], [94mLoss[0m : 2.19449
[1mStep[0m  [16/42], [94mLoss[0m : 2.39379
[1mStep[0m  [20/42], [94mLoss[0m : 2.15001
[1mStep[0m  [24/42], [94mLoss[0m : 2.24762
[1mStep[0m  [28/42], [94mLoss[0m : 2.38386
[1mStep[0m  [32/42], [94mLoss[0m : 2.30009
[1mStep[0m  [36/42], [94mLoss[0m : 2.43370
[1mStep[0m  [40/42], [94mLoss[0m : 2.17913

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.307, [92mTest[0m: 2.471, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.04589
[1mStep[0m  [4/42], [94mLoss[0m : 2.32998
[1mStep[0m  [8/42], [94mLoss[0m : 2.19580
[1mStep[0m  [12/42], [94mLoss[0m : 2.22223
[1mStep[0m  [16/42], [94mLoss[0m : 2.06647
[1mStep[0m  [20/42], [94mLoss[0m : 2.22730
[1mStep[0m  [24/42], [94mLoss[0m : 2.24080
[1mStep[0m  [28/42], [94mLoss[0m : 2.29064
[1mStep[0m  [32/42], [94mLoss[0m : 2.32024
[1mStep[0m  [36/42], [94mLoss[0m : 2.28425
[1mStep[0m  [40/42], [94mLoss[0m : 2.27586

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.253, [92mTest[0m: 2.451, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.16047
[1mStep[0m  [4/42], [94mLoss[0m : 2.11183
[1mStep[0m  [8/42], [94mLoss[0m : 2.19156
[1mStep[0m  [12/42], [94mLoss[0m : 2.37095
[1mStep[0m  [16/42], [94mLoss[0m : 2.18229
[1mStep[0m  [20/42], [94mLoss[0m : 2.02727
[1mStep[0m  [24/42], [94mLoss[0m : 2.12646
[1mStep[0m  [28/42], [94mLoss[0m : 2.08487
[1mStep[0m  [32/42], [94mLoss[0m : 2.36170
[1mStep[0m  [36/42], [94mLoss[0m : 2.25593
[1mStep[0m  [40/42], [94mLoss[0m : 2.30861

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.218, [92mTest[0m: 2.471, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.14324
[1mStep[0m  [4/42], [94mLoss[0m : 2.24400
[1mStep[0m  [8/42], [94mLoss[0m : 2.29069
[1mStep[0m  [12/42], [94mLoss[0m : 2.25243
[1mStep[0m  [16/42], [94mLoss[0m : 2.05142
[1mStep[0m  [20/42], [94mLoss[0m : 2.06594
[1mStep[0m  [24/42], [94mLoss[0m : 1.96700
[1mStep[0m  [28/42], [94mLoss[0m : 1.93052
[1mStep[0m  [32/42], [94mLoss[0m : 2.21250
[1mStep[0m  [36/42], [94mLoss[0m : 2.16030
[1mStep[0m  [40/42], [94mLoss[0m : 2.30476

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.185, [92mTest[0m: 2.434, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21801
[1mStep[0m  [4/42], [94mLoss[0m : 2.02949
[1mStep[0m  [8/42], [94mLoss[0m : 2.21494
[1mStep[0m  [12/42], [94mLoss[0m : 2.01176
[1mStep[0m  [16/42], [94mLoss[0m : 1.92460
[1mStep[0m  [20/42], [94mLoss[0m : 2.16516
[1mStep[0m  [24/42], [94mLoss[0m : 1.87403
[1mStep[0m  [28/42], [94mLoss[0m : 1.97855
[1mStep[0m  [32/42], [94mLoss[0m : 2.16574
[1mStep[0m  [36/42], [94mLoss[0m : 2.27160
[1mStep[0m  [40/42], [94mLoss[0m : 2.23505

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.151, [92mTest[0m: 2.460, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.20815
[1mStep[0m  [4/42], [94mLoss[0m : 2.05096
[1mStep[0m  [8/42], [94mLoss[0m : 2.16922
[1mStep[0m  [12/42], [94mLoss[0m : 1.99296
[1mStep[0m  [16/42], [94mLoss[0m : 2.10234
[1mStep[0m  [20/42], [94mLoss[0m : 2.10822
[1mStep[0m  [24/42], [94mLoss[0m : 2.04139
[1mStep[0m  [28/42], [94mLoss[0m : 2.18946
[1mStep[0m  [32/42], [94mLoss[0m : 2.15716
[1mStep[0m  [36/42], [94mLoss[0m : 2.16208
[1mStep[0m  [40/42], [94mLoss[0m : 2.34374

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.121, [92mTest[0m: 2.434, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.16757
[1mStep[0m  [4/42], [94mLoss[0m : 2.05707
[1mStep[0m  [8/42], [94mLoss[0m : 2.11890
[1mStep[0m  [12/42], [94mLoss[0m : 2.17866
[1mStep[0m  [16/42], [94mLoss[0m : 2.10822
[1mStep[0m  [20/42], [94mLoss[0m : 1.97034
[1mStep[0m  [24/42], [94mLoss[0m : 2.13306
[1mStep[0m  [28/42], [94mLoss[0m : 2.09017
[1mStep[0m  [32/42], [94mLoss[0m : 1.94272
[1mStep[0m  [36/42], [94mLoss[0m : 2.07375
[1mStep[0m  [40/42], [94mLoss[0m : 2.13521

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.070, [92mTest[0m: 2.453, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.94745
[1mStep[0m  [4/42], [94mLoss[0m : 2.01940
[1mStep[0m  [8/42], [94mLoss[0m : 2.05345
[1mStep[0m  [12/42], [94mLoss[0m : 1.97682
[1mStep[0m  [16/42], [94mLoss[0m : 2.00392
[1mStep[0m  [20/42], [94mLoss[0m : 2.10082
[1mStep[0m  [24/42], [94mLoss[0m : 2.06400
[1mStep[0m  [28/42], [94mLoss[0m : 1.99551
[1mStep[0m  [32/42], [94mLoss[0m : 2.05139
[1mStep[0m  [36/42], [94mLoss[0m : 2.03052
[1mStep[0m  [40/42], [94mLoss[0m : 2.12106

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.054, [92mTest[0m: 2.486, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.92617
[1mStep[0m  [4/42], [94mLoss[0m : 2.06455
[1mStep[0m  [8/42], [94mLoss[0m : 1.88315
[1mStep[0m  [12/42], [94mLoss[0m : 2.00391
[1mStep[0m  [16/42], [94mLoss[0m : 1.91569
[1mStep[0m  [20/42], [94mLoss[0m : 1.87337
[1mStep[0m  [24/42], [94mLoss[0m : 2.12320
[1mStep[0m  [28/42], [94mLoss[0m : 2.03758
[1mStep[0m  [32/42], [94mLoss[0m : 2.09815
[1mStep[0m  [36/42], [94mLoss[0m : 2.13125
[1mStep[0m  [40/42], [94mLoss[0m : 2.05893

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.015, [92mTest[0m: 2.492, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.95541
[1mStep[0m  [4/42], [94mLoss[0m : 2.10250
[1mStep[0m  [8/42], [94mLoss[0m : 1.83413
[1mStep[0m  [12/42], [94mLoss[0m : 1.75877
[1mStep[0m  [16/42], [94mLoss[0m : 2.10964
[1mStep[0m  [20/42], [94mLoss[0m : 2.16045
[1mStep[0m  [24/42], [94mLoss[0m : 2.17669
[1mStep[0m  [28/42], [94mLoss[0m : 2.00285
[1mStep[0m  [32/42], [94mLoss[0m : 1.93856
[1mStep[0m  [36/42], [94mLoss[0m : 1.96797
[1mStep[0m  [40/42], [94mLoss[0m : 1.97206

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.984, [92mTest[0m: 2.459, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.82486
[1mStep[0m  [4/42], [94mLoss[0m : 1.99763
[1mStep[0m  [8/42], [94mLoss[0m : 1.98332
[1mStep[0m  [12/42], [94mLoss[0m : 2.00087
[1mStep[0m  [16/42], [94mLoss[0m : 1.79717
[1mStep[0m  [20/42], [94mLoss[0m : 1.77292
[1mStep[0m  [24/42], [94mLoss[0m : 2.08170
[1mStep[0m  [28/42], [94mLoss[0m : 1.93913
[1mStep[0m  [32/42], [94mLoss[0m : 1.92289
[1mStep[0m  [36/42], [94mLoss[0m : 1.96173
[1mStep[0m  [40/42], [94mLoss[0m : 2.12345

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.946, [92mTest[0m: 2.534, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.91624
[1mStep[0m  [4/42], [94mLoss[0m : 1.82519
[1mStep[0m  [8/42], [94mLoss[0m : 1.81834
[1mStep[0m  [12/42], [94mLoss[0m : 1.89409
[1mStep[0m  [16/42], [94mLoss[0m : 1.89472
[1mStep[0m  [20/42], [94mLoss[0m : 1.93363
[1mStep[0m  [24/42], [94mLoss[0m : 1.89182
[1mStep[0m  [28/42], [94mLoss[0m : 1.76638
[1mStep[0m  [32/42], [94mLoss[0m : 1.95538
[1mStep[0m  [36/42], [94mLoss[0m : 2.08670
[1mStep[0m  [40/42], [94mLoss[0m : 1.97066

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.926, [92mTest[0m: 2.503, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.85037
[1mStep[0m  [4/42], [94mLoss[0m : 2.04573
[1mStep[0m  [8/42], [94mLoss[0m : 1.96073
[1mStep[0m  [12/42], [94mLoss[0m : 2.00186
[1mStep[0m  [16/42], [94mLoss[0m : 1.86372
[1mStep[0m  [20/42], [94mLoss[0m : 1.87163
[1mStep[0m  [24/42], [94mLoss[0m : 1.78023
[1mStep[0m  [28/42], [94mLoss[0m : 1.84887
[1mStep[0m  [32/42], [94mLoss[0m : 1.97101
[1mStep[0m  [36/42], [94mLoss[0m : 1.93715
[1mStep[0m  [40/42], [94mLoss[0m : 1.83868

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.897, [92mTest[0m: 2.444, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.90462
[1mStep[0m  [4/42], [94mLoss[0m : 1.88415
[1mStep[0m  [8/42], [94mLoss[0m : 1.95703
[1mStep[0m  [12/42], [94mLoss[0m : 1.94259
[1mStep[0m  [16/42], [94mLoss[0m : 1.66123
[1mStep[0m  [20/42], [94mLoss[0m : 1.84535
[1mStep[0m  [24/42], [94mLoss[0m : 1.68540
[1mStep[0m  [28/42], [94mLoss[0m : 2.14256
[1mStep[0m  [32/42], [94mLoss[0m : 1.78742
[1mStep[0m  [36/42], [94mLoss[0m : 1.92952
[1mStep[0m  [40/42], [94mLoss[0m : 1.78189

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.885, [92mTest[0m: 2.506, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.85831
[1mStep[0m  [4/42], [94mLoss[0m : 1.95252
[1mStep[0m  [8/42], [94mLoss[0m : 1.90027
[1mStep[0m  [12/42], [94mLoss[0m : 1.84104
[1mStep[0m  [16/42], [94mLoss[0m : 1.81229
[1mStep[0m  [20/42], [94mLoss[0m : 1.94787
[1mStep[0m  [24/42], [94mLoss[0m : 1.80347
[1mStep[0m  [28/42], [94mLoss[0m : 1.99183
[1mStep[0m  [32/42], [94mLoss[0m : 1.88704
[1mStep[0m  [36/42], [94mLoss[0m : 1.81493
[1mStep[0m  [40/42], [94mLoss[0m : 1.89239

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.879, [92mTest[0m: 2.493, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.77535
[1mStep[0m  [4/42], [94mLoss[0m : 1.86265
[1mStep[0m  [8/42], [94mLoss[0m : 1.71912
[1mStep[0m  [12/42], [94mLoss[0m : 1.95208
[1mStep[0m  [16/42], [94mLoss[0m : 1.76350
[1mStep[0m  [20/42], [94mLoss[0m : 1.91417
[1mStep[0m  [24/42], [94mLoss[0m : 1.70670
[1mStep[0m  [28/42], [94mLoss[0m : 1.79946
[1mStep[0m  [32/42], [94mLoss[0m : 1.84815
[1mStep[0m  [36/42], [94mLoss[0m : 1.90997
[1mStep[0m  [40/42], [94mLoss[0m : 1.86945

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.830, [92mTest[0m: 2.524, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.79494
[1mStep[0m  [4/42], [94mLoss[0m : 1.85126
[1mStep[0m  [8/42], [94mLoss[0m : 1.78027
[1mStep[0m  [12/42], [94mLoss[0m : 1.74674
[1mStep[0m  [16/42], [94mLoss[0m : 1.88551
[1mStep[0m  [20/42], [94mLoss[0m : 1.85897
[1mStep[0m  [24/42], [94mLoss[0m : 1.94227
[1mStep[0m  [28/42], [94mLoss[0m : 1.73687
[1mStep[0m  [32/42], [94mLoss[0m : 1.71172
[1mStep[0m  [36/42], [94mLoss[0m : 1.58581
[1mStep[0m  [40/42], [94mLoss[0m : 1.74553

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.814, [92mTest[0m: 2.539, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.80496
[1mStep[0m  [4/42], [94mLoss[0m : 1.79906
[1mStep[0m  [8/42], [94mLoss[0m : 1.74653
[1mStep[0m  [12/42], [94mLoss[0m : 1.83985
[1mStep[0m  [16/42], [94mLoss[0m : 2.05184
[1mStep[0m  [20/42], [94mLoss[0m : 1.76024
[1mStep[0m  [24/42], [94mLoss[0m : 1.82747
[1mStep[0m  [28/42], [94mLoss[0m : 1.86238
[1mStep[0m  [32/42], [94mLoss[0m : 1.84985
[1mStep[0m  [36/42], [94mLoss[0m : 1.80276
[1mStep[0m  [40/42], [94mLoss[0m : 1.79444

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.801, [92mTest[0m: 2.575, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.472
====================================

Phase 2 - Evaluation MAE:  2.471506817000253
MAE score P1       2.359464
MAE score P2       2.471507
loss               1.801372
learning_rate      0.007525
batch_size              256
hidden_sizes      [250, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.5
weight_decay         0.0001
Name: 31, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 10.93391
[1mStep[0m  [8/84], [94mLoss[0m : 11.17008
[1mStep[0m  [16/84], [94mLoss[0m : 10.28779
[1mStep[0m  [24/84], [94mLoss[0m : 8.94046
[1mStep[0m  [32/84], [94mLoss[0m : 9.09113
[1mStep[0m  [40/84], [94mLoss[0m : 8.88227
[1mStep[0m  [48/84], [94mLoss[0m : 7.11844
[1mStep[0m  [56/84], [94mLoss[0m : 6.56028
[1mStep[0m  [64/84], [94mLoss[0m : 6.50186
[1mStep[0m  [72/84], [94mLoss[0m : 5.84389
[1mStep[0m  [80/84], [94mLoss[0m : 3.88991

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 8.141, [92mTest[0m: 11.045, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.24556
[1mStep[0m  [8/84], [94mLoss[0m : 4.74475
[1mStep[0m  [16/84], [94mLoss[0m : 3.68669
[1mStep[0m  [24/84], [94mLoss[0m : 3.54462
[1mStep[0m  [32/84], [94mLoss[0m : 3.17654
[1mStep[0m  [40/84], [94mLoss[0m : 3.18847
[1mStep[0m  [48/84], [94mLoss[0m : 3.18535
[1mStep[0m  [56/84], [94mLoss[0m : 2.96991
[1mStep[0m  [64/84], [94mLoss[0m : 2.65619
[1mStep[0m  [72/84], [94mLoss[0m : 2.47740
[1mStep[0m  [80/84], [94mLoss[0m : 2.64272

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.305, [92mTest[0m: 6.199, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.71849
[1mStep[0m  [8/84], [94mLoss[0m : 2.65710
[1mStep[0m  [16/84], [94mLoss[0m : 2.81429
[1mStep[0m  [24/84], [94mLoss[0m : 2.95326
[1mStep[0m  [32/84], [94mLoss[0m : 2.85810
[1mStep[0m  [40/84], [94mLoss[0m : 3.03485
[1mStep[0m  [48/84], [94mLoss[0m : 2.81830
[1mStep[0m  [56/84], [94mLoss[0m : 2.52858
[1mStep[0m  [64/84], [94mLoss[0m : 2.78406
[1mStep[0m  [72/84], [94mLoss[0m : 2.57241
[1mStep[0m  [80/84], [94mLoss[0m : 2.93723

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.712, [92mTest[0m: 2.700, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.62646
[1mStep[0m  [8/84], [94mLoss[0m : 2.41552
[1mStep[0m  [16/84], [94mLoss[0m : 2.66025
[1mStep[0m  [24/84], [94mLoss[0m : 2.91684
[1mStep[0m  [32/84], [94mLoss[0m : 2.89691
[1mStep[0m  [40/84], [94mLoss[0m : 2.81976
[1mStep[0m  [48/84], [94mLoss[0m : 2.65076
[1mStep[0m  [56/84], [94mLoss[0m : 2.67655
[1mStep[0m  [64/84], [94mLoss[0m : 2.75592
[1mStep[0m  [72/84], [94mLoss[0m : 2.74551
[1mStep[0m  [80/84], [94mLoss[0m : 2.61685

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.654, [92mTest[0m: 2.467, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67699
[1mStep[0m  [8/84], [94mLoss[0m : 2.60890
[1mStep[0m  [16/84], [94mLoss[0m : 2.75274
[1mStep[0m  [24/84], [94mLoss[0m : 2.42522
[1mStep[0m  [32/84], [94mLoss[0m : 2.34951
[1mStep[0m  [40/84], [94mLoss[0m : 2.66358
[1mStep[0m  [48/84], [94mLoss[0m : 2.53342
[1mStep[0m  [56/84], [94mLoss[0m : 2.63382
[1mStep[0m  [64/84], [94mLoss[0m : 2.75527
[1mStep[0m  [72/84], [94mLoss[0m : 2.60615
[1mStep[0m  [80/84], [94mLoss[0m : 2.32941

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.637, [92mTest[0m: 2.413, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.88182
[1mStep[0m  [8/84], [94mLoss[0m : 2.56847
[1mStep[0m  [16/84], [94mLoss[0m : 2.53365
[1mStep[0m  [24/84], [94mLoss[0m : 2.64500
[1mStep[0m  [32/84], [94mLoss[0m : 2.65738
[1mStep[0m  [40/84], [94mLoss[0m : 2.48701
[1mStep[0m  [48/84], [94mLoss[0m : 2.48923
[1mStep[0m  [56/84], [94mLoss[0m : 2.77290
[1mStep[0m  [64/84], [94mLoss[0m : 2.70540
[1mStep[0m  [72/84], [94mLoss[0m : 2.62381
[1mStep[0m  [80/84], [94mLoss[0m : 2.65295

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.584, [92mTest[0m: 2.423, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.93800
[1mStep[0m  [8/84], [94mLoss[0m : 2.58920
[1mStep[0m  [16/84], [94mLoss[0m : 2.47334
[1mStep[0m  [24/84], [94mLoss[0m : 2.54617
[1mStep[0m  [32/84], [94mLoss[0m : 2.52727
[1mStep[0m  [40/84], [94mLoss[0m : 2.40156
[1mStep[0m  [48/84], [94mLoss[0m : 2.43847
[1mStep[0m  [56/84], [94mLoss[0m : 2.53285
[1mStep[0m  [64/84], [94mLoss[0m : 2.71495
[1mStep[0m  [72/84], [94mLoss[0m : 2.66783
[1mStep[0m  [80/84], [94mLoss[0m : 2.64558

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.564, [92mTest[0m: 2.386, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53791
[1mStep[0m  [8/84], [94mLoss[0m : 2.24227
[1mStep[0m  [16/84], [94mLoss[0m : 2.50584
[1mStep[0m  [24/84], [94mLoss[0m : 2.28442
[1mStep[0m  [32/84], [94mLoss[0m : 2.70146
[1mStep[0m  [40/84], [94mLoss[0m : 2.35234
[1mStep[0m  [48/84], [94mLoss[0m : 2.59559
[1mStep[0m  [56/84], [94mLoss[0m : 2.30036
[1mStep[0m  [64/84], [94mLoss[0m : 3.04056
[1mStep[0m  [72/84], [94mLoss[0m : 2.51836
[1mStep[0m  [80/84], [94mLoss[0m : 2.77272

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.553, [92mTest[0m: 2.397, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54532
[1mStep[0m  [8/84], [94mLoss[0m : 2.16184
[1mStep[0m  [16/84], [94mLoss[0m : 2.79963
[1mStep[0m  [24/84], [94mLoss[0m : 2.25739
[1mStep[0m  [32/84], [94mLoss[0m : 2.70563
[1mStep[0m  [40/84], [94mLoss[0m : 2.82311
[1mStep[0m  [48/84], [94mLoss[0m : 2.45411
[1mStep[0m  [56/84], [94mLoss[0m : 2.51124
[1mStep[0m  [64/84], [94mLoss[0m : 2.41513
[1mStep[0m  [72/84], [94mLoss[0m : 2.72459
[1mStep[0m  [80/84], [94mLoss[0m : 2.70602

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.535, [92mTest[0m: 2.356, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48766
[1mStep[0m  [8/84], [94mLoss[0m : 2.44096
[1mStep[0m  [16/84], [94mLoss[0m : 2.80765
[1mStep[0m  [24/84], [94mLoss[0m : 2.67932
[1mStep[0m  [32/84], [94mLoss[0m : 2.62394
[1mStep[0m  [40/84], [94mLoss[0m : 2.86308
[1mStep[0m  [48/84], [94mLoss[0m : 2.62754
[1mStep[0m  [56/84], [94mLoss[0m : 2.75104
[1mStep[0m  [64/84], [94mLoss[0m : 2.50268
[1mStep[0m  [72/84], [94mLoss[0m : 2.47517
[1mStep[0m  [80/84], [94mLoss[0m : 2.44070

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.531, [92mTest[0m: 2.344, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48869
[1mStep[0m  [8/84], [94mLoss[0m : 2.64832
[1mStep[0m  [16/84], [94mLoss[0m : 2.59143
[1mStep[0m  [24/84], [94mLoss[0m : 2.44978
[1mStep[0m  [32/84], [94mLoss[0m : 2.30766
[1mStep[0m  [40/84], [94mLoss[0m : 2.59416
[1mStep[0m  [48/84], [94mLoss[0m : 2.89357
[1mStep[0m  [56/84], [94mLoss[0m : 2.82333
[1mStep[0m  [64/84], [94mLoss[0m : 2.43705
[1mStep[0m  [72/84], [94mLoss[0m : 2.37584
[1mStep[0m  [80/84], [94mLoss[0m : 2.80564

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.539, [92mTest[0m: 2.348, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49063
[1mStep[0m  [8/84], [94mLoss[0m : 2.65432
[1mStep[0m  [16/84], [94mLoss[0m : 2.53762
[1mStep[0m  [24/84], [94mLoss[0m : 2.29814
[1mStep[0m  [32/84], [94mLoss[0m : 2.56427
[1mStep[0m  [40/84], [94mLoss[0m : 2.55580
[1mStep[0m  [48/84], [94mLoss[0m : 2.58539
[1mStep[0m  [56/84], [94mLoss[0m : 2.57549
[1mStep[0m  [64/84], [94mLoss[0m : 2.45599
[1mStep[0m  [72/84], [94mLoss[0m : 2.36486
[1mStep[0m  [80/84], [94mLoss[0m : 2.23101

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.346, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.86556
[1mStep[0m  [8/84], [94mLoss[0m : 2.54747
[1mStep[0m  [16/84], [94mLoss[0m : 2.31446
[1mStep[0m  [24/84], [94mLoss[0m : 2.54210
[1mStep[0m  [32/84], [94mLoss[0m : 2.47373
[1mStep[0m  [40/84], [94mLoss[0m : 2.87452
[1mStep[0m  [48/84], [94mLoss[0m : 2.35860
[1mStep[0m  [56/84], [94mLoss[0m : 2.45128
[1mStep[0m  [64/84], [94mLoss[0m : 2.61805
[1mStep[0m  [72/84], [94mLoss[0m : 2.49979
[1mStep[0m  [80/84], [94mLoss[0m : 2.54584

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.336, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43368
[1mStep[0m  [8/84], [94mLoss[0m : 2.39616
[1mStep[0m  [16/84], [94mLoss[0m : 2.42371
[1mStep[0m  [24/84], [94mLoss[0m : 2.73747
[1mStep[0m  [32/84], [94mLoss[0m : 2.36166
[1mStep[0m  [40/84], [94mLoss[0m : 2.13871
[1mStep[0m  [48/84], [94mLoss[0m : 2.66810
[1mStep[0m  [56/84], [94mLoss[0m : 2.67948
[1mStep[0m  [64/84], [94mLoss[0m : 2.36277
[1mStep[0m  [72/84], [94mLoss[0m : 2.22590
[1mStep[0m  [80/84], [94mLoss[0m : 2.47325

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.343, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.32357
[1mStep[0m  [8/84], [94mLoss[0m : 2.63186
[1mStep[0m  [16/84], [94mLoss[0m : 2.71838
[1mStep[0m  [24/84], [94mLoss[0m : 2.55842
[1mStep[0m  [32/84], [94mLoss[0m : 2.43296
[1mStep[0m  [40/84], [94mLoss[0m : 2.49198
[1mStep[0m  [48/84], [94mLoss[0m : 2.86307
[1mStep[0m  [56/84], [94mLoss[0m : 2.59416
[1mStep[0m  [64/84], [94mLoss[0m : 2.65009
[1mStep[0m  [72/84], [94mLoss[0m : 2.47386
[1mStep[0m  [80/84], [94mLoss[0m : 2.55133

====================================
