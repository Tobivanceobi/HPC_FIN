no change     /home/modelrep/sadiya/miniconda/condabin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda-env
no change     /home/modelrep/sadiya/miniconda/bin/activate
no change     /home/modelrep/sadiya/miniconda/bin/deactivate
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.sh
no change     /home/modelrep/sadiya/miniconda/etc/fish/conf.d/conda.fish
no change     /home/modelrep/sadiya/miniconda/shell/condabin/Conda.psm1
no change     /home/modelrep/sadiya/miniconda/shell/condabin/conda-hook.ps1
no change     /home/modelrep/sadiya/miniconda/lib/python3.10/site-packages/xontrib/conda.xsh
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.csh
no change     /home/modelrep/sadiya/.bashrc
No action taken.
-----------------
Process ID:  11
Number of Nodes:  1
Number of Array Tasks:  16
-----------------
Using device: cuda

AMD Instinct MI210
Memory Usage:
Allocated: 0.0 GB
Cached:    0.0 GB
theta
whole_spec
delta
alpha
beta
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 11.08122
[1mStep[0m  [8/84], [94mLoss[0m : 10.34869
[1mStep[0m  [16/84], [94mLoss[0m : 9.68590
[1mStep[0m  [24/84], [94mLoss[0m : 8.00320
[1mStep[0m  [32/84], [94mLoss[0m : 7.65980
[1mStep[0m  [40/84], [94mLoss[0m : 6.50158
[1mStep[0m  [48/84], [94mLoss[0m : 5.71371
[1mStep[0m  [56/84], [94mLoss[0m : 4.78773
[1mStep[0m  [64/84], [94mLoss[0m : 4.32251
[1mStep[0m  [72/84], [94mLoss[0m : 3.73344
[1mStep[0m  [80/84], [94mLoss[0m : 4.14354

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.767, [92mTest[0m: 11.176, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.66810
[1mStep[0m  [8/84], [94mLoss[0m : 3.01618
[1mStep[0m  [16/84], [94mLoss[0m : 3.38487
[1mStep[0m  [24/84], [94mLoss[0m : 2.95420
[1mStep[0m  [32/84], [94mLoss[0m : 2.48703
[1mStep[0m  [40/84], [94mLoss[0m : 2.86059
[1mStep[0m  [48/84], [94mLoss[0m : 3.16759
[1mStep[0m  [56/84], [94mLoss[0m : 2.41940
[1mStep[0m  [64/84], [94mLoss[0m : 2.86356
[1mStep[0m  [72/84], [94mLoss[0m : 2.44363
[1mStep[0m  [80/84], [94mLoss[0m : 3.12529

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.919, [92mTest[0m: 3.313, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.97500
[1mStep[0m  [8/84], [94mLoss[0m : 2.80763
[1mStep[0m  [16/84], [94mLoss[0m : 2.42773
[1mStep[0m  [24/84], [94mLoss[0m : 3.19796
[1mStep[0m  [32/84], [94mLoss[0m : 2.95413
[1mStep[0m  [40/84], [94mLoss[0m : 2.57779
[1mStep[0m  [48/84], [94mLoss[0m : 2.35775
[1mStep[0m  [56/84], [94mLoss[0m : 2.77554
[1mStep[0m  [64/84], [94mLoss[0m : 2.58883
[1mStep[0m  [72/84], [94mLoss[0m : 2.52031
[1mStep[0m  [80/84], [94mLoss[0m : 2.88745

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.709, [92mTest[0m: 2.450, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35412
[1mStep[0m  [8/84], [94mLoss[0m : 2.67599
[1mStep[0m  [16/84], [94mLoss[0m : 2.69241
[1mStep[0m  [24/84], [94mLoss[0m : 2.82317
[1mStep[0m  [32/84], [94mLoss[0m : 2.97734
[1mStep[0m  [40/84], [94mLoss[0m : 2.40852
[1mStep[0m  [48/84], [94mLoss[0m : 2.89008
[1mStep[0m  [56/84], [94mLoss[0m : 2.85170
[1mStep[0m  [64/84], [94mLoss[0m : 2.72246
[1mStep[0m  [72/84], [94mLoss[0m : 2.85578
[1mStep[0m  [80/84], [94mLoss[0m : 2.60434

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.685, [92mTest[0m: 2.411, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.66122
[1mStep[0m  [8/84], [94mLoss[0m : 2.41656
[1mStep[0m  [16/84], [94mLoss[0m : 2.75816
[1mStep[0m  [24/84], [94mLoss[0m : 2.28892
[1mStep[0m  [32/84], [94mLoss[0m : 2.76571
[1mStep[0m  [40/84], [94mLoss[0m : 2.77701
[1mStep[0m  [48/84], [94mLoss[0m : 2.68642
[1mStep[0m  [56/84], [94mLoss[0m : 2.49496
[1mStep[0m  [64/84], [94mLoss[0m : 2.85890
[1mStep[0m  [72/84], [94mLoss[0m : 2.55456
[1mStep[0m  [80/84], [94mLoss[0m : 2.65713

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.680, [92mTest[0m: 2.391, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.80369
[1mStep[0m  [8/84], [94mLoss[0m : 2.62562
[1mStep[0m  [16/84], [94mLoss[0m : 2.51381
[1mStep[0m  [24/84], [94mLoss[0m : 2.58820
[1mStep[0m  [32/84], [94mLoss[0m : 2.80803
[1mStep[0m  [40/84], [94mLoss[0m : 2.78026
[1mStep[0m  [48/84], [94mLoss[0m : 2.48370
[1mStep[0m  [56/84], [94mLoss[0m : 2.80717
[1mStep[0m  [64/84], [94mLoss[0m : 2.51843
[1mStep[0m  [72/84], [94mLoss[0m : 2.60026
[1mStep[0m  [80/84], [94mLoss[0m : 2.44791

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.670, [92mTest[0m: 2.375, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55656
[1mStep[0m  [8/84], [94mLoss[0m : 2.79867
[1mStep[0m  [16/84], [94mLoss[0m : 2.79069
[1mStep[0m  [24/84], [94mLoss[0m : 2.70132
[1mStep[0m  [32/84], [94mLoss[0m : 2.26311
[1mStep[0m  [40/84], [94mLoss[0m : 2.41269
[1mStep[0m  [48/84], [94mLoss[0m : 2.33083
[1mStep[0m  [56/84], [94mLoss[0m : 2.54123
[1mStep[0m  [64/84], [94mLoss[0m : 2.82535
[1mStep[0m  [72/84], [94mLoss[0m : 2.92218
[1mStep[0m  [80/84], [94mLoss[0m : 2.55056

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.641, [92mTest[0m: 2.362, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.70902
[1mStep[0m  [8/84], [94mLoss[0m : 2.81062
[1mStep[0m  [16/84], [94mLoss[0m : 2.14158
[1mStep[0m  [24/84], [94mLoss[0m : 2.67065
[1mStep[0m  [32/84], [94mLoss[0m : 2.58749
[1mStep[0m  [40/84], [94mLoss[0m : 2.86931
[1mStep[0m  [48/84], [94mLoss[0m : 2.54566
[1mStep[0m  [56/84], [94mLoss[0m : 2.70684
[1mStep[0m  [64/84], [94mLoss[0m : 2.63640
[1mStep[0m  [72/84], [94mLoss[0m : 2.48335
[1mStep[0m  [80/84], [94mLoss[0m : 2.36059

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.649, [92mTest[0m: 2.361, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50621
[1mStep[0m  [8/84], [94mLoss[0m : 2.67775
[1mStep[0m  [16/84], [94mLoss[0m : 2.50422
[1mStep[0m  [24/84], [94mLoss[0m : 2.57541
[1mStep[0m  [32/84], [94mLoss[0m : 2.50819
[1mStep[0m  [40/84], [94mLoss[0m : 2.76969
[1mStep[0m  [48/84], [94mLoss[0m : 2.89577
[1mStep[0m  [56/84], [94mLoss[0m : 2.58904
[1mStep[0m  [64/84], [94mLoss[0m : 2.45079
[1mStep[0m  [72/84], [94mLoss[0m : 2.83915
[1mStep[0m  [80/84], [94mLoss[0m : 2.43461

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.652, [92mTest[0m: 2.356, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.82145
[1mStep[0m  [8/84], [94mLoss[0m : 2.48691
[1mStep[0m  [16/84], [94mLoss[0m : 2.44550
[1mStep[0m  [24/84], [94mLoss[0m : 2.48502
[1mStep[0m  [32/84], [94mLoss[0m : 2.70023
[1mStep[0m  [40/84], [94mLoss[0m : 2.19301
[1mStep[0m  [48/84], [94mLoss[0m : 2.42951
[1mStep[0m  [56/84], [94mLoss[0m : 2.74344
[1mStep[0m  [64/84], [94mLoss[0m : 2.73786
[1mStep[0m  [72/84], [94mLoss[0m : 2.64502
[1mStep[0m  [80/84], [94mLoss[0m : 2.13130

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.640, [92mTest[0m: 2.355, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.76550
[1mStep[0m  [8/84], [94mLoss[0m : 2.80689
[1mStep[0m  [16/84], [94mLoss[0m : 2.49983
[1mStep[0m  [24/84], [94mLoss[0m : 2.79102
[1mStep[0m  [32/84], [94mLoss[0m : 2.42428
[1mStep[0m  [40/84], [94mLoss[0m : 2.52600
[1mStep[0m  [48/84], [94mLoss[0m : 2.82521
[1mStep[0m  [56/84], [94mLoss[0m : 2.76430
[1mStep[0m  [64/84], [94mLoss[0m : 2.55011
[1mStep[0m  [72/84], [94mLoss[0m : 2.79345
[1mStep[0m  [80/84], [94mLoss[0m : 2.54191

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.611, [92mTest[0m: 2.354, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.75176
[1mStep[0m  [8/84], [94mLoss[0m : 2.82955
[1mStep[0m  [16/84], [94mLoss[0m : 2.35276
[1mStep[0m  [24/84], [94mLoss[0m : 2.70772
[1mStep[0m  [32/84], [94mLoss[0m : 2.66193
[1mStep[0m  [40/84], [94mLoss[0m : 2.75682
[1mStep[0m  [48/84], [94mLoss[0m : 2.65995
[1mStep[0m  [56/84], [94mLoss[0m : 2.76074
[1mStep[0m  [64/84], [94mLoss[0m : 2.73885
[1mStep[0m  [72/84], [94mLoss[0m : 2.47222
[1mStep[0m  [80/84], [94mLoss[0m : 2.74640

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.636, [92mTest[0m: 2.341, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60954
[1mStep[0m  [8/84], [94mLoss[0m : 2.61465
[1mStep[0m  [16/84], [94mLoss[0m : 2.70525
[1mStep[0m  [24/84], [94mLoss[0m : 2.48978
[1mStep[0m  [32/84], [94mLoss[0m : 2.64710
[1mStep[0m  [40/84], [94mLoss[0m : 2.40996
[1mStep[0m  [48/84], [94mLoss[0m : 2.49910
[1mStep[0m  [56/84], [94mLoss[0m : 2.68939
[1mStep[0m  [64/84], [94mLoss[0m : 2.71630
[1mStep[0m  [72/84], [94mLoss[0m : 2.58908
[1mStep[0m  [80/84], [94mLoss[0m : 2.63669

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.628, [92mTest[0m: 2.344, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.72794
[1mStep[0m  [8/84], [94mLoss[0m : 2.85517
[1mStep[0m  [16/84], [94mLoss[0m : 2.67167
[1mStep[0m  [24/84], [94mLoss[0m : 2.72334
[1mStep[0m  [32/84], [94mLoss[0m : 2.85772
[1mStep[0m  [40/84], [94mLoss[0m : 2.83002
[1mStep[0m  [48/84], [94mLoss[0m : 2.93480
[1mStep[0m  [56/84], [94mLoss[0m : 2.63686
[1mStep[0m  [64/84], [94mLoss[0m : 2.29354
[1mStep[0m  [72/84], [94mLoss[0m : 2.51225
[1mStep[0m  [80/84], [94mLoss[0m : 2.56777

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.651, [92mTest[0m: 2.343, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46075
[1mStep[0m  [8/84], [94mLoss[0m : 2.78515
[1mStep[0m  [16/84], [94mLoss[0m : 2.45412
[1mStep[0m  [24/84], [94mLoss[0m : 2.57334
[1mStep[0m  [32/84], [94mLoss[0m : 2.82815
[1mStep[0m  [40/84], [94mLoss[0m : 2.62383
[1mStep[0m  [48/84], [94mLoss[0m : 2.72276
[1mStep[0m  [56/84], [94mLoss[0m : 2.61224
[1mStep[0m  [64/84], [94mLoss[0m : 2.30441
[1mStep[0m  [72/84], [94mLoss[0m : 2.29035
[1mStep[0m  [80/84], [94mLoss[0m : 2.68379

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.625, [92mTest[0m: 2.346, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.64131
[1mStep[0m  [8/84], [94mLoss[0m : 2.57722
[1mStep[0m  [16/84], [94mLoss[0m : 2.66174
[1mStep[0m  [24/84], [94mLoss[0m : 2.57622
[1mStep[0m  [32/84], [94mLoss[0m : 2.62561
[1mStep[0m  [40/84], [94mLoss[0m : 2.42732
[1mStep[0m  [48/84], [94mLoss[0m : 2.59878
[1mStep[0m  [56/84], [94mLoss[0m : 2.56289
[1mStep[0m  [64/84], [94mLoss[0m : 2.59106
[1mStep[0m  [72/84], [94mLoss[0m : 2.59749
[1mStep[0m  [80/84], [94mLoss[0m : 2.46524

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.634, [92mTest[0m: 2.348, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49356
[1mStep[0m  [8/84], [94mLoss[0m : 2.61795
[1mStep[0m  [16/84], [94mLoss[0m : 2.39448
[1mStep[0m  [24/84], [94mLoss[0m : 2.59436
[1mStep[0m  [32/84], [94mLoss[0m : 2.42297
[1mStep[0m  [40/84], [94mLoss[0m : 2.36428
[1mStep[0m  [48/84], [94mLoss[0m : 2.56887
[1mStep[0m  [56/84], [94mLoss[0m : 2.70997
[1mStep[0m  [64/84], [94mLoss[0m : 2.67079
[1mStep[0m  [72/84], [94mLoss[0m : 2.53573
[1mStep[0m  [80/84], [94mLoss[0m : 2.22626

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.615, [92mTest[0m: 2.342, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.57251
[1mStep[0m  [8/84], [94mLoss[0m : 2.67347
[1mStep[0m  [16/84], [94mLoss[0m : 2.59758
[1mStep[0m  [24/84], [94mLoss[0m : 2.39533
[1mStep[0m  [32/84], [94mLoss[0m : 2.32463
[1mStep[0m  [40/84], [94mLoss[0m : 2.44889
[1mStep[0m  [48/84], [94mLoss[0m : 2.37375
[1mStep[0m  [56/84], [94mLoss[0m : 2.78705
[1mStep[0m  [64/84], [94mLoss[0m : 2.89994
[1mStep[0m  [72/84], [94mLoss[0m : 2.59153
[1mStep[0m  [80/84], [94mLoss[0m : 2.66972

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.606, [92mTest[0m: 2.338, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.79788
[1mStep[0m  [8/84], [94mLoss[0m : 2.61689
[1mStep[0m  [16/84], [94mLoss[0m : 2.45898
[1mStep[0m  [24/84], [94mLoss[0m : 2.67681
[1mStep[0m  [32/84], [94mLoss[0m : 2.84590
[1mStep[0m  [40/84], [94mLoss[0m : 2.57701
[1mStep[0m  [48/84], [94mLoss[0m : 2.51688
[1mStep[0m  [56/84], [94mLoss[0m : 2.85275
[1mStep[0m  [64/84], [94mLoss[0m : 2.55819
[1mStep[0m  [72/84], [94mLoss[0m : 2.81449
[1mStep[0m  [80/84], [94mLoss[0m : 2.58878

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.619, [92mTest[0m: 2.343, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.82603
[1mStep[0m  [8/84], [94mLoss[0m : 2.47660
[1mStep[0m  [16/84], [94mLoss[0m : 2.72559
[1mStep[0m  [24/84], [94mLoss[0m : 2.53178
[1mStep[0m  [32/84], [94mLoss[0m : 2.33415
[1mStep[0m  [40/84], [94mLoss[0m : 2.73798
[1mStep[0m  [48/84], [94mLoss[0m : 2.76946
[1mStep[0m  [56/84], [94mLoss[0m : 2.58693
[1mStep[0m  [64/84], [94mLoss[0m : 2.47092
[1mStep[0m  [72/84], [94mLoss[0m : 2.64658
[1mStep[0m  [80/84], [94mLoss[0m : 2.61107

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.603, [92mTest[0m: 2.335, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47313
[1mStep[0m  [8/84], [94mLoss[0m : 2.47030
[1mStep[0m  [16/84], [94mLoss[0m : 2.52046
[1mStep[0m  [24/84], [94mLoss[0m : 2.75078
[1mStep[0m  [32/84], [94mLoss[0m : 2.62293
[1mStep[0m  [40/84], [94mLoss[0m : 2.46143
[1mStep[0m  [48/84], [94mLoss[0m : 2.81295
[1mStep[0m  [56/84], [94mLoss[0m : 2.41142
[1mStep[0m  [64/84], [94mLoss[0m : 2.72785
[1mStep[0m  [72/84], [94mLoss[0m : 2.85511
[1mStep[0m  [80/84], [94mLoss[0m : 2.50420

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.595, [92mTest[0m: 2.342, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52741
[1mStep[0m  [8/84], [94mLoss[0m : 2.74604
[1mStep[0m  [16/84], [94mLoss[0m : 2.21592
[1mStep[0m  [24/84], [94mLoss[0m : 2.68789
[1mStep[0m  [32/84], [94mLoss[0m : 2.61837
[1mStep[0m  [40/84], [94mLoss[0m : 2.72146
[1mStep[0m  [48/84], [94mLoss[0m : 2.83032
[1mStep[0m  [56/84], [94mLoss[0m : 2.48981
[1mStep[0m  [64/84], [94mLoss[0m : 2.67937
[1mStep[0m  [72/84], [94mLoss[0m : 2.63594
[1mStep[0m  [80/84], [94mLoss[0m : 2.49775

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.606, [92mTest[0m: 2.336, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.83219
[1mStep[0m  [8/84], [94mLoss[0m : 2.36680
[1mStep[0m  [16/84], [94mLoss[0m : 2.34536
[1mStep[0m  [24/84], [94mLoss[0m : 2.43750
[1mStep[0m  [32/84], [94mLoss[0m : 2.79288
[1mStep[0m  [40/84], [94mLoss[0m : 2.29005
[1mStep[0m  [48/84], [94mLoss[0m : 2.66246
[1mStep[0m  [56/84], [94mLoss[0m : 2.57323
[1mStep[0m  [64/84], [94mLoss[0m : 2.96915
[1mStep[0m  [72/84], [94mLoss[0m : 2.78685
[1mStep[0m  [80/84], [94mLoss[0m : 2.53400

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.604, [92mTest[0m: 2.338, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60909
[1mStep[0m  [8/84], [94mLoss[0m : 2.50935
[1mStep[0m  [16/84], [94mLoss[0m : 2.56796
[1mStep[0m  [24/84], [94mLoss[0m : 2.63812
[1mStep[0m  [32/84], [94mLoss[0m : 2.68373
[1mStep[0m  [40/84], [94mLoss[0m : 2.35354
[1mStep[0m  [48/84], [94mLoss[0m : 2.70035
[1mStep[0m  [56/84], [94mLoss[0m : 2.88768
[1mStep[0m  [64/84], [94mLoss[0m : 2.59954
[1mStep[0m  [72/84], [94mLoss[0m : 2.51239
[1mStep[0m  [80/84], [94mLoss[0m : 2.71477

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.589, [92mTest[0m: 2.345, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.62420
[1mStep[0m  [8/84], [94mLoss[0m : 2.62661
[1mStep[0m  [16/84], [94mLoss[0m : 2.38702
[1mStep[0m  [24/84], [94mLoss[0m : 2.71001
[1mStep[0m  [32/84], [94mLoss[0m : 2.63290
[1mStep[0m  [40/84], [94mLoss[0m : 2.63316
[1mStep[0m  [48/84], [94mLoss[0m : 2.26532
[1mStep[0m  [56/84], [94mLoss[0m : 2.61516
[1mStep[0m  [64/84], [94mLoss[0m : 2.50051
[1mStep[0m  [72/84], [94mLoss[0m : 2.59731
[1mStep[0m  [80/84], [94mLoss[0m : 2.50906

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.592, [92mTest[0m: 2.339, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.24046
[1mStep[0m  [8/84], [94mLoss[0m : 2.34932
[1mStep[0m  [16/84], [94mLoss[0m : 2.59278
[1mStep[0m  [24/84], [94mLoss[0m : 2.79880
[1mStep[0m  [32/84], [94mLoss[0m : 2.44554
[1mStep[0m  [40/84], [94mLoss[0m : 2.49041
[1mStep[0m  [48/84], [94mLoss[0m : 2.86793
[1mStep[0m  [56/84], [94mLoss[0m : 2.35615
[1mStep[0m  [64/84], [94mLoss[0m : 2.89047
[1mStep[0m  [72/84], [94mLoss[0m : 2.39796
[1mStep[0m  [80/84], [94mLoss[0m : 2.28002

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.620, [92mTest[0m: 2.328, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41843
[1mStep[0m  [8/84], [94mLoss[0m : 2.67848
[1mStep[0m  [16/84], [94mLoss[0m : 2.34113
[1mStep[0m  [24/84], [94mLoss[0m : 2.32716
[1mStep[0m  [32/84], [94mLoss[0m : 2.83640
[1mStep[0m  [40/84], [94mLoss[0m : 2.77451
[1mStep[0m  [48/84], [94mLoss[0m : 2.51160
[1mStep[0m  [56/84], [94mLoss[0m : 2.29268
[1mStep[0m  [64/84], [94mLoss[0m : 2.54705
[1mStep[0m  [72/84], [94mLoss[0m : 2.69298
[1mStep[0m  [80/84], [94mLoss[0m : 2.81190

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.580, [92mTest[0m: 2.337, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47840
[1mStep[0m  [8/84], [94mLoss[0m : 2.68086
[1mStep[0m  [16/84], [94mLoss[0m : 2.53414
[1mStep[0m  [24/84], [94mLoss[0m : 2.71297
[1mStep[0m  [32/84], [94mLoss[0m : 2.53324
[1mStep[0m  [40/84], [94mLoss[0m : 2.41141
[1mStep[0m  [48/84], [94mLoss[0m : 3.10157
[1mStep[0m  [56/84], [94mLoss[0m : 2.68225
[1mStep[0m  [64/84], [94mLoss[0m : 2.47922
[1mStep[0m  [72/84], [94mLoss[0m : 2.17778
[1mStep[0m  [80/84], [94mLoss[0m : 2.65861

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.593, [92mTest[0m: 2.339, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.84718
[1mStep[0m  [8/84], [94mLoss[0m : 2.50930
[1mStep[0m  [16/84], [94mLoss[0m : 2.68925
[1mStep[0m  [24/84], [94mLoss[0m : 2.49844
[1mStep[0m  [32/84], [94mLoss[0m : 2.24102
[1mStep[0m  [40/84], [94mLoss[0m : 2.36052
[1mStep[0m  [48/84], [94mLoss[0m : 2.34545
[1mStep[0m  [56/84], [94mLoss[0m : 2.22256
[1mStep[0m  [64/84], [94mLoss[0m : 2.29085
[1mStep[0m  [72/84], [94mLoss[0m : 2.62615
[1mStep[0m  [80/84], [94mLoss[0m : 2.43276

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.588, [92mTest[0m: 2.341, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.74972
[1mStep[0m  [8/84], [94mLoss[0m : 2.90437
[1mStep[0m  [16/84], [94mLoss[0m : 2.58717
[1mStep[0m  [24/84], [94mLoss[0m : 2.84988
[1mStep[0m  [32/84], [94mLoss[0m : 2.57058
[1mStep[0m  [40/84], [94mLoss[0m : 2.88260
[1mStep[0m  [48/84], [94mLoss[0m : 2.83576
[1mStep[0m  [56/84], [94mLoss[0m : 2.56448
[1mStep[0m  [64/84], [94mLoss[0m : 2.68128
[1mStep[0m  [72/84], [94mLoss[0m : 2.42183
[1mStep[0m  [80/84], [94mLoss[0m : 2.25957

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.592, [92mTest[0m: 2.341, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.330
====================================

Phase 1 - Evaluation MAE:  2.330377561705453
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 2.50477
[1mStep[0m  [8/84], [94mLoss[0m : 2.60140
[1mStep[0m  [16/84], [94mLoss[0m : 2.58339
[1mStep[0m  [24/84], [94mLoss[0m : 2.03128
[1mStep[0m  [32/84], [94mLoss[0m : 2.42231
[1mStep[0m  [40/84], [94mLoss[0m : 2.70562
[1mStep[0m  [48/84], [94mLoss[0m : 2.43567
[1mStep[0m  [56/84], [94mLoss[0m : 2.63999
[1mStep[0m  [64/84], [94mLoss[0m : 2.57160
[1mStep[0m  [72/84], [94mLoss[0m : 2.91521
[1mStep[0m  [80/84], [94mLoss[0m : 2.73277

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.590, [92mTest[0m: 2.334, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40279
[1mStep[0m  [8/84], [94mLoss[0m : 3.13035
[1mStep[0m  [16/84], [94mLoss[0m : 2.44502
[1mStep[0m  [24/84], [94mLoss[0m : 2.53421
[1mStep[0m  [32/84], [94mLoss[0m : 2.82385
[1mStep[0m  [40/84], [94mLoss[0m : 2.42020
[1mStep[0m  [48/84], [94mLoss[0m : 2.30446
[1mStep[0m  [56/84], [94mLoss[0m : 2.40054
[1mStep[0m  [64/84], [94mLoss[0m : 2.73293
[1mStep[0m  [72/84], [94mLoss[0m : 2.70206
[1mStep[0m  [80/84], [94mLoss[0m : 2.85592

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.574, [92mTest[0m: 2.431, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30410
[1mStep[0m  [8/84], [94mLoss[0m : 2.55660
[1mStep[0m  [16/84], [94mLoss[0m : 2.78540
[1mStep[0m  [24/84], [94mLoss[0m : 2.63185
[1mStep[0m  [32/84], [94mLoss[0m : 2.68211
[1mStep[0m  [40/84], [94mLoss[0m : 2.48050
[1mStep[0m  [48/84], [94mLoss[0m : 2.61075
[1mStep[0m  [56/84], [94mLoss[0m : 2.46159
[1mStep[0m  [64/84], [94mLoss[0m : 2.40881
[1mStep[0m  [72/84], [94mLoss[0m : 2.53369
[1mStep[0m  [80/84], [94mLoss[0m : 2.73926

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.545, [92mTest[0m: 2.721, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41906
[1mStep[0m  [8/84], [94mLoss[0m : 2.43309
[1mStep[0m  [16/84], [94mLoss[0m : 2.70120
[1mStep[0m  [24/84], [94mLoss[0m : 2.22871
[1mStep[0m  [32/84], [94mLoss[0m : 2.49099
[1mStep[0m  [40/84], [94mLoss[0m : 2.49370
[1mStep[0m  [48/84], [94mLoss[0m : 2.53355
[1mStep[0m  [56/84], [94mLoss[0m : 2.52265
[1mStep[0m  [64/84], [94mLoss[0m : 2.47893
[1mStep[0m  [72/84], [94mLoss[0m : 2.10528
[1mStep[0m  [80/84], [94mLoss[0m : 2.60494

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.686, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.69631
[1mStep[0m  [8/84], [94mLoss[0m : 2.35693
[1mStep[0m  [16/84], [94mLoss[0m : 2.23898
[1mStep[0m  [24/84], [94mLoss[0m : 2.77959
[1mStep[0m  [32/84], [94mLoss[0m : 2.28661
[1mStep[0m  [40/84], [94mLoss[0m : 2.35178
[1mStep[0m  [48/84], [94mLoss[0m : 2.43414
[1mStep[0m  [56/84], [94mLoss[0m : 2.49854
[1mStep[0m  [64/84], [94mLoss[0m : 2.48388
[1mStep[0m  [72/84], [94mLoss[0m : 2.62814
[1mStep[0m  [80/84], [94mLoss[0m : 2.55129

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.647, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27612
[1mStep[0m  [8/84], [94mLoss[0m : 2.50427
[1mStep[0m  [16/84], [94mLoss[0m : 2.80225
[1mStep[0m  [24/84], [94mLoss[0m : 2.24427
[1mStep[0m  [32/84], [94mLoss[0m : 2.07694
[1mStep[0m  [40/84], [94mLoss[0m : 2.53446
[1mStep[0m  [48/84], [94mLoss[0m : 2.35680
[1mStep[0m  [56/84], [94mLoss[0m : 2.33420
[1mStep[0m  [64/84], [94mLoss[0m : 2.93977
[1mStep[0m  [72/84], [94mLoss[0m : 2.45704
[1mStep[0m  [80/84], [94mLoss[0m : 2.65784

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.635, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40596
[1mStep[0m  [8/84], [94mLoss[0m : 2.37055
[1mStep[0m  [16/84], [94mLoss[0m : 2.41287
[1mStep[0m  [24/84], [94mLoss[0m : 2.41949
[1mStep[0m  [32/84], [94mLoss[0m : 2.67459
[1mStep[0m  [40/84], [94mLoss[0m : 2.60254
[1mStep[0m  [48/84], [94mLoss[0m : 2.37534
[1mStep[0m  [56/84], [94mLoss[0m : 2.42087
[1mStep[0m  [64/84], [94mLoss[0m : 2.34411
[1mStep[0m  [72/84], [94mLoss[0m : 2.49032
[1mStep[0m  [80/84], [94mLoss[0m : 2.16834

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.667, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33820
[1mStep[0m  [8/84], [94mLoss[0m : 2.40262
[1mStep[0m  [16/84], [94mLoss[0m : 2.23353
[1mStep[0m  [24/84], [94mLoss[0m : 2.15999
[1mStep[0m  [32/84], [94mLoss[0m : 2.54063
[1mStep[0m  [40/84], [94mLoss[0m : 2.60188
[1mStep[0m  [48/84], [94mLoss[0m : 2.17185
[1mStep[0m  [56/84], [94mLoss[0m : 2.40217
[1mStep[0m  [64/84], [94mLoss[0m : 2.48767
[1mStep[0m  [72/84], [94mLoss[0m : 2.62547
[1mStep[0m  [80/84], [94mLoss[0m : 2.16921

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.581, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.62904
[1mStep[0m  [8/84], [94mLoss[0m : 2.38861
[1mStep[0m  [16/84], [94mLoss[0m : 2.37410
[1mStep[0m  [24/84], [94mLoss[0m : 2.39278
[1mStep[0m  [32/84], [94mLoss[0m : 2.40753
[1mStep[0m  [40/84], [94mLoss[0m : 2.21652
[1mStep[0m  [48/84], [94mLoss[0m : 2.06459
[1mStep[0m  [56/84], [94mLoss[0m : 2.35451
[1mStep[0m  [64/84], [94mLoss[0m : 2.20379
[1mStep[0m  [72/84], [94mLoss[0m : 2.16254
[1mStep[0m  [80/84], [94mLoss[0m : 2.44157

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.352, [92mTest[0m: 2.616, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27100
[1mStep[0m  [8/84], [94mLoss[0m : 2.13486
[1mStep[0m  [16/84], [94mLoss[0m : 2.28641
[1mStep[0m  [24/84], [94mLoss[0m : 2.36025
[1mStep[0m  [32/84], [94mLoss[0m : 2.30654
[1mStep[0m  [40/84], [94mLoss[0m : 2.07882
[1mStep[0m  [48/84], [94mLoss[0m : 2.30581
[1mStep[0m  [56/84], [94mLoss[0m : 2.56772
[1mStep[0m  [64/84], [94mLoss[0m : 2.44531
[1mStep[0m  [72/84], [94mLoss[0m : 2.36880
[1mStep[0m  [80/84], [94mLoss[0m : 2.26849

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.336, [92mTest[0m: 2.602, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27136
[1mStep[0m  [8/84], [94mLoss[0m : 2.18779
[1mStep[0m  [16/84], [94mLoss[0m : 1.92037
[1mStep[0m  [24/84], [94mLoss[0m : 2.36337
[1mStep[0m  [32/84], [94mLoss[0m : 2.35738
[1mStep[0m  [40/84], [94mLoss[0m : 2.10212
[1mStep[0m  [48/84], [94mLoss[0m : 2.17563
[1mStep[0m  [56/84], [94mLoss[0m : 2.14110
[1mStep[0m  [64/84], [94mLoss[0m : 2.53673
[1mStep[0m  [72/84], [94mLoss[0m : 2.32640
[1mStep[0m  [80/84], [94mLoss[0m : 2.54954

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.293, [92mTest[0m: 2.620, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.11017
[1mStep[0m  [8/84], [94mLoss[0m : 2.27207
[1mStep[0m  [16/84], [94mLoss[0m : 2.38688
[1mStep[0m  [24/84], [94mLoss[0m : 2.20921
[1mStep[0m  [32/84], [94mLoss[0m : 2.28553
[1mStep[0m  [40/84], [94mLoss[0m : 2.09655
[1mStep[0m  [48/84], [94mLoss[0m : 2.48476
[1mStep[0m  [56/84], [94mLoss[0m : 2.24317
[1mStep[0m  [64/84], [94mLoss[0m : 2.38517
[1mStep[0m  [72/84], [94mLoss[0m : 2.17664
[1mStep[0m  [80/84], [94mLoss[0m : 2.27937

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.275, [92mTest[0m: 2.496, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.01415
[1mStep[0m  [8/84], [94mLoss[0m : 2.38073
[1mStep[0m  [16/84], [94mLoss[0m : 2.36164
[1mStep[0m  [24/84], [94mLoss[0m : 2.33148
[1mStep[0m  [32/84], [94mLoss[0m : 2.44913
[1mStep[0m  [40/84], [94mLoss[0m : 2.56394
[1mStep[0m  [48/84], [94mLoss[0m : 2.30799
[1mStep[0m  [56/84], [94mLoss[0m : 2.19762
[1mStep[0m  [64/84], [94mLoss[0m : 2.30986
[1mStep[0m  [72/84], [94mLoss[0m : 2.23670
[1mStep[0m  [80/84], [94mLoss[0m : 2.03007

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.250, [92mTest[0m: 2.505, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.07787
[1mStep[0m  [8/84], [94mLoss[0m : 2.17165
[1mStep[0m  [16/84], [94mLoss[0m : 2.16245
[1mStep[0m  [24/84], [94mLoss[0m : 2.56666
[1mStep[0m  [32/84], [94mLoss[0m : 2.36887
[1mStep[0m  [40/84], [94mLoss[0m : 2.24952
[1mStep[0m  [48/84], [94mLoss[0m : 1.94841
[1mStep[0m  [56/84], [94mLoss[0m : 2.31642
[1mStep[0m  [64/84], [94mLoss[0m : 2.26459
[1mStep[0m  [72/84], [94mLoss[0m : 2.10724
[1mStep[0m  [80/84], [94mLoss[0m : 2.21440

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.196, [92mTest[0m: 2.541, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.97793
[1mStep[0m  [8/84], [94mLoss[0m : 2.12813
[1mStep[0m  [16/84], [94mLoss[0m : 1.98466
[1mStep[0m  [24/84], [94mLoss[0m : 1.92487
[1mStep[0m  [32/84], [94mLoss[0m : 2.27038
[1mStep[0m  [40/84], [94mLoss[0m : 2.08231
[1mStep[0m  [48/84], [94mLoss[0m : 2.00678
[1mStep[0m  [56/84], [94mLoss[0m : 2.10652
[1mStep[0m  [64/84], [94mLoss[0m : 1.99456
[1mStep[0m  [72/84], [94mLoss[0m : 2.38368
[1mStep[0m  [80/84], [94mLoss[0m : 2.18541

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.170, [92mTest[0m: 2.536, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.88854
[1mStep[0m  [8/84], [94mLoss[0m : 2.19994
[1mStep[0m  [16/84], [94mLoss[0m : 1.97336
[1mStep[0m  [24/84], [94mLoss[0m : 2.03810
[1mStep[0m  [32/84], [94mLoss[0m : 2.36015
[1mStep[0m  [40/84], [94mLoss[0m : 2.04729
[1mStep[0m  [48/84], [94mLoss[0m : 2.33349
[1mStep[0m  [56/84], [94mLoss[0m : 1.99219
[1mStep[0m  [64/84], [94mLoss[0m : 2.07328
[1mStep[0m  [72/84], [94mLoss[0m : 2.26460
[1mStep[0m  [80/84], [94mLoss[0m : 2.01735

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.096, [92mTest[0m: 2.572, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.92135
[1mStep[0m  [8/84], [94mLoss[0m : 1.91962
[1mStep[0m  [16/84], [94mLoss[0m : 1.96848
[1mStep[0m  [24/84], [94mLoss[0m : 2.48475
[1mStep[0m  [32/84], [94mLoss[0m : 1.84540
[1mStep[0m  [40/84], [94mLoss[0m : 1.95807
[1mStep[0m  [48/84], [94mLoss[0m : 2.05061
[1mStep[0m  [56/84], [94mLoss[0m : 2.23190
[1mStep[0m  [64/84], [94mLoss[0m : 1.97357
[1mStep[0m  [72/84], [94mLoss[0m : 1.98152
[1mStep[0m  [80/84], [94mLoss[0m : 2.15502

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.119, [92mTest[0m: 2.580, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.91385
[1mStep[0m  [8/84], [94mLoss[0m : 2.11751
[1mStep[0m  [16/84], [94mLoss[0m : 2.05654
[1mStep[0m  [24/84], [94mLoss[0m : 1.58556
[1mStep[0m  [32/84], [94mLoss[0m : 1.90820
[1mStep[0m  [40/84], [94mLoss[0m : 1.99859
[1mStep[0m  [48/84], [94mLoss[0m : 2.21807
[1mStep[0m  [56/84], [94mLoss[0m : 2.08610
[1mStep[0m  [64/84], [94mLoss[0m : 2.28060
[1mStep[0m  [72/84], [94mLoss[0m : 1.77469
[1mStep[0m  [80/84], [94mLoss[0m : 2.07159

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.036, [92mTest[0m: 2.538, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.82329
[1mStep[0m  [8/84], [94mLoss[0m : 1.95739
[1mStep[0m  [16/84], [94mLoss[0m : 2.15652
[1mStep[0m  [24/84], [94mLoss[0m : 1.68211
[1mStep[0m  [32/84], [94mLoss[0m : 1.78269
[1mStep[0m  [40/84], [94mLoss[0m : 2.41233
[1mStep[0m  [48/84], [94mLoss[0m : 2.14346
[1mStep[0m  [56/84], [94mLoss[0m : 1.75947
[1mStep[0m  [64/84], [94mLoss[0m : 2.13760
[1mStep[0m  [72/84], [94mLoss[0m : 2.18601
[1mStep[0m  [80/84], [94mLoss[0m : 1.91507

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.039, [92mTest[0m: 2.683, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.02979
[1mStep[0m  [8/84], [94mLoss[0m : 1.97757
[1mStep[0m  [16/84], [94mLoss[0m : 1.94481
[1mStep[0m  [24/84], [94mLoss[0m : 1.92904
[1mStep[0m  [32/84], [94mLoss[0m : 2.06539
[1mStep[0m  [40/84], [94mLoss[0m : 1.86602
[1mStep[0m  [48/84], [94mLoss[0m : 2.17993
[1mStep[0m  [56/84], [94mLoss[0m : 2.32704
[1mStep[0m  [64/84], [94mLoss[0m : 1.95142
[1mStep[0m  [72/84], [94mLoss[0m : 1.74808
[1mStep[0m  [80/84], [94mLoss[0m : 2.07666

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.998, [92mTest[0m: 2.478, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.02635
[1mStep[0m  [8/84], [94mLoss[0m : 1.90246
[1mStep[0m  [16/84], [94mLoss[0m : 1.88834
[1mStep[0m  [24/84], [94mLoss[0m : 1.74680
[1mStep[0m  [32/84], [94mLoss[0m : 2.06980
[1mStep[0m  [40/84], [94mLoss[0m : 1.86291
[1mStep[0m  [48/84], [94mLoss[0m : 1.98510
[1mStep[0m  [56/84], [94mLoss[0m : 1.83149
[1mStep[0m  [64/84], [94mLoss[0m : 1.98643
[1mStep[0m  [72/84], [94mLoss[0m : 1.91442
[1mStep[0m  [80/84], [94mLoss[0m : 2.11484

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.957, [92mTest[0m: 2.534, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.13913
[1mStep[0m  [8/84], [94mLoss[0m : 1.72966
[1mStep[0m  [16/84], [94mLoss[0m : 1.99315
[1mStep[0m  [24/84], [94mLoss[0m : 2.01277
[1mStep[0m  [32/84], [94mLoss[0m : 2.08887
[1mStep[0m  [40/84], [94mLoss[0m : 2.08505
[1mStep[0m  [48/84], [94mLoss[0m : 1.89294
[1mStep[0m  [56/84], [94mLoss[0m : 2.01322
[1mStep[0m  [64/84], [94mLoss[0m : 1.82761
[1mStep[0m  [72/84], [94mLoss[0m : 1.78288
[1mStep[0m  [80/84], [94mLoss[0m : 1.78385

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.927, [92mTest[0m: 2.455, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.95526
[1mStep[0m  [8/84], [94mLoss[0m : 1.82009
[1mStep[0m  [16/84], [94mLoss[0m : 1.86763
[1mStep[0m  [24/84], [94mLoss[0m : 1.94463
[1mStep[0m  [32/84], [94mLoss[0m : 1.97149
[1mStep[0m  [40/84], [94mLoss[0m : 1.88443
[1mStep[0m  [48/84], [94mLoss[0m : 2.01292
[1mStep[0m  [56/84], [94mLoss[0m : 1.84600
[1mStep[0m  [64/84], [94mLoss[0m : 1.88592
[1mStep[0m  [72/84], [94mLoss[0m : 2.03021
[1mStep[0m  [80/84], [94mLoss[0m : 2.03091

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.914, [92mTest[0m: 2.555, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.85580
[1mStep[0m  [8/84], [94mLoss[0m : 1.98821
[1mStep[0m  [16/84], [94mLoss[0m : 1.96153
[1mStep[0m  [24/84], [94mLoss[0m : 1.62680
[1mStep[0m  [32/84], [94mLoss[0m : 1.55208
[1mStep[0m  [40/84], [94mLoss[0m : 1.94410
[1mStep[0m  [48/84], [94mLoss[0m : 1.89459
[1mStep[0m  [56/84], [94mLoss[0m : 1.81631
[1mStep[0m  [64/84], [94mLoss[0m : 1.96963
[1mStep[0m  [72/84], [94mLoss[0m : 1.86204
[1mStep[0m  [80/84], [94mLoss[0m : 1.94054

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.874, [92mTest[0m: 2.517, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.00915
[1mStep[0m  [8/84], [94mLoss[0m : 1.82107
[1mStep[0m  [16/84], [94mLoss[0m : 1.91090
[1mStep[0m  [24/84], [94mLoss[0m : 1.60733
[1mStep[0m  [32/84], [94mLoss[0m : 1.63309
[1mStep[0m  [40/84], [94mLoss[0m : 1.92692
[1mStep[0m  [48/84], [94mLoss[0m : 1.85286
[1mStep[0m  [56/84], [94mLoss[0m : 2.14676
[1mStep[0m  [64/84], [94mLoss[0m : 1.79339
[1mStep[0m  [72/84], [94mLoss[0m : 1.75249
[1mStep[0m  [80/84], [94mLoss[0m : 2.01406

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.884, [92mTest[0m: 2.489, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.05970
[1mStep[0m  [8/84], [94mLoss[0m : 1.93502
[1mStep[0m  [16/84], [94mLoss[0m : 2.07699
[1mStep[0m  [24/84], [94mLoss[0m : 1.75886
[1mStep[0m  [32/84], [94mLoss[0m : 1.93497
[1mStep[0m  [40/84], [94mLoss[0m : 1.80346
[1mStep[0m  [48/84], [94mLoss[0m : 1.99111
[1mStep[0m  [56/84], [94mLoss[0m : 1.60750
[1mStep[0m  [64/84], [94mLoss[0m : 1.82146
[1mStep[0m  [72/84], [94mLoss[0m : 1.82091
[1mStep[0m  [80/84], [94mLoss[0m : 1.89682

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.845, [92mTest[0m: 2.551, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.77453
[1mStep[0m  [8/84], [94mLoss[0m : 1.75062
[1mStep[0m  [16/84], [94mLoss[0m : 1.81036
[1mStep[0m  [24/84], [94mLoss[0m : 1.81780
[1mStep[0m  [32/84], [94mLoss[0m : 1.65439
[1mStep[0m  [40/84], [94mLoss[0m : 1.82721
[1mStep[0m  [48/84], [94mLoss[0m : 1.72083
[1mStep[0m  [56/84], [94mLoss[0m : 1.92691
[1mStep[0m  [64/84], [94mLoss[0m : 1.89280
[1mStep[0m  [72/84], [94mLoss[0m : 1.80132
[1mStep[0m  [80/84], [94mLoss[0m : 1.95244

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.822, [92mTest[0m: 2.669, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.73515
[1mStep[0m  [8/84], [94mLoss[0m : 1.52322
[1mStep[0m  [16/84], [94mLoss[0m : 1.84780
[1mStep[0m  [24/84], [94mLoss[0m : 1.96427
[1mStep[0m  [32/84], [94mLoss[0m : 1.90819
[1mStep[0m  [40/84], [94mLoss[0m : 1.82265
[1mStep[0m  [48/84], [94mLoss[0m : 1.91188
[1mStep[0m  [56/84], [94mLoss[0m : 1.96232
[1mStep[0m  [64/84], [94mLoss[0m : 1.81814
[1mStep[0m  [72/84], [94mLoss[0m : 1.99453
[1mStep[0m  [80/84], [94mLoss[0m : 1.78874

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.816, [92mTest[0m: 2.666, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.81156
[1mStep[0m  [8/84], [94mLoss[0m : 1.89567
[1mStep[0m  [16/84], [94mLoss[0m : 1.65667
[1mStep[0m  [24/84], [94mLoss[0m : 1.71573
[1mStep[0m  [32/84], [94mLoss[0m : 1.53637
[1mStep[0m  [40/84], [94mLoss[0m : 1.75017
[1mStep[0m  [48/84], [94mLoss[0m : 1.93789
[1mStep[0m  [56/84], [94mLoss[0m : 1.64947
[1mStep[0m  [64/84], [94mLoss[0m : 1.67469
[1mStep[0m  [72/84], [94mLoss[0m : 2.13035
[1mStep[0m  [80/84], [94mLoss[0m : 1.91846

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.781, [92mTest[0m: 2.563, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.68864
[1mStep[0m  [8/84], [94mLoss[0m : 1.94219
[1mStep[0m  [16/84], [94mLoss[0m : 2.12545
[1mStep[0m  [24/84], [94mLoss[0m : 1.74670
[1mStep[0m  [32/84], [94mLoss[0m : 1.95523
[1mStep[0m  [40/84], [94mLoss[0m : 1.66980
[1mStep[0m  [48/84], [94mLoss[0m : 1.81379
[1mStep[0m  [56/84], [94mLoss[0m : 1.78281
[1mStep[0m  [64/84], [94mLoss[0m : 1.93343
[1mStep[0m  [72/84], [94mLoss[0m : 1.92270
[1mStep[0m  [80/84], [94mLoss[0m : 1.93658

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.768, [92mTest[0m: 2.572, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.622
====================================

Phase 2 - Evaluation MAE:  2.622489188398634
MAE score P1       2.330378
MAE score P2       2.622489
loss               1.768056
learning_rate      0.007525
batch_size              128
hidden_sizes      [250, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.1
weight_decay          0.001
Name: 0, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 11.04815
[1mStep[0m  [8/84], [94mLoss[0m : 7.59594
[1mStep[0m  [16/84], [94mLoss[0m : 2.89774
[1mStep[0m  [24/84], [94mLoss[0m : 3.59808
[1mStep[0m  [32/84], [94mLoss[0m : 3.03120
[1mStep[0m  [40/84], [94mLoss[0m : 2.76296
[1mStep[0m  [48/84], [94mLoss[0m : 3.01964
[1mStep[0m  [56/84], [94mLoss[0m : 3.04647
[1mStep[0m  [64/84], [94mLoss[0m : 2.48253
[1mStep[0m  [72/84], [94mLoss[0m : 2.83852
[1mStep[0m  [80/84], [94mLoss[0m : 2.64729

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.829, [92mTest[0m: 10.952, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.85975
[1mStep[0m  [8/84], [94mLoss[0m : 3.03446
[1mStep[0m  [16/84], [94mLoss[0m : 2.58848
[1mStep[0m  [24/84], [94mLoss[0m : 2.49280
[1mStep[0m  [32/84], [94mLoss[0m : 2.81090
[1mStep[0m  [40/84], [94mLoss[0m : 2.47730
[1mStep[0m  [48/84], [94mLoss[0m : 2.66753
[1mStep[0m  [56/84], [94mLoss[0m : 2.84087
[1mStep[0m  [64/84], [94mLoss[0m : 2.47505
[1mStep[0m  [72/84], [94mLoss[0m : 2.51308
[1mStep[0m  [80/84], [94mLoss[0m : 2.84239

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.634, [92mTest[0m: 2.565, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.69998
[1mStep[0m  [8/84], [94mLoss[0m : 2.49661
[1mStep[0m  [16/84], [94mLoss[0m : 2.43076
[1mStep[0m  [24/84], [94mLoss[0m : 2.79246
[1mStep[0m  [32/84], [94mLoss[0m : 2.85717
[1mStep[0m  [40/84], [94mLoss[0m : 2.49995
[1mStep[0m  [48/84], [94mLoss[0m : 2.81240
[1mStep[0m  [56/84], [94mLoss[0m : 2.62486
[1mStep[0m  [64/84], [94mLoss[0m : 2.47725
[1mStep[0m  [72/84], [94mLoss[0m : 2.65787
[1mStep[0m  [80/84], [94mLoss[0m : 2.20623

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.588, [92mTest[0m: 2.401, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.74398
[1mStep[0m  [8/84], [94mLoss[0m : 2.37157
[1mStep[0m  [16/84], [94mLoss[0m : 2.22773
[1mStep[0m  [24/84], [94mLoss[0m : 2.60936
[1mStep[0m  [32/84], [94mLoss[0m : 2.55371
[1mStep[0m  [40/84], [94mLoss[0m : 2.23707
[1mStep[0m  [48/84], [94mLoss[0m : 2.50560
[1mStep[0m  [56/84], [94mLoss[0m : 2.35066
[1mStep[0m  [64/84], [94mLoss[0m : 2.35783
[1mStep[0m  [72/84], [94mLoss[0m : 2.39650
[1mStep[0m  [80/84], [94mLoss[0m : 2.33298

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.536, [92mTest[0m: 2.406, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.61319
[1mStep[0m  [8/84], [94mLoss[0m : 2.48755
[1mStep[0m  [16/84], [94mLoss[0m : 2.49053
[1mStep[0m  [24/84], [94mLoss[0m : 2.65018
[1mStep[0m  [32/84], [94mLoss[0m : 2.35641
[1mStep[0m  [40/84], [94mLoss[0m : 2.34241
[1mStep[0m  [48/84], [94mLoss[0m : 2.58361
[1mStep[0m  [56/84], [94mLoss[0m : 2.51106
[1mStep[0m  [64/84], [94mLoss[0m : 2.34113
[1mStep[0m  [72/84], [94mLoss[0m : 2.79184
[1mStep[0m  [80/84], [94mLoss[0m : 2.58165

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.519, [92mTest[0m: 2.384, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60293
[1mStep[0m  [8/84], [94mLoss[0m : 2.18919
[1mStep[0m  [16/84], [94mLoss[0m : 2.41954
[1mStep[0m  [24/84], [94mLoss[0m : 2.61850
[1mStep[0m  [32/84], [94mLoss[0m : 2.34823
[1mStep[0m  [40/84], [94mLoss[0m : 2.35326
[1mStep[0m  [48/84], [94mLoss[0m : 2.37695
[1mStep[0m  [56/84], [94mLoss[0m : 2.69294
[1mStep[0m  [64/84], [94mLoss[0m : 2.57371
[1mStep[0m  [72/84], [94mLoss[0m : 2.41522
[1mStep[0m  [80/84], [94mLoss[0m : 2.57410

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.392, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.73818
[1mStep[0m  [8/84], [94mLoss[0m : 2.25936
[1mStep[0m  [16/84], [94mLoss[0m : 2.54348
[1mStep[0m  [24/84], [94mLoss[0m : 2.84733
[1mStep[0m  [32/84], [94mLoss[0m : 2.35035
[1mStep[0m  [40/84], [94mLoss[0m : 2.52190
[1mStep[0m  [48/84], [94mLoss[0m : 2.21062
[1mStep[0m  [56/84], [94mLoss[0m : 2.64941
[1mStep[0m  [64/84], [94mLoss[0m : 2.33969
[1mStep[0m  [72/84], [94mLoss[0m : 2.24050
[1mStep[0m  [80/84], [94mLoss[0m : 2.75245

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.367, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53748
[1mStep[0m  [8/84], [94mLoss[0m : 2.59472
[1mStep[0m  [16/84], [94mLoss[0m : 2.39997
[1mStep[0m  [24/84], [94mLoss[0m : 2.55864
[1mStep[0m  [32/84], [94mLoss[0m : 2.46000
[1mStep[0m  [40/84], [94mLoss[0m : 2.52818
[1mStep[0m  [48/84], [94mLoss[0m : 2.35423
[1mStep[0m  [56/84], [94mLoss[0m : 2.54911
[1mStep[0m  [64/84], [94mLoss[0m : 2.53265
[1mStep[0m  [72/84], [94mLoss[0m : 2.43330
[1mStep[0m  [80/84], [94mLoss[0m : 2.10288

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.350, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.24417
[1mStep[0m  [8/84], [94mLoss[0m : 2.27591
[1mStep[0m  [16/84], [94mLoss[0m : 2.38215
[1mStep[0m  [24/84], [94mLoss[0m : 2.12647
[1mStep[0m  [32/84], [94mLoss[0m : 2.41382
[1mStep[0m  [40/84], [94mLoss[0m : 2.63287
[1mStep[0m  [48/84], [94mLoss[0m : 2.33624
[1mStep[0m  [56/84], [94mLoss[0m : 2.47527
[1mStep[0m  [64/84], [94mLoss[0m : 2.22866
[1mStep[0m  [72/84], [94mLoss[0m : 2.21824
[1mStep[0m  [80/84], [94mLoss[0m : 2.41093

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.343, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.12044
[1mStep[0m  [8/84], [94mLoss[0m : 2.42858
[1mStep[0m  [16/84], [94mLoss[0m : 2.34784
[1mStep[0m  [24/84], [94mLoss[0m : 2.33326
[1mStep[0m  [32/84], [94mLoss[0m : 2.52572
[1mStep[0m  [40/84], [94mLoss[0m : 2.64227
[1mStep[0m  [48/84], [94mLoss[0m : 2.56918
[1mStep[0m  [56/84], [94mLoss[0m : 2.48822
[1mStep[0m  [64/84], [94mLoss[0m : 2.68330
[1mStep[0m  [72/84], [94mLoss[0m : 2.53588
[1mStep[0m  [80/84], [94mLoss[0m : 2.20679

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.374, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.09604
[1mStep[0m  [8/84], [94mLoss[0m : 2.32273
[1mStep[0m  [16/84], [94mLoss[0m : 2.02480
[1mStep[0m  [24/84], [94mLoss[0m : 2.47632
[1mStep[0m  [32/84], [94mLoss[0m : 2.40088
[1mStep[0m  [40/84], [94mLoss[0m : 2.49273
[1mStep[0m  [48/84], [94mLoss[0m : 2.34775
[1mStep[0m  [56/84], [94mLoss[0m : 2.63629
[1mStep[0m  [64/84], [94mLoss[0m : 2.61127
[1mStep[0m  [72/84], [94mLoss[0m : 2.20740
[1mStep[0m  [80/84], [94mLoss[0m : 2.22330

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.390, [92mTest[0m: 2.353, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.19364
[1mStep[0m  [8/84], [94mLoss[0m : 2.59206
[1mStep[0m  [16/84], [94mLoss[0m : 2.35400
[1mStep[0m  [24/84], [94mLoss[0m : 2.03861
[1mStep[0m  [32/84], [94mLoss[0m : 2.39759
[1mStep[0m  [40/84], [94mLoss[0m : 2.30985
[1mStep[0m  [48/84], [94mLoss[0m : 2.26948
[1mStep[0m  [56/84], [94mLoss[0m : 2.38526
[1mStep[0m  [64/84], [94mLoss[0m : 2.25161
[1mStep[0m  [72/84], [94mLoss[0m : 2.34925
[1mStep[0m  [80/84], [94mLoss[0m : 2.56459

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.373, [92mTest[0m: 2.330, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37011
[1mStep[0m  [8/84], [94mLoss[0m : 2.60540
[1mStep[0m  [16/84], [94mLoss[0m : 2.23525
[1mStep[0m  [24/84], [94mLoss[0m : 2.12577
[1mStep[0m  [32/84], [94mLoss[0m : 2.34426
[1mStep[0m  [40/84], [94mLoss[0m : 2.26603
[1mStep[0m  [48/84], [94mLoss[0m : 2.32198
[1mStep[0m  [56/84], [94mLoss[0m : 2.71783
[1mStep[0m  [64/84], [94mLoss[0m : 2.36106
[1mStep[0m  [72/84], [94mLoss[0m : 2.26582
[1mStep[0m  [80/84], [94mLoss[0m : 2.53096

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.368, [92mTest[0m: 2.335, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30960
[1mStep[0m  [8/84], [94mLoss[0m : 2.14408
[1mStep[0m  [16/84], [94mLoss[0m : 2.26112
[1mStep[0m  [24/84], [94mLoss[0m : 2.50146
[1mStep[0m  [32/84], [94mLoss[0m : 2.09880
[1mStep[0m  [40/84], [94mLoss[0m : 2.58746
[1mStep[0m  [48/84], [94mLoss[0m : 2.44208
[1mStep[0m  [56/84], [94mLoss[0m : 2.52176
[1mStep[0m  [64/84], [94mLoss[0m : 2.42585
[1mStep[0m  [72/84], [94mLoss[0m : 2.55387
[1mStep[0m  [80/84], [94mLoss[0m : 2.35493

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.372, [92mTest[0m: 2.324, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.68903
[1mStep[0m  [8/84], [94mLoss[0m : 2.59003
[1mStep[0m  [16/84], [94mLoss[0m : 2.50015
[1mStep[0m  [24/84], [94mLoss[0m : 2.33406
[1mStep[0m  [32/84], [94mLoss[0m : 2.28939
[1mStep[0m  [40/84], [94mLoss[0m : 2.41165
[1mStep[0m  [48/84], [94mLoss[0m : 2.35256
[1mStep[0m  [56/84], [94mLoss[0m : 2.56057
[1mStep[0m  [64/84], [94mLoss[0m : 2.43784
[1mStep[0m  [72/84], [94mLoss[0m : 2.51220
[1mStep[0m  [80/84], [94mLoss[0m : 2.39589

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.377, [92mTest[0m: 2.337, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44000
[1mStep[0m  [8/84], [94mLoss[0m : 2.34768
[1mStep[0m  [16/84], [94mLoss[0m : 2.11873
[1mStep[0m  [24/84], [94mLoss[0m : 2.03216
[1mStep[0m  [32/84], [94mLoss[0m : 2.37232
[1mStep[0m  [40/84], [94mLoss[0m : 2.29457
[1mStep[0m  [48/84], [94mLoss[0m : 2.24260
[1mStep[0m  [56/84], [94mLoss[0m : 2.39096
[1mStep[0m  [64/84], [94mLoss[0m : 2.49251
[1mStep[0m  [72/84], [94mLoss[0m : 2.14779
[1mStep[0m  [80/84], [94mLoss[0m : 2.39353

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.347, [92mTest[0m: 2.322, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.25663
[1mStep[0m  [8/84], [94mLoss[0m : 2.31221
[1mStep[0m  [16/84], [94mLoss[0m : 2.35086
[1mStep[0m  [24/84], [94mLoss[0m : 2.22309
[1mStep[0m  [32/84], [94mLoss[0m : 2.45903
[1mStep[0m  [40/84], [94mLoss[0m : 2.27812
[1mStep[0m  [48/84], [94mLoss[0m : 2.14851
[1mStep[0m  [56/84], [94mLoss[0m : 2.30996
[1mStep[0m  [64/84], [94mLoss[0m : 2.32570
[1mStep[0m  [72/84], [94mLoss[0m : 2.15945
[1mStep[0m  [80/84], [94mLoss[0m : 2.09490

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.332, [92mTest[0m: 2.335, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.11720
[1mStep[0m  [8/84], [94mLoss[0m : 2.42581
[1mStep[0m  [16/84], [94mLoss[0m : 2.41217
[1mStep[0m  [24/84], [94mLoss[0m : 2.20412
[1mStep[0m  [32/84], [94mLoss[0m : 2.15635
[1mStep[0m  [40/84], [94mLoss[0m : 2.39438
[1mStep[0m  [48/84], [94mLoss[0m : 2.23200
[1mStep[0m  [56/84], [94mLoss[0m : 2.08947
[1mStep[0m  [64/84], [94mLoss[0m : 2.18730
[1mStep[0m  [72/84], [94mLoss[0m : 2.56177
[1mStep[0m  [80/84], [94mLoss[0m : 2.58938

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.335, [92mTest[0m: 2.313, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.20437
[1mStep[0m  [8/84], [94mLoss[0m : 2.62956
[1mStep[0m  [16/84], [94mLoss[0m : 2.10822
[1mStep[0m  [24/84], [94mLoss[0m : 2.29435
[1mStep[0m  [32/84], [94mLoss[0m : 2.63222
[1mStep[0m  [40/84], [94mLoss[0m : 2.23649
[1mStep[0m  [48/84], [94mLoss[0m : 2.13846
[1mStep[0m  [56/84], [94mLoss[0m : 2.21889
[1mStep[0m  [64/84], [94mLoss[0m : 2.15882
[1mStep[0m  [72/84], [94mLoss[0m : 2.17648
[1mStep[0m  [80/84], [94mLoss[0m : 2.64248

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.317, [92mTest[0m: 2.306, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.95738
[1mStep[0m  [8/84], [94mLoss[0m : 2.43801
[1mStep[0m  [16/84], [94mLoss[0m : 2.05862
[1mStep[0m  [24/84], [94mLoss[0m : 1.99681
[1mStep[0m  [32/84], [94mLoss[0m : 2.39340
[1mStep[0m  [40/84], [94mLoss[0m : 2.39843
[1mStep[0m  [48/84], [94mLoss[0m : 2.31934
[1mStep[0m  [56/84], [94mLoss[0m : 2.25555
[1mStep[0m  [64/84], [94mLoss[0m : 2.27399
[1mStep[0m  [72/84], [94mLoss[0m : 2.49139
[1mStep[0m  [80/84], [94mLoss[0m : 2.10894

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.307, [92mTest[0m: 2.340, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42894
[1mStep[0m  [8/84], [94mLoss[0m : 2.02371
[1mStep[0m  [16/84], [94mLoss[0m : 2.42992
[1mStep[0m  [24/84], [94mLoss[0m : 2.70267
[1mStep[0m  [32/84], [94mLoss[0m : 2.27877
[1mStep[0m  [40/84], [94mLoss[0m : 2.20783
[1mStep[0m  [48/84], [94mLoss[0m : 2.34907
[1mStep[0m  [56/84], [94mLoss[0m : 2.10231
[1mStep[0m  [64/84], [94mLoss[0m : 2.18778
[1mStep[0m  [72/84], [94mLoss[0m : 2.37244
[1mStep[0m  [80/84], [94mLoss[0m : 2.12861

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.290, [92mTest[0m: 2.338, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.97455
[1mStep[0m  [8/84], [94mLoss[0m : 2.24590
[1mStep[0m  [16/84], [94mLoss[0m : 2.25075
[1mStep[0m  [24/84], [94mLoss[0m : 2.42121
[1mStep[0m  [32/84], [94mLoss[0m : 1.94223
[1mStep[0m  [40/84], [94mLoss[0m : 2.20018
[1mStep[0m  [48/84], [94mLoss[0m : 2.12353
[1mStep[0m  [56/84], [94mLoss[0m : 2.30537
[1mStep[0m  [64/84], [94mLoss[0m : 2.31783
[1mStep[0m  [72/84], [94mLoss[0m : 2.49569
[1mStep[0m  [80/84], [94mLoss[0m : 2.30093

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.296, [92mTest[0m: 2.311, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27524
[1mStep[0m  [8/84], [94mLoss[0m : 2.35314
[1mStep[0m  [16/84], [94mLoss[0m : 2.41035
[1mStep[0m  [24/84], [94mLoss[0m : 2.23801
[1mStep[0m  [32/84], [94mLoss[0m : 2.30297
[1mStep[0m  [40/84], [94mLoss[0m : 2.45674
[1mStep[0m  [48/84], [94mLoss[0m : 2.42112
[1mStep[0m  [56/84], [94mLoss[0m : 2.25754
[1mStep[0m  [64/84], [94mLoss[0m : 2.32831
[1mStep[0m  [72/84], [94mLoss[0m : 2.47330
[1mStep[0m  [80/84], [94mLoss[0m : 2.08718

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.299, [92mTest[0m: 2.326, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30458
[1mStep[0m  [8/84], [94mLoss[0m : 2.29972
[1mStep[0m  [16/84], [94mLoss[0m : 2.10347
[1mStep[0m  [24/84], [94mLoss[0m : 1.90350
[1mStep[0m  [32/84], [94mLoss[0m : 2.41335
[1mStep[0m  [40/84], [94mLoss[0m : 2.02787
[1mStep[0m  [48/84], [94mLoss[0m : 2.22026
[1mStep[0m  [56/84], [94mLoss[0m : 2.06731
[1mStep[0m  [64/84], [94mLoss[0m : 2.30371
[1mStep[0m  [72/84], [94mLoss[0m : 2.55500
[1mStep[0m  [80/84], [94mLoss[0m : 2.44011

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.287, [92mTest[0m: 2.317, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44630
[1mStep[0m  [8/84], [94mLoss[0m : 2.19307
[1mStep[0m  [16/84], [94mLoss[0m : 2.11286
[1mStep[0m  [24/84], [94mLoss[0m : 2.25117
[1mStep[0m  [32/84], [94mLoss[0m : 2.34944
[1mStep[0m  [40/84], [94mLoss[0m : 1.97337
[1mStep[0m  [48/84], [94mLoss[0m : 2.37789
[1mStep[0m  [56/84], [94mLoss[0m : 2.27746
[1mStep[0m  [64/84], [94mLoss[0m : 2.36982
[1mStep[0m  [72/84], [94mLoss[0m : 2.08859
[1mStep[0m  [80/84], [94mLoss[0m : 2.48189

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.278, [92mTest[0m: 2.320, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.22464
[1mStep[0m  [8/84], [94mLoss[0m : 2.33590
[1mStep[0m  [16/84], [94mLoss[0m : 2.25983
[1mStep[0m  [24/84], [94mLoss[0m : 2.28263
[1mStep[0m  [32/84], [94mLoss[0m : 2.29492
[1mStep[0m  [40/84], [94mLoss[0m : 2.58942
[1mStep[0m  [48/84], [94mLoss[0m : 2.29103
[1mStep[0m  [56/84], [94mLoss[0m : 2.35516
[1mStep[0m  [64/84], [94mLoss[0m : 2.40174
[1mStep[0m  [72/84], [94mLoss[0m : 2.40228
[1mStep[0m  [80/84], [94mLoss[0m : 2.45753

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.288, [92mTest[0m: 2.326, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.10223
[1mStep[0m  [8/84], [94mLoss[0m : 2.29545
[1mStep[0m  [16/84], [94mLoss[0m : 2.17139
[1mStep[0m  [24/84], [94mLoss[0m : 2.15941
[1mStep[0m  [32/84], [94mLoss[0m : 2.25755
[1mStep[0m  [40/84], [94mLoss[0m : 2.23131
[1mStep[0m  [48/84], [94mLoss[0m : 2.21595
[1mStep[0m  [56/84], [94mLoss[0m : 1.98227
[1mStep[0m  [64/84], [94mLoss[0m : 2.31048
[1mStep[0m  [72/84], [94mLoss[0m : 2.26636
[1mStep[0m  [80/84], [94mLoss[0m : 2.24860

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.253, [92mTest[0m: 2.319, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60361
[1mStep[0m  [8/84], [94mLoss[0m : 2.28272
[1mStep[0m  [16/84], [94mLoss[0m : 2.30897
[1mStep[0m  [24/84], [94mLoss[0m : 2.05989
[1mStep[0m  [32/84], [94mLoss[0m : 2.11078
[1mStep[0m  [40/84], [94mLoss[0m : 2.37914
[1mStep[0m  [48/84], [94mLoss[0m : 2.40110
[1mStep[0m  [56/84], [94mLoss[0m : 1.74302
[1mStep[0m  [64/84], [94mLoss[0m : 2.14731
[1mStep[0m  [72/84], [94mLoss[0m : 2.16610
[1mStep[0m  [80/84], [94mLoss[0m : 2.09615

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.253, [92mTest[0m: 2.329, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30356
[1mStep[0m  [8/84], [94mLoss[0m : 2.06879
[1mStep[0m  [16/84], [94mLoss[0m : 2.60638
[1mStep[0m  [24/84], [94mLoss[0m : 2.11743
[1mStep[0m  [32/84], [94mLoss[0m : 2.01547
[1mStep[0m  [40/84], [94mLoss[0m : 2.47197
[1mStep[0m  [48/84], [94mLoss[0m : 2.39488
[1mStep[0m  [56/84], [94mLoss[0m : 2.21605
[1mStep[0m  [64/84], [94mLoss[0m : 2.17059
[1mStep[0m  [72/84], [94mLoss[0m : 2.21797
[1mStep[0m  [80/84], [94mLoss[0m : 2.10892

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.269, [92mTest[0m: 2.348, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30816
[1mStep[0m  [8/84], [94mLoss[0m : 2.24629
[1mStep[0m  [16/84], [94mLoss[0m : 2.10378
[1mStep[0m  [24/84], [94mLoss[0m : 2.30151
[1mStep[0m  [32/84], [94mLoss[0m : 2.38172
[1mStep[0m  [40/84], [94mLoss[0m : 2.35304
[1mStep[0m  [48/84], [94mLoss[0m : 2.25320
[1mStep[0m  [56/84], [94mLoss[0m : 2.19639
[1mStep[0m  [64/84], [94mLoss[0m : 2.49988
[1mStep[0m  [72/84], [94mLoss[0m : 2.23192
[1mStep[0m  [80/84], [94mLoss[0m : 2.16243

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.257, [92mTest[0m: 2.304, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.325
====================================

Phase 1 - Evaluation MAE:  2.325331994465419
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 2.18750
[1mStep[0m  [8/84], [94mLoss[0m : 2.33759
[1mStep[0m  [16/84], [94mLoss[0m : 2.45554
[1mStep[0m  [24/84], [94mLoss[0m : 2.56161
[1mStep[0m  [32/84], [94mLoss[0m : 2.44135
[1mStep[0m  [40/84], [94mLoss[0m : 2.43053
[1mStep[0m  [48/84], [94mLoss[0m : 2.68573
[1mStep[0m  [56/84], [94mLoss[0m : 2.67205
[1mStep[0m  [64/84], [94mLoss[0m : 2.45356
[1mStep[0m  [72/84], [94mLoss[0m : 2.46281
[1mStep[0m  [80/84], [94mLoss[0m : 2.23009

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.332, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.18285
[1mStep[0m  [8/84], [94mLoss[0m : 2.29726
[1mStep[0m  [16/84], [94mLoss[0m : 2.31819
[1mStep[0m  [24/84], [94mLoss[0m : 2.44527
[1mStep[0m  [32/84], [94mLoss[0m : 2.39393
[1mStep[0m  [40/84], [94mLoss[0m : 2.21365
[1mStep[0m  [48/84], [94mLoss[0m : 2.48244
[1mStep[0m  [56/84], [94mLoss[0m : 2.11572
[1mStep[0m  [64/84], [94mLoss[0m : 2.24742
[1mStep[0m  [72/84], [94mLoss[0m : 2.40907
[1mStep[0m  [80/84], [94mLoss[0m : 2.22679

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.321, [92mTest[0m: 2.385, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.83956
[1mStep[0m  [8/84], [94mLoss[0m : 2.24865
[1mStep[0m  [16/84], [94mLoss[0m : 2.11988
[1mStep[0m  [24/84], [94mLoss[0m : 1.89145
[1mStep[0m  [32/84], [94mLoss[0m : 2.20128
[1mStep[0m  [40/84], [94mLoss[0m : 2.33193
[1mStep[0m  [48/84], [94mLoss[0m : 2.21274
[1mStep[0m  [56/84], [94mLoss[0m : 2.33481
[1mStep[0m  [64/84], [94mLoss[0m : 2.04691
[1mStep[0m  [72/84], [94mLoss[0m : 2.03794
[1mStep[0m  [80/84], [94mLoss[0m : 2.02999

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.219, [92mTest[0m: 2.354, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.24623
[1mStep[0m  [8/84], [94mLoss[0m : 2.12726
[1mStep[0m  [16/84], [94mLoss[0m : 2.17419
[1mStep[0m  [24/84], [94mLoss[0m : 2.35050
[1mStep[0m  [32/84], [94mLoss[0m : 2.25133
[1mStep[0m  [40/84], [94mLoss[0m : 2.43996
[1mStep[0m  [48/84], [94mLoss[0m : 2.27639
[1mStep[0m  [56/84], [94mLoss[0m : 2.25838
[1mStep[0m  [64/84], [94mLoss[0m : 2.31128
[1mStep[0m  [72/84], [94mLoss[0m : 2.11908
[1mStep[0m  [80/84], [94mLoss[0m : 2.07670

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.140, [92mTest[0m: 2.372, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.96074
[1mStep[0m  [8/84], [94mLoss[0m : 1.88135
[1mStep[0m  [16/84], [94mLoss[0m : 2.00181
[1mStep[0m  [24/84], [94mLoss[0m : 2.16013
[1mStep[0m  [32/84], [94mLoss[0m : 2.07618
[1mStep[0m  [40/84], [94mLoss[0m : 1.93099
[1mStep[0m  [48/84], [94mLoss[0m : 2.18346
[1mStep[0m  [56/84], [94mLoss[0m : 1.98747
[1mStep[0m  [64/84], [94mLoss[0m : 2.14021
[1mStep[0m  [72/84], [94mLoss[0m : 2.13227
[1mStep[0m  [80/84], [94mLoss[0m : 2.10487

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.064, [92mTest[0m: 2.396, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.19545
[1mStep[0m  [8/84], [94mLoss[0m : 2.33352
[1mStep[0m  [16/84], [94mLoss[0m : 2.22171
[1mStep[0m  [24/84], [94mLoss[0m : 1.83057
[1mStep[0m  [32/84], [94mLoss[0m : 2.02923
[1mStep[0m  [40/84], [94mLoss[0m : 1.97017
[1mStep[0m  [48/84], [94mLoss[0m : 2.10615
[1mStep[0m  [56/84], [94mLoss[0m : 2.28732
[1mStep[0m  [64/84], [94mLoss[0m : 1.88118
[1mStep[0m  [72/84], [94mLoss[0m : 1.99774
[1mStep[0m  [80/84], [94mLoss[0m : 1.85386

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.008, [92mTest[0m: 2.408, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.11864
[1mStep[0m  [8/84], [94mLoss[0m : 1.97418
[1mStep[0m  [16/84], [94mLoss[0m : 1.77594
[1mStep[0m  [24/84], [94mLoss[0m : 2.18115
[1mStep[0m  [32/84], [94mLoss[0m : 2.15925
[1mStep[0m  [40/84], [94mLoss[0m : 2.06330
[1mStep[0m  [48/84], [94mLoss[0m : 1.80249
[1mStep[0m  [56/84], [94mLoss[0m : 1.95359
[1mStep[0m  [64/84], [94mLoss[0m : 1.81049
[1mStep[0m  [72/84], [94mLoss[0m : 2.11716
[1mStep[0m  [80/84], [94mLoss[0m : 2.05913

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.956, [92mTest[0m: 2.387, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.93013
[1mStep[0m  [8/84], [94mLoss[0m : 1.74469
[1mStep[0m  [16/84], [94mLoss[0m : 1.85040
[1mStep[0m  [24/84], [94mLoss[0m : 2.06485
[1mStep[0m  [32/84], [94mLoss[0m : 1.86802
[1mStep[0m  [40/84], [94mLoss[0m : 1.84754
[1mStep[0m  [48/84], [94mLoss[0m : 1.82325
[1mStep[0m  [56/84], [94mLoss[0m : 1.84379
[1mStep[0m  [64/84], [94mLoss[0m : 1.90854
[1mStep[0m  [72/84], [94mLoss[0m : 1.61339
[1mStep[0m  [80/84], [94mLoss[0m : 2.00548

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.901, [92mTest[0m: 2.479, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.77286
[1mStep[0m  [8/84], [94mLoss[0m : 1.88602
[1mStep[0m  [16/84], [94mLoss[0m : 2.13026
[1mStep[0m  [24/84], [94mLoss[0m : 1.81652
[1mStep[0m  [32/84], [94mLoss[0m : 2.12225
[1mStep[0m  [40/84], [94mLoss[0m : 2.00425
[1mStep[0m  [48/84], [94mLoss[0m : 2.02627
[1mStep[0m  [56/84], [94mLoss[0m : 1.97111
[1mStep[0m  [64/84], [94mLoss[0m : 1.79890
[1mStep[0m  [72/84], [94mLoss[0m : 1.90757
[1mStep[0m  [80/84], [94mLoss[0m : 2.07654

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.832, [92mTest[0m: 2.469, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.80046
[1mStep[0m  [8/84], [94mLoss[0m : 1.64785
[1mStep[0m  [16/84], [94mLoss[0m : 1.52744
[1mStep[0m  [24/84], [94mLoss[0m : 1.83257
[1mStep[0m  [32/84], [94mLoss[0m : 1.79635
[1mStep[0m  [40/84], [94mLoss[0m : 1.85158
[1mStep[0m  [48/84], [94mLoss[0m : 1.86853
[1mStep[0m  [56/84], [94mLoss[0m : 1.86984
[1mStep[0m  [64/84], [94mLoss[0m : 1.67703
[1mStep[0m  [72/84], [94mLoss[0m : 1.75094
[1mStep[0m  [80/84], [94mLoss[0m : 1.80211

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.759, [92mTest[0m: 2.435, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.69381
[1mStep[0m  [8/84], [94mLoss[0m : 1.66214
[1mStep[0m  [16/84], [94mLoss[0m : 1.41379
[1mStep[0m  [24/84], [94mLoss[0m : 1.80738
[1mStep[0m  [32/84], [94mLoss[0m : 1.56825
[1mStep[0m  [40/84], [94mLoss[0m : 1.61303
[1mStep[0m  [48/84], [94mLoss[0m : 1.95712
[1mStep[0m  [56/84], [94mLoss[0m : 1.68721
[1mStep[0m  [64/84], [94mLoss[0m : 1.59911
[1mStep[0m  [72/84], [94mLoss[0m : 1.80598
[1mStep[0m  [80/84], [94mLoss[0m : 1.96762

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.707, [92mTest[0m: 2.500, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.49039
[1mStep[0m  [8/84], [94mLoss[0m : 1.70082
[1mStep[0m  [16/84], [94mLoss[0m : 1.72189
[1mStep[0m  [24/84], [94mLoss[0m : 1.70494
[1mStep[0m  [32/84], [94mLoss[0m : 1.71514
[1mStep[0m  [40/84], [94mLoss[0m : 1.53143
[1mStep[0m  [48/84], [94mLoss[0m : 1.65045
[1mStep[0m  [56/84], [94mLoss[0m : 1.69914
[1mStep[0m  [64/84], [94mLoss[0m : 1.68185
[1mStep[0m  [72/84], [94mLoss[0m : 1.87510
[1mStep[0m  [80/84], [94mLoss[0m : 1.81716

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.663, [92mTest[0m: 2.452, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.59162
[1mStep[0m  [8/84], [94mLoss[0m : 1.37287
[1mStep[0m  [16/84], [94mLoss[0m : 1.36256
[1mStep[0m  [24/84], [94mLoss[0m : 1.51996
[1mStep[0m  [32/84], [94mLoss[0m : 1.72989
[1mStep[0m  [40/84], [94mLoss[0m : 1.69138
[1mStep[0m  [48/84], [94mLoss[0m : 1.57003
[1mStep[0m  [56/84], [94mLoss[0m : 1.62422
[1mStep[0m  [64/84], [94mLoss[0m : 1.35675
[1mStep[0m  [72/84], [94mLoss[0m : 1.79706
[1mStep[0m  [80/84], [94mLoss[0m : 1.70147

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.613, [92mTest[0m: 2.522, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.53232
[1mStep[0m  [8/84], [94mLoss[0m : 1.69849
[1mStep[0m  [16/84], [94mLoss[0m : 1.63121
[1mStep[0m  [24/84], [94mLoss[0m : 1.59648
[1mStep[0m  [32/84], [94mLoss[0m : 1.82823
[1mStep[0m  [40/84], [94mLoss[0m : 1.71998
[1mStep[0m  [48/84], [94mLoss[0m : 1.52284
[1mStep[0m  [56/84], [94mLoss[0m : 1.69540
[1mStep[0m  [64/84], [94mLoss[0m : 1.62858
[1mStep[0m  [72/84], [94mLoss[0m : 1.37977
[1mStep[0m  [80/84], [94mLoss[0m : 1.76599

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.613, [92mTest[0m: 2.537, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.44116
[1mStep[0m  [8/84], [94mLoss[0m : 1.68463
[1mStep[0m  [16/84], [94mLoss[0m : 1.46014
[1mStep[0m  [24/84], [94mLoss[0m : 1.66306
[1mStep[0m  [32/84], [94mLoss[0m : 1.77267
[1mStep[0m  [40/84], [94mLoss[0m : 1.68703
[1mStep[0m  [48/84], [94mLoss[0m : 1.39984
[1mStep[0m  [56/84], [94mLoss[0m : 1.57935
[1mStep[0m  [64/84], [94mLoss[0m : 1.41935
[1mStep[0m  [72/84], [94mLoss[0m : 1.67956
[1mStep[0m  [80/84], [94mLoss[0m : 1.67161

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.577, [92mTest[0m: 2.555, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.46841
[1mStep[0m  [8/84], [94mLoss[0m : 1.40388
[1mStep[0m  [16/84], [94mLoss[0m : 1.51323
[1mStep[0m  [24/84], [94mLoss[0m : 1.46017
[1mStep[0m  [32/84], [94mLoss[0m : 1.51266
[1mStep[0m  [40/84], [94mLoss[0m : 1.53578
[1mStep[0m  [48/84], [94mLoss[0m : 1.51199
[1mStep[0m  [56/84], [94mLoss[0m : 1.73135
[1mStep[0m  [64/84], [94mLoss[0m : 1.58754
[1mStep[0m  [72/84], [94mLoss[0m : 1.57003
[1mStep[0m  [80/84], [94mLoss[0m : 1.70690

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.522, [92mTest[0m: 2.477, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.56944
[1mStep[0m  [8/84], [94mLoss[0m : 1.46766
[1mStep[0m  [16/84], [94mLoss[0m : 1.42612
[1mStep[0m  [24/84], [94mLoss[0m : 1.51426
[1mStep[0m  [32/84], [94mLoss[0m : 1.53655
[1mStep[0m  [40/84], [94mLoss[0m : 1.52640
[1mStep[0m  [48/84], [94mLoss[0m : 1.37632
[1mStep[0m  [56/84], [94mLoss[0m : 1.49510
[1mStep[0m  [64/84], [94mLoss[0m : 1.47091
[1mStep[0m  [72/84], [94mLoss[0m : 1.62234
[1mStep[0m  [80/84], [94mLoss[0m : 1.45045

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.484, [92mTest[0m: 2.513, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.60267
[1mStep[0m  [8/84], [94mLoss[0m : 1.43457
[1mStep[0m  [16/84], [94mLoss[0m : 1.53547
[1mStep[0m  [24/84], [94mLoss[0m : 1.36791
[1mStep[0m  [32/84], [94mLoss[0m : 1.49961
[1mStep[0m  [40/84], [94mLoss[0m : 1.48232
[1mStep[0m  [48/84], [94mLoss[0m : 1.29918
[1mStep[0m  [56/84], [94mLoss[0m : 1.61744
[1mStep[0m  [64/84], [94mLoss[0m : 1.37222
[1mStep[0m  [72/84], [94mLoss[0m : 1.27186
[1mStep[0m  [80/84], [94mLoss[0m : 1.75564

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.472, [92mTest[0m: 2.496, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.31998
[1mStep[0m  [8/84], [94mLoss[0m : 1.51771
[1mStep[0m  [16/84], [94mLoss[0m : 1.42810
[1mStep[0m  [24/84], [94mLoss[0m : 1.37915
[1mStep[0m  [32/84], [94mLoss[0m : 1.52513
[1mStep[0m  [40/84], [94mLoss[0m : 1.35930
[1mStep[0m  [48/84], [94mLoss[0m : 1.33100
[1mStep[0m  [56/84], [94mLoss[0m : 1.49449
[1mStep[0m  [64/84], [94mLoss[0m : 1.32762
[1mStep[0m  [72/84], [94mLoss[0m : 1.49621
[1mStep[0m  [80/84], [94mLoss[0m : 1.57655

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.441, [92mTest[0m: 2.559, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.17993
[1mStep[0m  [8/84], [94mLoss[0m : 1.31391
[1mStep[0m  [16/84], [94mLoss[0m : 1.42667
[1mStep[0m  [24/84], [94mLoss[0m : 1.43572
[1mStep[0m  [32/84], [94mLoss[0m : 1.23125
[1mStep[0m  [40/84], [94mLoss[0m : 1.32226
[1mStep[0m  [48/84], [94mLoss[0m : 1.33337
[1mStep[0m  [56/84], [94mLoss[0m : 1.60275
[1mStep[0m  [64/84], [94mLoss[0m : 1.30067
[1mStep[0m  [72/84], [94mLoss[0m : 1.34046
[1mStep[0m  [80/84], [94mLoss[0m : 1.48838

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.400, [92mTest[0m: 2.539, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.37231
[1mStep[0m  [8/84], [94mLoss[0m : 1.23418
[1mStep[0m  [16/84], [94mLoss[0m : 1.36769
[1mStep[0m  [24/84], [94mLoss[0m : 1.06104
[1mStep[0m  [32/84], [94mLoss[0m : 1.35057
[1mStep[0m  [40/84], [94mLoss[0m : 1.48208
[1mStep[0m  [48/84], [94mLoss[0m : 1.53640
[1mStep[0m  [56/84], [94mLoss[0m : 1.42235
[1mStep[0m  [64/84], [94mLoss[0m : 1.20520
[1mStep[0m  [72/84], [94mLoss[0m : 1.30845
[1mStep[0m  [80/84], [94mLoss[0m : 1.27702

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.345, [92mTest[0m: 2.567, [96mlr[0m: 0.006772500000000002
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 20 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.487
====================================

Phase 2 - Evaluation MAE:  2.4871589967182706
MAE score P1        2.325332
MAE score P2        2.487159
loss                1.344862
learning_rate       0.007525
batch_size               128
hidden_sizes      [300, 100]
epochs                    30
activation              relu
optimizer                sgd
early stopping          True
dropout                  0.3
momentum                 0.9
weight_decay          0.0001
Name: 1, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 11.23463
[1mStep[0m  [8/84], [94mLoss[0m : 9.94366
[1mStep[0m  [16/84], [94mLoss[0m : 9.38896
[1mStep[0m  [24/84], [94mLoss[0m : 8.69245
[1mStep[0m  [32/84], [94mLoss[0m : 6.60408
[1mStep[0m  [40/84], [94mLoss[0m : 6.17646
[1mStep[0m  [48/84], [94mLoss[0m : 4.79742
[1mStep[0m  [56/84], [94mLoss[0m : 3.78293
[1mStep[0m  [64/84], [94mLoss[0m : 3.01722
[1mStep[0m  [72/84], [94mLoss[0m : 2.89463
[1mStep[0m  [80/84], [94mLoss[0m : 3.14007

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.242, [92mTest[0m: 11.106, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.61659
[1mStep[0m  [8/84], [94mLoss[0m : 2.78391
[1mStep[0m  [16/84], [94mLoss[0m : 2.74948
[1mStep[0m  [24/84], [94mLoss[0m : 2.71041
[1mStep[0m  [32/84], [94mLoss[0m : 2.66238
[1mStep[0m  [40/84], [94mLoss[0m : 2.76339
[1mStep[0m  [48/84], [94mLoss[0m : 2.64523
[1mStep[0m  [56/84], [94mLoss[0m : 2.82132
[1mStep[0m  [64/84], [94mLoss[0m : 2.33400
[1mStep[0m  [72/84], [94mLoss[0m : 2.39922
[1mStep[0m  [80/84], [94mLoss[0m : 2.44690

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.735, [92mTest[0m: 2.927, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.85968
[1mStep[0m  [8/84], [94mLoss[0m : 2.61534
[1mStep[0m  [16/84], [94mLoss[0m : 2.52730
[1mStep[0m  [24/84], [94mLoss[0m : 2.58817
[1mStep[0m  [32/84], [94mLoss[0m : 2.68953
[1mStep[0m  [40/84], [94mLoss[0m : 2.59022
[1mStep[0m  [48/84], [94mLoss[0m : 2.68236
[1mStep[0m  [56/84], [94mLoss[0m : 2.58704
[1mStep[0m  [64/84], [94mLoss[0m : 2.80770
[1mStep[0m  [72/84], [94mLoss[0m : 2.76050
[1mStep[0m  [80/84], [94mLoss[0m : 2.82932

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.649, [92mTest[0m: 2.443, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33557
[1mStep[0m  [8/84], [94mLoss[0m : 2.51130
[1mStep[0m  [16/84], [94mLoss[0m : 2.47202
[1mStep[0m  [24/84], [94mLoss[0m : 2.74833
[1mStep[0m  [32/84], [94mLoss[0m : 2.82877
[1mStep[0m  [40/84], [94mLoss[0m : 2.78504
[1mStep[0m  [48/84], [94mLoss[0m : 2.41640
[1mStep[0m  [56/84], [94mLoss[0m : 2.71337
[1mStep[0m  [64/84], [94mLoss[0m : 2.43292
[1mStep[0m  [72/84], [94mLoss[0m : 2.73172
[1mStep[0m  [80/84], [94mLoss[0m : 2.39789

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.603, [92mTest[0m: 2.442, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43255
[1mStep[0m  [8/84], [94mLoss[0m : 2.46210
[1mStep[0m  [16/84], [94mLoss[0m : 2.48922
[1mStep[0m  [24/84], [94mLoss[0m : 3.11806
[1mStep[0m  [32/84], [94mLoss[0m : 2.89610
[1mStep[0m  [40/84], [94mLoss[0m : 2.65209
[1mStep[0m  [48/84], [94mLoss[0m : 2.45524
[1mStep[0m  [56/84], [94mLoss[0m : 2.26891
[1mStep[0m  [64/84], [94mLoss[0m : 2.58104
[1mStep[0m  [72/84], [94mLoss[0m : 2.90029
[1mStep[0m  [80/84], [94mLoss[0m : 2.53836

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.577, [92mTest[0m: 2.367, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.70698
[1mStep[0m  [8/84], [94mLoss[0m : 2.50547
[1mStep[0m  [16/84], [94mLoss[0m : 2.51377
[1mStep[0m  [24/84], [94mLoss[0m : 2.77357
[1mStep[0m  [32/84], [94mLoss[0m : 2.70768
[1mStep[0m  [40/84], [94mLoss[0m : 2.32367
[1mStep[0m  [48/84], [94mLoss[0m : 2.94790
[1mStep[0m  [56/84], [94mLoss[0m : 2.78331
[1mStep[0m  [64/84], [94mLoss[0m : 2.58630
[1mStep[0m  [72/84], [94mLoss[0m : 2.39764
[1mStep[0m  [80/84], [94mLoss[0m : 2.46132

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.545, [92mTest[0m: 2.377, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.32370
[1mStep[0m  [8/84], [94mLoss[0m : 2.58878
[1mStep[0m  [16/84], [94mLoss[0m : 3.17832
[1mStep[0m  [24/84], [94mLoss[0m : 2.44972
[1mStep[0m  [32/84], [94mLoss[0m : 2.98417
[1mStep[0m  [40/84], [94mLoss[0m : 2.36479
[1mStep[0m  [48/84], [94mLoss[0m : 2.43755
[1mStep[0m  [56/84], [94mLoss[0m : 2.37902
[1mStep[0m  [64/84], [94mLoss[0m : 2.46286
[1mStep[0m  [72/84], [94mLoss[0m : 2.51609
[1mStep[0m  [80/84], [94mLoss[0m : 2.78166

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.361, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.70981
[1mStep[0m  [8/84], [94mLoss[0m : 2.28868
[1mStep[0m  [16/84], [94mLoss[0m : 2.84397
[1mStep[0m  [24/84], [94mLoss[0m : 2.62974
[1mStep[0m  [32/84], [94mLoss[0m : 2.48860
[1mStep[0m  [40/84], [94mLoss[0m : 2.35637
[1mStep[0m  [48/84], [94mLoss[0m : 2.73152
[1mStep[0m  [56/84], [94mLoss[0m : 2.49948
[1mStep[0m  [64/84], [94mLoss[0m : 2.94863
[1mStep[0m  [72/84], [94mLoss[0m : 2.48051
[1mStep[0m  [80/84], [94mLoss[0m : 2.37942

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.389, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.65017
[1mStep[0m  [8/84], [94mLoss[0m : 2.42979
[1mStep[0m  [16/84], [94mLoss[0m : 2.84993
[1mStep[0m  [24/84], [94mLoss[0m : 2.40594
[1mStep[0m  [32/84], [94mLoss[0m : 2.67089
[1mStep[0m  [40/84], [94mLoss[0m : 2.49651
[1mStep[0m  [48/84], [94mLoss[0m : 3.16399
[1mStep[0m  [56/84], [94mLoss[0m : 2.35642
[1mStep[0m  [64/84], [94mLoss[0m : 2.38507
[1mStep[0m  [72/84], [94mLoss[0m : 2.57839
[1mStep[0m  [80/84], [94mLoss[0m : 2.41693

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.348, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42920
[1mStep[0m  [8/84], [94mLoss[0m : 2.56532
[1mStep[0m  [16/84], [94mLoss[0m : 2.60603
[1mStep[0m  [24/84], [94mLoss[0m : 2.04059
[1mStep[0m  [32/84], [94mLoss[0m : 2.47864
[1mStep[0m  [40/84], [94mLoss[0m : 2.53637
[1mStep[0m  [48/84], [94mLoss[0m : 2.63236
[1mStep[0m  [56/84], [94mLoss[0m : 2.29459
[1mStep[0m  [64/84], [94mLoss[0m : 2.94440
[1mStep[0m  [72/84], [94mLoss[0m : 2.46651
[1mStep[0m  [80/84], [94mLoss[0m : 2.53358

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.363, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.75191
[1mStep[0m  [8/84], [94mLoss[0m : 2.41472
[1mStep[0m  [16/84], [94mLoss[0m : 2.23333
[1mStep[0m  [24/84], [94mLoss[0m : 2.36706
[1mStep[0m  [32/84], [94mLoss[0m : 2.30008
[1mStep[0m  [40/84], [94mLoss[0m : 2.31749
[1mStep[0m  [48/84], [94mLoss[0m : 2.35116
[1mStep[0m  [56/84], [94mLoss[0m : 2.86376
[1mStep[0m  [64/84], [94mLoss[0m : 2.55070
[1mStep[0m  [72/84], [94mLoss[0m : 2.31980
[1mStep[0m  [80/84], [94mLoss[0m : 2.65062

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.339, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44787
[1mStep[0m  [8/84], [94mLoss[0m : 2.75180
[1mStep[0m  [16/84], [94mLoss[0m : 2.74603
[1mStep[0m  [24/84], [94mLoss[0m : 2.38537
[1mStep[0m  [32/84], [94mLoss[0m : 2.45255
[1mStep[0m  [40/84], [94mLoss[0m : 2.53355
[1mStep[0m  [48/84], [94mLoss[0m : 2.55552
[1mStep[0m  [56/84], [94mLoss[0m : 2.77519
[1mStep[0m  [64/84], [94mLoss[0m : 2.27378
[1mStep[0m  [72/84], [94mLoss[0m : 2.73444
[1mStep[0m  [80/84], [94mLoss[0m : 2.47523

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.343, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.66710
[1mStep[0m  [8/84], [94mLoss[0m : 2.37290
[1mStep[0m  [16/84], [94mLoss[0m : 2.58243
[1mStep[0m  [24/84], [94mLoss[0m : 2.59018
[1mStep[0m  [32/84], [94mLoss[0m : 2.50366
[1mStep[0m  [40/84], [94mLoss[0m : 2.66877
[1mStep[0m  [48/84], [94mLoss[0m : 2.61361
[1mStep[0m  [56/84], [94mLoss[0m : 2.30246
[1mStep[0m  [64/84], [94mLoss[0m : 2.68232
[1mStep[0m  [72/84], [94mLoss[0m : 2.69274
[1mStep[0m  [80/84], [94mLoss[0m : 2.68086

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.335, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54820
[1mStep[0m  [8/84], [94mLoss[0m : 2.26314
[1mStep[0m  [16/84], [94mLoss[0m : 2.67105
[1mStep[0m  [24/84], [94mLoss[0m : 2.45463
[1mStep[0m  [32/84], [94mLoss[0m : 2.37361
[1mStep[0m  [40/84], [94mLoss[0m : 2.59050
[1mStep[0m  [48/84], [94mLoss[0m : 2.37646
[1mStep[0m  [56/84], [94mLoss[0m : 2.18238
[1mStep[0m  [64/84], [94mLoss[0m : 2.36835
[1mStep[0m  [72/84], [94mLoss[0m : 2.55373
[1mStep[0m  [80/84], [94mLoss[0m : 2.57829

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.333, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43840
[1mStep[0m  [8/84], [94mLoss[0m : 2.62228
[1mStep[0m  [16/84], [94mLoss[0m : 2.23619
[1mStep[0m  [24/84], [94mLoss[0m : 2.37147
[1mStep[0m  [32/84], [94mLoss[0m : 2.51269
[1mStep[0m  [40/84], [94mLoss[0m : 2.54340
[1mStep[0m  [48/84], [94mLoss[0m : 2.44922
[1mStep[0m  [56/84], [94mLoss[0m : 2.42489
[1mStep[0m  [64/84], [94mLoss[0m : 2.50218
[1mStep[0m  [72/84], [94mLoss[0m : 2.42630
[1mStep[0m  [80/84], [94mLoss[0m : 2.55221

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.345, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54876
[1mStep[0m  [8/84], [94mLoss[0m : 2.73463
[1mStep[0m  [16/84], [94mLoss[0m : 2.47966
[1mStep[0m  [24/84], [94mLoss[0m : 2.50079
[1mStep[0m  [32/84], [94mLoss[0m : 2.61361
[1mStep[0m  [40/84], [94mLoss[0m : 2.43062
[1mStep[0m  [48/84], [94mLoss[0m : 2.29047
[1mStep[0m  [56/84], [94mLoss[0m : 2.58596
[1mStep[0m  [64/84], [94mLoss[0m : 2.51100
[1mStep[0m  [72/84], [94mLoss[0m : 2.26958
[1mStep[0m  [80/84], [94mLoss[0m : 2.63846

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.326, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.29637
[1mStep[0m  [8/84], [94mLoss[0m : 2.48712
[1mStep[0m  [16/84], [94mLoss[0m : 2.28588
[1mStep[0m  [24/84], [94mLoss[0m : 2.64952
[1mStep[0m  [32/84], [94mLoss[0m : 2.37251
[1mStep[0m  [40/84], [94mLoss[0m : 2.44543
[1mStep[0m  [48/84], [94mLoss[0m : 2.45115
[1mStep[0m  [56/84], [94mLoss[0m : 2.51194
[1mStep[0m  [64/84], [94mLoss[0m : 2.55316
[1mStep[0m  [72/84], [94mLoss[0m : 2.12032
[1mStep[0m  [80/84], [94mLoss[0m : 2.39892

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.325, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31388
[1mStep[0m  [8/84], [94mLoss[0m : 2.53627
[1mStep[0m  [16/84], [94mLoss[0m : 2.47675
[1mStep[0m  [24/84], [94mLoss[0m : 2.39246
[1mStep[0m  [32/84], [94mLoss[0m : 2.23806
[1mStep[0m  [40/84], [94mLoss[0m : 2.25527
[1mStep[0m  [48/84], [94mLoss[0m : 2.71780
[1mStep[0m  [56/84], [94mLoss[0m : 2.51670
[1mStep[0m  [64/84], [94mLoss[0m : 2.38308
[1mStep[0m  [72/84], [94mLoss[0m : 2.38896
[1mStep[0m  [80/84], [94mLoss[0m : 2.88346

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.333, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26550
[1mStep[0m  [8/84], [94mLoss[0m : 2.66258
[1mStep[0m  [16/84], [94mLoss[0m : 2.14841
[1mStep[0m  [24/84], [94mLoss[0m : 2.47021
[1mStep[0m  [32/84], [94mLoss[0m : 2.24864
[1mStep[0m  [40/84], [94mLoss[0m : 2.38691
[1mStep[0m  [48/84], [94mLoss[0m : 2.19214
[1mStep[0m  [56/84], [94mLoss[0m : 2.62395
[1mStep[0m  [64/84], [94mLoss[0m : 2.45764
[1mStep[0m  [72/84], [94mLoss[0m : 2.11769
[1mStep[0m  [80/84], [94mLoss[0m : 2.28777

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.386, [92mTest[0m: 2.323, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.39503
[1mStep[0m  [8/84], [94mLoss[0m : 2.34233
[1mStep[0m  [16/84], [94mLoss[0m : 2.71546
[1mStep[0m  [24/84], [94mLoss[0m : 2.31462
[1mStep[0m  [32/84], [94mLoss[0m : 2.43725
[1mStep[0m  [40/84], [94mLoss[0m : 2.25286
[1mStep[0m  [48/84], [94mLoss[0m : 2.45743
[1mStep[0m  [56/84], [94mLoss[0m : 2.28585
[1mStep[0m  [64/84], [94mLoss[0m : 2.43945
[1mStep[0m  [72/84], [94mLoss[0m : 2.51102
[1mStep[0m  [80/84], [94mLoss[0m : 2.27341

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.391, [92mTest[0m: 2.327, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55727
[1mStep[0m  [8/84], [94mLoss[0m : 2.45747
[1mStep[0m  [16/84], [94mLoss[0m : 2.71395
[1mStep[0m  [24/84], [94mLoss[0m : 2.35358
[1mStep[0m  [32/84], [94mLoss[0m : 2.46551
[1mStep[0m  [40/84], [94mLoss[0m : 2.30511
[1mStep[0m  [48/84], [94mLoss[0m : 2.40403
[1mStep[0m  [56/84], [94mLoss[0m : 2.35557
[1mStep[0m  [64/84], [94mLoss[0m : 2.21617
[1mStep[0m  [72/84], [94mLoss[0m : 2.41669
[1mStep[0m  [80/84], [94mLoss[0m : 2.14608

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.338, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44587
[1mStep[0m  [8/84], [94mLoss[0m : 2.24921
[1mStep[0m  [16/84], [94mLoss[0m : 2.30700
[1mStep[0m  [24/84], [94mLoss[0m : 2.00267
[1mStep[0m  [32/84], [94mLoss[0m : 2.09105
[1mStep[0m  [40/84], [94mLoss[0m : 2.38416
[1mStep[0m  [48/84], [94mLoss[0m : 2.44610
[1mStep[0m  [56/84], [94mLoss[0m : 2.68137
[1mStep[0m  [64/84], [94mLoss[0m : 2.40553
[1mStep[0m  [72/84], [94mLoss[0m : 2.34057
[1mStep[0m  [80/84], [94mLoss[0m : 2.52434

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.381, [92mTest[0m: 2.330, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47658
[1mStep[0m  [8/84], [94mLoss[0m : 2.45097
[1mStep[0m  [16/84], [94mLoss[0m : 2.66187
[1mStep[0m  [24/84], [94mLoss[0m : 2.15326
[1mStep[0m  [32/84], [94mLoss[0m : 2.46453
[1mStep[0m  [40/84], [94mLoss[0m : 2.56279
[1mStep[0m  [48/84], [94mLoss[0m : 2.56428
[1mStep[0m  [56/84], [94mLoss[0m : 2.47150
[1mStep[0m  [64/84], [94mLoss[0m : 2.35468
[1mStep[0m  [72/84], [94mLoss[0m : 2.53345
[1mStep[0m  [80/84], [94mLoss[0m : 2.58492

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.394, [92mTest[0m: 2.331, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37942
[1mStep[0m  [8/84], [94mLoss[0m : 2.58952
[1mStep[0m  [16/84], [94mLoss[0m : 2.22108
[1mStep[0m  [24/84], [94mLoss[0m : 2.50764
[1mStep[0m  [32/84], [94mLoss[0m : 2.54276
[1mStep[0m  [40/84], [94mLoss[0m : 2.43084
[1mStep[0m  [48/84], [94mLoss[0m : 2.19551
[1mStep[0m  [56/84], [94mLoss[0m : 2.39192
[1mStep[0m  [64/84], [94mLoss[0m : 2.36489
[1mStep[0m  [72/84], [94mLoss[0m : 2.76276
[1mStep[0m  [80/84], [94mLoss[0m : 2.34013

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.394, [92mTest[0m: 2.324, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27049
[1mStep[0m  [8/84], [94mLoss[0m : 2.43089
[1mStep[0m  [16/84], [94mLoss[0m : 2.39485
[1mStep[0m  [24/84], [94mLoss[0m : 2.09738
[1mStep[0m  [32/84], [94mLoss[0m : 2.69011
[1mStep[0m  [40/84], [94mLoss[0m : 2.54597
[1mStep[0m  [48/84], [94mLoss[0m : 2.31055
[1mStep[0m  [56/84], [94mLoss[0m : 2.46919
[1mStep[0m  [64/84], [94mLoss[0m : 2.73096
[1mStep[0m  [72/84], [94mLoss[0m : 2.49250
[1mStep[0m  [80/84], [94mLoss[0m : 2.20581

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.372, [92mTest[0m: 2.328, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26567
[1mStep[0m  [8/84], [94mLoss[0m : 2.38006
[1mStep[0m  [16/84], [94mLoss[0m : 2.16274
[1mStep[0m  [24/84], [94mLoss[0m : 2.17166
[1mStep[0m  [32/84], [94mLoss[0m : 2.37275
[1mStep[0m  [40/84], [94mLoss[0m : 2.10486
[1mStep[0m  [48/84], [94mLoss[0m : 2.29756
[1mStep[0m  [56/84], [94mLoss[0m : 2.22360
[1mStep[0m  [64/84], [94mLoss[0m : 2.53450
[1mStep[0m  [72/84], [94mLoss[0m : 2.44317
[1mStep[0m  [80/84], [94mLoss[0m : 2.05012

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.355, [92mTest[0m: 2.320, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53427
[1mStep[0m  [8/84], [94mLoss[0m : 2.26431
[1mStep[0m  [16/84], [94mLoss[0m : 2.35555
[1mStep[0m  [24/84], [94mLoss[0m : 2.66911
[1mStep[0m  [32/84], [94mLoss[0m : 2.28551
[1mStep[0m  [40/84], [94mLoss[0m : 2.56362
[1mStep[0m  [48/84], [94mLoss[0m : 2.44757
[1mStep[0m  [56/84], [94mLoss[0m : 2.49546
[1mStep[0m  [64/84], [94mLoss[0m : 2.18869
[1mStep[0m  [72/84], [94mLoss[0m : 2.31819
[1mStep[0m  [80/84], [94mLoss[0m : 2.12282

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.360, [92mTest[0m: 2.331, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.74047
[1mStep[0m  [8/84], [94mLoss[0m : 2.33635
[1mStep[0m  [16/84], [94mLoss[0m : 2.39477
[1mStep[0m  [24/84], [94mLoss[0m : 2.28413
[1mStep[0m  [32/84], [94mLoss[0m : 2.57324
[1mStep[0m  [40/84], [94mLoss[0m : 2.08067
[1mStep[0m  [48/84], [94mLoss[0m : 2.41741
[1mStep[0m  [56/84], [94mLoss[0m : 2.51308
[1mStep[0m  [64/84], [94mLoss[0m : 2.36719
[1mStep[0m  [72/84], [94mLoss[0m : 2.48799
[1mStep[0m  [80/84], [94mLoss[0m : 2.38280

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.361, [92mTest[0m: 2.313, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.19464
[1mStep[0m  [8/84], [94mLoss[0m : 2.47249
[1mStep[0m  [16/84], [94mLoss[0m : 2.45271
[1mStep[0m  [24/84], [94mLoss[0m : 2.26137
[1mStep[0m  [32/84], [94mLoss[0m : 2.42933
[1mStep[0m  [40/84], [94mLoss[0m : 2.43799
[1mStep[0m  [48/84], [94mLoss[0m : 2.40082
[1mStep[0m  [56/84], [94mLoss[0m : 2.21596
[1mStep[0m  [64/84], [94mLoss[0m : 2.29838
[1mStep[0m  [72/84], [94mLoss[0m : 2.42319
[1mStep[0m  [80/84], [94mLoss[0m : 2.67345

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.372, [92mTest[0m: 2.332, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.25370
[1mStep[0m  [8/84], [94mLoss[0m : 2.27085
[1mStep[0m  [16/84], [94mLoss[0m : 2.38396
[1mStep[0m  [24/84], [94mLoss[0m : 1.99659
[1mStep[0m  [32/84], [94mLoss[0m : 2.31933
[1mStep[0m  [40/84], [94mLoss[0m : 2.66946
[1mStep[0m  [48/84], [94mLoss[0m : 2.54651
[1mStep[0m  [56/84], [94mLoss[0m : 2.40891
[1mStep[0m  [64/84], [94mLoss[0m : 2.32170
[1mStep[0m  [72/84], [94mLoss[0m : 2.46521
[1mStep[0m  [80/84], [94mLoss[0m : 2.39105

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.338, [92mTest[0m: 2.316, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.326
====================================

Phase 1 - Evaluation MAE:  2.326129470552717
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 2.56645
[1mStep[0m  [8/84], [94mLoss[0m : 2.49348
[1mStep[0m  [16/84], [94mLoss[0m : 2.58628
[1mStep[0m  [24/84], [94mLoss[0m : 2.50567
[1mStep[0m  [32/84], [94mLoss[0m : 2.31591
[1mStep[0m  [40/84], [94mLoss[0m : 2.41783
[1mStep[0m  [48/84], [94mLoss[0m : 2.09345
[1mStep[0m  [56/84], [94mLoss[0m : 2.50711
[1mStep[0m  [64/84], [94mLoss[0m : 2.55867
[1mStep[0m  [72/84], [94mLoss[0m : 2.36709
[1mStep[0m  [80/84], [94mLoss[0m : 2.27894

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.324, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.69432
[1mStep[0m  [8/84], [94mLoss[0m : 2.60247
[1mStep[0m  [16/84], [94mLoss[0m : 1.92547
[1mStep[0m  [24/84], [94mLoss[0m : 2.35523
[1mStep[0m  [32/84], [94mLoss[0m : 2.21390
[1mStep[0m  [40/84], [94mLoss[0m : 2.54307
[1mStep[0m  [48/84], [94mLoss[0m : 2.43904
[1mStep[0m  [56/84], [94mLoss[0m : 2.55727
[1mStep[0m  [64/84], [94mLoss[0m : 2.60874
[1mStep[0m  [72/84], [94mLoss[0m : 2.26835
[1mStep[0m  [80/84], [94mLoss[0m : 2.42703

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.410, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37774
[1mStep[0m  [8/84], [94mLoss[0m : 2.51851
[1mStep[0m  [16/84], [94mLoss[0m : 2.33410
[1mStep[0m  [24/84], [94mLoss[0m : 2.34690
[1mStep[0m  [32/84], [94mLoss[0m : 2.14817
[1mStep[0m  [40/84], [94mLoss[0m : 2.38833
[1mStep[0m  [48/84], [94mLoss[0m : 2.32441
[1mStep[0m  [56/84], [94mLoss[0m : 2.46805
[1mStep[0m  [64/84], [94mLoss[0m : 1.98299
[1mStep[0m  [72/84], [94mLoss[0m : 2.43446
[1mStep[0m  [80/84], [94mLoss[0m : 2.06930

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.330, [92mTest[0m: 2.351, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27323
[1mStep[0m  [8/84], [94mLoss[0m : 2.20723
[1mStep[0m  [16/84], [94mLoss[0m : 2.35807
[1mStep[0m  [24/84], [94mLoss[0m : 2.01387
[1mStep[0m  [32/84], [94mLoss[0m : 2.77673
[1mStep[0m  [40/84], [94mLoss[0m : 2.44693
[1mStep[0m  [48/84], [94mLoss[0m : 2.49688
[1mStep[0m  [56/84], [94mLoss[0m : 2.39121
[1mStep[0m  [64/84], [94mLoss[0m : 2.46286
[1mStep[0m  [72/84], [94mLoss[0m : 2.14468
[1mStep[0m  [80/84], [94mLoss[0m : 2.18961

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.279, [92mTest[0m: 2.350, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.12480
[1mStep[0m  [8/84], [94mLoss[0m : 2.09803
[1mStep[0m  [16/84], [94mLoss[0m : 2.12331
[1mStep[0m  [24/84], [94mLoss[0m : 1.91344
[1mStep[0m  [32/84], [94mLoss[0m : 2.34731
[1mStep[0m  [40/84], [94mLoss[0m : 2.32048
[1mStep[0m  [48/84], [94mLoss[0m : 2.32446
[1mStep[0m  [56/84], [94mLoss[0m : 2.09523
[1mStep[0m  [64/84], [94mLoss[0m : 2.31729
[1mStep[0m  [72/84], [94mLoss[0m : 2.22717
[1mStep[0m  [80/84], [94mLoss[0m : 2.08449

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.219, [92mTest[0m: 2.345, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.19353
[1mStep[0m  [8/84], [94mLoss[0m : 2.20662
[1mStep[0m  [16/84], [94mLoss[0m : 2.19076
[1mStep[0m  [24/84], [94mLoss[0m : 1.97171
[1mStep[0m  [32/84], [94mLoss[0m : 2.25445
[1mStep[0m  [40/84], [94mLoss[0m : 2.21873
[1mStep[0m  [48/84], [94mLoss[0m : 2.34748
[1mStep[0m  [56/84], [94mLoss[0m : 2.05858
[1mStep[0m  [64/84], [94mLoss[0m : 2.13943
[1mStep[0m  [72/84], [94mLoss[0m : 1.96775
[1mStep[0m  [80/84], [94mLoss[0m : 2.13774

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.162, [92mTest[0m: 2.383, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.12748
[1mStep[0m  [8/84], [94mLoss[0m : 2.43928
[1mStep[0m  [16/84], [94mLoss[0m : 2.16342
[1mStep[0m  [24/84], [94mLoss[0m : 1.94904
[1mStep[0m  [32/84], [94mLoss[0m : 2.45256
[1mStep[0m  [40/84], [94mLoss[0m : 2.06057
[1mStep[0m  [48/84], [94mLoss[0m : 2.50536
[1mStep[0m  [56/84], [94mLoss[0m : 1.98348
[1mStep[0m  [64/84], [94mLoss[0m : 1.98674
[1mStep[0m  [72/84], [94mLoss[0m : 1.91335
[1mStep[0m  [80/84], [94mLoss[0m : 2.12136

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.094, [92mTest[0m: 2.394, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.15163
[1mStep[0m  [8/84], [94mLoss[0m : 1.83406
[1mStep[0m  [16/84], [94mLoss[0m : 2.19016
[1mStep[0m  [24/84], [94mLoss[0m : 2.23755
[1mStep[0m  [32/84], [94mLoss[0m : 1.94368
[1mStep[0m  [40/84], [94mLoss[0m : 1.93587
[1mStep[0m  [48/84], [94mLoss[0m : 1.96004
[1mStep[0m  [56/84], [94mLoss[0m : 2.08676
[1mStep[0m  [64/84], [94mLoss[0m : 1.99047
[1mStep[0m  [72/84], [94mLoss[0m : 2.16140
[1mStep[0m  [80/84], [94mLoss[0m : 1.87789

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.060, [92mTest[0m: 2.429, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.66225
[1mStep[0m  [8/84], [94mLoss[0m : 1.91464
[1mStep[0m  [16/84], [94mLoss[0m : 1.75892
[1mStep[0m  [24/84], [94mLoss[0m : 2.00101
[1mStep[0m  [32/84], [94mLoss[0m : 1.98378
[1mStep[0m  [40/84], [94mLoss[0m : 2.08048
[1mStep[0m  [48/84], [94mLoss[0m : 1.86551
[1mStep[0m  [56/84], [94mLoss[0m : 2.04279
[1mStep[0m  [64/84], [94mLoss[0m : 2.11927
[1mStep[0m  [72/84], [94mLoss[0m : 2.04320
[1mStep[0m  [80/84], [94mLoss[0m : 1.99271

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.996, [92mTest[0m: 2.450, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.94591
[1mStep[0m  [8/84], [94mLoss[0m : 1.94783
[1mStep[0m  [16/84], [94mLoss[0m : 1.94038
[1mStep[0m  [24/84], [94mLoss[0m : 2.28625
[1mStep[0m  [32/84], [94mLoss[0m : 1.80660
[1mStep[0m  [40/84], [94mLoss[0m : 2.11191
[1mStep[0m  [48/84], [94mLoss[0m : 2.09092
[1mStep[0m  [56/84], [94mLoss[0m : 1.88715
[1mStep[0m  [64/84], [94mLoss[0m : 2.09624
[1mStep[0m  [72/84], [94mLoss[0m : 2.08442
[1mStep[0m  [80/84], [94mLoss[0m : 2.21808

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.970, [92mTest[0m: 2.422, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.61967
[1mStep[0m  [8/84], [94mLoss[0m : 1.69807
[1mStep[0m  [16/84], [94mLoss[0m : 2.38011
[1mStep[0m  [24/84], [94mLoss[0m : 1.72183
[1mStep[0m  [32/84], [94mLoss[0m : 2.22580
[1mStep[0m  [40/84], [94mLoss[0m : 1.87081
[1mStep[0m  [48/84], [94mLoss[0m : 1.89736
[1mStep[0m  [56/84], [94mLoss[0m : 1.70879
[1mStep[0m  [64/84], [94mLoss[0m : 2.06052
[1mStep[0m  [72/84], [94mLoss[0m : 1.70365
[1mStep[0m  [80/84], [94mLoss[0m : 1.73380

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.920, [92mTest[0m: 2.414, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.80838
[1mStep[0m  [8/84], [94mLoss[0m : 2.01360
[1mStep[0m  [16/84], [94mLoss[0m : 1.97185
[1mStep[0m  [24/84], [94mLoss[0m : 1.89075
[1mStep[0m  [32/84], [94mLoss[0m : 1.93575
[1mStep[0m  [40/84], [94mLoss[0m : 1.73881
[1mStep[0m  [48/84], [94mLoss[0m : 1.70554
[1mStep[0m  [56/84], [94mLoss[0m : 1.76665
[1mStep[0m  [64/84], [94mLoss[0m : 2.13350
[1mStep[0m  [72/84], [94mLoss[0m : 1.85731
[1mStep[0m  [80/84], [94mLoss[0m : 1.95724

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.878, [92mTest[0m: 2.448, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.81900
[1mStep[0m  [8/84], [94mLoss[0m : 1.85343
[1mStep[0m  [16/84], [94mLoss[0m : 1.83412
[1mStep[0m  [24/84], [94mLoss[0m : 2.00262
[1mStep[0m  [32/84], [94mLoss[0m : 1.85025
[1mStep[0m  [40/84], [94mLoss[0m : 1.94295
[1mStep[0m  [48/84], [94mLoss[0m : 1.85889
[1mStep[0m  [56/84], [94mLoss[0m : 1.73433
[1mStep[0m  [64/84], [94mLoss[0m : 1.85950
[1mStep[0m  [72/84], [94mLoss[0m : 2.08929
[1mStep[0m  [80/84], [94mLoss[0m : 1.84150

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.845, [92mTest[0m: 2.399, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.09873
[1mStep[0m  [8/84], [94mLoss[0m : 1.78483
[1mStep[0m  [16/84], [94mLoss[0m : 1.65351
[1mStep[0m  [24/84], [94mLoss[0m : 1.72710
[1mStep[0m  [32/84], [94mLoss[0m : 1.72719
[1mStep[0m  [40/84], [94mLoss[0m : 2.13104
[1mStep[0m  [48/84], [94mLoss[0m : 1.76228
[1mStep[0m  [56/84], [94mLoss[0m : 1.64092
[1mStep[0m  [64/84], [94mLoss[0m : 1.66396
[1mStep[0m  [72/84], [94mLoss[0m : 1.55514
[1mStep[0m  [80/84], [94mLoss[0m : 1.90986

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.811, [92mTest[0m: 2.472, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.81377
[1mStep[0m  [8/84], [94mLoss[0m : 1.66371
[1mStep[0m  [16/84], [94mLoss[0m : 1.51778
[1mStep[0m  [24/84], [94mLoss[0m : 1.72013
[1mStep[0m  [32/84], [94mLoss[0m : 1.64416
[1mStep[0m  [40/84], [94mLoss[0m : 1.95279
[1mStep[0m  [48/84], [94mLoss[0m : 1.76287
[1mStep[0m  [56/84], [94mLoss[0m : 1.65706
[1mStep[0m  [64/84], [94mLoss[0m : 1.80879
[1mStep[0m  [72/84], [94mLoss[0m : 1.81251
[1mStep[0m  [80/84], [94mLoss[0m : 1.64416

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.773, [92mTest[0m: 2.508, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.67068
[1mStep[0m  [8/84], [94mLoss[0m : 1.46386
[1mStep[0m  [16/84], [94mLoss[0m : 1.80332
[1mStep[0m  [24/84], [94mLoss[0m : 1.61703
[1mStep[0m  [32/84], [94mLoss[0m : 1.75577
[1mStep[0m  [40/84], [94mLoss[0m : 1.68566
[1mStep[0m  [48/84], [94mLoss[0m : 1.98120
[1mStep[0m  [56/84], [94mLoss[0m : 1.59533
[1mStep[0m  [64/84], [94mLoss[0m : 1.63693
[1mStep[0m  [72/84], [94mLoss[0m : 1.75442
[1mStep[0m  [80/84], [94mLoss[0m : 1.74527

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.738, [92mTest[0m: 2.448, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.73046
[1mStep[0m  [8/84], [94mLoss[0m : 1.72809
[1mStep[0m  [16/84], [94mLoss[0m : 1.63578
[1mStep[0m  [24/84], [94mLoss[0m : 1.64837
[1mStep[0m  [32/84], [94mLoss[0m : 1.55024
[1mStep[0m  [40/84], [94mLoss[0m : 1.88976
[1mStep[0m  [48/84], [94mLoss[0m : 1.87982
[1mStep[0m  [56/84], [94mLoss[0m : 1.67417
[1mStep[0m  [64/84], [94mLoss[0m : 2.24415
[1mStep[0m  [72/84], [94mLoss[0m : 1.84203
[1mStep[0m  [80/84], [94mLoss[0m : 1.75952

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.718, [92mTest[0m: 2.506, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.60047
[1mStep[0m  [8/84], [94mLoss[0m : 1.54114
[1mStep[0m  [16/84], [94mLoss[0m : 1.65749
[1mStep[0m  [24/84], [94mLoss[0m : 1.80191
[1mStep[0m  [32/84], [94mLoss[0m : 1.53968
[1mStep[0m  [40/84], [94mLoss[0m : 1.92701
[1mStep[0m  [48/84], [94mLoss[0m : 1.68868
[1mStep[0m  [56/84], [94mLoss[0m : 1.95484
[1mStep[0m  [64/84], [94mLoss[0m : 1.46768
[1mStep[0m  [72/84], [94mLoss[0m : 1.58549
[1mStep[0m  [80/84], [94mLoss[0m : 1.56809

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.685, [92mTest[0m: 2.472, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.82746
[1mStep[0m  [8/84], [94mLoss[0m : 1.35982
[1mStep[0m  [16/84], [94mLoss[0m : 1.63490
[1mStep[0m  [24/84], [94mLoss[0m : 1.82682
[1mStep[0m  [32/84], [94mLoss[0m : 1.45539
[1mStep[0m  [40/84], [94mLoss[0m : 1.58915
[1mStep[0m  [48/84], [94mLoss[0m : 1.85825
[1mStep[0m  [56/84], [94mLoss[0m : 1.43467
[1mStep[0m  [64/84], [94mLoss[0m : 1.75191
[1mStep[0m  [72/84], [94mLoss[0m : 1.68924
[1mStep[0m  [80/84], [94mLoss[0m : 1.92803

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.647, [92mTest[0m: 2.434, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.58889
[1mStep[0m  [8/84], [94mLoss[0m : 1.61239
[1mStep[0m  [16/84], [94mLoss[0m : 1.64752
[1mStep[0m  [24/84], [94mLoss[0m : 1.58915
[1mStep[0m  [32/84], [94mLoss[0m : 1.35922
[1mStep[0m  [40/84], [94mLoss[0m : 1.51665
[1mStep[0m  [48/84], [94mLoss[0m : 1.72367
[1mStep[0m  [56/84], [94mLoss[0m : 1.71983
[1mStep[0m  [64/84], [94mLoss[0m : 1.67649
[1mStep[0m  [72/84], [94mLoss[0m : 1.54659
[1mStep[0m  [80/84], [94mLoss[0m : 1.71348

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.614, [92mTest[0m: 2.525, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.46004
[1mStep[0m  [8/84], [94mLoss[0m : 1.58317
[1mStep[0m  [16/84], [94mLoss[0m : 1.65384
[1mStep[0m  [24/84], [94mLoss[0m : 1.74578
[1mStep[0m  [32/84], [94mLoss[0m : 1.69788
[1mStep[0m  [40/84], [94mLoss[0m : 1.50002
[1mStep[0m  [48/84], [94mLoss[0m : 1.66086
[1mStep[0m  [56/84], [94mLoss[0m : 1.91424
[1mStep[0m  [64/84], [94mLoss[0m : 1.65225
[1mStep[0m  [72/84], [94mLoss[0m : 1.74271
[1mStep[0m  [80/84], [94mLoss[0m : 1.68680

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.603, [92mTest[0m: 2.520, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.64179
[1mStep[0m  [8/84], [94mLoss[0m : 1.47004
[1mStep[0m  [16/84], [94mLoss[0m : 1.43883
[1mStep[0m  [24/84], [94mLoss[0m : 1.66142
[1mStep[0m  [32/84], [94mLoss[0m : 1.49740
[1mStep[0m  [40/84], [94mLoss[0m : 1.59440
[1mStep[0m  [48/84], [94mLoss[0m : 1.71151
[1mStep[0m  [56/84], [94mLoss[0m : 1.61877
[1mStep[0m  [64/84], [94mLoss[0m : 1.48902
[1mStep[0m  [72/84], [94mLoss[0m : 1.54106
[1mStep[0m  [80/84], [94mLoss[0m : 1.85443

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.564, [92mTest[0m: 2.497, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.33593
[1mStep[0m  [8/84], [94mLoss[0m : 1.59947
[1mStep[0m  [16/84], [94mLoss[0m : 1.37411
[1mStep[0m  [24/84], [94mLoss[0m : 1.44137
[1mStep[0m  [32/84], [94mLoss[0m : 1.61728
[1mStep[0m  [40/84], [94mLoss[0m : 1.68126
[1mStep[0m  [48/84], [94mLoss[0m : 1.69935
[1mStep[0m  [56/84], [94mLoss[0m : 1.49168
[1mStep[0m  [64/84], [94mLoss[0m : 1.55824
[1mStep[0m  [72/84], [94mLoss[0m : 1.49877
[1mStep[0m  [80/84], [94mLoss[0m : 1.40238

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.547, [92mTest[0m: 2.489, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.78844
[1mStep[0m  [8/84], [94mLoss[0m : 1.52527
[1mStep[0m  [16/84], [94mLoss[0m : 1.41629
[1mStep[0m  [24/84], [94mLoss[0m : 1.60331
[1mStep[0m  [32/84], [94mLoss[0m : 1.63967
[1mStep[0m  [40/84], [94mLoss[0m : 1.37881
[1mStep[0m  [48/84], [94mLoss[0m : 1.32874
[1mStep[0m  [56/84], [94mLoss[0m : 1.67262
[1mStep[0m  [64/84], [94mLoss[0m : 1.45601
[1mStep[0m  [72/84], [94mLoss[0m : 1.47701
[1mStep[0m  [80/84], [94mLoss[0m : 1.59199

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.519, [92mTest[0m: 2.571, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.45712
[1mStep[0m  [8/84], [94mLoss[0m : 1.70058
[1mStep[0m  [16/84], [94mLoss[0m : 1.57951
[1mStep[0m  [24/84], [94mLoss[0m : 1.46114
[1mStep[0m  [32/84], [94mLoss[0m : 1.33692
[1mStep[0m  [40/84], [94mLoss[0m : 1.51457
[1mStep[0m  [48/84], [94mLoss[0m : 1.32722
[1mStep[0m  [56/84], [94mLoss[0m : 1.42566
[1mStep[0m  [64/84], [94mLoss[0m : 1.58924
[1mStep[0m  [72/84], [94mLoss[0m : 1.65266
[1mStep[0m  [80/84], [94mLoss[0m : 1.70112

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.513, [92mTest[0m: 2.637, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.39112
[1mStep[0m  [8/84], [94mLoss[0m : 1.40724
[1mStep[0m  [16/84], [94mLoss[0m : 1.38390
[1mStep[0m  [24/84], [94mLoss[0m : 1.42575
[1mStep[0m  [32/84], [94mLoss[0m : 1.45521
[1mStep[0m  [40/84], [94mLoss[0m : 1.68990
[1mStep[0m  [48/84], [94mLoss[0m : 1.68557
[1mStep[0m  [56/84], [94mLoss[0m : 1.59596
[1mStep[0m  [64/84], [94mLoss[0m : 1.39136
[1mStep[0m  [72/84], [94mLoss[0m : 1.46616
[1mStep[0m  [80/84], [94mLoss[0m : 1.66211

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.479, [92mTest[0m: 2.444, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.49312
[1mStep[0m  [8/84], [94mLoss[0m : 1.33615
[1mStep[0m  [16/84], [94mLoss[0m : 1.50220
[1mStep[0m  [24/84], [94mLoss[0m : 1.46128
[1mStep[0m  [32/84], [94mLoss[0m : 1.35055
[1mStep[0m  [40/84], [94mLoss[0m : 1.35991
[1mStep[0m  [48/84], [94mLoss[0m : 1.55722
[1mStep[0m  [56/84], [94mLoss[0m : 1.32311
[1mStep[0m  [64/84], [94mLoss[0m : 1.29885
[1mStep[0m  [72/84], [94mLoss[0m : 1.62878
[1mStep[0m  [80/84], [94mLoss[0m : 1.53329

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.483, [92mTest[0m: 2.501, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.53483
[1mStep[0m  [8/84], [94mLoss[0m : 1.49491
[1mStep[0m  [16/84], [94mLoss[0m : 1.69792
[1mStep[0m  [24/84], [94mLoss[0m : 1.50415
[1mStep[0m  [32/84], [94mLoss[0m : 1.57357
[1mStep[0m  [40/84], [94mLoss[0m : 1.53328
[1mStep[0m  [48/84], [94mLoss[0m : 1.73819
[1mStep[0m  [56/84], [94mLoss[0m : 1.65096
[1mStep[0m  [64/84], [94mLoss[0m : 1.22148
[1mStep[0m  [72/84], [94mLoss[0m : 1.49150
[1mStep[0m  [80/84], [94mLoss[0m : 1.66737

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.471, [92mTest[0m: 2.509, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.41500
[1mStep[0m  [8/84], [94mLoss[0m : 1.42143
[1mStep[0m  [16/84], [94mLoss[0m : 1.31054
[1mStep[0m  [24/84], [94mLoss[0m : 1.34897
[1mStep[0m  [32/84], [94mLoss[0m : 1.32881
[1mStep[0m  [40/84], [94mLoss[0m : 1.64679
[1mStep[0m  [48/84], [94mLoss[0m : 1.50777
[1mStep[0m  [56/84], [94mLoss[0m : 1.49920
[1mStep[0m  [64/84], [94mLoss[0m : 1.42338
[1mStep[0m  [72/84], [94mLoss[0m : 1.48884
[1mStep[0m  [80/84], [94mLoss[0m : 1.34019

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.432, [92mTest[0m: 2.516, [96mlr[0m: 0.006772500000000002
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 28 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.505
====================================

Phase 2 - Evaluation MAE:  2.5052718009267534
MAE score P1       2.326129
MAE score P2       2.505272
loss               1.432325
learning_rate      0.007525
batch_size              128
hidden_sizes      [200, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping         True
dropout                 0.2
momentum                0.5
weight_decay           0.01
Name: 2, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 10.79479
[1mStep[0m  [4/42], [94mLoss[0m : 9.69808
[1mStep[0m  [8/42], [94mLoss[0m : 8.92778
[1mStep[0m  [12/42], [94mLoss[0m : 8.38661
[1mStep[0m  [16/42], [94mLoss[0m : 7.20713
[1mStep[0m  [20/42], [94mLoss[0m : 6.02830
[1mStep[0m  [24/42], [94mLoss[0m : 5.42781
[1mStep[0m  [28/42], [94mLoss[0m : 4.96775
[1mStep[0m  [32/42], [94mLoss[0m : 4.27946
[1mStep[0m  [36/42], [94mLoss[0m : 3.84146
[1mStep[0m  [40/42], [94mLoss[0m : 3.44953

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.469, [92mTest[0m: 10.673, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.36753
[1mStep[0m  [4/42], [94mLoss[0m : 3.15347
[1mStep[0m  [8/42], [94mLoss[0m : 2.83406
[1mStep[0m  [12/42], [94mLoss[0m : 2.64598
[1mStep[0m  [16/42], [94mLoss[0m : 2.88275
[1mStep[0m  [20/42], [94mLoss[0m : 3.01827
[1mStep[0m  [24/42], [94mLoss[0m : 2.67637
[1mStep[0m  [28/42], [94mLoss[0m : 2.64262
[1mStep[0m  [32/42], [94mLoss[0m : 2.63095
[1mStep[0m  [36/42], [94mLoss[0m : 2.51895
[1mStep[0m  [40/42], [94mLoss[0m : 2.57434

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.822, [92mTest[0m: 3.302, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47913
[1mStep[0m  [4/42], [94mLoss[0m : 2.40301
[1mStep[0m  [8/42], [94mLoss[0m : 2.46621
[1mStep[0m  [12/42], [94mLoss[0m : 2.44085
[1mStep[0m  [16/42], [94mLoss[0m : 2.51330
[1mStep[0m  [20/42], [94mLoss[0m : 2.63889
[1mStep[0m  [24/42], [94mLoss[0m : 2.53619
[1mStep[0m  [28/42], [94mLoss[0m : 2.72252
[1mStep[0m  [32/42], [94mLoss[0m : 2.74531
[1mStep[0m  [36/42], [94mLoss[0m : 2.44231
[1mStep[0m  [40/42], [94mLoss[0m : 2.49756

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.550, [92mTest[0m: 2.479, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.65268
[1mStep[0m  [4/42], [94mLoss[0m : 2.58051
[1mStep[0m  [8/42], [94mLoss[0m : 2.18769
[1mStep[0m  [12/42], [94mLoss[0m : 2.45316
[1mStep[0m  [16/42], [94mLoss[0m : 2.73878
[1mStep[0m  [20/42], [94mLoss[0m : 2.57605
[1mStep[0m  [24/42], [94mLoss[0m : 2.35662
[1mStep[0m  [28/42], [94mLoss[0m : 2.72553
[1mStep[0m  [32/42], [94mLoss[0m : 2.62743
[1mStep[0m  [36/42], [94mLoss[0m : 2.43051
[1mStep[0m  [40/42], [94mLoss[0m : 2.50098

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.510, [92mTest[0m: 2.424, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34311
[1mStep[0m  [4/42], [94mLoss[0m : 2.37518
[1mStep[0m  [8/42], [94mLoss[0m : 2.56398
[1mStep[0m  [12/42], [94mLoss[0m : 2.46873
[1mStep[0m  [16/42], [94mLoss[0m : 2.55811
[1mStep[0m  [20/42], [94mLoss[0m : 2.44214
[1mStep[0m  [24/42], [94mLoss[0m : 2.48167
[1mStep[0m  [28/42], [94mLoss[0m : 2.34795
[1mStep[0m  [32/42], [94mLoss[0m : 2.63917
[1mStep[0m  [36/42], [94mLoss[0m : 2.37984
[1mStep[0m  [40/42], [94mLoss[0m : 2.43442

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.410, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49892
[1mStep[0m  [4/42], [94mLoss[0m : 2.46130
[1mStep[0m  [8/42], [94mLoss[0m : 2.48699
[1mStep[0m  [12/42], [94mLoss[0m : 2.38972
[1mStep[0m  [16/42], [94mLoss[0m : 2.22375
[1mStep[0m  [20/42], [94mLoss[0m : 2.34349
[1mStep[0m  [24/42], [94mLoss[0m : 2.58577
[1mStep[0m  [28/42], [94mLoss[0m : 2.56269
[1mStep[0m  [32/42], [94mLoss[0m : 2.66738
[1mStep[0m  [36/42], [94mLoss[0m : 2.35125
[1mStep[0m  [40/42], [94mLoss[0m : 2.57102

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.390, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52149
[1mStep[0m  [4/42], [94mLoss[0m : 2.70436
[1mStep[0m  [8/42], [94mLoss[0m : 2.55617
[1mStep[0m  [12/42], [94mLoss[0m : 2.47721
[1mStep[0m  [16/42], [94mLoss[0m : 2.53698
[1mStep[0m  [20/42], [94mLoss[0m : 2.56296
[1mStep[0m  [24/42], [94mLoss[0m : 2.53516
[1mStep[0m  [28/42], [94mLoss[0m : 2.53539
[1mStep[0m  [32/42], [94mLoss[0m : 2.43131
[1mStep[0m  [36/42], [94mLoss[0m : 2.35820
[1mStep[0m  [40/42], [94mLoss[0m : 2.47597

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.382, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49655
[1mStep[0m  [4/42], [94mLoss[0m : 2.48327
[1mStep[0m  [8/42], [94mLoss[0m : 2.51845
[1mStep[0m  [12/42], [94mLoss[0m : 2.47960
[1mStep[0m  [16/42], [94mLoss[0m : 2.52485
[1mStep[0m  [20/42], [94mLoss[0m : 2.27042
[1mStep[0m  [24/42], [94mLoss[0m : 2.50592
[1mStep[0m  [28/42], [94mLoss[0m : 2.73340
[1mStep[0m  [32/42], [94mLoss[0m : 2.52146
[1mStep[0m  [36/42], [94mLoss[0m : 2.42012
[1mStep[0m  [40/42], [94mLoss[0m : 2.42236

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.380, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56188
[1mStep[0m  [4/42], [94mLoss[0m : 2.26656
[1mStep[0m  [8/42], [94mLoss[0m : 2.14447
[1mStep[0m  [12/42], [94mLoss[0m : 2.70071
[1mStep[0m  [16/42], [94mLoss[0m : 2.47182
[1mStep[0m  [20/42], [94mLoss[0m : 2.60381
[1mStep[0m  [24/42], [94mLoss[0m : 2.76000
[1mStep[0m  [28/42], [94mLoss[0m : 2.45385
[1mStep[0m  [32/42], [94mLoss[0m : 2.47609
[1mStep[0m  [36/42], [94mLoss[0m : 2.28752
[1mStep[0m  [40/42], [94mLoss[0m : 2.46156

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.375, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52572
[1mStep[0m  [4/42], [94mLoss[0m : 2.36574
[1mStep[0m  [8/42], [94mLoss[0m : 2.39528
[1mStep[0m  [12/42], [94mLoss[0m : 2.45240
[1mStep[0m  [16/42], [94mLoss[0m : 2.27250
[1mStep[0m  [20/42], [94mLoss[0m : 2.44714
[1mStep[0m  [24/42], [94mLoss[0m : 2.37606
[1mStep[0m  [28/42], [94mLoss[0m : 2.74654
[1mStep[0m  [32/42], [94mLoss[0m : 2.49471
[1mStep[0m  [36/42], [94mLoss[0m : 2.49458
[1mStep[0m  [40/42], [94mLoss[0m : 2.49399

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.358, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64404
[1mStep[0m  [4/42], [94mLoss[0m : 2.43810
[1mStep[0m  [8/42], [94mLoss[0m : 2.55956
[1mStep[0m  [12/42], [94mLoss[0m : 2.43230
[1mStep[0m  [16/42], [94mLoss[0m : 2.48825
[1mStep[0m  [20/42], [94mLoss[0m : 2.40616
[1mStep[0m  [24/42], [94mLoss[0m : 2.68595
[1mStep[0m  [28/42], [94mLoss[0m : 2.41283
[1mStep[0m  [32/42], [94mLoss[0m : 2.39672
[1mStep[0m  [36/42], [94mLoss[0m : 2.48891
[1mStep[0m  [40/42], [94mLoss[0m : 2.71792

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.364, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57405
[1mStep[0m  [4/42], [94mLoss[0m : 2.48751
[1mStep[0m  [8/42], [94mLoss[0m : 2.49016
[1mStep[0m  [12/42], [94mLoss[0m : 2.50775
[1mStep[0m  [16/42], [94mLoss[0m : 2.52907
[1mStep[0m  [20/42], [94mLoss[0m : 2.37990
[1mStep[0m  [24/42], [94mLoss[0m : 2.39570
[1mStep[0m  [28/42], [94mLoss[0m : 2.34042
[1mStep[0m  [32/42], [94mLoss[0m : 2.33113
[1mStep[0m  [36/42], [94mLoss[0m : 2.59178
[1mStep[0m  [40/42], [94mLoss[0m : 2.45128

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.363, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49190
[1mStep[0m  [4/42], [94mLoss[0m : 2.76469
[1mStep[0m  [8/42], [94mLoss[0m : 2.57577
[1mStep[0m  [12/42], [94mLoss[0m : 2.46382
[1mStep[0m  [16/42], [94mLoss[0m : 2.49516
[1mStep[0m  [20/42], [94mLoss[0m : 2.44904
[1mStep[0m  [24/42], [94mLoss[0m : 2.35180
[1mStep[0m  [28/42], [94mLoss[0m : 2.81357
[1mStep[0m  [32/42], [94mLoss[0m : 2.41941
[1mStep[0m  [36/42], [94mLoss[0m : 2.41205
[1mStep[0m  [40/42], [94mLoss[0m : 2.33740

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.352, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39371
[1mStep[0m  [4/42], [94mLoss[0m : 2.32328
[1mStep[0m  [8/42], [94mLoss[0m : 2.39731
[1mStep[0m  [12/42], [94mLoss[0m : 2.38788
[1mStep[0m  [16/42], [94mLoss[0m : 2.51826
[1mStep[0m  [20/42], [94mLoss[0m : 2.61285
[1mStep[0m  [24/42], [94mLoss[0m : 2.48487
[1mStep[0m  [28/42], [94mLoss[0m : 2.79365
[1mStep[0m  [32/42], [94mLoss[0m : 2.53897
[1mStep[0m  [36/42], [94mLoss[0m : 2.51448
[1mStep[0m  [40/42], [94mLoss[0m : 2.27911

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.352, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48948
[1mStep[0m  [4/42], [94mLoss[0m : 2.64897
[1mStep[0m  [8/42], [94mLoss[0m : 2.45845
[1mStep[0m  [12/42], [94mLoss[0m : 2.47623
[1mStep[0m  [16/42], [94mLoss[0m : 2.25934
[1mStep[0m  [20/42], [94mLoss[0m : 2.40665
[1mStep[0m  [24/42], [94mLoss[0m : 2.26518
[1mStep[0m  [28/42], [94mLoss[0m : 2.32538
[1mStep[0m  [32/42], [94mLoss[0m : 2.67863
[1mStep[0m  [36/42], [94mLoss[0m : 2.39082
[1mStep[0m  [40/42], [94mLoss[0m : 2.61772

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.344, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28733
[1mStep[0m  [4/42], [94mLoss[0m : 2.57493
[1mStep[0m  [8/42], [94mLoss[0m : 2.40007
[1mStep[0m  [12/42], [94mLoss[0m : 2.37014
[1mStep[0m  [16/42], [94mLoss[0m : 2.45713
[1mStep[0m  [20/42], [94mLoss[0m : 2.31622
[1mStep[0m  [24/42], [94mLoss[0m : 2.38140
[1mStep[0m  [28/42], [94mLoss[0m : 2.57273
[1mStep[0m  [32/42], [94mLoss[0m : 2.58362
[1mStep[0m  [36/42], [94mLoss[0m : 2.58556
[1mStep[0m  [40/42], [94mLoss[0m : 2.37684

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.343, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45042
[1mStep[0m  [4/42], [94mLoss[0m : 2.57280
[1mStep[0m  [8/42], [94mLoss[0m : 2.46848
[1mStep[0m  [12/42], [94mLoss[0m : 2.47906
[1mStep[0m  [16/42], [94mLoss[0m : 2.44461
[1mStep[0m  [20/42], [94mLoss[0m : 2.46704
[1mStep[0m  [24/42], [94mLoss[0m : 2.43138
[1mStep[0m  [28/42], [94mLoss[0m : 2.46987
[1mStep[0m  [32/42], [94mLoss[0m : 2.54350
[1mStep[0m  [36/42], [94mLoss[0m : 2.47368
[1mStep[0m  [40/42], [94mLoss[0m : 2.50744

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.343, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49748
[1mStep[0m  [4/42], [94mLoss[0m : 2.27537
[1mStep[0m  [8/42], [94mLoss[0m : 2.28485
[1mStep[0m  [12/42], [94mLoss[0m : 2.31882
[1mStep[0m  [16/42], [94mLoss[0m : 2.71388
[1mStep[0m  [20/42], [94mLoss[0m : 2.53480
[1mStep[0m  [24/42], [94mLoss[0m : 2.47049
[1mStep[0m  [28/42], [94mLoss[0m : 2.40765
[1mStep[0m  [32/42], [94mLoss[0m : 2.59926
[1mStep[0m  [36/42], [94mLoss[0m : 2.68423
[1mStep[0m  [40/42], [94mLoss[0m : 2.37384

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.349, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43310
[1mStep[0m  [4/42], [94mLoss[0m : 2.46651
[1mStep[0m  [8/42], [94mLoss[0m : 2.38228
[1mStep[0m  [12/42], [94mLoss[0m : 2.56169
[1mStep[0m  [16/42], [94mLoss[0m : 2.24004
[1mStep[0m  [20/42], [94mLoss[0m : 2.42260
[1mStep[0m  [24/42], [94mLoss[0m : 2.49247
[1mStep[0m  [28/42], [94mLoss[0m : 2.74077
[1mStep[0m  [32/42], [94mLoss[0m : 2.52094
[1mStep[0m  [36/42], [94mLoss[0m : 2.39545
[1mStep[0m  [40/42], [94mLoss[0m : 2.30817

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.343, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46533
[1mStep[0m  [4/42], [94mLoss[0m : 2.49905
[1mStep[0m  [8/42], [94mLoss[0m : 2.49078
[1mStep[0m  [12/42], [94mLoss[0m : 2.59689
[1mStep[0m  [16/42], [94mLoss[0m : 2.48907
[1mStep[0m  [20/42], [94mLoss[0m : 2.29670
[1mStep[0m  [24/42], [94mLoss[0m : 2.54292
[1mStep[0m  [28/42], [94mLoss[0m : 2.50337
[1mStep[0m  [32/42], [94mLoss[0m : 2.36089
[1mStep[0m  [36/42], [94mLoss[0m : 2.49174
[1mStep[0m  [40/42], [94mLoss[0m : 2.48110

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.348, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43816
[1mStep[0m  [4/42], [94mLoss[0m : 2.39081
[1mStep[0m  [8/42], [94mLoss[0m : 2.48681
[1mStep[0m  [12/42], [94mLoss[0m : 2.31572
[1mStep[0m  [16/42], [94mLoss[0m : 2.73462
[1mStep[0m  [20/42], [94mLoss[0m : 2.41019
[1mStep[0m  [24/42], [94mLoss[0m : 2.42284
[1mStep[0m  [28/42], [94mLoss[0m : 2.35020
[1mStep[0m  [32/42], [94mLoss[0m : 2.52733
[1mStep[0m  [36/42], [94mLoss[0m : 2.61638
[1mStep[0m  [40/42], [94mLoss[0m : 2.42760

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.338, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59561
[1mStep[0m  [4/42], [94mLoss[0m : 2.36643
[1mStep[0m  [8/42], [94mLoss[0m : 2.41028
[1mStep[0m  [12/42], [94mLoss[0m : 2.31289
[1mStep[0m  [16/42], [94mLoss[0m : 2.26234
[1mStep[0m  [20/42], [94mLoss[0m : 2.19957
[1mStep[0m  [24/42], [94mLoss[0m : 2.35119
[1mStep[0m  [28/42], [94mLoss[0m : 2.49473
[1mStep[0m  [32/42], [94mLoss[0m : 2.62380
[1mStep[0m  [36/42], [94mLoss[0m : 2.45280
[1mStep[0m  [40/42], [94mLoss[0m : 2.33466

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.334, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47844
[1mStep[0m  [4/42], [94mLoss[0m : 2.51739
[1mStep[0m  [8/42], [94mLoss[0m : 2.27825
[1mStep[0m  [12/42], [94mLoss[0m : 2.57853
[1mStep[0m  [16/42], [94mLoss[0m : 2.55877
[1mStep[0m  [20/42], [94mLoss[0m : 2.55579
[1mStep[0m  [24/42], [94mLoss[0m : 2.28606
[1mStep[0m  [28/42], [94mLoss[0m : 2.62361
[1mStep[0m  [32/42], [94mLoss[0m : 2.44303
[1mStep[0m  [36/42], [94mLoss[0m : 2.45285
[1mStep[0m  [40/42], [94mLoss[0m : 2.39756

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.336, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38240
[1mStep[0m  [4/42], [94mLoss[0m : 2.51317
[1mStep[0m  [8/42], [94mLoss[0m : 2.33294
[1mStep[0m  [12/42], [94mLoss[0m : 2.15656
[1mStep[0m  [16/42], [94mLoss[0m : 2.49249
[1mStep[0m  [20/42], [94mLoss[0m : 2.41265
[1mStep[0m  [24/42], [94mLoss[0m : 2.38783
[1mStep[0m  [28/42], [94mLoss[0m : 2.40433
[1mStep[0m  [32/42], [94mLoss[0m : 2.56322
[1mStep[0m  [36/42], [94mLoss[0m : 2.30779
[1mStep[0m  [40/42], [94mLoss[0m : 2.35430

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.337, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62347
[1mStep[0m  [4/42], [94mLoss[0m : 2.28784
[1mStep[0m  [8/42], [94mLoss[0m : 2.47180
[1mStep[0m  [12/42], [94mLoss[0m : 2.55970
[1mStep[0m  [16/42], [94mLoss[0m : 2.39804
[1mStep[0m  [20/42], [94mLoss[0m : 2.56868
[1mStep[0m  [24/42], [94mLoss[0m : 2.21986
[1mStep[0m  [28/42], [94mLoss[0m : 2.49473
[1mStep[0m  [32/42], [94mLoss[0m : 2.40287
[1mStep[0m  [36/42], [94mLoss[0m : 2.33435
[1mStep[0m  [40/42], [94mLoss[0m : 2.62465

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.329, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49502
[1mStep[0m  [4/42], [94mLoss[0m : 2.63144
[1mStep[0m  [8/42], [94mLoss[0m : 2.58880
[1mStep[0m  [12/42], [94mLoss[0m : 2.57043
[1mStep[0m  [16/42], [94mLoss[0m : 2.54261
[1mStep[0m  [20/42], [94mLoss[0m : 2.44169
[1mStep[0m  [24/42], [94mLoss[0m : 2.45629
[1mStep[0m  [28/42], [94mLoss[0m : 2.45294
[1mStep[0m  [32/42], [94mLoss[0m : 2.38311
[1mStep[0m  [36/42], [94mLoss[0m : 2.33131
[1mStep[0m  [40/42], [94mLoss[0m : 2.64234

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.337, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58132
[1mStep[0m  [4/42], [94mLoss[0m : 2.41780
[1mStep[0m  [8/42], [94mLoss[0m : 2.34603
[1mStep[0m  [12/42], [94mLoss[0m : 2.54872
[1mStep[0m  [16/42], [94mLoss[0m : 2.57910
[1mStep[0m  [20/42], [94mLoss[0m : 2.29346
[1mStep[0m  [24/42], [94mLoss[0m : 2.36126
[1mStep[0m  [28/42], [94mLoss[0m : 2.34290
[1mStep[0m  [32/42], [94mLoss[0m : 2.44660
[1mStep[0m  [36/42], [94mLoss[0m : 2.40136
[1mStep[0m  [40/42], [94mLoss[0m : 2.42728

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.332, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59711
[1mStep[0m  [4/42], [94mLoss[0m : 2.47551
[1mStep[0m  [8/42], [94mLoss[0m : 2.39145
[1mStep[0m  [12/42], [94mLoss[0m : 2.49120
[1mStep[0m  [16/42], [94mLoss[0m : 2.56852
[1mStep[0m  [20/42], [94mLoss[0m : 2.28764
[1mStep[0m  [24/42], [94mLoss[0m : 2.40107
[1mStep[0m  [28/42], [94mLoss[0m : 2.32347
[1mStep[0m  [32/42], [94mLoss[0m : 2.64258
[1mStep[0m  [36/42], [94mLoss[0m : 2.45630
[1mStep[0m  [40/42], [94mLoss[0m : 2.42539

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.327, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41272
[1mStep[0m  [4/42], [94mLoss[0m : 2.37412
[1mStep[0m  [8/42], [94mLoss[0m : 2.64944
[1mStep[0m  [12/42], [94mLoss[0m : 2.04394
[1mStep[0m  [16/42], [94mLoss[0m : 2.48449
[1mStep[0m  [20/42], [94mLoss[0m : 2.20950
[1mStep[0m  [24/42], [94mLoss[0m : 2.59187
[1mStep[0m  [28/42], [94mLoss[0m : 2.45182
[1mStep[0m  [32/42], [94mLoss[0m : 2.28177
[1mStep[0m  [36/42], [94mLoss[0m : 2.43802
[1mStep[0m  [40/42], [94mLoss[0m : 2.45098

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.337, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46399
[1mStep[0m  [4/42], [94mLoss[0m : 2.35537
[1mStep[0m  [8/42], [94mLoss[0m : 2.44727
[1mStep[0m  [12/42], [94mLoss[0m : 2.55694
[1mStep[0m  [16/42], [94mLoss[0m : 2.33927
[1mStep[0m  [20/42], [94mLoss[0m : 2.36876
[1mStep[0m  [24/42], [94mLoss[0m : 2.54510
[1mStep[0m  [28/42], [94mLoss[0m : 2.46394
[1mStep[0m  [32/42], [94mLoss[0m : 2.44984
[1mStep[0m  [36/42], [94mLoss[0m : 2.35672
[1mStep[0m  [40/42], [94mLoss[0m : 2.41286

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.327, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.332
====================================

Phase 1 - Evaluation MAE:  2.3322817598070418
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 2.36917
[1mStep[0m  [4/42], [94mLoss[0m : 2.37044
[1mStep[0m  [8/42], [94mLoss[0m : 2.47367
[1mStep[0m  [12/42], [94mLoss[0m : 2.40671
[1mStep[0m  [16/42], [94mLoss[0m : 2.46376
[1mStep[0m  [20/42], [94mLoss[0m : 2.61134
[1mStep[0m  [24/42], [94mLoss[0m : 2.50532
[1mStep[0m  [28/42], [94mLoss[0m : 2.39678
[1mStep[0m  [32/42], [94mLoss[0m : 2.37501
[1mStep[0m  [36/42], [94mLoss[0m : 2.47546
[1mStep[0m  [40/42], [94mLoss[0m : 2.45905

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.335, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42435
[1mStep[0m  [4/42], [94mLoss[0m : 2.62406
[1mStep[0m  [8/42], [94mLoss[0m : 2.50529
[1mStep[0m  [12/42], [94mLoss[0m : 2.53393
[1mStep[0m  [16/42], [94mLoss[0m : 2.43056
[1mStep[0m  [20/42], [94mLoss[0m : 2.51915
[1mStep[0m  [24/42], [94mLoss[0m : 2.63990
[1mStep[0m  [28/42], [94mLoss[0m : 2.79384
[1mStep[0m  [32/42], [94mLoss[0m : 2.24726
[1mStep[0m  [36/42], [94mLoss[0m : 2.42827
[1mStep[0m  [40/42], [94mLoss[0m : 2.56254

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.330, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.20724
[1mStep[0m  [4/42], [94mLoss[0m : 2.33060
[1mStep[0m  [8/42], [94mLoss[0m : 2.49413
[1mStep[0m  [12/42], [94mLoss[0m : 2.34474
[1mStep[0m  [16/42], [94mLoss[0m : 2.39166
[1mStep[0m  [20/42], [94mLoss[0m : 2.19838
[1mStep[0m  [24/42], [94mLoss[0m : 2.16252
[1mStep[0m  [28/42], [94mLoss[0m : 2.25996
[1mStep[0m  [32/42], [94mLoss[0m : 2.53118
[1mStep[0m  [36/42], [94mLoss[0m : 2.48212
[1mStep[0m  [40/42], [94mLoss[0m : 2.38592

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.441, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33891
[1mStep[0m  [4/42], [94mLoss[0m : 2.39296
[1mStep[0m  [8/42], [94mLoss[0m : 2.19074
[1mStep[0m  [12/42], [94mLoss[0m : 2.32269
[1mStep[0m  [16/42], [94mLoss[0m : 2.44699
[1mStep[0m  [20/42], [94mLoss[0m : 2.27502
[1mStep[0m  [24/42], [94mLoss[0m : 2.27589
[1mStep[0m  [28/42], [94mLoss[0m : 2.27279
[1mStep[0m  [32/42], [94mLoss[0m : 2.49548
[1mStep[0m  [36/42], [94mLoss[0m : 2.40352
[1mStep[0m  [40/42], [94mLoss[0m : 2.40577

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.395, [92mTest[0m: 2.488, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40235
[1mStep[0m  [4/42], [94mLoss[0m : 2.13498
[1mStep[0m  [8/42], [94mLoss[0m : 2.43377
[1mStep[0m  [12/42], [94mLoss[0m : 2.35024
[1mStep[0m  [16/42], [94mLoss[0m : 2.55955
[1mStep[0m  [20/42], [94mLoss[0m : 2.31495
[1mStep[0m  [24/42], [94mLoss[0m : 2.54533
[1mStep[0m  [28/42], [94mLoss[0m : 2.22070
[1mStep[0m  [32/42], [94mLoss[0m : 2.43037
[1mStep[0m  [36/42], [94mLoss[0m : 2.18476
[1mStep[0m  [40/42], [94mLoss[0m : 2.35039

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.379, [92mTest[0m: 2.525, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24587
[1mStep[0m  [4/42], [94mLoss[0m : 2.32347
[1mStep[0m  [8/42], [94mLoss[0m : 2.39351
[1mStep[0m  [12/42], [94mLoss[0m : 2.39572
[1mStep[0m  [16/42], [94mLoss[0m : 2.51506
[1mStep[0m  [20/42], [94mLoss[0m : 2.27738
[1mStep[0m  [24/42], [94mLoss[0m : 2.53865
[1mStep[0m  [28/42], [94mLoss[0m : 2.39789
[1mStep[0m  [32/42], [94mLoss[0m : 2.14619
[1mStep[0m  [36/42], [94mLoss[0m : 2.35747
[1mStep[0m  [40/42], [94mLoss[0m : 2.31616

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.357, [92mTest[0m: 2.517, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42170
[1mStep[0m  [4/42], [94mLoss[0m : 2.13227
[1mStep[0m  [8/42], [94mLoss[0m : 2.36855
[1mStep[0m  [12/42], [94mLoss[0m : 2.36684
[1mStep[0m  [16/42], [94mLoss[0m : 2.36544
[1mStep[0m  [20/42], [94mLoss[0m : 2.48542
[1mStep[0m  [24/42], [94mLoss[0m : 2.52868
[1mStep[0m  [28/42], [94mLoss[0m : 2.42686
[1mStep[0m  [32/42], [94mLoss[0m : 2.11172
[1mStep[0m  [36/42], [94mLoss[0m : 2.57931
[1mStep[0m  [40/42], [94mLoss[0m : 2.18524

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.327, [92mTest[0m: 2.557, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34022
[1mStep[0m  [4/42], [94mLoss[0m : 2.27292
[1mStep[0m  [8/42], [94mLoss[0m : 2.38174
[1mStep[0m  [12/42], [94mLoss[0m : 2.47526
[1mStep[0m  [16/42], [94mLoss[0m : 2.19550
[1mStep[0m  [20/42], [94mLoss[0m : 2.38059
[1mStep[0m  [24/42], [94mLoss[0m : 2.32648
[1mStep[0m  [28/42], [94mLoss[0m : 2.44097
[1mStep[0m  [32/42], [94mLoss[0m : 2.31395
[1mStep[0m  [36/42], [94mLoss[0m : 2.31438
[1mStep[0m  [40/42], [94mLoss[0m : 2.15997

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.309, [92mTest[0m: 2.572, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.11548
[1mStep[0m  [4/42], [94mLoss[0m : 2.37044
[1mStep[0m  [8/42], [94mLoss[0m : 2.48690
[1mStep[0m  [12/42], [94mLoss[0m : 2.09948
[1mStep[0m  [16/42], [94mLoss[0m : 2.19354
[1mStep[0m  [20/42], [94mLoss[0m : 2.31911
[1mStep[0m  [24/42], [94mLoss[0m : 2.25008
[1mStep[0m  [28/42], [94mLoss[0m : 2.26003
[1mStep[0m  [32/42], [94mLoss[0m : 2.25211
[1mStep[0m  [36/42], [94mLoss[0m : 2.21852
[1mStep[0m  [40/42], [94mLoss[0m : 2.36562

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.282, [92mTest[0m: 2.590, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.14981
[1mStep[0m  [4/42], [94mLoss[0m : 2.20386
[1mStep[0m  [8/42], [94mLoss[0m : 2.20694
[1mStep[0m  [12/42], [94mLoss[0m : 2.08911
[1mStep[0m  [16/42], [94mLoss[0m : 2.26471
[1mStep[0m  [20/42], [94mLoss[0m : 2.36405
[1mStep[0m  [24/42], [94mLoss[0m : 2.29345
[1mStep[0m  [28/42], [94mLoss[0m : 2.28033
[1mStep[0m  [32/42], [94mLoss[0m : 2.18734
[1mStep[0m  [36/42], [94mLoss[0m : 2.40339
[1mStep[0m  [40/42], [94mLoss[0m : 2.27568

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.252, [92mTest[0m: 2.544, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.13706
[1mStep[0m  [4/42], [94mLoss[0m : 2.28789
[1mStep[0m  [8/42], [94mLoss[0m : 2.43272
[1mStep[0m  [12/42], [94mLoss[0m : 2.32184
[1mStep[0m  [16/42], [94mLoss[0m : 2.38330
[1mStep[0m  [20/42], [94mLoss[0m : 2.15345
[1mStep[0m  [24/42], [94mLoss[0m : 2.02617
[1mStep[0m  [28/42], [94mLoss[0m : 2.16232
[1mStep[0m  [32/42], [94mLoss[0m : 2.24228
[1mStep[0m  [36/42], [94mLoss[0m : 2.19755
[1mStep[0m  [40/42], [94mLoss[0m : 2.27957

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.236, [92mTest[0m: 2.526, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29297
[1mStep[0m  [4/42], [94mLoss[0m : 2.33221
[1mStep[0m  [8/42], [94mLoss[0m : 2.22891
[1mStep[0m  [12/42], [94mLoss[0m : 2.12610
[1mStep[0m  [16/42], [94mLoss[0m : 2.32563
[1mStep[0m  [20/42], [94mLoss[0m : 2.29677
[1mStep[0m  [24/42], [94mLoss[0m : 2.11673
[1mStep[0m  [28/42], [94mLoss[0m : 2.19461
[1mStep[0m  [32/42], [94mLoss[0m : 2.30406
[1mStep[0m  [36/42], [94mLoss[0m : 2.46987
[1mStep[0m  [40/42], [94mLoss[0m : 2.08284

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.208, [92mTest[0m: 2.548, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.23564
[1mStep[0m  [4/42], [94mLoss[0m : 2.27815
[1mStep[0m  [8/42], [94mLoss[0m : 2.22662
[1mStep[0m  [12/42], [94mLoss[0m : 2.26254
[1mStep[0m  [16/42], [94mLoss[0m : 2.48916
[1mStep[0m  [20/42], [94mLoss[0m : 2.27488
[1mStep[0m  [24/42], [94mLoss[0m : 2.36303
[1mStep[0m  [28/42], [94mLoss[0m : 2.24694
[1mStep[0m  [32/42], [94mLoss[0m : 2.17935
[1mStep[0m  [36/42], [94mLoss[0m : 1.97404
[1mStep[0m  [40/42], [94mLoss[0m : 2.27350

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.175, [92mTest[0m: 2.542, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.20832
[1mStep[0m  [4/42], [94mLoss[0m : 2.19171
[1mStep[0m  [8/42], [94mLoss[0m : 2.21213
[1mStep[0m  [12/42], [94mLoss[0m : 2.22235
[1mStep[0m  [16/42], [94mLoss[0m : 2.29354
[1mStep[0m  [20/42], [94mLoss[0m : 2.09631
[1mStep[0m  [24/42], [94mLoss[0m : 2.19412
[1mStep[0m  [28/42], [94mLoss[0m : 2.11642
[1mStep[0m  [32/42], [94mLoss[0m : 2.02192
[1mStep[0m  [36/42], [94mLoss[0m : 2.03797
[1mStep[0m  [40/42], [94mLoss[0m : 2.21835

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.138, [92mTest[0m: 2.549, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.92189
[1mStep[0m  [4/42], [94mLoss[0m : 1.92675
[1mStep[0m  [8/42], [94mLoss[0m : 2.00902
[1mStep[0m  [12/42], [94mLoss[0m : 2.01940
[1mStep[0m  [16/42], [94mLoss[0m : 2.21229
[1mStep[0m  [20/42], [94mLoss[0m : 2.08881
[1mStep[0m  [24/42], [94mLoss[0m : 2.08572
[1mStep[0m  [28/42], [94mLoss[0m : 1.98372
[1mStep[0m  [32/42], [94mLoss[0m : 2.10054
[1mStep[0m  [36/42], [94mLoss[0m : 2.04865
[1mStep[0m  [40/42], [94mLoss[0m : 2.10866

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.108, [92mTest[0m: 2.563, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.97575
[1mStep[0m  [4/42], [94mLoss[0m : 2.25590
[1mStep[0m  [8/42], [94mLoss[0m : 2.04538
[1mStep[0m  [12/42], [94mLoss[0m : 1.95151
[1mStep[0m  [16/42], [94mLoss[0m : 2.07749
[1mStep[0m  [20/42], [94mLoss[0m : 2.20480
[1mStep[0m  [24/42], [94mLoss[0m : 2.05254
[1mStep[0m  [28/42], [94mLoss[0m : 2.07573
[1mStep[0m  [32/42], [94mLoss[0m : 1.94659
[1mStep[0m  [36/42], [94mLoss[0m : 2.18471
[1mStep[0m  [40/42], [94mLoss[0m : 2.11908

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.084, [92mTest[0m: 2.592, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.94228
[1mStep[0m  [4/42], [94mLoss[0m : 2.01008
[1mStep[0m  [8/42], [94mLoss[0m : 2.11045
[1mStep[0m  [12/42], [94mLoss[0m : 2.15019
[1mStep[0m  [16/42], [94mLoss[0m : 2.07239
[1mStep[0m  [20/42], [94mLoss[0m : 1.99136
[1mStep[0m  [24/42], [94mLoss[0m : 2.07096
[1mStep[0m  [28/42], [94mLoss[0m : 2.07690
[1mStep[0m  [32/42], [94mLoss[0m : 1.95971
[1mStep[0m  [36/42], [94mLoss[0m : 2.09078
[1mStep[0m  [40/42], [94mLoss[0m : 2.19754

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.037, [92mTest[0m: 2.537, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.02284
[1mStep[0m  [4/42], [94mLoss[0m : 2.04512
[1mStep[0m  [8/42], [94mLoss[0m : 2.08122
[1mStep[0m  [12/42], [94mLoss[0m : 1.96556
[1mStep[0m  [16/42], [94mLoss[0m : 1.81652
[1mStep[0m  [20/42], [94mLoss[0m : 2.11473
[1mStep[0m  [24/42], [94mLoss[0m : 2.06353
[1mStep[0m  [28/42], [94mLoss[0m : 1.86081
[1mStep[0m  [32/42], [94mLoss[0m : 2.25719
[1mStep[0m  [36/42], [94mLoss[0m : 2.08569
[1mStep[0m  [40/42], [94mLoss[0m : 2.16846

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.004, [92mTest[0m: 2.521, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.01675
[1mStep[0m  [4/42], [94mLoss[0m : 1.86962
[1mStep[0m  [8/42], [94mLoss[0m : 1.97459
[1mStep[0m  [12/42], [94mLoss[0m : 2.01516
[1mStep[0m  [16/42], [94mLoss[0m : 1.88939
[1mStep[0m  [20/42], [94mLoss[0m : 1.95592
[1mStep[0m  [24/42], [94mLoss[0m : 1.67064
[1mStep[0m  [28/42], [94mLoss[0m : 1.82843
[1mStep[0m  [32/42], [94mLoss[0m : 2.13006
[1mStep[0m  [36/42], [94mLoss[0m : 1.96907
[1mStep[0m  [40/42], [94mLoss[0m : 2.04860

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.957, [92mTest[0m: 2.628, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.85587
[1mStep[0m  [4/42], [94mLoss[0m : 1.88010
[1mStep[0m  [8/42], [94mLoss[0m : 2.06493
[1mStep[0m  [12/42], [94mLoss[0m : 2.00649
[1mStep[0m  [16/42], [94mLoss[0m : 1.88291
[1mStep[0m  [20/42], [94mLoss[0m : 1.95648
[1mStep[0m  [24/42], [94mLoss[0m : 1.91299
[1mStep[0m  [28/42], [94mLoss[0m : 1.80052
[1mStep[0m  [32/42], [94mLoss[0m : 2.00560
[1mStep[0m  [36/42], [94mLoss[0m : 1.91316
[1mStep[0m  [40/42], [94mLoss[0m : 1.99306

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.920, [92mTest[0m: 2.541, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.82225
[1mStep[0m  [4/42], [94mLoss[0m : 1.69264
[1mStep[0m  [8/42], [94mLoss[0m : 1.83547
[1mStep[0m  [12/42], [94mLoss[0m : 2.25824
[1mStep[0m  [16/42], [94mLoss[0m : 1.76679
[1mStep[0m  [20/42], [94mLoss[0m : 1.86195
[1mStep[0m  [24/42], [94mLoss[0m : 1.98755
[1mStep[0m  [28/42], [94mLoss[0m : 1.82065
[1mStep[0m  [32/42], [94mLoss[0m : 1.94398
[1mStep[0m  [36/42], [94mLoss[0m : 1.83573
[1mStep[0m  [40/42], [94mLoss[0m : 1.76092

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.892, [92mTest[0m: 2.758, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.85990
[1mStep[0m  [4/42], [94mLoss[0m : 1.86732
[1mStep[0m  [8/42], [94mLoss[0m : 1.85498
[1mStep[0m  [12/42], [94mLoss[0m : 1.72039
[1mStep[0m  [16/42], [94mLoss[0m : 1.79111
[1mStep[0m  [20/42], [94mLoss[0m : 1.86619
[1mStep[0m  [24/42], [94mLoss[0m : 1.82863
[1mStep[0m  [28/42], [94mLoss[0m : 1.69575
[1mStep[0m  [32/42], [94mLoss[0m : 2.05465
[1mStep[0m  [36/42], [94mLoss[0m : 2.00909
[1mStep[0m  [40/42], [94mLoss[0m : 1.81324

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.882, [92mTest[0m: 2.504, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.87575
[1mStep[0m  [4/42], [94mLoss[0m : 2.02012
[1mStep[0m  [8/42], [94mLoss[0m : 1.83508
[1mStep[0m  [12/42], [94mLoss[0m : 1.96524
[1mStep[0m  [16/42], [94mLoss[0m : 1.80954
[1mStep[0m  [20/42], [94mLoss[0m : 1.86164
[1mStep[0m  [24/42], [94mLoss[0m : 1.85029
[1mStep[0m  [28/42], [94mLoss[0m : 2.01945
[1mStep[0m  [32/42], [94mLoss[0m : 1.81231
[1mStep[0m  [36/42], [94mLoss[0m : 1.84976
[1mStep[0m  [40/42], [94mLoss[0m : 1.77724

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.846, [92mTest[0m: 2.486, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.73465
[1mStep[0m  [4/42], [94mLoss[0m : 1.78609
[1mStep[0m  [8/42], [94mLoss[0m : 1.68401
[1mStep[0m  [12/42], [94mLoss[0m : 1.83329
[1mStep[0m  [16/42], [94mLoss[0m : 1.74315
[1mStep[0m  [20/42], [94mLoss[0m : 1.81399
[1mStep[0m  [24/42], [94mLoss[0m : 1.74749
[1mStep[0m  [28/42], [94mLoss[0m : 1.93782
[1mStep[0m  [32/42], [94mLoss[0m : 1.74611
[1mStep[0m  [36/42], [94mLoss[0m : 1.87103
[1mStep[0m  [40/42], [94mLoss[0m : 1.86075

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.806, [92mTest[0m: 2.543, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.85344
[1mStep[0m  [4/42], [94mLoss[0m : 1.69399
[1mStep[0m  [8/42], [94mLoss[0m : 1.82670
[1mStep[0m  [12/42], [94mLoss[0m : 1.70114
[1mStep[0m  [16/42], [94mLoss[0m : 1.80966
[1mStep[0m  [20/42], [94mLoss[0m : 1.80343
[1mStep[0m  [24/42], [94mLoss[0m : 1.75122
[1mStep[0m  [28/42], [94mLoss[0m : 2.04112
[1mStep[0m  [32/42], [94mLoss[0m : 1.80142
[1mStep[0m  [36/42], [94mLoss[0m : 1.86525
[1mStep[0m  [40/42], [94mLoss[0m : 1.81789

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.790, [92mTest[0m: 2.518, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.80291
[1mStep[0m  [4/42], [94mLoss[0m : 1.53387
[1mStep[0m  [8/42], [94mLoss[0m : 1.91377
[1mStep[0m  [12/42], [94mLoss[0m : 1.74103
[1mStep[0m  [16/42], [94mLoss[0m : 1.63770
[1mStep[0m  [20/42], [94mLoss[0m : 1.93024
[1mStep[0m  [24/42], [94mLoss[0m : 2.01521
[1mStep[0m  [28/42], [94mLoss[0m : 1.78467
[1mStep[0m  [32/42], [94mLoss[0m : 1.83596
[1mStep[0m  [36/42], [94mLoss[0m : 1.67882
[1mStep[0m  [40/42], [94mLoss[0m : 1.84923

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.760, [92mTest[0m: 2.463, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.73090
[1mStep[0m  [4/42], [94mLoss[0m : 1.62540
[1mStep[0m  [8/42], [94mLoss[0m : 1.58629
[1mStep[0m  [12/42], [94mLoss[0m : 1.62833
[1mStep[0m  [16/42], [94mLoss[0m : 1.69090
[1mStep[0m  [20/42], [94mLoss[0m : 1.87420
[1mStep[0m  [24/42], [94mLoss[0m : 1.82659
[1mStep[0m  [28/42], [94mLoss[0m : 1.68766
[1mStep[0m  [32/42], [94mLoss[0m : 1.73380
[1mStep[0m  [36/42], [94mLoss[0m : 1.89879
[1mStep[0m  [40/42], [94mLoss[0m : 1.81911

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.738, [92mTest[0m: 2.526, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.95374
[1mStep[0m  [4/42], [94mLoss[0m : 1.53860
[1mStep[0m  [8/42], [94mLoss[0m : 1.75716
[1mStep[0m  [12/42], [94mLoss[0m : 1.60206
[1mStep[0m  [16/42], [94mLoss[0m : 1.66884
[1mStep[0m  [20/42], [94mLoss[0m : 1.60536
[1mStep[0m  [24/42], [94mLoss[0m : 1.73900
[1mStep[0m  [28/42], [94mLoss[0m : 1.57481
[1mStep[0m  [32/42], [94mLoss[0m : 1.68326
[1mStep[0m  [36/42], [94mLoss[0m : 1.84532
[1mStep[0m  [40/42], [94mLoss[0m : 1.66805

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.718, [92mTest[0m: 2.485, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.68555
[1mStep[0m  [4/42], [94mLoss[0m : 1.59233
[1mStep[0m  [8/42], [94mLoss[0m : 1.88891
[1mStep[0m  [12/42], [94mLoss[0m : 1.63047
[1mStep[0m  [16/42], [94mLoss[0m : 1.74375
[1mStep[0m  [20/42], [94mLoss[0m : 1.66823
[1mStep[0m  [24/42], [94mLoss[0m : 1.75377
[1mStep[0m  [28/42], [94mLoss[0m : 1.75741
[1mStep[0m  [32/42], [94mLoss[0m : 1.70064
[1mStep[0m  [36/42], [94mLoss[0m : 1.59139
[1mStep[0m  [40/42], [94mLoss[0m : 1.68193

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.689, [92mTest[0m: 2.555, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.74344
[1mStep[0m  [4/42], [94mLoss[0m : 1.71441
[1mStep[0m  [8/42], [94mLoss[0m : 1.66339
[1mStep[0m  [12/42], [94mLoss[0m : 1.67329
[1mStep[0m  [16/42], [94mLoss[0m : 1.62598
[1mStep[0m  [20/42], [94mLoss[0m : 1.38743
[1mStep[0m  [24/42], [94mLoss[0m : 1.74993
[1mStep[0m  [28/42], [94mLoss[0m : 1.70546
[1mStep[0m  [32/42], [94mLoss[0m : 1.66841
[1mStep[0m  [36/42], [94mLoss[0m : 1.89138
[1mStep[0m  [40/42], [94mLoss[0m : 1.76005

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.673, [92mTest[0m: 2.660, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.561
====================================

Phase 2 - Evaluation MAE:  2.561463338988168
MAE score P1        2.332282
MAE score P2        2.561463
loss                1.673365
learning_rate       0.007525
batch_size               256
hidden_sizes      [300, 100]
epochs                    30
activation           sigmoid
optimizer                sgd
early stopping         False
dropout                  0.2
momentum                 0.1
weight_decay           0.001
Name: 3, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 11.10500
[1mStep[0m  [4/42], [94mLoss[0m : 10.92534
[1mStep[0m  [8/42], [94mLoss[0m : 10.63715
[1mStep[0m  [12/42], [94mLoss[0m : 10.41090
[1mStep[0m  [16/42], [94mLoss[0m : 10.29745
[1mStep[0m  [20/42], [94mLoss[0m : 10.78114
[1mStep[0m  [24/42], [94mLoss[0m : 10.39938
[1mStep[0m  [28/42], [94mLoss[0m : 10.65548
[1mStep[0m  [32/42], [94mLoss[0m : 10.54286
[1mStep[0m  [36/42], [94mLoss[0m : 10.22235
[1mStep[0m  [40/42], [94mLoss[0m : 10.50344

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.604, [92mTest[0m: 10.844, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.19988
[1mStep[0m  [4/42], [94mLoss[0m : 10.08173
[1mStep[0m  [8/42], [94mLoss[0m : 9.94748
[1mStep[0m  [12/42], [94mLoss[0m : 10.42758
[1mStep[0m  [16/42], [94mLoss[0m : 10.21732
[1mStep[0m  [20/42], [94mLoss[0m : 9.54481
[1mStep[0m  [24/42], [94mLoss[0m : 9.85145
[1mStep[0m  [28/42], [94mLoss[0m : 9.94477
[1mStep[0m  [32/42], [94mLoss[0m : 9.45605
[1mStep[0m  [36/42], [94mLoss[0m : 9.77677
[1mStep[0m  [40/42], [94mLoss[0m : 9.74480

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.970, [92mTest[0m: 10.139, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.57649
[1mStep[0m  [4/42], [94mLoss[0m : 9.09873
[1mStep[0m  [8/42], [94mLoss[0m : 9.52975
[1mStep[0m  [12/42], [94mLoss[0m : 9.30264
[1mStep[0m  [16/42], [94mLoss[0m : 9.35788
[1mStep[0m  [20/42], [94mLoss[0m : 9.26307
[1mStep[0m  [24/42], [94mLoss[0m : 9.40554
[1mStep[0m  [28/42], [94mLoss[0m : 9.16586
[1mStep[0m  [32/42], [94mLoss[0m : 9.23800
[1mStep[0m  [36/42], [94mLoss[0m : 9.02790
[1mStep[0m  [40/42], [94mLoss[0m : 9.06529

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.224, [92mTest[0m: 9.213, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.85940
[1mStep[0m  [4/42], [94mLoss[0m : 8.47356
[1mStep[0m  [8/42], [94mLoss[0m : 8.62650
[1mStep[0m  [12/42], [94mLoss[0m : 8.69949
[1mStep[0m  [16/42], [94mLoss[0m : 8.91448
[1mStep[0m  [20/42], [94mLoss[0m : 8.31582
[1mStep[0m  [24/42], [94mLoss[0m : 8.28268
[1mStep[0m  [28/42], [94mLoss[0m : 8.14469
[1mStep[0m  [32/42], [94mLoss[0m : 8.08230
[1mStep[0m  [36/42], [94mLoss[0m : 7.73413
[1mStep[0m  [40/42], [94mLoss[0m : 7.75969

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 8.374, [92mTest[0m: 8.092, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.02804
[1mStep[0m  [4/42], [94mLoss[0m : 8.07506
[1mStep[0m  [8/42], [94mLoss[0m : 7.81802
[1mStep[0m  [12/42], [94mLoss[0m : 7.55227
[1mStep[0m  [16/42], [94mLoss[0m : 7.59362
[1mStep[0m  [20/42], [94mLoss[0m : 7.45263
[1mStep[0m  [24/42], [94mLoss[0m : 7.69909
[1mStep[0m  [28/42], [94mLoss[0m : 7.28332
[1mStep[0m  [32/42], [94mLoss[0m : 7.37537
[1mStep[0m  [36/42], [94mLoss[0m : 7.39381
[1mStep[0m  [40/42], [94mLoss[0m : 7.40416

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 7.583, [92mTest[0m: 7.171, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.79107
[1mStep[0m  [4/42], [94mLoss[0m : 7.16405
[1mStep[0m  [8/42], [94mLoss[0m : 6.98643
[1mStep[0m  [12/42], [94mLoss[0m : 6.95901
[1mStep[0m  [16/42], [94mLoss[0m : 6.75233
[1mStep[0m  [20/42], [94mLoss[0m : 6.90267
[1mStep[0m  [24/42], [94mLoss[0m : 6.99201
[1mStep[0m  [28/42], [94mLoss[0m : 7.06570
[1mStep[0m  [32/42], [94mLoss[0m : 6.61521
[1mStep[0m  [36/42], [94mLoss[0m : 6.50790
[1mStep[0m  [40/42], [94mLoss[0m : 6.46286

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 6.887, [92mTest[0m: 6.591, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.45591
[1mStep[0m  [4/42], [94mLoss[0m : 6.11593
[1mStep[0m  [8/42], [94mLoss[0m : 6.32480
[1mStep[0m  [12/42], [94mLoss[0m : 6.12119
[1mStep[0m  [16/42], [94mLoss[0m : 6.07660
[1mStep[0m  [20/42], [94mLoss[0m : 6.26704
[1mStep[0m  [24/42], [94mLoss[0m : 6.09136
[1mStep[0m  [28/42], [94mLoss[0m : 6.36710
[1mStep[0m  [32/42], [94mLoss[0m : 5.86767
[1mStep[0m  [36/42], [94mLoss[0m : 5.85264
[1mStep[0m  [40/42], [94mLoss[0m : 6.29364

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 6.233, [92mTest[0m: 5.670, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.22164
[1mStep[0m  [4/42], [94mLoss[0m : 5.99562
[1mStep[0m  [8/42], [94mLoss[0m : 5.82273
[1mStep[0m  [12/42], [94mLoss[0m : 5.58100
[1mStep[0m  [16/42], [94mLoss[0m : 5.43300
[1mStep[0m  [20/42], [94mLoss[0m : 5.40428
[1mStep[0m  [24/42], [94mLoss[0m : 5.39115
[1mStep[0m  [28/42], [94mLoss[0m : 5.23084
[1mStep[0m  [32/42], [94mLoss[0m : 5.00785
[1mStep[0m  [36/42], [94mLoss[0m : 5.09157
[1mStep[0m  [40/42], [94mLoss[0m : 5.16943

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 5.488, [92mTest[0m: 4.928, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.31519
[1mStep[0m  [4/42], [94mLoss[0m : 4.84346
[1mStep[0m  [8/42], [94mLoss[0m : 5.04563
[1mStep[0m  [12/42], [94mLoss[0m : 5.00810
[1mStep[0m  [16/42], [94mLoss[0m : 4.90284
[1mStep[0m  [20/42], [94mLoss[0m : 4.96001
[1mStep[0m  [24/42], [94mLoss[0m : 4.31999
[1mStep[0m  [28/42], [94mLoss[0m : 4.67762
[1mStep[0m  [32/42], [94mLoss[0m : 4.37697
[1mStep[0m  [36/42], [94mLoss[0m : 4.69930
[1mStep[0m  [40/42], [94mLoss[0m : 4.40393

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 4.697, [92mTest[0m: 4.051, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.20840
[1mStep[0m  [4/42], [94mLoss[0m : 4.22279
[1mStep[0m  [8/42], [94mLoss[0m : 4.15335
[1mStep[0m  [12/42], [94mLoss[0m : 4.15171
[1mStep[0m  [16/42], [94mLoss[0m : 3.92295
[1mStep[0m  [20/42], [94mLoss[0m : 3.88805
[1mStep[0m  [24/42], [94mLoss[0m : 3.79582
[1mStep[0m  [28/42], [94mLoss[0m : 3.74421
[1mStep[0m  [32/42], [94mLoss[0m : 3.47907
[1mStep[0m  [36/42], [94mLoss[0m : 3.40173
[1mStep[0m  [40/42], [94mLoss[0m : 3.31216

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 3.848, [92mTest[0m: 3.349, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.17677
[1mStep[0m  [4/42], [94mLoss[0m : 3.01254
[1mStep[0m  [8/42], [94mLoss[0m : 3.31655
[1mStep[0m  [12/42], [94mLoss[0m : 3.49671
[1mStep[0m  [16/42], [94mLoss[0m : 2.97172
[1mStep[0m  [20/42], [94mLoss[0m : 3.29947
[1mStep[0m  [24/42], [94mLoss[0m : 2.95853
[1mStep[0m  [28/42], [94mLoss[0m : 3.20224
[1mStep[0m  [32/42], [94mLoss[0m : 3.01080
[1mStep[0m  [36/42], [94mLoss[0m : 2.99926
[1mStep[0m  [40/42], [94mLoss[0m : 3.00504

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 3.112, [92mTest[0m: 2.771, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.87278
[1mStep[0m  [4/42], [94mLoss[0m : 2.80019
[1mStep[0m  [8/42], [94mLoss[0m : 2.84303
[1mStep[0m  [12/42], [94mLoss[0m : 2.36162
[1mStep[0m  [16/42], [94mLoss[0m : 2.89366
[1mStep[0m  [20/42], [94mLoss[0m : 2.60855
[1mStep[0m  [24/42], [94mLoss[0m : 2.81039
[1mStep[0m  [28/42], [94mLoss[0m : 2.52552
[1mStep[0m  [32/42], [94mLoss[0m : 2.44656
[1mStep[0m  [36/42], [94mLoss[0m : 2.46464
[1mStep[0m  [40/42], [94mLoss[0m : 2.74154

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.727, [92mTest[0m: 2.437, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.92099
[1mStep[0m  [4/42], [94mLoss[0m : 2.62119
[1mStep[0m  [8/42], [94mLoss[0m : 2.30935
[1mStep[0m  [12/42], [94mLoss[0m : 2.58754
[1mStep[0m  [16/42], [94mLoss[0m : 2.44810
[1mStep[0m  [20/42], [94mLoss[0m : 2.52620
[1mStep[0m  [24/42], [94mLoss[0m : 2.44081
[1mStep[0m  [28/42], [94mLoss[0m : 2.67722
[1mStep[0m  [32/42], [94mLoss[0m : 2.47963
[1mStep[0m  [36/42], [94mLoss[0m : 2.45705
[1mStep[0m  [40/42], [94mLoss[0m : 2.41211

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.620, [92mTest[0m: 2.402, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58522
[1mStep[0m  [4/42], [94mLoss[0m : 2.76919
[1mStep[0m  [8/42], [94mLoss[0m : 2.69912
[1mStep[0m  [12/42], [94mLoss[0m : 2.57805
[1mStep[0m  [16/42], [94mLoss[0m : 2.38068
[1mStep[0m  [20/42], [94mLoss[0m : 2.57429
[1mStep[0m  [24/42], [94mLoss[0m : 2.60992
[1mStep[0m  [28/42], [94mLoss[0m : 2.64136
[1mStep[0m  [32/42], [94mLoss[0m : 2.33526
[1mStep[0m  [36/42], [94mLoss[0m : 2.55231
[1mStep[0m  [40/42], [94mLoss[0m : 2.62681

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.556, [92mTest[0m: 2.408, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.63884
[1mStep[0m  [4/42], [94mLoss[0m : 2.73100
[1mStep[0m  [8/42], [94mLoss[0m : 2.67983
[1mStep[0m  [12/42], [94mLoss[0m : 2.60715
[1mStep[0m  [16/42], [94mLoss[0m : 2.73582
[1mStep[0m  [20/42], [94mLoss[0m : 2.26343
[1mStep[0m  [24/42], [94mLoss[0m : 2.57919
[1mStep[0m  [28/42], [94mLoss[0m : 2.59581
[1mStep[0m  [32/42], [94mLoss[0m : 2.42134
[1mStep[0m  [36/42], [94mLoss[0m : 2.42638
[1mStep[0m  [40/42], [94mLoss[0m : 2.61264

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.557, [92mTest[0m: 2.425, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59217
[1mStep[0m  [4/42], [94mLoss[0m : 2.50352
[1mStep[0m  [8/42], [94mLoss[0m : 2.49031
[1mStep[0m  [12/42], [94mLoss[0m : 2.69894
[1mStep[0m  [16/42], [94mLoss[0m : 2.62545
[1mStep[0m  [20/42], [94mLoss[0m : 2.41985
[1mStep[0m  [24/42], [94mLoss[0m : 2.31989
[1mStep[0m  [28/42], [94mLoss[0m : 2.64351
[1mStep[0m  [32/42], [94mLoss[0m : 2.47647
[1mStep[0m  [36/42], [94mLoss[0m : 2.51381
[1mStep[0m  [40/42], [94mLoss[0m : 2.63662

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.539, [92mTest[0m: 2.371, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57862
[1mStep[0m  [4/42], [94mLoss[0m : 2.49589
[1mStep[0m  [8/42], [94mLoss[0m : 2.46459
[1mStep[0m  [12/42], [94mLoss[0m : 2.32730
[1mStep[0m  [16/42], [94mLoss[0m : 2.57560
[1mStep[0m  [20/42], [94mLoss[0m : 2.51576
[1mStep[0m  [24/42], [94mLoss[0m : 2.53275
[1mStep[0m  [28/42], [94mLoss[0m : 2.56321
[1mStep[0m  [32/42], [94mLoss[0m : 2.52746
[1mStep[0m  [36/42], [94mLoss[0m : 2.54475
[1mStep[0m  [40/42], [94mLoss[0m : 2.57760

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.526, [92mTest[0m: 2.432, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57989
[1mStep[0m  [4/42], [94mLoss[0m : 2.36834
[1mStep[0m  [8/42], [94mLoss[0m : 2.44291
[1mStep[0m  [12/42], [94mLoss[0m : 2.55041
[1mStep[0m  [16/42], [94mLoss[0m : 2.68990
[1mStep[0m  [20/42], [94mLoss[0m : 2.50274
[1mStep[0m  [24/42], [94mLoss[0m : 2.38921
[1mStep[0m  [28/42], [94mLoss[0m : 2.80628
[1mStep[0m  [32/42], [94mLoss[0m : 2.60832
[1mStep[0m  [36/42], [94mLoss[0m : 2.47850
[1mStep[0m  [40/42], [94mLoss[0m : 2.56356

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.423, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62338
[1mStep[0m  [4/42], [94mLoss[0m : 2.56334
[1mStep[0m  [8/42], [94mLoss[0m : 2.53962
[1mStep[0m  [12/42], [94mLoss[0m : 2.62430
[1mStep[0m  [16/42], [94mLoss[0m : 2.37641
[1mStep[0m  [20/42], [94mLoss[0m : 2.47192
[1mStep[0m  [24/42], [94mLoss[0m : 2.35820
[1mStep[0m  [28/42], [94mLoss[0m : 2.29034
[1mStep[0m  [32/42], [94mLoss[0m : 2.59473
[1mStep[0m  [36/42], [94mLoss[0m : 2.58805
[1mStep[0m  [40/42], [94mLoss[0m : 2.51461

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.515, [92mTest[0m: 2.383, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60183
[1mStep[0m  [4/42], [94mLoss[0m : 2.37534
[1mStep[0m  [8/42], [94mLoss[0m : 2.21812
[1mStep[0m  [12/42], [94mLoss[0m : 2.45392
[1mStep[0m  [16/42], [94mLoss[0m : 2.39164
[1mStep[0m  [20/42], [94mLoss[0m : 2.60037
[1mStep[0m  [24/42], [94mLoss[0m : 2.57750
[1mStep[0m  [28/42], [94mLoss[0m : 2.51724
[1mStep[0m  [32/42], [94mLoss[0m : 2.64423
[1mStep[0m  [36/42], [94mLoss[0m : 2.60907
[1mStep[0m  [40/42], [94mLoss[0m : 2.44254

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.368, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53600
[1mStep[0m  [4/42], [94mLoss[0m : 2.31777
[1mStep[0m  [8/42], [94mLoss[0m : 2.55114
[1mStep[0m  [12/42], [94mLoss[0m : 2.73492
[1mStep[0m  [16/42], [94mLoss[0m : 2.50420
[1mStep[0m  [20/42], [94mLoss[0m : 2.41373
[1mStep[0m  [24/42], [94mLoss[0m : 2.73436
[1mStep[0m  [28/42], [94mLoss[0m : 2.55391
[1mStep[0m  [32/42], [94mLoss[0m : 2.62615
[1mStep[0m  [36/42], [94mLoss[0m : 2.54310
[1mStep[0m  [40/42], [94mLoss[0m : 2.53602

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.381, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53572
[1mStep[0m  [4/42], [94mLoss[0m : 2.57032
[1mStep[0m  [8/42], [94mLoss[0m : 2.67414
[1mStep[0m  [12/42], [94mLoss[0m : 2.44364
[1mStep[0m  [16/42], [94mLoss[0m : 2.50394
[1mStep[0m  [20/42], [94mLoss[0m : 2.67584
[1mStep[0m  [24/42], [94mLoss[0m : 2.64629
[1mStep[0m  [28/42], [94mLoss[0m : 2.59111
[1mStep[0m  [32/42], [94mLoss[0m : 2.35722
[1mStep[0m  [36/42], [94mLoss[0m : 2.42056
[1mStep[0m  [40/42], [94mLoss[0m : 2.53643

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.364, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39762
[1mStep[0m  [4/42], [94mLoss[0m : 2.69914
[1mStep[0m  [8/42], [94mLoss[0m : 2.25757
[1mStep[0m  [12/42], [94mLoss[0m : 2.61461
[1mStep[0m  [16/42], [94mLoss[0m : 2.41840
[1mStep[0m  [20/42], [94mLoss[0m : 2.45342
[1mStep[0m  [24/42], [94mLoss[0m : 2.32056
[1mStep[0m  [28/42], [94mLoss[0m : 2.30442
[1mStep[0m  [32/42], [94mLoss[0m : 2.54095
[1mStep[0m  [36/42], [94mLoss[0m : 2.37308
[1mStep[0m  [40/42], [94mLoss[0m : 2.45529

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.377, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55231
[1mStep[0m  [4/42], [94mLoss[0m : 2.39051
[1mStep[0m  [8/42], [94mLoss[0m : 2.35305
[1mStep[0m  [12/42], [94mLoss[0m : 2.42811
[1mStep[0m  [16/42], [94mLoss[0m : 2.44137
[1mStep[0m  [20/42], [94mLoss[0m : 2.44183
[1mStep[0m  [24/42], [94mLoss[0m : 2.50232
[1mStep[0m  [28/42], [94mLoss[0m : 2.35966
[1mStep[0m  [32/42], [94mLoss[0m : 2.45104
[1mStep[0m  [36/42], [94mLoss[0m : 2.56614
[1mStep[0m  [40/42], [94mLoss[0m : 2.30983

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.369, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58944
[1mStep[0m  [4/42], [94mLoss[0m : 2.52127
[1mStep[0m  [8/42], [94mLoss[0m : 2.57685
[1mStep[0m  [12/42], [94mLoss[0m : 2.59484
[1mStep[0m  [16/42], [94mLoss[0m : 2.51304
[1mStep[0m  [20/42], [94mLoss[0m : 2.47163
[1mStep[0m  [24/42], [94mLoss[0m : 2.69371
[1mStep[0m  [28/42], [94mLoss[0m : 2.41654
[1mStep[0m  [32/42], [94mLoss[0m : 2.49723
[1mStep[0m  [36/42], [94mLoss[0m : 2.49605
[1mStep[0m  [40/42], [94mLoss[0m : 2.50642

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.377, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41655
[1mStep[0m  [4/42], [94mLoss[0m : 2.32407
[1mStep[0m  [8/42], [94mLoss[0m : 2.29037
[1mStep[0m  [12/42], [94mLoss[0m : 2.72796
[1mStep[0m  [16/42], [94mLoss[0m : 2.56009
[1mStep[0m  [20/42], [94mLoss[0m : 2.53898
[1mStep[0m  [24/42], [94mLoss[0m : 2.68264
[1mStep[0m  [28/42], [94mLoss[0m : 2.37051
[1mStep[0m  [32/42], [94mLoss[0m : 2.37052
[1mStep[0m  [36/42], [94mLoss[0m : 2.48675
[1mStep[0m  [40/42], [94mLoss[0m : 2.60389

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.398, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.23406
[1mStep[0m  [4/42], [94mLoss[0m : 2.49946
[1mStep[0m  [8/42], [94mLoss[0m : 2.49453
[1mStep[0m  [12/42], [94mLoss[0m : 2.43547
[1mStep[0m  [16/42], [94mLoss[0m : 2.29026
[1mStep[0m  [20/42], [94mLoss[0m : 2.47764
[1mStep[0m  [24/42], [94mLoss[0m : 2.32255
[1mStep[0m  [28/42], [94mLoss[0m : 2.53963
[1mStep[0m  [32/42], [94mLoss[0m : 2.49299
[1mStep[0m  [36/42], [94mLoss[0m : 2.29683
[1mStep[0m  [40/42], [94mLoss[0m : 2.55620

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.357, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43764
[1mStep[0m  [4/42], [94mLoss[0m : 2.43472
[1mStep[0m  [8/42], [94mLoss[0m : 2.43897
[1mStep[0m  [12/42], [94mLoss[0m : 2.52700
[1mStep[0m  [16/42], [94mLoss[0m : 2.42514
[1mStep[0m  [20/42], [94mLoss[0m : 2.65056
[1mStep[0m  [24/42], [94mLoss[0m : 2.49416
[1mStep[0m  [28/42], [94mLoss[0m : 2.45331
[1mStep[0m  [32/42], [94mLoss[0m : 2.44907
[1mStep[0m  [36/42], [94mLoss[0m : 2.55793
[1mStep[0m  [40/42], [94mLoss[0m : 2.41165

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.388, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39387
[1mStep[0m  [4/42], [94mLoss[0m : 2.34996
[1mStep[0m  [8/42], [94mLoss[0m : 2.51839
[1mStep[0m  [12/42], [94mLoss[0m : 2.61694
[1mStep[0m  [16/42], [94mLoss[0m : 2.38744
[1mStep[0m  [20/42], [94mLoss[0m : 2.39663
[1mStep[0m  [24/42], [94mLoss[0m : 2.61759
[1mStep[0m  [28/42], [94mLoss[0m : 2.41208
[1mStep[0m  [32/42], [94mLoss[0m : 2.50227
[1mStep[0m  [36/42], [94mLoss[0m : 2.32716
[1mStep[0m  [40/42], [94mLoss[0m : 2.47858

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.345, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44416
[1mStep[0m  [4/42], [94mLoss[0m : 2.31939
[1mStep[0m  [8/42], [94mLoss[0m : 2.48195
[1mStep[0m  [12/42], [94mLoss[0m : 2.58178
[1mStep[0m  [16/42], [94mLoss[0m : 2.43913
[1mStep[0m  [20/42], [94mLoss[0m : 2.59751
[1mStep[0m  [24/42], [94mLoss[0m : 2.32370
[1mStep[0m  [28/42], [94mLoss[0m : 2.27377
[1mStep[0m  [32/42], [94mLoss[0m : 2.28104
[1mStep[0m  [36/42], [94mLoss[0m : 2.26582
[1mStep[0m  [40/42], [94mLoss[0m : 2.44230

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.405, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.373
====================================

Phase 1 - Evaluation MAE:  2.3729117597852434
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 2.30392
[1mStep[0m  [4/42], [94mLoss[0m : 2.68647
[1mStep[0m  [8/42], [94mLoss[0m : 2.60017
[1mStep[0m  [12/42], [94mLoss[0m : 2.49959
[1mStep[0m  [16/42], [94mLoss[0m : 2.54779
[1mStep[0m  [20/42], [94mLoss[0m : 2.49788
[1mStep[0m  [24/42], [94mLoss[0m : 2.57441
[1mStep[0m  [28/42], [94mLoss[0m : 2.48662
[1mStep[0m  [32/42], [94mLoss[0m : 2.39908
[1mStep[0m  [36/42], [94mLoss[0m : 2.54301
[1mStep[0m  [40/42], [94mLoss[0m : 2.50377

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.510, [92mTest[0m: 2.374, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.70353
[1mStep[0m  [4/42], [94mLoss[0m : 2.42968
[1mStep[0m  [8/42], [94mLoss[0m : 2.53493
[1mStep[0m  [12/42], [94mLoss[0m : 2.40869
[1mStep[0m  [16/42], [94mLoss[0m : 2.67203
[1mStep[0m  [20/42], [94mLoss[0m : 2.49849
[1mStep[0m  [24/42], [94mLoss[0m : 2.54119
[1mStep[0m  [28/42], [94mLoss[0m : 2.45279
[1mStep[0m  [32/42], [94mLoss[0m : 2.42600
[1mStep[0m  [36/42], [94mLoss[0m : 2.42781
[1mStep[0m  [40/42], [94mLoss[0m : 2.41709

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.339, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60642
[1mStep[0m  [4/42], [94mLoss[0m : 2.31104
[1mStep[0m  [8/42], [94mLoss[0m : 2.42242
[1mStep[0m  [12/42], [94mLoss[0m : 2.27100
[1mStep[0m  [16/42], [94mLoss[0m : 2.34527
[1mStep[0m  [20/42], [94mLoss[0m : 2.56324
[1mStep[0m  [24/42], [94mLoss[0m : 2.34744
[1mStep[0m  [28/42], [94mLoss[0m : 2.65361
[1mStep[0m  [32/42], [94mLoss[0m : 2.46603
[1mStep[0m  [36/42], [94mLoss[0m : 2.60811
[1mStep[0m  [40/42], [94mLoss[0m : 2.27116

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.453, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50267
[1mStep[0m  [4/42], [94mLoss[0m : 2.17395
[1mStep[0m  [8/42], [94mLoss[0m : 2.39326
[1mStep[0m  [12/42], [94mLoss[0m : 2.35140
[1mStep[0m  [16/42], [94mLoss[0m : 2.64197
[1mStep[0m  [20/42], [94mLoss[0m : 2.47997
[1mStep[0m  [24/42], [94mLoss[0m : 2.46497
[1mStep[0m  [28/42], [94mLoss[0m : 2.52379
[1mStep[0m  [32/42], [94mLoss[0m : 2.44023
[1mStep[0m  [36/42], [94mLoss[0m : 2.30143
[1mStep[0m  [40/42], [94mLoss[0m : 2.44474

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.552, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44994
[1mStep[0m  [4/42], [94mLoss[0m : 2.41144
[1mStep[0m  [8/42], [94mLoss[0m : 2.41471
[1mStep[0m  [12/42], [94mLoss[0m : 2.15219
[1mStep[0m  [16/42], [94mLoss[0m : 2.28270
[1mStep[0m  [20/42], [94mLoss[0m : 2.38992
[1mStep[0m  [24/42], [94mLoss[0m : 2.41526
[1mStep[0m  [28/42], [94mLoss[0m : 2.51257
[1mStep[0m  [32/42], [94mLoss[0m : 2.28005
[1mStep[0m  [36/42], [94mLoss[0m : 2.26897
[1mStep[0m  [40/42], [94mLoss[0m : 2.24259

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.366, [92mTest[0m: 2.573, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28353
[1mStep[0m  [4/42], [94mLoss[0m : 2.28957
[1mStep[0m  [8/42], [94mLoss[0m : 2.37267
[1mStep[0m  [12/42], [94mLoss[0m : 2.20611
[1mStep[0m  [16/42], [94mLoss[0m : 2.46902
[1mStep[0m  [20/42], [94mLoss[0m : 2.31143
[1mStep[0m  [24/42], [94mLoss[0m : 2.28646
[1mStep[0m  [28/42], [94mLoss[0m : 2.33074
[1mStep[0m  [32/42], [94mLoss[0m : 2.25484
[1mStep[0m  [36/42], [94mLoss[0m : 2.31606
[1mStep[0m  [40/42], [94mLoss[0m : 2.27511

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.327, [92mTest[0m: 2.556, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.23308
[1mStep[0m  [4/42], [94mLoss[0m : 2.33513
[1mStep[0m  [8/42], [94mLoss[0m : 2.34753
[1mStep[0m  [12/42], [94mLoss[0m : 2.35756
[1mStep[0m  [16/42], [94mLoss[0m : 2.24519
[1mStep[0m  [20/42], [94mLoss[0m : 2.32515
[1mStep[0m  [24/42], [94mLoss[0m : 2.19506
[1mStep[0m  [28/42], [94mLoss[0m : 2.29402
[1mStep[0m  [32/42], [94mLoss[0m : 2.13260
[1mStep[0m  [36/42], [94mLoss[0m : 2.22068
[1mStep[0m  [40/42], [94mLoss[0m : 2.05206

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.286, [92mTest[0m: 2.551, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36004
[1mStep[0m  [4/42], [94mLoss[0m : 2.27262
[1mStep[0m  [8/42], [94mLoss[0m : 2.40448
[1mStep[0m  [12/42], [94mLoss[0m : 2.36115
[1mStep[0m  [16/42], [94mLoss[0m : 1.97603
[1mStep[0m  [20/42], [94mLoss[0m : 2.37447
[1mStep[0m  [24/42], [94mLoss[0m : 2.05891
[1mStep[0m  [28/42], [94mLoss[0m : 2.29694
[1mStep[0m  [32/42], [94mLoss[0m : 2.20167
[1mStep[0m  [36/42], [94mLoss[0m : 2.27847
[1mStep[0m  [40/42], [94mLoss[0m : 2.47303

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.266, [92mTest[0m: 2.648, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39762
[1mStep[0m  [4/42], [94mLoss[0m : 2.14507
[1mStep[0m  [8/42], [94mLoss[0m : 2.25377
[1mStep[0m  [12/42], [94mLoss[0m : 2.11975
[1mStep[0m  [16/42], [94mLoss[0m : 2.22241
[1mStep[0m  [20/42], [94mLoss[0m : 2.49313
[1mStep[0m  [24/42], [94mLoss[0m : 2.13424
[1mStep[0m  [28/42], [94mLoss[0m : 2.30370
[1mStep[0m  [32/42], [94mLoss[0m : 2.13502
[1mStep[0m  [36/42], [94mLoss[0m : 2.20534
[1mStep[0m  [40/42], [94mLoss[0m : 2.13458

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.214, [92mTest[0m: 2.536, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.08723
[1mStep[0m  [4/42], [94mLoss[0m : 2.21584
[1mStep[0m  [8/42], [94mLoss[0m : 2.20511
[1mStep[0m  [12/42], [94mLoss[0m : 2.34293
[1mStep[0m  [16/42], [94mLoss[0m : 2.01055
[1mStep[0m  [20/42], [94mLoss[0m : 2.20859
[1mStep[0m  [24/42], [94mLoss[0m : 2.19839
[1mStep[0m  [28/42], [94mLoss[0m : 2.17331
[1mStep[0m  [32/42], [94mLoss[0m : 2.15297
[1mStep[0m  [36/42], [94mLoss[0m : 2.05209
[1mStep[0m  [40/42], [94mLoss[0m : 2.12963

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.167, [92mTest[0m: 2.647, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.13469
[1mStep[0m  [4/42], [94mLoss[0m : 2.17424
[1mStep[0m  [8/42], [94mLoss[0m : 2.13358
[1mStep[0m  [12/42], [94mLoss[0m : 2.16017
[1mStep[0m  [16/42], [94mLoss[0m : 2.24330
[1mStep[0m  [20/42], [94mLoss[0m : 2.12418
[1mStep[0m  [24/42], [94mLoss[0m : 2.13630
[1mStep[0m  [28/42], [94mLoss[0m : 2.23640
[1mStep[0m  [32/42], [94mLoss[0m : 2.18163
[1mStep[0m  [36/42], [94mLoss[0m : 2.06612
[1mStep[0m  [40/42], [94mLoss[0m : 2.23704

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.149, [92mTest[0m: 2.700, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.11191
[1mStep[0m  [4/42], [94mLoss[0m : 1.97883
[1mStep[0m  [8/42], [94mLoss[0m : 2.07696
[1mStep[0m  [12/42], [94mLoss[0m : 2.10808
[1mStep[0m  [16/42], [94mLoss[0m : 2.08793
[1mStep[0m  [20/42], [94mLoss[0m : 2.13850
[1mStep[0m  [24/42], [94mLoss[0m : 2.22280
[1mStep[0m  [28/42], [94mLoss[0m : 2.14333
[1mStep[0m  [32/42], [94mLoss[0m : 2.38344
[1mStep[0m  [36/42], [94mLoss[0m : 2.06203
[1mStep[0m  [40/42], [94mLoss[0m : 2.29645

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.116, [92mTest[0m: 2.794, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.91245
[1mStep[0m  [4/42], [94mLoss[0m : 1.95836
[1mStep[0m  [8/42], [94mLoss[0m : 1.85423
[1mStep[0m  [12/42], [94mLoss[0m : 2.12491
[1mStep[0m  [16/42], [94mLoss[0m : 1.99997
[1mStep[0m  [20/42], [94mLoss[0m : 2.08301
[1mStep[0m  [24/42], [94mLoss[0m : 2.24034
[1mStep[0m  [28/42], [94mLoss[0m : 2.02378
[1mStep[0m  [32/42], [94mLoss[0m : 1.86030
[1mStep[0m  [36/42], [94mLoss[0m : 2.19189
[1mStep[0m  [40/42], [94mLoss[0m : 2.09188

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.060, [92mTest[0m: 2.698, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.03619
[1mStep[0m  [4/42], [94mLoss[0m : 2.01629
[1mStep[0m  [8/42], [94mLoss[0m : 2.06149
[1mStep[0m  [12/42], [94mLoss[0m : 1.92716
[1mStep[0m  [16/42], [94mLoss[0m : 1.95545
[1mStep[0m  [20/42], [94mLoss[0m : 2.08966
[1mStep[0m  [24/42], [94mLoss[0m : 2.02688
[1mStep[0m  [28/42], [94mLoss[0m : 2.21663
[1mStep[0m  [32/42], [94mLoss[0m : 2.10022
[1mStep[0m  [36/42], [94mLoss[0m : 2.20231
[1mStep[0m  [40/42], [94mLoss[0m : 2.06663

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.044, [92mTest[0m: 2.593, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.13112
[1mStep[0m  [4/42], [94mLoss[0m : 2.11843
[1mStep[0m  [8/42], [94mLoss[0m : 1.97749
[1mStep[0m  [12/42], [94mLoss[0m : 1.88251
[1mStep[0m  [16/42], [94mLoss[0m : 1.75648
[1mStep[0m  [20/42], [94mLoss[0m : 2.02107
[1mStep[0m  [24/42], [94mLoss[0m : 1.93511
[1mStep[0m  [28/42], [94mLoss[0m : 1.78544
[1mStep[0m  [32/42], [94mLoss[0m : 2.05098
[1mStep[0m  [36/42], [94mLoss[0m : 1.93141
[1mStep[0m  [40/42], [94mLoss[0m : 2.13564

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.993, [92mTest[0m: 2.584, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.96424
[1mStep[0m  [4/42], [94mLoss[0m : 1.90621
[1mStep[0m  [8/42], [94mLoss[0m : 2.13913
[1mStep[0m  [12/42], [94mLoss[0m : 1.81209
[1mStep[0m  [16/42], [94mLoss[0m : 1.97650
[1mStep[0m  [20/42], [94mLoss[0m : 1.86454
[1mStep[0m  [24/42], [94mLoss[0m : 1.99435
[1mStep[0m  [28/42], [94mLoss[0m : 1.87355
[1mStep[0m  [32/42], [94mLoss[0m : 1.98896
[1mStep[0m  [36/42], [94mLoss[0m : 1.89528
[1mStep[0m  [40/42], [94mLoss[0m : 1.96597

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.946, [92mTest[0m: 2.556, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.88432
[1mStep[0m  [4/42], [94mLoss[0m : 2.02634
[1mStep[0m  [8/42], [94mLoss[0m : 1.94518
[1mStep[0m  [12/42], [94mLoss[0m : 2.08626
[1mStep[0m  [16/42], [94mLoss[0m : 1.75992
[1mStep[0m  [20/42], [94mLoss[0m : 1.92917
[1mStep[0m  [24/42], [94mLoss[0m : 2.08945
[1mStep[0m  [28/42], [94mLoss[0m : 1.98159
[1mStep[0m  [32/42], [94mLoss[0m : 1.94174
[1mStep[0m  [36/42], [94mLoss[0m : 1.96060
[1mStep[0m  [40/42], [94mLoss[0m : 1.90990

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.933, [92mTest[0m: 2.647, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.90978
[1mStep[0m  [4/42], [94mLoss[0m : 1.68934
[1mStep[0m  [8/42], [94mLoss[0m : 1.84471
[1mStep[0m  [12/42], [94mLoss[0m : 1.90910
[1mStep[0m  [16/42], [94mLoss[0m : 1.78860
[1mStep[0m  [20/42], [94mLoss[0m : 1.81706
[1mStep[0m  [24/42], [94mLoss[0m : 1.87327
[1mStep[0m  [28/42], [94mLoss[0m : 1.87412
[1mStep[0m  [32/42], [94mLoss[0m : 2.04392
[1mStep[0m  [36/42], [94mLoss[0m : 1.98380
[1mStep[0m  [40/42], [94mLoss[0m : 2.10997

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.898, [92mTest[0m: 2.497, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.81528
[1mStep[0m  [4/42], [94mLoss[0m : 1.89575
[1mStep[0m  [8/42], [94mLoss[0m : 1.93991
[1mStep[0m  [12/42], [94mLoss[0m : 1.96462
[1mStep[0m  [16/42], [94mLoss[0m : 1.93895
[1mStep[0m  [20/42], [94mLoss[0m : 1.92236
[1mStep[0m  [24/42], [94mLoss[0m : 1.66960
[1mStep[0m  [28/42], [94mLoss[0m : 1.93006
[1mStep[0m  [32/42], [94mLoss[0m : 1.76173
[1mStep[0m  [36/42], [94mLoss[0m : 2.06244
[1mStep[0m  [40/42], [94mLoss[0m : 1.83572

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.860, [92mTest[0m: 2.580, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.87161
[1mStep[0m  [4/42], [94mLoss[0m : 1.74367
[1mStep[0m  [8/42], [94mLoss[0m : 1.85959
[1mStep[0m  [12/42], [94mLoss[0m : 1.78001
[1mStep[0m  [16/42], [94mLoss[0m : 1.79975
[1mStep[0m  [20/42], [94mLoss[0m : 1.83989
[1mStep[0m  [24/42], [94mLoss[0m : 1.90715
[1mStep[0m  [28/42], [94mLoss[0m : 1.71117
[1mStep[0m  [32/42], [94mLoss[0m : 1.92057
[1mStep[0m  [36/42], [94mLoss[0m : 1.95907
[1mStep[0m  [40/42], [94mLoss[0m : 1.77778

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.837, [92mTest[0m: 2.600, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.80001
[1mStep[0m  [4/42], [94mLoss[0m : 1.92801
[1mStep[0m  [8/42], [94mLoss[0m : 1.91453
[1mStep[0m  [12/42], [94mLoss[0m : 1.65313
[1mStep[0m  [16/42], [94mLoss[0m : 2.04500
[1mStep[0m  [20/42], [94mLoss[0m : 1.77416
[1mStep[0m  [24/42], [94mLoss[0m : 1.87794
[1mStep[0m  [28/42], [94mLoss[0m : 2.04387
[1mStep[0m  [32/42], [94mLoss[0m : 1.66936
[1mStep[0m  [36/42], [94mLoss[0m : 1.97310
[1mStep[0m  [40/42], [94mLoss[0m : 1.82436

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.808, [92mTest[0m: 2.690, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.60343
[1mStep[0m  [4/42], [94mLoss[0m : 1.83277
[1mStep[0m  [8/42], [94mLoss[0m : 1.48628
[1mStep[0m  [12/42], [94mLoss[0m : 2.03074
[1mStep[0m  [16/42], [94mLoss[0m : 1.81616
[1mStep[0m  [20/42], [94mLoss[0m : 1.50009
[1mStep[0m  [24/42], [94mLoss[0m : 1.82051
[1mStep[0m  [28/42], [94mLoss[0m : 1.72987
[1mStep[0m  [32/42], [94mLoss[0m : 1.61271
[1mStep[0m  [36/42], [94mLoss[0m : 1.87134
[1mStep[0m  [40/42], [94mLoss[0m : 1.77988

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.762, [92mTest[0m: 2.750, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.72720
[1mStep[0m  [4/42], [94mLoss[0m : 1.71935
[1mStep[0m  [8/42], [94mLoss[0m : 1.81035
[1mStep[0m  [12/42], [94mLoss[0m : 1.77453
[1mStep[0m  [16/42], [94mLoss[0m : 1.69549
[1mStep[0m  [20/42], [94mLoss[0m : 1.81424
[1mStep[0m  [24/42], [94mLoss[0m : 1.72799
[1mStep[0m  [28/42], [94mLoss[0m : 1.74274
[1mStep[0m  [32/42], [94mLoss[0m : 1.83477
[1mStep[0m  [36/42], [94mLoss[0m : 1.88144
[1mStep[0m  [40/42], [94mLoss[0m : 1.72720

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.764, [92mTest[0m: 2.678, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.63310
[1mStep[0m  [4/42], [94mLoss[0m : 1.58769
[1mStep[0m  [8/42], [94mLoss[0m : 1.59674
[1mStep[0m  [12/42], [94mLoss[0m : 1.72672
[1mStep[0m  [16/42], [94mLoss[0m : 1.67733
[1mStep[0m  [20/42], [94mLoss[0m : 1.71455
[1mStep[0m  [24/42], [94mLoss[0m : 1.75733
[1mStep[0m  [28/42], [94mLoss[0m : 1.74998
[1mStep[0m  [32/42], [94mLoss[0m : 1.88208
[1mStep[0m  [36/42], [94mLoss[0m : 1.59918
[1mStep[0m  [40/42], [94mLoss[0m : 1.67481

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.707, [92mTest[0m: 2.575, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.77202
[1mStep[0m  [4/42], [94mLoss[0m : 1.73697
[1mStep[0m  [8/42], [94mLoss[0m : 1.73016
[1mStep[0m  [12/42], [94mLoss[0m : 1.61395
[1mStep[0m  [16/42], [94mLoss[0m : 1.77346
[1mStep[0m  [20/42], [94mLoss[0m : 1.93701
[1mStep[0m  [24/42], [94mLoss[0m : 1.72760
[1mStep[0m  [28/42], [94mLoss[0m : 1.71363
[1mStep[0m  [32/42], [94mLoss[0m : 1.88969
[1mStep[0m  [36/42], [94mLoss[0m : 1.56032
[1mStep[0m  [40/42], [94mLoss[0m : 1.75811

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.705, [92mTest[0m: 2.621, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.49392
[1mStep[0m  [4/42], [94mLoss[0m : 1.49175
[1mStep[0m  [8/42], [94mLoss[0m : 1.65753
[1mStep[0m  [12/42], [94mLoss[0m : 1.79497
[1mStep[0m  [16/42], [94mLoss[0m : 1.58382
[1mStep[0m  [20/42], [94mLoss[0m : 1.64337
[1mStep[0m  [24/42], [94mLoss[0m : 1.85167
[1mStep[0m  [28/42], [94mLoss[0m : 1.80882
[1mStep[0m  [32/42], [94mLoss[0m : 1.63144
[1mStep[0m  [36/42], [94mLoss[0m : 1.65811
[1mStep[0m  [40/42], [94mLoss[0m : 1.58849

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.665, [92mTest[0m: 2.686, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.62799
[1mStep[0m  [4/42], [94mLoss[0m : 1.73990
[1mStep[0m  [8/42], [94mLoss[0m : 1.60628
[1mStep[0m  [12/42], [94mLoss[0m : 1.46454
[1mStep[0m  [16/42], [94mLoss[0m : 1.69186
[1mStep[0m  [20/42], [94mLoss[0m : 1.70060
[1mStep[0m  [24/42], [94mLoss[0m : 1.62515
[1mStep[0m  [28/42], [94mLoss[0m : 1.64173
[1mStep[0m  [32/42], [94mLoss[0m : 1.61310
[1mStep[0m  [36/42], [94mLoss[0m : 1.70575
[1mStep[0m  [40/42], [94mLoss[0m : 1.69971

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.644, [92mTest[0m: 2.688, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.56398
[1mStep[0m  [4/42], [94mLoss[0m : 1.76685
[1mStep[0m  [8/42], [94mLoss[0m : 1.66494
[1mStep[0m  [12/42], [94mLoss[0m : 1.56819
[1mStep[0m  [16/42], [94mLoss[0m : 1.65114
[1mStep[0m  [20/42], [94mLoss[0m : 1.50884
[1mStep[0m  [24/42], [94mLoss[0m : 1.54579
[1mStep[0m  [28/42], [94mLoss[0m : 1.54332
[1mStep[0m  [32/42], [94mLoss[0m : 1.67053
[1mStep[0m  [36/42], [94mLoss[0m : 1.74154
[1mStep[0m  [40/42], [94mLoss[0m : 1.60672

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.632, [92mTest[0m: 2.700, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.75665
[1mStep[0m  [4/42], [94mLoss[0m : 1.65342
[1mStep[0m  [8/42], [94mLoss[0m : 1.45100
[1mStep[0m  [12/42], [94mLoss[0m : 1.65541
[1mStep[0m  [16/42], [94mLoss[0m : 1.69701
[1mStep[0m  [20/42], [94mLoss[0m : 1.71870
[1mStep[0m  [24/42], [94mLoss[0m : 1.62908
[1mStep[0m  [28/42], [94mLoss[0m : 1.54841
[1mStep[0m  [32/42], [94mLoss[0m : 1.62132
[1mStep[0m  [36/42], [94mLoss[0m : 1.53871
[1mStep[0m  [40/42], [94mLoss[0m : 1.61055

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.628, [92mTest[0m: 2.673, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.58905
[1mStep[0m  [4/42], [94mLoss[0m : 1.75885
[1mStep[0m  [8/42], [94mLoss[0m : 1.50672
[1mStep[0m  [12/42], [94mLoss[0m : 1.60252
[1mStep[0m  [16/42], [94mLoss[0m : 1.65837
[1mStep[0m  [20/42], [94mLoss[0m : 1.45975
[1mStep[0m  [24/42], [94mLoss[0m : 1.65024
[1mStep[0m  [28/42], [94mLoss[0m : 1.39964
[1mStep[0m  [32/42], [94mLoss[0m : 1.66401
[1mStep[0m  [36/42], [94mLoss[0m : 1.56283
[1mStep[0m  [40/42], [94mLoss[0m : 1.55241

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.579, [92mTest[0m: 2.566, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.718
====================================

Phase 2 - Evaluation MAE:  2.7184663840702603
MAE score P1      2.372912
MAE score P2      2.718466
loss              1.579491
learning_rate     0.007525
batch_size             256
hidden_sizes         [300]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.1
weight_decay          0.01
Name: 4, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 10.94336
[1mStep[0m  [8/84], [94mLoss[0m : 10.49538
[1mStep[0m  [16/84], [94mLoss[0m : 9.98899
[1mStep[0m  [24/84], [94mLoss[0m : 8.60467
[1mStep[0m  [32/84], [94mLoss[0m : 7.69978
[1mStep[0m  [40/84], [94mLoss[0m : 6.51427
[1mStep[0m  [48/84], [94mLoss[0m : 5.92188
[1mStep[0m  [56/84], [94mLoss[0m : 5.46296
[1mStep[0m  [64/84], [94mLoss[0m : 4.14171
[1mStep[0m  [72/84], [94mLoss[0m : 3.76418
[1mStep[0m  [80/84], [94mLoss[0m : 2.53915

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.862, [92mTest[0m: 10.882, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.63982
[1mStep[0m  [8/84], [94mLoss[0m : 2.38444
[1mStep[0m  [16/84], [94mLoss[0m : 2.88800
[1mStep[0m  [24/84], [94mLoss[0m : 2.85549
[1mStep[0m  [32/84], [94mLoss[0m : 2.90288
[1mStep[0m  [40/84], [94mLoss[0m : 2.69820
[1mStep[0m  [48/84], [94mLoss[0m : 2.83248
[1mStep[0m  [56/84], [94mLoss[0m : 2.87362
[1mStep[0m  [64/84], [94mLoss[0m : 2.59463
[1mStep[0m  [72/84], [94mLoss[0m : 3.05537
[1mStep[0m  [80/84], [94mLoss[0m : 2.17918

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.637, [92mTest[0m: 2.910, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.68497
[1mStep[0m  [8/84], [94mLoss[0m : 2.48030
[1mStep[0m  [16/84], [94mLoss[0m : 2.47977
[1mStep[0m  [24/84], [94mLoss[0m : 2.61257
[1mStep[0m  [32/84], [94mLoss[0m : 2.37385
[1mStep[0m  [40/84], [94mLoss[0m : 3.04624
[1mStep[0m  [48/84], [94mLoss[0m : 2.44643
[1mStep[0m  [56/84], [94mLoss[0m : 2.49008
[1mStep[0m  [64/84], [94mLoss[0m : 2.63180
[1mStep[0m  [72/84], [94mLoss[0m : 2.63181
[1mStep[0m  [80/84], [94mLoss[0m : 2.51929

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.542, [92mTest[0m: 2.411, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42192
[1mStep[0m  [8/84], [94mLoss[0m : 2.49001
[1mStep[0m  [16/84], [94mLoss[0m : 2.32932
[1mStep[0m  [24/84], [94mLoss[0m : 2.45845
[1mStep[0m  [32/84], [94mLoss[0m : 2.37193
[1mStep[0m  [40/84], [94mLoss[0m : 2.18444
[1mStep[0m  [48/84], [94mLoss[0m : 2.34935
[1mStep[0m  [56/84], [94mLoss[0m : 2.56949
[1mStep[0m  [64/84], [94mLoss[0m : 2.58994
[1mStep[0m  [72/84], [94mLoss[0m : 2.38832
[1mStep[0m  [80/84], [94mLoss[0m : 2.42636

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.385, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31935
[1mStep[0m  [8/84], [94mLoss[0m : 2.05840
[1mStep[0m  [16/84], [94mLoss[0m : 2.41529
[1mStep[0m  [24/84], [94mLoss[0m : 2.69368
[1mStep[0m  [32/84], [94mLoss[0m : 2.47033
[1mStep[0m  [40/84], [94mLoss[0m : 2.52871
[1mStep[0m  [48/84], [94mLoss[0m : 2.33892
[1mStep[0m  [56/84], [94mLoss[0m : 2.85881
[1mStep[0m  [64/84], [94mLoss[0m : 2.46615
[1mStep[0m  [72/84], [94mLoss[0m : 2.57477
[1mStep[0m  [80/84], [94mLoss[0m : 2.44994

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.346, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60778
[1mStep[0m  [8/84], [94mLoss[0m : 2.53253
[1mStep[0m  [16/84], [94mLoss[0m : 2.53953
[1mStep[0m  [24/84], [94mLoss[0m : 2.16983
[1mStep[0m  [32/84], [94mLoss[0m : 2.69480
[1mStep[0m  [40/84], [94mLoss[0m : 2.55146
[1mStep[0m  [48/84], [94mLoss[0m : 2.46028
[1mStep[0m  [56/84], [94mLoss[0m : 2.57978
[1mStep[0m  [64/84], [94mLoss[0m : 2.81507
[1mStep[0m  [72/84], [94mLoss[0m : 2.42403
[1mStep[0m  [80/84], [94mLoss[0m : 2.37506

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.373, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.65034
[1mStep[0m  [8/84], [94mLoss[0m : 2.61163
[1mStep[0m  [16/84], [94mLoss[0m : 2.58379
[1mStep[0m  [24/84], [94mLoss[0m : 2.60975
[1mStep[0m  [32/84], [94mLoss[0m : 2.28047
[1mStep[0m  [40/84], [94mLoss[0m : 2.36433
[1mStep[0m  [48/84], [94mLoss[0m : 2.52471
[1mStep[0m  [56/84], [94mLoss[0m : 2.50096
[1mStep[0m  [64/84], [94mLoss[0m : 2.36183
[1mStep[0m  [72/84], [94mLoss[0m : 2.43693
[1mStep[0m  [80/84], [94mLoss[0m : 2.69938

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.352, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46341
[1mStep[0m  [8/84], [94mLoss[0m : 2.56668
[1mStep[0m  [16/84], [94mLoss[0m : 2.25733
[1mStep[0m  [24/84], [94mLoss[0m : 2.27344
[1mStep[0m  [32/84], [94mLoss[0m : 2.52584
[1mStep[0m  [40/84], [94mLoss[0m : 2.44833
[1mStep[0m  [48/84], [94mLoss[0m : 2.19825
[1mStep[0m  [56/84], [94mLoss[0m : 2.45622
[1mStep[0m  [64/84], [94mLoss[0m : 2.45210
[1mStep[0m  [72/84], [94mLoss[0m : 2.73901
[1mStep[0m  [80/84], [94mLoss[0m : 2.41888

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.347, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43412
[1mStep[0m  [8/84], [94mLoss[0m : 2.59374
[1mStep[0m  [16/84], [94mLoss[0m : 2.20927
[1mStep[0m  [24/84], [94mLoss[0m : 2.61244
[1mStep[0m  [32/84], [94mLoss[0m : 2.37097
[1mStep[0m  [40/84], [94mLoss[0m : 2.37652
[1mStep[0m  [48/84], [94mLoss[0m : 2.27302
[1mStep[0m  [56/84], [94mLoss[0m : 2.64430
[1mStep[0m  [64/84], [94mLoss[0m : 2.34752
[1mStep[0m  [72/84], [94mLoss[0m : 2.40356
[1mStep[0m  [80/84], [94mLoss[0m : 2.17407

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.363, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.23594
[1mStep[0m  [8/84], [94mLoss[0m : 2.30581
[1mStep[0m  [16/84], [94mLoss[0m : 2.42303
[1mStep[0m  [24/84], [94mLoss[0m : 2.60063
[1mStep[0m  [32/84], [94mLoss[0m : 2.31633
[1mStep[0m  [40/84], [94mLoss[0m : 2.62389
[1mStep[0m  [48/84], [94mLoss[0m : 2.31475
[1mStep[0m  [56/84], [94mLoss[0m : 2.54200
[1mStep[0m  [64/84], [94mLoss[0m : 2.42576
[1mStep[0m  [72/84], [94mLoss[0m : 2.14586
[1mStep[0m  [80/84], [94mLoss[0m : 2.62359

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.398, [92mTest[0m: 2.355, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50843
[1mStep[0m  [8/84], [94mLoss[0m : 2.32721
[1mStep[0m  [16/84], [94mLoss[0m : 2.25199
[1mStep[0m  [24/84], [94mLoss[0m : 2.54381
[1mStep[0m  [32/84], [94mLoss[0m : 2.31696
[1mStep[0m  [40/84], [94mLoss[0m : 2.49016
[1mStep[0m  [48/84], [94mLoss[0m : 2.29974
[1mStep[0m  [56/84], [94mLoss[0m : 2.25440
[1mStep[0m  [64/84], [94mLoss[0m : 2.43520
[1mStep[0m  [72/84], [94mLoss[0m : 2.38247
[1mStep[0m  [80/84], [94mLoss[0m : 2.34082

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.368, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43628
[1mStep[0m  [8/84], [94mLoss[0m : 2.52817
[1mStep[0m  [16/84], [94mLoss[0m : 2.26416
[1mStep[0m  [24/84], [94mLoss[0m : 2.36828
[1mStep[0m  [32/84], [94mLoss[0m : 2.62787
[1mStep[0m  [40/84], [94mLoss[0m : 2.20489
[1mStep[0m  [48/84], [94mLoss[0m : 2.76045
[1mStep[0m  [56/84], [94mLoss[0m : 2.42291
[1mStep[0m  [64/84], [94mLoss[0m : 2.49941
[1mStep[0m  [72/84], [94mLoss[0m : 2.30176
[1mStep[0m  [80/84], [94mLoss[0m : 2.39172

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.386, [92mTest[0m: 2.341, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.28360
[1mStep[0m  [8/84], [94mLoss[0m : 2.31490
[1mStep[0m  [16/84], [94mLoss[0m : 2.50715
[1mStep[0m  [24/84], [94mLoss[0m : 2.37174
[1mStep[0m  [32/84], [94mLoss[0m : 2.32338
[1mStep[0m  [40/84], [94mLoss[0m : 2.12514
[1mStep[0m  [48/84], [94mLoss[0m : 2.36549
[1mStep[0m  [56/84], [94mLoss[0m : 2.34162
[1mStep[0m  [64/84], [94mLoss[0m : 2.33689
[1mStep[0m  [72/84], [94mLoss[0m : 2.19067
[1mStep[0m  [80/84], [94mLoss[0m : 2.39602

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.379, [92mTest[0m: 2.339, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.62029
[1mStep[0m  [8/84], [94mLoss[0m : 2.53165
[1mStep[0m  [16/84], [94mLoss[0m : 2.22378
[1mStep[0m  [24/84], [94mLoss[0m : 2.48707
[1mStep[0m  [32/84], [94mLoss[0m : 1.95812
[1mStep[0m  [40/84], [94mLoss[0m : 2.63664
[1mStep[0m  [48/84], [94mLoss[0m : 2.25266
[1mStep[0m  [56/84], [94mLoss[0m : 2.63437
[1mStep[0m  [64/84], [94mLoss[0m : 2.41902
[1mStep[0m  [72/84], [94mLoss[0m : 2.45978
[1mStep[0m  [80/84], [94mLoss[0m : 2.41579

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.382, [92mTest[0m: 2.337, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31570
[1mStep[0m  [8/84], [94mLoss[0m : 2.59979
[1mStep[0m  [16/84], [94mLoss[0m : 2.65197
[1mStep[0m  [24/84], [94mLoss[0m : 2.23573
[1mStep[0m  [32/84], [94mLoss[0m : 2.45531
[1mStep[0m  [40/84], [94mLoss[0m : 2.40638
[1mStep[0m  [48/84], [94mLoss[0m : 2.18931
[1mStep[0m  [56/84], [94mLoss[0m : 2.28546
[1mStep[0m  [64/84], [94mLoss[0m : 2.33423
[1mStep[0m  [72/84], [94mLoss[0m : 2.61322
[1mStep[0m  [80/84], [94mLoss[0m : 2.45832

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.374, [92mTest[0m: 2.356, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.14218
[1mStep[0m  [8/84], [94mLoss[0m : 2.41600
[1mStep[0m  [16/84], [94mLoss[0m : 2.45315
[1mStep[0m  [24/84], [94mLoss[0m : 2.13757
[1mStep[0m  [32/84], [94mLoss[0m : 2.50619
[1mStep[0m  [40/84], [94mLoss[0m : 2.57489
[1mStep[0m  [48/84], [94mLoss[0m : 2.61909
[1mStep[0m  [56/84], [94mLoss[0m : 2.26540
[1mStep[0m  [64/84], [94mLoss[0m : 2.59832
[1mStep[0m  [72/84], [94mLoss[0m : 2.38791
[1mStep[0m  [80/84], [94mLoss[0m : 2.29112

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.374, [92mTest[0m: 2.385, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.07286
[1mStep[0m  [8/84], [94mLoss[0m : 2.14214
[1mStep[0m  [16/84], [94mLoss[0m : 2.32559
[1mStep[0m  [24/84], [94mLoss[0m : 2.22274
[1mStep[0m  [32/84], [94mLoss[0m : 2.43406
[1mStep[0m  [40/84], [94mLoss[0m : 2.37577
[1mStep[0m  [48/84], [94mLoss[0m : 2.28449
[1mStep[0m  [56/84], [94mLoss[0m : 2.33351
[1mStep[0m  [64/84], [94mLoss[0m : 2.33619
[1mStep[0m  [72/84], [94mLoss[0m : 2.35467
[1mStep[0m  [80/84], [94mLoss[0m : 2.34532

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.359, [92mTest[0m: 2.327, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.51017
[1mStep[0m  [8/84], [94mLoss[0m : 2.22153
[1mStep[0m  [16/84], [94mLoss[0m : 2.16568
[1mStep[0m  [24/84], [94mLoss[0m : 2.25626
[1mStep[0m  [32/84], [94mLoss[0m : 2.27206
[1mStep[0m  [40/84], [94mLoss[0m : 2.49325
[1mStep[0m  [48/84], [94mLoss[0m : 2.34277
[1mStep[0m  [56/84], [94mLoss[0m : 2.64514
[1mStep[0m  [64/84], [94mLoss[0m : 2.10084
[1mStep[0m  [72/84], [94mLoss[0m : 2.50340
[1mStep[0m  [80/84], [94mLoss[0m : 1.99322

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.359, [92mTest[0m: 2.349, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45605
[1mStep[0m  [8/84], [94mLoss[0m : 2.34618
[1mStep[0m  [16/84], [94mLoss[0m : 2.09291
[1mStep[0m  [24/84], [94mLoss[0m : 2.07291
[1mStep[0m  [32/84], [94mLoss[0m : 2.42055
[1mStep[0m  [40/84], [94mLoss[0m : 2.32898
[1mStep[0m  [48/84], [94mLoss[0m : 2.24442
[1mStep[0m  [56/84], [94mLoss[0m : 2.18498
[1mStep[0m  [64/84], [94mLoss[0m : 2.44230
[1mStep[0m  [72/84], [94mLoss[0m : 2.44281
[1mStep[0m  [80/84], [94mLoss[0m : 2.35854

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.363, [92mTest[0m: 2.375, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43698
[1mStep[0m  [8/84], [94mLoss[0m : 2.30975
[1mStep[0m  [16/84], [94mLoss[0m : 2.10449
[1mStep[0m  [24/84], [94mLoss[0m : 2.15391
[1mStep[0m  [32/84], [94mLoss[0m : 2.54964
[1mStep[0m  [40/84], [94mLoss[0m : 2.49232
[1mStep[0m  [48/84], [94mLoss[0m : 2.30922
[1mStep[0m  [56/84], [94mLoss[0m : 2.58831
[1mStep[0m  [64/84], [94mLoss[0m : 2.41030
[1mStep[0m  [72/84], [94mLoss[0m : 2.30031
[1mStep[0m  [80/84], [94mLoss[0m : 2.18241

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.354, [92mTest[0m: 2.327, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.01379
[1mStep[0m  [8/84], [94mLoss[0m : 2.47869
[1mStep[0m  [16/84], [94mLoss[0m : 2.19957
[1mStep[0m  [24/84], [94mLoss[0m : 2.50298
[1mStep[0m  [32/84], [94mLoss[0m : 2.37841
[1mStep[0m  [40/84], [94mLoss[0m : 2.40239
[1mStep[0m  [48/84], [94mLoss[0m : 2.21736
[1mStep[0m  [56/84], [94mLoss[0m : 2.31921
[1mStep[0m  [64/84], [94mLoss[0m : 2.24719
[1mStep[0m  [72/84], [94mLoss[0m : 2.65037
[1mStep[0m  [80/84], [94mLoss[0m : 2.34722

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.349, [92mTest[0m: 2.334, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47624
[1mStep[0m  [8/84], [94mLoss[0m : 2.41694
[1mStep[0m  [16/84], [94mLoss[0m : 2.35098
[1mStep[0m  [24/84], [94mLoss[0m : 2.46491
[1mStep[0m  [32/84], [94mLoss[0m : 2.49459
[1mStep[0m  [40/84], [94mLoss[0m : 2.03950
[1mStep[0m  [48/84], [94mLoss[0m : 2.34470
[1mStep[0m  [56/84], [94mLoss[0m : 1.97040
[1mStep[0m  [64/84], [94mLoss[0m : 1.99565
[1mStep[0m  [72/84], [94mLoss[0m : 2.14681
[1mStep[0m  [80/84], [94mLoss[0m : 2.20311

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.331, [92mTest[0m: 2.346, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26384
[1mStep[0m  [8/84], [94mLoss[0m : 2.21850
[1mStep[0m  [16/84], [94mLoss[0m : 2.80871
[1mStep[0m  [24/84], [94mLoss[0m : 2.41054
[1mStep[0m  [32/84], [94mLoss[0m : 2.08174
[1mStep[0m  [40/84], [94mLoss[0m : 2.38121
[1mStep[0m  [48/84], [94mLoss[0m : 2.23496
[1mStep[0m  [56/84], [94mLoss[0m : 2.87911
[1mStep[0m  [64/84], [94mLoss[0m : 2.14608
[1mStep[0m  [72/84], [94mLoss[0m : 2.42145
[1mStep[0m  [80/84], [94mLoss[0m : 2.63373

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.333, [92mTest[0m: 2.323, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.32002
[1mStep[0m  [8/84], [94mLoss[0m : 2.33235
[1mStep[0m  [16/84], [94mLoss[0m : 2.11591
[1mStep[0m  [24/84], [94mLoss[0m : 2.32414
[1mStep[0m  [32/84], [94mLoss[0m : 2.56310
[1mStep[0m  [40/84], [94mLoss[0m : 2.58705
[1mStep[0m  [48/84], [94mLoss[0m : 2.43106
[1mStep[0m  [56/84], [94mLoss[0m : 2.74315
[1mStep[0m  [64/84], [94mLoss[0m : 2.47154
[1mStep[0m  [72/84], [94mLoss[0m : 2.50977
[1mStep[0m  [80/84], [94mLoss[0m : 2.46129

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.358, [92mTest[0m: 2.328, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.09249
[1mStep[0m  [8/84], [94mLoss[0m : 2.65512
[1mStep[0m  [16/84], [94mLoss[0m : 2.62801
[1mStep[0m  [24/84], [94mLoss[0m : 2.23498
[1mStep[0m  [32/84], [94mLoss[0m : 2.08244
[1mStep[0m  [40/84], [94mLoss[0m : 2.59107
[1mStep[0m  [48/84], [94mLoss[0m : 2.44069
[1mStep[0m  [56/84], [94mLoss[0m : 2.13944
[1mStep[0m  [64/84], [94mLoss[0m : 2.44799
[1mStep[0m  [72/84], [94mLoss[0m : 2.40930
[1mStep[0m  [80/84], [94mLoss[0m : 2.37938

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.334, [92mTest[0m: 2.335, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.16220
[1mStep[0m  [8/84], [94mLoss[0m : 2.33576
[1mStep[0m  [16/84], [94mLoss[0m : 2.35221
[1mStep[0m  [24/84], [94mLoss[0m : 2.54959
[1mStep[0m  [32/84], [94mLoss[0m : 2.34659
[1mStep[0m  [40/84], [94mLoss[0m : 2.35077
[1mStep[0m  [48/84], [94mLoss[0m : 2.16983
[1mStep[0m  [56/84], [94mLoss[0m : 2.58100
[1mStep[0m  [64/84], [94mLoss[0m : 2.46824
[1mStep[0m  [72/84], [94mLoss[0m : 2.14171
[1mStep[0m  [80/84], [94mLoss[0m : 2.04986

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.327, [92mTest[0m: 2.327, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30707
[1mStep[0m  [8/84], [94mLoss[0m : 2.04473
[1mStep[0m  [16/84], [94mLoss[0m : 2.31331
[1mStep[0m  [24/84], [94mLoss[0m : 2.31000
[1mStep[0m  [32/84], [94mLoss[0m : 2.40767
[1mStep[0m  [40/84], [94mLoss[0m : 2.47683
[1mStep[0m  [48/84], [94mLoss[0m : 2.31825
[1mStep[0m  [56/84], [94mLoss[0m : 2.38725
[1mStep[0m  [64/84], [94mLoss[0m : 2.50880
[1mStep[0m  [72/84], [94mLoss[0m : 2.25065
[1mStep[0m  [80/84], [94mLoss[0m : 2.17254

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.321, [92mTest[0m: 2.328, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48706
[1mStep[0m  [8/84], [94mLoss[0m : 2.63692
[1mStep[0m  [16/84], [94mLoss[0m : 2.10995
[1mStep[0m  [24/84], [94mLoss[0m : 2.05411
[1mStep[0m  [32/84], [94mLoss[0m : 2.41471
[1mStep[0m  [40/84], [94mLoss[0m : 2.42114
[1mStep[0m  [48/84], [94mLoss[0m : 2.26580
[1mStep[0m  [56/84], [94mLoss[0m : 2.26788
[1mStep[0m  [64/84], [94mLoss[0m : 2.47556
[1mStep[0m  [72/84], [94mLoss[0m : 2.40971
[1mStep[0m  [80/84], [94mLoss[0m : 2.34950

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.338, [92mTest[0m: 2.336, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.19010
[1mStep[0m  [8/84], [94mLoss[0m : 2.52799
[1mStep[0m  [16/84], [94mLoss[0m : 2.45683
[1mStep[0m  [24/84], [94mLoss[0m : 2.37986
[1mStep[0m  [32/84], [94mLoss[0m : 2.22995
[1mStep[0m  [40/84], [94mLoss[0m : 2.35315
[1mStep[0m  [48/84], [94mLoss[0m : 2.44841
[1mStep[0m  [56/84], [94mLoss[0m : 2.38259
[1mStep[0m  [64/84], [94mLoss[0m : 2.19417
[1mStep[0m  [72/84], [94mLoss[0m : 2.54183
[1mStep[0m  [80/84], [94mLoss[0m : 2.27463

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.318, [92mTest[0m: 2.331, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.29743
[1mStep[0m  [8/84], [94mLoss[0m : 2.04422
[1mStep[0m  [16/84], [94mLoss[0m : 2.01071
[1mStep[0m  [24/84], [94mLoss[0m : 2.27983
[1mStep[0m  [32/84], [94mLoss[0m : 2.28064
[1mStep[0m  [40/84], [94mLoss[0m : 2.20411
[1mStep[0m  [48/84], [94mLoss[0m : 2.19980
[1mStep[0m  [56/84], [94mLoss[0m : 2.47120
[1mStep[0m  [64/84], [94mLoss[0m : 2.50846
[1mStep[0m  [72/84], [94mLoss[0m : 2.29025
[1mStep[0m  [80/84], [94mLoss[0m : 2.56581

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.317, [92mTest[0m: 2.344, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.332
====================================

Phase 1 - Evaluation MAE:  2.33168579850878
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 2.19339
[1mStep[0m  [8/84], [94mLoss[0m : 2.57062
[1mStep[0m  [16/84], [94mLoss[0m : 2.53472
[1mStep[0m  [24/84], [94mLoss[0m : 2.37329
[1mStep[0m  [32/84], [94mLoss[0m : 2.17842
[1mStep[0m  [40/84], [94mLoss[0m : 2.72032
[1mStep[0m  [48/84], [94mLoss[0m : 2.86771
[1mStep[0m  [56/84], [94mLoss[0m : 2.34220
[1mStep[0m  [64/84], [94mLoss[0m : 2.54132
[1mStep[0m  [72/84], [94mLoss[0m : 2.48057
[1mStep[0m  [80/84], [94mLoss[0m : 2.52541

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.338, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42910
[1mStep[0m  [8/84], [94mLoss[0m : 2.13843
[1mStep[0m  [16/84], [94mLoss[0m : 2.30920
[1mStep[0m  [24/84], [94mLoss[0m : 2.36382
[1mStep[0m  [32/84], [94mLoss[0m : 2.34057
[1mStep[0m  [40/84], [94mLoss[0m : 2.06803
[1mStep[0m  [48/84], [94mLoss[0m : 2.06081
[1mStep[0m  [56/84], [94mLoss[0m : 1.96663
[1mStep[0m  [64/84], [94mLoss[0m : 2.17189
[1mStep[0m  [72/84], [94mLoss[0m : 2.29008
[1mStep[0m  [80/84], [94mLoss[0m : 2.06820

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.291, [92mTest[0m: 2.372, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.92146
[1mStep[0m  [8/84], [94mLoss[0m : 2.39373
[1mStep[0m  [16/84], [94mLoss[0m : 1.95320
[1mStep[0m  [24/84], [94mLoss[0m : 1.92874
[1mStep[0m  [32/84], [94mLoss[0m : 2.27337
[1mStep[0m  [40/84], [94mLoss[0m : 2.04674
[1mStep[0m  [48/84], [94mLoss[0m : 2.17463
[1mStep[0m  [56/84], [94mLoss[0m : 2.23421
[1mStep[0m  [64/84], [94mLoss[0m : 2.06281
[1mStep[0m  [72/84], [94mLoss[0m : 2.19775
[1mStep[0m  [80/84], [94mLoss[0m : 2.52394

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.178, [92mTest[0m: 2.349, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.98108
[1mStep[0m  [8/84], [94mLoss[0m : 2.15806
[1mStep[0m  [16/84], [94mLoss[0m : 2.52882
[1mStep[0m  [24/84], [94mLoss[0m : 2.17035
[1mStep[0m  [32/84], [94mLoss[0m : 2.01004
[1mStep[0m  [40/84], [94mLoss[0m : 2.50490
[1mStep[0m  [48/84], [94mLoss[0m : 2.05052
[1mStep[0m  [56/84], [94mLoss[0m : 2.06559
[1mStep[0m  [64/84], [94mLoss[0m : 2.15142
[1mStep[0m  [72/84], [94mLoss[0m : 2.21532
[1mStep[0m  [80/84], [94mLoss[0m : 2.26392

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.121, [92mTest[0m: 2.376, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.98505
[1mStep[0m  [8/84], [94mLoss[0m : 2.06765
[1mStep[0m  [16/84], [94mLoss[0m : 1.75266
[1mStep[0m  [24/84], [94mLoss[0m : 2.19122
[1mStep[0m  [32/84], [94mLoss[0m : 2.00767
[1mStep[0m  [40/84], [94mLoss[0m : 1.77450
[1mStep[0m  [48/84], [94mLoss[0m : 2.04428
[1mStep[0m  [56/84], [94mLoss[0m : 2.47454
[1mStep[0m  [64/84], [94mLoss[0m : 1.82469
[1mStep[0m  [72/84], [94mLoss[0m : 1.99578
[1mStep[0m  [80/84], [94mLoss[0m : 2.12605

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.035, [92mTest[0m: 2.346, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.13029
[1mStep[0m  [8/84], [94mLoss[0m : 1.64594
[1mStep[0m  [16/84], [94mLoss[0m : 1.91304
[1mStep[0m  [24/84], [94mLoss[0m : 1.90460
[1mStep[0m  [32/84], [94mLoss[0m : 1.83895
[1mStep[0m  [40/84], [94mLoss[0m : 1.91687
[1mStep[0m  [48/84], [94mLoss[0m : 2.05960
[1mStep[0m  [56/84], [94mLoss[0m : 1.74839
[1mStep[0m  [64/84], [94mLoss[0m : 1.90348
[1mStep[0m  [72/84], [94mLoss[0m : 1.94380
[1mStep[0m  [80/84], [94mLoss[0m : 1.83126

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 1.968, [92mTest[0m: 2.369, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.73701
[1mStep[0m  [8/84], [94mLoss[0m : 1.52152
[1mStep[0m  [16/84], [94mLoss[0m : 1.88720
[1mStep[0m  [24/84], [94mLoss[0m : 1.95879
[1mStep[0m  [32/84], [94mLoss[0m : 1.87579
[1mStep[0m  [40/84], [94mLoss[0m : 1.78235
[1mStep[0m  [48/84], [94mLoss[0m : 1.66142
[1mStep[0m  [56/84], [94mLoss[0m : 1.99838
[1mStep[0m  [64/84], [94mLoss[0m : 2.09358
[1mStep[0m  [72/84], [94mLoss[0m : 2.01776
[1mStep[0m  [80/84], [94mLoss[0m : 1.91601

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.877, [92mTest[0m: 2.392, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.85899
[1mStep[0m  [8/84], [94mLoss[0m : 1.73699
[1mStep[0m  [16/84], [94mLoss[0m : 1.94082
[1mStep[0m  [24/84], [94mLoss[0m : 1.76442
[1mStep[0m  [32/84], [94mLoss[0m : 1.61914
[1mStep[0m  [40/84], [94mLoss[0m : 1.89061
[1mStep[0m  [48/84], [94mLoss[0m : 1.69082
[1mStep[0m  [56/84], [94mLoss[0m : 1.82685
[1mStep[0m  [64/84], [94mLoss[0m : 1.56772
[1mStep[0m  [72/84], [94mLoss[0m : 1.68774
[1mStep[0m  [80/84], [94mLoss[0m : 1.93850

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.820, [92mTest[0m: 2.423, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.71366
[1mStep[0m  [8/84], [94mLoss[0m : 1.78304
[1mStep[0m  [16/84], [94mLoss[0m : 1.68344
[1mStep[0m  [24/84], [94mLoss[0m : 1.72248
[1mStep[0m  [32/84], [94mLoss[0m : 1.66354
[1mStep[0m  [40/84], [94mLoss[0m : 1.77593
[1mStep[0m  [48/84], [94mLoss[0m : 1.74145
[1mStep[0m  [56/84], [94mLoss[0m : 1.63318
[1mStep[0m  [64/84], [94mLoss[0m : 1.58550
[1mStep[0m  [72/84], [94mLoss[0m : 1.82140
[1mStep[0m  [80/84], [94mLoss[0m : 1.77436

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.765, [92mTest[0m: 2.431, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.67223
[1mStep[0m  [8/84], [94mLoss[0m : 1.75676
[1mStep[0m  [16/84], [94mLoss[0m : 1.56349
[1mStep[0m  [24/84], [94mLoss[0m : 1.67201
[1mStep[0m  [32/84], [94mLoss[0m : 1.84329
[1mStep[0m  [40/84], [94mLoss[0m : 1.92006
[1mStep[0m  [48/84], [94mLoss[0m : 1.62312
[1mStep[0m  [56/84], [94mLoss[0m : 1.71967
[1mStep[0m  [64/84], [94mLoss[0m : 1.61521
[1mStep[0m  [72/84], [94mLoss[0m : 1.77994
[1mStep[0m  [80/84], [94mLoss[0m : 2.04440

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.723, [92mTest[0m: 2.422, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.47558
[1mStep[0m  [8/84], [94mLoss[0m : 1.38992
[1mStep[0m  [16/84], [94mLoss[0m : 1.67203
[1mStep[0m  [24/84], [94mLoss[0m : 1.69803
[1mStep[0m  [32/84], [94mLoss[0m : 1.85139
[1mStep[0m  [40/84], [94mLoss[0m : 1.60893
[1mStep[0m  [48/84], [94mLoss[0m : 1.58926
[1mStep[0m  [56/84], [94mLoss[0m : 1.67967
[1mStep[0m  [64/84], [94mLoss[0m : 1.75500
[1mStep[0m  [72/84], [94mLoss[0m : 1.74850
[1mStep[0m  [80/84], [94mLoss[0m : 1.71013

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.674, [92mTest[0m: 2.408, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.62314
[1mStep[0m  [8/84], [94mLoss[0m : 1.76125
[1mStep[0m  [16/84], [94mLoss[0m : 1.57265
[1mStep[0m  [24/84], [94mLoss[0m : 1.64353
[1mStep[0m  [32/84], [94mLoss[0m : 1.65828
[1mStep[0m  [40/84], [94mLoss[0m : 1.73346
[1mStep[0m  [48/84], [94mLoss[0m : 1.50259
[1mStep[0m  [56/84], [94mLoss[0m : 1.71507
[1mStep[0m  [64/84], [94mLoss[0m : 1.63393
[1mStep[0m  [72/84], [94mLoss[0m : 1.48387
[1mStep[0m  [80/84], [94mLoss[0m : 1.64980

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.625, [92mTest[0m: 2.478, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.39282
[1mStep[0m  [8/84], [94mLoss[0m : 1.44655
[1mStep[0m  [16/84], [94mLoss[0m : 1.63972
[1mStep[0m  [24/84], [94mLoss[0m : 1.41947
[1mStep[0m  [32/84], [94mLoss[0m : 1.72249
[1mStep[0m  [40/84], [94mLoss[0m : 1.49848
[1mStep[0m  [48/84], [94mLoss[0m : 1.65200
[1mStep[0m  [56/84], [94mLoss[0m : 1.51704
[1mStep[0m  [64/84], [94mLoss[0m : 1.80084
[1mStep[0m  [72/84], [94mLoss[0m : 1.45862
[1mStep[0m  [80/84], [94mLoss[0m : 1.73058

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.600, [92mTest[0m: 2.448, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.52849
[1mStep[0m  [8/84], [94mLoss[0m : 1.51674
[1mStep[0m  [16/84], [94mLoss[0m : 1.62173
[1mStep[0m  [24/84], [94mLoss[0m : 1.49930
[1mStep[0m  [32/84], [94mLoss[0m : 1.61334
[1mStep[0m  [40/84], [94mLoss[0m : 1.28198
[1mStep[0m  [48/84], [94mLoss[0m : 1.32781
[1mStep[0m  [56/84], [94mLoss[0m : 1.48298
[1mStep[0m  [64/84], [94mLoss[0m : 1.62831
[1mStep[0m  [72/84], [94mLoss[0m : 1.45199
[1mStep[0m  [80/84], [94mLoss[0m : 1.76074

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.536, [92mTest[0m: 2.498, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.33986
[1mStep[0m  [8/84], [94mLoss[0m : 1.36385
[1mStep[0m  [16/84], [94mLoss[0m : 1.57127
[1mStep[0m  [24/84], [94mLoss[0m : 1.35627
[1mStep[0m  [32/84], [94mLoss[0m : 1.50988
[1mStep[0m  [40/84], [94mLoss[0m : 1.43549
[1mStep[0m  [48/84], [94mLoss[0m : 1.54642
[1mStep[0m  [56/84], [94mLoss[0m : 1.52330
[1mStep[0m  [64/84], [94mLoss[0m : 1.70216
[1mStep[0m  [72/84], [94mLoss[0m : 1.56368
[1mStep[0m  [80/84], [94mLoss[0m : 1.53482

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.496, [92mTest[0m: 2.436, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.39971
[1mStep[0m  [8/84], [94mLoss[0m : 1.25139
[1mStep[0m  [16/84], [94mLoss[0m : 1.32239
[1mStep[0m  [24/84], [94mLoss[0m : 1.41329
[1mStep[0m  [32/84], [94mLoss[0m : 1.34814
[1mStep[0m  [40/84], [94mLoss[0m : 1.49382
[1mStep[0m  [48/84], [94mLoss[0m : 1.43298
[1mStep[0m  [56/84], [94mLoss[0m : 1.62843
[1mStep[0m  [64/84], [94mLoss[0m : 1.48709
[1mStep[0m  [72/84], [94mLoss[0m : 1.32934
[1mStep[0m  [80/84], [94mLoss[0m : 1.61703

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.463, [92mTest[0m: 2.555, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.35333
[1mStep[0m  [8/84], [94mLoss[0m : 1.44212
[1mStep[0m  [16/84], [94mLoss[0m : 1.37292
[1mStep[0m  [24/84], [94mLoss[0m : 1.68222
[1mStep[0m  [32/84], [94mLoss[0m : 1.32463
[1mStep[0m  [40/84], [94mLoss[0m : 1.55602
[1mStep[0m  [48/84], [94mLoss[0m : 1.44496
[1mStep[0m  [56/84], [94mLoss[0m : 1.31172
[1mStep[0m  [64/84], [94mLoss[0m : 1.57127
[1mStep[0m  [72/84], [94mLoss[0m : 1.43191
[1mStep[0m  [80/84], [94mLoss[0m : 1.49302

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.452, [92mTest[0m: 2.440, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.37405
[1mStep[0m  [8/84], [94mLoss[0m : 1.16177
[1mStep[0m  [16/84], [94mLoss[0m : 1.35858
[1mStep[0m  [24/84], [94mLoss[0m : 1.37718
[1mStep[0m  [32/84], [94mLoss[0m : 1.39924
[1mStep[0m  [40/84], [94mLoss[0m : 1.40456
[1mStep[0m  [48/84], [94mLoss[0m : 1.41519
[1mStep[0m  [56/84], [94mLoss[0m : 1.45291
[1mStep[0m  [64/84], [94mLoss[0m : 1.55218
[1mStep[0m  [72/84], [94mLoss[0m : 1.54031
[1mStep[0m  [80/84], [94mLoss[0m : 1.73887

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.410, [92mTest[0m: 2.529, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.51514
[1mStep[0m  [8/84], [94mLoss[0m : 1.39427
[1mStep[0m  [16/84], [94mLoss[0m : 1.59588
[1mStep[0m  [24/84], [94mLoss[0m : 1.51769
[1mStep[0m  [32/84], [94mLoss[0m : 1.45319
[1mStep[0m  [40/84], [94mLoss[0m : 1.36384
[1mStep[0m  [48/84], [94mLoss[0m : 1.45034
[1mStep[0m  [56/84], [94mLoss[0m : 1.36957
[1mStep[0m  [64/84], [94mLoss[0m : 1.33620
[1mStep[0m  [72/84], [94mLoss[0m : 1.22778
[1mStep[0m  [80/84], [94mLoss[0m : 1.25556

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.386, [92mTest[0m: 2.506, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.39648
[1mStep[0m  [8/84], [94mLoss[0m : 1.30011
[1mStep[0m  [16/84], [94mLoss[0m : 1.30612
[1mStep[0m  [24/84], [94mLoss[0m : 1.64065
[1mStep[0m  [32/84], [94mLoss[0m : 1.45152
[1mStep[0m  [40/84], [94mLoss[0m : 1.09620
[1mStep[0m  [48/84], [94mLoss[0m : 1.29892
[1mStep[0m  [56/84], [94mLoss[0m : 1.45401
[1mStep[0m  [64/84], [94mLoss[0m : 1.27492
[1mStep[0m  [72/84], [94mLoss[0m : 1.33729
[1mStep[0m  [80/84], [94mLoss[0m : 1.19804

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.331, [92mTest[0m: 2.530, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.16188
[1mStep[0m  [8/84], [94mLoss[0m : 1.13838
[1mStep[0m  [16/84], [94mLoss[0m : 1.27969
[1mStep[0m  [24/84], [94mLoss[0m : 1.33369
[1mStep[0m  [32/84], [94mLoss[0m : 1.30088
[1mStep[0m  [40/84], [94mLoss[0m : 1.57596
[1mStep[0m  [48/84], [94mLoss[0m : 1.16421
[1mStep[0m  [56/84], [94mLoss[0m : 1.16229
[1mStep[0m  [64/84], [94mLoss[0m : 1.22288
[1mStep[0m  [72/84], [94mLoss[0m : 1.39213
[1mStep[0m  [80/84], [94mLoss[0m : 1.17298

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.306, [92mTest[0m: 2.497, [96mlr[0m: 0.006772500000000002
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 20 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.494
====================================

Phase 2 - Evaluation MAE:  2.4940301094736372
MAE score P1      2.331686
MAE score P2       2.49403
loss                1.3061
learning_rate     0.007525
batch_size             128
hidden_sizes         [250]
epochs                  30
activation            tanh
optimizer              sgd
early stopping        True
dropout                0.2
momentum               0.9
weight_decay        0.0001
Name: 5, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 10.24483
[1mStep[0m  [8/84], [94mLoss[0m : 9.22018
[1mStep[0m  [16/84], [94mLoss[0m : 8.58401
[1mStep[0m  [24/84], [94mLoss[0m : 7.18717
[1mStep[0m  [32/84], [94mLoss[0m : 6.29775
[1mStep[0m  [40/84], [94mLoss[0m : 5.59200
[1mStep[0m  [48/84], [94mLoss[0m : 4.38467
[1mStep[0m  [56/84], [94mLoss[0m : 4.08935
[1mStep[0m  [64/84], [94mLoss[0m : 3.87073
[1mStep[0m  [72/84], [94mLoss[0m : 3.18497
[1mStep[0m  [80/84], [94mLoss[0m : 2.98297

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 5.801, [92mTest[0m: 11.004, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.06656
[1mStep[0m  [8/84], [94mLoss[0m : 3.11712
[1mStep[0m  [16/84], [94mLoss[0m : 2.67936
[1mStep[0m  [24/84], [94mLoss[0m : 2.67076
[1mStep[0m  [32/84], [94mLoss[0m : 2.88052
[1mStep[0m  [40/84], [94mLoss[0m : 2.57102
[1mStep[0m  [48/84], [94mLoss[0m : 2.48770
[1mStep[0m  [56/84], [94mLoss[0m : 2.31209
[1mStep[0m  [64/84], [94mLoss[0m : 2.59509
[1mStep[0m  [72/84], [94mLoss[0m : 2.62155
[1mStep[0m  [80/84], [94mLoss[0m : 2.81292

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.655, [92mTest[0m: 3.377, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36654
[1mStep[0m  [8/84], [94mLoss[0m : 2.41563
[1mStep[0m  [16/84], [94mLoss[0m : 2.74721
[1mStep[0m  [24/84], [94mLoss[0m : 2.50802
[1mStep[0m  [32/84], [94mLoss[0m : 2.48978
[1mStep[0m  [40/84], [94mLoss[0m : 2.66003
[1mStep[0m  [48/84], [94mLoss[0m : 2.54735
[1mStep[0m  [56/84], [94mLoss[0m : 2.39673
[1mStep[0m  [64/84], [94mLoss[0m : 2.69221
[1mStep[0m  [72/84], [94mLoss[0m : 2.62697
[1mStep[0m  [80/84], [94mLoss[0m : 2.63092

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.585, [92mTest[0m: 2.747, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49156
[1mStep[0m  [8/84], [94mLoss[0m : 3.04541
[1mStep[0m  [16/84], [94mLoss[0m : 2.55294
[1mStep[0m  [24/84], [94mLoss[0m : 2.49317
[1mStep[0m  [32/84], [94mLoss[0m : 2.75337
[1mStep[0m  [40/84], [94mLoss[0m : 2.20629
[1mStep[0m  [48/84], [94mLoss[0m : 2.57493
[1mStep[0m  [56/84], [94mLoss[0m : 2.53949
[1mStep[0m  [64/84], [94mLoss[0m : 2.29367
[1mStep[0m  [72/84], [94mLoss[0m : 2.45341
[1mStep[0m  [80/84], [94mLoss[0m : 2.34757

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.561, [92mTest[0m: 2.515, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.21958
[1mStep[0m  [8/84], [94mLoss[0m : 2.44374
[1mStep[0m  [16/84], [94mLoss[0m : 2.52393
[1mStep[0m  [24/84], [94mLoss[0m : 2.68097
[1mStep[0m  [32/84], [94mLoss[0m : 2.34223
[1mStep[0m  [40/84], [94mLoss[0m : 2.57077
[1mStep[0m  [48/84], [94mLoss[0m : 2.19826
[1mStep[0m  [56/84], [94mLoss[0m : 2.59690
[1mStep[0m  [64/84], [94mLoss[0m : 2.60315
[1mStep[0m  [72/84], [94mLoss[0m : 2.66812
[1mStep[0m  [80/84], [94mLoss[0m : 2.48986

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.433, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.34975
[1mStep[0m  [8/84], [94mLoss[0m : 2.42488
[1mStep[0m  [16/84], [94mLoss[0m : 2.37896
[1mStep[0m  [24/84], [94mLoss[0m : 2.42794
[1mStep[0m  [32/84], [94mLoss[0m : 2.28106
[1mStep[0m  [40/84], [94mLoss[0m : 2.49826
[1mStep[0m  [48/84], [94mLoss[0m : 2.59092
[1mStep[0m  [56/84], [94mLoss[0m : 2.52330
[1mStep[0m  [64/84], [94mLoss[0m : 2.25744
[1mStep[0m  [72/84], [94mLoss[0m : 2.70763
[1mStep[0m  [80/84], [94mLoss[0m : 2.53762

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.450, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46299
[1mStep[0m  [8/84], [94mLoss[0m : 2.47055
[1mStep[0m  [16/84], [94mLoss[0m : 2.33017
[1mStep[0m  [24/84], [94mLoss[0m : 2.44508
[1mStep[0m  [32/84], [94mLoss[0m : 2.32193
[1mStep[0m  [40/84], [94mLoss[0m : 2.41534
[1mStep[0m  [48/84], [94mLoss[0m : 2.36015
[1mStep[0m  [56/84], [94mLoss[0m : 2.34638
[1mStep[0m  [64/84], [94mLoss[0m : 2.55938
[1mStep[0m  [72/84], [94mLoss[0m : 2.81900
[1mStep[0m  [80/84], [94mLoss[0m : 2.48106

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.438, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50187
[1mStep[0m  [8/84], [94mLoss[0m : 2.63162
[1mStep[0m  [16/84], [94mLoss[0m : 2.56367
[1mStep[0m  [24/84], [94mLoss[0m : 2.62868
[1mStep[0m  [32/84], [94mLoss[0m : 2.73782
[1mStep[0m  [40/84], [94mLoss[0m : 2.45273
[1mStep[0m  [48/84], [94mLoss[0m : 2.58676
[1mStep[0m  [56/84], [94mLoss[0m : 2.63182
[1mStep[0m  [64/84], [94mLoss[0m : 2.28359
[1mStep[0m  [72/84], [94mLoss[0m : 2.60739
[1mStep[0m  [80/84], [94mLoss[0m : 2.51558

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.399, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.28988
[1mStep[0m  [8/84], [94mLoss[0m : 2.19727
[1mStep[0m  [16/84], [94mLoss[0m : 2.90147
[1mStep[0m  [24/84], [94mLoss[0m : 2.30218
[1mStep[0m  [32/84], [94mLoss[0m : 2.37621
[1mStep[0m  [40/84], [94mLoss[0m : 2.46108
[1mStep[0m  [48/84], [94mLoss[0m : 2.36963
[1mStep[0m  [56/84], [94mLoss[0m : 2.59271
[1mStep[0m  [64/84], [94mLoss[0m : 2.50886
[1mStep[0m  [72/84], [94mLoss[0m : 2.40072
[1mStep[0m  [80/84], [94mLoss[0m : 2.56020

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.402, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.29587
[1mStep[0m  [8/84], [94mLoss[0m : 2.50449
[1mStep[0m  [16/84], [94mLoss[0m : 2.40078
[1mStep[0m  [24/84], [94mLoss[0m : 2.24162
[1mStep[0m  [32/84], [94mLoss[0m : 2.58072
[1mStep[0m  [40/84], [94mLoss[0m : 2.27025
[1mStep[0m  [48/84], [94mLoss[0m : 2.48208
[1mStep[0m  [56/84], [94mLoss[0m : 2.40105
[1mStep[0m  [64/84], [94mLoss[0m : 2.42348
[1mStep[0m  [72/84], [94mLoss[0m : 2.48624
[1mStep[0m  [80/84], [94mLoss[0m : 2.61483

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.383, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.65715
[1mStep[0m  [8/84], [94mLoss[0m : 2.44989
[1mStep[0m  [16/84], [94mLoss[0m : 1.99377
[1mStep[0m  [24/84], [94mLoss[0m : 2.59204
[1mStep[0m  [32/84], [94mLoss[0m : 2.74774
[1mStep[0m  [40/84], [94mLoss[0m : 2.25128
[1mStep[0m  [48/84], [94mLoss[0m : 2.40768
[1mStep[0m  [56/84], [94mLoss[0m : 2.53154
[1mStep[0m  [64/84], [94mLoss[0m : 2.72267
[1mStep[0m  [72/84], [94mLoss[0m : 2.20481
[1mStep[0m  [80/84], [94mLoss[0m : 2.76276

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.367, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41563
[1mStep[0m  [8/84], [94mLoss[0m : 2.26683
[1mStep[0m  [16/84], [94mLoss[0m : 2.48261
[1mStep[0m  [24/84], [94mLoss[0m : 1.96166
[1mStep[0m  [32/84], [94mLoss[0m : 2.55050
[1mStep[0m  [40/84], [94mLoss[0m : 2.46654
[1mStep[0m  [48/84], [94mLoss[0m : 2.34281
[1mStep[0m  [56/84], [94mLoss[0m : 2.32202
[1mStep[0m  [64/84], [94mLoss[0m : 2.38111
[1mStep[0m  [72/84], [94mLoss[0m : 2.57161
[1mStep[0m  [80/84], [94mLoss[0m : 2.43363

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.367, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.80837
[1mStep[0m  [8/84], [94mLoss[0m : 2.30494
[1mStep[0m  [16/84], [94mLoss[0m : 2.46135
[1mStep[0m  [24/84], [94mLoss[0m : 2.44230
[1mStep[0m  [32/84], [94mLoss[0m : 2.56329
[1mStep[0m  [40/84], [94mLoss[0m : 2.42623
[1mStep[0m  [48/84], [94mLoss[0m : 2.62508
[1mStep[0m  [56/84], [94mLoss[0m : 2.34285
[1mStep[0m  [64/84], [94mLoss[0m : 1.99054
[1mStep[0m  [72/84], [94mLoss[0m : 2.52676
[1mStep[0m  [80/84], [94mLoss[0m : 2.45282

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.344, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.62582
[1mStep[0m  [8/84], [94mLoss[0m : 2.26706
[1mStep[0m  [16/84], [94mLoss[0m : 2.72950
[1mStep[0m  [24/84], [94mLoss[0m : 2.43866
[1mStep[0m  [32/84], [94mLoss[0m : 2.40847
[1mStep[0m  [40/84], [94mLoss[0m : 2.58094
[1mStep[0m  [48/84], [94mLoss[0m : 2.14348
[1mStep[0m  [56/84], [94mLoss[0m : 2.53007
[1mStep[0m  [64/84], [94mLoss[0m : 2.48045
[1mStep[0m  [72/84], [94mLoss[0m : 2.28157
[1mStep[0m  [80/84], [94mLoss[0m : 2.18714

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.356, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.24671
[1mStep[0m  [8/84], [94mLoss[0m : 2.51151
[1mStep[0m  [16/84], [94mLoss[0m : 2.33846
[1mStep[0m  [24/84], [94mLoss[0m : 2.36347
[1mStep[0m  [32/84], [94mLoss[0m : 2.52465
[1mStep[0m  [40/84], [94mLoss[0m : 2.70308
[1mStep[0m  [48/84], [94mLoss[0m : 2.59902
[1mStep[0m  [56/84], [94mLoss[0m : 2.48734
[1mStep[0m  [64/84], [94mLoss[0m : 2.65775
[1mStep[0m  [72/84], [94mLoss[0m : 2.51106
[1mStep[0m  [80/84], [94mLoss[0m : 2.65017

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.374, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48592
[1mStep[0m  [8/84], [94mLoss[0m : 2.34034
[1mStep[0m  [16/84], [94mLoss[0m : 2.62469
[1mStep[0m  [24/84], [94mLoss[0m : 2.24012
[1mStep[0m  [32/84], [94mLoss[0m : 2.43941
[1mStep[0m  [40/84], [94mLoss[0m : 2.20131
[1mStep[0m  [48/84], [94mLoss[0m : 2.52301
[1mStep[0m  [56/84], [94mLoss[0m : 2.42130
[1mStep[0m  [64/84], [94mLoss[0m : 2.43650
[1mStep[0m  [72/84], [94mLoss[0m : 2.17100
[1mStep[0m  [80/84], [94mLoss[0m : 2.32705

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.391, [92mTest[0m: 2.354, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.18950
[1mStep[0m  [8/84], [94mLoss[0m : 2.44782
[1mStep[0m  [16/84], [94mLoss[0m : 2.48911
[1mStep[0m  [24/84], [94mLoss[0m : 2.14182
[1mStep[0m  [32/84], [94mLoss[0m : 2.55313
[1mStep[0m  [40/84], [94mLoss[0m : 2.21821
[1mStep[0m  [48/84], [94mLoss[0m : 2.40602
[1mStep[0m  [56/84], [94mLoss[0m : 2.29328
[1mStep[0m  [64/84], [94mLoss[0m : 2.27931
[1mStep[0m  [72/84], [94mLoss[0m : 2.15597
[1mStep[0m  [80/84], [94mLoss[0m : 2.51771

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.347, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41623
[1mStep[0m  [8/84], [94mLoss[0m : 2.28532
[1mStep[0m  [16/84], [94mLoss[0m : 2.49317
[1mStep[0m  [24/84], [94mLoss[0m : 2.24371
[1mStep[0m  [32/84], [94mLoss[0m : 2.17090
[1mStep[0m  [40/84], [94mLoss[0m : 2.87865
[1mStep[0m  [48/84], [94mLoss[0m : 2.51786
[1mStep[0m  [56/84], [94mLoss[0m : 2.45631
[1mStep[0m  [64/84], [94mLoss[0m : 2.32439
[1mStep[0m  [72/84], [94mLoss[0m : 2.64355
[1mStep[0m  [80/84], [94mLoss[0m : 2.07272

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.381, [92mTest[0m: 2.338, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46763
[1mStep[0m  [8/84], [94mLoss[0m : 2.17603
[1mStep[0m  [16/84], [94mLoss[0m : 2.21284
[1mStep[0m  [24/84], [94mLoss[0m : 2.29941
[1mStep[0m  [32/84], [94mLoss[0m : 2.25989
[1mStep[0m  [40/84], [94mLoss[0m : 2.30551
[1mStep[0m  [48/84], [94mLoss[0m : 2.56591
[1mStep[0m  [56/84], [94mLoss[0m : 2.70981
[1mStep[0m  [64/84], [94mLoss[0m : 2.22479
[1mStep[0m  [72/84], [94mLoss[0m : 2.44553
[1mStep[0m  [80/84], [94mLoss[0m : 2.15466

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.344, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.09097
[1mStep[0m  [8/84], [94mLoss[0m : 2.32853
[1mStep[0m  [16/84], [94mLoss[0m : 2.31247
[1mStep[0m  [24/84], [94mLoss[0m : 2.57628
[1mStep[0m  [32/84], [94mLoss[0m : 2.45935
[1mStep[0m  [40/84], [94mLoss[0m : 2.37335
[1mStep[0m  [48/84], [94mLoss[0m : 2.10806
[1mStep[0m  [56/84], [94mLoss[0m : 2.31588
[1mStep[0m  [64/84], [94mLoss[0m : 2.34640
[1mStep[0m  [72/84], [94mLoss[0m : 2.28073
[1mStep[0m  [80/84], [94mLoss[0m : 2.43345

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.376, [92mTest[0m: 2.343, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49925
[1mStep[0m  [8/84], [94mLoss[0m : 2.41354
[1mStep[0m  [16/84], [94mLoss[0m : 2.18231
[1mStep[0m  [24/84], [94mLoss[0m : 2.38318
[1mStep[0m  [32/84], [94mLoss[0m : 2.39418
[1mStep[0m  [40/84], [94mLoss[0m : 2.38254
[1mStep[0m  [48/84], [94mLoss[0m : 2.36533
[1mStep[0m  [56/84], [94mLoss[0m : 2.31211
[1mStep[0m  [64/84], [94mLoss[0m : 2.44263
[1mStep[0m  [72/84], [94mLoss[0m : 2.27230
[1mStep[0m  [80/84], [94mLoss[0m : 2.46376

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.384, [92mTest[0m: 2.354, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52281
[1mStep[0m  [8/84], [94mLoss[0m : 2.08469
[1mStep[0m  [16/84], [94mLoss[0m : 2.25984
[1mStep[0m  [24/84], [94mLoss[0m : 2.39704
[1mStep[0m  [32/84], [94mLoss[0m : 2.34444
[1mStep[0m  [40/84], [94mLoss[0m : 2.23224
[1mStep[0m  [48/84], [94mLoss[0m : 2.72991
[1mStep[0m  [56/84], [94mLoss[0m : 2.39749
[1mStep[0m  [64/84], [94mLoss[0m : 2.23193
[1mStep[0m  [72/84], [94mLoss[0m : 2.34869
[1mStep[0m  [80/84], [94mLoss[0m : 2.45419

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.370, [92mTest[0m: 2.377, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43833
[1mStep[0m  [8/84], [94mLoss[0m : 2.19076
[1mStep[0m  [16/84], [94mLoss[0m : 2.57545
[1mStep[0m  [24/84], [94mLoss[0m : 2.09409
[1mStep[0m  [32/84], [94mLoss[0m : 2.18154
[1mStep[0m  [40/84], [94mLoss[0m : 2.29882
[1mStep[0m  [48/84], [94mLoss[0m : 2.42803
[1mStep[0m  [56/84], [94mLoss[0m : 2.39628
[1mStep[0m  [64/84], [94mLoss[0m : 2.12066
[1mStep[0m  [72/84], [94mLoss[0m : 2.06870
[1mStep[0m  [80/84], [94mLoss[0m : 2.61831

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.343, [92mTest[0m: 2.335, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59364
[1mStep[0m  [8/84], [94mLoss[0m : 2.30524
[1mStep[0m  [16/84], [94mLoss[0m : 2.42453
[1mStep[0m  [24/84], [94mLoss[0m : 2.13868
[1mStep[0m  [32/84], [94mLoss[0m : 2.26889
[1mStep[0m  [40/84], [94mLoss[0m : 2.28091
[1mStep[0m  [48/84], [94mLoss[0m : 2.20984
[1mStep[0m  [56/84], [94mLoss[0m : 2.30175
[1mStep[0m  [64/84], [94mLoss[0m : 2.36039
[1mStep[0m  [72/84], [94mLoss[0m : 2.30624
[1mStep[0m  [80/84], [94mLoss[0m : 2.53116

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.337, [92mTest[0m: 2.338, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40368
[1mStep[0m  [8/84], [94mLoss[0m : 2.24050
[1mStep[0m  [16/84], [94mLoss[0m : 2.68700
[1mStep[0m  [24/84], [94mLoss[0m : 2.32783
[1mStep[0m  [32/84], [94mLoss[0m : 2.45461
[1mStep[0m  [40/84], [94mLoss[0m : 2.02960
[1mStep[0m  [48/84], [94mLoss[0m : 2.25562
[1mStep[0m  [56/84], [94mLoss[0m : 2.38233
[1mStep[0m  [64/84], [94mLoss[0m : 2.47646
[1mStep[0m  [72/84], [94mLoss[0m : 2.33002
[1mStep[0m  [80/84], [94mLoss[0m : 2.39667

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.342, [92mTest[0m: 2.346, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.21274
[1mStep[0m  [8/84], [94mLoss[0m : 2.33835
[1mStep[0m  [16/84], [94mLoss[0m : 2.12752
[1mStep[0m  [24/84], [94mLoss[0m : 2.17622
[1mStep[0m  [32/84], [94mLoss[0m : 2.41656
[1mStep[0m  [40/84], [94mLoss[0m : 2.15029
[1mStep[0m  [48/84], [94mLoss[0m : 2.38863
[1mStep[0m  [56/84], [94mLoss[0m : 2.17683
[1mStep[0m  [64/84], [94mLoss[0m : 2.55970
[1mStep[0m  [72/84], [94mLoss[0m : 2.32131
[1mStep[0m  [80/84], [94mLoss[0m : 2.43719

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.344, [92mTest[0m: 2.341, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36074
[1mStep[0m  [8/84], [94mLoss[0m : 2.54071
[1mStep[0m  [16/84], [94mLoss[0m : 2.38090
[1mStep[0m  [24/84], [94mLoss[0m : 2.13781
[1mStep[0m  [32/84], [94mLoss[0m : 2.38677
[1mStep[0m  [40/84], [94mLoss[0m : 2.54949
[1mStep[0m  [48/84], [94mLoss[0m : 2.13408
[1mStep[0m  [56/84], [94mLoss[0m : 2.33147
[1mStep[0m  [64/84], [94mLoss[0m : 2.48007
[1mStep[0m  [72/84], [94mLoss[0m : 2.27038
[1mStep[0m  [80/84], [94mLoss[0m : 2.64242

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.333, [92mTest[0m: 2.333, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.24328
[1mStep[0m  [8/84], [94mLoss[0m : 2.22733
[1mStep[0m  [16/84], [94mLoss[0m : 2.72645
[1mStep[0m  [24/84], [94mLoss[0m : 2.17569
[1mStep[0m  [32/84], [94mLoss[0m : 2.01907
[1mStep[0m  [40/84], [94mLoss[0m : 2.38911
[1mStep[0m  [48/84], [94mLoss[0m : 2.38352
[1mStep[0m  [56/84], [94mLoss[0m : 2.29702
[1mStep[0m  [64/84], [94mLoss[0m : 2.21036
[1mStep[0m  [72/84], [94mLoss[0m : 2.16777
[1mStep[0m  [80/84], [94mLoss[0m : 2.05264

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.332, [92mTest[0m: 2.333, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35825
[1mStep[0m  [8/84], [94mLoss[0m : 2.41869
[1mStep[0m  [16/84], [94mLoss[0m : 2.52298
[1mStep[0m  [24/84], [94mLoss[0m : 2.18365
[1mStep[0m  [32/84], [94mLoss[0m : 2.38347
[1mStep[0m  [40/84], [94mLoss[0m : 2.41450
[1mStep[0m  [48/84], [94mLoss[0m : 2.37387
[1mStep[0m  [56/84], [94mLoss[0m : 2.30548
[1mStep[0m  [64/84], [94mLoss[0m : 2.40981
[1mStep[0m  [72/84], [94mLoss[0m : 2.24677
[1mStep[0m  [80/84], [94mLoss[0m : 2.35206

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.327, [92mTest[0m: 2.316, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.19597
[1mStep[0m  [8/84], [94mLoss[0m : 2.24559
[1mStep[0m  [16/84], [94mLoss[0m : 2.27148
[1mStep[0m  [24/84], [94mLoss[0m : 2.34563
[1mStep[0m  [32/84], [94mLoss[0m : 2.44198
[1mStep[0m  [40/84], [94mLoss[0m : 2.15871
[1mStep[0m  [48/84], [94mLoss[0m : 2.25025
[1mStep[0m  [56/84], [94mLoss[0m : 2.28193
[1mStep[0m  [64/84], [94mLoss[0m : 2.13097
[1mStep[0m  [72/84], [94mLoss[0m : 2.30091
[1mStep[0m  [80/84], [94mLoss[0m : 2.22545

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.337, [92mTest[0m: 2.318, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.335
====================================

Phase 1 - Evaluation MAE:  2.3354389795235226
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 2.35594
[1mStep[0m  [8/84], [94mLoss[0m : 2.28675
[1mStep[0m  [16/84], [94mLoss[0m : 2.42899
[1mStep[0m  [24/84], [94mLoss[0m : 2.47174
[1mStep[0m  [32/84], [94mLoss[0m : 2.34310
[1mStep[0m  [40/84], [94mLoss[0m : 2.34964
[1mStep[0m  [48/84], [94mLoss[0m : 2.29986
[1mStep[0m  [56/84], [94mLoss[0m : 2.20493
[1mStep[0m  [64/84], [94mLoss[0m : 2.46727
[1mStep[0m  [72/84], [94mLoss[0m : 2.49978
[1mStep[0m  [80/84], [94mLoss[0m : 2.56514

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.335, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67000
[1mStep[0m  [8/84], [94mLoss[0m : 2.39343
[1mStep[0m  [16/84], [94mLoss[0m : 2.07611
[1mStep[0m  [24/84], [94mLoss[0m : 2.29947
[1mStep[0m  [32/84], [94mLoss[0m : 2.46402
[1mStep[0m  [40/84], [94mLoss[0m : 2.33293
[1mStep[0m  [48/84], [94mLoss[0m : 2.61735
[1mStep[0m  [56/84], [94mLoss[0m : 2.52701
[1mStep[0m  [64/84], [94mLoss[0m : 2.39853
[1mStep[0m  [72/84], [94mLoss[0m : 2.24750
[1mStep[0m  [80/84], [94mLoss[0m : 2.58371

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.440, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27858
[1mStep[0m  [8/84], [94mLoss[0m : 2.33474
[1mStep[0m  [16/84], [94mLoss[0m : 2.15589
[1mStep[0m  [24/84], [94mLoss[0m : 2.33552
[1mStep[0m  [32/84], [94mLoss[0m : 2.50378
[1mStep[0m  [40/84], [94mLoss[0m : 2.45077
[1mStep[0m  [48/84], [94mLoss[0m : 2.28305
[1mStep[0m  [56/84], [94mLoss[0m : 2.36811
[1mStep[0m  [64/84], [94mLoss[0m : 2.09059
[1mStep[0m  [72/84], [94mLoss[0m : 2.36982
[1mStep[0m  [80/84], [94mLoss[0m : 2.23298

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.353, [92mTest[0m: 2.437, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.25657
[1mStep[0m  [8/84], [94mLoss[0m : 1.95627
[1mStep[0m  [16/84], [94mLoss[0m : 2.44279
[1mStep[0m  [24/84], [94mLoss[0m : 2.38878
[1mStep[0m  [32/84], [94mLoss[0m : 2.57916
[1mStep[0m  [40/84], [94mLoss[0m : 2.59500
[1mStep[0m  [48/84], [94mLoss[0m : 2.52668
[1mStep[0m  [56/84], [94mLoss[0m : 2.48819
[1mStep[0m  [64/84], [94mLoss[0m : 2.35580
[1mStep[0m  [72/84], [94mLoss[0m : 2.35333
[1mStep[0m  [80/84], [94mLoss[0m : 2.49127

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.318, [92mTest[0m: 2.543, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27592
[1mStep[0m  [8/84], [94mLoss[0m : 2.28945
[1mStep[0m  [16/84], [94mLoss[0m : 2.40460
[1mStep[0m  [24/84], [94mLoss[0m : 2.12202
[1mStep[0m  [32/84], [94mLoss[0m : 2.14171
[1mStep[0m  [40/84], [94mLoss[0m : 2.15314
[1mStep[0m  [48/84], [94mLoss[0m : 2.26332
[1mStep[0m  [56/84], [94mLoss[0m : 2.34490
[1mStep[0m  [64/84], [94mLoss[0m : 2.08633
[1mStep[0m  [72/84], [94mLoss[0m : 2.18147
[1mStep[0m  [80/84], [94mLoss[0m : 1.97643

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.241, [92mTest[0m: 2.370, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.15415
[1mStep[0m  [8/84], [94mLoss[0m : 2.45174
[1mStep[0m  [16/84], [94mLoss[0m : 1.89526
[1mStep[0m  [24/84], [94mLoss[0m : 2.33256
[1mStep[0m  [32/84], [94mLoss[0m : 1.83338
[1mStep[0m  [40/84], [94mLoss[0m : 1.99587
[1mStep[0m  [48/84], [94mLoss[0m : 2.09332
[1mStep[0m  [56/84], [94mLoss[0m : 1.93524
[1mStep[0m  [64/84], [94mLoss[0m : 2.51845
[1mStep[0m  [72/84], [94mLoss[0m : 1.94457
[1mStep[0m  [80/84], [94mLoss[0m : 2.26110

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.209, [92mTest[0m: 2.394, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.90382
[1mStep[0m  [8/84], [94mLoss[0m : 1.92298
[1mStep[0m  [16/84], [94mLoss[0m : 1.97394
[1mStep[0m  [24/84], [94mLoss[0m : 2.15519
[1mStep[0m  [32/84], [94mLoss[0m : 2.15051
[1mStep[0m  [40/84], [94mLoss[0m : 1.94608
[1mStep[0m  [48/84], [94mLoss[0m : 2.22995
[1mStep[0m  [56/84], [94mLoss[0m : 2.07419
[1mStep[0m  [64/84], [94mLoss[0m : 1.77434
[1mStep[0m  [72/84], [94mLoss[0m : 2.44542
[1mStep[0m  [80/84], [94mLoss[0m : 2.22314

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.156, [92mTest[0m: 2.419, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.02271
[1mStep[0m  [8/84], [94mLoss[0m : 1.87391
[1mStep[0m  [16/84], [94mLoss[0m : 2.21000
[1mStep[0m  [24/84], [94mLoss[0m : 1.96457
[1mStep[0m  [32/84], [94mLoss[0m : 2.05618
[1mStep[0m  [40/84], [94mLoss[0m : 2.13017
[1mStep[0m  [48/84], [94mLoss[0m : 2.14919
[1mStep[0m  [56/84], [94mLoss[0m : 2.23124
[1mStep[0m  [64/84], [94mLoss[0m : 2.04823
[1mStep[0m  [72/84], [94mLoss[0m : 2.10399
[1mStep[0m  [80/84], [94mLoss[0m : 2.09114

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.123, [92mTest[0m: 2.515, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.97964
[1mStep[0m  [8/84], [94mLoss[0m : 2.16474
[1mStep[0m  [16/84], [94mLoss[0m : 2.19912
[1mStep[0m  [24/84], [94mLoss[0m : 1.91406
[1mStep[0m  [32/84], [94mLoss[0m : 1.97811
[1mStep[0m  [40/84], [94mLoss[0m : 1.79511
[1mStep[0m  [48/84], [94mLoss[0m : 1.89656
[1mStep[0m  [56/84], [94mLoss[0m : 2.18764
[1mStep[0m  [64/84], [94mLoss[0m : 2.06565
[1mStep[0m  [72/84], [94mLoss[0m : 1.96235
[1mStep[0m  [80/84], [94mLoss[0m : 1.85823

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.048, [92mTest[0m: 2.397, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.71254
[1mStep[0m  [8/84], [94mLoss[0m : 1.98771
[1mStep[0m  [16/84], [94mLoss[0m : 2.17066
[1mStep[0m  [24/84], [94mLoss[0m : 1.99407
[1mStep[0m  [32/84], [94mLoss[0m : 1.85251
[1mStep[0m  [40/84], [94mLoss[0m : 1.96068
[1mStep[0m  [48/84], [94mLoss[0m : 1.93404
[1mStep[0m  [56/84], [94mLoss[0m : 2.06701
[1mStep[0m  [64/84], [94mLoss[0m : 2.05594
[1mStep[0m  [72/84], [94mLoss[0m : 1.89509
[1mStep[0m  [80/84], [94mLoss[0m : 1.78418

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.021, [92mTest[0m: 2.427, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.00374
[1mStep[0m  [8/84], [94mLoss[0m : 1.87995
[1mStep[0m  [16/84], [94mLoss[0m : 1.86421
[1mStep[0m  [24/84], [94mLoss[0m : 2.07789
[1mStep[0m  [32/84], [94mLoss[0m : 1.78612
[1mStep[0m  [40/84], [94mLoss[0m : 2.18868
[1mStep[0m  [48/84], [94mLoss[0m : 2.08936
[1mStep[0m  [56/84], [94mLoss[0m : 2.15049
[1mStep[0m  [64/84], [94mLoss[0m : 2.29167
[1mStep[0m  [72/84], [94mLoss[0m : 2.07613
[1mStep[0m  [80/84], [94mLoss[0m : 2.02949

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.982, [92mTest[0m: 2.431, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.80043
[1mStep[0m  [8/84], [94mLoss[0m : 1.93345
[1mStep[0m  [16/84], [94mLoss[0m : 1.91267
[1mStep[0m  [24/84], [94mLoss[0m : 1.88426
[1mStep[0m  [32/84], [94mLoss[0m : 1.81725
[1mStep[0m  [40/84], [94mLoss[0m : 1.89725
[1mStep[0m  [48/84], [94mLoss[0m : 1.81475
[1mStep[0m  [56/84], [94mLoss[0m : 1.99658
[1mStep[0m  [64/84], [94mLoss[0m : 1.98930
[1mStep[0m  [72/84], [94mLoss[0m : 1.89444
[1mStep[0m  [80/84], [94mLoss[0m : 2.01518

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.930, [92mTest[0m: 2.515, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.88880
[1mStep[0m  [8/84], [94mLoss[0m : 1.69879
[1mStep[0m  [16/84], [94mLoss[0m : 1.92962
[1mStep[0m  [24/84], [94mLoss[0m : 1.88623
[1mStep[0m  [32/84], [94mLoss[0m : 1.88580
[1mStep[0m  [40/84], [94mLoss[0m : 1.86472
[1mStep[0m  [48/84], [94mLoss[0m : 1.65077
[1mStep[0m  [56/84], [94mLoss[0m : 2.12685
[1mStep[0m  [64/84], [94mLoss[0m : 1.94005
[1mStep[0m  [72/84], [94mLoss[0m : 1.80795
[1mStep[0m  [80/84], [94mLoss[0m : 1.69578

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.905, [92mTest[0m: 2.484, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.85782
[1mStep[0m  [8/84], [94mLoss[0m : 1.71513
[1mStep[0m  [16/84], [94mLoss[0m : 1.79798
[1mStep[0m  [24/84], [94mLoss[0m : 1.90127
[1mStep[0m  [32/84], [94mLoss[0m : 1.84044
[1mStep[0m  [40/84], [94mLoss[0m : 2.03939
[1mStep[0m  [48/84], [94mLoss[0m : 2.20025
[1mStep[0m  [56/84], [94mLoss[0m : 1.66933
[1mStep[0m  [64/84], [94mLoss[0m : 1.92803
[1mStep[0m  [72/84], [94mLoss[0m : 1.84088
[1mStep[0m  [80/84], [94mLoss[0m : 1.79603

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.871, [92mTest[0m: 2.405, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.93912
[1mStep[0m  [8/84], [94mLoss[0m : 2.12841
[1mStep[0m  [16/84], [94mLoss[0m : 2.00265
[1mStep[0m  [24/84], [94mLoss[0m : 1.83766
[1mStep[0m  [32/84], [94mLoss[0m : 1.77068
[1mStep[0m  [40/84], [94mLoss[0m : 1.83681
[1mStep[0m  [48/84], [94mLoss[0m : 2.09037
[1mStep[0m  [56/84], [94mLoss[0m : 2.09564
[1mStep[0m  [64/84], [94mLoss[0m : 1.91780
[1mStep[0m  [72/84], [94mLoss[0m : 1.65443
[1mStep[0m  [80/84], [94mLoss[0m : 1.84178

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.854, [92mTest[0m: 2.434, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.78530
[1mStep[0m  [8/84], [94mLoss[0m : 1.69401
[1mStep[0m  [16/84], [94mLoss[0m : 1.64054
[1mStep[0m  [24/84], [94mLoss[0m : 2.01776
[1mStep[0m  [32/84], [94mLoss[0m : 1.80772
[1mStep[0m  [40/84], [94mLoss[0m : 1.64141
[1mStep[0m  [48/84], [94mLoss[0m : 2.00218
[1mStep[0m  [56/84], [94mLoss[0m : 1.90471
[1mStep[0m  [64/84], [94mLoss[0m : 1.67974
[1mStep[0m  [72/84], [94mLoss[0m : 1.74013
[1mStep[0m  [80/84], [94mLoss[0m : 1.80651

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.809, [92mTest[0m: 2.490, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.54798
[1mStep[0m  [8/84], [94mLoss[0m : 1.59422
[1mStep[0m  [16/84], [94mLoss[0m : 1.81800
[1mStep[0m  [24/84], [94mLoss[0m : 1.74682
[1mStep[0m  [32/84], [94mLoss[0m : 1.64928
[1mStep[0m  [40/84], [94mLoss[0m : 2.04368
[1mStep[0m  [48/84], [94mLoss[0m : 1.88636
[1mStep[0m  [56/84], [94mLoss[0m : 1.90262
[1mStep[0m  [64/84], [94mLoss[0m : 1.75826
[1mStep[0m  [72/84], [94mLoss[0m : 1.72564
[1mStep[0m  [80/84], [94mLoss[0m : 1.88749

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.783, [92mTest[0m: 2.526, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.65110
[1mStep[0m  [8/84], [94mLoss[0m : 1.77454
[1mStep[0m  [16/84], [94mLoss[0m : 1.55811
[1mStep[0m  [24/84], [94mLoss[0m : 1.83836
[1mStep[0m  [32/84], [94mLoss[0m : 1.61234
[1mStep[0m  [40/84], [94mLoss[0m : 1.81762
[1mStep[0m  [48/84], [94mLoss[0m : 1.64433
[1mStep[0m  [56/84], [94mLoss[0m : 1.66358
[1mStep[0m  [64/84], [94mLoss[0m : 1.80366
[1mStep[0m  [72/84], [94mLoss[0m : 1.80464
[1mStep[0m  [80/84], [94mLoss[0m : 1.86278

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.735, [92mTest[0m: 2.500, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.74529
[1mStep[0m  [8/84], [94mLoss[0m : 1.57124
[1mStep[0m  [16/84], [94mLoss[0m : 1.69960
[1mStep[0m  [24/84], [94mLoss[0m : 1.90288
[1mStep[0m  [32/84], [94mLoss[0m : 1.86340
[1mStep[0m  [40/84], [94mLoss[0m : 1.54940
[1mStep[0m  [48/84], [94mLoss[0m : 1.79894
[1mStep[0m  [56/84], [94mLoss[0m : 1.75802
[1mStep[0m  [64/84], [94mLoss[0m : 1.71900
[1mStep[0m  [72/84], [94mLoss[0m : 1.66014
[1mStep[0m  [80/84], [94mLoss[0m : 1.55354

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.700, [92mTest[0m: 2.517, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.57516
[1mStep[0m  [8/84], [94mLoss[0m : 1.57865
[1mStep[0m  [16/84], [94mLoss[0m : 1.69870
[1mStep[0m  [24/84], [94mLoss[0m : 1.75568
[1mStep[0m  [32/84], [94mLoss[0m : 1.48611
[1mStep[0m  [40/84], [94mLoss[0m : 1.86955
[1mStep[0m  [48/84], [94mLoss[0m : 1.67760
[1mStep[0m  [56/84], [94mLoss[0m : 1.74187
[1mStep[0m  [64/84], [94mLoss[0m : 1.66804
[1mStep[0m  [72/84], [94mLoss[0m : 1.80617
[1mStep[0m  [80/84], [94mLoss[0m : 1.61583

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.673, [92mTest[0m: 2.495, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.52211
[1mStep[0m  [8/84], [94mLoss[0m : 1.68243
[1mStep[0m  [16/84], [94mLoss[0m : 1.53008
[1mStep[0m  [24/84], [94mLoss[0m : 1.63912
[1mStep[0m  [32/84], [94mLoss[0m : 1.55626
[1mStep[0m  [40/84], [94mLoss[0m : 1.52270
[1mStep[0m  [48/84], [94mLoss[0m : 1.61786
[1mStep[0m  [56/84], [94mLoss[0m : 1.41424
[1mStep[0m  [64/84], [94mLoss[0m : 1.67360
[1mStep[0m  [72/84], [94mLoss[0m : 1.72126
[1mStep[0m  [80/84], [94mLoss[0m : 1.77138

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.640, [92mTest[0m: 2.486, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.70821
[1mStep[0m  [8/84], [94mLoss[0m : 1.62825
[1mStep[0m  [16/84], [94mLoss[0m : 1.61797
[1mStep[0m  [24/84], [94mLoss[0m : 1.80828
[1mStep[0m  [32/84], [94mLoss[0m : 1.47029
[1mStep[0m  [40/84], [94mLoss[0m : 1.55181
[1mStep[0m  [48/84], [94mLoss[0m : 1.55482
[1mStep[0m  [56/84], [94mLoss[0m : 2.11939
[1mStep[0m  [64/84], [94mLoss[0m : 1.52926
[1mStep[0m  [72/84], [94mLoss[0m : 1.43229
[1mStep[0m  [80/84], [94mLoss[0m : 1.63967

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.599, [92mTest[0m: 2.484, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.52705
[1mStep[0m  [8/84], [94mLoss[0m : 1.42098
[1mStep[0m  [16/84], [94mLoss[0m : 1.68203
[1mStep[0m  [24/84], [94mLoss[0m : 1.49012
[1mStep[0m  [32/84], [94mLoss[0m : 1.36104
[1mStep[0m  [40/84], [94mLoss[0m : 1.59948
[1mStep[0m  [48/84], [94mLoss[0m : 1.52870
[1mStep[0m  [56/84], [94mLoss[0m : 1.44514
[1mStep[0m  [64/84], [94mLoss[0m : 1.67797
[1mStep[0m  [72/84], [94mLoss[0m : 1.56363
[1mStep[0m  [80/84], [94mLoss[0m : 1.41348

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.594, [92mTest[0m: 2.502, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.40544
[1mStep[0m  [8/84], [94mLoss[0m : 1.41255
[1mStep[0m  [16/84], [94mLoss[0m : 1.28131
[1mStep[0m  [24/84], [94mLoss[0m : 1.73507
[1mStep[0m  [32/84], [94mLoss[0m : 1.57066
[1mStep[0m  [40/84], [94mLoss[0m : 1.48944
[1mStep[0m  [48/84], [94mLoss[0m : 1.50344
[1mStep[0m  [56/84], [94mLoss[0m : 1.63103
[1mStep[0m  [64/84], [94mLoss[0m : 1.66647
[1mStep[0m  [72/84], [94mLoss[0m : 1.68270
[1mStep[0m  [80/84], [94mLoss[0m : 1.74525

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.563, [92mTest[0m: 2.509, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.65438
[1mStep[0m  [8/84], [94mLoss[0m : 1.54951
[1mStep[0m  [16/84], [94mLoss[0m : 1.45339
[1mStep[0m  [24/84], [94mLoss[0m : 1.52018
[1mStep[0m  [32/84], [94mLoss[0m : 1.49165
[1mStep[0m  [40/84], [94mLoss[0m : 1.55601
[1mStep[0m  [48/84], [94mLoss[0m : 1.48478
[1mStep[0m  [56/84], [94mLoss[0m : 1.47998
[1mStep[0m  [64/84], [94mLoss[0m : 1.45876
[1mStep[0m  [72/84], [94mLoss[0m : 1.44025
[1mStep[0m  [80/84], [94mLoss[0m : 1.75853

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.541, [92mTest[0m: 2.486, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.44142
[1mStep[0m  [8/84], [94mLoss[0m : 1.69623
[1mStep[0m  [16/84], [94mLoss[0m : 1.79595
[1mStep[0m  [24/84], [94mLoss[0m : 1.52741
[1mStep[0m  [32/84], [94mLoss[0m : 1.63588
[1mStep[0m  [40/84], [94mLoss[0m : 1.90054
[1mStep[0m  [48/84], [94mLoss[0m : 1.53098
[1mStep[0m  [56/84], [94mLoss[0m : 1.39128
[1mStep[0m  [64/84], [94mLoss[0m : 1.55556
[1mStep[0m  [72/84], [94mLoss[0m : 1.62586
[1mStep[0m  [80/84], [94mLoss[0m : 1.59182

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.530, [92mTest[0m: 2.529, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.60791
[1mStep[0m  [8/84], [94mLoss[0m : 1.55871
[1mStep[0m  [16/84], [94mLoss[0m : 1.55871
[1mStep[0m  [24/84], [94mLoss[0m : 1.42459
[1mStep[0m  [32/84], [94mLoss[0m : 1.52209
[1mStep[0m  [40/84], [94mLoss[0m : 1.47119
[1mStep[0m  [48/84], [94mLoss[0m : 1.41605
[1mStep[0m  [56/84], [94mLoss[0m : 1.56948
[1mStep[0m  [64/84], [94mLoss[0m : 1.87062
[1mStep[0m  [72/84], [94mLoss[0m : 1.57441
[1mStep[0m  [80/84], [94mLoss[0m : 1.59969

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.528, [92mTest[0m: 2.488, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.72103
[1mStep[0m  [8/84], [94mLoss[0m : 1.32611
[1mStep[0m  [16/84], [94mLoss[0m : 1.59069
[1mStep[0m  [24/84], [94mLoss[0m : 1.52501
[1mStep[0m  [32/84], [94mLoss[0m : 1.48851
[1mStep[0m  [40/84], [94mLoss[0m : 1.35964
[1mStep[0m  [48/84], [94mLoss[0m : 1.34920
[1mStep[0m  [56/84], [94mLoss[0m : 1.48085
[1mStep[0m  [64/84], [94mLoss[0m : 1.35749
[1mStep[0m  [72/84], [94mLoss[0m : 1.42904
[1mStep[0m  [80/84], [94mLoss[0m : 1.36342

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.506, [92mTest[0m: 2.481, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.34615
[1mStep[0m  [8/84], [94mLoss[0m : 1.28780
[1mStep[0m  [16/84], [94mLoss[0m : 1.50600
[1mStep[0m  [24/84], [94mLoss[0m : 1.53492
[1mStep[0m  [32/84], [94mLoss[0m : 1.51340
[1mStep[0m  [40/84], [94mLoss[0m : 1.42484
[1mStep[0m  [48/84], [94mLoss[0m : 1.55322
[1mStep[0m  [56/84], [94mLoss[0m : 1.40139
[1mStep[0m  [64/84], [94mLoss[0m : 1.89976
[1mStep[0m  [72/84], [94mLoss[0m : 1.50350
[1mStep[0m  [80/84], [94mLoss[0m : 1.18890

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.493, [92mTest[0m: 2.520, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.55767
[1mStep[0m  [8/84], [94mLoss[0m : 1.41411
[1mStep[0m  [16/84], [94mLoss[0m : 1.48709
[1mStep[0m  [24/84], [94mLoss[0m : 1.65539
[1mStep[0m  [32/84], [94mLoss[0m : 1.47942
[1mStep[0m  [40/84], [94mLoss[0m : 1.26614
[1mStep[0m  [48/84], [94mLoss[0m : 1.56853
[1mStep[0m  [56/84], [94mLoss[0m : 1.68890
[1mStep[0m  [64/84], [94mLoss[0m : 1.56780
[1mStep[0m  [72/84], [94mLoss[0m : 1.54357
[1mStep[0m  [80/84], [94mLoss[0m : 1.34204

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.475, [92mTest[0m: 2.517, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.568
====================================

Phase 2 - Evaluation MAE:  2.5682510222707475
MAE score P1        2.335439
MAE score P2        2.568251
loss                1.474685
learning_rate       0.007525
batch_size               128
hidden_sizes      [300, 100]
epochs                    30
activation              relu
optimizer                sgd
early stopping         False
dropout                  0.2
momentum                 0.1
weight_decay            0.01
Name: 6, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 10.63670
[1mStep[0m  [4/42], [94mLoss[0m : 10.96583
[1mStep[0m  [8/42], [94mLoss[0m : 10.79660
[1mStep[0m  [12/42], [94mLoss[0m : 10.32884
[1mStep[0m  [16/42], [94mLoss[0m : 10.71859
[1mStep[0m  [20/42], [94mLoss[0m : 10.76501
[1mStep[0m  [24/42], [94mLoss[0m : 10.43428
[1mStep[0m  [28/42], [94mLoss[0m : 10.42978
[1mStep[0m  [32/42], [94mLoss[0m : 10.40984
[1mStep[0m  [36/42], [94mLoss[0m : 10.35655
[1mStep[0m  [40/42], [94mLoss[0m : 10.21898

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.561, [92mTest[0m: 10.934, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.57436
[1mStep[0m  [4/42], [94mLoss[0m : 9.76740
[1mStep[0m  [8/42], [94mLoss[0m : 9.67837
[1mStep[0m  [12/42], [94mLoss[0m : 9.86158
[1mStep[0m  [16/42], [94mLoss[0m : 10.03784
[1mStep[0m  [20/42], [94mLoss[0m : 9.79774
[1mStep[0m  [24/42], [94mLoss[0m : 9.74987
[1mStep[0m  [28/42], [94mLoss[0m : 9.81559
[1mStep[0m  [32/42], [94mLoss[0m : 9.12922
[1mStep[0m  [36/42], [94mLoss[0m : 9.57183
[1mStep[0m  [40/42], [94mLoss[0m : 9.16899

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.752, [92mTest[0m: 10.031, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.34431
[1mStep[0m  [4/42], [94mLoss[0m : 8.61021
[1mStep[0m  [8/42], [94mLoss[0m : 8.91652
[1mStep[0m  [12/42], [94mLoss[0m : 9.03866
[1mStep[0m  [16/42], [94mLoss[0m : 8.96802
[1mStep[0m  [20/42], [94mLoss[0m : 8.60033
[1mStep[0m  [24/42], [94mLoss[0m : 8.53194
[1mStep[0m  [28/42], [94mLoss[0m : 8.70751
[1mStep[0m  [32/42], [94mLoss[0m : 8.40148
[1mStep[0m  [36/42], [94mLoss[0m : 8.24429
[1mStep[0m  [40/42], [94mLoss[0m : 7.60663

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 8.744, [92mTest[0m: 9.038, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.20453
[1mStep[0m  [4/42], [94mLoss[0m : 8.29039
[1mStep[0m  [8/42], [94mLoss[0m : 7.44071
[1mStep[0m  [12/42], [94mLoss[0m : 7.62143
[1mStep[0m  [16/42], [94mLoss[0m : 7.24820
[1mStep[0m  [20/42], [94mLoss[0m : 7.47427
[1mStep[0m  [24/42], [94mLoss[0m : 7.02380
[1mStep[0m  [28/42], [94mLoss[0m : 6.78769
[1mStep[0m  [32/42], [94mLoss[0m : 6.52986
[1mStep[0m  [36/42], [94mLoss[0m : 6.58428
[1mStep[0m  [40/42], [94mLoss[0m : 6.16253

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 7.321, [92mTest[0m: 7.653, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.20863
[1mStep[0m  [4/42], [94mLoss[0m : 6.73779
[1mStep[0m  [8/42], [94mLoss[0m : 6.09124
[1mStep[0m  [12/42], [94mLoss[0m : 6.26916
[1mStep[0m  [16/42], [94mLoss[0m : 6.28536
[1mStep[0m  [20/42], [94mLoss[0m : 6.34090
[1mStep[0m  [24/42], [94mLoss[0m : 5.85427
[1mStep[0m  [28/42], [94mLoss[0m : 5.64598
[1mStep[0m  [32/42], [94mLoss[0m : 5.53198
[1mStep[0m  [36/42], [94mLoss[0m : 5.00907
[1mStep[0m  [40/42], [94mLoss[0m : 5.51391

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 5.856, [92mTest[0m: 6.021, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.15263
[1mStep[0m  [4/42], [94mLoss[0m : 4.64587
[1mStep[0m  [8/42], [94mLoss[0m : 5.01054
[1mStep[0m  [12/42], [94mLoss[0m : 4.84998
[1mStep[0m  [16/42], [94mLoss[0m : 4.54269
[1mStep[0m  [20/42], [94mLoss[0m : 4.36560
[1mStep[0m  [24/42], [94mLoss[0m : 4.17241
[1mStep[0m  [28/42], [94mLoss[0m : 3.96742
[1mStep[0m  [32/42], [94mLoss[0m : 3.93333
[1mStep[0m  [36/42], [94mLoss[0m : 3.73100
[1mStep[0m  [40/42], [94mLoss[0m : 3.49967

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 4.448, [92mTest[0m: 4.250, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.67587
[1mStep[0m  [4/42], [94mLoss[0m : 3.38566
[1mStep[0m  [8/42], [94mLoss[0m : 3.78692
[1mStep[0m  [12/42], [94mLoss[0m : 3.58773
[1mStep[0m  [16/42], [94mLoss[0m : 3.32091
[1mStep[0m  [20/42], [94mLoss[0m : 3.63723
[1mStep[0m  [24/42], [94mLoss[0m : 3.09936
[1mStep[0m  [28/42], [94mLoss[0m : 3.36546
[1mStep[0m  [32/42], [94mLoss[0m : 3.33443
[1mStep[0m  [36/42], [94mLoss[0m : 2.96746
[1mStep[0m  [40/42], [94mLoss[0m : 3.13639

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 3.383, [92mTest[0m: 3.127, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.86841
[1mStep[0m  [4/42], [94mLoss[0m : 3.20975
[1mStep[0m  [8/42], [94mLoss[0m : 2.80873
[1mStep[0m  [12/42], [94mLoss[0m : 3.18641
[1mStep[0m  [16/42], [94mLoss[0m : 3.00897
[1mStep[0m  [20/42], [94mLoss[0m : 3.16164
[1mStep[0m  [24/42], [94mLoss[0m : 2.75402
[1mStep[0m  [28/42], [94mLoss[0m : 2.94351
[1mStep[0m  [32/42], [94mLoss[0m : 3.03071
[1mStep[0m  [36/42], [94mLoss[0m : 3.01443
[1mStep[0m  [40/42], [94mLoss[0m : 2.71947

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.953, [92mTest[0m: 2.523, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.85258
[1mStep[0m  [4/42], [94mLoss[0m : 2.91380
[1mStep[0m  [8/42], [94mLoss[0m : 2.68189
[1mStep[0m  [12/42], [94mLoss[0m : 2.81510
[1mStep[0m  [16/42], [94mLoss[0m : 2.99844
[1mStep[0m  [20/42], [94mLoss[0m : 3.02390
[1mStep[0m  [24/42], [94mLoss[0m : 2.62277
[1mStep[0m  [28/42], [94mLoss[0m : 2.72392
[1mStep[0m  [32/42], [94mLoss[0m : 2.93837
[1mStep[0m  [36/42], [94mLoss[0m : 2.87578
[1mStep[0m  [40/42], [94mLoss[0m : 2.89615

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.829, [92mTest[0m: 2.409, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57043
[1mStep[0m  [4/42], [94mLoss[0m : 2.95788
[1mStep[0m  [8/42], [94mLoss[0m : 2.89127
[1mStep[0m  [12/42], [94mLoss[0m : 2.71256
[1mStep[0m  [16/42], [94mLoss[0m : 2.98060
[1mStep[0m  [20/42], [94mLoss[0m : 2.56266
[1mStep[0m  [24/42], [94mLoss[0m : 2.74848
[1mStep[0m  [28/42], [94mLoss[0m : 2.75094
[1mStep[0m  [32/42], [94mLoss[0m : 2.97848
[1mStep[0m  [36/42], [94mLoss[0m : 2.92200
[1mStep[0m  [40/42], [94mLoss[0m : 2.82364

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.834, [92mTest[0m: 2.392, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.72786
[1mStep[0m  [4/42], [94mLoss[0m : 2.76935
[1mStep[0m  [8/42], [94mLoss[0m : 2.98724
[1mStep[0m  [12/42], [94mLoss[0m : 2.78319
[1mStep[0m  [16/42], [94mLoss[0m : 3.00872
[1mStep[0m  [20/42], [94mLoss[0m : 2.87299
[1mStep[0m  [24/42], [94mLoss[0m : 2.98055
[1mStep[0m  [28/42], [94mLoss[0m : 2.73518
[1mStep[0m  [32/42], [94mLoss[0m : 2.70847
[1mStep[0m  [36/42], [94mLoss[0m : 2.70199
[1mStep[0m  [40/42], [94mLoss[0m : 2.72766

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.825, [92mTest[0m: 2.368, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54826
[1mStep[0m  [4/42], [94mLoss[0m : 2.72863
[1mStep[0m  [8/42], [94mLoss[0m : 2.60719
[1mStep[0m  [12/42], [94mLoss[0m : 2.98651
[1mStep[0m  [16/42], [94mLoss[0m : 2.78780
[1mStep[0m  [20/42], [94mLoss[0m : 2.81726
[1mStep[0m  [24/42], [94mLoss[0m : 2.78081
[1mStep[0m  [28/42], [94mLoss[0m : 2.76046
[1mStep[0m  [32/42], [94mLoss[0m : 2.76617
[1mStep[0m  [36/42], [94mLoss[0m : 2.73468
[1mStep[0m  [40/42], [94mLoss[0m : 2.68458

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.762, [92mTest[0m: 2.361, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.71698
[1mStep[0m  [4/42], [94mLoss[0m : 2.68955
[1mStep[0m  [8/42], [94mLoss[0m : 2.88224
[1mStep[0m  [12/42], [94mLoss[0m : 2.85366
[1mStep[0m  [16/42], [94mLoss[0m : 3.06042
[1mStep[0m  [20/42], [94mLoss[0m : 2.90739
[1mStep[0m  [24/42], [94mLoss[0m : 2.59071
[1mStep[0m  [28/42], [94mLoss[0m : 2.83784
[1mStep[0m  [32/42], [94mLoss[0m : 2.59461
[1mStep[0m  [36/42], [94mLoss[0m : 2.83187
[1mStep[0m  [40/42], [94mLoss[0m : 2.99361

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.759, [92mTest[0m: 2.338, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.81663
[1mStep[0m  [4/42], [94mLoss[0m : 2.72897
[1mStep[0m  [8/42], [94mLoss[0m : 2.50253
[1mStep[0m  [12/42], [94mLoss[0m : 2.65303
[1mStep[0m  [16/42], [94mLoss[0m : 2.93997
[1mStep[0m  [20/42], [94mLoss[0m : 2.93672
[1mStep[0m  [24/42], [94mLoss[0m : 2.79997
[1mStep[0m  [28/42], [94mLoss[0m : 2.91106
[1mStep[0m  [32/42], [94mLoss[0m : 2.84265
[1mStep[0m  [36/42], [94mLoss[0m : 2.54637
[1mStep[0m  [40/42], [94mLoss[0m : 2.87643

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.727, [92mTest[0m: 2.344, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.94875
[1mStep[0m  [4/42], [94mLoss[0m : 2.58748
[1mStep[0m  [8/42], [94mLoss[0m : 2.85004
[1mStep[0m  [12/42], [94mLoss[0m : 2.55597
[1mStep[0m  [16/42], [94mLoss[0m : 2.87454
[1mStep[0m  [20/42], [94mLoss[0m : 2.83633
[1mStep[0m  [24/42], [94mLoss[0m : 2.52133
[1mStep[0m  [28/42], [94mLoss[0m : 2.73002
[1mStep[0m  [32/42], [94mLoss[0m : 2.89330
[1mStep[0m  [36/42], [94mLoss[0m : 2.57558
[1mStep[0m  [40/42], [94mLoss[0m : 2.69593

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.728, [92mTest[0m: 2.331, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.77612
[1mStep[0m  [4/42], [94mLoss[0m : 2.86257
[1mStep[0m  [8/42], [94mLoss[0m : 2.49463
[1mStep[0m  [12/42], [94mLoss[0m : 2.46349
[1mStep[0m  [16/42], [94mLoss[0m : 2.83570
[1mStep[0m  [20/42], [94mLoss[0m : 2.59911
[1mStep[0m  [24/42], [94mLoss[0m : 2.65655
[1mStep[0m  [28/42], [94mLoss[0m : 2.58663
[1mStep[0m  [32/42], [94mLoss[0m : 3.01131
[1mStep[0m  [36/42], [94mLoss[0m : 2.87816
[1mStep[0m  [40/42], [94mLoss[0m : 2.85296

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.713, [92mTest[0m: 2.342, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.76210
[1mStep[0m  [4/42], [94mLoss[0m : 2.81401
[1mStep[0m  [8/42], [94mLoss[0m : 2.69726
[1mStep[0m  [12/42], [94mLoss[0m : 2.77570
[1mStep[0m  [16/42], [94mLoss[0m : 2.62864
[1mStep[0m  [20/42], [94mLoss[0m : 2.76015
[1mStep[0m  [24/42], [94mLoss[0m : 2.71663
[1mStep[0m  [28/42], [94mLoss[0m : 3.00299
[1mStep[0m  [32/42], [94mLoss[0m : 2.66620
[1mStep[0m  [36/42], [94mLoss[0m : 2.53937
[1mStep[0m  [40/42], [94mLoss[0m : 2.62850

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.707, [92mTest[0m: 2.323, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.82008
[1mStep[0m  [4/42], [94mLoss[0m : 2.58258
[1mStep[0m  [8/42], [94mLoss[0m : 2.68569
[1mStep[0m  [12/42], [94mLoss[0m : 2.83843
[1mStep[0m  [16/42], [94mLoss[0m : 2.67171
[1mStep[0m  [20/42], [94mLoss[0m : 2.55349
[1mStep[0m  [24/42], [94mLoss[0m : 2.61992
[1mStep[0m  [28/42], [94mLoss[0m : 2.74796
[1mStep[0m  [32/42], [94mLoss[0m : 2.76569
[1mStep[0m  [36/42], [94mLoss[0m : 2.73081
[1mStep[0m  [40/42], [94mLoss[0m : 2.73795

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.677, [92mTest[0m: 2.334, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.71496
[1mStep[0m  [4/42], [94mLoss[0m : 2.66522
[1mStep[0m  [8/42], [94mLoss[0m : 2.99330
[1mStep[0m  [12/42], [94mLoss[0m : 2.76438
[1mStep[0m  [16/42], [94mLoss[0m : 2.89231
[1mStep[0m  [20/42], [94mLoss[0m : 2.75485
[1mStep[0m  [24/42], [94mLoss[0m : 2.90445
[1mStep[0m  [28/42], [94mLoss[0m : 2.82786
[1mStep[0m  [32/42], [94mLoss[0m : 2.39878
[1mStep[0m  [36/42], [94mLoss[0m : 2.62499
[1mStep[0m  [40/42], [94mLoss[0m : 2.57075

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.673, [92mTest[0m: 2.332, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61774
[1mStep[0m  [4/42], [94mLoss[0m : 2.60047
[1mStep[0m  [8/42], [94mLoss[0m : 2.68333
[1mStep[0m  [12/42], [94mLoss[0m : 2.72880
[1mStep[0m  [16/42], [94mLoss[0m : 2.59285
[1mStep[0m  [20/42], [94mLoss[0m : 2.63815
[1mStep[0m  [24/42], [94mLoss[0m : 2.58719
[1mStep[0m  [28/42], [94mLoss[0m : 2.65874
[1mStep[0m  [32/42], [94mLoss[0m : 2.58799
[1mStep[0m  [36/42], [94mLoss[0m : 2.51136
[1mStep[0m  [40/42], [94mLoss[0m : 2.96782

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.662, [92mTest[0m: 2.336, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.78429
[1mStep[0m  [4/42], [94mLoss[0m : 2.65624
[1mStep[0m  [8/42], [94mLoss[0m : 2.60450
[1mStep[0m  [12/42], [94mLoss[0m : 2.53320
[1mStep[0m  [16/42], [94mLoss[0m : 2.78363
[1mStep[0m  [20/42], [94mLoss[0m : 2.49599
[1mStep[0m  [24/42], [94mLoss[0m : 2.52320
[1mStep[0m  [28/42], [94mLoss[0m : 2.58394
[1mStep[0m  [32/42], [94mLoss[0m : 2.53974
[1mStep[0m  [36/42], [94mLoss[0m : 2.58768
[1mStep[0m  [40/42], [94mLoss[0m : 2.51940

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.637, [92mTest[0m: 2.328, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56766
[1mStep[0m  [4/42], [94mLoss[0m : 2.58773
[1mStep[0m  [8/42], [94mLoss[0m : 2.80827
[1mStep[0m  [12/42], [94mLoss[0m : 2.69123
[1mStep[0m  [16/42], [94mLoss[0m : 2.59480
[1mStep[0m  [20/42], [94mLoss[0m : 2.68963
[1mStep[0m  [24/42], [94mLoss[0m : 2.67288
[1mStep[0m  [28/42], [94mLoss[0m : 2.63334
[1mStep[0m  [32/42], [94mLoss[0m : 2.74539
[1mStep[0m  [36/42], [94mLoss[0m : 2.70452
[1mStep[0m  [40/42], [94mLoss[0m : 2.68201

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.628, [92mTest[0m: 2.325, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59251
[1mStep[0m  [4/42], [94mLoss[0m : 2.52614
[1mStep[0m  [8/42], [94mLoss[0m : 2.54277
[1mStep[0m  [12/42], [94mLoss[0m : 2.77885
[1mStep[0m  [16/42], [94mLoss[0m : 2.61816
[1mStep[0m  [20/42], [94mLoss[0m : 2.66148
[1mStep[0m  [24/42], [94mLoss[0m : 2.47910
[1mStep[0m  [28/42], [94mLoss[0m : 2.34069
[1mStep[0m  [32/42], [94mLoss[0m : 2.72709
[1mStep[0m  [36/42], [94mLoss[0m : 2.82024
[1mStep[0m  [40/42], [94mLoss[0m : 2.40828

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.616, [92mTest[0m: 2.332, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.73321
[1mStep[0m  [4/42], [94mLoss[0m : 2.49624
[1mStep[0m  [8/42], [94mLoss[0m : 2.63145
[1mStep[0m  [12/42], [94mLoss[0m : 2.64549
[1mStep[0m  [16/42], [94mLoss[0m : 2.30163
[1mStep[0m  [20/42], [94mLoss[0m : 2.61830
[1mStep[0m  [24/42], [94mLoss[0m : 2.66638
[1mStep[0m  [28/42], [94mLoss[0m : 2.59351
[1mStep[0m  [32/42], [94mLoss[0m : 2.78673
[1mStep[0m  [36/42], [94mLoss[0m : 2.71674
[1mStep[0m  [40/42], [94mLoss[0m : 2.56977

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.610, [92mTest[0m: 2.332, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.68183
[1mStep[0m  [4/42], [94mLoss[0m : 2.67426
[1mStep[0m  [8/42], [94mLoss[0m : 2.89352
[1mStep[0m  [12/42], [94mLoss[0m : 2.38311
[1mStep[0m  [16/42], [94mLoss[0m : 2.46477
[1mStep[0m  [20/42], [94mLoss[0m : 2.66418
[1mStep[0m  [24/42], [94mLoss[0m : 2.71462
[1mStep[0m  [28/42], [94mLoss[0m : 2.62661
[1mStep[0m  [32/42], [94mLoss[0m : 2.72573
[1mStep[0m  [36/42], [94mLoss[0m : 2.62024
[1mStep[0m  [40/42], [94mLoss[0m : 2.52888

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.595, [92mTest[0m: 2.331, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60640
[1mStep[0m  [4/42], [94mLoss[0m : 2.70835
[1mStep[0m  [8/42], [94mLoss[0m : 2.72256
[1mStep[0m  [12/42], [94mLoss[0m : 2.71177
[1mStep[0m  [16/42], [94mLoss[0m : 2.60461
[1mStep[0m  [20/42], [94mLoss[0m : 2.51538
[1mStep[0m  [24/42], [94mLoss[0m : 2.61740
[1mStep[0m  [28/42], [94mLoss[0m : 2.56302
[1mStep[0m  [32/42], [94mLoss[0m : 2.54821
[1mStep[0m  [36/42], [94mLoss[0m : 2.63806
[1mStep[0m  [40/42], [94mLoss[0m : 2.80428

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.618, [92mTest[0m: 2.323, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56814
[1mStep[0m  [4/42], [94mLoss[0m : 2.67785
[1mStep[0m  [8/42], [94mLoss[0m : 2.68378
[1mStep[0m  [12/42], [94mLoss[0m : 2.71332
[1mStep[0m  [16/42], [94mLoss[0m : 2.56133
[1mStep[0m  [20/42], [94mLoss[0m : 2.40075
[1mStep[0m  [24/42], [94mLoss[0m : 2.45320
[1mStep[0m  [28/42], [94mLoss[0m : 2.56664
[1mStep[0m  [32/42], [94mLoss[0m : 2.71939
[1mStep[0m  [36/42], [94mLoss[0m : 2.54334
[1mStep[0m  [40/42], [94mLoss[0m : 2.67881

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.611, [92mTest[0m: 2.323, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41930
[1mStep[0m  [4/42], [94mLoss[0m : 2.54896
[1mStep[0m  [8/42], [94mLoss[0m : 2.80398
[1mStep[0m  [12/42], [94mLoss[0m : 2.51283
[1mStep[0m  [16/42], [94mLoss[0m : 2.62717
[1mStep[0m  [20/42], [94mLoss[0m : 2.59879
[1mStep[0m  [24/42], [94mLoss[0m : 2.42838
[1mStep[0m  [28/42], [94mLoss[0m : 2.79302
[1mStep[0m  [32/42], [94mLoss[0m : 2.70001
[1mStep[0m  [36/42], [94mLoss[0m : 2.63522
[1mStep[0m  [40/42], [94mLoss[0m : 2.45588

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.593, [92mTest[0m: 2.319, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60753
[1mStep[0m  [4/42], [94mLoss[0m : 2.70817
[1mStep[0m  [8/42], [94mLoss[0m : 2.81762
[1mStep[0m  [12/42], [94mLoss[0m : 2.60026
[1mStep[0m  [16/42], [94mLoss[0m : 2.56434
[1mStep[0m  [20/42], [94mLoss[0m : 2.53762
[1mStep[0m  [24/42], [94mLoss[0m : 2.41588
[1mStep[0m  [28/42], [94mLoss[0m : 2.66326
[1mStep[0m  [32/42], [94mLoss[0m : 2.57405
[1mStep[0m  [36/42], [94mLoss[0m : 2.59439
[1mStep[0m  [40/42], [94mLoss[0m : 2.40601

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.585, [92mTest[0m: 2.326, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40464
[1mStep[0m  [4/42], [94mLoss[0m : 2.45275
[1mStep[0m  [8/42], [94mLoss[0m : 2.61410
[1mStep[0m  [12/42], [94mLoss[0m : 2.46790
[1mStep[0m  [16/42], [94mLoss[0m : 2.52027
[1mStep[0m  [20/42], [94mLoss[0m : 2.75355
[1mStep[0m  [24/42], [94mLoss[0m : 2.32232
[1mStep[0m  [28/42], [94mLoss[0m : 2.81398
[1mStep[0m  [32/42], [94mLoss[0m : 2.71646
[1mStep[0m  [36/42], [94mLoss[0m : 2.42381
[1mStep[0m  [40/42], [94mLoss[0m : 2.66910

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.573, [92mTest[0m: 2.319, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.325
====================================

Phase 1 - Evaluation MAE:  2.3251096180507114
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 2.41155
[1mStep[0m  [4/42], [94mLoss[0m : 2.63339
[1mStep[0m  [8/42], [94mLoss[0m : 2.45097
[1mStep[0m  [12/42], [94mLoss[0m : 2.58399
[1mStep[0m  [16/42], [94mLoss[0m : 2.69442
[1mStep[0m  [20/42], [94mLoss[0m : 2.41248
[1mStep[0m  [24/42], [94mLoss[0m : 2.61199
[1mStep[0m  [28/42], [94mLoss[0m : 2.63071
[1mStep[0m  [32/42], [94mLoss[0m : 2.56515
[1mStep[0m  [36/42], [94mLoss[0m : 2.70618
[1mStep[0m  [40/42], [94mLoss[0m : 2.68497

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.630, [92mTest[0m: 2.325, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41946
[1mStep[0m  [4/42], [94mLoss[0m : 2.64825
[1mStep[0m  [8/42], [94mLoss[0m : 2.44691
[1mStep[0m  [12/42], [94mLoss[0m : 2.54411
[1mStep[0m  [16/42], [94mLoss[0m : 2.60640
[1mStep[0m  [20/42], [94mLoss[0m : 2.87948
[1mStep[0m  [24/42], [94mLoss[0m : 2.61806
[1mStep[0m  [28/42], [94mLoss[0m : 2.47312
[1mStep[0m  [32/42], [94mLoss[0m : 2.66201
[1mStep[0m  [36/42], [94mLoss[0m : 2.57642
[1mStep[0m  [40/42], [94mLoss[0m : 2.46442

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.566, [92mTest[0m: 2.327, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55460
[1mStep[0m  [4/42], [94mLoss[0m : 2.46367
[1mStep[0m  [8/42], [94mLoss[0m : 2.54836
[1mStep[0m  [12/42], [94mLoss[0m : 2.45466
[1mStep[0m  [16/42], [94mLoss[0m : 2.65241
[1mStep[0m  [20/42], [94mLoss[0m : 2.45748
[1mStep[0m  [24/42], [94mLoss[0m : 2.56839
[1mStep[0m  [28/42], [94mLoss[0m : 2.42372
[1mStep[0m  [32/42], [94mLoss[0m : 2.24787
[1mStep[0m  [36/42], [94mLoss[0m : 2.41276
[1mStep[0m  [40/42], [94mLoss[0m : 2.52410

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.380, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41436
[1mStep[0m  [4/42], [94mLoss[0m : 2.60387
[1mStep[0m  [8/42], [94mLoss[0m : 2.55095
[1mStep[0m  [12/42], [94mLoss[0m : 2.63433
[1mStep[0m  [16/42], [94mLoss[0m : 2.50284
[1mStep[0m  [20/42], [94mLoss[0m : 2.34842
[1mStep[0m  [24/42], [94mLoss[0m : 2.44222
[1mStep[0m  [28/42], [94mLoss[0m : 2.41844
[1mStep[0m  [32/42], [94mLoss[0m : 2.55011
[1mStep[0m  [36/42], [94mLoss[0m : 2.52542
[1mStep[0m  [40/42], [94mLoss[0m : 2.64110

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.434, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50649
[1mStep[0m  [4/42], [94mLoss[0m : 2.16584
[1mStep[0m  [8/42], [94mLoss[0m : 2.53840
[1mStep[0m  [12/42], [94mLoss[0m : 2.61914
[1mStep[0m  [16/42], [94mLoss[0m : 2.69289
[1mStep[0m  [20/42], [94mLoss[0m : 2.54335
[1mStep[0m  [24/42], [94mLoss[0m : 2.37027
[1mStep[0m  [28/42], [94mLoss[0m : 2.44061
[1mStep[0m  [32/42], [94mLoss[0m : 2.37408
[1mStep[0m  [36/42], [94mLoss[0m : 2.42492
[1mStep[0m  [40/42], [94mLoss[0m : 2.48880

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.410, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36001
[1mStep[0m  [4/42], [94mLoss[0m : 2.29941
[1mStep[0m  [8/42], [94mLoss[0m : 2.25685
[1mStep[0m  [12/42], [94mLoss[0m : 2.47886
[1mStep[0m  [16/42], [94mLoss[0m : 2.48485
[1mStep[0m  [20/42], [94mLoss[0m : 2.38569
[1mStep[0m  [24/42], [94mLoss[0m : 2.37807
[1mStep[0m  [28/42], [94mLoss[0m : 2.50105
[1mStep[0m  [32/42], [94mLoss[0m : 2.42745
[1mStep[0m  [36/42], [94mLoss[0m : 2.57693
[1mStep[0m  [40/42], [94mLoss[0m : 2.47686

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.375, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.30199
[1mStep[0m  [4/42], [94mLoss[0m : 2.44926
[1mStep[0m  [8/42], [94mLoss[0m : 2.30441
[1mStep[0m  [12/42], [94mLoss[0m : 2.24500
[1mStep[0m  [16/42], [94mLoss[0m : 2.42241
[1mStep[0m  [20/42], [94mLoss[0m : 2.37390
[1mStep[0m  [24/42], [94mLoss[0m : 2.37970
[1mStep[0m  [28/42], [94mLoss[0m : 2.31933
[1mStep[0m  [32/42], [94mLoss[0m : 2.22041
[1mStep[0m  [36/42], [94mLoss[0m : 2.23520
[1mStep[0m  [40/42], [94mLoss[0m : 2.46607

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.365, [92mTest[0m: 2.367, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21240
[1mStep[0m  [4/42], [94mLoss[0m : 2.17394
[1mStep[0m  [8/42], [94mLoss[0m : 2.41107
[1mStep[0m  [12/42], [94mLoss[0m : 2.29076
[1mStep[0m  [16/42], [94mLoss[0m : 2.23317
[1mStep[0m  [20/42], [94mLoss[0m : 2.15323
[1mStep[0m  [24/42], [94mLoss[0m : 2.43466
[1mStep[0m  [28/42], [94mLoss[0m : 2.20440
[1mStep[0m  [32/42], [94mLoss[0m : 2.30621
[1mStep[0m  [36/42], [94mLoss[0m : 2.50149
[1mStep[0m  [40/42], [94mLoss[0m : 2.53620

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.334, [92mTest[0m: 2.369, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29280
[1mStep[0m  [4/42], [94mLoss[0m : 2.36456
[1mStep[0m  [8/42], [94mLoss[0m : 2.24199
[1mStep[0m  [12/42], [94mLoss[0m : 2.47307
[1mStep[0m  [16/42], [94mLoss[0m : 2.01323
[1mStep[0m  [20/42], [94mLoss[0m : 2.41967
[1mStep[0m  [24/42], [94mLoss[0m : 2.29826
[1mStep[0m  [28/42], [94mLoss[0m : 2.28143
[1mStep[0m  [32/42], [94mLoss[0m : 2.34102
[1mStep[0m  [36/42], [94mLoss[0m : 2.10843
[1mStep[0m  [40/42], [94mLoss[0m : 2.07972

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.306, [92mTest[0m: 2.398, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24924
[1mStep[0m  [4/42], [94mLoss[0m : 2.23793
[1mStep[0m  [8/42], [94mLoss[0m : 2.12175
[1mStep[0m  [12/42], [94mLoss[0m : 2.09588
[1mStep[0m  [16/42], [94mLoss[0m : 2.13910
[1mStep[0m  [20/42], [94mLoss[0m : 2.19664
[1mStep[0m  [24/42], [94mLoss[0m : 2.43707
[1mStep[0m  [28/42], [94mLoss[0m : 2.45010
[1mStep[0m  [32/42], [94mLoss[0m : 2.20396
[1mStep[0m  [36/42], [94mLoss[0m : 2.15969
[1mStep[0m  [40/42], [94mLoss[0m : 2.43421

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.262, [92mTest[0m: 2.378, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.18412
[1mStep[0m  [4/42], [94mLoss[0m : 2.37401
[1mStep[0m  [8/42], [94mLoss[0m : 2.30077
[1mStep[0m  [12/42], [94mLoss[0m : 2.41766
[1mStep[0m  [16/42], [94mLoss[0m : 2.16921
[1mStep[0m  [20/42], [94mLoss[0m : 2.24119
[1mStep[0m  [24/42], [94mLoss[0m : 2.24108
[1mStep[0m  [28/42], [94mLoss[0m : 2.27012
[1mStep[0m  [32/42], [94mLoss[0m : 2.15148
[1mStep[0m  [36/42], [94mLoss[0m : 2.33233
[1mStep[0m  [40/42], [94mLoss[0m : 2.21452

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.227, [92mTest[0m: 2.368, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27182
[1mStep[0m  [4/42], [94mLoss[0m : 2.25702
[1mStep[0m  [8/42], [94mLoss[0m : 2.11322
[1mStep[0m  [12/42], [94mLoss[0m : 2.04680
[1mStep[0m  [16/42], [94mLoss[0m : 2.14924
[1mStep[0m  [20/42], [94mLoss[0m : 2.10438
[1mStep[0m  [24/42], [94mLoss[0m : 2.13796
[1mStep[0m  [28/42], [94mLoss[0m : 2.06853
[1mStep[0m  [32/42], [94mLoss[0m : 2.32846
[1mStep[0m  [36/42], [94mLoss[0m : 2.32428
[1mStep[0m  [40/42], [94mLoss[0m : 2.25139

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.173, [92mTest[0m: 2.415, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.30660
[1mStep[0m  [4/42], [94mLoss[0m : 2.14101
[1mStep[0m  [8/42], [94mLoss[0m : 2.18232
[1mStep[0m  [12/42], [94mLoss[0m : 2.14581
[1mStep[0m  [16/42], [94mLoss[0m : 2.13818
[1mStep[0m  [20/42], [94mLoss[0m : 2.15206
[1mStep[0m  [24/42], [94mLoss[0m : 2.06282
[1mStep[0m  [28/42], [94mLoss[0m : 2.06131
[1mStep[0m  [32/42], [94mLoss[0m : 2.00940
[1mStep[0m  [36/42], [94mLoss[0m : 2.11288
[1mStep[0m  [40/42], [94mLoss[0m : 2.09220

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.152, [92mTest[0m: 2.423, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26642
[1mStep[0m  [4/42], [94mLoss[0m : 2.05032
[1mStep[0m  [8/42], [94mLoss[0m : 2.06280
[1mStep[0m  [12/42], [94mLoss[0m : 2.03065
[1mStep[0m  [16/42], [94mLoss[0m : 2.24939
[1mStep[0m  [20/42], [94mLoss[0m : 2.11672
[1mStep[0m  [24/42], [94mLoss[0m : 1.95172
[1mStep[0m  [28/42], [94mLoss[0m : 2.22261
[1mStep[0m  [32/42], [94mLoss[0m : 2.21991
[1mStep[0m  [36/42], [94mLoss[0m : 2.08236
[1mStep[0m  [40/42], [94mLoss[0m : 2.19940

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.117, [92mTest[0m: 2.364, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28243
[1mStep[0m  [4/42], [94mLoss[0m : 1.97152
[1mStep[0m  [8/42], [94mLoss[0m : 2.24956
[1mStep[0m  [12/42], [94mLoss[0m : 2.22532
[1mStep[0m  [16/42], [94mLoss[0m : 1.83913
[1mStep[0m  [20/42], [94mLoss[0m : 1.90355
[1mStep[0m  [24/42], [94mLoss[0m : 2.03083
[1mStep[0m  [28/42], [94mLoss[0m : 2.13683
[1mStep[0m  [32/42], [94mLoss[0m : 2.07292
[1mStep[0m  [36/42], [94mLoss[0m : 2.08117
[1mStep[0m  [40/42], [94mLoss[0m : 2.07502

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.058, [92mTest[0m: 2.460, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.06192
[1mStep[0m  [4/42], [94mLoss[0m : 1.93158
[1mStep[0m  [8/42], [94mLoss[0m : 2.04289
[1mStep[0m  [12/42], [94mLoss[0m : 1.98430
[1mStep[0m  [16/42], [94mLoss[0m : 1.95738
[1mStep[0m  [20/42], [94mLoss[0m : 1.97818
[1mStep[0m  [24/42], [94mLoss[0m : 1.76332
[1mStep[0m  [28/42], [94mLoss[0m : 1.94347
[1mStep[0m  [32/42], [94mLoss[0m : 2.09064
[1mStep[0m  [36/42], [94mLoss[0m : 2.24529
[1mStep[0m  [40/42], [94mLoss[0m : 2.05679

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.034, [92mTest[0m: 2.403, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.12205
[1mStep[0m  [4/42], [94mLoss[0m : 1.85824
[1mStep[0m  [8/42], [94mLoss[0m : 2.05437
[1mStep[0m  [12/42], [94mLoss[0m : 1.98062
[1mStep[0m  [16/42], [94mLoss[0m : 1.96023
[1mStep[0m  [20/42], [94mLoss[0m : 1.89209
[1mStep[0m  [24/42], [94mLoss[0m : 1.98372
[1mStep[0m  [28/42], [94mLoss[0m : 1.88118
[1mStep[0m  [32/42], [94mLoss[0m : 1.98363
[1mStep[0m  [36/42], [94mLoss[0m : 2.03578
[1mStep[0m  [40/42], [94mLoss[0m : 2.22083

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.002, [92mTest[0m: 2.491, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.02264
[1mStep[0m  [4/42], [94mLoss[0m : 1.96245
[1mStep[0m  [8/42], [94mLoss[0m : 1.85485
[1mStep[0m  [12/42], [94mLoss[0m : 1.89768
[1mStep[0m  [16/42], [94mLoss[0m : 2.12782
[1mStep[0m  [20/42], [94mLoss[0m : 2.04045
[1mStep[0m  [24/42], [94mLoss[0m : 2.06945
[1mStep[0m  [28/42], [94mLoss[0m : 2.13213
[1mStep[0m  [32/42], [94mLoss[0m : 1.99247
[1mStep[0m  [36/42], [94mLoss[0m : 1.97139
[1mStep[0m  [40/42], [94mLoss[0m : 2.00884

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.976, [92mTest[0m: 2.416, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.91930
[1mStep[0m  [4/42], [94mLoss[0m : 1.97342
[1mStep[0m  [8/42], [94mLoss[0m : 2.02059
[1mStep[0m  [12/42], [94mLoss[0m : 1.76635
[1mStep[0m  [16/42], [94mLoss[0m : 1.91004
[1mStep[0m  [20/42], [94mLoss[0m : 1.81407
[1mStep[0m  [24/42], [94mLoss[0m : 2.02540
[1mStep[0m  [28/42], [94mLoss[0m : 2.02125
[1mStep[0m  [32/42], [94mLoss[0m : 1.96421
[1mStep[0m  [36/42], [94mLoss[0m : 1.99285
[1mStep[0m  [40/42], [94mLoss[0m : 1.99744

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.933, [92mTest[0m: 2.538, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.87527
[1mStep[0m  [4/42], [94mLoss[0m : 1.80381
[1mStep[0m  [8/42], [94mLoss[0m : 1.77863
[1mStep[0m  [12/42], [94mLoss[0m : 2.02557
[1mStep[0m  [16/42], [94mLoss[0m : 1.77624
[1mStep[0m  [20/42], [94mLoss[0m : 1.99064
[1mStep[0m  [24/42], [94mLoss[0m : 1.79042
[1mStep[0m  [28/42], [94mLoss[0m : 2.00696
[1mStep[0m  [32/42], [94mLoss[0m : 1.95735
[1mStep[0m  [36/42], [94mLoss[0m : 1.90679
[1mStep[0m  [40/42], [94mLoss[0m : 1.85387

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.880, [92mTest[0m: 2.478, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.81373
[1mStep[0m  [4/42], [94mLoss[0m : 1.70730
[1mStep[0m  [8/42], [94mLoss[0m : 1.77191
[1mStep[0m  [12/42], [94mLoss[0m : 1.79409
[1mStep[0m  [16/42], [94mLoss[0m : 2.02842
[1mStep[0m  [20/42], [94mLoss[0m : 1.94541
[1mStep[0m  [24/42], [94mLoss[0m : 2.07032
[1mStep[0m  [28/42], [94mLoss[0m : 2.06649
[1mStep[0m  [32/42], [94mLoss[0m : 2.02840
[1mStep[0m  [36/42], [94mLoss[0m : 1.71890
[1mStep[0m  [40/42], [94mLoss[0m : 1.82933

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.861, [92mTest[0m: 2.449, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.69026
[1mStep[0m  [4/42], [94mLoss[0m : 2.04084
[1mStep[0m  [8/42], [94mLoss[0m : 1.95982
[1mStep[0m  [12/42], [94mLoss[0m : 1.88543
[1mStep[0m  [16/42], [94mLoss[0m : 1.77516
[1mStep[0m  [20/42], [94mLoss[0m : 1.89499
[1mStep[0m  [24/42], [94mLoss[0m : 1.73003
[1mStep[0m  [28/42], [94mLoss[0m : 1.89455
[1mStep[0m  [32/42], [94mLoss[0m : 1.92654
[1mStep[0m  [36/42], [94mLoss[0m : 1.97178
[1mStep[0m  [40/42], [94mLoss[0m : 1.92017

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.843, [92mTest[0m: 2.445, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.72104
[1mStep[0m  [4/42], [94mLoss[0m : 1.84561
[1mStep[0m  [8/42], [94mLoss[0m : 1.81526
[1mStep[0m  [12/42], [94mLoss[0m : 1.79050
[1mStep[0m  [16/42], [94mLoss[0m : 2.09444
[1mStep[0m  [20/42], [94mLoss[0m : 1.69327
[1mStep[0m  [24/42], [94mLoss[0m : 1.89239
[1mStep[0m  [28/42], [94mLoss[0m : 1.73078
[1mStep[0m  [32/42], [94mLoss[0m : 1.99587
[1mStep[0m  [36/42], [94mLoss[0m : 1.91110
[1mStep[0m  [40/42], [94mLoss[0m : 2.03074

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.817, [92mTest[0m: 2.462, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.62957
[1mStep[0m  [4/42], [94mLoss[0m : 1.78502
[1mStep[0m  [8/42], [94mLoss[0m : 1.71155
[1mStep[0m  [12/42], [94mLoss[0m : 1.66440
[1mStep[0m  [16/42], [94mLoss[0m : 1.86203
[1mStep[0m  [20/42], [94mLoss[0m : 1.73174
[1mStep[0m  [24/42], [94mLoss[0m : 1.64382
[1mStep[0m  [28/42], [94mLoss[0m : 1.74099
[1mStep[0m  [32/42], [94mLoss[0m : 1.72170
[1mStep[0m  [36/42], [94mLoss[0m : 1.80467
[1mStep[0m  [40/42], [94mLoss[0m : 2.11920

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.799, [92mTest[0m: 2.478, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.66389
[1mStep[0m  [4/42], [94mLoss[0m : 1.62198
[1mStep[0m  [8/42], [94mLoss[0m : 1.62975
[1mStep[0m  [12/42], [94mLoss[0m : 1.90304
[1mStep[0m  [16/42], [94mLoss[0m : 1.68879
[1mStep[0m  [20/42], [94mLoss[0m : 1.82920
[1mStep[0m  [24/42], [94mLoss[0m : 1.85661
[1mStep[0m  [28/42], [94mLoss[0m : 1.65759
[1mStep[0m  [32/42], [94mLoss[0m : 1.88877
[1mStep[0m  [36/42], [94mLoss[0m : 1.73376
[1mStep[0m  [40/42], [94mLoss[0m : 1.71874

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.774, [92mTest[0m: 2.457, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.64982
[1mStep[0m  [4/42], [94mLoss[0m : 1.56841
[1mStep[0m  [8/42], [94mLoss[0m : 1.81104
[1mStep[0m  [12/42], [94mLoss[0m : 1.70090
[1mStep[0m  [16/42], [94mLoss[0m : 1.63042
[1mStep[0m  [20/42], [94mLoss[0m : 1.71462
[1mStep[0m  [24/42], [94mLoss[0m : 1.70043
[1mStep[0m  [28/42], [94mLoss[0m : 1.83927
[1mStep[0m  [32/42], [94mLoss[0m : 1.65106
[1mStep[0m  [36/42], [94mLoss[0m : 1.81942
[1mStep[0m  [40/42], [94mLoss[0m : 1.77124

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.729, [92mTest[0m: 2.502, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.71053
[1mStep[0m  [4/42], [94mLoss[0m : 1.77274
[1mStep[0m  [8/42], [94mLoss[0m : 1.46846
[1mStep[0m  [12/42], [94mLoss[0m : 1.86788
[1mStep[0m  [16/42], [94mLoss[0m : 1.72881
[1mStep[0m  [20/42], [94mLoss[0m : 1.78392
[1mStep[0m  [24/42], [94mLoss[0m : 1.67591
[1mStep[0m  [28/42], [94mLoss[0m : 1.69232
[1mStep[0m  [32/42], [94mLoss[0m : 1.69456
[1mStep[0m  [36/42], [94mLoss[0m : 1.82219
[1mStep[0m  [40/42], [94mLoss[0m : 1.66074

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.714, [92mTest[0m: 2.482, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.75729
[1mStep[0m  [4/42], [94mLoss[0m : 1.68193
[1mStep[0m  [8/42], [94mLoss[0m : 1.62546
[1mStep[0m  [12/42], [94mLoss[0m : 1.63867
[1mStep[0m  [16/42], [94mLoss[0m : 1.67015
[1mStep[0m  [20/42], [94mLoss[0m : 1.60891
[1mStep[0m  [24/42], [94mLoss[0m : 1.70549
[1mStep[0m  [28/42], [94mLoss[0m : 1.74684
[1mStep[0m  [32/42], [94mLoss[0m : 1.65619
[1mStep[0m  [36/42], [94mLoss[0m : 1.67878
[1mStep[0m  [40/42], [94mLoss[0m : 1.58295

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.690, [92mTest[0m: 2.511, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.74588
[1mStep[0m  [4/42], [94mLoss[0m : 1.65506
[1mStep[0m  [8/42], [94mLoss[0m : 1.88152
[1mStep[0m  [12/42], [94mLoss[0m : 1.84043
[1mStep[0m  [16/42], [94mLoss[0m : 1.80786
[1mStep[0m  [20/42], [94mLoss[0m : 1.73506
[1mStep[0m  [24/42], [94mLoss[0m : 1.67230
[1mStep[0m  [28/42], [94mLoss[0m : 1.80194
[1mStep[0m  [32/42], [94mLoss[0m : 1.75978
[1mStep[0m  [36/42], [94mLoss[0m : 1.75191
[1mStep[0m  [40/42], [94mLoss[0m : 1.65733

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.661, [92mTest[0m: 2.477, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.62526
[1mStep[0m  [4/42], [94mLoss[0m : 1.66737
[1mStep[0m  [8/42], [94mLoss[0m : 1.58112
[1mStep[0m  [12/42], [94mLoss[0m : 1.54751
[1mStep[0m  [16/42], [94mLoss[0m : 1.65319
[1mStep[0m  [20/42], [94mLoss[0m : 1.55169
[1mStep[0m  [24/42], [94mLoss[0m : 1.65282
[1mStep[0m  [28/42], [94mLoss[0m : 1.66027
[1mStep[0m  [32/42], [94mLoss[0m : 1.67809
[1mStep[0m  [36/42], [94mLoss[0m : 1.56144
[1mStep[0m  [40/42], [94mLoss[0m : 1.58834

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.668, [92mTest[0m: 2.489, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.450
====================================

Phase 2 - Evaluation MAE:  2.449551752635411
MAE score P1        2.32511
MAE score P2       2.449552
loss               1.661443
learning_rate      0.007525
batch_size              256
hidden_sizes      [250, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.5
weight_decay         0.0001
Name: 7, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 11.37543
[1mStep[0m  [4/42], [94mLoss[0m : 10.83506
[1mStep[0m  [8/42], [94mLoss[0m : 10.43515
[1mStep[0m  [12/42], [94mLoss[0m : 10.77794
[1mStep[0m  [16/42], [94mLoss[0m : 10.56358
[1mStep[0m  [20/42], [94mLoss[0m : 10.46384
[1mStep[0m  [24/42], [94mLoss[0m : 10.47155
[1mStep[0m  [28/42], [94mLoss[0m : 10.31992
[1mStep[0m  [32/42], [94mLoss[0m : 10.60278
[1mStep[0m  [36/42], [94mLoss[0m : 9.98746
[1mStep[0m  [40/42], [94mLoss[0m : 10.11959

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.591, [92mTest[0m: 10.749, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.28218
[1mStep[0m  [4/42], [94mLoss[0m : 10.49447
[1mStep[0m  [8/42], [94mLoss[0m : 10.37443
[1mStep[0m  [12/42], [94mLoss[0m : 10.02222
[1mStep[0m  [16/42], [94mLoss[0m : 10.09066
[1mStep[0m  [20/42], [94mLoss[0m : 10.05244
[1mStep[0m  [24/42], [94mLoss[0m : 9.72093
[1mStep[0m  [28/42], [94mLoss[0m : 9.80998
[1mStep[0m  [32/42], [94mLoss[0m : 9.74027
[1mStep[0m  [36/42], [94mLoss[0m : 9.76577
[1mStep[0m  [40/42], [94mLoss[0m : 9.93688

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.033, [92mTest[0m: 10.199, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.43466
[1mStep[0m  [4/42], [94mLoss[0m : 9.70963
[1mStep[0m  [8/42], [94mLoss[0m : 9.73539
[1mStep[0m  [12/42], [94mLoss[0m : 8.94165
[1mStep[0m  [16/42], [94mLoss[0m : 9.60951
[1mStep[0m  [20/42], [94mLoss[0m : 9.45742
[1mStep[0m  [24/42], [94mLoss[0m : 9.61861
[1mStep[0m  [28/42], [94mLoss[0m : 9.11681
[1mStep[0m  [32/42], [94mLoss[0m : 9.82667
[1mStep[0m  [36/42], [94mLoss[0m : 9.11781
[1mStep[0m  [40/42], [94mLoss[0m : 8.77777

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.385, [92mTest[0m: 9.469, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.27906
[1mStep[0m  [4/42], [94mLoss[0m : 9.23682
[1mStep[0m  [8/42], [94mLoss[0m : 8.92109
[1mStep[0m  [12/42], [94mLoss[0m : 8.71798
[1mStep[0m  [16/42], [94mLoss[0m : 8.80866
[1mStep[0m  [20/42], [94mLoss[0m : 8.86947
[1mStep[0m  [24/42], [94mLoss[0m : 8.25451
[1mStep[0m  [28/42], [94mLoss[0m : 8.67261
[1mStep[0m  [32/42], [94mLoss[0m : 8.24666
[1mStep[0m  [36/42], [94mLoss[0m : 8.12656
[1mStep[0m  [40/42], [94mLoss[0m : 8.35616

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 8.572, [92mTest[0m: 8.667, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.27978
[1mStep[0m  [4/42], [94mLoss[0m : 7.56276
[1mStep[0m  [8/42], [94mLoss[0m : 7.97071
[1mStep[0m  [12/42], [94mLoss[0m : 7.67458
[1mStep[0m  [16/42], [94mLoss[0m : 8.02997
[1mStep[0m  [20/42], [94mLoss[0m : 7.66491
[1mStep[0m  [24/42], [94mLoss[0m : 7.57342
[1mStep[0m  [28/42], [94mLoss[0m : 7.59555
[1mStep[0m  [32/42], [94mLoss[0m : 7.28451
[1mStep[0m  [36/42], [94mLoss[0m : 7.33624
[1mStep[0m  [40/42], [94mLoss[0m : 7.42091

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 7.599, [92mTest[0m: 7.585, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.74574
[1mStep[0m  [4/42], [94mLoss[0m : 6.85270
[1mStep[0m  [8/42], [94mLoss[0m : 6.63405
[1mStep[0m  [12/42], [94mLoss[0m : 7.14080
[1mStep[0m  [16/42], [94mLoss[0m : 6.75705
[1mStep[0m  [20/42], [94mLoss[0m : 6.78561
[1mStep[0m  [24/42], [94mLoss[0m : 6.85924
[1mStep[0m  [28/42], [94mLoss[0m : 7.05236
[1mStep[0m  [32/42], [94mLoss[0m : 6.54761
[1mStep[0m  [36/42], [94mLoss[0m : 6.72826
[1mStep[0m  [40/42], [94mLoss[0m : 6.42043

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 6.734, [92mTest[0m: 6.470, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.40922
[1mStep[0m  [4/42], [94mLoss[0m : 6.42966
[1mStep[0m  [8/42], [94mLoss[0m : 6.26593
[1mStep[0m  [12/42], [94mLoss[0m : 6.40905
[1mStep[0m  [16/42], [94mLoss[0m : 5.99459
[1mStep[0m  [20/42], [94mLoss[0m : 6.07134
[1mStep[0m  [24/42], [94mLoss[0m : 6.24004
[1mStep[0m  [28/42], [94mLoss[0m : 5.97660
[1mStep[0m  [32/42], [94mLoss[0m : 5.85460
[1mStep[0m  [36/42], [94mLoss[0m : 5.84295
[1mStep[0m  [40/42], [94mLoss[0m : 5.43829

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 6.043, [92mTest[0m: 5.616, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.38830
[1mStep[0m  [4/42], [94mLoss[0m : 5.58743
[1mStep[0m  [8/42], [94mLoss[0m : 5.86432
[1mStep[0m  [12/42], [94mLoss[0m : 5.39872
[1mStep[0m  [16/42], [94mLoss[0m : 5.66189
[1mStep[0m  [20/42], [94mLoss[0m : 5.40114
[1mStep[0m  [24/42], [94mLoss[0m : 5.65610
[1mStep[0m  [28/42], [94mLoss[0m : 5.61021
[1mStep[0m  [32/42], [94mLoss[0m : 5.23251
[1mStep[0m  [36/42], [94mLoss[0m : 5.15452
[1mStep[0m  [40/42], [94mLoss[0m : 4.70335

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 5.412, [92mTest[0m: 4.775, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.41550
[1mStep[0m  [4/42], [94mLoss[0m : 5.27421
[1mStep[0m  [8/42], [94mLoss[0m : 5.11380
[1mStep[0m  [12/42], [94mLoss[0m : 5.17228
[1mStep[0m  [16/42], [94mLoss[0m : 4.88976
[1mStep[0m  [20/42], [94mLoss[0m : 4.68533
[1mStep[0m  [24/42], [94mLoss[0m : 4.73982
[1mStep[0m  [28/42], [94mLoss[0m : 4.88564
[1mStep[0m  [32/42], [94mLoss[0m : 4.79699
[1mStep[0m  [36/42], [94mLoss[0m : 5.03769
[1mStep[0m  [40/42], [94mLoss[0m : 4.47403

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 4.781, [92mTest[0m: 4.150, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.49204
[1mStep[0m  [4/42], [94mLoss[0m : 4.29833
[1mStep[0m  [8/42], [94mLoss[0m : 4.45418
[1mStep[0m  [12/42], [94mLoss[0m : 4.32309
[1mStep[0m  [16/42], [94mLoss[0m : 4.23499
[1mStep[0m  [20/42], [94mLoss[0m : 4.14495
[1mStep[0m  [24/42], [94mLoss[0m : 4.18055
[1mStep[0m  [28/42], [94mLoss[0m : 3.64501
[1mStep[0m  [32/42], [94mLoss[0m : 3.79686
[1mStep[0m  [36/42], [94mLoss[0m : 3.99355
[1mStep[0m  [40/42], [94mLoss[0m : 3.82375

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 4.082, [92mTest[0m: 3.698, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.52111
[1mStep[0m  [4/42], [94mLoss[0m : 3.44444
[1mStep[0m  [8/42], [94mLoss[0m : 3.36482
[1mStep[0m  [12/42], [94mLoss[0m : 3.76848
[1mStep[0m  [16/42], [94mLoss[0m : 3.05554
[1mStep[0m  [20/42], [94mLoss[0m : 3.45388
[1mStep[0m  [24/42], [94mLoss[0m : 3.44470
[1mStep[0m  [28/42], [94mLoss[0m : 3.05823
[1mStep[0m  [32/42], [94mLoss[0m : 3.33288
[1mStep[0m  [36/42], [94mLoss[0m : 3.07924
[1mStep[0m  [40/42], [94mLoss[0m : 3.13627

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 3.398, [92mTest[0m: 3.007, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.07255
[1mStep[0m  [4/42], [94mLoss[0m : 3.30104
[1mStep[0m  [8/42], [94mLoss[0m : 2.91949
[1mStep[0m  [12/42], [94mLoss[0m : 2.83213
[1mStep[0m  [16/42], [94mLoss[0m : 2.80425
[1mStep[0m  [20/42], [94mLoss[0m : 2.96113
[1mStep[0m  [24/42], [94mLoss[0m : 2.94246
[1mStep[0m  [28/42], [94mLoss[0m : 2.91375
[1mStep[0m  [32/42], [94mLoss[0m : 2.66859
[1mStep[0m  [36/42], [94mLoss[0m : 2.97447
[1mStep[0m  [40/42], [94mLoss[0m : 2.86736

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.864, [92mTest[0m: 2.618, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51039
[1mStep[0m  [4/42], [94mLoss[0m : 3.00908
[1mStep[0m  [8/42], [94mLoss[0m : 2.61616
[1mStep[0m  [12/42], [94mLoss[0m : 3.16120
[1mStep[0m  [16/42], [94mLoss[0m : 2.46966
[1mStep[0m  [20/42], [94mLoss[0m : 2.90267
[1mStep[0m  [24/42], [94mLoss[0m : 2.90728
[1mStep[0m  [28/42], [94mLoss[0m : 2.62831
[1mStep[0m  [32/42], [94mLoss[0m : 2.84471
[1mStep[0m  [36/42], [94mLoss[0m : 2.59964
[1mStep[0m  [40/42], [94mLoss[0m : 2.57350

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.674, [92mTest[0m: 2.424, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.79850
[1mStep[0m  [4/42], [94mLoss[0m : 2.39909
[1mStep[0m  [8/42], [94mLoss[0m : 2.63568
[1mStep[0m  [12/42], [94mLoss[0m : 2.72252
[1mStep[0m  [16/42], [94mLoss[0m : 2.88070
[1mStep[0m  [20/42], [94mLoss[0m : 2.51579
[1mStep[0m  [24/42], [94mLoss[0m : 2.58065
[1mStep[0m  [28/42], [94mLoss[0m : 2.75925
[1mStep[0m  [32/42], [94mLoss[0m : 2.71844
[1mStep[0m  [36/42], [94mLoss[0m : 2.39705
[1mStep[0m  [40/42], [94mLoss[0m : 2.71932

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.588, [92mTest[0m: 2.374, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61880
[1mStep[0m  [4/42], [94mLoss[0m : 2.67264
[1mStep[0m  [8/42], [94mLoss[0m : 2.23912
[1mStep[0m  [12/42], [94mLoss[0m : 2.89533
[1mStep[0m  [16/42], [94mLoss[0m : 2.69242
[1mStep[0m  [20/42], [94mLoss[0m : 2.42380
[1mStep[0m  [24/42], [94mLoss[0m : 2.62783
[1mStep[0m  [28/42], [94mLoss[0m : 2.59844
[1mStep[0m  [32/42], [94mLoss[0m : 2.47078
[1mStep[0m  [36/42], [94mLoss[0m : 2.47367
[1mStep[0m  [40/42], [94mLoss[0m : 2.46244

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.573, [92mTest[0m: 2.373, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.68227
[1mStep[0m  [4/42], [94mLoss[0m : 2.42060
[1mStep[0m  [8/42], [94mLoss[0m : 2.33940
[1mStep[0m  [12/42], [94mLoss[0m : 2.56793
[1mStep[0m  [16/42], [94mLoss[0m : 2.64873
[1mStep[0m  [20/42], [94mLoss[0m : 2.66039
[1mStep[0m  [24/42], [94mLoss[0m : 2.84656
[1mStep[0m  [28/42], [94mLoss[0m : 2.65410
[1mStep[0m  [32/42], [94mLoss[0m : 2.49028
[1mStep[0m  [36/42], [94mLoss[0m : 2.73396
[1mStep[0m  [40/42], [94mLoss[0m : 2.52470

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.550, [92mTest[0m: 2.369, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.63375
[1mStep[0m  [4/42], [94mLoss[0m : 2.39396
[1mStep[0m  [8/42], [94mLoss[0m : 2.74621
[1mStep[0m  [12/42], [94mLoss[0m : 2.73064
[1mStep[0m  [16/42], [94mLoss[0m : 2.55389
[1mStep[0m  [20/42], [94mLoss[0m : 2.48611
[1mStep[0m  [24/42], [94mLoss[0m : 2.53405
[1mStep[0m  [28/42], [94mLoss[0m : 2.52485
[1mStep[0m  [32/42], [94mLoss[0m : 2.53284
[1mStep[0m  [36/42], [94mLoss[0m : 2.62583
[1mStep[0m  [40/42], [94mLoss[0m : 2.45089

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.545, [92mTest[0m: 2.384, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58383
[1mStep[0m  [4/42], [94mLoss[0m : 2.56720
[1mStep[0m  [8/42], [94mLoss[0m : 2.54676
[1mStep[0m  [12/42], [94mLoss[0m : 2.54203
[1mStep[0m  [16/42], [94mLoss[0m : 2.49719
[1mStep[0m  [20/42], [94mLoss[0m : 2.63537
[1mStep[0m  [24/42], [94mLoss[0m : 2.40305
[1mStep[0m  [28/42], [94mLoss[0m : 2.52243
[1mStep[0m  [32/42], [94mLoss[0m : 2.74145
[1mStep[0m  [36/42], [94mLoss[0m : 2.69664
[1mStep[0m  [40/42], [94mLoss[0m : 2.56623

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.535, [92mTest[0m: 2.364, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61117
[1mStep[0m  [4/42], [94mLoss[0m : 2.37930
[1mStep[0m  [8/42], [94mLoss[0m : 2.50155
[1mStep[0m  [12/42], [94mLoss[0m : 2.52147
[1mStep[0m  [16/42], [94mLoss[0m : 2.55003
[1mStep[0m  [20/42], [94mLoss[0m : 2.60416
[1mStep[0m  [24/42], [94mLoss[0m : 2.49303
[1mStep[0m  [28/42], [94mLoss[0m : 2.56824
[1mStep[0m  [32/42], [94mLoss[0m : 2.74798
[1mStep[0m  [36/42], [94mLoss[0m : 2.35003
[1mStep[0m  [40/42], [94mLoss[0m : 2.53131

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.519, [92mTest[0m: 2.380, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.72801
[1mStep[0m  [4/42], [94mLoss[0m : 2.38300
[1mStep[0m  [8/42], [94mLoss[0m : 2.50174
[1mStep[0m  [12/42], [94mLoss[0m : 2.60786
[1mStep[0m  [16/42], [94mLoss[0m : 2.45500
[1mStep[0m  [20/42], [94mLoss[0m : 2.55658
[1mStep[0m  [24/42], [94mLoss[0m : 2.36066
[1mStep[0m  [28/42], [94mLoss[0m : 2.27159
[1mStep[0m  [32/42], [94mLoss[0m : 2.48441
[1mStep[0m  [36/42], [94mLoss[0m : 2.68847
[1mStep[0m  [40/42], [94mLoss[0m : 2.37666

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.352, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47531
[1mStep[0m  [4/42], [94mLoss[0m : 2.37827
[1mStep[0m  [8/42], [94mLoss[0m : 2.44030
[1mStep[0m  [12/42], [94mLoss[0m : 2.38668
[1mStep[0m  [16/42], [94mLoss[0m : 2.63035
[1mStep[0m  [20/42], [94mLoss[0m : 2.71539
[1mStep[0m  [24/42], [94mLoss[0m : 2.48686
[1mStep[0m  [28/42], [94mLoss[0m : 2.59731
[1mStep[0m  [32/42], [94mLoss[0m : 2.66636
[1mStep[0m  [36/42], [94mLoss[0m : 2.67167
[1mStep[0m  [40/42], [94mLoss[0m : 2.54659

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.365, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42441
[1mStep[0m  [4/42], [94mLoss[0m : 2.49206
[1mStep[0m  [8/42], [94mLoss[0m : 2.58035
[1mStep[0m  [12/42], [94mLoss[0m : 2.27556
[1mStep[0m  [16/42], [94mLoss[0m : 2.38059
[1mStep[0m  [20/42], [94mLoss[0m : 2.48772
[1mStep[0m  [24/42], [94mLoss[0m : 2.45479
[1mStep[0m  [28/42], [94mLoss[0m : 2.50908
[1mStep[0m  [32/42], [94mLoss[0m : 2.45901
[1mStep[0m  [36/42], [94mLoss[0m : 2.46019
[1mStep[0m  [40/42], [94mLoss[0m : 2.38609

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.362, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49010
[1mStep[0m  [4/42], [94mLoss[0m : 2.40930
[1mStep[0m  [8/42], [94mLoss[0m : 2.44078
[1mStep[0m  [12/42], [94mLoss[0m : 2.45028
[1mStep[0m  [16/42], [94mLoss[0m : 2.48984
[1mStep[0m  [20/42], [94mLoss[0m : 2.34673
[1mStep[0m  [24/42], [94mLoss[0m : 2.54912
[1mStep[0m  [28/42], [94mLoss[0m : 2.38370
[1mStep[0m  [32/42], [94mLoss[0m : 2.50470
[1mStep[0m  [36/42], [94mLoss[0m : 2.60766
[1mStep[0m  [40/42], [94mLoss[0m : 2.54107

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.356, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44987
[1mStep[0m  [4/42], [94mLoss[0m : 2.40861
[1mStep[0m  [8/42], [94mLoss[0m : 2.48186
[1mStep[0m  [12/42], [94mLoss[0m : 2.43442
[1mStep[0m  [16/42], [94mLoss[0m : 2.43442
[1mStep[0m  [20/42], [94mLoss[0m : 2.33980
[1mStep[0m  [24/42], [94mLoss[0m : 2.36212
[1mStep[0m  [28/42], [94mLoss[0m : 2.69989
[1mStep[0m  [32/42], [94mLoss[0m : 2.32382
[1mStep[0m  [36/42], [94mLoss[0m : 2.43086
[1mStep[0m  [40/42], [94mLoss[0m : 2.33552

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.363, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64803
[1mStep[0m  [4/42], [94mLoss[0m : 2.43815
[1mStep[0m  [8/42], [94mLoss[0m : 2.44087
[1mStep[0m  [12/42], [94mLoss[0m : 2.47221
[1mStep[0m  [16/42], [94mLoss[0m : 2.42213
[1mStep[0m  [20/42], [94mLoss[0m : 2.48594
[1mStep[0m  [24/42], [94mLoss[0m : 2.61291
[1mStep[0m  [28/42], [94mLoss[0m : 2.45825
[1mStep[0m  [32/42], [94mLoss[0m : 2.28695
[1mStep[0m  [36/42], [94mLoss[0m : 2.50284
[1mStep[0m  [40/42], [94mLoss[0m : 2.55768

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.342, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57065
[1mStep[0m  [4/42], [94mLoss[0m : 2.50652
[1mStep[0m  [8/42], [94mLoss[0m : 2.45643
[1mStep[0m  [12/42], [94mLoss[0m : 2.44300
[1mStep[0m  [16/42], [94mLoss[0m : 2.52189
[1mStep[0m  [20/42], [94mLoss[0m : 2.46707
[1mStep[0m  [24/42], [94mLoss[0m : 2.45875
[1mStep[0m  [28/42], [94mLoss[0m : 2.47779
[1mStep[0m  [32/42], [94mLoss[0m : 2.38436
[1mStep[0m  [36/42], [94mLoss[0m : 2.31581
[1mStep[0m  [40/42], [94mLoss[0m : 2.41392

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.346, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57771
[1mStep[0m  [4/42], [94mLoss[0m : 2.47428
[1mStep[0m  [8/42], [94mLoss[0m : 2.48659
[1mStep[0m  [12/42], [94mLoss[0m : 2.33918
[1mStep[0m  [16/42], [94mLoss[0m : 2.33124
[1mStep[0m  [20/42], [94mLoss[0m : 2.41068
[1mStep[0m  [24/42], [94mLoss[0m : 2.49314
[1mStep[0m  [28/42], [94mLoss[0m : 2.41981
[1mStep[0m  [32/42], [94mLoss[0m : 2.47841
[1mStep[0m  [36/42], [94mLoss[0m : 2.21301
[1mStep[0m  [40/42], [94mLoss[0m : 2.38063

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.342, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.66497
[1mStep[0m  [4/42], [94mLoss[0m : 2.72795
[1mStep[0m  [8/42], [94mLoss[0m : 2.26130
[1mStep[0m  [12/42], [94mLoss[0m : 2.41905
[1mStep[0m  [16/42], [94mLoss[0m : 2.33557
[1mStep[0m  [20/42], [94mLoss[0m : 2.50338
[1mStep[0m  [24/42], [94mLoss[0m : 2.55550
[1mStep[0m  [28/42], [94mLoss[0m : 2.58525
[1mStep[0m  [32/42], [94mLoss[0m : 2.45825
[1mStep[0m  [36/42], [94mLoss[0m : 2.28198
[1mStep[0m  [40/42], [94mLoss[0m : 2.55369

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.362, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39256
[1mStep[0m  [4/42], [94mLoss[0m : 2.42177
[1mStep[0m  [8/42], [94mLoss[0m : 2.46758
[1mStep[0m  [12/42], [94mLoss[0m : 2.59072
[1mStep[0m  [16/42], [94mLoss[0m : 2.70244
[1mStep[0m  [20/42], [94mLoss[0m : 2.38754
[1mStep[0m  [24/42], [94mLoss[0m : 2.57745
[1mStep[0m  [28/42], [94mLoss[0m : 2.67713
[1mStep[0m  [32/42], [94mLoss[0m : 2.47090
[1mStep[0m  [36/42], [94mLoss[0m : 2.38813
[1mStep[0m  [40/42], [94mLoss[0m : 2.69902

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.351, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40135
[1mStep[0m  [4/42], [94mLoss[0m : 2.31439
[1mStep[0m  [8/42], [94mLoss[0m : 2.60577
[1mStep[0m  [12/42], [94mLoss[0m : 2.46701
[1mStep[0m  [16/42], [94mLoss[0m : 2.52997
[1mStep[0m  [20/42], [94mLoss[0m : 2.59409
[1mStep[0m  [24/42], [94mLoss[0m : 2.63644
[1mStep[0m  [28/42], [94mLoss[0m : 2.38704
[1mStep[0m  [32/42], [94mLoss[0m : 2.49620
[1mStep[0m  [36/42], [94mLoss[0m : 2.34693
[1mStep[0m  [40/42], [94mLoss[0m : 2.46944

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.344, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.341
====================================

Phase 1 - Evaluation MAE:  2.340895346232823
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 2.28304
[1mStep[0m  [4/42], [94mLoss[0m : 2.38584
[1mStep[0m  [8/42], [94mLoss[0m : 2.44496
[1mStep[0m  [12/42], [94mLoss[0m : 2.44308
[1mStep[0m  [16/42], [94mLoss[0m : 2.43347
[1mStep[0m  [20/42], [94mLoss[0m : 2.43031
[1mStep[0m  [24/42], [94mLoss[0m : 2.33378
[1mStep[0m  [28/42], [94mLoss[0m : 2.71052
[1mStep[0m  [32/42], [94mLoss[0m : 2.79286
[1mStep[0m  [36/42], [94mLoss[0m : 2.50802
[1mStep[0m  [40/42], [94mLoss[0m : 2.39101

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.335, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.77295
[1mStep[0m  [4/42], [94mLoss[0m : 2.68489
[1mStep[0m  [8/42], [94mLoss[0m : 2.52661
[1mStep[0m  [12/42], [94mLoss[0m : 2.47901
[1mStep[0m  [16/42], [94mLoss[0m : 2.33327
[1mStep[0m  [20/42], [94mLoss[0m : 2.63512
[1mStep[0m  [24/42], [94mLoss[0m : 2.44006
[1mStep[0m  [28/42], [94mLoss[0m : 2.41027
[1mStep[0m  [32/42], [94mLoss[0m : 2.25807
[1mStep[0m  [36/42], [94mLoss[0m : 2.53368
[1mStep[0m  [40/42], [94mLoss[0m : 2.48107

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.403, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49251
[1mStep[0m  [4/42], [94mLoss[0m : 2.37006
[1mStep[0m  [8/42], [94mLoss[0m : 2.71107
[1mStep[0m  [12/42], [94mLoss[0m : 2.62607
[1mStep[0m  [16/42], [94mLoss[0m : 2.49762
[1mStep[0m  [20/42], [94mLoss[0m : 2.61339
[1mStep[0m  [24/42], [94mLoss[0m : 2.47008
[1mStep[0m  [28/42], [94mLoss[0m : 2.63893
[1mStep[0m  [32/42], [94mLoss[0m : 2.63907
[1mStep[0m  [36/42], [94mLoss[0m : 2.31988
[1mStep[0m  [40/42], [94mLoss[0m : 2.54645

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.434, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.30150
[1mStep[0m  [4/42], [94mLoss[0m : 2.15142
[1mStep[0m  [8/42], [94mLoss[0m : 2.52233
[1mStep[0m  [12/42], [94mLoss[0m : 2.19599
[1mStep[0m  [16/42], [94mLoss[0m : 2.41321
[1mStep[0m  [20/42], [94mLoss[0m : 2.22775
[1mStep[0m  [24/42], [94mLoss[0m : 2.67583
[1mStep[0m  [28/42], [94mLoss[0m : 2.23491
[1mStep[0m  [32/42], [94mLoss[0m : 2.29862
[1mStep[0m  [36/42], [94mLoss[0m : 2.32893
[1mStep[0m  [40/42], [94mLoss[0m : 2.54467

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.488, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34916
[1mStep[0m  [4/42], [94mLoss[0m : 2.31413
[1mStep[0m  [8/42], [94mLoss[0m : 2.38423
[1mStep[0m  [12/42], [94mLoss[0m : 2.29721
[1mStep[0m  [16/42], [94mLoss[0m : 2.12409
[1mStep[0m  [20/42], [94mLoss[0m : 2.46060
[1mStep[0m  [24/42], [94mLoss[0m : 2.10150
[1mStep[0m  [28/42], [94mLoss[0m : 2.46499
[1mStep[0m  [32/42], [94mLoss[0m : 2.53090
[1mStep[0m  [36/42], [94mLoss[0m : 2.32855
[1mStep[0m  [40/42], [94mLoss[0m : 2.67061

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.532, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48866
[1mStep[0m  [4/42], [94mLoss[0m : 2.28536
[1mStep[0m  [8/42], [94mLoss[0m : 2.42414
[1mStep[0m  [12/42], [94mLoss[0m : 2.25308
[1mStep[0m  [16/42], [94mLoss[0m : 2.26590
[1mStep[0m  [20/42], [94mLoss[0m : 2.08765
[1mStep[0m  [24/42], [94mLoss[0m : 2.38717
[1mStep[0m  [28/42], [94mLoss[0m : 2.12007
[1mStep[0m  [32/42], [94mLoss[0m : 2.41841
[1mStep[0m  [36/42], [94mLoss[0m : 2.43686
[1mStep[0m  [40/42], [94mLoss[0m : 2.23194

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.355, [92mTest[0m: 2.510, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40208
[1mStep[0m  [4/42], [94mLoss[0m : 2.14992
[1mStep[0m  [8/42], [94mLoss[0m : 2.40044
[1mStep[0m  [12/42], [94mLoss[0m : 2.46151
[1mStep[0m  [16/42], [94mLoss[0m : 2.25132
[1mStep[0m  [20/42], [94mLoss[0m : 2.63469
[1mStep[0m  [24/42], [94mLoss[0m : 2.33882
[1mStep[0m  [28/42], [94mLoss[0m : 2.16967
[1mStep[0m  [32/42], [94mLoss[0m : 2.23540
[1mStep[0m  [36/42], [94mLoss[0m : 2.35381
[1mStep[0m  [40/42], [94mLoss[0m : 2.10173

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.315, [92mTest[0m: 2.455, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.12546
[1mStep[0m  [4/42], [94mLoss[0m : 2.12807
[1mStep[0m  [8/42], [94mLoss[0m : 2.29728
[1mStep[0m  [12/42], [94mLoss[0m : 2.30863
[1mStep[0m  [16/42], [94mLoss[0m : 2.28575
[1mStep[0m  [20/42], [94mLoss[0m : 2.23055
[1mStep[0m  [24/42], [94mLoss[0m : 2.40110
[1mStep[0m  [28/42], [94mLoss[0m : 2.43417
[1mStep[0m  [32/42], [94mLoss[0m : 2.54272
[1mStep[0m  [36/42], [94mLoss[0m : 2.35668
[1mStep[0m  [40/42], [94mLoss[0m : 2.36360

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.280, [92mTest[0m: 2.589, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.20520
[1mStep[0m  [4/42], [94mLoss[0m : 2.42306
[1mStep[0m  [8/42], [94mLoss[0m : 2.27417
[1mStep[0m  [12/42], [94mLoss[0m : 2.38743
[1mStep[0m  [16/42], [94mLoss[0m : 2.33592
[1mStep[0m  [20/42], [94mLoss[0m : 2.26579
[1mStep[0m  [24/42], [94mLoss[0m : 2.34796
[1mStep[0m  [28/42], [94mLoss[0m : 2.01441
[1mStep[0m  [32/42], [94mLoss[0m : 2.02861
[1mStep[0m  [36/42], [94mLoss[0m : 2.12665
[1mStep[0m  [40/42], [94mLoss[0m : 2.12645

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.241, [92mTest[0m: 2.402, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27933
[1mStep[0m  [4/42], [94mLoss[0m : 2.05646
[1mStep[0m  [8/42], [94mLoss[0m : 2.16581
[1mStep[0m  [12/42], [94mLoss[0m : 2.20000
[1mStep[0m  [16/42], [94mLoss[0m : 2.01018
[1mStep[0m  [20/42], [94mLoss[0m : 2.20643
[1mStep[0m  [24/42], [94mLoss[0m : 2.14296
[1mStep[0m  [28/42], [94mLoss[0m : 2.41454
[1mStep[0m  [32/42], [94mLoss[0m : 2.03653
[1mStep[0m  [36/42], [94mLoss[0m : 2.22583
[1mStep[0m  [40/42], [94mLoss[0m : 2.26199

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.208, [92mTest[0m: 2.421, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.07161
[1mStep[0m  [4/42], [94mLoss[0m : 2.47518
[1mStep[0m  [8/42], [94mLoss[0m : 2.36862
[1mStep[0m  [12/42], [94mLoss[0m : 2.03667
[1mStep[0m  [16/42], [94mLoss[0m : 2.24213
[1mStep[0m  [20/42], [94mLoss[0m : 2.12582
[1mStep[0m  [24/42], [94mLoss[0m : 2.19871
[1mStep[0m  [28/42], [94mLoss[0m : 2.01931
[1mStep[0m  [32/42], [94mLoss[0m : 2.10990
[1mStep[0m  [36/42], [94mLoss[0m : 2.34253
[1mStep[0m  [40/42], [94mLoss[0m : 2.13048

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.167, [92mTest[0m: 2.509, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.06576
[1mStep[0m  [4/42], [94mLoss[0m : 1.95431
[1mStep[0m  [8/42], [94mLoss[0m : 2.07820
[1mStep[0m  [12/42], [94mLoss[0m : 2.12560
[1mStep[0m  [16/42], [94mLoss[0m : 2.10997
[1mStep[0m  [20/42], [94mLoss[0m : 2.00368
[1mStep[0m  [24/42], [94mLoss[0m : 2.27383
[1mStep[0m  [28/42], [94mLoss[0m : 2.03969
[1mStep[0m  [32/42], [94mLoss[0m : 2.31705
[1mStep[0m  [36/42], [94mLoss[0m : 2.24089
[1mStep[0m  [40/42], [94mLoss[0m : 2.29445

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.127, [92mTest[0m: 2.447, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.11913
[1mStep[0m  [4/42], [94mLoss[0m : 2.16101
[1mStep[0m  [8/42], [94mLoss[0m : 1.87952
[1mStep[0m  [12/42], [94mLoss[0m : 2.04842
[1mStep[0m  [16/42], [94mLoss[0m : 2.01004
[1mStep[0m  [20/42], [94mLoss[0m : 2.12465
[1mStep[0m  [24/42], [94mLoss[0m : 2.13717
[1mStep[0m  [28/42], [94mLoss[0m : 2.15954
[1mStep[0m  [32/42], [94mLoss[0m : 1.91518
[1mStep[0m  [36/42], [94mLoss[0m : 1.89082
[1mStep[0m  [40/42], [94mLoss[0m : 2.19426

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.089, [92mTest[0m: 2.439, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.18466
[1mStep[0m  [4/42], [94mLoss[0m : 2.03992
[1mStep[0m  [8/42], [94mLoss[0m : 2.02408
[1mStep[0m  [12/42], [94mLoss[0m : 2.06534
[1mStep[0m  [16/42], [94mLoss[0m : 2.13790
[1mStep[0m  [20/42], [94mLoss[0m : 1.96911
[1mStep[0m  [24/42], [94mLoss[0m : 2.06287
[1mStep[0m  [28/42], [94mLoss[0m : 2.05275
[1mStep[0m  [32/42], [94mLoss[0m : 2.03547
[1mStep[0m  [36/42], [94mLoss[0m : 2.14508
[1mStep[0m  [40/42], [94mLoss[0m : 2.03547

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.064, [92mTest[0m: 2.497, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.04092
[1mStep[0m  [4/42], [94mLoss[0m : 2.13495
[1mStep[0m  [8/42], [94mLoss[0m : 2.05043
[1mStep[0m  [12/42], [94mLoss[0m : 1.80958
[1mStep[0m  [16/42], [94mLoss[0m : 2.19593
[1mStep[0m  [20/42], [94mLoss[0m : 1.84068
[1mStep[0m  [24/42], [94mLoss[0m : 1.94300
[1mStep[0m  [28/42], [94mLoss[0m : 1.91275
[1mStep[0m  [32/42], [94mLoss[0m : 2.10630
[1mStep[0m  [36/42], [94mLoss[0m : 1.90020
[1mStep[0m  [40/42], [94mLoss[0m : 1.89635

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.021, [92mTest[0m: 2.439, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.81095
[1mStep[0m  [4/42], [94mLoss[0m : 2.05421
[1mStep[0m  [8/42], [94mLoss[0m : 2.02930
[1mStep[0m  [12/42], [94mLoss[0m : 1.93818
[1mStep[0m  [16/42], [94mLoss[0m : 1.88657
[1mStep[0m  [20/42], [94mLoss[0m : 2.00165
[1mStep[0m  [24/42], [94mLoss[0m : 1.93744
[1mStep[0m  [28/42], [94mLoss[0m : 1.98595
[1mStep[0m  [32/42], [94mLoss[0m : 1.85995
[1mStep[0m  [36/42], [94mLoss[0m : 1.90143
[1mStep[0m  [40/42], [94mLoss[0m : 1.73825

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.973, [92mTest[0m: 2.468, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.88669
[1mStep[0m  [4/42], [94mLoss[0m : 1.86335
[1mStep[0m  [8/42], [94mLoss[0m : 2.18016
[1mStep[0m  [12/42], [94mLoss[0m : 1.96166
[1mStep[0m  [16/42], [94mLoss[0m : 1.93334
[1mStep[0m  [20/42], [94mLoss[0m : 2.00263
[1mStep[0m  [24/42], [94mLoss[0m : 1.96308
[1mStep[0m  [28/42], [94mLoss[0m : 1.93182
[1mStep[0m  [32/42], [94mLoss[0m : 2.09136
[1mStep[0m  [36/42], [94mLoss[0m : 1.89350
[1mStep[0m  [40/42], [94mLoss[0m : 2.04822

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.949, [92mTest[0m: 2.489, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.02426
[1mStep[0m  [4/42], [94mLoss[0m : 1.87674
[1mStep[0m  [8/42], [94mLoss[0m : 1.78700
[1mStep[0m  [12/42], [94mLoss[0m : 2.00810
[1mStep[0m  [16/42], [94mLoss[0m : 1.83520
[1mStep[0m  [20/42], [94mLoss[0m : 1.80065
[1mStep[0m  [24/42], [94mLoss[0m : 1.92188
[1mStep[0m  [28/42], [94mLoss[0m : 2.00550
[1mStep[0m  [32/42], [94mLoss[0m : 1.95153
[1mStep[0m  [36/42], [94mLoss[0m : 2.08251
[1mStep[0m  [40/42], [94mLoss[0m : 2.05065

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.937, [92mTest[0m: 2.585, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.09675
[1mStep[0m  [4/42], [94mLoss[0m : 1.64041
[1mStep[0m  [8/42], [94mLoss[0m : 1.90175
[1mStep[0m  [12/42], [94mLoss[0m : 1.70175
[1mStep[0m  [16/42], [94mLoss[0m : 1.96926
[1mStep[0m  [20/42], [94mLoss[0m : 1.99162
[1mStep[0m  [24/42], [94mLoss[0m : 2.09108
[1mStep[0m  [28/42], [94mLoss[0m : 1.94730
[1mStep[0m  [32/42], [94mLoss[0m : 1.97438
[1mStep[0m  [36/42], [94mLoss[0m : 1.87037
[1mStep[0m  [40/42], [94mLoss[0m : 1.93165

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.902, [92mTest[0m: 2.562, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.86372
[1mStep[0m  [4/42], [94mLoss[0m : 1.79358
[1mStep[0m  [8/42], [94mLoss[0m : 1.85327
[1mStep[0m  [12/42], [94mLoss[0m : 1.95867
[1mStep[0m  [16/42], [94mLoss[0m : 1.72136
[1mStep[0m  [20/42], [94mLoss[0m : 2.02449
[1mStep[0m  [24/42], [94mLoss[0m : 1.96985
[1mStep[0m  [28/42], [94mLoss[0m : 1.88130
[1mStep[0m  [32/42], [94mLoss[0m : 1.80202
[1mStep[0m  [36/42], [94mLoss[0m : 1.84652
[1mStep[0m  [40/42], [94mLoss[0m : 1.91865

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.851, [92mTest[0m: 2.522, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.93316
[1mStep[0m  [4/42], [94mLoss[0m : 1.93176
[1mStep[0m  [8/42], [94mLoss[0m : 1.80711
[1mStep[0m  [12/42], [94mLoss[0m : 1.84434
[1mStep[0m  [16/42], [94mLoss[0m : 1.98025
[1mStep[0m  [20/42], [94mLoss[0m : 1.95219
[1mStep[0m  [24/42], [94mLoss[0m : 1.96326
[1mStep[0m  [28/42], [94mLoss[0m : 1.94486
[1mStep[0m  [32/42], [94mLoss[0m : 1.87279
[1mStep[0m  [36/42], [94mLoss[0m : 1.83597
[1mStep[0m  [40/42], [94mLoss[0m : 1.95467

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.859, [92mTest[0m: 2.534, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.72786
[1mStep[0m  [4/42], [94mLoss[0m : 1.77544
[1mStep[0m  [8/42], [94mLoss[0m : 1.73465
[1mStep[0m  [12/42], [94mLoss[0m : 1.70021
[1mStep[0m  [16/42], [94mLoss[0m : 1.76719
[1mStep[0m  [20/42], [94mLoss[0m : 1.74932
[1mStep[0m  [24/42], [94mLoss[0m : 1.81570
[1mStep[0m  [28/42], [94mLoss[0m : 1.94588
[1mStep[0m  [32/42], [94mLoss[0m : 1.81714
[1mStep[0m  [36/42], [94mLoss[0m : 1.72881
[1mStep[0m  [40/42], [94mLoss[0m : 1.77058

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.805, [92mTest[0m: 2.464, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.82356
[1mStep[0m  [4/42], [94mLoss[0m : 1.78214
[1mStep[0m  [8/42], [94mLoss[0m : 1.69212
[1mStep[0m  [12/42], [94mLoss[0m : 1.95051
[1mStep[0m  [16/42], [94mLoss[0m : 1.60232
[1mStep[0m  [20/42], [94mLoss[0m : 1.88517
[1mStep[0m  [24/42], [94mLoss[0m : 1.82851
[1mStep[0m  [28/42], [94mLoss[0m : 1.82822
[1mStep[0m  [32/42], [94mLoss[0m : 1.91483
[1mStep[0m  [36/42], [94mLoss[0m : 1.98359
[1mStep[0m  [40/42], [94mLoss[0m : 1.93922

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.798, [92mTest[0m: 2.506, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.83615
[1mStep[0m  [4/42], [94mLoss[0m : 1.79647
[1mStep[0m  [8/42], [94mLoss[0m : 1.71141
[1mStep[0m  [12/42], [94mLoss[0m : 1.67198
[1mStep[0m  [16/42], [94mLoss[0m : 1.82641
[1mStep[0m  [20/42], [94mLoss[0m : 1.93580
[1mStep[0m  [24/42], [94mLoss[0m : 1.71777
[1mStep[0m  [28/42], [94mLoss[0m : 1.73539
[1mStep[0m  [32/42], [94mLoss[0m : 1.63487
[1mStep[0m  [36/42], [94mLoss[0m : 1.80266
[1mStep[0m  [40/42], [94mLoss[0m : 1.68358

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.771, [92mTest[0m: 2.526, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.72598
[1mStep[0m  [4/42], [94mLoss[0m : 1.65749
[1mStep[0m  [8/42], [94mLoss[0m : 1.89106
[1mStep[0m  [12/42], [94mLoss[0m : 1.79698
[1mStep[0m  [16/42], [94mLoss[0m : 1.69889
[1mStep[0m  [20/42], [94mLoss[0m : 1.80509
[1mStep[0m  [24/42], [94mLoss[0m : 1.70613
[1mStep[0m  [28/42], [94mLoss[0m : 1.81680
[1mStep[0m  [32/42], [94mLoss[0m : 1.65447
[1mStep[0m  [36/42], [94mLoss[0m : 1.65655
[1mStep[0m  [40/42], [94mLoss[0m : 2.04026

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.731, [92mTest[0m: 2.474, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.87058
[1mStep[0m  [4/42], [94mLoss[0m : 1.67531
[1mStep[0m  [8/42], [94mLoss[0m : 1.60799
[1mStep[0m  [12/42], [94mLoss[0m : 1.78510
[1mStep[0m  [16/42], [94mLoss[0m : 1.84065
[1mStep[0m  [20/42], [94mLoss[0m : 1.62223
[1mStep[0m  [24/42], [94mLoss[0m : 1.76499
[1mStep[0m  [28/42], [94mLoss[0m : 1.71803
[1mStep[0m  [32/42], [94mLoss[0m : 1.78971
[1mStep[0m  [36/42], [94mLoss[0m : 1.65281
[1mStep[0m  [40/42], [94mLoss[0m : 1.66620

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.718, [92mTest[0m: 2.554, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.75681
[1mStep[0m  [4/42], [94mLoss[0m : 1.78898
[1mStep[0m  [8/42], [94mLoss[0m : 1.66479
[1mStep[0m  [12/42], [94mLoss[0m : 1.67398
[1mStep[0m  [16/42], [94mLoss[0m : 1.68102
[1mStep[0m  [20/42], [94mLoss[0m : 1.66588
[1mStep[0m  [24/42], [94mLoss[0m : 1.72043
[1mStep[0m  [28/42], [94mLoss[0m : 1.54271
[1mStep[0m  [32/42], [94mLoss[0m : 1.63195
[1mStep[0m  [36/42], [94mLoss[0m : 1.68201
[1mStep[0m  [40/42], [94mLoss[0m : 1.68727

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.674, [92mTest[0m: 2.592, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.67010
[1mStep[0m  [4/42], [94mLoss[0m : 1.54568
[1mStep[0m  [8/42], [94mLoss[0m : 1.78185
[1mStep[0m  [12/42], [94mLoss[0m : 1.67001
[1mStep[0m  [16/42], [94mLoss[0m : 1.81748
[1mStep[0m  [20/42], [94mLoss[0m : 1.69201
[1mStep[0m  [24/42], [94mLoss[0m : 1.63165
[1mStep[0m  [28/42], [94mLoss[0m : 1.81837
[1mStep[0m  [32/42], [94mLoss[0m : 1.57900
[1mStep[0m  [36/42], [94mLoss[0m : 1.75476
[1mStep[0m  [40/42], [94mLoss[0m : 1.64822

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.692, [92mTest[0m: 2.617, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.67416
[1mStep[0m  [4/42], [94mLoss[0m : 1.78937
[1mStep[0m  [8/42], [94mLoss[0m : 1.64422
[1mStep[0m  [12/42], [94mLoss[0m : 1.66650
[1mStep[0m  [16/42], [94mLoss[0m : 1.74807
[1mStep[0m  [20/42], [94mLoss[0m : 1.61988
[1mStep[0m  [24/42], [94mLoss[0m : 1.62236
[1mStep[0m  [28/42], [94mLoss[0m : 1.52279
[1mStep[0m  [32/42], [94mLoss[0m : 1.72542
[1mStep[0m  [36/42], [94mLoss[0m : 1.67076
[1mStep[0m  [40/42], [94mLoss[0m : 1.60138

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.653, [92mTest[0m: 2.585, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.63731
[1mStep[0m  [4/42], [94mLoss[0m : 1.65126
[1mStep[0m  [8/42], [94mLoss[0m : 1.69453
[1mStep[0m  [12/42], [94mLoss[0m : 1.69076
[1mStep[0m  [16/42], [94mLoss[0m : 1.59721
[1mStep[0m  [20/42], [94mLoss[0m : 1.60149
[1mStep[0m  [24/42], [94mLoss[0m : 1.73400
[1mStep[0m  [28/42], [94mLoss[0m : 1.53712
[1mStep[0m  [32/42], [94mLoss[0m : 1.87564
[1mStep[0m  [36/42], [94mLoss[0m : 1.67795
[1mStep[0m  [40/42], [94mLoss[0m : 1.66367

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.643, [92mTest[0m: 2.673, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.534
====================================

Phase 2 - Evaluation MAE:  2.5342288187571933
MAE score P1      2.340895
MAE score P2      2.534229
loss               1.64259
learning_rate     0.007525
batch_size             256
hidden_sizes         [100]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.1
weight_decay         0.001
Name: 8, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 10.81266
[1mStep[0m  [8/84], [94mLoss[0m : 9.90380
[1mStep[0m  [16/84], [94mLoss[0m : 7.50697
[1mStep[0m  [24/84], [94mLoss[0m : 6.04029
[1mStep[0m  [32/84], [94mLoss[0m : 4.69211
[1mStep[0m  [40/84], [94mLoss[0m : 3.62649
[1mStep[0m  [48/84], [94mLoss[0m : 3.48024
[1mStep[0m  [56/84], [94mLoss[0m : 2.54323
[1mStep[0m  [64/84], [94mLoss[0m : 2.40896
[1mStep[0m  [72/84], [94mLoss[0m : 2.73824
[1mStep[0m  [80/84], [94mLoss[0m : 2.84161

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.923, [92mTest[0m: 10.851, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.88179
[1mStep[0m  [8/84], [94mLoss[0m : 2.40589
[1mStep[0m  [16/84], [94mLoss[0m : 2.91862
[1mStep[0m  [24/84], [94mLoss[0m : 2.65564
[1mStep[0m  [32/84], [94mLoss[0m : 2.61410
[1mStep[0m  [40/84], [94mLoss[0m : 2.36208
[1mStep[0m  [48/84], [94mLoss[0m : 2.56003
[1mStep[0m  [56/84], [94mLoss[0m : 2.54228
[1mStep[0m  [64/84], [94mLoss[0m : 2.57985
[1mStep[0m  [72/84], [94mLoss[0m : 2.53948
[1mStep[0m  [80/84], [94mLoss[0m : 2.85144

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.569, [92mTest[0m: 2.433, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59897
[1mStep[0m  [8/84], [94mLoss[0m : 2.47785
[1mStep[0m  [16/84], [94mLoss[0m : 2.46100
[1mStep[0m  [24/84], [94mLoss[0m : 2.94715
[1mStep[0m  [32/84], [94mLoss[0m : 2.41633
[1mStep[0m  [40/84], [94mLoss[0m : 2.61087
[1mStep[0m  [48/84], [94mLoss[0m : 2.77796
[1mStep[0m  [56/84], [94mLoss[0m : 2.47250
[1mStep[0m  [64/84], [94mLoss[0m : 2.67470
[1mStep[0m  [72/84], [94mLoss[0m : 2.40652
[1mStep[0m  [80/84], [94mLoss[0m : 2.36655

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.540, [92mTest[0m: 2.363, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52927
[1mStep[0m  [8/84], [94mLoss[0m : 2.88198
[1mStep[0m  [16/84], [94mLoss[0m : 2.55870
[1mStep[0m  [24/84], [94mLoss[0m : 2.69294
[1mStep[0m  [32/84], [94mLoss[0m : 2.33461
[1mStep[0m  [40/84], [94mLoss[0m : 2.78505
[1mStep[0m  [48/84], [94mLoss[0m : 2.67000
[1mStep[0m  [56/84], [94mLoss[0m : 2.54223
[1mStep[0m  [64/84], [94mLoss[0m : 2.30443
[1mStep[0m  [72/84], [94mLoss[0m : 2.31967
[1mStep[0m  [80/84], [94mLoss[0m : 2.36979

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.522, [92mTest[0m: 2.351, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.07816
[1mStep[0m  [8/84], [94mLoss[0m : 2.57997
[1mStep[0m  [16/84], [94mLoss[0m : 2.76972
[1mStep[0m  [24/84], [94mLoss[0m : 2.50769
[1mStep[0m  [32/84], [94mLoss[0m : 2.42586
[1mStep[0m  [40/84], [94mLoss[0m : 2.43678
[1mStep[0m  [48/84], [94mLoss[0m : 2.45575
[1mStep[0m  [56/84], [94mLoss[0m : 2.57975
[1mStep[0m  [64/84], [94mLoss[0m : 2.65233
[1mStep[0m  [72/84], [94mLoss[0m : 2.61912
[1mStep[0m  [80/84], [94mLoss[0m : 2.41807

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.352, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.69136
[1mStep[0m  [8/84], [94mLoss[0m : 2.45564
[1mStep[0m  [16/84], [94mLoss[0m : 2.03694
[1mStep[0m  [24/84], [94mLoss[0m : 2.34084
[1mStep[0m  [32/84], [94mLoss[0m : 2.31563
[1mStep[0m  [40/84], [94mLoss[0m : 2.59928
[1mStep[0m  [48/84], [94mLoss[0m : 2.47843
[1mStep[0m  [56/84], [94mLoss[0m : 2.30751
[1mStep[0m  [64/84], [94mLoss[0m : 2.65680
[1mStep[0m  [72/84], [94mLoss[0m : 2.31220
[1mStep[0m  [80/84], [94mLoss[0m : 2.60206

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.357, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45266
[1mStep[0m  [8/84], [94mLoss[0m : 2.31782
[1mStep[0m  [16/84], [94mLoss[0m : 2.32882
[1mStep[0m  [24/84], [94mLoss[0m : 2.39179
[1mStep[0m  [32/84], [94mLoss[0m : 2.32230
[1mStep[0m  [40/84], [94mLoss[0m : 2.24371
[1mStep[0m  [48/84], [94mLoss[0m : 2.22927
[1mStep[0m  [56/84], [94mLoss[0m : 2.37871
[1mStep[0m  [64/84], [94mLoss[0m : 2.16870
[1mStep[0m  [72/84], [94mLoss[0m : 2.63518
[1mStep[0m  [80/84], [94mLoss[0m : 2.50228

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.341, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.19705
[1mStep[0m  [8/84], [94mLoss[0m : 2.68007
[1mStep[0m  [16/84], [94mLoss[0m : 2.34763
[1mStep[0m  [24/84], [94mLoss[0m : 2.47056
[1mStep[0m  [32/84], [94mLoss[0m : 2.33403
[1mStep[0m  [40/84], [94mLoss[0m : 2.85516
[1mStep[0m  [48/84], [94mLoss[0m : 2.16052
[1mStep[0m  [56/84], [94mLoss[0m : 2.71060
[1mStep[0m  [64/84], [94mLoss[0m : 2.65837
[1mStep[0m  [72/84], [94mLoss[0m : 2.44922
[1mStep[0m  [80/84], [94mLoss[0m : 2.49109

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.334, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43073
[1mStep[0m  [8/84], [94mLoss[0m : 2.42905
[1mStep[0m  [16/84], [94mLoss[0m : 2.32279
[1mStep[0m  [24/84], [94mLoss[0m : 2.43460
[1mStep[0m  [32/84], [94mLoss[0m : 2.72177
[1mStep[0m  [40/84], [94mLoss[0m : 2.32762
[1mStep[0m  [48/84], [94mLoss[0m : 2.45158
[1mStep[0m  [56/84], [94mLoss[0m : 2.76238
[1mStep[0m  [64/84], [94mLoss[0m : 2.52370
[1mStep[0m  [72/84], [94mLoss[0m : 2.65714
[1mStep[0m  [80/84], [94mLoss[0m : 2.50804

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.334, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55159
[1mStep[0m  [8/84], [94mLoss[0m : 2.26158
[1mStep[0m  [16/84], [94mLoss[0m : 2.54881
[1mStep[0m  [24/84], [94mLoss[0m : 2.35950
[1mStep[0m  [32/84], [94mLoss[0m : 2.65117
[1mStep[0m  [40/84], [94mLoss[0m : 2.68322
[1mStep[0m  [48/84], [94mLoss[0m : 2.29194
[1mStep[0m  [56/84], [94mLoss[0m : 2.71778
[1mStep[0m  [64/84], [94mLoss[0m : 2.60643
[1mStep[0m  [72/84], [94mLoss[0m : 2.58439
[1mStep[0m  [80/84], [94mLoss[0m : 2.27993

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.337, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.29524
[1mStep[0m  [8/84], [94mLoss[0m : 2.56941
[1mStep[0m  [16/84], [94mLoss[0m : 2.10285
[1mStep[0m  [24/84], [94mLoss[0m : 2.33103
[1mStep[0m  [32/84], [94mLoss[0m : 2.47922
[1mStep[0m  [40/84], [94mLoss[0m : 2.63624
[1mStep[0m  [48/84], [94mLoss[0m : 2.65803
[1mStep[0m  [56/84], [94mLoss[0m : 2.45085
[1mStep[0m  [64/84], [94mLoss[0m : 2.74512
[1mStep[0m  [72/84], [94mLoss[0m : 2.24461
[1mStep[0m  [80/84], [94mLoss[0m : 2.20919

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.328, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.85092
[1mStep[0m  [8/84], [94mLoss[0m : 2.72609
[1mStep[0m  [16/84], [94mLoss[0m : 2.44324
[1mStep[0m  [24/84], [94mLoss[0m : 2.37896
[1mStep[0m  [32/84], [94mLoss[0m : 2.36989
[1mStep[0m  [40/84], [94mLoss[0m : 2.36539
[1mStep[0m  [48/84], [94mLoss[0m : 2.64727
[1mStep[0m  [56/84], [94mLoss[0m : 2.47697
[1mStep[0m  [64/84], [94mLoss[0m : 2.59516
[1mStep[0m  [72/84], [94mLoss[0m : 2.46098
[1mStep[0m  [80/84], [94mLoss[0m : 2.70384

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.322, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.64470
[1mStep[0m  [8/84], [94mLoss[0m : 2.32939
[1mStep[0m  [16/84], [94mLoss[0m : 2.38578
[1mStep[0m  [24/84], [94mLoss[0m : 2.34639
[1mStep[0m  [32/84], [94mLoss[0m : 2.52921
[1mStep[0m  [40/84], [94mLoss[0m : 2.71738
[1mStep[0m  [48/84], [94mLoss[0m : 2.70379
[1mStep[0m  [56/84], [94mLoss[0m : 2.74634
[1mStep[0m  [64/84], [94mLoss[0m : 2.80440
[1mStep[0m  [72/84], [94mLoss[0m : 2.43349
[1mStep[0m  [80/84], [94mLoss[0m : 2.30709

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.332, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49348
[1mStep[0m  [8/84], [94mLoss[0m : 2.69404
[1mStep[0m  [16/84], [94mLoss[0m : 2.68645
[1mStep[0m  [24/84], [94mLoss[0m : 2.56016
[1mStep[0m  [32/84], [94mLoss[0m : 2.20248
[1mStep[0m  [40/84], [94mLoss[0m : 2.41072
[1mStep[0m  [48/84], [94mLoss[0m : 2.26617
[1mStep[0m  [56/84], [94mLoss[0m : 2.38062
[1mStep[0m  [64/84], [94mLoss[0m : 2.37429
[1mStep[0m  [72/84], [94mLoss[0m : 2.56949
[1mStep[0m  [80/84], [94mLoss[0m : 2.51796

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.328, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48896
[1mStep[0m  [8/84], [94mLoss[0m : 2.39183
[1mStep[0m  [16/84], [94mLoss[0m : 2.63327
[1mStep[0m  [24/84], [94mLoss[0m : 2.61488
[1mStep[0m  [32/84], [94mLoss[0m : 2.80137
[1mStep[0m  [40/84], [94mLoss[0m : 2.50550
[1mStep[0m  [48/84], [94mLoss[0m : 2.29423
[1mStep[0m  [56/84], [94mLoss[0m : 2.60533
[1mStep[0m  [64/84], [94mLoss[0m : 2.38983
[1mStep[0m  [72/84], [94mLoss[0m : 2.14702
[1mStep[0m  [80/84], [94mLoss[0m : 2.53431

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.329, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46407
[1mStep[0m  [8/84], [94mLoss[0m : 2.68485
[1mStep[0m  [16/84], [94mLoss[0m : 2.53991
[1mStep[0m  [24/84], [94mLoss[0m : 2.72725
[1mStep[0m  [32/84], [94mLoss[0m : 2.33807
[1mStep[0m  [40/84], [94mLoss[0m : 2.69626
[1mStep[0m  [48/84], [94mLoss[0m : 2.60452
[1mStep[0m  [56/84], [94mLoss[0m : 2.59986
[1mStep[0m  [64/84], [94mLoss[0m : 2.50505
[1mStep[0m  [72/84], [94mLoss[0m : 2.46521
[1mStep[0m  [80/84], [94mLoss[0m : 2.59004

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.334, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.69346
[1mStep[0m  [8/84], [94mLoss[0m : 2.54185
[1mStep[0m  [16/84], [94mLoss[0m : 2.45045
[1mStep[0m  [24/84], [94mLoss[0m : 2.60801
[1mStep[0m  [32/84], [94mLoss[0m : 2.61740
[1mStep[0m  [40/84], [94mLoss[0m : 2.72172
[1mStep[0m  [48/84], [94mLoss[0m : 2.54970
[1mStep[0m  [56/84], [94mLoss[0m : 2.33702
[1mStep[0m  [64/84], [94mLoss[0m : 2.37176
[1mStep[0m  [72/84], [94mLoss[0m : 2.34556
[1mStep[0m  [80/84], [94mLoss[0m : 2.48491

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.344, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43387
[1mStep[0m  [8/84], [94mLoss[0m : 2.43648
[1mStep[0m  [16/84], [94mLoss[0m : 2.46096
[1mStep[0m  [24/84], [94mLoss[0m : 2.60495
[1mStep[0m  [32/84], [94mLoss[0m : 2.24891
[1mStep[0m  [40/84], [94mLoss[0m : 2.83800
[1mStep[0m  [48/84], [94mLoss[0m : 2.49824
[1mStep[0m  [56/84], [94mLoss[0m : 2.54720
[1mStep[0m  [64/84], [94mLoss[0m : 2.42632
[1mStep[0m  [72/84], [94mLoss[0m : 2.82068
[1mStep[0m  [80/84], [94mLoss[0m : 2.27587

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.329, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35546
[1mStep[0m  [8/84], [94mLoss[0m : 2.41876
[1mStep[0m  [16/84], [94mLoss[0m : 2.61829
[1mStep[0m  [24/84], [94mLoss[0m : 2.37715
[1mStep[0m  [32/84], [94mLoss[0m : 2.62619
[1mStep[0m  [40/84], [94mLoss[0m : 2.52380
[1mStep[0m  [48/84], [94mLoss[0m : 2.28865
[1mStep[0m  [56/84], [94mLoss[0m : 2.37892
[1mStep[0m  [64/84], [94mLoss[0m : 2.75532
[1mStep[0m  [72/84], [94mLoss[0m : 2.64451
[1mStep[0m  [80/84], [94mLoss[0m : 2.76422

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.330, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.18195
[1mStep[0m  [8/84], [94mLoss[0m : 2.67352
[1mStep[0m  [16/84], [94mLoss[0m : 2.46175
[1mStep[0m  [24/84], [94mLoss[0m : 2.61735
[1mStep[0m  [32/84], [94mLoss[0m : 2.22415
[1mStep[0m  [40/84], [94mLoss[0m : 2.44342
[1mStep[0m  [48/84], [94mLoss[0m : 2.64344
[1mStep[0m  [56/84], [94mLoss[0m : 2.24353
[1mStep[0m  [64/84], [94mLoss[0m : 2.61870
[1mStep[0m  [72/84], [94mLoss[0m : 2.48044
[1mStep[0m  [80/84], [94mLoss[0m : 2.48947

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.333, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44525
[1mStep[0m  [8/84], [94mLoss[0m : 2.56050
[1mStep[0m  [16/84], [94mLoss[0m : 2.55722
[1mStep[0m  [24/84], [94mLoss[0m : 2.44126
[1mStep[0m  [32/84], [94mLoss[0m : 2.50093
[1mStep[0m  [40/84], [94mLoss[0m : 2.33424
[1mStep[0m  [48/84], [94mLoss[0m : 2.75084
[1mStep[0m  [56/84], [94mLoss[0m : 2.41335
[1mStep[0m  [64/84], [94mLoss[0m : 2.31806
[1mStep[0m  [72/84], [94mLoss[0m : 2.60671
[1mStep[0m  [80/84], [94mLoss[0m : 2.62780

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.332, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44532
[1mStep[0m  [8/84], [94mLoss[0m : 2.40523
[1mStep[0m  [16/84], [94mLoss[0m : 2.50464
[1mStep[0m  [24/84], [94mLoss[0m : 2.30162
[1mStep[0m  [32/84], [94mLoss[0m : 2.21818
[1mStep[0m  [40/84], [94mLoss[0m : 2.34729
[1mStep[0m  [48/84], [94mLoss[0m : 2.39292
[1mStep[0m  [56/84], [94mLoss[0m : 2.36591
[1mStep[0m  [64/84], [94mLoss[0m : 2.38891
[1mStep[0m  [72/84], [94mLoss[0m : 2.78901
[1mStep[0m  [80/84], [94mLoss[0m : 2.47710

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.331, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.58789
[1mStep[0m  [8/84], [94mLoss[0m : 2.90373
[1mStep[0m  [16/84], [94mLoss[0m : 2.73536
[1mStep[0m  [24/84], [94mLoss[0m : 2.26743
[1mStep[0m  [32/84], [94mLoss[0m : 2.71870
[1mStep[0m  [40/84], [94mLoss[0m : 2.54345
[1mStep[0m  [48/84], [94mLoss[0m : 2.93901
[1mStep[0m  [56/84], [94mLoss[0m : 2.91649
[1mStep[0m  [64/84], [94mLoss[0m : 2.20391
[1mStep[0m  [72/84], [94mLoss[0m : 2.37989
[1mStep[0m  [80/84], [94mLoss[0m : 2.46345

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.338, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.57705
[1mStep[0m  [8/84], [94mLoss[0m : 2.23025
[1mStep[0m  [16/84], [94mLoss[0m : 2.61176
[1mStep[0m  [24/84], [94mLoss[0m : 2.22859
[1mStep[0m  [32/84], [94mLoss[0m : 2.69716
[1mStep[0m  [40/84], [94mLoss[0m : 2.33944
[1mStep[0m  [48/84], [94mLoss[0m : 2.56963
[1mStep[0m  [56/84], [94mLoss[0m : 2.45356
[1mStep[0m  [64/84], [94mLoss[0m : 2.62867
[1mStep[0m  [72/84], [94mLoss[0m : 2.65700
[1mStep[0m  [80/84], [94mLoss[0m : 2.38747

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.326, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41767
[1mStep[0m  [8/84], [94mLoss[0m : 2.62355
[1mStep[0m  [16/84], [94mLoss[0m : 2.46434
[1mStep[0m  [24/84], [94mLoss[0m : 2.35903
[1mStep[0m  [32/84], [94mLoss[0m : 2.33641
[1mStep[0m  [40/84], [94mLoss[0m : 3.02489
[1mStep[0m  [48/84], [94mLoss[0m : 2.32216
[1mStep[0m  [56/84], [94mLoss[0m : 2.68390
[1mStep[0m  [64/84], [94mLoss[0m : 2.33921
[1mStep[0m  [72/84], [94mLoss[0m : 2.77102
[1mStep[0m  [80/84], [94mLoss[0m : 2.57811

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.320, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.34949
[1mStep[0m  [8/84], [94mLoss[0m : 2.32447
[1mStep[0m  [16/84], [94mLoss[0m : 2.55589
[1mStep[0m  [24/84], [94mLoss[0m : 2.26622
[1mStep[0m  [32/84], [94mLoss[0m : 2.28801
[1mStep[0m  [40/84], [94mLoss[0m : 2.51680
[1mStep[0m  [48/84], [94mLoss[0m : 2.60802
[1mStep[0m  [56/84], [94mLoss[0m : 2.55006
[1mStep[0m  [64/84], [94mLoss[0m : 2.33521
[1mStep[0m  [72/84], [94mLoss[0m : 2.22758
[1mStep[0m  [80/84], [94mLoss[0m : 2.87562

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.332, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49419
[1mStep[0m  [8/84], [94mLoss[0m : 2.42762
[1mStep[0m  [16/84], [94mLoss[0m : 2.56884
[1mStep[0m  [24/84], [94mLoss[0m : 2.50695
[1mStep[0m  [32/84], [94mLoss[0m : 2.48188
[1mStep[0m  [40/84], [94mLoss[0m : 2.50900
[1mStep[0m  [48/84], [94mLoss[0m : 2.40246
[1mStep[0m  [56/84], [94mLoss[0m : 2.44871
[1mStep[0m  [64/84], [94mLoss[0m : 2.35557
[1mStep[0m  [72/84], [94mLoss[0m : 2.58985
[1mStep[0m  [80/84], [94mLoss[0m : 2.48762

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.327, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36011
[1mStep[0m  [8/84], [94mLoss[0m : 2.38577
[1mStep[0m  [16/84], [94mLoss[0m : 2.51366
[1mStep[0m  [24/84], [94mLoss[0m : 2.74606
[1mStep[0m  [32/84], [94mLoss[0m : 2.39472
[1mStep[0m  [40/84], [94mLoss[0m : 2.81106
[1mStep[0m  [48/84], [94mLoss[0m : 2.35975
[1mStep[0m  [56/84], [94mLoss[0m : 2.26128
[1mStep[0m  [64/84], [94mLoss[0m : 2.67189
[1mStep[0m  [72/84], [94mLoss[0m : 2.50148
[1mStep[0m  [80/84], [94mLoss[0m : 2.35976

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.333, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.81946
[1mStep[0m  [8/84], [94mLoss[0m : 2.41769
[1mStep[0m  [16/84], [94mLoss[0m : 2.59453
[1mStep[0m  [24/84], [94mLoss[0m : 2.14860
[1mStep[0m  [32/84], [94mLoss[0m : 2.80289
[1mStep[0m  [40/84], [94mLoss[0m : 2.47570
[1mStep[0m  [48/84], [94mLoss[0m : 2.51547
[1mStep[0m  [56/84], [94mLoss[0m : 2.23677
[1mStep[0m  [64/84], [94mLoss[0m : 2.62196
[1mStep[0m  [72/84], [94mLoss[0m : 2.18988
[1mStep[0m  [80/84], [94mLoss[0m : 2.14169

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.328, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45770
[1mStep[0m  [8/84], [94mLoss[0m : 2.42361
[1mStep[0m  [16/84], [94mLoss[0m : 2.28947
[1mStep[0m  [24/84], [94mLoss[0m : 2.39106
[1mStep[0m  [32/84], [94mLoss[0m : 2.00822
[1mStep[0m  [40/84], [94mLoss[0m : 2.81974
[1mStep[0m  [48/84], [94mLoss[0m : 2.43138
[1mStep[0m  [56/84], [94mLoss[0m : 2.40086
[1mStep[0m  [64/84], [94mLoss[0m : 2.39067
[1mStep[0m  [72/84], [94mLoss[0m : 2.63614
[1mStep[0m  [80/84], [94mLoss[0m : 2.62476

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.327, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.327
====================================

Phase 1 - Evaluation MAE:  2.3274461797305515
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 2.58563
[1mStep[0m  [8/84], [94mLoss[0m : 2.35365
[1mStep[0m  [16/84], [94mLoss[0m : 2.35346
[1mStep[0m  [24/84], [94mLoss[0m : 2.73406
[1mStep[0m  [32/84], [94mLoss[0m : 2.72013
[1mStep[0m  [40/84], [94mLoss[0m : 2.44805
[1mStep[0m  [48/84], [94mLoss[0m : 2.51481
[1mStep[0m  [56/84], [94mLoss[0m : 2.53110
[1mStep[0m  [64/84], [94mLoss[0m : 2.35883
[1mStep[0m  [72/84], [94mLoss[0m : 2.49466
[1mStep[0m  [80/84], [94mLoss[0m : 2.22755

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.312, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44393
[1mStep[0m  [8/84], [94mLoss[0m : 2.10954
[1mStep[0m  [16/84], [94mLoss[0m : 2.66399
[1mStep[0m  [24/84], [94mLoss[0m : 2.63788
[1mStep[0m  [32/84], [94mLoss[0m : 2.60900
[1mStep[0m  [40/84], [94mLoss[0m : 2.55243
[1mStep[0m  [48/84], [94mLoss[0m : 2.49834
[1mStep[0m  [56/84], [94mLoss[0m : 2.34170
[1mStep[0m  [64/84], [94mLoss[0m : 2.60431
[1mStep[0m  [72/84], [94mLoss[0m : 2.84992
[1mStep[0m  [80/84], [94mLoss[0m : 2.40723

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.616, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47661
[1mStep[0m  [8/84], [94mLoss[0m : 2.16874
[1mStep[0m  [16/84], [94mLoss[0m : 2.37745
[1mStep[0m  [24/84], [94mLoss[0m : 2.26715
[1mStep[0m  [32/84], [94mLoss[0m : 2.18082
[1mStep[0m  [40/84], [94mLoss[0m : 2.32154
[1mStep[0m  [48/84], [94mLoss[0m : 2.18582
[1mStep[0m  [56/84], [94mLoss[0m : 2.50687
[1mStep[0m  [64/84], [94mLoss[0m : 2.44637
[1mStep[0m  [72/84], [94mLoss[0m : 2.24828
[1mStep[0m  [80/84], [94mLoss[0m : 2.24229

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.360, [92mTest[0m: 2.513, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53037
[1mStep[0m  [8/84], [94mLoss[0m : 2.39040
[1mStep[0m  [16/84], [94mLoss[0m : 2.12419
[1mStep[0m  [24/84], [94mLoss[0m : 2.25719
[1mStep[0m  [32/84], [94mLoss[0m : 2.37205
[1mStep[0m  [40/84], [94mLoss[0m : 2.45257
[1mStep[0m  [48/84], [94mLoss[0m : 2.28714
[1mStep[0m  [56/84], [94mLoss[0m : 2.15524
[1mStep[0m  [64/84], [94mLoss[0m : 2.05261
[1mStep[0m  [72/84], [94mLoss[0m : 2.32352
[1mStep[0m  [80/84], [94mLoss[0m : 2.18961

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.307, [92mTest[0m: 2.411, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.09298
[1mStep[0m  [8/84], [94mLoss[0m : 2.28657
[1mStep[0m  [16/84], [94mLoss[0m : 2.09804
[1mStep[0m  [24/84], [94mLoss[0m : 2.35696
[1mStep[0m  [32/84], [94mLoss[0m : 2.16072
[1mStep[0m  [40/84], [94mLoss[0m : 2.37402
[1mStep[0m  [48/84], [94mLoss[0m : 2.40884
[1mStep[0m  [56/84], [94mLoss[0m : 2.40604
[1mStep[0m  [64/84], [94mLoss[0m : 2.17669
[1mStep[0m  [72/84], [94mLoss[0m : 2.33681
[1mStep[0m  [80/84], [94mLoss[0m : 2.14269

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.253, [92mTest[0m: 2.413, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26818
[1mStep[0m  [8/84], [94mLoss[0m : 2.12330
[1mStep[0m  [16/84], [94mLoss[0m : 2.09976
[1mStep[0m  [24/84], [94mLoss[0m : 2.09707
[1mStep[0m  [32/84], [94mLoss[0m : 2.08702
[1mStep[0m  [40/84], [94mLoss[0m : 2.32434
[1mStep[0m  [48/84], [94mLoss[0m : 2.31247
[1mStep[0m  [56/84], [94mLoss[0m : 2.43570
[1mStep[0m  [64/84], [94mLoss[0m : 2.27672
[1mStep[0m  [72/84], [94mLoss[0m : 2.50933
[1mStep[0m  [80/84], [94mLoss[0m : 2.15468

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.206, [92mTest[0m: 2.399, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.11199
[1mStep[0m  [8/84], [94mLoss[0m : 2.54191
[1mStep[0m  [16/84], [94mLoss[0m : 2.00109
[1mStep[0m  [24/84], [94mLoss[0m : 1.96458
[1mStep[0m  [32/84], [94mLoss[0m : 2.07924
[1mStep[0m  [40/84], [94mLoss[0m : 2.00554
[1mStep[0m  [48/84], [94mLoss[0m : 1.98630
[1mStep[0m  [56/84], [94mLoss[0m : 2.12222
[1mStep[0m  [64/84], [94mLoss[0m : 2.37658
[1mStep[0m  [72/84], [94mLoss[0m : 2.14675
[1mStep[0m  [80/84], [94mLoss[0m : 2.54330

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.132, [92mTest[0m: 2.478, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.22579
[1mStep[0m  [8/84], [94mLoss[0m : 2.08296
[1mStep[0m  [16/84], [94mLoss[0m : 1.82742
[1mStep[0m  [24/84], [94mLoss[0m : 2.15023
[1mStep[0m  [32/84], [94mLoss[0m : 2.33875
[1mStep[0m  [40/84], [94mLoss[0m : 2.12085
[1mStep[0m  [48/84], [94mLoss[0m : 2.07617
[1mStep[0m  [56/84], [94mLoss[0m : 1.91671
[1mStep[0m  [64/84], [94mLoss[0m : 2.19048
[1mStep[0m  [72/84], [94mLoss[0m : 2.09776
[1mStep[0m  [80/84], [94mLoss[0m : 1.73642

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.082, [92mTest[0m: 2.376, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.94352
[1mStep[0m  [8/84], [94mLoss[0m : 1.91845
[1mStep[0m  [16/84], [94mLoss[0m : 1.93890
[1mStep[0m  [24/84], [94mLoss[0m : 2.29088
[1mStep[0m  [32/84], [94mLoss[0m : 1.90104
[1mStep[0m  [40/84], [94mLoss[0m : 2.32631
[1mStep[0m  [48/84], [94mLoss[0m : 1.80868
[1mStep[0m  [56/84], [94mLoss[0m : 2.11496
[1mStep[0m  [64/84], [94mLoss[0m : 2.32805
[1mStep[0m  [72/84], [94mLoss[0m : 2.04682
[1mStep[0m  [80/84], [94mLoss[0m : 2.13259

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.044, [92mTest[0m: 2.442, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.72553
[1mStep[0m  [8/84], [94mLoss[0m : 2.13415
[1mStep[0m  [16/84], [94mLoss[0m : 2.11947
[1mStep[0m  [24/84], [94mLoss[0m : 2.12800
[1mStep[0m  [32/84], [94mLoss[0m : 1.79926
[1mStep[0m  [40/84], [94mLoss[0m : 1.95236
[1mStep[0m  [48/84], [94mLoss[0m : 1.81551
[1mStep[0m  [56/84], [94mLoss[0m : 2.15454
[1mStep[0m  [64/84], [94mLoss[0m : 1.78774
[1mStep[0m  [72/84], [94mLoss[0m : 2.23706
[1mStep[0m  [80/84], [94mLoss[0m : 2.25073

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.978, [92mTest[0m: 2.385, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.89128
[1mStep[0m  [8/84], [94mLoss[0m : 1.79506
[1mStep[0m  [16/84], [94mLoss[0m : 1.95732
[1mStep[0m  [24/84], [94mLoss[0m : 1.72827
[1mStep[0m  [32/84], [94mLoss[0m : 2.01581
[1mStep[0m  [40/84], [94mLoss[0m : 1.67614
[1mStep[0m  [48/84], [94mLoss[0m : 1.80197
[1mStep[0m  [56/84], [94mLoss[0m : 1.94503
[1mStep[0m  [64/84], [94mLoss[0m : 1.68104
[1mStep[0m  [72/84], [94mLoss[0m : 2.04385
[1mStep[0m  [80/84], [94mLoss[0m : 2.31431

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.930, [92mTest[0m: 2.534, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.92076
[1mStep[0m  [8/84], [94mLoss[0m : 1.84739
[1mStep[0m  [16/84], [94mLoss[0m : 1.75252
[1mStep[0m  [24/84], [94mLoss[0m : 1.56091
[1mStep[0m  [32/84], [94mLoss[0m : 2.01519
[1mStep[0m  [40/84], [94mLoss[0m : 1.90505
[1mStep[0m  [48/84], [94mLoss[0m : 1.83901
[1mStep[0m  [56/84], [94mLoss[0m : 1.81061
[1mStep[0m  [64/84], [94mLoss[0m : 2.02589
[1mStep[0m  [72/84], [94mLoss[0m : 1.90026
[1mStep[0m  [80/84], [94mLoss[0m : 1.92192

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.895, [92mTest[0m: 2.532, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.64181
[1mStep[0m  [8/84], [94mLoss[0m : 1.71986
[1mStep[0m  [16/84], [94mLoss[0m : 1.71512
[1mStep[0m  [24/84], [94mLoss[0m : 1.82981
[1mStep[0m  [32/84], [94mLoss[0m : 1.73627
[1mStep[0m  [40/84], [94mLoss[0m : 1.74483
[1mStep[0m  [48/84], [94mLoss[0m : 2.04556
[1mStep[0m  [56/84], [94mLoss[0m : 1.88983
[1mStep[0m  [64/84], [94mLoss[0m : 2.16568
[1mStep[0m  [72/84], [94mLoss[0m : 1.94819
[1mStep[0m  [80/84], [94mLoss[0m : 1.82601

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.857, [92mTest[0m: 2.438, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.80941
[1mStep[0m  [8/84], [94mLoss[0m : 1.69463
[1mStep[0m  [16/84], [94mLoss[0m : 1.73086
[1mStep[0m  [24/84], [94mLoss[0m : 1.77301
[1mStep[0m  [32/84], [94mLoss[0m : 1.84261
[1mStep[0m  [40/84], [94mLoss[0m : 1.70768
[1mStep[0m  [48/84], [94mLoss[0m : 1.69175
[1mStep[0m  [56/84], [94mLoss[0m : 2.14028
[1mStep[0m  [64/84], [94mLoss[0m : 1.74594
[1mStep[0m  [72/84], [94mLoss[0m : 1.91224
[1mStep[0m  [80/84], [94mLoss[0m : 1.89138

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.818, [92mTest[0m: 2.481, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.69746
[1mStep[0m  [8/84], [94mLoss[0m : 1.63587
[1mStep[0m  [16/84], [94mLoss[0m : 1.70914
[1mStep[0m  [24/84], [94mLoss[0m : 2.06881
[1mStep[0m  [32/84], [94mLoss[0m : 1.79419
[1mStep[0m  [40/84], [94mLoss[0m : 1.84813
[1mStep[0m  [48/84], [94mLoss[0m : 1.63897
[1mStep[0m  [56/84], [94mLoss[0m : 1.75974
[1mStep[0m  [64/84], [94mLoss[0m : 1.71176
[1mStep[0m  [72/84], [94mLoss[0m : 1.76746
[1mStep[0m  [80/84], [94mLoss[0m : 1.66209

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.772, [92mTest[0m: 2.429, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.83751
[1mStep[0m  [8/84], [94mLoss[0m : 1.73238
[1mStep[0m  [16/84], [94mLoss[0m : 1.79752
[1mStep[0m  [24/84], [94mLoss[0m : 1.63132
[1mStep[0m  [32/84], [94mLoss[0m : 1.82510
[1mStep[0m  [40/84], [94mLoss[0m : 1.79678
[1mStep[0m  [48/84], [94mLoss[0m : 1.66074
[1mStep[0m  [56/84], [94mLoss[0m : 1.68744
[1mStep[0m  [64/84], [94mLoss[0m : 1.60530
[1mStep[0m  [72/84], [94mLoss[0m : 1.79098
[1mStep[0m  [80/84], [94mLoss[0m : 1.65781

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.760, [92mTest[0m: 2.519, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.61972
[1mStep[0m  [8/84], [94mLoss[0m : 1.46890
[1mStep[0m  [16/84], [94mLoss[0m : 1.74115
[1mStep[0m  [24/84], [94mLoss[0m : 1.82839
[1mStep[0m  [32/84], [94mLoss[0m : 1.67590
[1mStep[0m  [40/84], [94mLoss[0m : 1.84692
[1mStep[0m  [48/84], [94mLoss[0m : 1.46551
[1mStep[0m  [56/84], [94mLoss[0m : 1.70884
[1mStep[0m  [64/84], [94mLoss[0m : 1.92430
[1mStep[0m  [72/84], [94mLoss[0m : 1.81337
[1mStep[0m  [80/84], [94mLoss[0m : 1.75627

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.699, [92mTest[0m: 2.504, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.62046
[1mStep[0m  [8/84], [94mLoss[0m : 1.55281
[1mStep[0m  [16/84], [94mLoss[0m : 1.46823
[1mStep[0m  [24/84], [94mLoss[0m : 2.04742
[1mStep[0m  [32/84], [94mLoss[0m : 1.88897
[1mStep[0m  [40/84], [94mLoss[0m : 1.75702
[1mStep[0m  [48/84], [94mLoss[0m : 1.62221
[1mStep[0m  [56/84], [94mLoss[0m : 1.58560
[1mStep[0m  [64/84], [94mLoss[0m : 1.86657
[1mStep[0m  [72/84], [94mLoss[0m : 1.64984
[1mStep[0m  [80/84], [94mLoss[0m : 1.72729

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.681, [92mTest[0m: 2.496, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.53824
[1mStep[0m  [8/84], [94mLoss[0m : 1.41992
[1mStep[0m  [16/84], [94mLoss[0m : 1.66557
[1mStep[0m  [24/84], [94mLoss[0m : 1.64685
[1mStep[0m  [32/84], [94mLoss[0m : 1.55771
[1mStep[0m  [40/84], [94mLoss[0m : 1.70113
[1mStep[0m  [48/84], [94mLoss[0m : 1.60979
[1mStep[0m  [56/84], [94mLoss[0m : 1.71157
[1mStep[0m  [64/84], [94mLoss[0m : 1.54572
[1mStep[0m  [72/84], [94mLoss[0m : 1.84045
[1mStep[0m  [80/84], [94mLoss[0m : 1.68669

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.624, [92mTest[0m: 2.506, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.45407
[1mStep[0m  [8/84], [94mLoss[0m : 1.49391
[1mStep[0m  [16/84], [94mLoss[0m : 1.49136
[1mStep[0m  [24/84], [94mLoss[0m : 1.64346
[1mStep[0m  [32/84], [94mLoss[0m : 1.78685
[1mStep[0m  [40/84], [94mLoss[0m : 1.75634
[1mStep[0m  [48/84], [94mLoss[0m : 1.50381
[1mStep[0m  [56/84], [94mLoss[0m : 1.51442
[1mStep[0m  [64/84], [94mLoss[0m : 1.54709
[1mStep[0m  [72/84], [94mLoss[0m : 1.76908
[1mStep[0m  [80/84], [94mLoss[0m : 1.54030

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.632, [92mTest[0m: 2.488, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.55466
[1mStep[0m  [8/84], [94mLoss[0m : 1.74454
[1mStep[0m  [16/84], [94mLoss[0m : 1.84770
[1mStep[0m  [24/84], [94mLoss[0m : 1.73593
[1mStep[0m  [32/84], [94mLoss[0m : 1.63386
[1mStep[0m  [40/84], [94mLoss[0m : 1.52683
[1mStep[0m  [48/84], [94mLoss[0m : 1.70211
[1mStep[0m  [56/84], [94mLoss[0m : 1.63938
[1mStep[0m  [64/84], [94mLoss[0m : 1.44738
[1mStep[0m  [72/84], [94mLoss[0m : 1.60720
[1mStep[0m  [80/84], [94mLoss[0m : 1.56392

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.575, [92mTest[0m: 2.631, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.51069
[1mStep[0m  [8/84], [94mLoss[0m : 1.51693
[1mStep[0m  [16/84], [94mLoss[0m : 1.40628
[1mStep[0m  [24/84], [94mLoss[0m : 1.45378
[1mStep[0m  [32/84], [94mLoss[0m : 1.49756
[1mStep[0m  [40/84], [94mLoss[0m : 1.85218
[1mStep[0m  [48/84], [94mLoss[0m : 1.56977
[1mStep[0m  [56/84], [94mLoss[0m : 1.65034
[1mStep[0m  [64/84], [94mLoss[0m : 1.54139
[1mStep[0m  [72/84], [94mLoss[0m : 1.68658
[1mStep[0m  [80/84], [94mLoss[0m : 1.53244

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.545, [92mTest[0m: 2.501, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.45429
[1mStep[0m  [8/84], [94mLoss[0m : 1.46865
[1mStep[0m  [16/84], [94mLoss[0m : 1.50804
[1mStep[0m  [24/84], [94mLoss[0m : 1.89217
[1mStep[0m  [32/84], [94mLoss[0m : 1.56654
[1mStep[0m  [40/84], [94mLoss[0m : 1.61827
[1mStep[0m  [48/84], [94mLoss[0m : 1.32164
[1mStep[0m  [56/84], [94mLoss[0m : 1.55735
[1mStep[0m  [64/84], [94mLoss[0m : 1.45835
[1mStep[0m  [72/84], [94mLoss[0m : 1.60968
[1mStep[0m  [80/84], [94mLoss[0m : 1.52523

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.531, [92mTest[0m: 2.502, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.39143
[1mStep[0m  [8/84], [94mLoss[0m : 1.29797
[1mStep[0m  [16/84], [94mLoss[0m : 1.54040
[1mStep[0m  [24/84], [94mLoss[0m : 1.45945
[1mStep[0m  [32/84], [94mLoss[0m : 1.41534
[1mStep[0m  [40/84], [94mLoss[0m : 1.47747
[1mStep[0m  [48/84], [94mLoss[0m : 1.46058
[1mStep[0m  [56/84], [94mLoss[0m : 1.71601
[1mStep[0m  [64/84], [94mLoss[0m : 1.42164
[1mStep[0m  [72/84], [94mLoss[0m : 1.61518
[1mStep[0m  [80/84], [94mLoss[0m : 1.49205

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.495, [92mTest[0m: 2.486, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.46583
[1mStep[0m  [8/84], [94mLoss[0m : 1.49886
[1mStep[0m  [16/84], [94mLoss[0m : 1.57481
[1mStep[0m  [24/84], [94mLoss[0m : 1.66674
[1mStep[0m  [32/84], [94mLoss[0m : 1.53430
[1mStep[0m  [40/84], [94mLoss[0m : 1.62731
[1mStep[0m  [48/84], [94mLoss[0m : 1.30653
[1mStep[0m  [56/84], [94mLoss[0m : 1.59581
[1mStep[0m  [64/84], [94mLoss[0m : 1.63593
[1mStep[0m  [72/84], [94mLoss[0m : 1.51472
[1mStep[0m  [80/84], [94mLoss[0m : 1.47063

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.485, [92mTest[0m: 2.522, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.30308
[1mStep[0m  [8/84], [94mLoss[0m : 1.44216
[1mStep[0m  [16/84], [94mLoss[0m : 1.15097
[1mStep[0m  [24/84], [94mLoss[0m : 1.66558
[1mStep[0m  [32/84], [94mLoss[0m : 1.40930
[1mStep[0m  [40/84], [94mLoss[0m : 1.45770
[1mStep[0m  [48/84], [94mLoss[0m : 1.70062
[1mStep[0m  [56/84], [94mLoss[0m : 1.35379
[1mStep[0m  [64/84], [94mLoss[0m : 1.62501
[1mStep[0m  [72/84], [94mLoss[0m : 1.42053
[1mStep[0m  [80/84], [94mLoss[0m : 1.48708

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.477, [92mTest[0m: 2.566, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.55671
[1mStep[0m  [8/84], [94mLoss[0m : 1.45626
[1mStep[0m  [16/84], [94mLoss[0m : 1.48975
[1mStep[0m  [24/84], [94mLoss[0m : 1.48985
[1mStep[0m  [32/84], [94mLoss[0m : 1.53778
[1mStep[0m  [40/84], [94mLoss[0m : 1.42984
[1mStep[0m  [48/84], [94mLoss[0m : 1.33909
[1mStep[0m  [56/84], [94mLoss[0m : 1.52632
[1mStep[0m  [64/84], [94mLoss[0m : 1.42821
[1mStep[0m  [72/84], [94mLoss[0m : 1.40082
[1mStep[0m  [80/84], [94mLoss[0m : 1.71800

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.446, [92mTest[0m: 2.524, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.33823
[1mStep[0m  [8/84], [94mLoss[0m : 1.43532
[1mStep[0m  [16/84], [94mLoss[0m : 1.40953
[1mStep[0m  [24/84], [94mLoss[0m : 1.46058
[1mStep[0m  [32/84], [94mLoss[0m : 1.54225
[1mStep[0m  [40/84], [94mLoss[0m : 1.50916
[1mStep[0m  [48/84], [94mLoss[0m : 1.76401
[1mStep[0m  [56/84], [94mLoss[0m : 1.42117
[1mStep[0m  [64/84], [94mLoss[0m : 1.51950
[1mStep[0m  [72/84], [94mLoss[0m : 1.61760
[1mStep[0m  [80/84], [94mLoss[0m : 1.18427

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.432, [92mTest[0m: 2.524, [96mlr[0m: 0.006772500000000002
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 27 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.529
====================================

Phase 2 - Evaluation MAE:  2.5285870092255727
MAE score P1       2.327446
MAE score P2       2.528587
loss                1.43232
learning_rate      0.007525
batch_size              128
hidden_sizes      [200, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping         True
dropout                 0.2
momentum                0.5
weight_decay         0.0001
Name: 9, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 10.49688
[1mStep[0m  [8/84], [94mLoss[0m : 10.77728
[1mStep[0m  [16/84], [94mLoss[0m : 10.72681
[1mStep[0m  [24/84], [94mLoss[0m : 10.89073
[1mStep[0m  [32/84], [94mLoss[0m : 11.14035
[1mStep[0m  [40/84], [94mLoss[0m : 10.17130
[1mStep[0m  [48/84], [94mLoss[0m : 10.58917
[1mStep[0m  [56/84], [94mLoss[0m : 9.99951
[1mStep[0m  [64/84], [94mLoss[0m : 10.63772
[1mStep[0m  [72/84], [94mLoss[0m : 9.67793
[1mStep[0m  [80/84], [94mLoss[0m : 9.99134

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.361, [92mTest[0m: 10.834, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.37140
[1mStep[0m  [8/84], [94mLoss[0m : 9.67155
[1mStep[0m  [16/84], [94mLoss[0m : 8.99606
[1mStep[0m  [24/84], [94mLoss[0m : 9.42942
[1mStep[0m  [32/84], [94mLoss[0m : 9.25056
[1mStep[0m  [40/84], [94mLoss[0m : 9.18692
[1mStep[0m  [48/84], [94mLoss[0m : 9.05825
[1mStep[0m  [56/84], [94mLoss[0m : 9.20055
[1mStep[0m  [64/84], [94mLoss[0m : 8.68604
[1mStep[0m  [72/84], [94mLoss[0m : 8.12147
[1mStep[0m  [80/84], [94mLoss[0m : 8.16723

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 8.943, [92mTest[0m: 9.277, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.89419
[1mStep[0m  [8/84], [94mLoss[0m : 8.09128
[1mStep[0m  [16/84], [94mLoss[0m : 7.63113
[1mStep[0m  [24/84], [94mLoss[0m : 7.23176
[1mStep[0m  [32/84], [94mLoss[0m : 7.49567
[1mStep[0m  [40/84], [94mLoss[0m : 7.83116
[1mStep[0m  [48/84], [94mLoss[0m : 7.34821
[1mStep[0m  [56/84], [94mLoss[0m : 7.03834
[1mStep[0m  [64/84], [94mLoss[0m : 6.86855
[1mStep[0m  [72/84], [94mLoss[0m : 6.67750
[1mStep[0m  [80/84], [94mLoss[0m : 6.21732

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 7.324, [92mTest[0m: 7.384, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.81622
[1mStep[0m  [8/84], [94mLoss[0m : 6.15637
[1mStep[0m  [16/84], [94mLoss[0m : 6.29540
[1mStep[0m  [24/84], [94mLoss[0m : 6.51480
[1mStep[0m  [32/84], [94mLoss[0m : 5.69877
[1mStep[0m  [40/84], [94mLoss[0m : 6.23369
[1mStep[0m  [48/84], [94mLoss[0m : 5.32581
[1mStep[0m  [56/84], [94mLoss[0m : 5.79956
[1mStep[0m  [64/84], [94mLoss[0m : 5.86779
[1mStep[0m  [72/84], [94mLoss[0m : 5.11168
[1mStep[0m  [80/84], [94mLoss[0m : 5.01122

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 5.938, [92mTest[0m: 5.524, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.10179
[1mStep[0m  [8/84], [94mLoss[0m : 4.75408
[1mStep[0m  [16/84], [94mLoss[0m : 4.73180
[1mStep[0m  [24/84], [94mLoss[0m : 4.55911
[1mStep[0m  [32/84], [94mLoss[0m : 4.77784
[1mStep[0m  [40/84], [94mLoss[0m : 4.76442
[1mStep[0m  [48/84], [94mLoss[0m : 4.06947
[1mStep[0m  [56/84], [94mLoss[0m : 4.33691
[1mStep[0m  [64/84], [94mLoss[0m : 3.90462
[1mStep[0m  [72/84], [94mLoss[0m : 3.22647
[1mStep[0m  [80/84], [94mLoss[0m : 3.53945

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 4.445, [92mTest[0m: 4.238, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.10965
[1mStep[0m  [8/84], [94mLoss[0m : 3.49907
[1mStep[0m  [16/84], [94mLoss[0m : 3.41737
[1mStep[0m  [24/84], [94mLoss[0m : 2.79713
[1mStep[0m  [32/84], [94mLoss[0m : 2.89025
[1mStep[0m  [40/84], [94mLoss[0m : 3.15668
[1mStep[0m  [48/84], [94mLoss[0m : 3.15752
[1mStep[0m  [56/84], [94mLoss[0m : 3.11827
[1mStep[0m  [64/84], [94mLoss[0m : 2.76621
[1mStep[0m  [72/84], [94mLoss[0m : 2.41308
[1mStep[0m  [80/84], [94mLoss[0m : 3.05088

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 3.084, [92mTest[0m: 2.814, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.58187
[1mStep[0m  [8/84], [94mLoss[0m : 2.49119
[1mStep[0m  [16/84], [94mLoss[0m : 2.68709
[1mStep[0m  [24/84], [94mLoss[0m : 2.83297
[1mStep[0m  [32/84], [94mLoss[0m : 2.80715
[1mStep[0m  [40/84], [94mLoss[0m : 2.94958
[1mStep[0m  [48/84], [94mLoss[0m : 2.97817
[1mStep[0m  [56/84], [94mLoss[0m : 2.64436
[1mStep[0m  [64/84], [94mLoss[0m : 2.50266
[1mStep[0m  [72/84], [94mLoss[0m : 2.66361
[1mStep[0m  [80/84], [94mLoss[0m : 2.76499

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.734, [92mTest[0m: 2.398, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.64163
[1mStep[0m  [8/84], [94mLoss[0m : 2.77112
[1mStep[0m  [16/84], [94mLoss[0m : 2.92121
[1mStep[0m  [24/84], [94mLoss[0m : 2.81304
[1mStep[0m  [32/84], [94mLoss[0m : 2.20406
[1mStep[0m  [40/84], [94mLoss[0m : 2.88829
[1mStep[0m  [48/84], [94mLoss[0m : 2.36613
[1mStep[0m  [56/84], [94mLoss[0m : 2.56116
[1mStep[0m  [64/84], [94mLoss[0m : 2.44737
[1mStep[0m  [72/84], [94mLoss[0m : 2.64981
[1mStep[0m  [80/84], [94mLoss[0m : 2.42009

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.658, [92mTest[0m: 2.416, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59499
[1mStep[0m  [8/84], [94mLoss[0m : 2.87898
[1mStep[0m  [16/84], [94mLoss[0m : 2.68279
[1mStep[0m  [24/84], [94mLoss[0m : 2.53936
[1mStep[0m  [32/84], [94mLoss[0m : 2.41539
[1mStep[0m  [40/84], [94mLoss[0m : 2.75141
[1mStep[0m  [48/84], [94mLoss[0m : 2.95146
[1mStep[0m  [56/84], [94mLoss[0m : 2.76821
[1mStep[0m  [64/84], [94mLoss[0m : 2.47548
[1mStep[0m  [72/84], [94mLoss[0m : 2.45143
[1mStep[0m  [80/84], [94mLoss[0m : 2.50751

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.637, [92mTest[0m: 2.392, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.63064
[1mStep[0m  [8/84], [94mLoss[0m : 2.54082
[1mStep[0m  [16/84], [94mLoss[0m : 2.47763
[1mStep[0m  [24/84], [94mLoss[0m : 2.67251
[1mStep[0m  [32/84], [94mLoss[0m : 2.87486
[1mStep[0m  [40/84], [94mLoss[0m : 2.45839
[1mStep[0m  [48/84], [94mLoss[0m : 2.46247
[1mStep[0m  [56/84], [94mLoss[0m : 2.96300
[1mStep[0m  [64/84], [94mLoss[0m : 2.82481
[1mStep[0m  [72/84], [94mLoss[0m : 2.42001
[1mStep[0m  [80/84], [94mLoss[0m : 2.33407

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.617, [92mTest[0m: 2.369, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.68011
[1mStep[0m  [8/84], [94mLoss[0m : 2.55723
[1mStep[0m  [16/84], [94mLoss[0m : 2.48365
[1mStep[0m  [24/84], [94mLoss[0m : 2.62236
[1mStep[0m  [32/84], [94mLoss[0m : 2.58181
[1mStep[0m  [40/84], [94mLoss[0m : 2.46339
[1mStep[0m  [48/84], [94mLoss[0m : 2.61227
[1mStep[0m  [56/84], [94mLoss[0m : 2.60203
[1mStep[0m  [64/84], [94mLoss[0m : 2.34056
[1mStep[0m  [72/84], [94mLoss[0m : 2.59685
[1mStep[0m  [80/84], [94mLoss[0m : 2.55772

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.604, [92mTest[0m: 2.377, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46194
[1mStep[0m  [8/84], [94mLoss[0m : 2.28772
[1mStep[0m  [16/84], [94mLoss[0m : 2.88948
[1mStep[0m  [24/84], [94mLoss[0m : 2.42005
[1mStep[0m  [32/84], [94mLoss[0m : 2.30602
[1mStep[0m  [40/84], [94mLoss[0m : 2.64026
[1mStep[0m  [48/84], [94mLoss[0m : 2.63202
[1mStep[0m  [56/84], [94mLoss[0m : 2.81231
[1mStep[0m  [64/84], [94mLoss[0m : 2.50506
[1mStep[0m  [72/84], [94mLoss[0m : 2.44149
[1mStep[0m  [80/84], [94mLoss[0m : 2.32051

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.583, [92mTest[0m: 2.368, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.57280
[1mStep[0m  [8/84], [94mLoss[0m : 2.55594
[1mStep[0m  [16/84], [94mLoss[0m : 2.69691
[1mStep[0m  [24/84], [94mLoss[0m : 2.69334
[1mStep[0m  [32/84], [94mLoss[0m : 3.08029
[1mStep[0m  [40/84], [94mLoss[0m : 2.95065
[1mStep[0m  [48/84], [94mLoss[0m : 2.55420
[1mStep[0m  [56/84], [94mLoss[0m : 2.68178
[1mStep[0m  [64/84], [94mLoss[0m : 2.52595
[1mStep[0m  [72/84], [94mLoss[0m : 2.47752
[1mStep[0m  [80/84], [94mLoss[0m : 2.53368

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.582, [92mTest[0m: 2.391, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30395
[1mStep[0m  [8/84], [94mLoss[0m : 2.75031
[1mStep[0m  [16/84], [94mLoss[0m : 2.21110
[1mStep[0m  [24/84], [94mLoss[0m : 2.43692
[1mStep[0m  [32/84], [94mLoss[0m : 2.80436
[1mStep[0m  [40/84], [94mLoss[0m : 2.81259
[1mStep[0m  [48/84], [94mLoss[0m : 2.59560
[1mStep[0m  [56/84], [94mLoss[0m : 2.84697
[1mStep[0m  [64/84], [94mLoss[0m : 2.09198
[1mStep[0m  [72/84], [94mLoss[0m : 2.80128
[1mStep[0m  [80/84], [94mLoss[0m : 2.51604

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.555, [92mTest[0m: 2.381, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30351
[1mStep[0m  [8/84], [94mLoss[0m : 2.72349
[1mStep[0m  [16/84], [94mLoss[0m : 2.55085
[1mStep[0m  [24/84], [94mLoss[0m : 2.18019
[1mStep[0m  [32/84], [94mLoss[0m : 2.65196
[1mStep[0m  [40/84], [94mLoss[0m : 2.51523
[1mStep[0m  [48/84], [94mLoss[0m : 2.84790
[1mStep[0m  [56/84], [94mLoss[0m : 2.47784
[1mStep[0m  [64/84], [94mLoss[0m : 2.79783
[1mStep[0m  [72/84], [94mLoss[0m : 2.50555
[1mStep[0m  [80/84], [94mLoss[0m : 2.75742

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.573, [92mTest[0m: 2.413, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46642
[1mStep[0m  [8/84], [94mLoss[0m : 2.93590
[1mStep[0m  [16/84], [94mLoss[0m : 2.67162
[1mStep[0m  [24/84], [94mLoss[0m : 2.33621
[1mStep[0m  [32/84], [94mLoss[0m : 2.49642
[1mStep[0m  [40/84], [94mLoss[0m : 2.66314
[1mStep[0m  [48/84], [94mLoss[0m : 2.43764
[1mStep[0m  [56/84], [94mLoss[0m : 2.37710
[1mStep[0m  [64/84], [94mLoss[0m : 2.29932
[1mStep[0m  [72/84], [94mLoss[0m : 2.60159
[1mStep[0m  [80/84], [94mLoss[0m : 2.96738

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.568, [92mTest[0m: 2.371, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.04966
[1mStep[0m  [8/84], [94mLoss[0m : 2.78006
[1mStep[0m  [16/84], [94mLoss[0m : 2.70977
[1mStep[0m  [24/84], [94mLoss[0m : 2.48039
[1mStep[0m  [32/84], [94mLoss[0m : 2.62644
[1mStep[0m  [40/84], [94mLoss[0m : 2.56785
[1mStep[0m  [48/84], [94mLoss[0m : 2.50118
[1mStep[0m  [56/84], [94mLoss[0m : 2.43975
[1mStep[0m  [64/84], [94mLoss[0m : 2.37133
[1mStep[0m  [72/84], [94mLoss[0m : 2.43726
[1mStep[0m  [80/84], [94mLoss[0m : 2.49999

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.529, [92mTest[0m: 2.339, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27031
[1mStep[0m  [8/84], [94mLoss[0m : 2.70678
[1mStep[0m  [16/84], [94mLoss[0m : 2.33325
[1mStep[0m  [24/84], [94mLoss[0m : 2.71661
[1mStep[0m  [32/84], [94mLoss[0m : 2.80193
[1mStep[0m  [40/84], [94mLoss[0m : 2.62410
[1mStep[0m  [48/84], [94mLoss[0m : 2.58581
[1mStep[0m  [56/84], [94mLoss[0m : 2.14327
[1mStep[0m  [64/84], [94mLoss[0m : 2.41564
[1mStep[0m  [72/84], [94mLoss[0m : 2.48596
[1mStep[0m  [80/84], [94mLoss[0m : 2.51151

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.537, [92mTest[0m: 2.371, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.65645
[1mStep[0m  [8/84], [94mLoss[0m : 2.75179
[1mStep[0m  [16/84], [94mLoss[0m : 2.50238
[1mStep[0m  [24/84], [94mLoss[0m : 2.36230
[1mStep[0m  [32/84], [94mLoss[0m : 2.03472
[1mStep[0m  [40/84], [94mLoss[0m : 2.57004
[1mStep[0m  [48/84], [94mLoss[0m : 2.41977
[1mStep[0m  [56/84], [94mLoss[0m : 2.56870
[1mStep[0m  [64/84], [94mLoss[0m : 2.85338
[1mStep[0m  [72/84], [94mLoss[0m : 2.32897
[1mStep[0m  [80/84], [94mLoss[0m : 2.69431

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.341, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48510
[1mStep[0m  [8/84], [94mLoss[0m : 2.56086
[1mStep[0m  [16/84], [94mLoss[0m : 2.59997
[1mStep[0m  [24/84], [94mLoss[0m : 2.35936
[1mStep[0m  [32/84], [94mLoss[0m : 2.25936
[1mStep[0m  [40/84], [94mLoss[0m : 2.33194
[1mStep[0m  [48/84], [94mLoss[0m : 2.33640
[1mStep[0m  [56/84], [94mLoss[0m : 2.26895
[1mStep[0m  [64/84], [94mLoss[0m : 2.58207
[1mStep[0m  [72/84], [94mLoss[0m : 2.29856
[1mStep[0m  [80/84], [94mLoss[0m : 2.53114

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.361, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37389
[1mStep[0m  [8/84], [94mLoss[0m : 2.61127
[1mStep[0m  [16/84], [94mLoss[0m : 2.87584
[1mStep[0m  [24/84], [94mLoss[0m : 2.41890
[1mStep[0m  [32/84], [94mLoss[0m : 2.37427
[1mStep[0m  [40/84], [94mLoss[0m : 2.82778
[1mStep[0m  [48/84], [94mLoss[0m : 2.09576
[1mStep[0m  [56/84], [94mLoss[0m : 2.46305
[1mStep[0m  [64/84], [94mLoss[0m : 2.55746
[1mStep[0m  [72/84], [94mLoss[0m : 2.62646
[1mStep[0m  [80/84], [94mLoss[0m : 2.71807

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.337, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36360
[1mStep[0m  [8/84], [94mLoss[0m : 2.18316
[1mStep[0m  [16/84], [94mLoss[0m : 2.50957
[1mStep[0m  [24/84], [94mLoss[0m : 2.47477
[1mStep[0m  [32/84], [94mLoss[0m : 2.37862
[1mStep[0m  [40/84], [94mLoss[0m : 2.41895
[1mStep[0m  [48/84], [94mLoss[0m : 2.29643
[1mStep[0m  [56/84], [94mLoss[0m : 2.59339
[1mStep[0m  [64/84], [94mLoss[0m : 2.64870
[1mStep[0m  [72/84], [94mLoss[0m : 2.25158
[1mStep[0m  [80/84], [94mLoss[0m : 2.84738

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.348, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54132
[1mStep[0m  [8/84], [94mLoss[0m : 2.44995
[1mStep[0m  [16/84], [94mLoss[0m : 2.41346
[1mStep[0m  [24/84], [94mLoss[0m : 2.34959
[1mStep[0m  [32/84], [94mLoss[0m : 2.28673
[1mStep[0m  [40/84], [94mLoss[0m : 2.39277
[1mStep[0m  [48/84], [94mLoss[0m : 2.34437
[1mStep[0m  [56/84], [94mLoss[0m : 2.78981
[1mStep[0m  [64/84], [94mLoss[0m : 2.67845
[1mStep[0m  [72/84], [94mLoss[0m : 2.26979
[1mStep[0m  [80/84], [94mLoss[0m : 2.37560

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.358, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.63183
[1mStep[0m  [8/84], [94mLoss[0m : 2.49830
[1mStep[0m  [16/84], [94mLoss[0m : 2.17802
[1mStep[0m  [24/84], [94mLoss[0m : 2.55800
[1mStep[0m  [32/84], [94mLoss[0m : 2.12856
[1mStep[0m  [40/84], [94mLoss[0m : 2.23798
[1mStep[0m  [48/84], [94mLoss[0m : 2.35586
[1mStep[0m  [56/84], [94mLoss[0m : 2.64485
[1mStep[0m  [64/84], [94mLoss[0m : 2.43538
[1mStep[0m  [72/84], [94mLoss[0m : 2.57547
[1mStep[0m  [80/84], [94mLoss[0m : 2.60754

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.336, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.25234
[1mStep[0m  [8/84], [94mLoss[0m : 2.66067
[1mStep[0m  [16/84], [94mLoss[0m : 2.13465
[1mStep[0m  [24/84], [94mLoss[0m : 2.52621
[1mStep[0m  [32/84], [94mLoss[0m : 2.22719
[1mStep[0m  [40/84], [94mLoss[0m : 2.47131
[1mStep[0m  [48/84], [94mLoss[0m : 2.44052
[1mStep[0m  [56/84], [94mLoss[0m : 2.25020
[1mStep[0m  [64/84], [94mLoss[0m : 2.48412
[1mStep[0m  [72/84], [94mLoss[0m : 2.38060
[1mStep[0m  [80/84], [94mLoss[0m : 2.86328

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.345, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.25476
[1mStep[0m  [8/84], [94mLoss[0m : 2.43199
[1mStep[0m  [16/84], [94mLoss[0m : 2.48599
[1mStep[0m  [24/84], [94mLoss[0m : 2.91962
[1mStep[0m  [32/84], [94mLoss[0m : 2.60342
[1mStep[0m  [40/84], [94mLoss[0m : 2.73284
[1mStep[0m  [48/84], [94mLoss[0m : 2.38931
[1mStep[0m  [56/84], [94mLoss[0m : 2.46555
[1mStep[0m  [64/84], [94mLoss[0m : 2.36325
[1mStep[0m  [72/84], [94mLoss[0m : 2.17993
[1mStep[0m  [80/84], [94mLoss[0m : 2.36382

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.356, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45412
[1mStep[0m  [8/84], [94mLoss[0m : 2.50247
[1mStep[0m  [16/84], [94mLoss[0m : 2.38310
[1mStep[0m  [24/84], [94mLoss[0m : 2.37576
[1mStep[0m  [32/84], [94mLoss[0m : 2.32099
[1mStep[0m  [40/84], [94mLoss[0m : 2.91087
[1mStep[0m  [48/84], [94mLoss[0m : 2.46036
[1mStep[0m  [56/84], [94mLoss[0m : 2.50476
[1mStep[0m  [64/84], [94mLoss[0m : 2.42825
[1mStep[0m  [72/84], [94mLoss[0m : 2.36375
[1mStep[0m  [80/84], [94mLoss[0m : 2.60107

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.358, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35120
[1mStep[0m  [8/84], [94mLoss[0m : 2.32796
[1mStep[0m  [16/84], [94mLoss[0m : 2.46962
[1mStep[0m  [24/84], [94mLoss[0m : 2.79859
[1mStep[0m  [32/84], [94mLoss[0m : 2.41283
[1mStep[0m  [40/84], [94mLoss[0m : 2.36994
[1mStep[0m  [48/84], [94mLoss[0m : 2.58406
[1mStep[0m  [56/84], [94mLoss[0m : 2.46368
[1mStep[0m  [64/84], [94mLoss[0m : 2.47483
[1mStep[0m  [72/84], [94mLoss[0m : 2.40624
[1mStep[0m  [80/84], [94mLoss[0m : 2.42803

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.335, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.22619
[1mStep[0m  [8/84], [94mLoss[0m : 2.31201
[1mStep[0m  [16/84], [94mLoss[0m : 2.54705
[1mStep[0m  [24/84], [94mLoss[0m : 2.39013
[1mStep[0m  [32/84], [94mLoss[0m : 2.75480
[1mStep[0m  [40/84], [94mLoss[0m : 2.22558
[1mStep[0m  [48/84], [94mLoss[0m : 2.28404
[1mStep[0m  [56/84], [94mLoss[0m : 2.49368
[1mStep[0m  [64/84], [94mLoss[0m : 2.61689
[1mStep[0m  [72/84], [94mLoss[0m : 2.42432
[1mStep[0m  [80/84], [94mLoss[0m : 2.73887

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.356, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54812
[1mStep[0m  [8/84], [94mLoss[0m : 2.66372
[1mStep[0m  [16/84], [94mLoss[0m : 2.69396
[1mStep[0m  [24/84], [94mLoss[0m : 2.09955
[1mStep[0m  [32/84], [94mLoss[0m : 2.67243
[1mStep[0m  [40/84], [94mLoss[0m : 2.11626
[1mStep[0m  [48/84], [94mLoss[0m : 2.30120
[1mStep[0m  [56/84], [94mLoss[0m : 2.27996
[1mStep[0m  [64/84], [94mLoss[0m : 2.35969
[1mStep[0m  [72/84], [94mLoss[0m : 2.46080
[1mStep[0m  [80/84], [94mLoss[0m : 2.48086

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.334, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.340
====================================

Phase 1 - Evaluation MAE:  2.3403914655957903
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 2.51725
[1mStep[0m  [8/84], [94mLoss[0m : 2.64041
[1mStep[0m  [16/84], [94mLoss[0m : 2.70977
[1mStep[0m  [24/84], [94mLoss[0m : 2.27416
[1mStep[0m  [32/84], [94mLoss[0m : 2.41090
[1mStep[0m  [40/84], [94mLoss[0m : 2.72672
[1mStep[0m  [48/84], [94mLoss[0m : 2.51333
[1mStep[0m  [56/84], [94mLoss[0m : 2.73214
[1mStep[0m  [64/84], [94mLoss[0m : 2.42995
[1mStep[0m  [72/84], [94mLoss[0m : 2.49080
[1mStep[0m  [80/84], [94mLoss[0m : 2.71536

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.336, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27584
[1mStep[0m  [8/84], [94mLoss[0m : 2.47813
[1mStep[0m  [16/84], [94mLoss[0m : 2.27699
[1mStep[0m  [24/84], [94mLoss[0m : 2.37081
[1mStep[0m  [32/84], [94mLoss[0m : 2.33878
[1mStep[0m  [40/84], [94mLoss[0m : 2.17959
[1mStep[0m  [48/84], [94mLoss[0m : 2.44360
[1mStep[0m  [56/84], [94mLoss[0m : 2.36795
[1mStep[0m  [64/84], [94mLoss[0m : 2.82541
[1mStep[0m  [72/84], [94mLoss[0m : 2.44841
[1mStep[0m  [80/84], [94mLoss[0m : 2.83566

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.378, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.08702
[1mStep[0m  [8/84], [94mLoss[0m : 2.26625
[1mStep[0m  [16/84], [94mLoss[0m : 2.70028
[1mStep[0m  [24/84], [94mLoss[0m : 2.11208
[1mStep[0m  [32/84], [94mLoss[0m : 2.52035
[1mStep[0m  [40/84], [94mLoss[0m : 2.54285
[1mStep[0m  [48/84], [94mLoss[0m : 2.50142
[1mStep[0m  [56/84], [94mLoss[0m : 2.37359
[1mStep[0m  [64/84], [94mLoss[0m : 2.46074
[1mStep[0m  [72/84], [94mLoss[0m : 2.37214
[1mStep[0m  [80/84], [94mLoss[0m : 2.40088

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.403, [92mTest[0m: 2.402, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48089
[1mStep[0m  [8/84], [94mLoss[0m : 2.29037
[1mStep[0m  [16/84], [94mLoss[0m : 2.16578
[1mStep[0m  [24/84], [94mLoss[0m : 2.25914
[1mStep[0m  [32/84], [94mLoss[0m : 2.58060
[1mStep[0m  [40/84], [94mLoss[0m : 2.48707
[1mStep[0m  [48/84], [94mLoss[0m : 2.46538
[1mStep[0m  [56/84], [94mLoss[0m : 2.10955
[1mStep[0m  [64/84], [94mLoss[0m : 2.51244
[1mStep[0m  [72/84], [94mLoss[0m : 2.36715
[1mStep[0m  [80/84], [94mLoss[0m : 2.31769

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.369, [92mTest[0m: 2.438, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37915
[1mStep[0m  [8/84], [94mLoss[0m : 2.33444
[1mStep[0m  [16/84], [94mLoss[0m : 2.64924
[1mStep[0m  [24/84], [94mLoss[0m : 2.42411
[1mStep[0m  [32/84], [94mLoss[0m : 2.25742
[1mStep[0m  [40/84], [94mLoss[0m : 2.18871
[1mStep[0m  [48/84], [94mLoss[0m : 2.01475
[1mStep[0m  [56/84], [94mLoss[0m : 2.54385
[1mStep[0m  [64/84], [94mLoss[0m : 2.44150
[1mStep[0m  [72/84], [94mLoss[0m : 2.27699
[1mStep[0m  [80/84], [94mLoss[0m : 2.33171

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.322, [92mTest[0m: 2.411, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.95907
[1mStep[0m  [8/84], [94mLoss[0m : 2.39355
[1mStep[0m  [16/84], [94mLoss[0m : 2.22599
[1mStep[0m  [24/84], [94mLoss[0m : 2.41959
[1mStep[0m  [32/84], [94mLoss[0m : 2.40418
[1mStep[0m  [40/84], [94mLoss[0m : 2.27616
[1mStep[0m  [48/84], [94mLoss[0m : 2.29818
[1mStep[0m  [56/84], [94mLoss[0m : 2.24003
[1mStep[0m  [64/84], [94mLoss[0m : 2.10115
[1mStep[0m  [72/84], [94mLoss[0m : 2.41365
[1mStep[0m  [80/84], [94mLoss[0m : 2.12815

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.277, [92mTest[0m: 2.508, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.86215
[1mStep[0m  [8/84], [94mLoss[0m : 2.23144
[1mStep[0m  [16/84], [94mLoss[0m : 2.10581
[1mStep[0m  [24/84], [94mLoss[0m : 2.09190
[1mStep[0m  [32/84], [94mLoss[0m : 2.29026
[1mStep[0m  [40/84], [94mLoss[0m : 2.14640
[1mStep[0m  [48/84], [94mLoss[0m : 2.23610
[1mStep[0m  [56/84], [94mLoss[0m : 2.32783
[1mStep[0m  [64/84], [94mLoss[0m : 2.24000
[1mStep[0m  [72/84], [94mLoss[0m : 2.02307
[1mStep[0m  [80/84], [94mLoss[0m : 2.00684

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.200, [92mTest[0m: 2.428, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.18271
[1mStep[0m  [8/84], [94mLoss[0m : 2.11493
[1mStep[0m  [16/84], [94mLoss[0m : 1.94608
[1mStep[0m  [24/84], [94mLoss[0m : 2.17993
[1mStep[0m  [32/84], [94mLoss[0m : 1.92724
[1mStep[0m  [40/84], [94mLoss[0m : 1.93980
[1mStep[0m  [48/84], [94mLoss[0m : 2.09026
[1mStep[0m  [56/84], [94mLoss[0m : 2.29454
[1mStep[0m  [64/84], [94mLoss[0m : 2.23924
[1mStep[0m  [72/84], [94mLoss[0m : 2.23915
[1mStep[0m  [80/84], [94mLoss[0m : 2.28682

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.171, [92mTest[0m: 2.446, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.12678
[1mStep[0m  [8/84], [94mLoss[0m : 1.93411
[1mStep[0m  [16/84], [94mLoss[0m : 1.96071
[1mStep[0m  [24/84], [94mLoss[0m : 2.00825
[1mStep[0m  [32/84], [94mLoss[0m : 2.02089
[1mStep[0m  [40/84], [94mLoss[0m : 2.13382
[1mStep[0m  [48/84], [94mLoss[0m : 2.06675
[1mStep[0m  [56/84], [94mLoss[0m : 2.20335
[1mStep[0m  [64/84], [94mLoss[0m : 2.06681
[1mStep[0m  [72/84], [94mLoss[0m : 2.17279
[1mStep[0m  [80/84], [94mLoss[0m : 1.89259

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.104, [92mTest[0m: 2.539, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.16669
[1mStep[0m  [8/84], [94mLoss[0m : 1.67792
[1mStep[0m  [16/84], [94mLoss[0m : 1.87967
[1mStep[0m  [24/84], [94mLoss[0m : 1.74011
[1mStep[0m  [32/84], [94mLoss[0m : 2.15901
[1mStep[0m  [40/84], [94mLoss[0m : 2.09645
[1mStep[0m  [48/84], [94mLoss[0m : 2.67602
[1mStep[0m  [56/84], [94mLoss[0m : 2.06592
[1mStep[0m  [64/84], [94mLoss[0m : 1.94136
[1mStep[0m  [72/84], [94mLoss[0m : 1.87052
[1mStep[0m  [80/84], [94mLoss[0m : 2.15416

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.069, [92mTest[0m: 2.433, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.62218
[1mStep[0m  [8/84], [94mLoss[0m : 1.73273
[1mStep[0m  [16/84], [94mLoss[0m : 2.04261
[1mStep[0m  [24/84], [94mLoss[0m : 2.12262
[1mStep[0m  [32/84], [94mLoss[0m : 1.87311
[1mStep[0m  [40/84], [94mLoss[0m : 1.95708
[1mStep[0m  [48/84], [94mLoss[0m : 1.85975
[1mStep[0m  [56/84], [94mLoss[0m : 2.09811
[1mStep[0m  [64/84], [94mLoss[0m : 1.76438
[1mStep[0m  [72/84], [94mLoss[0m : 2.00019
[1mStep[0m  [80/84], [94mLoss[0m : 2.14699

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.021, [92mTest[0m: 2.483, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.98825
[1mStep[0m  [8/84], [94mLoss[0m : 2.02604
[1mStep[0m  [16/84], [94mLoss[0m : 1.54363
[1mStep[0m  [24/84], [94mLoss[0m : 2.11704
[1mStep[0m  [32/84], [94mLoss[0m : 1.80549
[1mStep[0m  [40/84], [94mLoss[0m : 2.03842
[1mStep[0m  [48/84], [94mLoss[0m : 2.07654
[1mStep[0m  [56/84], [94mLoss[0m : 2.29279
[1mStep[0m  [64/84], [94mLoss[0m : 2.02069
[1mStep[0m  [72/84], [94mLoss[0m : 2.28278
[1mStep[0m  [80/84], [94mLoss[0m : 1.91750

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.999, [92mTest[0m: 2.533, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.97852
[1mStep[0m  [8/84], [94mLoss[0m : 1.89153
[1mStep[0m  [16/84], [94mLoss[0m : 2.09790
[1mStep[0m  [24/84], [94mLoss[0m : 2.11241
[1mStep[0m  [32/84], [94mLoss[0m : 2.16984
[1mStep[0m  [40/84], [94mLoss[0m : 2.00585
[1mStep[0m  [48/84], [94mLoss[0m : 2.23923
[1mStep[0m  [56/84], [94mLoss[0m : 1.87195
[1mStep[0m  [64/84], [94mLoss[0m : 2.02400
[1mStep[0m  [72/84], [94mLoss[0m : 1.99792
[1mStep[0m  [80/84], [94mLoss[0m : 1.71085

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.948, [92mTest[0m: 2.436, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.99759
[1mStep[0m  [8/84], [94mLoss[0m : 1.80170
[1mStep[0m  [16/84], [94mLoss[0m : 1.91032
[1mStep[0m  [24/84], [94mLoss[0m : 1.56359
[1mStep[0m  [32/84], [94mLoss[0m : 1.72226
[1mStep[0m  [40/84], [94mLoss[0m : 1.91783
[1mStep[0m  [48/84], [94mLoss[0m : 1.99301
[1mStep[0m  [56/84], [94mLoss[0m : 1.71625
[1mStep[0m  [64/84], [94mLoss[0m : 1.94258
[1mStep[0m  [72/84], [94mLoss[0m : 2.05897
[1mStep[0m  [80/84], [94mLoss[0m : 1.77552

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.916, [92mTest[0m: 2.591, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.84768
[1mStep[0m  [8/84], [94mLoss[0m : 1.91304
[1mStep[0m  [16/84], [94mLoss[0m : 1.47142
[1mStep[0m  [24/84], [94mLoss[0m : 1.86341
[1mStep[0m  [32/84], [94mLoss[0m : 1.92112
[1mStep[0m  [40/84], [94mLoss[0m : 1.95671
[1mStep[0m  [48/84], [94mLoss[0m : 1.62985
[1mStep[0m  [56/84], [94mLoss[0m : 1.91085
[1mStep[0m  [64/84], [94mLoss[0m : 1.82498
[1mStep[0m  [72/84], [94mLoss[0m : 1.99694
[1mStep[0m  [80/84], [94mLoss[0m : 1.97030

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.856, [92mTest[0m: 2.447, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.67682
[1mStep[0m  [8/84], [94mLoss[0m : 1.96635
[1mStep[0m  [16/84], [94mLoss[0m : 1.70489
[1mStep[0m  [24/84], [94mLoss[0m : 1.88118
[1mStep[0m  [32/84], [94mLoss[0m : 1.95524
[1mStep[0m  [40/84], [94mLoss[0m : 1.91485
[1mStep[0m  [48/84], [94mLoss[0m : 1.75169
[1mStep[0m  [56/84], [94mLoss[0m : 1.82111
[1mStep[0m  [64/84], [94mLoss[0m : 2.03733
[1mStep[0m  [72/84], [94mLoss[0m : 2.13489
[1mStep[0m  [80/84], [94mLoss[0m : 1.73778

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.840, [92mTest[0m: 2.429, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.53904
[1mStep[0m  [8/84], [94mLoss[0m : 1.63370
[1mStep[0m  [16/84], [94mLoss[0m : 1.63562
[1mStep[0m  [24/84], [94mLoss[0m : 2.00347
[1mStep[0m  [32/84], [94mLoss[0m : 1.84493
[1mStep[0m  [40/84], [94mLoss[0m : 1.57077
[1mStep[0m  [48/84], [94mLoss[0m : 1.81277
[1mStep[0m  [56/84], [94mLoss[0m : 1.55178
[1mStep[0m  [64/84], [94mLoss[0m : 1.86603
[1mStep[0m  [72/84], [94mLoss[0m : 2.00007
[1mStep[0m  [80/84], [94mLoss[0m : 1.73764

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.809, [92mTest[0m: 2.424, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.93319
[1mStep[0m  [8/84], [94mLoss[0m : 1.67236
[1mStep[0m  [16/84], [94mLoss[0m : 2.21827
[1mStep[0m  [24/84], [94mLoss[0m : 1.84690
[1mStep[0m  [32/84], [94mLoss[0m : 1.55358
[1mStep[0m  [40/84], [94mLoss[0m : 1.68827
[1mStep[0m  [48/84], [94mLoss[0m : 1.76996
[1mStep[0m  [56/84], [94mLoss[0m : 1.71381
[1mStep[0m  [64/84], [94mLoss[0m : 1.49849
[1mStep[0m  [72/84], [94mLoss[0m : 1.76638
[1mStep[0m  [80/84], [94mLoss[0m : 1.77514

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.768, [92mTest[0m: 2.503, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.77410
[1mStep[0m  [8/84], [94mLoss[0m : 1.68489
[1mStep[0m  [16/84], [94mLoss[0m : 1.76037
[1mStep[0m  [24/84], [94mLoss[0m : 1.63893
[1mStep[0m  [32/84], [94mLoss[0m : 1.97397
[1mStep[0m  [40/84], [94mLoss[0m : 1.58251
[1mStep[0m  [48/84], [94mLoss[0m : 1.60693
[1mStep[0m  [56/84], [94mLoss[0m : 1.67009
[1mStep[0m  [64/84], [94mLoss[0m : 1.67455
[1mStep[0m  [72/84], [94mLoss[0m : 1.54369
[1mStep[0m  [80/84], [94mLoss[0m : 1.87912

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.733, [92mTest[0m: 2.489, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.57589
[1mStep[0m  [8/84], [94mLoss[0m : 1.56385
[1mStep[0m  [16/84], [94mLoss[0m : 1.78369
[1mStep[0m  [24/84], [94mLoss[0m : 1.53393
[1mStep[0m  [32/84], [94mLoss[0m : 1.81072
[1mStep[0m  [40/84], [94mLoss[0m : 1.75544
[1mStep[0m  [48/84], [94mLoss[0m : 1.78518
[1mStep[0m  [56/84], [94mLoss[0m : 1.73295
[1mStep[0m  [64/84], [94mLoss[0m : 1.53960
[1mStep[0m  [72/84], [94mLoss[0m : 1.92311
[1mStep[0m  [80/84], [94mLoss[0m : 1.59234

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.724, [92mTest[0m: 2.424, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.77011
[1mStep[0m  [8/84], [94mLoss[0m : 1.50410
[1mStep[0m  [16/84], [94mLoss[0m : 1.70174
[1mStep[0m  [24/84], [94mLoss[0m : 1.55905
[1mStep[0m  [32/84], [94mLoss[0m : 1.70436
[1mStep[0m  [40/84], [94mLoss[0m : 1.60517
[1mStep[0m  [48/84], [94mLoss[0m : 1.56564
[1mStep[0m  [56/84], [94mLoss[0m : 1.72836
[1mStep[0m  [64/84], [94mLoss[0m : 1.28940
[1mStep[0m  [72/84], [94mLoss[0m : 1.62245
[1mStep[0m  [80/84], [94mLoss[0m : 1.89342

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.663, [92mTest[0m: 2.512, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.70193
[1mStep[0m  [8/84], [94mLoss[0m : 1.46949
[1mStep[0m  [16/84], [94mLoss[0m : 1.81969
[1mStep[0m  [24/84], [94mLoss[0m : 1.76749
[1mStep[0m  [32/84], [94mLoss[0m : 1.68670
[1mStep[0m  [40/84], [94mLoss[0m : 1.62144
[1mStep[0m  [48/84], [94mLoss[0m : 1.77981
[1mStep[0m  [56/84], [94mLoss[0m : 1.89065
[1mStep[0m  [64/84], [94mLoss[0m : 1.47651
[1mStep[0m  [72/84], [94mLoss[0m : 1.55536
[1mStep[0m  [80/84], [94mLoss[0m : 1.43578

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.632, [92mTest[0m: 2.462, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.47681
[1mStep[0m  [8/84], [94mLoss[0m : 1.52567
[1mStep[0m  [16/84], [94mLoss[0m : 1.43783
[1mStep[0m  [24/84], [94mLoss[0m : 1.48788
[1mStep[0m  [32/84], [94mLoss[0m : 1.46621
[1mStep[0m  [40/84], [94mLoss[0m : 1.49700
[1mStep[0m  [48/84], [94mLoss[0m : 1.85555
[1mStep[0m  [56/84], [94mLoss[0m : 1.74763
[1mStep[0m  [64/84], [94mLoss[0m : 1.56299
[1mStep[0m  [72/84], [94mLoss[0m : 1.77601
[1mStep[0m  [80/84], [94mLoss[0m : 1.56458

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.606, [92mTest[0m: 2.448, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.45539
[1mStep[0m  [8/84], [94mLoss[0m : 1.56845
[1mStep[0m  [16/84], [94mLoss[0m : 1.65582
[1mStep[0m  [24/84], [94mLoss[0m : 1.69433
[1mStep[0m  [32/84], [94mLoss[0m : 1.49552
[1mStep[0m  [40/84], [94mLoss[0m : 1.67738
[1mStep[0m  [48/84], [94mLoss[0m : 1.75676
[1mStep[0m  [56/84], [94mLoss[0m : 1.40764
[1mStep[0m  [64/84], [94mLoss[0m : 1.36638
[1mStep[0m  [72/84], [94mLoss[0m : 1.71753
[1mStep[0m  [80/84], [94mLoss[0m : 1.69207

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.594, [92mTest[0m: 2.470, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.56081
[1mStep[0m  [8/84], [94mLoss[0m : 1.57549
[1mStep[0m  [16/84], [94mLoss[0m : 1.55894
[1mStep[0m  [24/84], [94mLoss[0m : 1.60224
[1mStep[0m  [32/84], [94mLoss[0m : 1.57914
[1mStep[0m  [40/84], [94mLoss[0m : 1.50053
[1mStep[0m  [48/84], [94mLoss[0m : 1.65268
[1mStep[0m  [56/84], [94mLoss[0m : 1.73306
[1mStep[0m  [64/84], [94mLoss[0m : 1.70329
[1mStep[0m  [72/84], [94mLoss[0m : 1.51250
[1mStep[0m  [80/84], [94mLoss[0m : 1.65392

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.567, [92mTest[0m: 2.540, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.44203
[1mStep[0m  [8/84], [94mLoss[0m : 1.47403
[1mStep[0m  [16/84], [94mLoss[0m : 1.55185
[1mStep[0m  [24/84], [94mLoss[0m : 1.42494
[1mStep[0m  [32/84], [94mLoss[0m : 1.72350
[1mStep[0m  [40/84], [94mLoss[0m : 1.46143
[1mStep[0m  [48/84], [94mLoss[0m : 1.63613
[1mStep[0m  [56/84], [94mLoss[0m : 1.43760
[1mStep[0m  [64/84], [94mLoss[0m : 1.74430
[1mStep[0m  [72/84], [94mLoss[0m : 1.63898
[1mStep[0m  [80/84], [94mLoss[0m : 1.70516

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.551, [92mTest[0m: 2.523, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.45417
[1mStep[0m  [8/84], [94mLoss[0m : 1.54501
[1mStep[0m  [16/84], [94mLoss[0m : 1.53943
[1mStep[0m  [24/84], [94mLoss[0m : 1.61337
[1mStep[0m  [32/84], [94mLoss[0m : 1.46807
[1mStep[0m  [40/84], [94mLoss[0m : 1.78524
[1mStep[0m  [48/84], [94mLoss[0m : 1.72688
[1mStep[0m  [56/84], [94mLoss[0m : 1.35594
[1mStep[0m  [64/84], [94mLoss[0m : 1.51127
[1mStep[0m  [72/84], [94mLoss[0m : 1.64272
[1mStep[0m  [80/84], [94mLoss[0m : 1.71694

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.536, [92mTest[0m: 2.588, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.44818
[1mStep[0m  [8/84], [94mLoss[0m : 1.51128
[1mStep[0m  [16/84], [94mLoss[0m : 1.66883
[1mStep[0m  [24/84], [94mLoss[0m : 1.57448
[1mStep[0m  [32/84], [94mLoss[0m : 1.36053
[1mStep[0m  [40/84], [94mLoss[0m : 1.47121
[1mStep[0m  [48/84], [94mLoss[0m : 1.51818
[1mStep[0m  [56/84], [94mLoss[0m : 1.53419
[1mStep[0m  [64/84], [94mLoss[0m : 1.39700
[1mStep[0m  [72/84], [94mLoss[0m : 1.46064
[1mStep[0m  [80/84], [94mLoss[0m : 1.42143

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.498, [92mTest[0m: 2.464, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.28998
[1mStep[0m  [8/84], [94mLoss[0m : 1.45584
[1mStep[0m  [16/84], [94mLoss[0m : 1.47243
[1mStep[0m  [24/84], [94mLoss[0m : 1.38205
[1mStep[0m  [32/84], [94mLoss[0m : 1.49172
[1mStep[0m  [40/84], [94mLoss[0m : 1.30979
[1mStep[0m  [48/84], [94mLoss[0m : 1.65620
[1mStep[0m  [56/84], [94mLoss[0m : 1.49160
[1mStep[0m  [64/84], [94mLoss[0m : 1.47572
[1mStep[0m  [72/84], [94mLoss[0m : 1.62443
[1mStep[0m  [80/84], [94mLoss[0m : 1.54243

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.495, [92mTest[0m: 2.522, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.47500
[1mStep[0m  [8/84], [94mLoss[0m : 1.52833
[1mStep[0m  [16/84], [94mLoss[0m : 1.51409
[1mStep[0m  [24/84], [94mLoss[0m : 1.67513
[1mStep[0m  [32/84], [94mLoss[0m : 1.52450
[1mStep[0m  [40/84], [94mLoss[0m : 1.41880
[1mStep[0m  [48/84], [94mLoss[0m : 1.58501
[1mStep[0m  [56/84], [94mLoss[0m : 1.35632
[1mStep[0m  [64/84], [94mLoss[0m : 1.60053
[1mStep[0m  [72/84], [94mLoss[0m : 1.51489
[1mStep[0m  [80/84], [94mLoss[0m : 1.48784

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.472, [92mTest[0m: 2.464, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.462
====================================

Phase 2 - Evaluation MAE:  2.461949586868286
MAE score P1      2.340391
MAE score P2       2.46195
loss              1.471654
learning_rate     0.007525
batch_size             128
hidden_sizes         [250]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.1
weight_decay          0.01
Name: 10, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 11.48011
[1mStep[0m  [8/84], [94mLoss[0m : 8.07007
[1mStep[0m  [16/84], [94mLoss[0m : 3.43286
[1mStep[0m  [24/84], [94mLoss[0m : 2.82727
[1mStep[0m  [32/84], [94mLoss[0m : 3.11337
[1mStep[0m  [40/84], [94mLoss[0m : 2.77510
[1mStep[0m  [48/84], [94mLoss[0m : 3.04558
[1mStep[0m  [56/84], [94mLoss[0m : 2.62621
[1mStep[0m  [64/84], [94mLoss[0m : 2.66627
[1mStep[0m  [72/84], [94mLoss[0m : 2.80312
[1mStep[0m  [80/84], [94mLoss[0m : 2.65027

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.842, [92mTest[0m: 11.043, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.71605
[1mStep[0m  [8/84], [94mLoss[0m : 2.36988
[1mStep[0m  [16/84], [94mLoss[0m : 2.53945
[1mStep[0m  [24/84], [94mLoss[0m : 2.49073
[1mStep[0m  [32/84], [94mLoss[0m : 2.61833
[1mStep[0m  [40/84], [94mLoss[0m : 2.77295
[1mStep[0m  [48/84], [94mLoss[0m : 2.77201
[1mStep[0m  [56/84], [94mLoss[0m : 2.73897
[1mStep[0m  [64/84], [94mLoss[0m : 2.64316
[1mStep[0m  [72/84], [94mLoss[0m : 2.45902
[1mStep[0m  [80/84], [94mLoss[0m : 2.75791

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.635, [92mTest[0m: 2.379, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.82153
[1mStep[0m  [8/84], [94mLoss[0m : 2.29317
[1mStep[0m  [16/84], [94mLoss[0m : 2.52626
[1mStep[0m  [24/84], [94mLoss[0m : 2.87494
[1mStep[0m  [32/84], [94mLoss[0m : 2.95018
[1mStep[0m  [40/84], [94mLoss[0m : 2.82207
[1mStep[0m  [48/84], [94mLoss[0m : 2.50263
[1mStep[0m  [56/84], [94mLoss[0m : 2.71304
[1mStep[0m  [64/84], [94mLoss[0m : 2.67407
[1mStep[0m  [72/84], [94mLoss[0m : 2.40075
[1mStep[0m  [80/84], [94mLoss[0m : 2.77367

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.630, [92mTest[0m: 2.356, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43274
[1mStep[0m  [8/84], [94mLoss[0m : 2.78365
[1mStep[0m  [16/84], [94mLoss[0m : 2.65212
[1mStep[0m  [24/84], [94mLoss[0m : 3.20168
[1mStep[0m  [32/84], [94mLoss[0m : 2.43319
[1mStep[0m  [40/84], [94mLoss[0m : 2.53114
[1mStep[0m  [48/84], [94mLoss[0m : 2.48308
[1mStep[0m  [56/84], [94mLoss[0m : 2.77772
[1mStep[0m  [64/84], [94mLoss[0m : 2.70883
[1mStep[0m  [72/84], [94mLoss[0m : 2.81457
[1mStep[0m  [80/84], [94mLoss[0m : 2.35981

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.615, [92mTest[0m: 2.348, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.77276
[1mStep[0m  [8/84], [94mLoss[0m : 2.34818
[1mStep[0m  [16/84], [94mLoss[0m : 2.66523
[1mStep[0m  [24/84], [94mLoss[0m : 2.58340
[1mStep[0m  [32/84], [94mLoss[0m : 2.67100
[1mStep[0m  [40/84], [94mLoss[0m : 2.48231
[1mStep[0m  [48/84], [94mLoss[0m : 2.63370
[1mStep[0m  [56/84], [94mLoss[0m : 2.38032
[1mStep[0m  [64/84], [94mLoss[0m : 2.66437
[1mStep[0m  [72/84], [94mLoss[0m : 2.63646
[1mStep[0m  [80/84], [94mLoss[0m : 2.36474

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.583, [92mTest[0m: 2.350, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41676
[1mStep[0m  [8/84], [94mLoss[0m : 2.57034
[1mStep[0m  [16/84], [94mLoss[0m : 2.25819
[1mStep[0m  [24/84], [94mLoss[0m : 2.29748
[1mStep[0m  [32/84], [94mLoss[0m : 2.46214
[1mStep[0m  [40/84], [94mLoss[0m : 2.75435
[1mStep[0m  [48/84], [94mLoss[0m : 2.67452
[1mStep[0m  [56/84], [94mLoss[0m : 2.53574
[1mStep[0m  [64/84], [94mLoss[0m : 2.51275
[1mStep[0m  [72/84], [94mLoss[0m : 2.47534
[1mStep[0m  [80/84], [94mLoss[0m : 2.67231

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.552, [92mTest[0m: 2.336, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.70099
[1mStep[0m  [8/84], [94mLoss[0m : 2.20734
[1mStep[0m  [16/84], [94mLoss[0m : 2.30052
[1mStep[0m  [24/84], [94mLoss[0m : 2.76214
[1mStep[0m  [32/84], [94mLoss[0m : 2.54741
[1mStep[0m  [40/84], [94mLoss[0m : 2.53295
[1mStep[0m  [48/84], [94mLoss[0m : 2.70134
[1mStep[0m  [56/84], [94mLoss[0m : 2.79818
[1mStep[0m  [64/84], [94mLoss[0m : 2.54369
[1mStep[0m  [72/84], [94mLoss[0m : 2.76968
[1mStep[0m  [80/84], [94mLoss[0m : 2.28134

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.562, [92mTest[0m: 2.340, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.58103
[1mStep[0m  [8/84], [94mLoss[0m : 2.75652
[1mStep[0m  [16/84], [94mLoss[0m : 2.63935
[1mStep[0m  [24/84], [94mLoss[0m : 2.45267
[1mStep[0m  [32/84], [94mLoss[0m : 2.43461
[1mStep[0m  [40/84], [94mLoss[0m : 2.69239
[1mStep[0m  [48/84], [94mLoss[0m : 2.74127
[1mStep[0m  [56/84], [94mLoss[0m : 2.59117
[1mStep[0m  [64/84], [94mLoss[0m : 2.35055
[1mStep[0m  [72/84], [94mLoss[0m : 2.44907
[1mStep[0m  [80/84], [94mLoss[0m : 2.31676

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.554, [92mTest[0m: 2.337, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.82920
[1mStep[0m  [8/84], [94mLoss[0m : 2.48510
[1mStep[0m  [16/84], [94mLoss[0m : 2.55893
[1mStep[0m  [24/84], [94mLoss[0m : 2.88272
[1mStep[0m  [32/84], [94mLoss[0m : 2.80725
[1mStep[0m  [40/84], [94mLoss[0m : 2.45713
[1mStep[0m  [48/84], [94mLoss[0m : 2.32274
[1mStep[0m  [56/84], [94mLoss[0m : 2.34425
[1mStep[0m  [64/84], [94mLoss[0m : 2.73931
[1mStep[0m  [72/84], [94mLoss[0m : 2.54500
[1mStep[0m  [80/84], [94mLoss[0m : 2.31341

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.549, [92mTest[0m: 2.343, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50315
[1mStep[0m  [8/84], [94mLoss[0m : 2.76914
[1mStep[0m  [16/84], [94mLoss[0m : 2.50242
[1mStep[0m  [24/84], [94mLoss[0m : 2.25260
[1mStep[0m  [32/84], [94mLoss[0m : 2.34582
[1mStep[0m  [40/84], [94mLoss[0m : 2.54504
[1mStep[0m  [48/84], [94mLoss[0m : 2.73186
[1mStep[0m  [56/84], [94mLoss[0m : 2.59780
[1mStep[0m  [64/84], [94mLoss[0m : 2.74826
[1mStep[0m  [72/84], [94mLoss[0m : 2.56721
[1mStep[0m  [80/84], [94mLoss[0m : 2.63135

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.345, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.24231
[1mStep[0m  [8/84], [94mLoss[0m : 2.55258
[1mStep[0m  [16/84], [94mLoss[0m : 2.50166
[1mStep[0m  [24/84], [94mLoss[0m : 2.64353
[1mStep[0m  [32/84], [94mLoss[0m : 2.52050
[1mStep[0m  [40/84], [94mLoss[0m : 2.34038
[1mStep[0m  [48/84], [94mLoss[0m : 2.82092
[1mStep[0m  [56/84], [94mLoss[0m : 2.53891
[1mStep[0m  [64/84], [94mLoss[0m : 2.52184
[1mStep[0m  [72/84], [94mLoss[0m : 2.62805
[1mStep[0m  [80/84], [94mLoss[0m : 2.57393

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.530, [92mTest[0m: 2.340, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.79696
[1mStep[0m  [8/84], [94mLoss[0m : 2.60661
[1mStep[0m  [16/84], [94mLoss[0m : 2.50409
[1mStep[0m  [24/84], [94mLoss[0m : 2.63888
[1mStep[0m  [32/84], [94mLoss[0m : 2.71103
[1mStep[0m  [40/84], [94mLoss[0m : 2.29918
[1mStep[0m  [48/84], [94mLoss[0m : 2.40861
[1mStep[0m  [56/84], [94mLoss[0m : 2.55156
[1mStep[0m  [64/84], [94mLoss[0m : 2.23456
[1mStep[0m  [72/84], [94mLoss[0m : 2.62152
[1mStep[0m  [80/84], [94mLoss[0m : 2.46529

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.337, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50219
[1mStep[0m  [8/84], [94mLoss[0m : 2.47607
[1mStep[0m  [16/84], [94mLoss[0m : 2.39211
[1mStep[0m  [24/84], [94mLoss[0m : 2.54336
[1mStep[0m  [32/84], [94mLoss[0m : 2.54542
[1mStep[0m  [40/84], [94mLoss[0m : 2.49608
[1mStep[0m  [48/84], [94mLoss[0m : 2.59774
[1mStep[0m  [56/84], [94mLoss[0m : 2.37861
[1mStep[0m  [64/84], [94mLoss[0m : 2.65777
[1mStep[0m  [72/84], [94mLoss[0m : 2.39069
[1mStep[0m  [80/84], [94mLoss[0m : 2.73837

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.514, [92mTest[0m: 2.329, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.61068
[1mStep[0m  [8/84], [94mLoss[0m : 2.71337
[1mStep[0m  [16/84], [94mLoss[0m : 2.18600
[1mStep[0m  [24/84], [94mLoss[0m : 2.64334
[1mStep[0m  [32/84], [94mLoss[0m : 2.40494
[1mStep[0m  [40/84], [94mLoss[0m : 2.58305
[1mStep[0m  [48/84], [94mLoss[0m : 2.35225
[1mStep[0m  [56/84], [94mLoss[0m : 2.68005
[1mStep[0m  [64/84], [94mLoss[0m : 2.41227
[1mStep[0m  [72/84], [94mLoss[0m : 2.74767
[1mStep[0m  [80/84], [94mLoss[0m : 2.45018

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.517, [92mTest[0m: 2.347, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.68151
[1mStep[0m  [8/84], [94mLoss[0m : 2.75327
[1mStep[0m  [16/84], [94mLoss[0m : 2.71791
[1mStep[0m  [24/84], [94mLoss[0m : 2.32784
[1mStep[0m  [32/84], [94mLoss[0m : 2.80068
[1mStep[0m  [40/84], [94mLoss[0m : 2.40497
[1mStep[0m  [48/84], [94mLoss[0m : 2.39785
[1mStep[0m  [56/84], [94mLoss[0m : 2.75432
[1mStep[0m  [64/84], [94mLoss[0m : 2.51835
[1mStep[0m  [72/84], [94mLoss[0m : 2.63755
[1mStep[0m  [80/84], [94mLoss[0m : 2.24705

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.343, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.17201
[1mStep[0m  [8/84], [94mLoss[0m : 2.48534
[1mStep[0m  [16/84], [94mLoss[0m : 2.34020
[1mStep[0m  [24/84], [94mLoss[0m : 2.30003
[1mStep[0m  [32/84], [94mLoss[0m : 2.61662
[1mStep[0m  [40/84], [94mLoss[0m : 2.81706
[1mStep[0m  [48/84], [94mLoss[0m : 2.31034
[1mStep[0m  [56/84], [94mLoss[0m : 2.59812
[1mStep[0m  [64/84], [94mLoss[0m : 2.41326
[1mStep[0m  [72/84], [94mLoss[0m : 2.58642
[1mStep[0m  [80/84], [94mLoss[0m : 2.84478

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.332, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.39029
[1mStep[0m  [8/84], [94mLoss[0m : 2.42194
[1mStep[0m  [16/84], [94mLoss[0m : 2.45656
[1mStep[0m  [24/84], [94mLoss[0m : 2.54863
[1mStep[0m  [32/84], [94mLoss[0m : 2.55106
[1mStep[0m  [40/84], [94mLoss[0m : 2.45759
[1mStep[0m  [48/84], [94mLoss[0m : 2.33785
[1mStep[0m  [56/84], [94mLoss[0m : 2.69966
[1mStep[0m  [64/84], [94mLoss[0m : 2.13636
[1mStep[0m  [72/84], [94mLoss[0m : 2.52283
[1mStep[0m  [80/84], [94mLoss[0m : 2.54940

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.326, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43046
[1mStep[0m  [8/84], [94mLoss[0m : 2.36880
[1mStep[0m  [16/84], [94mLoss[0m : 2.61245
[1mStep[0m  [24/84], [94mLoss[0m : 2.32999
[1mStep[0m  [32/84], [94mLoss[0m : 2.35018
[1mStep[0m  [40/84], [94mLoss[0m : 2.42519
[1mStep[0m  [48/84], [94mLoss[0m : 2.67925
[1mStep[0m  [56/84], [94mLoss[0m : 2.61789
[1mStep[0m  [64/84], [94mLoss[0m : 2.96307
[1mStep[0m  [72/84], [94mLoss[0m : 2.40748
[1mStep[0m  [80/84], [94mLoss[0m : 2.74790

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.328, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60708
[1mStep[0m  [8/84], [94mLoss[0m : 2.73928
[1mStep[0m  [16/84], [94mLoss[0m : 2.42035
[1mStep[0m  [24/84], [94mLoss[0m : 2.37362
[1mStep[0m  [32/84], [94mLoss[0m : 2.29187
[1mStep[0m  [40/84], [94mLoss[0m : 2.49852
[1mStep[0m  [48/84], [94mLoss[0m : 2.27996
[1mStep[0m  [56/84], [94mLoss[0m : 2.48487
[1mStep[0m  [64/84], [94mLoss[0m : 2.41390
[1mStep[0m  [72/84], [94mLoss[0m : 2.71999
[1mStep[0m  [80/84], [94mLoss[0m : 2.45859

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.333, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.66579
[1mStep[0m  [8/84], [94mLoss[0m : 2.25128
[1mStep[0m  [16/84], [94mLoss[0m : 2.60933
[1mStep[0m  [24/84], [94mLoss[0m : 2.38631
[1mStep[0m  [32/84], [94mLoss[0m : 2.55134
[1mStep[0m  [40/84], [94mLoss[0m : 2.44075
[1mStep[0m  [48/84], [94mLoss[0m : 2.23466
[1mStep[0m  [56/84], [94mLoss[0m : 2.58757
[1mStep[0m  [64/84], [94mLoss[0m : 2.45750
[1mStep[0m  [72/84], [94mLoss[0m : 2.47358
[1mStep[0m  [80/84], [94mLoss[0m : 2.14423

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.344, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.77655
[1mStep[0m  [8/84], [94mLoss[0m : 2.58251
[1mStep[0m  [16/84], [94mLoss[0m : 2.58285
[1mStep[0m  [24/84], [94mLoss[0m : 2.39700
[1mStep[0m  [32/84], [94mLoss[0m : 2.50483
[1mStep[0m  [40/84], [94mLoss[0m : 2.20216
[1mStep[0m  [48/84], [94mLoss[0m : 2.83298
[1mStep[0m  [56/84], [94mLoss[0m : 2.53627
[1mStep[0m  [64/84], [94mLoss[0m : 2.64695
[1mStep[0m  [72/84], [94mLoss[0m : 2.70899
[1mStep[0m  [80/84], [94mLoss[0m : 2.72819

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.334, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.68854
[1mStep[0m  [8/84], [94mLoss[0m : 2.40200
[1mStep[0m  [16/84], [94mLoss[0m : 2.51310
[1mStep[0m  [24/84], [94mLoss[0m : 2.39180
[1mStep[0m  [32/84], [94mLoss[0m : 2.37911
[1mStep[0m  [40/84], [94mLoss[0m : 2.66994
[1mStep[0m  [48/84], [94mLoss[0m : 2.39210
[1mStep[0m  [56/84], [94mLoss[0m : 2.59426
[1mStep[0m  [64/84], [94mLoss[0m : 2.51003
[1mStep[0m  [72/84], [94mLoss[0m : 2.57333
[1mStep[0m  [80/84], [94mLoss[0m : 2.41044

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.328, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.06958
[1mStep[0m  [8/84], [94mLoss[0m : 2.54024
[1mStep[0m  [16/84], [94mLoss[0m : 2.40200
[1mStep[0m  [24/84], [94mLoss[0m : 2.60921
[1mStep[0m  [32/84], [94mLoss[0m : 2.48755
[1mStep[0m  [40/84], [94mLoss[0m : 2.63793
[1mStep[0m  [48/84], [94mLoss[0m : 2.23645
[1mStep[0m  [56/84], [94mLoss[0m : 2.37094
[1mStep[0m  [64/84], [94mLoss[0m : 2.30141
[1mStep[0m  [72/84], [94mLoss[0m : 2.72692
[1mStep[0m  [80/84], [94mLoss[0m : 2.39348

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.323, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50758
[1mStep[0m  [8/84], [94mLoss[0m : 2.56932
[1mStep[0m  [16/84], [94mLoss[0m : 2.46786
[1mStep[0m  [24/84], [94mLoss[0m : 2.65767
[1mStep[0m  [32/84], [94mLoss[0m : 2.61909
[1mStep[0m  [40/84], [94mLoss[0m : 2.41422
[1mStep[0m  [48/84], [94mLoss[0m : 2.44193
[1mStep[0m  [56/84], [94mLoss[0m : 2.52358
[1mStep[0m  [64/84], [94mLoss[0m : 2.44403
[1mStep[0m  [72/84], [94mLoss[0m : 2.33067
[1mStep[0m  [80/84], [94mLoss[0m : 2.58165

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.335, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.38720
[1mStep[0m  [8/84], [94mLoss[0m : 2.42653
[1mStep[0m  [16/84], [94mLoss[0m : 2.76728
[1mStep[0m  [24/84], [94mLoss[0m : 2.07968
[1mStep[0m  [32/84], [94mLoss[0m : 2.67674
[1mStep[0m  [40/84], [94mLoss[0m : 2.50148
[1mStep[0m  [48/84], [94mLoss[0m : 2.54911
[1mStep[0m  [56/84], [94mLoss[0m : 2.32638
[1mStep[0m  [64/84], [94mLoss[0m : 2.78039
[1mStep[0m  [72/84], [94mLoss[0m : 2.34656
[1mStep[0m  [80/84], [94mLoss[0m : 2.66802

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.334, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26607
[1mStep[0m  [8/84], [94mLoss[0m : 2.46017
[1mStep[0m  [16/84], [94mLoss[0m : 2.56883
[1mStep[0m  [24/84], [94mLoss[0m : 2.60585
[1mStep[0m  [32/84], [94mLoss[0m : 2.62636
[1mStep[0m  [40/84], [94mLoss[0m : 2.67290
[1mStep[0m  [48/84], [94mLoss[0m : 2.36422
[1mStep[0m  [56/84], [94mLoss[0m : 2.44151
[1mStep[0m  [64/84], [94mLoss[0m : 2.21167
[1mStep[0m  [72/84], [94mLoss[0m : 2.49389
[1mStep[0m  [80/84], [94mLoss[0m : 2.64417

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.331, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42211
[1mStep[0m  [8/84], [94mLoss[0m : 2.51164
[1mStep[0m  [16/84], [94mLoss[0m : 2.86904
[1mStep[0m  [24/84], [94mLoss[0m : 2.58972
[1mStep[0m  [32/84], [94mLoss[0m : 2.56331
[1mStep[0m  [40/84], [94mLoss[0m : 2.41258
[1mStep[0m  [48/84], [94mLoss[0m : 2.41537
[1mStep[0m  [56/84], [94mLoss[0m : 2.78458
[1mStep[0m  [64/84], [94mLoss[0m : 2.29526
[1mStep[0m  [72/84], [94mLoss[0m : 2.20762
[1mStep[0m  [80/84], [94mLoss[0m : 2.65776

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.325, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46874
[1mStep[0m  [8/84], [94mLoss[0m : 2.66739
[1mStep[0m  [16/84], [94mLoss[0m : 2.38109
[1mStep[0m  [24/84], [94mLoss[0m : 2.36701
[1mStep[0m  [32/84], [94mLoss[0m : 2.36634
[1mStep[0m  [40/84], [94mLoss[0m : 2.57581
[1mStep[0m  [48/84], [94mLoss[0m : 2.01803
[1mStep[0m  [56/84], [94mLoss[0m : 2.70802
[1mStep[0m  [64/84], [94mLoss[0m : 2.45869
[1mStep[0m  [72/84], [94mLoss[0m : 2.45790
[1mStep[0m  [80/84], [94mLoss[0m : 2.41446

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.324, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27678
[1mStep[0m  [8/84], [94mLoss[0m : 2.37205
[1mStep[0m  [16/84], [94mLoss[0m : 2.31315
[1mStep[0m  [24/84], [94mLoss[0m : 2.62226
[1mStep[0m  [32/84], [94mLoss[0m : 2.35661
[1mStep[0m  [40/84], [94mLoss[0m : 2.38826
[1mStep[0m  [48/84], [94mLoss[0m : 1.91735
[1mStep[0m  [56/84], [94mLoss[0m : 2.74109
[1mStep[0m  [64/84], [94mLoss[0m : 2.59754
[1mStep[0m  [72/84], [94mLoss[0m : 2.14576
[1mStep[0m  [80/84], [94mLoss[0m : 2.29078

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.334, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.56503
[1mStep[0m  [8/84], [94mLoss[0m : 2.27776
[1mStep[0m  [16/84], [94mLoss[0m : 2.81259
[1mStep[0m  [24/84], [94mLoss[0m : 2.59715
[1mStep[0m  [32/84], [94mLoss[0m : 2.30825
[1mStep[0m  [40/84], [94mLoss[0m : 2.37292
[1mStep[0m  [48/84], [94mLoss[0m : 2.44582
[1mStep[0m  [56/84], [94mLoss[0m : 2.68124
[1mStep[0m  [64/84], [94mLoss[0m : 2.68047
[1mStep[0m  [72/84], [94mLoss[0m : 2.40928
[1mStep[0m  [80/84], [94mLoss[0m : 2.40555

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.327, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.330
====================================

Phase 1 - Evaluation MAE:  2.330002495220729
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 2.40869
[1mStep[0m  [8/84], [94mLoss[0m : 2.55264
[1mStep[0m  [16/84], [94mLoss[0m : 2.65956
[1mStep[0m  [24/84], [94mLoss[0m : 2.42908
[1mStep[0m  [32/84], [94mLoss[0m : 2.16393
[1mStep[0m  [40/84], [94mLoss[0m : 2.39020
[1mStep[0m  [48/84], [94mLoss[0m : 2.49825
[1mStep[0m  [56/84], [94mLoss[0m : 2.47287
[1mStep[0m  [64/84], [94mLoss[0m : 2.35161
[1mStep[0m  [72/84], [94mLoss[0m : 2.69114
[1mStep[0m  [80/84], [94mLoss[0m : 2.69804

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.328, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.21281
[1mStep[0m  [8/84], [94mLoss[0m : 2.39804
[1mStep[0m  [16/84], [94mLoss[0m : 2.58809
[1mStep[0m  [24/84], [94mLoss[0m : 2.46837
[1mStep[0m  [32/84], [94mLoss[0m : 2.52514
[1mStep[0m  [40/84], [94mLoss[0m : 2.51523
[1mStep[0m  [48/84], [94mLoss[0m : 2.29805
[1mStep[0m  [56/84], [94mLoss[0m : 2.34589
[1mStep[0m  [64/84], [94mLoss[0m : 2.35141
[1mStep[0m  [72/84], [94mLoss[0m : 2.33249
[1mStep[0m  [80/84], [94mLoss[0m : 2.27346

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.367, [92mTest[0m: 2.394, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.57733
[1mStep[0m  [8/84], [94mLoss[0m : 2.20688
[1mStep[0m  [16/84], [94mLoss[0m : 2.14899
[1mStep[0m  [24/84], [94mLoss[0m : 2.19823
[1mStep[0m  [32/84], [94mLoss[0m : 2.64171
[1mStep[0m  [40/84], [94mLoss[0m : 2.10785
[1mStep[0m  [48/84], [94mLoss[0m : 2.25894
[1mStep[0m  [56/84], [94mLoss[0m : 2.36463
[1mStep[0m  [64/84], [94mLoss[0m : 2.03101
[1mStep[0m  [72/84], [94mLoss[0m : 2.58858
[1mStep[0m  [80/84], [94mLoss[0m : 2.57009

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.293, [92mTest[0m: 2.447, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.83620
[1mStep[0m  [8/84], [94mLoss[0m : 2.12760
[1mStep[0m  [16/84], [94mLoss[0m : 2.23096
[1mStep[0m  [24/84], [94mLoss[0m : 2.35334
[1mStep[0m  [32/84], [94mLoss[0m : 2.39734
[1mStep[0m  [40/84], [94mLoss[0m : 2.09364
[1mStep[0m  [48/84], [94mLoss[0m : 2.02701
[1mStep[0m  [56/84], [94mLoss[0m : 2.13350
[1mStep[0m  [64/84], [94mLoss[0m : 2.22500
[1mStep[0m  [72/84], [94mLoss[0m : 2.22201
[1mStep[0m  [80/84], [94mLoss[0m : 2.18541

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.210, [92mTest[0m: 2.369, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.02069
[1mStep[0m  [8/84], [94mLoss[0m : 2.00910
[1mStep[0m  [16/84], [94mLoss[0m : 2.27417
[1mStep[0m  [24/84], [94mLoss[0m : 2.30886
[1mStep[0m  [32/84], [94mLoss[0m : 2.48694
[1mStep[0m  [40/84], [94mLoss[0m : 2.21644
[1mStep[0m  [48/84], [94mLoss[0m : 2.31681
[1mStep[0m  [56/84], [94mLoss[0m : 2.48260
[1mStep[0m  [64/84], [94mLoss[0m : 2.38432
[1mStep[0m  [72/84], [94mLoss[0m : 2.10110
[1mStep[0m  [80/84], [94mLoss[0m : 2.19571

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.157, [92mTest[0m: 2.405, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.23654
[1mStep[0m  [8/84], [94mLoss[0m : 2.27025
[1mStep[0m  [16/84], [94mLoss[0m : 1.78061
[1mStep[0m  [24/84], [94mLoss[0m : 2.10060
[1mStep[0m  [32/84], [94mLoss[0m : 2.43115
[1mStep[0m  [40/84], [94mLoss[0m : 2.27220
[1mStep[0m  [48/84], [94mLoss[0m : 2.08401
[1mStep[0m  [56/84], [94mLoss[0m : 2.16999
[1mStep[0m  [64/84], [94mLoss[0m : 1.84532
[1mStep[0m  [72/84], [94mLoss[0m : 1.83999
[1mStep[0m  [80/84], [94mLoss[0m : 2.00771

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.107, [92mTest[0m: 2.453, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.15475
[1mStep[0m  [8/84], [94mLoss[0m : 1.97564
[1mStep[0m  [16/84], [94mLoss[0m : 2.16828
[1mStep[0m  [24/84], [94mLoss[0m : 1.87776
[1mStep[0m  [32/84], [94mLoss[0m : 2.23675
[1mStep[0m  [40/84], [94mLoss[0m : 2.43488
[1mStep[0m  [48/84], [94mLoss[0m : 1.99405
[1mStep[0m  [56/84], [94mLoss[0m : 1.87617
[1mStep[0m  [64/84], [94mLoss[0m : 1.92814
[1mStep[0m  [72/84], [94mLoss[0m : 2.04725
[1mStep[0m  [80/84], [94mLoss[0m : 2.46586

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.050, [92mTest[0m: 2.388, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.84734
[1mStep[0m  [8/84], [94mLoss[0m : 1.97676
[1mStep[0m  [16/84], [94mLoss[0m : 1.82428
[1mStep[0m  [24/84], [94mLoss[0m : 1.78498
[1mStep[0m  [32/84], [94mLoss[0m : 1.93996
[1mStep[0m  [40/84], [94mLoss[0m : 2.00441
[1mStep[0m  [48/84], [94mLoss[0m : 1.71218
[1mStep[0m  [56/84], [94mLoss[0m : 1.98596
[1mStep[0m  [64/84], [94mLoss[0m : 2.07869
[1mStep[0m  [72/84], [94mLoss[0m : 1.73683
[1mStep[0m  [80/84], [94mLoss[0m : 2.01578

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.004, [92mTest[0m: 2.413, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.79301
[1mStep[0m  [8/84], [94mLoss[0m : 1.81412
[1mStep[0m  [16/84], [94mLoss[0m : 1.87989
[1mStep[0m  [24/84], [94mLoss[0m : 2.16928
[1mStep[0m  [32/84], [94mLoss[0m : 1.89306
[1mStep[0m  [40/84], [94mLoss[0m : 1.71671
[1mStep[0m  [48/84], [94mLoss[0m : 2.06491
[1mStep[0m  [56/84], [94mLoss[0m : 1.74097
[1mStep[0m  [64/84], [94mLoss[0m : 1.83366
[1mStep[0m  [72/84], [94mLoss[0m : 1.97758
[1mStep[0m  [80/84], [94mLoss[0m : 2.06762

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.936, [92mTest[0m: 2.417, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.63657
[1mStep[0m  [8/84], [94mLoss[0m : 1.74839
[1mStep[0m  [16/84], [94mLoss[0m : 1.89806
[1mStep[0m  [24/84], [94mLoss[0m : 1.75605
[1mStep[0m  [32/84], [94mLoss[0m : 1.73603
[1mStep[0m  [40/84], [94mLoss[0m : 1.91341
[1mStep[0m  [48/84], [94mLoss[0m : 1.90657
[1mStep[0m  [56/84], [94mLoss[0m : 2.04354
[1mStep[0m  [64/84], [94mLoss[0m : 2.17729
[1mStep[0m  [72/84], [94mLoss[0m : 2.16411
[1mStep[0m  [80/84], [94mLoss[0m : 1.97363

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.900, [92mTest[0m: 2.472, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.85937
[1mStep[0m  [8/84], [94mLoss[0m : 1.83219
[1mStep[0m  [16/84], [94mLoss[0m : 1.71452
[1mStep[0m  [24/84], [94mLoss[0m : 1.81205
[1mStep[0m  [32/84], [94mLoss[0m : 1.81308
[1mStep[0m  [40/84], [94mLoss[0m : 1.70614
[1mStep[0m  [48/84], [94mLoss[0m : 1.73656
[1mStep[0m  [56/84], [94mLoss[0m : 1.76664
[1mStep[0m  [64/84], [94mLoss[0m : 1.95720
[1mStep[0m  [72/84], [94mLoss[0m : 1.91906
[1mStep[0m  [80/84], [94mLoss[0m : 1.95326

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.846, [92mTest[0m: 2.466, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.76281
[1mStep[0m  [8/84], [94mLoss[0m : 1.86797
[1mStep[0m  [16/84], [94mLoss[0m : 1.70101
[1mStep[0m  [24/84], [94mLoss[0m : 2.01023
[1mStep[0m  [32/84], [94mLoss[0m : 1.75785
[1mStep[0m  [40/84], [94mLoss[0m : 1.65839
[1mStep[0m  [48/84], [94mLoss[0m : 1.97313
[1mStep[0m  [56/84], [94mLoss[0m : 1.69297
[1mStep[0m  [64/84], [94mLoss[0m : 1.89055
[1mStep[0m  [72/84], [94mLoss[0m : 1.79436
[1mStep[0m  [80/84], [94mLoss[0m : 1.81242

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.811, [92mTest[0m: 2.524, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.48047
[1mStep[0m  [8/84], [94mLoss[0m : 1.59597
[1mStep[0m  [16/84], [94mLoss[0m : 1.63137
[1mStep[0m  [24/84], [94mLoss[0m : 1.71913
[1mStep[0m  [32/84], [94mLoss[0m : 1.81388
[1mStep[0m  [40/84], [94mLoss[0m : 1.97089
[1mStep[0m  [48/84], [94mLoss[0m : 1.69672
[1mStep[0m  [56/84], [94mLoss[0m : 1.89818
[1mStep[0m  [64/84], [94mLoss[0m : 1.81308
[1mStep[0m  [72/84], [94mLoss[0m : 1.92642
[1mStep[0m  [80/84], [94mLoss[0m : 1.74302

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.767, [92mTest[0m: 2.512, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.52510
[1mStep[0m  [8/84], [94mLoss[0m : 1.89162
[1mStep[0m  [16/84], [94mLoss[0m : 1.73005
[1mStep[0m  [24/84], [94mLoss[0m : 1.55490
[1mStep[0m  [32/84], [94mLoss[0m : 1.66596
[1mStep[0m  [40/84], [94mLoss[0m : 1.66626
[1mStep[0m  [48/84], [94mLoss[0m : 1.79151
[1mStep[0m  [56/84], [94mLoss[0m : 1.68248
[1mStep[0m  [64/84], [94mLoss[0m : 1.74071
[1mStep[0m  [72/84], [94mLoss[0m : 1.87898
[1mStep[0m  [80/84], [94mLoss[0m : 1.90579

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.730, [92mTest[0m: 2.487, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.54412
[1mStep[0m  [8/84], [94mLoss[0m : 1.80219
[1mStep[0m  [16/84], [94mLoss[0m : 1.66523
[1mStep[0m  [24/84], [94mLoss[0m : 1.56330
[1mStep[0m  [32/84], [94mLoss[0m : 1.40488
[1mStep[0m  [40/84], [94mLoss[0m : 1.67092
[1mStep[0m  [48/84], [94mLoss[0m : 1.62238
[1mStep[0m  [56/84], [94mLoss[0m : 1.94429
[1mStep[0m  [64/84], [94mLoss[0m : 1.61394
[1mStep[0m  [72/84], [94mLoss[0m : 1.75448
[1mStep[0m  [80/84], [94mLoss[0m : 2.12776

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.707, [92mTest[0m: 2.506, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.53151
[1mStep[0m  [8/84], [94mLoss[0m : 1.45132
[1mStep[0m  [16/84], [94mLoss[0m : 1.70400
[1mStep[0m  [24/84], [94mLoss[0m : 1.66430
[1mStep[0m  [32/84], [94mLoss[0m : 1.66783
[1mStep[0m  [40/84], [94mLoss[0m : 1.76497
[1mStep[0m  [48/84], [94mLoss[0m : 1.60603
[1mStep[0m  [56/84], [94mLoss[0m : 1.79861
[1mStep[0m  [64/84], [94mLoss[0m : 1.73070
[1mStep[0m  [72/84], [94mLoss[0m : 1.67255
[1mStep[0m  [80/84], [94mLoss[0m : 1.68608

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.680, [92mTest[0m: 2.472, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.50921
[1mStep[0m  [8/84], [94mLoss[0m : 1.73839
[1mStep[0m  [16/84], [94mLoss[0m : 1.64886
[1mStep[0m  [24/84], [94mLoss[0m : 1.91726
[1mStep[0m  [32/84], [94mLoss[0m : 1.66846
[1mStep[0m  [40/84], [94mLoss[0m : 1.66608
[1mStep[0m  [48/84], [94mLoss[0m : 1.83792
[1mStep[0m  [56/84], [94mLoss[0m : 1.80527
[1mStep[0m  [64/84], [94mLoss[0m : 1.51816
[1mStep[0m  [72/84], [94mLoss[0m : 1.86023
[1mStep[0m  [80/84], [94mLoss[0m : 1.71730

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.691, [92mTest[0m: 2.508, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.55652
[1mStep[0m  [8/84], [94mLoss[0m : 1.72171
[1mStep[0m  [16/84], [94mLoss[0m : 1.79683
[1mStep[0m  [24/84], [94mLoss[0m : 1.47208
[1mStep[0m  [32/84], [94mLoss[0m : 1.61994
[1mStep[0m  [40/84], [94mLoss[0m : 1.46019
[1mStep[0m  [48/84], [94mLoss[0m : 1.47594
[1mStep[0m  [56/84], [94mLoss[0m : 1.91031
[1mStep[0m  [64/84], [94mLoss[0m : 1.73456
[1mStep[0m  [72/84], [94mLoss[0m : 1.46147
[1mStep[0m  [80/84], [94mLoss[0m : 1.53457

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.616, [92mTest[0m: 2.502, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.48325
[1mStep[0m  [8/84], [94mLoss[0m : 1.48029
[1mStep[0m  [16/84], [94mLoss[0m : 1.49302
[1mStep[0m  [24/84], [94mLoss[0m : 1.64561
[1mStep[0m  [32/84], [94mLoss[0m : 1.54108
[1mStep[0m  [40/84], [94mLoss[0m : 1.85302
[1mStep[0m  [48/84], [94mLoss[0m : 1.58421
[1mStep[0m  [56/84], [94mLoss[0m : 1.66188
[1mStep[0m  [64/84], [94mLoss[0m : 1.59510
[1mStep[0m  [72/84], [94mLoss[0m : 1.74862
[1mStep[0m  [80/84], [94mLoss[0m : 1.92692

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.603, [92mTest[0m: 2.549, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.63380
[1mStep[0m  [8/84], [94mLoss[0m : 1.54590
[1mStep[0m  [16/84], [94mLoss[0m : 1.63064
[1mStep[0m  [24/84], [94mLoss[0m : 1.63199
[1mStep[0m  [32/84], [94mLoss[0m : 1.48973
[1mStep[0m  [40/84], [94mLoss[0m : 1.64603
[1mStep[0m  [48/84], [94mLoss[0m : 1.73505
[1mStep[0m  [56/84], [94mLoss[0m : 1.69884
[1mStep[0m  [64/84], [94mLoss[0m : 1.38353
[1mStep[0m  [72/84], [94mLoss[0m : 1.47478
[1mStep[0m  [80/84], [94mLoss[0m : 1.58905

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.592, [92mTest[0m: 2.578, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.46516
[1mStep[0m  [8/84], [94mLoss[0m : 1.64082
[1mStep[0m  [16/84], [94mLoss[0m : 1.73890
[1mStep[0m  [24/84], [94mLoss[0m : 1.30822
[1mStep[0m  [32/84], [94mLoss[0m : 1.54341
[1mStep[0m  [40/84], [94mLoss[0m : 1.65372
[1mStep[0m  [48/84], [94mLoss[0m : 1.48935
[1mStep[0m  [56/84], [94mLoss[0m : 1.38232
[1mStep[0m  [64/84], [94mLoss[0m : 1.70894
[1mStep[0m  [72/84], [94mLoss[0m : 1.66184
[1mStep[0m  [80/84], [94mLoss[0m : 1.38980

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.548, [92mTest[0m: 2.514, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.30137
[1mStep[0m  [8/84], [94mLoss[0m : 1.48860
[1mStep[0m  [16/84], [94mLoss[0m : 1.58620
[1mStep[0m  [24/84], [94mLoss[0m : 1.45225
[1mStep[0m  [32/84], [94mLoss[0m : 1.56899
[1mStep[0m  [40/84], [94mLoss[0m : 1.62747
[1mStep[0m  [48/84], [94mLoss[0m : 1.57695
[1mStep[0m  [56/84], [94mLoss[0m : 1.36934
[1mStep[0m  [64/84], [94mLoss[0m : 1.67134
[1mStep[0m  [72/84], [94mLoss[0m : 1.55956
[1mStep[0m  [80/84], [94mLoss[0m : 1.79771

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.504, [92mTest[0m: 2.544, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.69142
[1mStep[0m  [8/84], [94mLoss[0m : 1.44938
[1mStep[0m  [16/84], [94mLoss[0m : 1.60095
[1mStep[0m  [24/84], [94mLoss[0m : 1.60267
[1mStep[0m  [32/84], [94mLoss[0m : 1.50147
[1mStep[0m  [40/84], [94mLoss[0m : 1.55460
[1mStep[0m  [48/84], [94mLoss[0m : 1.59492
[1mStep[0m  [56/84], [94mLoss[0m : 1.49432
[1mStep[0m  [64/84], [94mLoss[0m : 1.36900
[1mStep[0m  [72/84], [94mLoss[0m : 1.53830
[1mStep[0m  [80/84], [94mLoss[0m : 1.53100

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.506, [92mTest[0m: 2.542, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.58303
[1mStep[0m  [8/84], [94mLoss[0m : 1.42306
[1mStep[0m  [16/84], [94mLoss[0m : 1.29485
[1mStep[0m  [24/84], [94mLoss[0m : 1.66870
[1mStep[0m  [32/84], [94mLoss[0m : 1.24294
[1mStep[0m  [40/84], [94mLoss[0m : 1.57086
[1mStep[0m  [48/84], [94mLoss[0m : 1.23843
[1mStep[0m  [56/84], [94mLoss[0m : 1.58243
[1mStep[0m  [64/84], [94mLoss[0m : 1.46319
[1mStep[0m  [72/84], [94mLoss[0m : 1.49912
[1mStep[0m  [80/84], [94mLoss[0m : 1.57234

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.476, [92mTest[0m: 2.520, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.51969
[1mStep[0m  [8/84], [94mLoss[0m : 1.57021
[1mStep[0m  [16/84], [94mLoss[0m : 1.39412
[1mStep[0m  [24/84], [94mLoss[0m : 1.53600
[1mStep[0m  [32/84], [94mLoss[0m : 1.45661
[1mStep[0m  [40/84], [94mLoss[0m : 1.47712
[1mStep[0m  [48/84], [94mLoss[0m : 1.46332
[1mStep[0m  [56/84], [94mLoss[0m : 1.65717
[1mStep[0m  [64/84], [94mLoss[0m : 1.56767
[1mStep[0m  [72/84], [94mLoss[0m : 1.34546
[1mStep[0m  [80/84], [94mLoss[0m : 1.52315

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.448, [92mTest[0m: 2.527, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.38489
[1mStep[0m  [8/84], [94mLoss[0m : 1.26510
[1mStep[0m  [16/84], [94mLoss[0m : 1.50327
[1mStep[0m  [24/84], [94mLoss[0m : 1.36538
[1mStep[0m  [32/84], [94mLoss[0m : 1.44117
[1mStep[0m  [40/84], [94mLoss[0m : 1.34591
[1mStep[0m  [48/84], [94mLoss[0m : 1.48739
[1mStep[0m  [56/84], [94mLoss[0m : 1.40092
[1mStep[0m  [64/84], [94mLoss[0m : 1.51411
[1mStep[0m  [72/84], [94mLoss[0m : 1.60477
[1mStep[0m  [80/84], [94mLoss[0m : 1.61939

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.435, [92mTest[0m: 2.524, [96mlr[0m: 0.006772500000000002
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 25 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.538
====================================

Phase 2 - Evaluation MAE:  2.5381924510002136
MAE score P1       2.330002
MAE score P2       2.538192
loss               1.435355
learning_rate      0.007525
batch_size              128
hidden_sizes      [250, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping         True
dropout                 0.4
momentum                0.9
weight_decay         0.0001
Name: 11, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 10.48214
[1mStep[0m  [8/84], [94mLoss[0m : 7.62021
[1mStep[0m  [16/84], [94mLoss[0m : 3.16146
[1mStep[0m  [24/84], [94mLoss[0m : 3.12053
[1mStep[0m  [32/84], [94mLoss[0m : 3.27537
[1mStep[0m  [40/84], [94mLoss[0m : 2.44692
[1mStep[0m  [48/84], [94mLoss[0m : 2.58474
[1mStep[0m  [56/84], [94mLoss[0m : 2.59946
[1mStep[0m  [64/84], [94mLoss[0m : 2.69440
[1mStep[0m  [72/84], [94mLoss[0m : 2.69082
[1mStep[0m  [80/84], [94mLoss[0m : 2.42157

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.729, [92mTest[0m: 10.547, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.98406
[1mStep[0m  [8/84], [94mLoss[0m : 2.73854
[1mStep[0m  [16/84], [94mLoss[0m : 3.01956
[1mStep[0m  [24/84], [94mLoss[0m : 2.79773
[1mStep[0m  [32/84], [94mLoss[0m : 2.90812
[1mStep[0m  [40/84], [94mLoss[0m : 2.56389
[1mStep[0m  [48/84], [94mLoss[0m : 2.56425
[1mStep[0m  [56/84], [94mLoss[0m : 2.21215
[1mStep[0m  [64/84], [94mLoss[0m : 2.60562
[1mStep[0m  [72/84], [94mLoss[0m : 2.37455
[1mStep[0m  [80/84], [94mLoss[0m : 2.81827

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.679, [92mTest[0m: 2.370, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.77382
[1mStep[0m  [8/84], [94mLoss[0m : 2.67485
[1mStep[0m  [16/84], [94mLoss[0m : 2.84800
[1mStep[0m  [24/84], [94mLoss[0m : 2.73570
[1mStep[0m  [32/84], [94mLoss[0m : 2.78411
[1mStep[0m  [40/84], [94mLoss[0m : 2.32782
[1mStep[0m  [48/84], [94mLoss[0m : 2.65851
[1mStep[0m  [56/84], [94mLoss[0m : 2.75743
[1mStep[0m  [64/84], [94mLoss[0m : 2.44337
[1mStep[0m  [72/84], [94mLoss[0m : 2.42131
[1mStep[0m  [80/84], [94mLoss[0m : 2.55131

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.603, [92mTest[0m: 2.345, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67556
[1mStep[0m  [8/84], [94mLoss[0m : 2.56610
[1mStep[0m  [16/84], [94mLoss[0m : 2.37401
[1mStep[0m  [24/84], [94mLoss[0m : 2.58672
[1mStep[0m  [32/84], [94mLoss[0m : 2.67955
[1mStep[0m  [40/84], [94mLoss[0m : 2.66758
[1mStep[0m  [48/84], [94mLoss[0m : 2.60239
[1mStep[0m  [56/84], [94mLoss[0m : 2.53483
[1mStep[0m  [64/84], [94mLoss[0m : 2.52140
[1mStep[0m  [72/84], [94mLoss[0m : 2.44753
[1mStep[0m  [80/84], [94mLoss[0m : 2.43806

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.601, [92mTest[0m: 2.366, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.58249
[1mStep[0m  [8/84], [94mLoss[0m : 2.56547
[1mStep[0m  [16/84], [94mLoss[0m : 2.41058
[1mStep[0m  [24/84], [94mLoss[0m : 2.75170
[1mStep[0m  [32/84], [94mLoss[0m : 2.79868
[1mStep[0m  [40/84], [94mLoss[0m : 2.42075
[1mStep[0m  [48/84], [94mLoss[0m : 2.49073
[1mStep[0m  [56/84], [94mLoss[0m : 2.56480
[1mStep[0m  [64/84], [94mLoss[0m : 2.60819
[1mStep[0m  [72/84], [94mLoss[0m : 2.85976
[1mStep[0m  [80/84], [94mLoss[0m : 2.18989

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.590, [92mTest[0m: 2.344, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54035
[1mStep[0m  [8/84], [94mLoss[0m : 2.48369
[1mStep[0m  [16/84], [94mLoss[0m : 2.60269
[1mStep[0m  [24/84], [94mLoss[0m : 2.40051
[1mStep[0m  [32/84], [94mLoss[0m : 2.60208
[1mStep[0m  [40/84], [94mLoss[0m : 2.91079
[1mStep[0m  [48/84], [94mLoss[0m : 2.63471
[1mStep[0m  [56/84], [94mLoss[0m : 3.10233
[1mStep[0m  [64/84], [94mLoss[0m : 2.87012
[1mStep[0m  [72/84], [94mLoss[0m : 2.48083
[1mStep[0m  [80/84], [94mLoss[0m : 2.53598

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.595, [92mTest[0m: 2.347, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.38360
[1mStep[0m  [8/84], [94mLoss[0m : 2.58079
[1mStep[0m  [16/84], [94mLoss[0m : 2.32029
[1mStep[0m  [24/84], [94mLoss[0m : 2.81100
[1mStep[0m  [32/84], [94mLoss[0m : 2.78679
[1mStep[0m  [40/84], [94mLoss[0m : 2.65748
[1mStep[0m  [48/84], [94mLoss[0m : 2.47027
[1mStep[0m  [56/84], [94mLoss[0m : 2.73223
[1mStep[0m  [64/84], [94mLoss[0m : 2.23211
[1mStep[0m  [72/84], [94mLoss[0m : 2.85755
[1mStep[0m  [80/84], [94mLoss[0m : 2.65064

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.557, [92mTest[0m: 2.350, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.62693
[1mStep[0m  [8/84], [94mLoss[0m : 2.57088
[1mStep[0m  [16/84], [94mLoss[0m : 2.57255
[1mStep[0m  [24/84], [94mLoss[0m : 2.44356
[1mStep[0m  [32/84], [94mLoss[0m : 3.04561
[1mStep[0m  [40/84], [94mLoss[0m : 2.24478
[1mStep[0m  [48/84], [94mLoss[0m : 2.66260
[1mStep[0m  [56/84], [94mLoss[0m : 2.55611
[1mStep[0m  [64/84], [94mLoss[0m : 2.14188
[1mStep[0m  [72/84], [94mLoss[0m : 2.62911
[1mStep[0m  [80/84], [94mLoss[0m : 2.55818

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.548, [92mTest[0m: 2.341, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.79468
[1mStep[0m  [8/84], [94mLoss[0m : 2.56427
[1mStep[0m  [16/84], [94mLoss[0m : 2.44324
[1mStep[0m  [24/84], [94mLoss[0m : 2.60859
[1mStep[0m  [32/84], [94mLoss[0m : 2.67191
[1mStep[0m  [40/84], [94mLoss[0m : 2.49788
[1mStep[0m  [48/84], [94mLoss[0m : 2.80943
[1mStep[0m  [56/84], [94mLoss[0m : 2.19546
[1mStep[0m  [64/84], [94mLoss[0m : 2.52581
[1mStep[0m  [72/84], [94mLoss[0m : 2.46887
[1mStep[0m  [80/84], [94mLoss[0m : 2.58876

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.328, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49246
[1mStep[0m  [8/84], [94mLoss[0m : 2.12454
[1mStep[0m  [16/84], [94mLoss[0m : 2.51027
[1mStep[0m  [24/84], [94mLoss[0m : 2.52301
[1mStep[0m  [32/84], [94mLoss[0m : 2.84427
[1mStep[0m  [40/84], [94mLoss[0m : 2.55194
[1mStep[0m  [48/84], [94mLoss[0m : 2.64395
[1mStep[0m  [56/84], [94mLoss[0m : 2.48970
[1mStep[0m  [64/84], [94mLoss[0m : 2.60229
[1mStep[0m  [72/84], [94mLoss[0m : 2.60934
[1mStep[0m  [80/84], [94mLoss[0m : 2.48830

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.547, [92mTest[0m: 2.338, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26318
[1mStep[0m  [8/84], [94mLoss[0m : 2.64500
[1mStep[0m  [16/84], [94mLoss[0m : 2.58488
[1mStep[0m  [24/84], [94mLoss[0m : 2.91928
[1mStep[0m  [32/84], [94mLoss[0m : 2.68768
[1mStep[0m  [40/84], [94mLoss[0m : 2.41454
[1mStep[0m  [48/84], [94mLoss[0m : 2.70314
[1mStep[0m  [56/84], [94mLoss[0m : 2.50959
[1mStep[0m  [64/84], [94mLoss[0m : 2.29132
[1mStep[0m  [72/84], [94mLoss[0m : 3.04157
[1mStep[0m  [80/84], [94mLoss[0m : 2.34838

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.534, [92mTest[0m: 2.340, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.29364
[1mStep[0m  [8/84], [94mLoss[0m : 2.42880
[1mStep[0m  [16/84], [94mLoss[0m : 2.74834
[1mStep[0m  [24/84], [94mLoss[0m : 2.55170
[1mStep[0m  [32/84], [94mLoss[0m : 2.57986
[1mStep[0m  [40/84], [94mLoss[0m : 2.79275
[1mStep[0m  [48/84], [94mLoss[0m : 2.05013
[1mStep[0m  [56/84], [94mLoss[0m : 2.35905
[1mStep[0m  [64/84], [94mLoss[0m : 2.42064
[1mStep[0m  [72/84], [94mLoss[0m : 2.81357
[1mStep[0m  [80/84], [94mLoss[0m : 2.35911

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.334, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.70609
[1mStep[0m  [8/84], [94mLoss[0m : 2.43431
[1mStep[0m  [16/84], [94mLoss[0m : 2.53944
[1mStep[0m  [24/84], [94mLoss[0m : 2.35739
[1mStep[0m  [32/84], [94mLoss[0m : 2.40293
[1mStep[0m  [40/84], [94mLoss[0m : 2.64933
[1mStep[0m  [48/84], [94mLoss[0m : 2.57560
[1mStep[0m  [56/84], [94mLoss[0m : 2.99776
[1mStep[0m  [64/84], [94mLoss[0m : 2.65443
[1mStep[0m  [72/84], [94mLoss[0m : 2.52244
[1mStep[0m  [80/84], [94mLoss[0m : 2.34329

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.526, [92mTest[0m: 2.337, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.69553
[1mStep[0m  [8/84], [94mLoss[0m : 2.53795
[1mStep[0m  [16/84], [94mLoss[0m : 2.40205
[1mStep[0m  [24/84], [94mLoss[0m : 2.51943
[1mStep[0m  [32/84], [94mLoss[0m : 2.31754
[1mStep[0m  [40/84], [94mLoss[0m : 3.11217
[1mStep[0m  [48/84], [94mLoss[0m : 2.44788
[1mStep[0m  [56/84], [94mLoss[0m : 2.53076
[1mStep[0m  [64/84], [94mLoss[0m : 2.35709
[1mStep[0m  [72/84], [94mLoss[0m : 2.63679
[1mStep[0m  [80/84], [94mLoss[0m : 2.43430

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.522, [92mTest[0m: 2.336, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44069
[1mStep[0m  [8/84], [94mLoss[0m : 2.43144
[1mStep[0m  [16/84], [94mLoss[0m : 2.74576
[1mStep[0m  [24/84], [94mLoss[0m : 2.54401
[1mStep[0m  [32/84], [94mLoss[0m : 2.55498
[1mStep[0m  [40/84], [94mLoss[0m : 2.75171
[1mStep[0m  [48/84], [94mLoss[0m : 2.47896
[1mStep[0m  [56/84], [94mLoss[0m : 2.36367
[1mStep[0m  [64/84], [94mLoss[0m : 2.69760
[1mStep[0m  [72/84], [94mLoss[0m : 2.27010
[1mStep[0m  [80/84], [94mLoss[0m : 2.62722

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.326, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35249
[1mStep[0m  [8/84], [94mLoss[0m : 2.51938
[1mStep[0m  [16/84], [94mLoss[0m : 2.83913
[1mStep[0m  [24/84], [94mLoss[0m : 2.47095
[1mStep[0m  [32/84], [94mLoss[0m : 2.42784
[1mStep[0m  [40/84], [94mLoss[0m : 2.37519
[1mStep[0m  [48/84], [94mLoss[0m : 2.70543
[1mStep[0m  [56/84], [94mLoss[0m : 2.90747
[1mStep[0m  [64/84], [94mLoss[0m : 2.59179
[1mStep[0m  [72/84], [94mLoss[0m : 2.44426
[1mStep[0m  [80/84], [94mLoss[0m : 2.31968

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.338, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43939
[1mStep[0m  [8/84], [94mLoss[0m : 2.53316
[1mStep[0m  [16/84], [94mLoss[0m : 2.35396
[1mStep[0m  [24/84], [94mLoss[0m : 2.33261
[1mStep[0m  [32/84], [94mLoss[0m : 2.24425
[1mStep[0m  [40/84], [94mLoss[0m : 2.71647
[1mStep[0m  [48/84], [94mLoss[0m : 2.53712
[1mStep[0m  [56/84], [94mLoss[0m : 2.46114
[1mStep[0m  [64/84], [94mLoss[0m : 2.36683
[1mStep[0m  [72/84], [94mLoss[0m : 2.71050
[1mStep[0m  [80/84], [94mLoss[0m : 1.97261

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.329, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47204
[1mStep[0m  [8/84], [94mLoss[0m : 2.55206
[1mStep[0m  [16/84], [94mLoss[0m : 2.25776
[1mStep[0m  [24/84], [94mLoss[0m : 2.47274
[1mStep[0m  [32/84], [94mLoss[0m : 2.45412
[1mStep[0m  [40/84], [94mLoss[0m : 2.54631
[1mStep[0m  [48/84], [94mLoss[0m : 2.52840
[1mStep[0m  [56/84], [94mLoss[0m : 2.35477
[1mStep[0m  [64/84], [94mLoss[0m : 2.45834
[1mStep[0m  [72/84], [94mLoss[0m : 2.54694
[1mStep[0m  [80/84], [94mLoss[0m : 2.67056

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.327, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.34290
[1mStep[0m  [8/84], [94mLoss[0m : 2.41669
[1mStep[0m  [16/84], [94mLoss[0m : 2.54927
[1mStep[0m  [24/84], [94mLoss[0m : 2.10569
[1mStep[0m  [32/84], [94mLoss[0m : 2.77485
[1mStep[0m  [40/84], [94mLoss[0m : 2.60275
[1mStep[0m  [48/84], [94mLoss[0m : 2.57791
[1mStep[0m  [56/84], [94mLoss[0m : 2.61364
[1mStep[0m  [64/84], [94mLoss[0m : 2.35890
[1mStep[0m  [72/84], [94mLoss[0m : 2.61919
[1mStep[0m  [80/84], [94mLoss[0m : 2.31269

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.335, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33541
[1mStep[0m  [8/84], [94mLoss[0m : 2.54241
[1mStep[0m  [16/84], [94mLoss[0m : 2.34593
[1mStep[0m  [24/84], [94mLoss[0m : 2.23445
[1mStep[0m  [32/84], [94mLoss[0m : 2.74914
[1mStep[0m  [40/84], [94mLoss[0m : 2.65518
[1mStep[0m  [48/84], [94mLoss[0m : 2.74583
[1mStep[0m  [56/84], [94mLoss[0m : 2.28738
[1mStep[0m  [64/84], [94mLoss[0m : 2.56358
[1mStep[0m  [72/84], [94mLoss[0m : 2.36454
[1mStep[0m  [80/84], [94mLoss[0m : 2.49281

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.335, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47693
[1mStep[0m  [8/84], [94mLoss[0m : 2.55407
[1mStep[0m  [16/84], [94mLoss[0m : 2.82989
[1mStep[0m  [24/84], [94mLoss[0m : 2.50514
[1mStep[0m  [32/84], [94mLoss[0m : 2.34835
[1mStep[0m  [40/84], [94mLoss[0m : 1.99551
[1mStep[0m  [48/84], [94mLoss[0m : 2.69017
[1mStep[0m  [56/84], [94mLoss[0m : 2.65856
[1mStep[0m  [64/84], [94mLoss[0m : 2.43056
[1mStep[0m  [72/84], [94mLoss[0m : 2.49310
[1mStep[0m  [80/84], [94mLoss[0m : 2.25761

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.329, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37143
[1mStep[0m  [8/84], [94mLoss[0m : 2.59712
[1mStep[0m  [16/84], [94mLoss[0m : 2.21226
[1mStep[0m  [24/84], [94mLoss[0m : 2.38994
[1mStep[0m  [32/84], [94mLoss[0m : 2.73908
[1mStep[0m  [40/84], [94mLoss[0m : 2.54397
[1mStep[0m  [48/84], [94mLoss[0m : 2.66003
[1mStep[0m  [56/84], [94mLoss[0m : 2.82864
[1mStep[0m  [64/84], [94mLoss[0m : 2.36855
[1mStep[0m  [72/84], [94mLoss[0m : 2.35807
[1mStep[0m  [80/84], [94mLoss[0m : 2.41698

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.328, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.24959
[1mStep[0m  [8/84], [94mLoss[0m : 2.56109
[1mStep[0m  [16/84], [94mLoss[0m : 2.29483
[1mStep[0m  [24/84], [94mLoss[0m : 2.14926
[1mStep[0m  [32/84], [94mLoss[0m : 2.47216
[1mStep[0m  [40/84], [94mLoss[0m : 2.19570
[1mStep[0m  [48/84], [94mLoss[0m : 2.21752
[1mStep[0m  [56/84], [94mLoss[0m : 2.57319
[1mStep[0m  [64/84], [94mLoss[0m : 2.74714
[1mStep[0m  [72/84], [94mLoss[0m : 2.57276
[1mStep[0m  [80/84], [94mLoss[0m : 2.62184

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.324, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67473
[1mStep[0m  [8/84], [94mLoss[0m : 2.31122
[1mStep[0m  [16/84], [94mLoss[0m : 2.40460
[1mStep[0m  [24/84], [94mLoss[0m : 2.36433
[1mStep[0m  [32/84], [94mLoss[0m : 2.53921
[1mStep[0m  [40/84], [94mLoss[0m : 2.37738
[1mStep[0m  [48/84], [94mLoss[0m : 2.51750
[1mStep[0m  [56/84], [94mLoss[0m : 2.47728
[1mStep[0m  [64/84], [94mLoss[0m : 2.20819
[1mStep[0m  [72/84], [94mLoss[0m : 2.39263
[1mStep[0m  [80/84], [94mLoss[0m : 2.17726

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.331, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.22596
[1mStep[0m  [8/84], [94mLoss[0m : 2.26328
[1mStep[0m  [16/84], [94mLoss[0m : 2.46151
[1mStep[0m  [24/84], [94mLoss[0m : 2.03393
[1mStep[0m  [32/84], [94mLoss[0m : 2.42229
[1mStep[0m  [40/84], [94mLoss[0m : 2.29641
[1mStep[0m  [48/84], [94mLoss[0m : 2.31833
[1mStep[0m  [56/84], [94mLoss[0m : 2.56856
[1mStep[0m  [64/84], [94mLoss[0m : 2.15948
[1mStep[0m  [72/84], [94mLoss[0m : 2.31819
[1mStep[0m  [80/84], [94mLoss[0m : 2.53818

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.332, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49820
[1mStep[0m  [8/84], [94mLoss[0m : 2.26482
[1mStep[0m  [16/84], [94mLoss[0m : 2.43510
[1mStep[0m  [24/84], [94mLoss[0m : 2.50694
[1mStep[0m  [32/84], [94mLoss[0m : 2.24119
[1mStep[0m  [40/84], [94mLoss[0m : 2.43448
[1mStep[0m  [48/84], [94mLoss[0m : 2.25835
[1mStep[0m  [56/84], [94mLoss[0m : 2.23620
[1mStep[0m  [64/84], [94mLoss[0m : 2.74522
[1mStep[0m  [72/84], [94mLoss[0m : 2.33650
[1mStep[0m  [80/84], [94mLoss[0m : 2.52202

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.332, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.09476
[1mStep[0m  [8/84], [94mLoss[0m : 2.23462
[1mStep[0m  [16/84], [94mLoss[0m : 2.45053
[1mStep[0m  [24/84], [94mLoss[0m : 2.68583
[1mStep[0m  [32/84], [94mLoss[0m : 2.50672
[1mStep[0m  [40/84], [94mLoss[0m : 2.51041
[1mStep[0m  [48/84], [94mLoss[0m : 2.23760
[1mStep[0m  [56/84], [94mLoss[0m : 2.62753
[1mStep[0m  [64/84], [94mLoss[0m : 2.57791
[1mStep[0m  [72/84], [94mLoss[0m : 2.80629
[1mStep[0m  [80/84], [94mLoss[0m : 2.35176

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.327, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45051
[1mStep[0m  [8/84], [94mLoss[0m : 2.44954
[1mStep[0m  [16/84], [94mLoss[0m : 2.10444
[1mStep[0m  [24/84], [94mLoss[0m : 2.13893
[1mStep[0m  [32/84], [94mLoss[0m : 2.44337
[1mStep[0m  [40/84], [94mLoss[0m : 2.62138
[1mStep[0m  [48/84], [94mLoss[0m : 2.54288
[1mStep[0m  [56/84], [94mLoss[0m : 2.54709
[1mStep[0m  [64/84], [94mLoss[0m : 2.71833
[1mStep[0m  [72/84], [94mLoss[0m : 2.71196
[1mStep[0m  [80/84], [94mLoss[0m : 2.26565

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.336, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.57798
[1mStep[0m  [8/84], [94mLoss[0m : 2.50769
[1mStep[0m  [16/84], [94mLoss[0m : 2.51218
[1mStep[0m  [24/84], [94mLoss[0m : 2.09617
[1mStep[0m  [32/84], [94mLoss[0m : 2.36198
[1mStep[0m  [40/84], [94mLoss[0m : 2.42650
[1mStep[0m  [48/84], [94mLoss[0m : 2.40660
[1mStep[0m  [56/84], [94mLoss[0m : 2.64493
[1mStep[0m  [64/84], [94mLoss[0m : 2.76909
[1mStep[0m  [72/84], [94mLoss[0m : 2.65800
[1mStep[0m  [80/84], [94mLoss[0m : 2.69539

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.332, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.32648
[1mStep[0m  [8/84], [94mLoss[0m : 2.29656
[1mStep[0m  [16/84], [94mLoss[0m : 2.38980
[1mStep[0m  [24/84], [94mLoss[0m : 2.56267
[1mStep[0m  [32/84], [94mLoss[0m : 2.18598
[1mStep[0m  [40/84], [94mLoss[0m : 2.27076
[1mStep[0m  [48/84], [94mLoss[0m : 2.63444
[1mStep[0m  [56/84], [94mLoss[0m : 2.28444
[1mStep[0m  [64/84], [94mLoss[0m : 2.23543
[1mStep[0m  [72/84], [94mLoss[0m : 2.50776
[1mStep[0m  [80/84], [94mLoss[0m : 2.28117

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.331, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.328
====================================

Phase 1 - Evaluation MAE:  2.328434092657907
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 2.59581
[1mStep[0m  [8/84], [94mLoss[0m : 2.73286
[1mStep[0m  [16/84], [94mLoss[0m : 2.45929
[1mStep[0m  [24/84], [94mLoss[0m : 2.26442
[1mStep[0m  [32/84], [94mLoss[0m : 2.59172
[1mStep[0m  [40/84], [94mLoss[0m : 2.26910
[1mStep[0m  [48/84], [94mLoss[0m : 2.75510
[1mStep[0m  [56/84], [94mLoss[0m : 2.66982
[1mStep[0m  [64/84], [94mLoss[0m : 2.21397
[1mStep[0m  [72/84], [94mLoss[0m : 2.32421
[1mStep[0m  [80/84], [94mLoss[0m : 2.52748

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.321, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.38190
[1mStep[0m  [8/84], [94mLoss[0m : 2.59277
[1mStep[0m  [16/84], [94mLoss[0m : 2.34901
[1mStep[0m  [24/84], [94mLoss[0m : 2.43141
[1mStep[0m  [32/84], [94mLoss[0m : 2.36795
[1mStep[0m  [40/84], [94mLoss[0m : 2.07495
[1mStep[0m  [48/84], [94mLoss[0m : 2.24589
[1mStep[0m  [56/84], [94mLoss[0m : 2.20654
[1mStep[0m  [64/84], [94mLoss[0m : 2.17366
[1mStep[0m  [72/84], [94mLoss[0m : 2.25558
[1mStep[0m  [80/84], [94mLoss[0m : 2.43944

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.377, [92mTest[0m: 2.392, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37866
[1mStep[0m  [8/84], [94mLoss[0m : 2.29580
[1mStep[0m  [16/84], [94mLoss[0m : 2.60768
[1mStep[0m  [24/84], [94mLoss[0m : 2.43530
[1mStep[0m  [32/84], [94mLoss[0m : 2.09348
[1mStep[0m  [40/84], [94mLoss[0m : 2.25331
[1mStep[0m  [48/84], [94mLoss[0m : 2.18866
[1mStep[0m  [56/84], [94mLoss[0m : 2.31655
[1mStep[0m  [64/84], [94mLoss[0m : 2.34716
[1mStep[0m  [72/84], [94mLoss[0m : 2.28059
[1mStep[0m  [80/84], [94mLoss[0m : 2.28026

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.304, [92mTest[0m: 2.350, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50573
[1mStep[0m  [8/84], [94mLoss[0m : 2.14048
[1mStep[0m  [16/84], [94mLoss[0m : 2.12769
[1mStep[0m  [24/84], [94mLoss[0m : 2.40317
[1mStep[0m  [32/84], [94mLoss[0m : 2.20992
[1mStep[0m  [40/84], [94mLoss[0m : 2.09869
[1mStep[0m  [48/84], [94mLoss[0m : 2.27093
[1mStep[0m  [56/84], [94mLoss[0m : 2.66792
[1mStep[0m  [64/84], [94mLoss[0m : 2.15746
[1mStep[0m  [72/84], [94mLoss[0m : 2.47764
[1mStep[0m  [80/84], [94mLoss[0m : 2.23772

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.262, [92mTest[0m: 2.342, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.05623
[1mStep[0m  [8/84], [94mLoss[0m : 1.97449
[1mStep[0m  [16/84], [94mLoss[0m : 2.27446
[1mStep[0m  [24/84], [94mLoss[0m : 2.34474
[1mStep[0m  [32/84], [94mLoss[0m : 1.97343
[1mStep[0m  [40/84], [94mLoss[0m : 2.25978
[1mStep[0m  [48/84], [94mLoss[0m : 1.99078
[1mStep[0m  [56/84], [94mLoss[0m : 2.43960
[1mStep[0m  [64/84], [94mLoss[0m : 2.01390
[1mStep[0m  [72/84], [94mLoss[0m : 2.30635
[1mStep[0m  [80/84], [94mLoss[0m : 2.25757

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.168, [92mTest[0m: 2.377, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.90825
[1mStep[0m  [8/84], [94mLoss[0m : 2.29627
[1mStep[0m  [16/84], [94mLoss[0m : 2.00926
[1mStep[0m  [24/84], [94mLoss[0m : 2.23630
[1mStep[0m  [32/84], [94mLoss[0m : 1.83910
[1mStep[0m  [40/84], [94mLoss[0m : 2.12504
[1mStep[0m  [48/84], [94mLoss[0m : 2.05273
[1mStep[0m  [56/84], [94mLoss[0m : 2.24490
[1mStep[0m  [64/84], [94mLoss[0m : 1.99264
[1mStep[0m  [72/84], [94mLoss[0m : 2.19504
[1mStep[0m  [80/84], [94mLoss[0m : 2.05078

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.132, [92mTest[0m: 2.434, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.94730
[1mStep[0m  [8/84], [94mLoss[0m : 1.82149
[1mStep[0m  [16/84], [94mLoss[0m : 2.25221
[1mStep[0m  [24/84], [94mLoss[0m : 2.06706
[1mStep[0m  [32/84], [94mLoss[0m : 2.08831
[1mStep[0m  [40/84], [94mLoss[0m : 2.14230
[1mStep[0m  [48/84], [94mLoss[0m : 2.22282
[1mStep[0m  [56/84], [94mLoss[0m : 2.66557
[1mStep[0m  [64/84], [94mLoss[0m : 1.80946
[1mStep[0m  [72/84], [94mLoss[0m : 1.99174
[1mStep[0m  [80/84], [94mLoss[0m : 1.94802

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.044, [92mTest[0m: 2.416, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.88593
[1mStep[0m  [8/84], [94mLoss[0m : 2.12472
[1mStep[0m  [16/84], [94mLoss[0m : 2.00042
[1mStep[0m  [24/84], [94mLoss[0m : 1.92068
[1mStep[0m  [32/84], [94mLoss[0m : 2.13644
[1mStep[0m  [40/84], [94mLoss[0m : 2.13008
[1mStep[0m  [48/84], [94mLoss[0m : 2.16579
[1mStep[0m  [56/84], [94mLoss[0m : 2.11870
[1mStep[0m  [64/84], [94mLoss[0m : 2.29440
[1mStep[0m  [72/84], [94mLoss[0m : 2.19743
[1mStep[0m  [80/84], [94mLoss[0m : 2.15145

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.028, [92mTest[0m: 2.432, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.00147
[1mStep[0m  [8/84], [94mLoss[0m : 2.05591
[1mStep[0m  [16/84], [94mLoss[0m : 2.01892
[1mStep[0m  [24/84], [94mLoss[0m : 1.67835
[1mStep[0m  [32/84], [94mLoss[0m : 1.82124
[1mStep[0m  [40/84], [94mLoss[0m : 2.06463
[1mStep[0m  [48/84], [94mLoss[0m : 2.34324
[1mStep[0m  [56/84], [94mLoss[0m : 2.22248
[1mStep[0m  [64/84], [94mLoss[0m : 1.96785
[1mStep[0m  [72/84], [94mLoss[0m : 2.19490
[1mStep[0m  [80/84], [94mLoss[0m : 1.79927

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.990, [92mTest[0m: 2.401, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.80182
[1mStep[0m  [8/84], [94mLoss[0m : 2.03407
[1mStep[0m  [16/84], [94mLoss[0m : 1.91411
[1mStep[0m  [24/84], [94mLoss[0m : 2.07760
[1mStep[0m  [32/84], [94mLoss[0m : 1.97294
[1mStep[0m  [40/84], [94mLoss[0m : 1.91416
[1mStep[0m  [48/84], [94mLoss[0m : 2.07931
[1mStep[0m  [56/84], [94mLoss[0m : 1.85192
[1mStep[0m  [64/84], [94mLoss[0m : 2.07989
[1mStep[0m  [72/84], [94mLoss[0m : 1.99941
[1mStep[0m  [80/84], [94mLoss[0m : 1.97453

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.913, [92mTest[0m: 2.442, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.77077
[1mStep[0m  [8/84], [94mLoss[0m : 2.01165
[1mStep[0m  [16/84], [94mLoss[0m : 1.91985
[1mStep[0m  [24/84], [94mLoss[0m : 2.00746
[1mStep[0m  [32/84], [94mLoss[0m : 1.82455
[1mStep[0m  [40/84], [94mLoss[0m : 1.80923
[1mStep[0m  [48/84], [94mLoss[0m : 1.90068
[1mStep[0m  [56/84], [94mLoss[0m : 2.06681
[1mStep[0m  [64/84], [94mLoss[0m : 1.93736
[1mStep[0m  [72/84], [94mLoss[0m : 2.35583
[1mStep[0m  [80/84], [94mLoss[0m : 1.97403

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.898, [92mTest[0m: 2.418, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.89994
[1mStep[0m  [8/84], [94mLoss[0m : 1.86180
[1mStep[0m  [16/84], [94mLoss[0m : 1.68359
[1mStep[0m  [24/84], [94mLoss[0m : 1.98239
[1mStep[0m  [32/84], [94mLoss[0m : 1.94964
[1mStep[0m  [40/84], [94mLoss[0m : 1.75983
[1mStep[0m  [48/84], [94mLoss[0m : 1.89611
[1mStep[0m  [56/84], [94mLoss[0m : 1.89245
[1mStep[0m  [64/84], [94mLoss[0m : 1.91214
[1mStep[0m  [72/84], [94mLoss[0m : 1.91294
[1mStep[0m  [80/84], [94mLoss[0m : 1.94596

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.868, [92mTest[0m: 2.462, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.76826
[1mStep[0m  [8/84], [94mLoss[0m : 1.97037
[1mStep[0m  [16/84], [94mLoss[0m : 1.80533
[1mStep[0m  [24/84], [94mLoss[0m : 1.88089
[1mStep[0m  [32/84], [94mLoss[0m : 1.73514
[1mStep[0m  [40/84], [94mLoss[0m : 1.95194
[1mStep[0m  [48/84], [94mLoss[0m : 1.69107
[1mStep[0m  [56/84], [94mLoss[0m : 1.54444
[1mStep[0m  [64/84], [94mLoss[0m : 1.92284
[1mStep[0m  [72/84], [94mLoss[0m : 1.95850
[1mStep[0m  [80/84], [94mLoss[0m : 1.86877

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.811, [92mTest[0m: 2.509, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.50890
[1mStep[0m  [8/84], [94mLoss[0m : 1.91786
[1mStep[0m  [16/84], [94mLoss[0m : 1.83524
[1mStep[0m  [24/84], [94mLoss[0m : 1.96255
[1mStep[0m  [32/84], [94mLoss[0m : 1.74276
[1mStep[0m  [40/84], [94mLoss[0m : 1.92917
[1mStep[0m  [48/84], [94mLoss[0m : 1.63294
[1mStep[0m  [56/84], [94mLoss[0m : 1.59176
[1mStep[0m  [64/84], [94mLoss[0m : 1.66714
[1mStep[0m  [72/84], [94mLoss[0m : 1.82146
[1mStep[0m  [80/84], [94mLoss[0m : 1.85808

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.796, [92mTest[0m: 2.437, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.50039
[1mStep[0m  [8/84], [94mLoss[0m : 1.72821
[1mStep[0m  [16/84], [94mLoss[0m : 1.44535
[1mStep[0m  [24/84], [94mLoss[0m : 1.83560
[1mStep[0m  [32/84], [94mLoss[0m : 1.61167
[1mStep[0m  [40/84], [94mLoss[0m : 1.77748
[1mStep[0m  [48/84], [94mLoss[0m : 1.98566
[1mStep[0m  [56/84], [94mLoss[0m : 1.61433
[1mStep[0m  [64/84], [94mLoss[0m : 1.78918
[1mStep[0m  [72/84], [94mLoss[0m : 1.71258
[1mStep[0m  [80/84], [94mLoss[0m : 1.86023

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.755, [92mTest[0m: 2.498, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.72052
[1mStep[0m  [8/84], [94mLoss[0m : 1.71156
[1mStep[0m  [16/84], [94mLoss[0m : 1.73473
[1mStep[0m  [24/84], [94mLoss[0m : 1.85858
[1mStep[0m  [32/84], [94mLoss[0m : 1.72196
[1mStep[0m  [40/84], [94mLoss[0m : 1.53622
[1mStep[0m  [48/84], [94mLoss[0m : 2.01408
[1mStep[0m  [56/84], [94mLoss[0m : 1.92101
[1mStep[0m  [64/84], [94mLoss[0m : 1.60243
[1mStep[0m  [72/84], [94mLoss[0m : 1.87802
[1mStep[0m  [80/84], [94mLoss[0m : 1.59177

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.740, [92mTest[0m: 2.503, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.66557
[1mStep[0m  [8/84], [94mLoss[0m : 1.95129
[1mStep[0m  [16/84], [94mLoss[0m : 1.53836
[1mStep[0m  [24/84], [94mLoss[0m : 1.75998
[1mStep[0m  [32/84], [94mLoss[0m : 1.89180
[1mStep[0m  [40/84], [94mLoss[0m : 1.77040
[1mStep[0m  [48/84], [94mLoss[0m : 1.72307
[1mStep[0m  [56/84], [94mLoss[0m : 1.82227
[1mStep[0m  [64/84], [94mLoss[0m : 1.53735
[1mStep[0m  [72/84], [94mLoss[0m : 1.64786
[1mStep[0m  [80/84], [94mLoss[0m : 1.91578

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.714, [92mTest[0m: 2.473, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.52715
[1mStep[0m  [8/84], [94mLoss[0m : 1.75186
[1mStep[0m  [16/84], [94mLoss[0m : 1.66584
[1mStep[0m  [24/84], [94mLoss[0m : 1.59949
[1mStep[0m  [32/84], [94mLoss[0m : 1.79282
[1mStep[0m  [40/84], [94mLoss[0m : 1.43093
[1mStep[0m  [48/84], [94mLoss[0m : 1.91497
[1mStep[0m  [56/84], [94mLoss[0m : 1.74156
[1mStep[0m  [64/84], [94mLoss[0m : 1.77044
[1mStep[0m  [72/84], [94mLoss[0m : 1.80978
[1mStep[0m  [80/84], [94mLoss[0m : 1.62566

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.664, [92mTest[0m: 2.516, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.77679
[1mStep[0m  [8/84], [94mLoss[0m : 1.67459
[1mStep[0m  [16/84], [94mLoss[0m : 1.33382
[1mStep[0m  [24/84], [94mLoss[0m : 1.45943
[1mStep[0m  [32/84], [94mLoss[0m : 1.54512
[1mStep[0m  [40/84], [94mLoss[0m : 1.70842
[1mStep[0m  [48/84], [94mLoss[0m : 1.76169
[1mStep[0m  [56/84], [94mLoss[0m : 1.95031
[1mStep[0m  [64/84], [94mLoss[0m : 1.58496
[1mStep[0m  [72/84], [94mLoss[0m : 1.60164
[1mStep[0m  [80/84], [94mLoss[0m : 1.77237

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.644, [92mTest[0m: 2.527, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.64195
[1mStep[0m  [8/84], [94mLoss[0m : 1.38922
[1mStep[0m  [16/84], [94mLoss[0m : 1.70781
[1mStep[0m  [24/84], [94mLoss[0m : 1.43761
[1mStep[0m  [32/84], [94mLoss[0m : 1.64275
[1mStep[0m  [40/84], [94mLoss[0m : 1.66135
[1mStep[0m  [48/84], [94mLoss[0m : 1.51804
[1mStep[0m  [56/84], [94mLoss[0m : 1.57809
[1mStep[0m  [64/84], [94mLoss[0m : 1.60042
[1mStep[0m  [72/84], [94mLoss[0m : 1.75563
[1mStep[0m  [80/84], [94mLoss[0m : 1.70687

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.642, [92mTest[0m: 2.513, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.60453
[1mStep[0m  [8/84], [94mLoss[0m : 1.52135
[1mStep[0m  [16/84], [94mLoss[0m : 1.56313
[1mStep[0m  [24/84], [94mLoss[0m : 1.51473
[1mStep[0m  [32/84], [94mLoss[0m : 1.60481
[1mStep[0m  [40/84], [94mLoss[0m : 1.58538
[1mStep[0m  [48/84], [94mLoss[0m : 1.46643
[1mStep[0m  [56/84], [94mLoss[0m : 1.68696
[1mStep[0m  [64/84], [94mLoss[0m : 1.64063
[1mStep[0m  [72/84], [94mLoss[0m : 1.54074
[1mStep[0m  [80/84], [94mLoss[0m : 1.57975

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.594, [92mTest[0m: 2.471, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.53830
[1mStep[0m  [8/84], [94mLoss[0m : 1.67531
[1mStep[0m  [16/84], [94mLoss[0m : 1.40878
[1mStep[0m  [24/84], [94mLoss[0m : 1.52707
[1mStep[0m  [32/84], [94mLoss[0m : 1.54898
[1mStep[0m  [40/84], [94mLoss[0m : 1.53678
[1mStep[0m  [48/84], [94mLoss[0m : 1.75240
[1mStep[0m  [56/84], [94mLoss[0m : 1.47643
[1mStep[0m  [64/84], [94mLoss[0m : 1.41210
[1mStep[0m  [72/84], [94mLoss[0m : 1.41935
[1mStep[0m  [80/84], [94mLoss[0m : 1.53141

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.564, [92mTest[0m: 2.473, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.25681
[1mStep[0m  [8/84], [94mLoss[0m : 1.54361
[1mStep[0m  [16/84], [94mLoss[0m : 1.59864
[1mStep[0m  [24/84], [94mLoss[0m : 1.68530
[1mStep[0m  [32/84], [94mLoss[0m : 1.57227
[1mStep[0m  [40/84], [94mLoss[0m : 1.49242
[1mStep[0m  [48/84], [94mLoss[0m : 1.55177
[1mStep[0m  [56/84], [94mLoss[0m : 1.49589
[1mStep[0m  [64/84], [94mLoss[0m : 1.49547
[1mStep[0m  [72/84], [94mLoss[0m : 1.62224
[1mStep[0m  [80/84], [94mLoss[0m : 1.44141

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.552, [92mTest[0m: 2.542, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.58879
[1mStep[0m  [8/84], [94mLoss[0m : 1.70640
[1mStep[0m  [16/84], [94mLoss[0m : 1.33647
[1mStep[0m  [24/84], [94mLoss[0m : 1.42579
[1mStep[0m  [32/84], [94mLoss[0m : 1.34016
[1mStep[0m  [40/84], [94mLoss[0m : 1.57036
[1mStep[0m  [48/84], [94mLoss[0m : 1.31231
[1mStep[0m  [56/84], [94mLoss[0m : 1.48388
[1mStep[0m  [64/84], [94mLoss[0m : 1.47357
[1mStep[0m  [72/84], [94mLoss[0m : 1.47451
[1mStep[0m  [80/84], [94mLoss[0m : 1.37724

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.521, [92mTest[0m: 2.572, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.56295
[1mStep[0m  [8/84], [94mLoss[0m : 1.29329
[1mStep[0m  [16/84], [94mLoss[0m : 1.53136
[1mStep[0m  [24/84], [94mLoss[0m : 1.54040
[1mStep[0m  [32/84], [94mLoss[0m : 1.41845
[1mStep[0m  [40/84], [94mLoss[0m : 1.64461
[1mStep[0m  [48/84], [94mLoss[0m : 1.53548
[1mStep[0m  [56/84], [94mLoss[0m : 1.59216
[1mStep[0m  [64/84], [94mLoss[0m : 1.56327
[1mStep[0m  [72/84], [94mLoss[0m : 1.73207
[1mStep[0m  [80/84], [94mLoss[0m : 1.61539

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.511, [92mTest[0m: 2.531, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.32708
[1mStep[0m  [8/84], [94mLoss[0m : 1.46338
[1mStep[0m  [16/84], [94mLoss[0m : 1.44737
[1mStep[0m  [24/84], [94mLoss[0m : 1.34791
[1mStep[0m  [32/84], [94mLoss[0m : 1.61015
[1mStep[0m  [40/84], [94mLoss[0m : 1.40364
[1mStep[0m  [48/84], [94mLoss[0m : 1.68122
[1mStep[0m  [56/84], [94mLoss[0m : 1.39547
[1mStep[0m  [64/84], [94mLoss[0m : 1.57333
[1mStep[0m  [72/84], [94mLoss[0m : 1.48302
[1mStep[0m  [80/84], [94mLoss[0m : 1.56739

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.479, [92mTest[0m: 2.516, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.54410
[1mStep[0m  [8/84], [94mLoss[0m : 1.66554
[1mStep[0m  [16/84], [94mLoss[0m : 1.41262
[1mStep[0m  [24/84], [94mLoss[0m : 1.31724
[1mStep[0m  [32/84], [94mLoss[0m : 1.32760
[1mStep[0m  [40/84], [94mLoss[0m : 1.35943
[1mStep[0m  [48/84], [94mLoss[0m : 1.46036
[1mStep[0m  [56/84], [94mLoss[0m : 1.19537
[1mStep[0m  [64/84], [94mLoss[0m : 1.58388
[1mStep[0m  [72/84], [94mLoss[0m : 1.57144
[1mStep[0m  [80/84], [94mLoss[0m : 1.52521

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.473, [92mTest[0m: 2.510, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.35631
[1mStep[0m  [8/84], [94mLoss[0m : 1.33902
[1mStep[0m  [16/84], [94mLoss[0m : 1.35752
[1mStep[0m  [24/84], [94mLoss[0m : 1.33857
[1mStep[0m  [32/84], [94mLoss[0m : 1.32240
[1mStep[0m  [40/84], [94mLoss[0m : 1.42310
[1mStep[0m  [48/84], [94mLoss[0m : 1.49926
[1mStep[0m  [56/84], [94mLoss[0m : 1.66115
[1mStep[0m  [64/84], [94mLoss[0m : 1.57035
[1mStep[0m  [72/84], [94mLoss[0m : 1.48516
[1mStep[0m  [80/84], [94mLoss[0m : 1.53880

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.468, [92mTest[0m: 2.502, [96mlr[0m: 0.006772500000000002
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 27 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.475
====================================

Phase 2 - Evaluation MAE:  2.475254237651825
MAE score P1       2.328434
MAE score P2       2.475254
loss                  1.468
learning_rate      0.007525
batch_size              128
hidden_sizes      [250, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping         True
dropout                 0.4
momentum                0.9
weight_decay          0.001
Name: 12, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 10.88619
[1mStep[0m  [4/42], [94mLoss[0m : 10.68000
[1mStep[0m  [8/42], [94mLoss[0m : 9.31425
[1mStep[0m  [12/42], [94mLoss[0m : 8.61248
[1mStep[0m  [16/42], [94mLoss[0m : 6.98032
[1mStep[0m  [20/42], [94mLoss[0m : 6.76091
[1mStep[0m  [24/42], [94mLoss[0m : 5.01073
[1mStep[0m  [28/42], [94mLoss[0m : 5.03478
[1mStep[0m  [32/42], [94mLoss[0m : 4.03324
[1mStep[0m  [36/42], [94mLoss[0m : 3.22350
[1mStep[0m  [40/42], [94mLoss[0m : 3.15809

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.654, [92mTest[0m: 10.945, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.15378
[1mStep[0m  [4/42], [94mLoss[0m : 2.90197
[1mStep[0m  [8/42], [94mLoss[0m : 3.13124
[1mStep[0m  [12/42], [94mLoss[0m : 2.82561
[1mStep[0m  [16/42], [94mLoss[0m : 2.60782
[1mStep[0m  [20/42], [94mLoss[0m : 2.73442
[1mStep[0m  [24/42], [94mLoss[0m : 2.60575
[1mStep[0m  [28/42], [94mLoss[0m : 2.63050
[1mStep[0m  [32/42], [94mLoss[0m : 2.54613
[1mStep[0m  [36/42], [94mLoss[0m : 2.58423
[1mStep[0m  [40/42], [94mLoss[0m : 2.90083

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.762, [92mTest[0m: 3.863, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64779
[1mStep[0m  [4/42], [94mLoss[0m : 2.42222
[1mStep[0m  [8/42], [94mLoss[0m : 2.87293
[1mStep[0m  [12/42], [94mLoss[0m : 2.52746
[1mStep[0m  [16/42], [94mLoss[0m : 2.81932
[1mStep[0m  [20/42], [94mLoss[0m : 2.54262
[1mStep[0m  [24/42], [94mLoss[0m : 2.85885
[1mStep[0m  [28/42], [94mLoss[0m : 2.51240
[1mStep[0m  [32/42], [94mLoss[0m : 2.98090
[1mStep[0m  [36/42], [94mLoss[0m : 2.64256
[1mStep[0m  [40/42], [94mLoss[0m : 2.32763

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.635, [92mTest[0m: 2.639, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.86827
[1mStep[0m  [4/42], [94mLoss[0m : 2.75765
[1mStep[0m  [8/42], [94mLoss[0m : 2.71770
[1mStep[0m  [12/42], [94mLoss[0m : 2.52182
[1mStep[0m  [16/42], [94mLoss[0m : 2.71164
[1mStep[0m  [20/42], [94mLoss[0m : 2.63911
[1mStep[0m  [24/42], [94mLoss[0m : 2.65026
[1mStep[0m  [28/42], [94mLoss[0m : 2.72736
[1mStep[0m  [32/42], [94mLoss[0m : 2.42312
[1mStep[0m  [36/42], [94mLoss[0m : 2.86246
[1mStep[0m  [40/42], [94mLoss[0m : 2.47527

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.609, [92mTest[0m: 2.622, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62213
[1mStep[0m  [4/42], [94mLoss[0m : 2.63164
[1mStep[0m  [8/42], [94mLoss[0m : 2.54858
[1mStep[0m  [12/42], [94mLoss[0m : 2.31242
[1mStep[0m  [16/42], [94mLoss[0m : 2.52161
[1mStep[0m  [20/42], [94mLoss[0m : 2.55547
[1mStep[0m  [24/42], [94mLoss[0m : 2.55736
[1mStep[0m  [28/42], [94mLoss[0m : 2.54687
[1mStep[0m  [32/42], [94mLoss[0m : 2.62182
[1mStep[0m  [36/42], [94mLoss[0m : 2.60446
[1mStep[0m  [40/42], [94mLoss[0m : 2.34652

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.570, [92mTest[0m: 2.453, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46774
[1mStep[0m  [4/42], [94mLoss[0m : 2.26122
[1mStep[0m  [8/42], [94mLoss[0m : 2.58246
[1mStep[0m  [12/42], [94mLoss[0m : 2.66713
[1mStep[0m  [16/42], [94mLoss[0m : 2.48843
[1mStep[0m  [20/42], [94mLoss[0m : 2.52306
[1mStep[0m  [24/42], [94mLoss[0m : 2.78573
[1mStep[0m  [28/42], [94mLoss[0m : 2.76207
[1mStep[0m  [32/42], [94mLoss[0m : 2.58917
[1mStep[0m  [36/42], [94mLoss[0m : 2.47669
[1mStep[0m  [40/42], [94mLoss[0m : 2.41731

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.547, [92mTest[0m: 2.483, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62141
[1mStep[0m  [4/42], [94mLoss[0m : 2.42971
[1mStep[0m  [8/42], [94mLoss[0m : 2.70521
[1mStep[0m  [12/42], [94mLoss[0m : 2.79873
[1mStep[0m  [16/42], [94mLoss[0m : 2.25614
[1mStep[0m  [20/42], [94mLoss[0m : 2.42560
[1mStep[0m  [24/42], [94mLoss[0m : 2.47045
[1mStep[0m  [28/42], [94mLoss[0m : 2.51601
[1mStep[0m  [32/42], [94mLoss[0m : 2.59652
[1mStep[0m  [36/42], [94mLoss[0m : 2.69016
[1mStep[0m  [40/42], [94mLoss[0m : 2.57928

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.542, [92mTest[0m: 2.489, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43006
[1mStep[0m  [4/42], [94mLoss[0m : 2.58238
[1mStep[0m  [8/42], [94mLoss[0m : 2.53729
[1mStep[0m  [12/42], [94mLoss[0m : 2.55636
[1mStep[0m  [16/42], [94mLoss[0m : 2.30240
[1mStep[0m  [20/42], [94mLoss[0m : 2.61802
[1mStep[0m  [24/42], [94mLoss[0m : 2.58787
[1mStep[0m  [28/42], [94mLoss[0m : 2.53837
[1mStep[0m  [32/42], [94mLoss[0m : 2.43909
[1mStep[0m  [36/42], [94mLoss[0m : 2.65490
[1mStep[0m  [40/42], [94mLoss[0m : 2.55028

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.534, [92mTest[0m: 2.448, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.68581
[1mStep[0m  [4/42], [94mLoss[0m : 2.33596
[1mStep[0m  [8/42], [94mLoss[0m : 2.36259
[1mStep[0m  [12/42], [94mLoss[0m : 2.41254
[1mStep[0m  [16/42], [94mLoss[0m : 2.42601
[1mStep[0m  [20/42], [94mLoss[0m : 2.50401
[1mStep[0m  [24/42], [94mLoss[0m : 2.52356
[1mStep[0m  [28/42], [94mLoss[0m : 2.43154
[1mStep[0m  [32/42], [94mLoss[0m : 2.43068
[1mStep[0m  [36/42], [94mLoss[0m : 2.57625
[1mStep[0m  [40/42], [94mLoss[0m : 2.52477

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.530, [92mTest[0m: 2.487, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52669
[1mStep[0m  [4/42], [94mLoss[0m : 2.34034
[1mStep[0m  [8/42], [94mLoss[0m : 2.53465
[1mStep[0m  [12/42], [94mLoss[0m : 2.43165
[1mStep[0m  [16/42], [94mLoss[0m : 2.33050
[1mStep[0m  [20/42], [94mLoss[0m : 2.52150
[1mStep[0m  [24/42], [94mLoss[0m : 2.53247
[1mStep[0m  [28/42], [94mLoss[0m : 2.60591
[1mStep[0m  [32/42], [94mLoss[0m : 2.47257
[1mStep[0m  [36/42], [94mLoss[0m : 2.59517
[1mStep[0m  [40/42], [94mLoss[0m : 2.39272

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.394, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.66006
[1mStep[0m  [4/42], [94mLoss[0m : 2.61764
[1mStep[0m  [8/42], [94mLoss[0m : 2.62511
[1mStep[0m  [12/42], [94mLoss[0m : 2.34189
[1mStep[0m  [16/42], [94mLoss[0m : 2.63332
[1mStep[0m  [20/42], [94mLoss[0m : 2.51022
[1mStep[0m  [24/42], [94mLoss[0m : 2.36863
[1mStep[0m  [28/42], [94mLoss[0m : 2.56836
[1mStep[0m  [32/42], [94mLoss[0m : 2.76670
[1mStep[0m  [36/42], [94mLoss[0m : 2.57489
[1mStep[0m  [40/42], [94mLoss[0m : 2.42031

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.389, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56397
[1mStep[0m  [4/42], [94mLoss[0m : 2.40380
[1mStep[0m  [8/42], [94mLoss[0m : 2.73367
[1mStep[0m  [12/42], [94mLoss[0m : 2.58336
[1mStep[0m  [16/42], [94mLoss[0m : 2.48957
[1mStep[0m  [20/42], [94mLoss[0m : 2.48635
[1mStep[0m  [24/42], [94mLoss[0m : 2.59696
[1mStep[0m  [28/42], [94mLoss[0m : 2.20483
[1mStep[0m  [32/42], [94mLoss[0m : 2.68115
[1mStep[0m  [36/42], [94mLoss[0m : 2.54403
[1mStep[0m  [40/42], [94mLoss[0m : 2.55916

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.354, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51551
[1mStep[0m  [4/42], [94mLoss[0m : 2.20538
[1mStep[0m  [8/42], [94mLoss[0m : 2.43748
[1mStep[0m  [12/42], [94mLoss[0m : 2.20144
[1mStep[0m  [16/42], [94mLoss[0m : 2.54300
[1mStep[0m  [20/42], [94mLoss[0m : 2.49114
[1mStep[0m  [24/42], [94mLoss[0m : 2.41831
[1mStep[0m  [28/42], [94mLoss[0m : 2.59620
[1mStep[0m  [32/42], [94mLoss[0m : 2.36651
[1mStep[0m  [36/42], [94mLoss[0m : 2.41305
[1mStep[0m  [40/42], [94mLoss[0m : 2.40004

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.372, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39431
[1mStep[0m  [4/42], [94mLoss[0m : 2.45298
[1mStep[0m  [8/42], [94mLoss[0m : 2.50839
[1mStep[0m  [12/42], [94mLoss[0m : 2.73153
[1mStep[0m  [16/42], [94mLoss[0m : 2.27325
[1mStep[0m  [20/42], [94mLoss[0m : 2.64127
[1mStep[0m  [24/42], [94mLoss[0m : 2.49330
[1mStep[0m  [28/42], [94mLoss[0m : 2.49161
[1mStep[0m  [32/42], [94mLoss[0m : 2.49585
[1mStep[0m  [36/42], [94mLoss[0m : 2.52537
[1mStep[0m  [40/42], [94mLoss[0m : 2.47113

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.374, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54112
[1mStep[0m  [4/42], [94mLoss[0m : 2.43114
[1mStep[0m  [8/42], [94mLoss[0m : 2.63127
[1mStep[0m  [12/42], [94mLoss[0m : 2.48815
[1mStep[0m  [16/42], [94mLoss[0m : 2.47162
[1mStep[0m  [20/42], [94mLoss[0m : 2.34120
[1mStep[0m  [24/42], [94mLoss[0m : 2.46252
[1mStep[0m  [28/42], [94mLoss[0m : 2.34901
[1mStep[0m  [32/42], [94mLoss[0m : 2.65133
[1mStep[0m  [36/42], [94mLoss[0m : 2.52870
[1mStep[0m  [40/42], [94mLoss[0m : 2.59045

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.360, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56429
[1mStep[0m  [4/42], [94mLoss[0m : 2.47100
[1mStep[0m  [8/42], [94mLoss[0m : 2.26708
[1mStep[0m  [12/42], [94mLoss[0m : 2.54707
[1mStep[0m  [16/42], [94mLoss[0m : 2.66278
[1mStep[0m  [20/42], [94mLoss[0m : 2.36291
[1mStep[0m  [24/42], [94mLoss[0m : 2.24744
[1mStep[0m  [28/42], [94mLoss[0m : 2.74794
[1mStep[0m  [32/42], [94mLoss[0m : 2.50157
[1mStep[0m  [36/42], [94mLoss[0m : 2.54149
[1mStep[0m  [40/42], [94mLoss[0m : 2.64889

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.359, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40508
[1mStep[0m  [4/42], [94mLoss[0m : 2.34760
[1mStep[0m  [8/42], [94mLoss[0m : 2.41502
[1mStep[0m  [12/42], [94mLoss[0m : 2.40741
[1mStep[0m  [16/42], [94mLoss[0m : 2.53404
[1mStep[0m  [20/42], [94mLoss[0m : 2.36708
[1mStep[0m  [24/42], [94mLoss[0m : 2.52947
[1mStep[0m  [28/42], [94mLoss[0m : 2.41336
[1mStep[0m  [32/42], [94mLoss[0m : 2.63225
[1mStep[0m  [36/42], [94mLoss[0m : 2.41980
[1mStep[0m  [40/42], [94mLoss[0m : 2.37489

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.366, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50049
[1mStep[0m  [4/42], [94mLoss[0m : 2.51486
[1mStep[0m  [8/42], [94mLoss[0m : 2.42479
[1mStep[0m  [12/42], [94mLoss[0m : 2.58133
[1mStep[0m  [16/42], [94mLoss[0m : 2.39289
[1mStep[0m  [20/42], [94mLoss[0m : 2.41964
[1mStep[0m  [24/42], [94mLoss[0m : 2.55051
[1mStep[0m  [28/42], [94mLoss[0m : 2.63863
[1mStep[0m  [32/42], [94mLoss[0m : 2.31993
[1mStep[0m  [36/42], [94mLoss[0m : 2.54728
[1mStep[0m  [40/42], [94mLoss[0m : 2.46246

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.346, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.72435
[1mStep[0m  [4/42], [94mLoss[0m : 2.60003
[1mStep[0m  [8/42], [94mLoss[0m : 2.48461
[1mStep[0m  [12/42], [94mLoss[0m : 2.47344
[1mStep[0m  [16/42], [94mLoss[0m : 2.41563
[1mStep[0m  [20/42], [94mLoss[0m : 2.64710
[1mStep[0m  [24/42], [94mLoss[0m : 2.47753
[1mStep[0m  [28/42], [94mLoss[0m : 2.63594
[1mStep[0m  [32/42], [94mLoss[0m : 2.48951
[1mStep[0m  [36/42], [94mLoss[0m : 2.51119
[1mStep[0m  [40/42], [94mLoss[0m : 2.41873

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.363, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48097
[1mStep[0m  [4/42], [94mLoss[0m : 2.41145
[1mStep[0m  [8/42], [94mLoss[0m : 2.57308
[1mStep[0m  [12/42], [94mLoss[0m : 2.48950
[1mStep[0m  [16/42], [94mLoss[0m : 2.41904
[1mStep[0m  [20/42], [94mLoss[0m : 2.40995
[1mStep[0m  [24/42], [94mLoss[0m : 2.33258
[1mStep[0m  [28/42], [94mLoss[0m : 2.35043
[1mStep[0m  [32/42], [94mLoss[0m : 2.47958
[1mStep[0m  [36/42], [94mLoss[0m : 2.57816
[1mStep[0m  [40/42], [94mLoss[0m : 2.34982

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.385, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29964
[1mStep[0m  [4/42], [94mLoss[0m : 2.34016
[1mStep[0m  [8/42], [94mLoss[0m : 2.63342
[1mStep[0m  [12/42], [94mLoss[0m : 2.46312
[1mStep[0m  [16/42], [94mLoss[0m : 2.43293
[1mStep[0m  [20/42], [94mLoss[0m : 2.33278
[1mStep[0m  [24/42], [94mLoss[0m : 2.47432
[1mStep[0m  [28/42], [94mLoss[0m : 2.49294
[1mStep[0m  [32/42], [94mLoss[0m : 2.26482
[1mStep[0m  [36/42], [94mLoss[0m : 2.63299
[1mStep[0m  [40/42], [94mLoss[0m : 2.39997

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.337, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.63338
[1mStep[0m  [4/42], [94mLoss[0m : 2.30851
[1mStep[0m  [8/42], [94mLoss[0m : 2.28572
[1mStep[0m  [12/42], [94mLoss[0m : 2.41155
[1mStep[0m  [16/42], [94mLoss[0m : 2.46674
[1mStep[0m  [20/42], [94mLoss[0m : 2.26579
[1mStep[0m  [24/42], [94mLoss[0m : 2.28657
[1mStep[0m  [28/42], [94mLoss[0m : 2.34775
[1mStep[0m  [32/42], [94mLoss[0m : 2.56138
[1mStep[0m  [36/42], [94mLoss[0m : 2.53514
[1mStep[0m  [40/42], [94mLoss[0m : 2.44505

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.378, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36508
[1mStep[0m  [4/42], [94mLoss[0m : 2.41417
[1mStep[0m  [8/42], [94mLoss[0m : 2.55005
[1mStep[0m  [12/42], [94mLoss[0m : 2.49566
[1mStep[0m  [16/42], [94mLoss[0m : 2.26974
[1mStep[0m  [20/42], [94mLoss[0m : 2.54566
[1mStep[0m  [24/42], [94mLoss[0m : 2.34765
[1mStep[0m  [28/42], [94mLoss[0m : 2.58766
[1mStep[0m  [32/42], [94mLoss[0m : 2.23536
[1mStep[0m  [36/42], [94mLoss[0m : 2.38556
[1mStep[0m  [40/42], [94mLoss[0m : 2.58775

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.328, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58854
[1mStep[0m  [4/42], [94mLoss[0m : 2.54685
[1mStep[0m  [8/42], [94mLoss[0m : 2.34578
[1mStep[0m  [12/42], [94mLoss[0m : 2.41449
[1mStep[0m  [16/42], [94mLoss[0m : 2.59451
[1mStep[0m  [20/42], [94mLoss[0m : 2.44017
[1mStep[0m  [24/42], [94mLoss[0m : 2.38176
[1mStep[0m  [28/42], [94mLoss[0m : 2.33038
[1mStep[0m  [32/42], [94mLoss[0m : 2.45928
[1mStep[0m  [36/42], [94mLoss[0m : 2.29784
[1mStep[0m  [40/42], [94mLoss[0m : 2.57419

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.334, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51952
[1mStep[0m  [4/42], [94mLoss[0m : 2.30385
[1mStep[0m  [8/42], [94mLoss[0m : 2.28354
[1mStep[0m  [12/42], [94mLoss[0m : 2.33795
[1mStep[0m  [16/42], [94mLoss[0m : 2.52291
[1mStep[0m  [20/42], [94mLoss[0m : 2.58014
[1mStep[0m  [24/42], [94mLoss[0m : 2.44459
[1mStep[0m  [28/42], [94mLoss[0m : 2.53720
[1mStep[0m  [32/42], [94mLoss[0m : 2.51591
[1mStep[0m  [36/42], [94mLoss[0m : 2.42537
[1mStep[0m  [40/42], [94mLoss[0m : 2.34034

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.308, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46316
[1mStep[0m  [4/42], [94mLoss[0m : 2.27712
[1mStep[0m  [8/42], [94mLoss[0m : 2.49131
[1mStep[0m  [12/42], [94mLoss[0m : 2.39521
[1mStep[0m  [16/42], [94mLoss[0m : 2.20875
[1mStep[0m  [20/42], [94mLoss[0m : 2.59701
[1mStep[0m  [24/42], [94mLoss[0m : 2.44446
[1mStep[0m  [28/42], [94mLoss[0m : 2.57321
[1mStep[0m  [32/42], [94mLoss[0m : 2.49709
[1mStep[0m  [36/42], [94mLoss[0m : 2.51121
[1mStep[0m  [40/42], [94mLoss[0m : 2.28252

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.320, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38911
[1mStep[0m  [4/42], [94mLoss[0m : 2.30593
[1mStep[0m  [8/42], [94mLoss[0m : 2.32967
[1mStep[0m  [12/42], [94mLoss[0m : 2.36704
[1mStep[0m  [16/42], [94mLoss[0m : 2.20637
[1mStep[0m  [20/42], [94mLoss[0m : 2.37583
[1mStep[0m  [24/42], [94mLoss[0m : 2.45561
[1mStep[0m  [28/42], [94mLoss[0m : 2.57427
[1mStep[0m  [32/42], [94mLoss[0m : 2.58995
[1mStep[0m  [36/42], [94mLoss[0m : 2.48018
[1mStep[0m  [40/42], [94mLoss[0m : 2.38345

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.343, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45489
[1mStep[0m  [4/42], [94mLoss[0m : 2.46652
[1mStep[0m  [8/42], [94mLoss[0m : 2.52325
[1mStep[0m  [12/42], [94mLoss[0m : 2.63320
[1mStep[0m  [16/42], [94mLoss[0m : 2.45250
[1mStep[0m  [20/42], [94mLoss[0m : 2.44656
[1mStep[0m  [24/42], [94mLoss[0m : 2.48038
[1mStep[0m  [28/42], [94mLoss[0m : 2.31931
[1mStep[0m  [32/42], [94mLoss[0m : 2.55064
[1mStep[0m  [36/42], [94mLoss[0m : 2.28952
[1mStep[0m  [40/42], [94mLoss[0m : 2.28726

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.418, [92mTest[0m: 2.315, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28977
[1mStep[0m  [4/42], [94mLoss[0m : 2.46285
[1mStep[0m  [8/42], [94mLoss[0m : 2.33810
[1mStep[0m  [12/42], [94mLoss[0m : 2.37347
[1mStep[0m  [16/42], [94mLoss[0m : 2.29061
[1mStep[0m  [20/42], [94mLoss[0m : 2.27370
[1mStep[0m  [24/42], [94mLoss[0m : 2.64021
[1mStep[0m  [28/42], [94mLoss[0m : 2.63819
[1mStep[0m  [32/42], [94mLoss[0m : 2.26490
[1mStep[0m  [36/42], [94mLoss[0m : 2.34348
[1mStep[0m  [40/42], [94mLoss[0m : 2.20937

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.368, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47202
[1mStep[0m  [4/42], [94mLoss[0m : 2.34080
[1mStep[0m  [8/42], [94mLoss[0m : 2.33513
[1mStep[0m  [12/42], [94mLoss[0m : 2.37006
[1mStep[0m  [16/42], [94mLoss[0m : 2.45483
[1mStep[0m  [20/42], [94mLoss[0m : 2.34400
[1mStep[0m  [24/42], [94mLoss[0m : 2.51577
[1mStep[0m  [28/42], [94mLoss[0m : 2.53329
[1mStep[0m  [32/42], [94mLoss[0m : 2.57745
[1mStep[0m  [36/42], [94mLoss[0m : 2.23818
[1mStep[0m  [40/42], [94mLoss[0m : 2.60707

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.327, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.328
====================================

Phase 1 - Evaluation MAE:  2.3284365279333934
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 2.34228
[1mStep[0m  [4/42], [94mLoss[0m : 2.56928
[1mStep[0m  [8/42], [94mLoss[0m : 2.51584
[1mStep[0m  [12/42], [94mLoss[0m : 2.34153
[1mStep[0m  [16/42], [94mLoss[0m : 2.41005
[1mStep[0m  [20/42], [94mLoss[0m : 2.47710
[1mStep[0m  [24/42], [94mLoss[0m : 2.66020
[1mStep[0m  [28/42], [94mLoss[0m : 2.33953
[1mStep[0m  [32/42], [94mLoss[0m : 2.55772
[1mStep[0m  [36/42], [94mLoss[0m : 2.44176
[1mStep[0m  [40/42], [94mLoss[0m : 2.57939

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.336, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54990
[1mStep[0m  [4/42], [94mLoss[0m : 2.58440
[1mStep[0m  [8/42], [94mLoss[0m : 2.35259
[1mStep[0m  [12/42], [94mLoss[0m : 2.28539
[1mStep[0m  [16/42], [94mLoss[0m : 2.51528
[1mStep[0m  [20/42], [94mLoss[0m : 2.29694
[1mStep[0m  [24/42], [94mLoss[0m : 2.63625
[1mStep[0m  [28/42], [94mLoss[0m : 2.13925
[1mStep[0m  [32/42], [94mLoss[0m : 2.43088
[1mStep[0m  [36/42], [94mLoss[0m : 2.39970
[1mStep[0m  [40/42], [94mLoss[0m : 2.60040

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.481, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31150
[1mStep[0m  [4/42], [94mLoss[0m : 2.40642
[1mStep[0m  [8/42], [94mLoss[0m : 2.27294
[1mStep[0m  [12/42], [94mLoss[0m : 2.35534
[1mStep[0m  [16/42], [94mLoss[0m : 2.36430
[1mStep[0m  [20/42], [94mLoss[0m : 2.14528
[1mStep[0m  [24/42], [94mLoss[0m : 2.45377
[1mStep[0m  [28/42], [94mLoss[0m : 2.40989
[1mStep[0m  [32/42], [94mLoss[0m : 2.47344
[1mStep[0m  [36/42], [94mLoss[0m : 2.42113
[1mStep[0m  [40/42], [94mLoss[0m : 2.43986

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.386, [92mTest[0m: 2.417, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24980
[1mStep[0m  [4/42], [94mLoss[0m : 2.41802
[1mStep[0m  [8/42], [94mLoss[0m : 2.36342
[1mStep[0m  [12/42], [94mLoss[0m : 2.38276
[1mStep[0m  [16/42], [94mLoss[0m : 2.40368
[1mStep[0m  [20/42], [94mLoss[0m : 2.55467
[1mStep[0m  [24/42], [94mLoss[0m : 2.32771
[1mStep[0m  [28/42], [94mLoss[0m : 2.37793
[1mStep[0m  [32/42], [94mLoss[0m : 2.36318
[1mStep[0m  [36/42], [94mLoss[0m : 2.41992
[1mStep[0m  [40/42], [94mLoss[0m : 2.08612

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.345, [92mTest[0m: 2.510, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26134
[1mStep[0m  [4/42], [94mLoss[0m : 2.43684
[1mStep[0m  [8/42], [94mLoss[0m : 2.23688
[1mStep[0m  [12/42], [94mLoss[0m : 2.32438
[1mStep[0m  [16/42], [94mLoss[0m : 2.28936
[1mStep[0m  [20/42], [94mLoss[0m : 2.25326
[1mStep[0m  [24/42], [94mLoss[0m : 2.33664
[1mStep[0m  [28/42], [94mLoss[0m : 2.01692
[1mStep[0m  [32/42], [94mLoss[0m : 2.34708
[1mStep[0m  [36/42], [94mLoss[0m : 2.26050
[1mStep[0m  [40/42], [94mLoss[0m : 2.37769

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.276, [92mTest[0m: 2.430, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.00225
[1mStep[0m  [4/42], [94mLoss[0m : 2.12508
[1mStep[0m  [8/42], [94mLoss[0m : 2.00806
[1mStep[0m  [12/42], [94mLoss[0m : 2.13636
[1mStep[0m  [16/42], [94mLoss[0m : 2.26621
[1mStep[0m  [20/42], [94mLoss[0m : 2.38866
[1mStep[0m  [24/42], [94mLoss[0m : 2.26570
[1mStep[0m  [28/42], [94mLoss[0m : 2.22475
[1mStep[0m  [32/42], [94mLoss[0m : 2.13486
[1mStep[0m  [36/42], [94mLoss[0m : 2.42786
[1mStep[0m  [40/42], [94mLoss[0m : 2.21952

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.195, [92mTest[0m: 2.365, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.19597
[1mStep[0m  [4/42], [94mLoss[0m : 2.18756
[1mStep[0m  [8/42], [94mLoss[0m : 2.32292
[1mStep[0m  [12/42], [94mLoss[0m : 2.02088
[1mStep[0m  [16/42], [94mLoss[0m : 2.17179
[1mStep[0m  [20/42], [94mLoss[0m : 2.14062
[1mStep[0m  [24/42], [94mLoss[0m : 2.00766
[1mStep[0m  [28/42], [94mLoss[0m : 2.13398
[1mStep[0m  [32/42], [94mLoss[0m : 2.22560
[1mStep[0m  [36/42], [94mLoss[0m : 2.28035
[1mStep[0m  [40/42], [94mLoss[0m : 2.10837

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.176, [92mTest[0m: 2.433, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.22180
[1mStep[0m  [4/42], [94mLoss[0m : 2.15592
[1mStep[0m  [8/42], [94mLoss[0m : 2.04631
[1mStep[0m  [12/42], [94mLoss[0m : 1.97861
[1mStep[0m  [16/42], [94mLoss[0m : 2.06588
[1mStep[0m  [20/42], [94mLoss[0m : 2.18624
[1mStep[0m  [24/42], [94mLoss[0m : 2.02172
[1mStep[0m  [28/42], [94mLoss[0m : 1.85333
[1mStep[0m  [32/42], [94mLoss[0m : 2.23449
[1mStep[0m  [36/42], [94mLoss[0m : 2.15365
[1mStep[0m  [40/42], [94mLoss[0m : 2.29371

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.115, [92mTest[0m: 2.415, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34611
[1mStep[0m  [4/42], [94mLoss[0m : 2.03792
[1mStep[0m  [8/42], [94mLoss[0m : 2.04586
[1mStep[0m  [12/42], [94mLoss[0m : 2.22835
[1mStep[0m  [16/42], [94mLoss[0m : 2.15631
[1mStep[0m  [20/42], [94mLoss[0m : 2.05931
[1mStep[0m  [24/42], [94mLoss[0m : 2.00188
[1mStep[0m  [28/42], [94mLoss[0m : 2.02811
[1mStep[0m  [32/42], [94mLoss[0m : 2.00362
[1mStep[0m  [36/42], [94mLoss[0m : 2.07881
[1mStep[0m  [40/42], [94mLoss[0m : 2.07880

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.081, [92mTest[0m: 2.437, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.17292
[1mStep[0m  [4/42], [94mLoss[0m : 1.86408
[1mStep[0m  [8/42], [94mLoss[0m : 2.02610
[1mStep[0m  [12/42], [94mLoss[0m : 1.96748
[1mStep[0m  [16/42], [94mLoss[0m : 2.21435
[1mStep[0m  [20/42], [94mLoss[0m : 2.12059
[1mStep[0m  [24/42], [94mLoss[0m : 2.08953
[1mStep[0m  [28/42], [94mLoss[0m : 1.74679
[1mStep[0m  [32/42], [94mLoss[0m : 2.10249
[1mStep[0m  [36/42], [94mLoss[0m : 1.97783
[1mStep[0m  [40/42], [94mLoss[0m : 2.02345

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.028, [92mTest[0m: 2.400, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.73079
[1mStep[0m  [4/42], [94mLoss[0m : 1.65380
[1mStep[0m  [8/42], [94mLoss[0m : 1.94399
[1mStep[0m  [12/42], [94mLoss[0m : 2.03367
[1mStep[0m  [16/42], [94mLoss[0m : 1.91147
[1mStep[0m  [20/42], [94mLoss[0m : 2.04905
[1mStep[0m  [24/42], [94mLoss[0m : 2.12546
[1mStep[0m  [28/42], [94mLoss[0m : 2.08460
[1mStep[0m  [32/42], [94mLoss[0m : 2.07585
[1mStep[0m  [36/42], [94mLoss[0m : 1.95875
[1mStep[0m  [40/42], [94mLoss[0m : 2.19298

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.974, [92mTest[0m: 2.416, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.86958
[1mStep[0m  [4/42], [94mLoss[0m : 1.95269
[1mStep[0m  [8/42], [94mLoss[0m : 2.17226
[1mStep[0m  [12/42], [94mLoss[0m : 1.99763
[1mStep[0m  [16/42], [94mLoss[0m : 2.11549
[1mStep[0m  [20/42], [94mLoss[0m : 2.06870
[1mStep[0m  [24/42], [94mLoss[0m : 1.89502
[1mStep[0m  [28/42], [94mLoss[0m : 1.96661
[1mStep[0m  [32/42], [94mLoss[0m : 1.77357
[1mStep[0m  [36/42], [94mLoss[0m : 1.76116
[1mStep[0m  [40/42], [94mLoss[0m : 2.03395

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.911, [92mTest[0m: 2.417, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.93503
[1mStep[0m  [4/42], [94mLoss[0m : 1.78644
[1mStep[0m  [8/42], [94mLoss[0m : 1.81022
[1mStep[0m  [12/42], [94mLoss[0m : 1.91707
[1mStep[0m  [16/42], [94mLoss[0m : 1.73606
[1mStep[0m  [20/42], [94mLoss[0m : 1.72725
[1mStep[0m  [24/42], [94mLoss[0m : 2.10485
[1mStep[0m  [28/42], [94mLoss[0m : 1.83721
[1mStep[0m  [32/42], [94mLoss[0m : 1.87792
[1mStep[0m  [36/42], [94mLoss[0m : 1.83463
[1mStep[0m  [40/42], [94mLoss[0m : 2.15685

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.888, [92mTest[0m: 2.463, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.80492
[1mStep[0m  [4/42], [94mLoss[0m : 2.02316
[1mStep[0m  [8/42], [94mLoss[0m : 1.88423
[1mStep[0m  [12/42], [94mLoss[0m : 1.66161
[1mStep[0m  [16/42], [94mLoss[0m : 1.84734
[1mStep[0m  [20/42], [94mLoss[0m : 1.86965
[1mStep[0m  [24/42], [94mLoss[0m : 1.71675
[1mStep[0m  [28/42], [94mLoss[0m : 1.86468
[1mStep[0m  [32/42], [94mLoss[0m : 1.91570
[1mStep[0m  [36/42], [94mLoss[0m : 1.86215
[1mStep[0m  [40/42], [94mLoss[0m : 2.00232

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.868, [92mTest[0m: 2.465, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.82758
[1mStep[0m  [4/42], [94mLoss[0m : 1.91042
[1mStep[0m  [8/42], [94mLoss[0m : 1.84404
[1mStep[0m  [12/42], [94mLoss[0m : 1.85246
[1mStep[0m  [16/42], [94mLoss[0m : 1.86271
[1mStep[0m  [20/42], [94mLoss[0m : 1.79839
[1mStep[0m  [24/42], [94mLoss[0m : 1.94203
[1mStep[0m  [28/42], [94mLoss[0m : 1.95561
[1mStep[0m  [32/42], [94mLoss[0m : 1.74002
[1mStep[0m  [36/42], [94mLoss[0m : 1.65150
[1mStep[0m  [40/42], [94mLoss[0m : 1.89631

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.838, [92mTest[0m: 2.463, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.91885
[1mStep[0m  [4/42], [94mLoss[0m : 1.83853
[1mStep[0m  [8/42], [94mLoss[0m : 1.98244
[1mStep[0m  [12/42], [94mLoss[0m : 1.76177
[1mStep[0m  [16/42], [94mLoss[0m : 1.58958
[1mStep[0m  [20/42], [94mLoss[0m : 1.61052
[1mStep[0m  [24/42], [94mLoss[0m : 1.78853
[1mStep[0m  [28/42], [94mLoss[0m : 1.89005
[1mStep[0m  [32/42], [94mLoss[0m : 1.96496
[1mStep[0m  [36/42], [94mLoss[0m : 1.79599
[1mStep[0m  [40/42], [94mLoss[0m : 1.80486

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.794, [92mTest[0m: 2.454, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.67252
[1mStep[0m  [4/42], [94mLoss[0m : 1.73067
[1mStep[0m  [8/42], [94mLoss[0m : 1.89090
[1mStep[0m  [12/42], [94mLoss[0m : 1.85669
[1mStep[0m  [16/42], [94mLoss[0m : 1.68486
[1mStep[0m  [20/42], [94mLoss[0m : 1.70039
[1mStep[0m  [24/42], [94mLoss[0m : 1.81652
[1mStep[0m  [28/42], [94mLoss[0m : 1.73728
[1mStep[0m  [32/42], [94mLoss[0m : 1.81264
[1mStep[0m  [36/42], [94mLoss[0m : 1.78537
[1mStep[0m  [40/42], [94mLoss[0m : 1.73400

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.752, [92mTest[0m: 2.508, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.59739
[1mStep[0m  [4/42], [94mLoss[0m : 1.82129
[1mStep[0m  [8/42], [94mLoss[0m : 1.74390
[1mStep[0m  [12/42], [94mLoss[0m : 1.84127
[1mStep[0m  [16/42], [94mLoss[0m : 1.86697
[1mStep[0m  [20/42], [94mLoss[0m : 1.79983
[1mStep[0m  [24/42], [94mLoss[0m : 1.73074
[1mStep[0m  [28/42], [94mLoss[0m : 1.63669
[1mStep[0m  [32/42], [94mLoss[0m : 1.62448
[1mStep[0m  [36/42], [94mLoss[0m : 1.64114
[1mStep[0m  [40/42], [94mLoss[0m : 1.72312

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.736, [92mTest[0m: 2.464, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.73200
[1mStep[0m  [4/42], [94mLoss[0m : 1.70369
[1mStep[0m  [8/42], [94mLoss[0m : 1.73844
[1mStep[0m  [12/42], [94mLoss[0m : 1.63975
[1mStep[0m  [16/42], [94mLoss[0m : 1.84651
[1mStep[0m  [20/42], [94mLoss[0m : 1.87289
[1mStep[0m  [24/42], [94mLoss[0m : 1.65880
[1mStep[0m  [28/42], [94mLoss[0m : 1.75181
[1mStep[0m  [32/42], [94mLoss[0m : 1.81958
[1mStep[0m  [36/42], [94mLoss[0m : 1.54004
[1mStep[0m  [40/42], [94mLoss[0m : 1.69317

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.709, [92mTest[0m: 2.474, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.74844
[1mStep[0m  [4/42], [94mLoss[0m : 1.69872
[1mStep[0m  [8/42], [94mLoss[0m : 1.67420
[1mStep[0m  [12/42], [94mLoss[0m : 1.60012
[1mStep[0m  [16/42], [94mLoss[0m : 1.55709
[1mStep[0m  [20/42], [94mLoss[0m : 1.64984
[1mStep[0m  [24/42], [94mLoss[0m : 1.67102
[1mStep[0m  [28/42], [94mLoss[0m : 1.64388
[1mStep[0m  [32/42], [94mLoss[0m : 1.54225
[1mStep[0m  [36/42], [94mLoss[0m : 1.77875
[1mStep[0m  [40/42], [94mLoss[0m : 1.65475

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.683, [92mTest[0m: 2.469, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.76442
[1mStep[0m  [4/42], [94mLoss[0m : 1.80693
[1mStep[0m  [8/42], [94mLoss[0m : 1.47690
[1mStep[0m  [12/42], [94mLoss[0m : 1.59730
[1mStep[0m  [16/42], [94mLoss[0m : 1.64656
[1mStep[0m  [20/42], [94mLoss[0m : 1.61658
[1mStep[0m  [24/42], [94mLoss[0m : 1.57923
[1mStep[0m  [28/42], [94mLoss[0m : 1.64322
[1mStep[0m  [32/42], [94mLoss[0m : 1.61777
[1mStep[0m  [36/42], [94mLoss[0m : 1.53630
[1mStep[0m  [40/42], [94mLoss[0m : 1.72490

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.647, [92mTest[0m: 2.464, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.52974
[1mStep[0m  [4/42], [94mLoss[0m : 1.57180
[1mStep[0m  [8/42], [94mLoss[0m : 1.67192
[1mStep[0m  [12/42], [94mLoss[0m : 1.48396
[1mStep[0m  [16/42], [94mLoss[0m : 1.62835
[1mStep[0m  [20/42], [94mLoss[0m : 1.50628
[1mStep[0m  [24/42], [94mLoss[0m : 1.56485
[1mStep[0m  [28/42], [94mLoss[0m : 1.64095
[1mStep[0m  [32/42], [94mLoss[0m : 1.57108
[1mStep[0m  [36/42], [94mLoss[0m : 1.64076
[1mStep[0m  [40/42], [94mLoss[0m : 1.56732

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.601, [92mTest[0m: 2.482, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.69045
[1mStep[0m  [4/42], [94mLoss[0m : 1.62099
[1mStep[0m  [8/42], [94mLoss[0m : 1.69605
[1mStep[0m  [12/42], [94mLoss[0m : 1.70905
[1mStep[0m  [16/42], [94mLoss[0m : 1.62301
[1mStep[0m  [20/42], [94mLoss[0m : 1.67992
[1mStep[0m  [24/42], [94mLoss[0m : 1.61356
[1mStep[0m  [28/42], [94mLoss[0m : 1.45512
[1mStep[0m  [32/42], [94mLoss[0m : 1.59534
[1mStep[0m  [36/42], [94mLoss[0m : 1.69435
[1mStep[0m  [40/42], [94mLoss[0m : 1.70954

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.586, [92mTest[0m: 2.485, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.60933
[1mStep[0m  [4/42], [94mLoss[0m : 1.62753
[1mStep[0m  [8/42], [94mLoss[0m : 1.49672
[1mStep[0m  [12/42], [94mLoss[0m : 1.66017
[1mStep[0m  [16/42], [94mLoss[0m : 1.61297
[1mStep[0m  [20/42], [94mLoss[0m : 1.62848
[1mStep[0m  [24/42], [94mLoss[0m : 1.58851
[1mStep[0m  [28/42], [94mLoss[0m : 1.64167
[1mStep[0m  [32/42], [94mLoss[0m : 1.46792
[1mStep[0m  [36/42], [94mLoss[0m : 1.63087
[1mStep[0m  [40/42], [94mLoss[0m : 1.50717

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.563, [92mTest[0m: 2.480, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.55117
[1mStep[0m  [4/42], [94mLoss[0m : 1.48319
[1mStep[0m  [8/42], [94mLoss[0m : 1.62826
[1mStep[0m  [12/42], [94mLoss[0m : 1.54163
[1mStep[0m  [16/42], [94mLoss[0m : 1.44651
[1mStep[0m  [20/42], [94mLoss[0m : 1.71347
[1mStep[0m  [24/42], [94mLoss[0m : 1.60617
[1mStep[0m  [28/42], [94mLoss[0m : 1.52130
[1mStep[0m  [32/42], [94mLoss[0m : 1.55374
[1mStep[0m  [36/42], [94mLoss[0m : 1.46568
[1mStep[0m  [40/42], [94mLoss[0m : 1.51142

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.540, [92mTest[0m: 2.500, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.43479
[1mStep[0m  [4/42], [94mLoss[0m : 1.57182
[1mStep[0m  [8/42], [94mLoss[0m : 1.39823
[1mStep[0m  [12/42], [94mLoss[0m : 1.43721
[1mStep[0m  [16/42], [94mLoss[0m : 1.52443
[1mStep[0m  [20/42], [94mLoss[0m : 1.68173
[1mStep[0m  [24/42], [94mLoss[0m : 1.55413
[1mStep[0m  [28/42], [94mLoss[0m : 1.49257
[1mStep[0m  [32/42], [94mLoss[0m : 1.57412
[1mStep[0m  [36/42], [94mLoss[0m : 1.46022
[1mStep[0m  [40/42], [94mLoss[0m : 1.52169

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.517, [92mTest[0m: 2.522, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.38911
[1mStep[0m  [4/42], [94mLoss[0m : 1.54717
[1mStep[0m  [8/42], [94mLoss[0m : 1.48865
[1mStep[0m  [12/42], [94mLoss[0m : 1.54482
[1mStep[0m  [16/42], [94mLoss[0m : 1.45128
[1mStep[0m  [20/42], [94mLoss[0m : 1.42086
[1mStep[0m  [24/42], [94mLoss[0m : 1.49210
[1mStep[0m  [28/42], [94mLoss[0m : 1.60234
[1mStep[0m  [32/42], [94mLoss[0m : 1.65999
[1mStep[0m  [36/42], [94mLoss[0m : 1.44314
[1mStep[0m  [40/42], [94mLoss[0m : 1.56657

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.506, [92mTest[0m: 2.521, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.33192
[1mStep[0m  [4/42], [94mLoss[0m : 1.49967
[1mStep[0m  [8/42], [94mLoss[0m : 1.60590
[1mStep[0m  [12/42], [94mLoss[0m : 1.64165
[1mStep[0m  [16/42], [94mLoss[0m : 1.40346
[1mStep[0m  [20/42], [94mLoss[0m : 1.45069
[1mStep[0m  [24/42], [94mLoss[0m : 1.59212
[1mStep[0m  [28/42], [94mLoss[0m : 1.70707
[1mStep[0m  [32/42], [94mLoss[0m : 1.39078
[1mStep[0m  [36/42], [94mLoss[0m : 1.38640
[1mStep[0m  [40/42], [94mLoss[0m : 1.42304

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.481, [92mTest[0m: 2.459, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.45210
[1mStep[0m  [4/42], [94mLoss[0m : 1.32791
[1mStep[0m  [8/42], [94mLoss[0m : 1.48859
[1mStep[0m  [12/42], [94mLoss[0m : 1.52248
[1mStep[0m  [16/42], [94mLoss[0m : 1.50365
[1mStep[0m  [20/42], [94mLoss[0m : 1.54778
[1mStep[0m  [24/42], [94mLoss[0m : 1.60965
[1mStep[0m  [28/42], [94mLoss[0m : 1.39128
[1mStep[0m  [32/42], [94mLoss[0m : 1.46675
[1mStep[0m  [36/42], [94mLoss[0m : 1.42306
[1mStep[0m  [40/42], [94mLoss[0m : 1.52973

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.471, [92mTest[0m: 2.495, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.52483
[1mStep[0m  [4/42], [94mLoss[0m : 1.29995
[1mStep[0m  [8/42], [94mLoss[0m : 1.66267
[1mStep[0m  [12/42], [94mLoss[0m : 1.57482
[1mStep[0m  [16/42], [94mLoss[0m : 1.42496
[1mStep[0m  [20/42], [94mLoss[0m : 1.40004
[1mStep[0m  [24/42], [94mLoss[0m : 1.32248
[1mStep[0m  [28/42], [94mLoss[0m : 1.47990
[1mStep[0m  [32/42], [94mLoss[0m : 1.51399
[1mStep[0m  [36/42], [94mLoss[0m : 1.58699
[1mStep[0m  [40/42], [94mLoss[0m : 1.51001

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.463, [92mTest[0m: 2.472, [96mlr[0m: 0.006772500000000002
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 29 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.479
====================================

Phase 2 - Evaluation MAE:  2.478714500154768
MAE score P1      2.328437
MAE score P2      2.478715
loss               1.46348
learning_rate     0.007525
batch_size             256
hidden_sizes         [100]
epochs                  30
activation            relu
optimizer              sgd
early stopping        True
dropout                0.3
momentum               0.5
weight_decay        0.0001
Name: 13, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 11.03334
[1mStep[0m  [8/84], [94mLoss[0m : 6.70679
[1mStep[0m  [16/84], [94mLoss[0m : 3.72576
[1mStep[0m  [24/84], [94mLoss[0m : 2.29114
[1mStep[0m  [32/84], [94mLoss[0m : 2.60637
[1mStep[0m  [40/84], [94mLoss[0m : 2.85608
[1mStep[0m  [48/84], [94mLoss[0m : 2.63712
[1mStep[0m  [56/84], [94mLoss[0m : 2.35753
[1mStep[0m  [64/84], [94mLoss[0m : 2.63037
[1mStep[0m  [72/84], [94mLoss[0m : 2.50419
[1mStep[0m  [80/84], [94mLoss[0m : 2.39571

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.523, [92mTest[0m: 11.002, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.85951
[1mStep[0m  [8/84], [94mLoss[0m : 2.70546
[1mStep[0m  [16/84], [94mLoss[0m : 2.34531
[1mStep[0m  [24/84], [94mLoss[0m : 2.20319
[1mStep[0m  [32/84], [94mLoss[0m : 2.54510
[1mStep[0m  [40/84], [94mLoss[0m : 2.56712
[1mStep[0m  [48/84], [94mLoss[0m : 2.62466
[1mStep[0m  [56/84], [94mLoss[0m : 2.50422
[1mStep[0m  [64/84], [94mLoss[0m : 2.51558
[1mStep[0m  [72/84], [94mLoss[0m : 2.66171
[1mStep[0m  [80/84], [94mLoss[0m : 2.45817

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.462, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30895
[1mStep[0m  [8/84], [94mLoss[0m : 2.42331
[1mStep[0m  [16/84], [94mLoss[0m : 2.48992
[1mStep[0m  [24/84], [94mLoss[0m : 2.48825
[1mStep[0m  [32/84], [94mLoss[0m : 2.13294
[1mStep[0m  [40/84], [94mLoss[0m : 2.21744
[1mStep[0m  [48/84], [94mLoss[0m : 2.50730
[1mStep[0m  [56/84], [94mLoss[0m : 2.39984
[1mStep[0m  [64/84], [94mLoss[0m : 2.37799
[1mStep[0m  [72/84], [94mLoss[0m : 2.36030
[1mStep[0m  [80/84], [94mLoss[0m : 2.70005

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.407, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.56808
[1mStep[0m  [8/84], [94mLoss[0m : 2.32355
[1mStep[0m  [16/84], [94mLoss[0m : 2.75360
[1mStep[0m  [24/84], [94mLoss[0m : 2.17135
[1mStep[0m  [32/84], [94mLoss[0m : 2.46454
[1mStep[0m  [40/84], [94mLoss[0m : 2.58518
[1mStep[0m  [48/84], [94mLoss[0m : 2.66806
[1mStep[0m  [56/84], [94mLoss[0m : 2.68086
[1mStep[0m  [64/84], [94mLoss[0m : 2.42298
[1mStep[0m  [72/84], [94mLoss[0m : 2.74693
[1mStep[0m  [80/84], [94mLoss[0m : 2.42906

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.405, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27425
[1mStep[0m  [8/84], [94mLoss[0m : 2.49848
[1mStep[0m  [16/84], [94mLoss[0m : 2.35149
[1mStep[0m  [24/84], [94mLoss[0m : 2.72470
[1mStep[0m  [32/84], [94mLoss[0m : 2.54493
[1mStep[0m  [40/84], [94mLoss[0m : 2.17411
[1mStep[0m  [48/84], [94mLoss[0m : 2.62886
[1mStep[0m  [56/84], [94mLoss[0m : 2.01618
[1mStep[0m  [64/84], [94mLoss[0m : 2.34734
[1mStep[0m  [72/84], [94mLoss[0m : 2.46327
[1mStep[0m  [80/84], [94mLoss[0m : 2.24240

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.372, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45834
[1mStep[0m  [8/84], [94mLoss[0m : 2.21119
[1mStep[0m  [16/84], [94mLoss[0m : 2.18699
[1mStep[0m  [24/84], [94mLoss[0m : 2.55663
[1mStep[0m  [32/84], [94mLoss[0m : 2.57176
[1mStep[0m  [40/84], [94mLoss[0m : 2.42283
[1mStep[0m  [48/84], [94mLoss[0m : 2.73801
[1mStep[0m  [56/84], [94mLoss[0m : 2.56119
[1mStep[0m  [64/84], [94mLoss[0m : 2.61144
[1mStep[0m  [72/84], [94mLoss[0m : 2.43560
[1mStep[0m  [80/84], [94mLoss[0m : 2.66904

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.349, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52165
[1mStep[0m  [8/84], [94mLoss[0m : 2.32019
[1mStep[0m  [16/84], [94mLoss[0m : 2.47460
[1mStep[0m  [24/84], [94mLoss[0m : 2.51094
[1mStep[0m  [32/84], [94mLoss[0m : 2.64128
[1mStep[0m  [40/84], [94mLoss[0m : 2.72020
[1mStep[0m  [48/84], [94mLoss[0m : 2.14804
[1mStep[0m  [56/84], [94mLoss[0m : 2.58447
[1mStep[0m  [64/84], [94mLoss[0m : 2.46197
[1mStep[0m  [72/84], [94mLoss[0m : 2.32211
[1mStep[0m  [80/84], [94mLoss[0m : 2.56520

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.354, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37557
[1mStep[0m  [8/84], [94mLoss[0m : 2.29956
[1mStep[0m  [16/84], [94mLoss[0m : 2.40990
[1mStep[0m  [24/84], [94mLoss[0m : 2.20696
[1mStep[0m  [32/84], [94mLoss[0m : 2.45540
[1mStep[0m  [40/84], [94mLoss[0m : 2.40636
[1mStep[0m  [48/84], [94mLoss[0m : 2.29195
[1mStep[0m  [56/84], [94mLoss[0m : 2.49651
[1mStep[0m  [64/84], [94mLoss[0m : 2.36647
[1mStep[0m  [72/84], [94mLoss[0m : 2.43792
[1mStep[0m  [80/84], [94mLoss[0m : 2.83893

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.339, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37731
[1mStep[0m  [8/84], [94mLoss[0m : 2.35990
[1mStep[0m  [16/84], [94mLoss[0m : 2.44027
[1mStep[0m  [24/84], [94mLoss[0m : 2.69491
[1mStep[0m  [32/84], [94mLoss[0m : 2.88508
[1mStep[0m  [40/84], [94mLoss[0m : 2.37775
[1mStep[0m  [48/84], [94mLoss[0m : 2.50106
[1mStep[0m  [56/84], [94mLoss[0m : 2.38810
[1mStep[0m  [64/84], [94mLoss[0m : 2.25645
[1mStep[0m  [72/84], [94mLoss[0m : 2.39383
[1mStep[0m  [80/84], [94mLoss[0m : 2.36515

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.341, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43628
[1mStep[0m  [8/84], [94mLoss[0m : 2.15088
[1mStep[0m  [16/84], [94mLoss[0m : 2.34366
[1mStep[0m  [24/84], [94mLoss[0m : 2.56809
[1mStep[0m  [32/84], [94mLoss[0m : 2.13468
[1mStep[0m  [40/84], [94mLoss[0m : 2.49413
[1mStep[0m  [48/84], [94mLoss[0m : 2.32265
[1mStep[0m  [56/84], [94mLoss[0m : 2.46648
[1mStep[0m  [64/84], [94mLoss[0m : 2.34679
[1mStep[0m  [72/84], [94mLoss[0m : 2.44974
[1mStep[0m  [80/84], [94mLoss[0m : 2.22548

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.342, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33534
[1mStep[0m  [8/84], [94mLoss[0m : 2.53652
[1mStep[0m  [16/84], [94mLoss[0m : 2.39299
[1mStep[0m  [24/84], [94mLoss[0m : 2.27429
[1mStep[0m  [32/84], [94mLoss[0m : 2.50017
[1mStep[0m  [40/84], [94mLoss[0m : 2.49171
[1mStep[0m  [48/84], [94mLoss[0m : 2.35105
[1mStep[0m  [56/84], [94mLoss[0m : 2.20599
[1mStep[0m  [64/84], [94mLoss[0m : 2.21892
[1mStep[0m  [72/84], [94mLoss[0m : 2.44901
[1mStep[0m  [80/84], [94mLoss[0m : 2.34591

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.336, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.65570
[1mStep[0m  [8/84], [94mLoss[0m : 2.26740
[1mStep[0m  [16/84], [94mLoss[0m : 2.47073
[1mStep[0m  [24/84], [94mLoss[0m : 2.24894
[1mStep[0m  [32/84], [94mLoss[0m : 2.41656
[1mStep[0m  [40/84], [94mLoss[0m : 2.47063
[1mStep[0m  [48/84], [94mLoss[0m : 2.49840
[1mStep[0m  [56/84], [94mLoss[0m : 2.47823
[1mStep[0m  [64/84], [94mLoss[0m : 2.27031
[1mStep[0m  [72/84], [94mLoss[0m : 2.42074
[1mStep[0m  [80/84], [94mLoss[0m : 2.57008

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.418, [92mTest[0m: 2.331, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47377
[1mStep[0m  [8/84], [94mLoss[0m : 2.59011
[1mStep[0m  [16/84], [94mLoss[0m : 2.33900
[1mStep[0m  [24/84], [94mLoss[0m : 2.27661
[1mStep[0m  [32/84], [94mLoss[0m : 2.28738
[1mStep[0m  [40/84], [94mLoss[0m : 2.36294
[1mStep[0m  [48/84], [94mLoss[0m : 2.14715
[1mStep[0m  [56/84], [94mLoss[0m : 2.02569
[1mStep[0m  [64/84], [94mLoss[0m : 2.32583
[1mStep[0m  [72/84], [94mLoss[0m : 2.27800
[1mStep[0m  [80/84], [94mLoss[0m : 2.17267

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.325, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59003
[1mStep[0m  [8/84], [94mLoss[0m : 2.51024
[1mStep[0m  [16/84], [94mLoss[0m : 2.25896
[1mStep[0m  [24/84], [94mLoss[0m : 2.34653
[1mStep[0m  [32/84], [94mLoss[0m : 2.32040
[1mStep[0m  [40/84], [94mLoss[0m : 2.20481
[1mStep[0m  [48/84], [94mLoss[0m : 2.32598
[1mStep[0m  [56/84], [94mLoss[0m : 2.49070
[1mStep[0m  [64/84], [94mLoss[0m : 2.52729
[1mStep[0m  [72/84], [94mLoss[0m : 2.61357
[1mStep[0m  [80/84], [94mLoss[0m : 2.26384

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.333, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37953
[1mStep[0m  [8/84], [94mLoss[0m : 2.92433
[1mStep[0m  [16/84], [94mLoss[0m : 2.29521
[1mStep[0m  [24/84], [94mLoss[0m : 2.36058
[1mStep[0m  [32/84], [94mLoss[0m : 2.28627
[1mStep[0m  [40/84], [94mLoss[0m : 2.16687
[1mStep[0m  [48/84], [94mLoss[0m : 2.55951
[1mStep[0m  [56/84], [94mLoss[0m : 2.32383
[1mStep[0m  [64/84], [94mLoss[0m : 2.41153
[1mStep[0m  [72/84], [94mLoss[0m : 2.34980
[1mStep[0m  [80/84], [94mLoss[0m : 2.15708

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.331, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.57123
[1mStep[0m  [8/84], [94mLoss[0m : 2.45039
[1mStep[0m  [16/84], [94mLoss[0m : 2.66347
[1mStep[0m  [24/84], [94mLoss[0m : 2.52228
[1mStep[0m  [32/84], [94mLoss[0m : 2.40259
[1mStep[0m  [40/84], [94mLoss[0m : 1.98819
[1mStep[0m  [48/84], [94mLoss[0m : 2.40442
[1mStep[0m  [56/84], [94mLoss[0m : 2.65918
[1mStep[0m  [64/84], [94mLoss[0m : 2.12917
[1mStep[0m  [72/84], [94mLoss[0m : 2.23118
[1mStep[0m  [80/84], [94mLoss[0m : 2.22149

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.332, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31730
[1mStep[0m  [8/84], [94mLoss[0m : 2.79708
[1mStep[0m  [16/84], [94mLoss[0m : 2.31236
[1mStep[0m  [24/84], [94mLoss[0m : 2.35967
[1mStep[0m  [32/84], [94mLoss[0m : 2.42300
[1mStep[0m  [40/84], [94mLoss[0m : 2.64212
[1mStep[0m  [48/84], [94mLoss[0m : 2.49071
[1mStep[0m  [56/84], [94mLoss[0m : 2.43401
[1mStep[0m  [64/84], [94mLoss[0m : 2.63094
[1mStep[0m  [72/84], [94mLoss[0m : 2.55988
[1mStep[0m  [80/84], [94mLoss[0m : 2.39229

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.339, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37584
[1mStep[0m  [8/84], [94mLoss[0m : 2.37870
[1mStep[0m  [16/84], [94mLoss[0m : 2.46397
[1mStep[0m  [24/84], [94mLoss[0m : 2.47995
[1mStep[0m  [32/84], [94mLoss[0m : 2.66358
[1mStep[0m  [40/84], [94mLoss[0m : 2.72896
[1mStep[0m  [48/84], [94mLoss[0m : 2.26526
[1mStep[0m  [56/84], [94mLoss[0m : 2.74161
[1mStep[0m  [64/84], [94mLoss[0m : 2.13009
[1mStep[0m  [72/84], [94mLoss[0m : 2.21568
[1mStep[0m  [80/84], [94mLoss[0m : 2.34587

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.330, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.76713
[1mStep[0m  [8/84], [94mLoss[0m : 2.24180
[1mStep[0m  [16/84], [94mLoss[0m : 2.53177
[1mStep[0m  [24/84], [94mLoss[0m : 2.20741
[1mStep[0m  [32/84], [94mLoss[0m : 2.46571
[1mStep[0m  [40/84], [94mLoss[0m : 2.21182
[1mStep[0m  [48/84], [94mLoss[0m : 2.32122
[1mStep[0m  [56/84], [94mLoss[0m : 2.50149
[1mStep[0m  [64/84], [94mLoss[0m : 2.46046
[1mStep[0m  [72/84], [94mLoss[0m : 2.38357
[1mStep[0m  [80/84], [94mLoss[0m : 2.09724

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.327, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41654
[1mStep[0m  [8/84], [94mLoss[0m : 2.16391
[1mStep[0m  [16/84], [94mLoss[0m : 2.63610
[1mStep[0m  [24/84], [94mLoss[0m : 2.11976
[1mStep[0m  [32/84], [94mLoss[0m : 2.21186
[1mStep[0m  [40/84], [94mLoss[0m : 2.43548
[1mStep[0m  [48/84], [94mLoss[0m : 2.46230
[1mStep[0m  [56/84], [94mLoss[0m : 2.17416
[1mStep[0m  [64/84], [94mLoss[0m : 2.38052
[1mStep[0m  [72/84], [94mLoss[0m : 2.22297
[1mStep[0m  [80/84], [94mLoss[0m : 2.61423

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.322, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.09046
[1mStep[0m  [8/84], [94mLoss[0m : 2.55473
[1mStep[0m  [16/84], [94mLoss[0m : 2.71587
[1mStep[0m  [24/84], [94mLoss[0m : 2.13711
[1mStep[0m  [32/84], [94mLoss[0m : 2.57792
[1mStep[0m  [40/84], [94mLoss[0m : 2.30296
[1mStep[0m  [48/84], [94mLoss[0m : 2.20857
[1mStep[0m  [56/84], [94mLoss[0m : 2.20034
[1mStep[0m  [64/84], [94mLoss[0m : 2.42712
[1mStep[0m  [72/84], [94mLoss[0m : 2.28813
[1mStep[0m  [80/84], [94mLoss[0m : 2.17122

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.328, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26814
[1mStep[0m  [8/84], [94mLoss[0m : 2.33363
[1mStep[0m  [16/84], [94mLoss[0m : 2.64899
[1mStep[0m  [24/84], [94mLoss[0m : 2.47035
[1mStep[0m  [32/84], [94mLoss[0m : 2.49874
[1mStep[0m  [40/84], [94mLoss[0m : 2.29229
[1mStep[0m  [48/84], [94mLoss[0m : 2.61905
[1mStep[0m  [56/84], [94mLoss[0m : 2.41894
[1mStep[0m  [64/84], [94mLoss[0m : 2.47989
[1mStep[0m  [72/84], [94mLoss[0m : 2.35508
[1mStep[0m  [80/84], [94mLoss[0m : 2.34028

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.332, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43085
[1mStep[0m  [8/84], [94mLoss[0m : 2.01514
[1mStep[0m  [16/84], [94mLoss[0m : 2.70256
[1mStep[0m  [24/84], [94mLoss[0m : 2.54152
[1mStep[0m  [32/84], [94mLoss[0m : 2.50191
[1mStep[0m  [40/84], [94mLoss[0m : 2.50997
[1mStep[0m  [48/84], [94mLoss[0m : 2.38065
[1mStep[0m  [56/84], [94mLoss[0m : 2.55541
[1mStep[0m  [64/84], [94mLoss[0m : 2.31886
[1mStep[0m  [72/84], [94mLoss[0m : 2.66278
[1mStep[0m  [80/84], [94mLoss[0m : 2.39028

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.325, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.03100
[1mStep[0m  [8/84], [94mLoss[0m : 2.34357
[1mStep[0m  [16/84], [94mLoss[0m : 2.37313
[1mStep[0m  [24/84], [94mLoss[0m : 2.43378
[1mStep[0m  [32/84], [94mLoss[0m : 2.37724
[1mStep[0m  [40/84], [94mLoss[0m : 2.53881
[1mStep[0m  [48/84], [94mLoss[0m : 2.60189
[1mStep[0m  [56/84], [94mLoss[0m : 2.20209
[1mStep[0m  [64/84], [94mLoss[0m : 2.21225
[1mStep[0m  [72/84], [94mLoss[0m : 2.19866
[1mStep[0m  [80/84], [94mLoss[0m : 2.46596

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.325, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53243
[1mStep[0m  [8/84], [94mLoss[0m : 2.02477
[1mStep[0m  [16/84], [94mLoss[0m : 2.26288
[1mStep[0m  [24/84], [94mLoss[0m : 2.53297
[1mStep[0m  [32/84], [94mLoss[0m : 2.23039
[1mStep[0m  [40/84], [94mLoss[0m : 2.41944
[1mStep[0m  [48/84], [94mLoss[0m : 2.36227
[1mStep[0m  [56/84], [94mLoss[0m : 2.18738
[1mStep[0m  [64/84], [94mLoss[0m : 2.22001
[1mStep[0m  [72/84], [94mLoss[0m : 2.36322
[1mStep[0m  [80/84], [94mLoss[0m : 2.22535

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.333, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48538
[1mStep[0m  [8/84], [94mLoss[0m : 2.30720
[1mStep[0m  [16/84], [94mLoss[0m : 2.08291
[1mStep[0m  [24/84], [94mLoss[0m : 2.20997
[1mStep[0m  [32/84], [94mLoss[0m : 2.32702
[1mStep[0m  [40/84], [94mLoss[0m : 2.24250
[1mStep[0m  [48/84], [94mLoss[0m : 2.59111
[1mStep[0m  [56/84], [94mLoss[0m : 2.40894
[1mStep[0m  [64/84], [94mLoss[0m : 2.52777
[1mStep[0m  [72/84], [94mLoss[0m : 2.62766
[1mStep[0m  [80/84], [94mLoss[0m : 2.45577

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.322, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.28249
[1mStep[0m  [8/84], [94mLoss[0m : 2.21040
[1mStep[0m  [16/84], [94mLoss[0m : 2.32509
[1mStep[0m  [24/84], [94mLoss[0m : 2.32900
[1mStep[0m  [32/84], [94mLoss[0m : 2.29107
[1mStep[0m  [40/84], [94mLoss[0m : 2.58226
[1mStep[0m  [48/84], [94mLoss[0m : 2.26324
[1mStep[0m  [56/84], [94mLoss[0m : 2.45299
[1mStep[0m  [64/84], [94mLoss[0m : 2.36023
[1mStep[0m  [72/84], [94mLoss[0m : 2.34065
[1mStep[0m  [80/84], [94mLoss[0m : 2.42335

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.403, [92mTest[0m: 2.330, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37742
[1mStep[0m  [8/84], [94mLoss[0m : 2.59864
[1mStep[0m  [16/84], [94mLoss[0m : 2.54470
[1mStep[0m  [24/84], [94mLoss[0m : 2.56425
[1mStep[0m  [32/84], [94mLoss[0m : 2.24240
[1mStep[0m  [40/84], [94mLoss[0m : 2.54037
[1mStep[0m  [48/84], [94mLoss[0m : 2.37189
[1mStep[0m  [56/84], [94mLoss[0m : 2.21057
[1mStep[0m  [64/84], [94mLoss[0m : 2.37168
[1mStep[0m  [72/84], [94mLoss[0m : 2.11322
[1mStep[0m  [80/84], [94mLoss[0m : 2.40349

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.333, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33266
[1mStep[0m  [8/84], [94mLoss[0m : 2.24034
[1mStep[0m  [16/84], [94mLoss[0m : 2.49824
[1mStep[0m  [24/84], [94mLoss[0m : 2.21676
[1mStep[0m  [32/84], [94mLoss[0m : 2.54348
[1mStep[0m  [40/84], [94mLoss[0m : 2.44145
[1mStep[0m  [48/84], [94mLoss[0m : 2.03348
[1mStep[0m  [56/84], [94mLoss[0m : 2.47293
[1mStep[0m  [64/84], [94mLoss[0m : 2.34037
[1mStep[0m  [72/84], [94mLoss[0m : 2.43515
[1mStep[0m  [80/84], [94mLoss[0m : 2.69539

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.331, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55418
[1mStep[0m  [8/84], [94mLoss[0m : 2.45863
[1mStep[0m  [16/84], [94mLoss[0m : 2.20308
[1mStep[0m  [24/84], [94mLoss[0m : 2.72133
[1mStep[0m  [32/84], [94mLoss[0m : 2.41950
[1mStep[0m  [40/84], [94mLoss[0m : 2.58100
[1mStep[0m  [48/84], [94mLoss[0m : 2.58811
[1mStep[0m  [56/84], [94mLoss[0m : 2.55644
[1mStep[0m  [64/84], [94mLoss[0m : 2.62250
[1mStep[0m  [72/84], [94mLoss[0m : 2.34066
[1mStep[0m  [80/84], [94mLoss[0m : 2.57338

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.325, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.326
====================================

Phase 1 - Evaluation MAE:  2.3262873717716763
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 2.50173
[1mStep[0m  [8/84], [94mLoss[0m : 2.44025
[1mStep[0m  [16/84], [94mLoss[0m : 2.64522
[1mStep[0m  [24/84], [94mLoss[0m : 2.42149
[1mStep[0m  [32/84], [94mLoss[0m : 2.33888
[1mStep[0m  [40/84], [94mLoss[0m : 2.26658
[1mStep[0m  [48/84], [94mLoss[0m : 2.32321
[1mStep[0m  [56/84], [94mLoss[0m : 2.66973
[1mStep[0m  [64/84], [94mLoss[0m : 2.19848
[1mStep[0m  [72/84], [94mLoss[0m : 2.55964
[1mStep[0m  [80/84], [94mLoss[0m : 2.24304

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.418, [92mTest[0m: 2.330, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30502
[1mStep[0m  [8/84], [94mLoss[0m : 2.38498
[1mStep[0m  [16/84], [94mLoss[0m : 2.54739
[1mStep[0m  [24/84], [94mLoss[0m : 2.32761
[1mStep[0m  [32/84], [94mLoss[0m : 2.42901
[1mStep[0m  [40/84], [94mLoss[0m : 2.59712
[1mStep[0m  [48/84], [94mLoss[0m : 2.37090
[1mStep[0m  [56/84], [94mLoss[0m : 2.16374
[1mStep[0m  [64/84], [94mLoss[0m : 2.21890
[1mStep[0m  [72/84], [94mLoss[0m : 2.10967
[1mStep[0m  [80/84], [94mLoss[0m : 2.23531

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.373, [92mTest[0m: 2.399, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.51360
[1mStep[0m  [8/84], [94mLoss[0m : 2.49842
[1mStep[0m  [16/84], [94mLoss[0m : 2.04118
[1mStep[0m  [24/84], [94mLoss[0m : 1.98458
[1mStep[0m  [32/84], [94mLoss[0m : 2.62607
[1mStep[0m  [40/84], [94mLoss[0m : 2.30342
[1mStep[0m  [48/84], [94mLoss[0m : 2.57774
[1mStep[0m  [56/84], [94mLoss[0m : 2.38912
[1mStep[0m  [64/84], [94mLoss[0m : 2.21358
[1mStep[0m  [72/84], [94mLoss[0m : 2.47300
[1mStep[0m  [80/84], [94mLoss[0m : 2.39552

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.338, [92mTest[0m: 2.526, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42146
[1mStep[0m  [8/84], [94mLoss[0m : 2.17474
[1mStep[0m  [16/84], [94mLoss[0m : 2.53111
[1mStep[0m  [24/84], [94mLoss[0m : 2.02796
[1mStep[0m  [32/84], [94mLoss[0m : 2.02894
[1mStep[0m  [40/84], [94mLoss[0m : 2.49949
[1mStep[0m  [48/84], [94mLoss[0m : 2.17856
[1mStep[0m  [56/84], [94mLoss[0m : 2.38265
[1mStep[0m  [64/84], [94mLoss[0m : 2.10002
[1mStep[0m  [72/84], [94mLoss[0m : 2.35962
[1mStep[0m  [80/84], [94mLoss[0m : 2.43220

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.292, [92mTest[0m: 2.580, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.07007
[1mStep[0m  [8/84], [94mLoss[0m : 2.10716
[1mStep[0m  [16/84], [94mLoss[0m : 2.28144
[1mStep[0m  [24/84], [94mLoss[0m : 2.49095
[1mStep[0m  [32/84], [94mLoss[0m : 2.26808
[1mStep[0m  [40/84], [94mLoss[0m : 2.29644
[1mStep[0m  [48/84], [94mLoss[0m : 2.56119
[1mStep[0m  [56/84], [94mLoss[0m : 2.11357
[1mStep[0m  [64/84], [94mLoss[0m : 2.13676
[1mStep[0m  [72/84], [94mLoss[0m : 2.16882
[1mStep[0m  [80/84], [94mLoss[0m : 2.39780

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.246, [92mTest[0m: 2.513, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30332
[1mStep[0m  [8/84], [94mLoss[0m : 2.27945
[1mStep[0m  [16/84], [94mLoss[0m : 2.47949
[1mStep[0m  [24/84], [94mLoss[0m : 2.35663
[1mStep[0m  [32/84], [94mLoss[0m : 2.24791
[1mStep[0m  [40/84], [94mLoss[0m : 1.98888
[1mStep[0m  [48/84], [94mLoss[0m : 2.19394
[1mStep[0m  [56/84], [94mLoss[0m : 2.56766
[1mStep[0m  [64/84], [94mLoss[0m : 2.20353
[1mStep[0m  [72/84], [94mLoss[0m : 2.04725
[1mStep[0m  [80/84], [94mLoss[0m : 2.38292

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.207, [92mTest[0m: 2.512, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33921
[1mStep[0m  [8/84], [94mLoss[0m : 2.22453
[1mStep[0m  [16/84], [94mLoss[0m : 1.99870
[1mStep[0m  [24/84], [94mLoss[0m : 2.21599
[1mStep[0m  [32/84], [94mLoss[0m : 2.37622
[1mStep[0m  [40/84], [94mLoss[0m : 2.07872
[1mStep[0m  [48/84], [94mLoss[0m : 2.27368
[1mStep[0m  [56/84], [94mLoss[0m : 2.04203
[1mStep[0m  [64/84], [94mLoss[0m : 2.29866
[1mStep[0m  [72/84], [94mLoss[0m : 2.02607
[1mStep[0m  [80/84], [94mLoss[0m : 2.11462

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.151, [92mTest[0m: 2.517, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.11345
[1mStep[0m  [8/84], [94mLoss[0m : 1.82268
[1mStep[0m  [16/84], [94mLoss[0m : 2.07111
[1mStep[0m  [24/84], [94mLoss[0m : 2.16305
[1mStep[0m  [32/84], [94mLoss[0m : 2.17941
[1mStep[0m  [40/84], [94mLoss[0m : 1.97175
[1mStep[0m  [48/84], [94mLoss[0m : 2.15056
[1mStep[0m  [56/84], [94mLoss[0m : 2.25709
[1mStep[0m  [64/84], [94mLoss[0m : 2.40384
[1mStep[0m  [72/84], [94mLoss[0m : 2.15788
[1mStep[0m  [80/84], [94mLoss[0m : 2.13085

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.106, [92mTest[0m: 2.439, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.94818
[1mStep[0m  [8/84], [94mLoss[0m : 2.12198
[1mStep[0m  [16/84], [94mLoss[0m : 2.09722
[1mStep[0m  [24/84], [94mLoss[0m : 2.03200
[1mStep[0m  [32/84], [94mLoss[0m : 1.97962
[1mStep[0m  [40/84], [94mLoss[0m : 2.17836
[1mStep[0m  [48/84], [94mLoss[0m : 2.48316
[1mStep[0m  [56/84], [94mLoss[0m : 2.18342
[1mStep[0m  [64/84], [94mLoss[0m : 1.88598
[1mStep[0m  [72/84], [94mLoss[0m : 2.11616
[1mStep[0m  [80/84], [94mLoss[0m : 2.01994

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.063, [92mTest[0m: 2.442, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.82847
[1mStep[0m  [8/84], [94mLoss[0m : 1.87691
[1mStep[0m  [16/84], [94mLoss[0m : 2.15144
[1mStep[0m  [24/84], [94mLoss[0m : 2.15960
[1mStep[0m  [32/84], [94mLoss[0m : 2.02805
[1mStep[0m  [40/84], [94mLoss[0m : 2.15754
[1mStep[0m  [48/84], [94mLoss[0m : 1.73963
[1mStep[0m  [56/84], [94mLoss[0m : 2.27174
[1mStep[0m  [64/84], [94mLoss[0m : 1.99488
[1mStep[0m  [72/84], [94mLoss[0m : 1.91657
[1mStep[0m  [80/84], [94mLoss[0m : 1.89309

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.007, [92mTest[0m: 2.508, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.02390
[1mStep[0m  [8/84], [94mLoss[0m : 1.95757
[1mStep[0m  [16/84], [94mLoss[0m : 1.92785
[1mStep[0m  [24/84], [94mLoss[0m : 2.02904
[1mStep[0m  [32/84], [94mLoss[0m : 1.71991
[1mStep[0m  [40/84], [94mLoss[0m : 2.00560
[1mStep[0m  [48/84], [94mLoss[0m : 2.13672
[1mStep[0m  [56/84], [94mLoss[0m : 2.02032
[1mStep[0m  [64/84], [94mLoss[0m : 1.95743
[1mStep[0m  [72/84], [94mLoss[0m : 2.22105
[1mStep[0m  [80/84], [94mLoss[0m : 1.88809

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.963, [92mTest[0m: 2.439, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.97602
[1mStep[0m  [8/84], [94mLoss[0m : 1.93718
[1mStep[0m  [16/84], [94mLoss[0m : 2.05997
[1mStep[0m  [24/84], [94mLoss[0m : 1.72785
[1mStep[0m  [32/84], [94mLoss[0m : 1.88920
[1mStep[0m  [40/84], [94mLoss[0m : 2.05616
[1mStep[0m  [48/84], [94mLoss[0m : 1.96986
[1mStep[0m  [56/84], [94mLoss[0m : 1.87679
[1mStep[0m  [64/84], [94mLoss[0m : 2.32360
[1mStep[0m  [72/84], [94mLoss[0m : 1.71473
[1mStep[0m  [80/84], [94mLoss[0m : 1.74837

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.905, [92mTest[0m: 2.454, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.79641
[1mStep[0m  [8/84], [94mLoss[0m : 1.85255
[1mStep[0m  [16/84], [94mLoss[0m : 1.76662
[1mStep[0m  [24/84], [94mLoss[0m : 1.70373
[1mStep[0m  [32/84], [94mLoss[0m : 1.70302
[1mStep[0m  [40/84], [94mLoss[0m : 1.94289
[1mStep[0m  [48/84], [94mLoss[0m : 2.27439
[1mStep[0m  [56/84], [94mLoss[0m : 1.59919
[1mStep[0m  [64/84], [94mLoss[0m : 1.83381
[1mStep[0m  [72/84], [94mLoss[0m : 1.69749
[1mStep[0m  [80/84], [94mLoss[0m : 2.28889

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.872, [92mTest[0m: 2.402, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.69014
[1mStep[0m  [8/84], [94mLoss[0m : 1.68262
[1mStep[0m  [16/84], [94mLoss[0m : 1.58878
[1mStep[0m  [24/84], [94mLoss[0m : 1.72679
[1mStep[0m  [32/84], [94mLoss[0m : 1.91471
[1mStep[0m  [40/84], [94mLoss[0m : 1.86277
[1mStep[0m  [48/84], [94mLoss[0m : 1.86754
[1mStep[0m  [56/84], [94mLoss[0m : 1.87511
[1mStep[0m  [64/84], [94mLoss[0m : 1.89035
[1mStep[0m  [72/84], [94mLoss[0m : 1.81690
[1mStep[0m  [80/84], [94mLoss[0m : 1.81326

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.833, [92mTest[0m: 2.472, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.75182
[1mStep[0m  [8/84], [94mLoss[0m : 1.83494
[1mStep[0m  [16/84], [94mLoss[0m : 1.82047
[1mStep[0m  [24/84], [94mLoss[0m : 1.91693
[1mStep[0m  [32/84], [94mLoss[0m : 1.69631
[1mStep[0m  [40/84], [94mLoss[0m : 1.69022
[1mStep[0m  [48/84], [94mLoss[0m : 1.91866
[1mStep[0m  [56/84], [94mLoss[0m : 1.97845
[1mStep[0m  [64/84], [94mLoss[0m : 1.73034
[1mStep[0m  [72/84], [94mLoss[0m : 1.66142
[1mStep[0m  [80/84], [94mLoss[0m : 1.79250

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.789, [92mTest[0m: 2.426, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.56597
[1mStep[0m  [8/84], [94mLoss[0m : 1.90868
[1mStep[0m  [16/84], [94mLoss[0m : 1.50312
[1mStep[0m  [24/84], [94mLoss[0m : 1.84876
[1mStep[0m  [32/84], [94mLoss[0m : 1.50101
[1mStep[0m  [40/84], [94mLoss[0m : 1.69782
[1mStep[0m  [48/84], [94mLoss[0m : 1.77451
[1mStep[0m  [56/84], [94mLoss[0m : 1.79604
[1mStep[0m  [64/84], [94mLoss[0m : 1.64090
[1mStep[0m  [72/84], [94mLoss[0m : 1.82405
[1mStep[0m  [80/84], [94mLoss[0m : 1.64247

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.749, [92mTest[0m: 2.438, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.77382
[1mStep[0m  [8/84], [94mLoss[0m : 1.46388
[1mStep[0m  [16/84], [94mLoss[0m : 1.97969
[1mStep[0m  [24/84], [94mLoss[0m : 1.63142
[1mStep[0m  [32/84], [94mLoss[0m : 1.55713
[1mStep[0m  [40/84], [94mLoss[0m : 1.74580
[1mStep[0m  [48/84], [94mLoss[0m : 1.74420
[1mStep[0m  [56/84], [94mLoss[0m : 1.78393
[1mStep[0m  [64/84], [94mLoss[0m : 1.56859
[1mStep[0m  [72/84], [94mLoss[0m : 1.84736
[1mStep[0m  [80/84], [94mLoss[0m : 1.59435

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.704, [92mTest[0m: 2.613, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.45055
[1mStep[0m  [8/84], [94mLoss[0m : 1.82815
[1mStep[0m  [16/84], [94mLoss[0m : 1.61714
[1mStep[0m  [24/84], [94mLoss[0m : 1.36835
[1mStep[0m  [32/84], [94mLoss[0m : 1.77289
[1mStep[0m  [40/84], [94mLoss[0m : 1.81705
[1mStep[0m  [48/84], [94mLoss[0m : 1.62219
[1mStep[0m  [56/84], [94mLoss[0m : 1.55479
[1mStep[0m  [64/84], [94mLoss[0m : 1.91179
[1mStep[0m  [72/84], [94mLoss[0m : 1.60505
[1mStep[0m  [80/84], [94mLoss[0m : 1.60455

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.664, [92mTest[0m: 2.504, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.85873
[1mStep[0m  [8/84], [94mLoss[0m : 1.84620
[1mStep[0m  [16/84], [94mLoss[0m : 1.73815
[1mStep[0m  [24/84], [94mLoss[0m : 1.53568
[1mStep[0m  [32/84], [94mLoss[0m : 1.58816
[1mStep[0m  [40/84], [94mLoss[0m : 1.46891
[1mStep[0m  [48/84], [94mLoss[0m : 1.69070
[1mStep[0m  [56/84], [94mLoss[0m : 1.67353
[1mStep[0m  [64/84], [94mLoss[0m : 1.68535
[1mStep[0m  [72/84], [94mLoss[0m : 1.66222
[1mStep[0m  [80/84], [94mLoss[0m : 1.72648

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.638, [92mTest[0m: 2.483, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.67862
[1mStep[0m  [8/84], [94mLoss[0m : 1.61087
[1mStep[0m  [16/84], [94mLoss[0m : 1.57327
[1mStep[0m  [24/84], [94mLoss[0m : 1.71205
[1mStep[0m  [32/84], [94mLoss[0m : 1.93948
[1mStep[0m  [40/84], [94mLoss[0m : 1.36708
[1mStep[0m  [48/84], [94mLoss[0m : 1.48268
[1mStep[0m  [56/84], [94mLoss[0m : 1.41249
[1mStep[0m  [64/84], [94mLoss[0m : 1.65233
[1mStep[0m  [72/84], [94mLoss[0m : 1.53550
[1mStep[0m  [80/84], [94mLoss[0m : 1.70018

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.608, [92mTest[0m: 2.826, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.67061
[1mStep[0m  [8/84], [94mLoss[0m : 1.65304
[1mStep[0m  [16/84], [94mLoss[0m : 1.63394
[1mStep[0m  [24/84], [94mLoss[0m : 1.65578
[1mStep[0m  [32/84], [94mLoss[0m : 1.56652
[1mStep[0m  [40/84], [94mLoss[0m : 1.43635
[1mStep[0m  [48/84], [94mLoss[0m : 1.52504
[1mStep[0m  [56/84], [94mLoss[0m : 1.47268
[1mStep[0m  [64/84], [94mLoss[0m : 1.56431
[1mStep[0m  [72/84], [94mLoss[0m : 1.54535
[1mStep[0m  [80/84], [94mLoss[0m : 1.63370

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.557, [92mTest[0m: 2.507, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.43043
[1mStep[0m  [8/84], [94mLoss[0m : 1.39593
[1mStep[0m  [16/84], [94mLoss[0m : 1.60476
[1mStep[0m  [24/84], [94mLoss[0m : 1.53082
[1mStep[0m  [32/84], [94mLoss[0m : 1.51178
[1mStep[0m  [40/84], [94mLoss[0m : 1.44433
[1mStep[0m  [48/84], [94mLoss[0m : 1.87053
[1mStep[0m  [56/84], [94mLoss[0m : 1.52398
[1mStep[0m  [64/84], [94mLoss[0m : 1.32788
[1mStep[0m  [72/84], [94mLoss[0m : 1.31920
[1mStep[0m  [80/84], [94mLoss[0m : 1.44286

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.525, [92mTest[0m: 2.500, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.59265
[1mStep[0m  [8/84], [94mLoss[0m : 1.45583
[1mStep[0m  [16/84], [94mLoss[0m : 1.58842
[1mStep[0m  [24/84], [94mLoss[0m : 1.49537
[1mStep[0m  [32/84], [94mLoss[0m : 1.58523
[1mStep[0m  [40/84], [94mLoss[0m : 1.64943
[1mStep[0m  [48/84], [94mLoss[0m : 1.45291
[1mStep[0m  [56/84], [94mLoss[0m : 1.29370
[1mStep[0m  [64/84], [94mLoss[0m : 1.45836
[1mStep[0m  [72/84], [94mLoss[0m : 1.42284
[1mStep[0m  [80/84], [94mLoss[0m : 1.50758

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.490, [92mTest[0m: 2.532, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.35433
[1mStep[0m  [8/84], [94mLoss[0m : 1.21854
[1mStep[0m  [16/84], [94mLoss[0m : 1.44930
[1mStep[0m  [24/84], [94mLoss[0m : 1.63329
[1mStep[0m  [32/84], [94mLoss[0m : 1.19711
[1mStep[0m  [40/84], [94mLoss[0m : 1.37997
[1mStep[0m  [48/84], [94mLoss[0m : 1.35636
[1mStep[0m  [56/84], [94mLoss[0m : 1.49923
[1mStep[0m  [64/84], [94mLoss[0m : 1.55401
[1mStep[0m  [72/84], [94mLoss[0m : 1.71938
[1mStep[0m  [80/84], [94mLoss[0m : 1.41780

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.476, [92mTest[0m: 2.504, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.35173
[1mStep[0m  [8/84], [94mLoss[0m : 1.32846
[1mStep[0m  [16/84], [94mLoss[0m : 1.44753
[1mStep[0m  [24/84], [94mLoss[0m : 1.58464
[1mStep[0m  [32/84], [94mLoss[0m : 1.30538
[1mStep[0m  [40/84], [94mLoss[0m : 1.32908
[1mStep[0m  [48/84], [94mLoss[0m : 1.32263
[1mStep[0m  [56/84], [94mLoss[0m : 1.50570
[1mStep[0m  [64/84], [94mLoss[0m : 1.49613
[1mStep[0m  [72/84], [94mLoss[0m : 1.48324
[1mStep[0m  [80/84], [94mLoss[0m : 1.40199

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.455, [92mTest[0m: 2.447, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.55420
[1mStep[0m  [8/84], [94mLoss[0m : 1.24958
[1mStep[0m  [16/84], [94mLoss[0m : 1.45191
[1mStep[0m  [24/84], [94mLoss[0m : 1.40560
[1mStep[0m  [32/84], [94mLoss[0m : 1.36074
[1mStep[0m  [40/84], [94mLoss[0m : 1.34385
[1mStep[0m  [48/84], [94mLoss[0m : 1.34424
[1mStep[0m  [56/84], [94mLoss[0m : 1.43522
[1mStep[0m  [64/84], [94mLoss[0m : 1.55002
[1mStep[0m  [72/84], [94mLoss[0m : 1.62268
[1mStep[0m  [80/84], [94mLoss[0m : 1.66562

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.443, [92mTest[0m: 2.502, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.45008
[1mStep[0m  [8/84], [94mLoss[0m : 1.29512
[1mStep[0m  [16/84], [94mLoss[0m : 1.41681
[1mStep[0m  [24/84], [94mLoss[0m : 1.31109
[1mStep[0m  [32/84], [94mLoss[0m : 1.52217
[1mStep[0m  [40/84], [94mLoss[0m : 1.34919
[1mStep[0m  [48/84], [94mLoss[0m : 1.41515
[1mStep[0m  [56/84], [94mLoss[0m : 1.45407
[1mStep[0m  [64/84], [94mLoss[0m : 1.43290
[1mStep[0m  [72/84], [94mLoss[0m : 1.28527
[1mStep[0m  [80/84], [94mLoss[0m : 1.12663

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.410, [92mTest[0m: 2.560, [96mlr[0m: 0.006772500000000002
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 26 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.495
====================================

Phase 2 - Evaluation MAE:  2.494740801198142
MAE score P1      2.326287
MAE score P2      2.494741
loss              1.409645
learning_rate     0.007525
batch_size             128
hidden_sizes         [250]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping        True
dropout                0.2
momentum               0.1
weight_decay        0.0001
Name: 14, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 9.78823
[1mStep[0m  [8/84], [94mLoss[0m : 4.97724
[1mStep[0m  [16/84], [94mLoss[0m : 3.41457
[1mStep[0m  [24/84], [94mLoss[0m : 2.98706
[1mStep[0m  [32/84], [94mLoss[0m : 2.90811
[1mStep[0m  [40/84], [94mLoss[0m : 2.41432
[1mStep[0m  [48/84], [94mLoss[0m : 2.79749
[1mStep[0m  [56/84], [94mLoss[0m : 2.63917
[1mStep[0m  [64/84], [94mLoss[0m : 2.46475
[1mStep[0m  [72/84], [94mLoss[0m : 2.69803
[1mStep[0m  [80/84], [94mLoss[0m : 2.51119

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.338, [92mTest[0m: 10.385, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50915
[1mStep[0m  [8/84], [94mLoss[0m : 2.58671
[1mStep[0m  [16/84], [94mLoss[0m : 2.63683
[1mStep[0m  [24/84], [94mLoss[0m : 2.58112
[1mStep[0m  [32/84], [94mLoss[0m : 2.62862
[1mStep[0m  [40/84], [94mLoss[0m : 2.30438
[1mStep[0m  [48/84], [94mLoss[0m : 2.45163
[1mStep[0m  [56/84], [94mLoss[0m : 2.51972
[1mStep[0m  [64/84], [94mLoss[0m : 2.59155
[1mStep[0m  [72/84], [94mLoss[0m : 2.48193
[1mStep[0m  [80/84], [94mLoss[0m : 2.53303

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.531, [92mTest[0m: 2.356, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.64696
[1mStep[0m  [8/84], [94mLoss[0m : 2.11036
[1mStep[0m  [16/84], [94mLoss[0m : 2.28459
[1mStep[0m  [24/84], [94mLoss[0m : 2.45802
[1mStep[0m  [32/84], [94mLoss[0m : 2.53731
[1mStep[0m  [40/84], [94mLoss[0m : 2.66064
[1mStep[0m  [48/84], [94mLoss[0m : 2.56452
[1mStep[0m  [56/84], [94mLoss[0m : 2.60480
[1mStep[0m  [64/84], [94mLoss[0m : 2.77656
[1mStep[0m  [72/84], [94mLoss[0m : 2.53612
[1mStep[0m  [80/84], [94mLoss[0m : 2.25126

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.345, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45913
[1mStep[0m  [8/84], [94mLoss[0m : 2.49764
[1mStep[0m  [16/84], [94mLoss[0m : 2.72701
[1mStep[0m  [24/84], [94mLoss[0m : 2.24148
[1mStep[0m  [32/84], [94mLoss[0m : 2.56653
[1mStep[0m  [40/84], [94mLoss[0m : 2.51459
[1mStep[0m  [48/84], [94mLoss[0m : 2.17899
[1mStep[0m  [56/84], [94mLoss[0m : 2.48627
[1mStep[0m  [64/84], [94mLoss[0m : 2.52528
[1mStep[0m  [72/84], [94mLoss[0m : 2.59470
[1mStep[0m  [80/84], [94mLoss[0m : 2.77715

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.338, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49011
[1mStep[0m  [8/84], [94mLoss[0m : 2.55388
[1mStep[0m  [16/84], [94mLoss[0m : 2.53341
[1mStep[0m  [24/84], [94mLoss[0m : 2.66043
[1mStep[0m  [32/84], [94mLoss[0m : 2.44714
[1mStep[0m  [40/84], [94mLoss[0m : 2.41414
[1mStep[0m  [48/84], [94mLoss[0m : 2.22160
[1mStep[0m  [56/84], [94mLoss[0m : 2.57410
[1mStep[0m  [64/84], [94mLoss[0m : 2.53720
[1mStep[0m  [72/84], [94mLoss[0m : 2.47833
[1mStep[0m  [80/84], [94mLoss[0m : 2.53761

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.342, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.34534
[1mStep[0m  [8/84], [94mLoss[0m : 2.74791
[1mStep[0m  [16/84], [94mLoss[0m : 2.57383
[1mStep[0m  [24/84], [94mLoss[0m : 2.39858
[1mStep[0m  [32/84], [94mLoss[0m : 2.53372
[1mStep[0m  [40/84], [94mLoss[0m : 2.23850
[1mStep[0m  [48/84], [94mLoss[0m : 2.32022
[1mStep[0m  [56/84], [94mLoss[0m : 2.39931
[1mStep[0m  [64/84], [94mLoss[0m : 2.64436
[1mStep[0m  [72/84], [94mLoss[0m : 2.34776
[1mStep[0m  [80/84], [94mLoss[0m : 2.32787

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.325, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.57586
[1mStep[0m  [8/84], [94mLoss[0m : 2.47241
[1mStep[0m  [16/84], [94mLoss[0m : 2.24629
[1mStep[0m  [24/84], [94mLoss[0m : 2.37230
[1mStep[0m  [32/84], [94mLoss[0m : 2.29322
[1mStep[0m  [40/84], [94mLoss[0m : 2.36174
[1mStep[0m  [48/84], [94mLoss[0m : 2.57636
[1mStep[0m  [56/84], [94mLoss[0m : 2.34491
[1mStep[0m  [64/84], [94mLoss[0m : 2.64267
[1mStep[0m  [72/84], [94mLoss[0m : 2.19598
[1mStep[0m  [80/84], [94mLoss[0m : 2.34856

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.367, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.88775
[1mStep[0m  [8/84], [94mLoss[0m : 2.62745
[1mStep[0m  [16/84], [94mLoss[0m : 2.67894
[1mStep[0m  [24/84], [94mLoss[0m : 2.42264
[1mStep[0m  [32/84], [94mLoss[0m : 2.44561
[1mStep[0m  [40/84], [94mLoss[0m : 2.44131
[1mStep[0m  [48/84], [94mLoss[0m : 2.62466
[1mStep[0m  [56/84], [94mLoss[0m : 2.34862
[1mStep[0m  [64/84], [94mLoss[0m : 2.37367
[1mStep[0m  [72/84], [94mLoss[0m : 2.70521
[1mStep[0m  [80/84], [94mLoss[0m : 2.83071

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.336, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.25241
[1mStep[0m  [8/84], [94mLoss[0m : 2.41212
[1mStep[0m  [16/84], [94mLoss[0m : 2.49700
[1mStep[0m  [24/84], [94mLoss[0m : 2.47763
[1mStep[0m  [32/84], [94mLoss[0m : 2.57821
[1mStep[0m  [40/84], [94mLoss[0m : 2.62492
[1mStep[0m  [48/84], [94mLoss[0m : 2.73231
[1mStep[0m  [56/84], [94mLoss[0m : 2.34414
[1mStep[0m  [64/84], [94mLoss[0m : 2.50217
[1mStep[0m  [72/84], [94mLoss[0m : 2.39377
[1mStep[0m  [80/84], [94mLoss[0m : 2.57887

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.333, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.05637
[1mStep[0m  [8/84], [94mLoss[0m : 2.38340
[1mStep[0m  [16/84], [94mLoss[0m : 2.81270
[1mStep[0m  [24/84], [94mLoss[0m : 2.19430
[1mStep[0m  [32/84], [94mLoss[0m : 2.30625
[1mStep[0m  [40/84], [94mLoss[0m : 2.25699
[1mStep[0m  [48/84], [94mLoss[0m : 2.66851
[1mStep[0m  [56/84], [94mLoss[0m : 2.60371
[1mStep[0m  [64/84], [94mLoss[0m : 2.70288
[1mStep[0m  [72/84], [94mLoss[0m : 2.60121
[1mStep[0m  [80/84], [94mLoss[0m : 2.57791

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.332, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.64394
[1mStep[0m  [8/84], [94mLoss[0m : 2.05818
[1mStep[0m  [16/84], [94mLoss[0m : 2.38268
[1mStep[0m  [24/84], [94mLoss[0m : 2.56194
[1mStep[0m  [32/84], [94mLoss[0m : 2.23168
[1mStep[0m  [40/84], [94mLoss[0m : 2.60039
[1mStep[0m  [48/84], [94mLoss[0m : 2.20846
[1mStep[0m  [56/84], [94mLoss[0m : 2.67683
[1mStep[0m  [64/84], [94mLoss[0m : 2.36182
[1mStep[0m  [72/84], [94mLoss[0m : 2.31728
[1mStep[0m  [80/84], [94mLoss[0m : 2.46558

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.330, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.76032
[1mStep[0m  [8/84], [94mLoss[0m : 2.37017
[1mStep[0m  [16/84], [94mLoss[0m : 2.42239
[1mStep[0m  [24/84], [94mLoss[0m : 2.43910
[1mStep[0m  [32/84], [94mLoss[0m : 2.28418
[1mStep[0m  [40/84], [94mLoss[0m : 2.30035
[1mStep[0m  [48/84], [94mLoss[0m : 2.35987
[1mStep[0m  [56/84], [94mLoss[0m : 2.41379
[1mStep[0m  [64/84], [94mLoss[0m : 2.44433
[1mStep[0m  [72/84], [94mLoss[0m : 2.39547
[1mStep[0m  [80/84], [94mLoss[0m : 2.60162

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.332, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67854
[1mStep[0m  [8/84], [94mLoss[0m : 2.36777
[1mStep[0m  [16/84], [94mLoss[0m : 2.26368
[1mStep[0m  [24/84], [94mLoss[0m : 2.61711
[1mStep[0m  [32/84], [94mLoss[0m : 2.52795
[1mStep[0m  [40/84], [94mLoss[0m : 2.64191
[1mStep[0m  [48/84], [94mLoss[0m : 2.70282
[1mStep[0m  [56/84], [94mLoss[0m : 2.22265
[1mStep[0m  [64/84], [94mLoss[0m : 2.28567
[1mStep[0m  [72/84], [94mLoss[0m : 2.54436
[1mStep[0m  [80/84], [94mLoss[0m : 2.51283

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.339, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.63079
[1mStep[0m  [8/84], [94mLoss[0m : 2.22719
[1mStep[0m  [16/84], [94mLoss[0m : 2.57267
[1mStep[0m  [24/84], [94mLoss[0m : 2.33014
[1mStep[0m  [32/84], [94mLoss[0m : 2.30503
[1mStep[0m  [40/84], [94mLoss[0m : 2.31038
[1mStep[0m  [48/84], [94mLoss[0m : 2.35326
[1mStep[0m  [56/84], [94mLoss[0m : 2.31143
[1mStep[0m  [64/84], [94mLoss[0m : 2.53981
[1mStep[0m  [72/84], [94mLoss[0m : 2.52864
[1mStep[0m  [80/84], [94mLoss[0m : 2.71212

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.330, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42194
[1mStep[0m  [8/84], [94mLoss[0m : 2.36950
[1mStep[0m  [16/84], [94mLoss[0m : 2.00933
[1mStep[0m  [24/84], [94mLoss[0m : 2.14517
[1mStep[0m  [32/84], [94mLoss[0m : 2.47922
[1mStep[0m  [40/84], [94mLoss[0m : 2.67587
[1mStep[0m  [48/84], [94mLoss[0m : 2.76782
[1mStep[0m  [56/84], [94mLoss[0m : 2.32238
[1mStep[0m  [64/84], [94mLoss[0m : 2.54553
[1mStep[0m  [72/84], [94mLoss[0m : 2.57928
[1mStep[0m  [80/84], [94mLoss[0m : 2.78141

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.331, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31653
[1mStep[0m  [8/84], [94mLoss[0m : 2.65565
[1mStep[0m  [16/84], [94mLoss[0m : 2.68948
[1mStep[0m  [24/84], [94mLoss[0m : 2.38996
[1mStep[0m  [32/84], [94mLoss[0m : 2.55122
[1mStep[0m  [40/84], [94mLoss[0m : 2.35912
[1mStep[0m  [48/84], [94mLoss[0m : 2.61514
[1mStep[0m  [56/84], [94mLoss[0m : 2.65885
[1mStep[0m  [64/84], [94mLoss[0m : 2.36985
[1mStep[0m  [72/84], [94mLoss[0m : 2.32647
[1mStep[0m  [80/84], [94mLoss[0m : 2.21779

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.335, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40024
[1mStep[0m  [8/84], [94mLoss[0m : 2.41052
[1mStep[0m  [16/84], [94mLoss[0m : 2.53954
[1mStep[0m  [24/84], [94mLoss[0m : 2.49770
[1mStep[0m  [32/84], [94mLoss[0m : 2.31799
[1mStep[0m  [40/84], [94mLoss[0m : 2.29992
[1mStep[0m  [48/84], [94mLoss[0m : 2.34118
[1mStep[0m  [56/84], [94mLoss[0m : 2.71235
[1mStep[0m  [64/84], [94mLoss[0m : 2.48153
[1mStep[0m  [72/84], [94mLoss[0m : 2.35138
[1mStep[0m  [80/84], [94mLoss[0m : 2.50093

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.331, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35306
[1mStep[0m  [8/84], [94mLoss[0m : 2.29722
[1mStep[0m  [16/84], [94mLoss[0m : 2.51756
[1mStep[0m  [24/84], [94mLoss[0m : 2.61135
[1mStep[0m  [32/84], [94mLoss[0m : 2.37697
[1mStep[0m  [40/84], [94mLoss[0m : 2.37944
[1mStep[0m  [48/84], [94mLoss[0m : 2.29513
[1mStep[0m  [56/84], [94mLoss[0m : 2.47800
[1mStep[0m  [64/84], [94mLoss[0m : 2.34069
[1mStep[0m  [72/84], [94mLoss[0m : 2.27633
[1mStep[0m  [80/84], [94mLoss[0m : 2.41059

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.327, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.70941
[1mStep[0m  [8/84], [94mLoss[0m : 2.43896
[1mStep[0m  [16/84], [94mLoss[0m : 2.05181
[1mStep[0m  [24/84], [94mLoss[0m : 2.44254
[1mStep[0m  [32/84], [94mLoss[0m : 2.22934
[1mStep[0m  [40/84], [94mLoss[0m : 2.60363
[1mStep[0m  [48/84], [94mLoss[0m : 2.37687
[1mStep[0m  [56/84], [94mLoss[0m : 2.37063
[1mStep[0m  [64/84], [94mLoss[0m : 2.74605
[1mStep[0m  [72/84], [94mLoss[0m : 2.50096
[1mStep[0m  [80/84], [94mLoss[0m : 2.25444

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.341, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.17099
[1mStep[0m  [8/84], [94mLoss[0m : 2.56795
[1mStep[0m  [16/84], [94mLoss[0m : 2.46970
[1mStep[0m  [24/84], [94mLoss[0m : 2.13464
[1mStep[0m  [32/84], [94mLoss[0m : 2.22831
[1mStep[0m  [40/84], [94mLoss[0m : 2.37597
[1mStep[0m  [48/84], [94mLoss[0m : 2.54002
[1mStep[0m  [56/84], [94mLoss[0m : 2.56508
[1mStep[0m  [64/84], [94mLoss[0m : 2.36039
[1mStep[0m  [72/84], [94mLoss[0m : 2.26271
[1mStep[0m  [80/84], [94mLoss[0m : 2.62075

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.338, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.78337
[1mStep[0m  [8/84], [94mLoss[0m : 2.20724
[1mStep[0m  [16/84], [94mLoss[0m : 2.71576
[1mStep[0m  [24/84], [94mLoss[0m : 2.52930
[1mStep[0m  [32/84], [94mLoss[0m : 2.34751
[1mStep[0m  [40/84], [94mLoss[0m : 2.56558
[1mStep[0m  [48/84], [94mLoss[0m : 2.21992
[1mStep[0m  [56/84], [94mLoss[0m : 2.53125
[1mStep[0m  [64/84], [94mLoss[0m : 2.33229
[1mStep[0m  [72/84], [94mLoss[0m : 2.76642
[1mStep[0m  [80/84], [94mLoss[0m : 2.56640

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.324, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.23170
[1mStep[0m  [8/84], [94mLoss[0m : 2.38100
[1mStep[0m  [16/84], [94mLoss[0m : 2.29853
[1mStep[0m  [24/84], [94mLoss[0m : 2.67573
[1mStep[0m  [32/84], [94mLoss[0m : 2.54408
[1mStep[0m  [40/84], [94mLoss[0m : 2.45404
[1mStep[0m  [48/84], [94mLoss[0m : 2.30408
[1mStep[0m  [56/84], [94mLoss[0m : 2.64910
[1mStep[0m  [64/84], [94mLoss[0m : 2.37996
[1mStep[0m  [72/84], [94mLoss[0m : 2.66434
[1mStep[0m  [80/84], [94mLoss[0m : 2.47344

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.330, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54784
[1mStep[0m  [8/84], [94mLoss[0m : 2.34521
[1mStep[0m  [16/84], [94mLoss[0m : 2.54566
[1mStep[0m  [24/84], [94mLoss[0m : 2.71080
[1mStep[0m  [32/84], [94mLoss[0m : 2.39406
[1mStep[0m  [40/84], [94mLoss[0m : 2.67156
[1mStep[0m  [48/84], [94mLoss[0m : 2.68357
[1mStep[0m  [56/84], [94mLoss[0m : 2.08220
[1mStep[0m  [64/84], [94mLoss[0m : 2.26939
[1mStep[0m  [72/84], [94mLoss[0m : 2.57855
[1mStep[0m  [80/84], [94mLoss[0m : 2.20571

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.337, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59526
[1mStep[0m  [8/84], [94mLoss[0m : 2.50321
[1mStep[0m  [16/84], [94mLoss[0m : 2.39843
[1mStep[0m  [24/84], [94mLoss[0m : 2.24365
[1mStep[0m  [32/84], [94mLoss[0m : 2.37846
[1mStep[0m  [40/84], [94mLoss[0m : 2.57145
[1mStep[0m  [48/84], [94mLoss[0m : 2.35527
[1mStep[0m  [56/84], [94mLoss[0m : 2.56785
[1mStep[0m  [64/84], [94mLoss[0m : 2.28496
[1mStep[0m  [72/84], [94mLoss[0m : 2.48140
[1mStep[0m  [80/84], [94mLoss[0m : 2.67917

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.323, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.32237
[1mStep[0m  [8/84], [94mLoss[0m : 2.39361
[1mStep[0m  [16/84], [94mLoss[0m : 2.55953
[1mStep[0m  [24/84], [94mLoss[0m : 2.22832
[1mStep[0m  [32/84], [94mLoss[0m : 2.76395
[1mStep[0m  [40/84], [94mLoss[0m : 2.68717
[1mStep[0m  [48/84], [94mLoss[0m : 2.58866
[1mStep[0m  [56/84], [94mLoss[0m : 2.48354
[1mStep[0m  [64/84], [94mLoss[0m : 2.52620
[1mStep[0m  [72/84], [94mLoss[0m : 2.53925
[1mStep[0m  [80/84], [94mLoss[0m : 2.37303

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.329, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.77966
[1mStep[0m  [8/84], [94mLoss[0m : 2.43341
[1mStep[0m  [16/84], [94mLoss[0m : 2.31105
[1mStep[0m  [24/84], [94mLoss[0m : 2.33921
[1mStep[0m  [32/84], [94mLoss[0m : 2.55541
[1mStep[0m  [40/84], [94mLoss[0m : 2.35636
[1mStep[0m  [48/84], [94mLoss[0m : 2.59006
[1mStep[0m  [56/84], [94mLoss[0m : 2.57999
[1mStep[0m  [64/84], [94mLoss[0m : 2.65193
[1mStep[0m  [72/84], [94mLoss[0m : 2.51929
[1mStep[0m  [80/84], [94mLoss[0m : 2.54446

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.329, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44484
[1mStep[0m  [8/84], [94mLoss[0m : 2.29755
[1mStep[0m  [16/84], [94mLoss[0m : 2.47588
[1mStep[0m  [24/84], [94mLoss[0m : 2.39953
[1mStep[0m  [32/84], [94mLoss[0m : 2.38876
[1mStep[0m  [40/84], [94mLoss[0m : 2.57899
[1mStep[0m  [48/84], [94mLoss[0m : 2.38857
[1mStep[0m  [56/84], [94mLoss[0m : 2.29665
[1mStep[0m  [64/84], [94mLoss[0m : 2.16650
[1mStep[0m  [72/84], [94mLoss[0m : 2.43186
[1mStep[0m  [80/84], [94mLoss[0m : 2.62558

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.336, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.68417
[1mStep[0m  [8/84], [94mLoss[0m : 2.54882
[1mStep[0m  [16/84], [94mLoss[0m : 2.15664
[1mStep[0m  [24/84], [94mLoss[0m : 2.38459
[1mStep[0m  [32/84], [94mLoss[0m : 2.05398
[1mStep[0m  [40/84], [94mLoss[0m : 2.46220
[1mStep[0m  [48/84], [94mLoss[0m : 2.60207
[1mStep[0m  [56/84], [94mLoss[0m : 2.32650
[1mStep[0m  [64/84], [94mLoss[0m : 2.17340
[1mStep[0m  [72/84], [94mLoss[0m : 2.29649
[1mStep[0m  [80/84], [94mLoss[0m : 2.59661

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.358, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.58962
[1mStep[0m  [8/84], [94mLoss[0m : 2.38464
[1mStep[0m  [16/84], [94mLoss[0m : 2.39112
[1mStep[0m  [24/84], [94mLoss[0m : 2.37945
[1mStep[0m  [32/84], [94mLoss[0m : 2.47264
[1mStep[0m  [40/84], [94mLoss[0m : 2.28771
[1mStep[0m  [48/84], [94mLoss[0m : 2.32915
[1mStep[0m  [56/84], [94mLoss[0m : 2.50285
[1mStep[0m  [64/84], [94mLoss[0m : 2.32489
[1mStep[0m  [72/84], [94mLoss[0m : 2.44220
[1mStep[0m  [80/84], [94mLoss[0m : 2.16943

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.347, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37195
[1mStep[0m  [8/84], [94mLoss[0m : 2.16791
[1mStep[0m  [16/84], [94mLoss[0m : 2.53622
[1mStep[0m  [24/84], [94mLoss[0m : 2.32637
[1mStep[0m  [32/84], [94mLoss[0m : 2.24260
[1mStep[0m  [40/84], [94mLoss[0m : 2.69406
[1mStep[0m  [48/84], [94mLoss[0m : 2.65858
[1mStep[0m  [56/84], [94mLoss[0m : 2.39496
[1mStep[0m  [64/84], [94mLoss[0m : 2.76542
[1mStep[0m  [72/84], [94mLoss[0m : 2.49096
[1mStep[0m  [80/84], [94mLoss[0m : 2.65646

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.325, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.326
====================================

Phase 1 - Evaluation MAE:  2.326081625052861
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 2.32149
[1mStep[0m  [8/84], [94mLoss[0m : 2.31025
[1mStep[0m  [16/84], [94mLoss[0m : 2.67287
[1mStep[0m  [24/84], [94mLoss[0m : 2.52199
[1mStep[0m  [32/84], [94mLoss[0m : 2.51875
[1mStep[0m  [40/84], [94mLoss[0m : 2.56066
[1mStep[0m  [48/84], [94mLoss[0m : 2.60659
[1mStep[0m  [56/84], [94mLoss[0m : 2.28722
[1mStep[0m  [64/84], [94mLoss[0m : 2.58784
[1mStep[0m  [72/84], [94mLoss[0m : 2.67457
[1mStep[0m  [80/84], [94mLoss[0m : 2.32129

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.322, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.32803
[1mStep[0m  [8/84], [94mLoss[0m : 2.54170
[1mStep[0m  [16/84], [94mLoss[0m : 2.18326
[1mStep[0m  [24/84], [94mLoss[0m : 2.29660
[1mStep[0m  [32/84], [94mLoss[0m : 2.43010
[1mStep[0m  [40/84], [94mLoss[0m : 2.44292
[1mStep[0m  [48/84], [94mLoss[0m : 2.58360
[1mStep[0m  [56/84], [94mLoss[0m : 2.33818
[1mStep[0m  [64/84], [94mLoss[0m : 2.11117
[1mStep[0m  [72/84], [94mLoss[0m : 2.32255
[1mStep[0m  [80/84], [94mLoss[0m : 2.17905

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.354, [92mTest[0m: 2.363, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33751
[1mStep[0m  [8/84], [94mLoss[0m : 2.32148
[1mStep[0m  [16/84], [94mLoss[0m : 2.14719
[1mStep[0m  [24/84], [94mLoss[0m : 2.32867
[1mStep[0m  [32/84], [94mLoss[0m : 2.18402
[1mStep[0m  [40/84], [94mLoss[0m : 2.23438
[1mStep[0m  [48/84], [94mLoss[0m : 2.39657
[1mStep[0m  [56/84], [94mLoss[0m : 2.09273
[1mStep[0m  [64/84], [94mLoss[0m : 1.99621
[1mStep[0m  [72/84], [94mLoss[0m : 2.07024
[1mStep[0m  [80/84], [94mLoss[0m : 2.15253

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.272, [92mTest[0m: 2.321, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.10562
[1mStep[0m  [8/84], [94mLoss[0m : 2.34890
[1mStep[0m  [16/84], [94mLoss[0m : 2.07502
[1mStep[0m  [24/84], [94mLoss[0m : 1.92884
[1mStep[0m  [32/84], [94mLoss[0m : 2.12345
[1mStep[0m  [40/84], [94mLoss[0m : 2.47048
[1mStep[0m  [48/84], [94mLoss[0m : 1.91319
[1mStep[0m  [56/84], [94mLoss[0m : 2.17233
[1mStep[0m  [64/84], [94mLoss[0m : 2.04670
[1mStep[0m  [72/84], [94mLoss[0m : 2.13364
[1mStep[0m  [80/84], [94mLoss[0m : 2.14908

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.178, [92mTest[0m: 2.377, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.06930
[1mStep[0m  [8/84], [94mLoss[0m : 1.97681
[1mStep[0m  [16/84], [94mLoss[0m : 2.10612
[1mStep[0m  [24/84], [94mLoss[0m : 2.32021
[1mStep[0m  [32/84], [94mLoss[0m : 2.10937
[1mStep[0m  [40/84], [94mLoss[0m : 2.31465
[1mStep[0m  [48/84], [94mLoss[0m : 2.19216
[1mStep[0m  [56/84], [94mLoss[0m : 2.18431
[1mStep[0m  [64/84], [94mLoss[0m : 2.27381
[1mStep[0m  [72/84], [94mLoss[0m : 2.38269
[1mStep[0m  [80/84], [94mLoss[0m : 2.03128

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.139, [92mTest[0m: 2.350, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.02484
[1mStep[0m  [8/84], [94mLoss[0m : 1.80963
[1mStep[0m  [16/84], [94mLoss[0m : 1.98667
[1mStep[0m  [24/84], [94mLoss[0m : 1.77108
[1mStep[0m  [32/84], [94mLoss[0m : 1.93045
[1mStep[0m  [40/84], [94mLoss[0m : 2.10455
[1mStep[0m  [48/84], [94mLoss[0m : 2.21108
[1mStep[0m  [56/84], [94mLoss[0m : 2.10458
[1mStep[0m  [64/84], [94mLoss[0m : 2.36859
[1mStep[0m  [72/84], [94mLoss[0m : 1.92949
[1mStep[0m  [80/84], [94mLoss[0m : 2.29566

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.067, [92mTest[0m: 2.395, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.06675
[1mStep[0m  [8/84], [94mLoss[0m : 2.04570
[1mStep[0m  [16/84], [94mLoss[0m : 1.97384
[1mStep[0m  [24/84], [94mLoss[0m : 1.95733
[1mStep[0m  [32/84], [94mLoss[0m : 1.86420
[1mStep[0m  [40/84], [94mLoss[0m : 1.76759
[1mStep[0m  [48/84], [94mLoss[0m : 1.92484
[1mStep[0m  [56/84], [94mLoss[0m : 1.95954
[1mStep[0m  [64/84], [94mLoss[0m : 1.86894
[1mStep[0m  [72/84], [94mLoss[0m : 1.87439
[1mStep[0m  [80/84], [94mLoss[0m : 2.34937

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.990, [92mTest[0m: 2.443, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.79481
[1mStep[0m  [8/84], [94mLoss[0m : 2.10692
[1mStep[0m  [16/84], [94mLoss[0m : 1.81468
[1mStep[0m  [24/84], [94mLoss[0m : 1.91468
[1mStep[0m  [32/84], [94mLoss[0m : 2.01832
[1mStep[0m  [40/84], [94mLoss[0m : 1.86716
[1mStep[0m  [48/84], [94mLoss[0m : 1.99685
[1mStep[0m  [56/84], [94mLoss[0m : 1.90564
[1mStep[0m  [64/84], [94mLoss[0m : 2.03038
[1mStep[0m  [72/84], [94mLoss[0m : 1.92356
[1mStep[0m  [80/84], [94mLoss[0m : 1.78512

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.964, [92mTest[0m: 2.411, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.89818
[1mStep[0m  [8/84], [94mLoss[0m : 2.06114
[1mStep[0m  [16/84], [94mLoss[0m : 2.01582
[1mStep[0m  [24/84], [94mLoss[0m : 1.72843
[1mStep[0m  [32/84], [94mLoss[0m : 1.60210
[1mStep[0m  [40/84], [94mLoss[0m : 2.09935
[1mStep[0m  [48/84], [94mLoss[0m : 2.00478
[1mStep[0m  [56/84], [94mLoss[0m : 2.14416
[1mStep[0m  [64/84], [94mLoss[0m : 2.01797
[1mStep[0m  [72/84], [94mLoss[0m : 1.92393
[1mStep[0m  [80/84], [94mLoss[0m : 1.74146

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.907, [92mTest[0m: 2.448, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.70213
[1mStep[0m  [8/84], [94mLoss[0m : 1.70057
[1mStep[0m  [16/84], [94mLoss[0m : 1.94516
[1mStep[0m  [24/84], [94mLoss[0m : 1.62431
[1mStep[0m  [32/84], [94mLoss[0m : 1.78479
[1mStep[0m  [40/84], [94mLoss[0m : 1.73124
[1mStep[0m  [48/84], [94mLoss[0m : 1.91410
[1mStep[0m  [56/84], [94mLoss[0m : 2.11041
[1mStep[0m  [64/84], [94mLoss[0m : 1.84409
[1mStep[0m  [72/84], [94mLoss[0m : 1.77422
[1mStep[0m  [80/84], [94mLoss[0m : 1.88524

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.858, [92mTest[0m: 2.433, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.68577
[1mStep[0m  [8/84], [94mLoss[0m : 1.81576
[1mStep[0m  [16/84], [94mLoss[0m : 1.48176
[1mStep[0m  [24/84], [94mLoss[0m : 1.84490
[1mStep[0m  [32/84], [94mLoss[0m : 1.76149
[1mStep[0m  [40/84], [94mLoss[0m : 2.02089
[1mStep[0m  [48/84], [94mLoss[0m : 1.94720
[1mStep[0m  [56/84], [94mLoss[0m : 1.84116
[1mStep[0m  [64/84], [94mLoss[0m : 1.73824
[1mStep[0m  [72/84], [94mLoss[0m : 1.76906
[1mStep[0m  [80/84], [94mLoss[0m : 1.87229

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.816, [92mTest[0m: 2.458, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.67610
[1mStep[0m  [8/84], [94mLoss[0m : 1.53957
[1mStep[0m  [16/84], [94mLoss[0m : 1.74876
[1mStep[0m  [24/84], [94mLoss[0m : 1.74651
[1mStep[0m  [32/84], [94mLoss[0m : 1.77781
[1mStep[0m  [40/84], [94mLoss[0m : 1.76707
[1mStep[0m  [48/84], [94mLoss[0m : 1.74409
[1mStep[0m  [56/84], [94mLoss[0m : 1.95290
[1mStep[0m  [64/84], [94mLoss[0m : 1.77459
[1mStep[0m  [72/84], [94mLoss[0m : 1.70595
[1mStep[0m  [80/84], [94mLoss[0m : 2.21483

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.785, [92mTest[0m: 2.478, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.76179
[1mStep[0m  [8/84], [94mLoss[0m : 1.65911
[1mStep[0m  [16/84], [94mLoss[0m : 1.70867
[1mStep[0m  [24/84], [94mLoss[0m : 1.71278
[1mStep[0m  [32/84], [94mLoss[0m : 1.70092
[1mStep[0m  [40/84], [94mLoss[0m : 1.94358
[1mStep[0m  [48/84], [94mLoss[0m : 1.91105
[1mStep[0m  [56/84], [94mLoss[0m : 2.11027
[1mStep[0m  [64/84], [94mLoss[0m : 1.60363
[1mStep[0m  [72/84], [94mLoss[0m : 1.93243
[1mStep[0m  [80/84], [94mLoss[0m : 1.90976

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.734, [92mTest[0m: 2.478, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.60657
[1mStep[0m  [8/84], [94mLoss[0m : 1.64139
[1mStep[0m  [16/84], [94mLoss[0m : 1.79336
[1mStep[0m  [24/84], [94mLoss[0m : 1.57167
[1mStep[0m  [32/84], [94mLoss[0m : 1.65874
[1mStep[0m  [40/84], [94mLoss[0m : 1.83846
[1mStep[0m  [48/84], [94mLoss[0m : 1.87679
[1mStep[0m  [56/84], [94mLoss[0m : 1.52285
[1mStep[0m  [64/84], [94mLoss[0m : 1.59778
[1mStep[0m  [72/84], [94mLoss[0m : 1.78240
[1mStep[0m  [80/84], [94mLoss[0m : 1.54337

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.698, [92mTest[0m: 2.478, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.60989
[1mStep[0m  [8/84], [94mLoss[0m : 1.59997
[1mStep[0m  [16/84], [94mLoss[0m : 1.68028
[1mStep[0m  [24/84], [94mLoss[0m : 1.57812
[1mStep[0m  [32/84], [94mLoss[0m : 1.70017
[1mStep[0m  [40/84], [94mLoss[0m : 1.58069
[1mStep[0m  [48/84], [94mLoss[0m : 1.68423
[1mStep[0m  [56/84], [94mLoss[0m : 1.61558
[1mStep[0m  [64/84], [94mLoss[0m : 1.47819
[1mStep[0m  [72/84], [94mLoss[0m : 1.68432
[1mStep[0m  [80/84], [94mLoss[0m : 1.53274

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.664, [92mTest[0m: 2.516, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.58792
[1mStep[0m  [8/84], [94mLoss[0m : 1.62224
[1mStep[0m  [16/84], [94mLoss[0m : 1.63560
[1mStep[0m  [24/84], [94mLoss[0m : 1.63969
[1mStep[0m  [32/84], [94mLoss[0m : 1.56149
[1mStep[0m  [40/84], [94mLoss[0m : 1.58455
[1mStep[0m  [48/84], [94mLoss[0m : 1.54248
[1mStep[0m  [56/84], [94mLoss[0m : 1.59122
[1mStep[0m  [64/84], [94mLoss[0m : 1.56217
[1mStep[0m  [72/84], [94mLoss[0m : 1.67361
[1mStep[0m  [80/84], [94mLoss[0m : 1.65235

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.636, [92mTest[0m: 2.498, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.90520
[1mStep[0m  [8/84], [94mLoss[0m : 1.58481
[1mStep[0m  [16/84], [94mLoss[0m : 1.39849
[1mStep[0m  [24/84], [94mLoss[0m : 1.66769
[1mStep[0m  [32/84], [94mLoss[0m : 1.58496
[1mStep[0m  [40/84], [94mLoss[0m : 1.56726
[1mStep[0m  [48/84], [94mLoss[0m : 1.39091
[1mStep[0m  [56/84], [94mLoss[0m : 1.50798
[1mStep[0m  [64/84], [94mLoss[0m : 1.60904
[1mStep[0m  [72/84], [94mLoss[0m : 1.49100
[1mStep[0m  [80/84], [94mLoss[0m : 1.46800

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.601, [92mTest[0m: 2.501, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.56458
[1mStep[0m  [8/84], [94mLoss[0m : 1.64210
[1mStep[0m  [16/84], [94mLoss[0m : 1.60331
[1mStep[0m  [24/84], [94mLoss[0m : 1.60452
[1mStep[0m  [32/84], [94mLoss[0m : 1.45215
[1mStep[0m  [40/84], [94mLoss[0m : 1.87434
[1mStep[0m  [48/84], [94mLoss[0m : 1.56260
[1mStep[0m  [56/84], [94mLoss[0m : 1.75890
[1mStep[0m  [64/84], [94mLoss[0m : 1.79155
[1mStep[0m  [72/84], [94mLoss[0m : 1.83982
[1mStep[0m  [80/84], [94mLoss[0m : 1.64362

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.579, [92mTest[0m: 2.548, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.45391
[1mStep[0m  [8/84], [94mLoss[0m : 1.47133
[1mStep[0m  [16/84], [94mLoss[0m : 1.52375
[1mStep[0m  [24/84], [94mLoss[0m : 1.45791
[1mStep[0m  [32/84], [94mLoss[0m : 1.45087
[1mStep[0m  [40/84], [94mLoss[0m : 1.62019
[1mStep[0m  [48/84], [94mLoss[0m : 1.42067
[1mStep[0m  [56/84], [94mLoss[0m : 1.66962
[1mStep[0m  [64/84], [94mLoss[0m : 1.53473
[1mStep[0m  [72/84], [94mLoss[0m : 1.38590
[1mStep[0m  [80/84], [94mLoss[0m : 1.93131

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.540, [92mTest[0m: 2.505, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.46456
[1mStep[0m  [8/84], [94mLoss[0m : 1.40490
[1mStep[0m  [16/84], [94mLoss[0m : 1.45779
[1mStep[0m  [24/84], [94mLoss[0m : 1.49880
[1mStep[0m  [32/84], [94mLoss[0m : 1.52269
[1mStep[0m  [40/84], [94mLoss[0m : 1.38911
[1mStep[0m  [48/84], [94mLoss[0m : 1.58378
[1mStep[0m  [56/84], [94mLoss[0m : 1.51748
[1mStep[0m  [64/84], [94mLoss[0m : 1.65683
[1mStep[0m  [72/84], [94mLoss[0m : 1.73361
[1mStep[0m  [80/84], [94mLoss[0m : 1.38228

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.538, [92mTest[0m: 2.550, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.31384
[1mStep[0m  [8/84], [94mLoss[0m : 1.43648
[1mStep[0m  [16/84], [94mLoss[0m : 1.49967
[1mStep[0m  [24/84], [94mLoss[0m : 1.53806
[1mStep[0m  [32/84], [94mLoss[0m : 1.50451
[1mStep[0m  [40/84], [94mLoss[0m : 1.57113
[1mStep[0m  [48/84], [94mLoss[0m : 1.31739
[1mStep[0m  [56/84], [94mLoss[0m : 1.57858
[1mStep[0m  [64/84], [94mLoss[0m : 1.32410
[1mStep[0m  [72/84], [94mLoss[0m : 1.56997
[1mStep[0m  [80/84], [94mLoss[0m : 1.68623

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.463, [92mTest[0m: 2.538, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.45554
[1mStep[0m  [8/84], [94mLoss[0m : 1.33279
[1mStep[0m  [16/84], [94mLoss[0m : 1.44525
[1mStep[0m  [24/84], [94mLoss[0m : 1.29040
[1mStep[0m  [32/84], [94mLoss[0m : 1.34433
[1mStep[0m  [40/84], [94mLoss[0m : 1.63332
[1mStep[0m  [48/84], [94mLoss[0m : 1.30309
[1mStep[0m  [56/84], [94mLoss[0m : 1.37466
[1mStep[0m  [64/84], [94mLoss[0m : 1.43530
[1mStep[0m  [72/84], [94mLoss[0m : 1.54494
[1mStep[0m  [80/84], [94mLoss[0m : 1.41951

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.464, [92mTest[0m: 2.466, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.40163
[1mStep[0m  [8/84], [94mLoss[0m : 1.43657
[1mStep[0m  [16/84], [94mLoss[0m : 1.25290
[1mStep[0m  [24/84], [94mLoss[0m : 1.46688
[1mStep[0m  [32/84], [94mLoss[0m : 1.37247
[1mStep[0m  [40/84], [94mLoss[0m : 1.41185
[1mStep[0m  [48/84], [94mLoss[0m : 1.23856
[1mStep[0m  [56/84], [94mLoss[0m : 1.50986
[1mStep[0m  [64/84], [94mLoss[0m : 1.34097
[1mStep[0m  [72/84], [94mLoss[0m : 1.66176
[1mStep[0m  [80/84], [94mLoss[0m : 1.63171

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.443, [92mTest[0m: 2.507, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.35527
[1mStep[0m  [8/84], [94mLoss[0m : 1.45441
[1mStep[0m  [16/84], [94mLoss[0m : 1.53102
[1mStep[0m  [24/84], [94mLoss[0m : 1.37173
[1mStep[0m  [32/84], [94mLoss[0m : 1.19692
[1mStep[0m  [40/84], [94mLoss[0m : 1.18253
[1mStep[0m  [48/84], [94mLoss[0m : 1.33103
[1mStep[0m  [56/84], [94mLoss[0m : 1.25891
[1mStep[0m  [64/84], [94mLoss[0m : 1.20372
[1mStep[0m  [72/84], [94mLoss[0m : 1.33001
[1mStep[0m  [80/84], [94mLoss[0m : 1.40871

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.419, [92mTest[0m: 2.501, [96mlr[0m: 0.006772500000000002
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 23 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.482
====================================

Phase 2 - Evaluation MAE:  2.481600420815604
MAE score P1        2.326082
MAE score P2          2.4816
loss                 1.41876
learning_rate       0.007525
batch_size               128
hidden_sizes      [300, 100]
epochs                    30
activation           sigmoid
optimizer                sgd
early stopping          True
dropout                  0.3
momentum                 0.9
weight_decay           0.001
Name: 15, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
